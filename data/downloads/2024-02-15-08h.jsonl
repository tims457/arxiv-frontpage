{"created":"2024-02-14 18:59:33","title":"AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability","abstract":"This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing interactive examples may inadvertently hurt few-shot performance. (3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance. (4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.","sentences":["This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS).","The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves.","We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs.","Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs.","(2) Naively providing interactive examples may inadvertently hurt few-shot performance.","(3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance.","(4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend.","We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning.","The code is available at https://github.com/UCSC-VLAA/AQA-Bench."],"url":"http://arxiv.org/abs/2402.09404v1","category":"cs.CL"}
{"created":"2024-02-14 18:58:40","title":"Reinforcement Learning from Human Feedback with Active Queries","abstract":"Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.","sentences":["Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF).","Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect.","In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods.","We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\\Delta$ is the sub-optimality gap over all the contexts.","We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs.","Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method."],"url":"http://arxiv.org/abs/2402.09401v1","category":"cs.LG"}
{"created":"2024-02-14 18:54:56","title":"Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference","abstract":"Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.","sentences":["Many computational factors limit broader deployment of large language models.","In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding.","While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens.","To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps.","Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient."],"url":"http://arxiv.org/abs/2402.09398v1","category":"cs.LG"}
{"created":"2024-02-14 18:54:23","title":"On the Assessment of Bootstrap Intervals for Samples of Fixed Size","abstract":"A reasonable confidence interval should have a confidence coefficient no less than the given nominal level and a small expected length to reliably and accurately estimate the parameter of interest, and the bootstrap interval is considered to be an efficient interval estimation technique. In this paper, we offer a first attempt at computing the coverage probability and expected length of a parametric or percentile bootstrap interval by exact probabilistic calculation for any fixed sample size. This method is applied to the basic bootstrap intervals for functions of binomial proportions and a normal mean. None of these intervals, however, are found to have a correct confidence coefficient, which leads to illogical conclusions including that the bootstrap interval is narrower than the z-interval when estimating a normal mean. This raises a general question of how to utilize bootstrap intervals appropriately in practice since the sample size is typically fixed.","sentences":["A reasonable confidence interval should have a confidence coefficient no less than the given nominal level and a small expected length to reliably and accurately estimate the parameter of interest, and the bootstrap interval is considered to be an efficient interval estimation technique.","In this paper, we offer a first attempt at computing the coverage probability and expected length of a parametric or percentile bootstrap interval by exact probabilistic calculation for any fixed sample size.","This method is applied to the basic bootstrap intervals for functions of binomial proportions and a normal mean.","None of these intervals, however, are found to have a correct confidence coefficient, which leads to illogical conclusions including that the bootstrap interval is narrower than the z-interval when estimating a normal mean.","This raises a general question of how to utilize bootstrap intervals appropriately in practice since the sample size is typically fixed."],"url":"http://arxiv.org/abs/2402.09397v1","category":"math.ST"}
{"created":"2024-02-14 18:53:29","title":"Strong existence for free discontinuity problems in linear elasticity","abstract":"In this note we show Ahlfors-regularity for a large class of quasiminimizers of the Griffith functional. This allows us to prove that, for a range of free discontinuity problems in linear elasticity with anisotropic, cohesive, or heterogeneous behavior, minimizers have an essentially closed jump set and are thus strong minimizers. Our notion of quasiminimality is inspired by and generalizes previous notions in the literature for the Mumford-Shah functional, and comprises functions which locally close to the crack have at most a fixed percentage of excess crack relative to minimizers. As for the case of minimizers of the Griffith functional, our proof of Ahlfors-regularity relies on contradiction-compactness and an approximation result for GSBD functions, showing the robustness of this approach with respect to generalization of bulk and surface densities.","sentences":["In this note we show Ahlfors-regularity for a large class of quasiminimizers of the Griffith functional.","This allows us to prove that, for a range of free discontinuity problems in linear elasticity with anisotropic, cohesive, or heterogeneous behavior, minimizers have an essentially closed jump set and are thus strong minimizers.","Our notion of quasiminimality is inspired by and generalizes previous notions in the literature for the Mumford-Shah functional, and comprises functions which locally close to the crack have at most a fixed percentage of excess crack relative to minimizers.","As for the case of minimizers of the Griffith functional, our proof of Ahlfors-regularity relies on contradiction-compactness and an approximation result for GSBD functions, showing the robustness of this approach with respect to generalization of bulk and surface densities."],"url":"http://arxiv.org/abs/2402.09396v1","category":"math.AP"}
{"created":"2024-02-14 18:45:14","title":"Long-form evaluation of model editing","abstract":"Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\\textbf{\\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods. Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues.","sentences":["Evaluations of model editing currently only use the `next few token' completions after a prompt.","As a result, the impact of these methods on longer natural language generation is largely unknown.","We introduce long-form evaluation of model editing (\\textbf{\\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings.","Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings.","Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods.","Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods.","Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues."],"url":"http://arxiv.org/abs/2402.09394v1","category":"cs.CL"}
{"created":"2024-02-14 18:44:48","title":"A superstatistical measure of distance from canonical equilibrium","abstract":"Non-equilibrium systems in steady states are commonly described by generalized statistical mechanical theories such as non-extensive statistics and superstatistics. Superstatistics assumes that the inverse temperature $\\beta = 1/(k_B T)$ follows some pre-established statistical distribution, however, it has been previously proved (Physica A 505, 864-870 [2018]) that $\\beta$ cannot be associated to an observable function $B(\\boldsymbol{\\Gamma})$ of the microstates $\\boldsymbol{\\Gamma}$. In this work, we provide an information-theoretical interpretation of this theorem by introducing a new quantity $\\mathcal{D}$, the mutual information between $\\beta$ and $\\boldsymbol{\\Gamma}$. Our results show that $\\mathcal{D}$ is also a measure of departure from canonical equilibrium, and reveal a minimum, non-zero uncertainty about $\\beta$ given $\\boldsymbol{\\Gamma}$ for every non-canonical superstatistical ensemble. This supports the use of the mutual information as a descriptor of complexity and correlation in complex systems, also providing in some cases a sound basis for the use of Tsallis' entropic index $q$ as a measure of distance from equilibrium, being in those cases a proxy for $\\mathcal{D}$.","sentences":["Non-equilibrium systems in steady states are commonly described by generalized statistical mechanical theories such as non-extensive statistics and superstatistics.","Superstatistics assumes that the inverse temperature $\\beta = 1/(k_B T)$ follows some pre-established statistical distribution, however, it has been previously proved (Physica A 505, 864-870 [2018]) that $\\beta$ cannot be associated to an observable function $B(\\boldsymbol{\\Gamma})$ of the microstates $\\boldsymbol{\\Gamma}$.","In this work, we provide an information-theoretical interpretation of this theorem by introducing a new quantity $\\mathcal{D}$, the mutual information between $\\beta$ and $\\boldsymbol{\\Gamma}$. Our results show that $\\mathcal{D}$ is also a measure of departure from canonical equilibrium, and reveal a minimum, non-zero uncertainty about $\\beta$ given $\\boldsymbol{\\Gamma}$ for every non-canonical superstatistical ensemble.","This supports the use of the mutual information as a descriptor of complexity and correlation in complex systems, also providing in some cases a sound basis for the use of Tsallis' entropic index $q$ as a measure of distance from equilibrium, being in those cases a proxy for $\\mathcal{D}$."],"url":"http://arxiv.org/abs/2402.09393v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-14 18:43:19","title":"LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning","abstract":"Over the recent years, research and development in adaptive bitrate (ABR) algorithms for live video streaming have been successful in improving users' quality of experience (QoE) by reducing latency to near real-time levels while delivering higher bitrate videos with minimal rebuffering time. However, the QoE models used by these ABR algorithms do not take into account that a large portion of live video streaming clients use mobile devices where a higher bitrate does not necessarily translate into higher perceived quality. Ignoring perceived quality results in playing videos at higher bitrates without a significant increase in perceptual video quality and becomes a burden for battery-constrained mobile devices due to higher energy consumption. In this paper, we propose LL-GABR, a deep reinforcement learning approach that models the QoE using perceived video quality instead of bitrate and uses energy consumption along with other metrics like latency, rebuffering events, and smoothness. LL-GABR makes no assumptions about the underlying video, environment, or network settings and can operate flexibly on different video titles, each having a different bitrate encoding ladder without additional re-training, unlike existing learning-based ABRs. Trace-driven experimental results show that LL-GABR outperforms the state-of-the-art approaches by up to 44% in terms of perceptual QoE and a 73% increase in energy efficiency as a result of reducing net energy consumption by 11%.","sentences":["Over the recent years, research and development in adaptive bitrate (ABR) algorithms for live video streaming have been successful in improving users' quality of experience (QoE) by reducing latency to near real-time levels while delivering higher bitrate videos with minimal rebuffering time.","However, the QoE models used by these ABR algorithms do not take into account that a large portion of live video streaming clients use mobile devices where a higher bitrate does not necessarily translate into higher perceived quality.","Ignoring perceived quality results in playing videos at higher bitrates without a significant increase in perceptual video quality and becomes a burden for battery-constrained mobile devices due to higher energy consumption.","In this paper, we propose LL-GABR, a deep reinforcement learning approach that models the QoE using perceived video quality instead of bitrate and uses energy consumption along with other metrics like latency, rebuffering events, and smoothness.","LL-GABR makes no assumptions about the underlying video, environment, or network settings and can operate flexibly on different video titles, each having a different bitrate encoding ladder without additional re-training, unlike existing learning-based ABRs.","Trace-driven experimental results show that LL-GABR outperforms the state-of-the-art approaches by up to 44% in terms of perceptual QoE and a 73% increase in energy efficiency as a result of reducing net energy consumption by 11%."],"url":"http://arxiv.org/abs/2402.09392v1","category":"cs.MM"}
{"created":"2024-02-14 18:42:25","title":"LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset","abstract":"Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks. We further conduct analysis on the impact of trainable parameters, providing insights for future research.","sentences":["Chemistry plays a crucial role in many domains, such as drug discovery and material science.","While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low.","In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models.","The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct.","It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry.","Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks.","We further conduct analysis on the impact of trainable parameters, providing insights for future research."],"url":"http://arxiv.org/abs/2402.09391v1","category":"cs.AI"}
{"created":"2024-02-14 18:41:19","title":"HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation","abstract":"With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning. The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality. This methodology introduces a weighted system in majority voting, prioritizing answers based on the citation quality of their thoughts. Additionally, we propose a scoring mechanism for evaluating retrieved passages, considering factors such as citation frequency and quality, self-consistency confidence, and the retrieval module's ranking. Experiments reveal that HGOT outperforms other retrieval-augmented in-context learning methods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and Retrieve-then-Read on different datasets by as much as $7\\%$, demonstrating its efficacy in enhancing the factuality of LLMs.","sentences":["With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns.","To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning.","The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries.","It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality.","This methodology introduces a weighted system in majority voting, prioritizing answers based on the citation quality of their thoughts.","Additionally, we propose a scoring mechanism for evaluating retrieved passages, considering factors such as citation frequency and quality, self-consistency confidence, and the retrieval module's ranking.","Experiments reveal that HGOT outperforms other retrieval-augmented in-context learning methods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and Retrieve-then-Read on different datasets by as much as $7\\%$, demonstrating its efficacy in enhancing the factuality of LLMs."],"url":"http://arxiv.org/abs/2402.09390v1","category":"cs.AI"}
{"created":"2024-02-14 18:37:47","title":"Entropy-regularized Point-based Value Iteration","abstract":"Model-based planners for partially observable problems must accommodate both model uncertainty during planning and goal uncertainty during objective inference. However, model-based planners may be brittle under these types of uncertainty because they rely on an exact model and tend to commit to a single optimal behavior. Inspired by results in the model-free setting, we propose an entropy-regularized model-based planner for partially observable problems. Entropy regularization promotes policy robustness for planning and objective inference by encouraging policies to be no more committed to a single action than necessary. We evaluate the robustness and objective inference performance of entropy-regularized policies in three problem domains. Our results show that entropy-regularized policies outperform non-entropy-regularized baselines in terms of higher expected returns under modeling errors and higher accuracy during objective inference.","sentences":["Model-based planners for partially observable problems must accommodate both model uncertainty during planning and goal uncertainty during objective inference.","However, model-based planners may be brittle under these types of uncertainty because they rely on an exact model and tend to commit to a single optimal behavior.","Inspired by results in the model-free setting, we propose an entropy-regularized model-based planner for partially observable problems.","Entropy regularization promotes policy robustness for planning and objective inference by encouraging policies to be no more committed to a single action than necessary.","We evaluate the robustness and objective inference performance of entropy-regularized policies in three problem domains.","Our results show that entropy-regularized policies outperform non-entropy-regularized baselines in terms of higher expected returns under modeling errors and higher accuracy during objective inference."],"url":"http://arxiv.org/abs/2402.09388v1","category":"cs.AI"}
{"created":"2024-02-14 18:35:26","title":"Introduction to Physically Unclonable Fuctions: Properties and Applications","abstract":"During the last years, Physically Unclonable Functions (PUFs) have become a very important research area in the field of hardware security due to their capability of generating volatile secret keys as well as providing a low-cost authentication. In this paper, an introduction to Physically Unclonable Functions is given, including their definition, properties and applications. Finally, as an example of how to design a PUF, the general structure of a ring oscillator PUF is presented.","sentences":["During the last years, Physically Unclonable Functions (PUFs) have become a very important research area in the field of hardware security due to their capability of generating volatile secret keys as well as providing a low-cost authentication.","In this paper, an introduction to Physically Unclonable Functions is given, including their definition, properties and applications.","Finally, as an example of how to design a PUF, the general structure of a ring oscillator PUF is presented."],"url":"http://arxiv.org/abs/2402.09386v1","category":"cs.CR"}
{"created":"2024-02-14 18:32:30","title":"Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions","abstract":"A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent's information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm's prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Common restrictions on algorithms, such as keeping a \"human-in-the-loop\" or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfectly aligned agents and state-revealing signals. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.","sentences":["A principal designs an algorithm that generates a publicly observable prediction of a binary state.","She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment.","We study the optimal design of the prediction algorithm and the delegation rule in such environments.","Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent's information.","(2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm's prediction.","Instead, the optimal algorithm may provide more information about one state and restrict information about the other.","(3) Common restrictions on algorithms, such as keeping a \"human-in-the-loop\" or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfectly aligned agents and state-revealing signals.","These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers."],"url":"http://arxiv.org/abs/2402.09384v1","category":"econ.TH"}
{"created":"2024-02-14 18:26:58","title":"GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly","abstract":"Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment. This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities. Detecting repeats is a crucial first step in overcoming these challenges. To address this issue, we propose GraSSRep, a novel approach that leverages the assembly graph's structure through graph neural networks (GNNs) within a self-supervised learning framework to classify DNA sequences into repetitive and non-repetitive categories. Specifically, we frame this problem as a node classification task within a metagenomic assembly graph. In a self-supervised fashion, we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the nodes. We then use those pseudo-labels to train a GNN embedding and a random forest classifier to propagate the labels to the remaining nodes. In this way, GraSSRep combines sequencing features with pre-defined and learned graph features to achieve state-of-the-art performance in repeat detection. We evaluate our method using simulated and synthetic metagenomic datasets. The results on the simulated data highlight our GraSSRep's robustness to repeat attributes, demonstrating its effectiveness in handling the complexity of repeated sequences. Additionally, our experiments with synthetic metagenomic datasets reveal that incorporating the graph structure and the GNN enhances our detection performance. Finally, in comparative analyses, GraSSRep outperforms existing repeat detection tools with respect to precision and recall.","sentences":["Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment.","This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities.","Detecting repeats is a crucial first step in overcoming these challenges.","To address this issue, we propose GraSSRep, a novel approach that leverages the assembly graph's structure through graph neural networks (GNNs) within a self-supervised learning framework to classify DNA sequences into repetitive and non-repetitive categories.","Specifically, we frame this problem as a node classification task within a metagenomic assembly graph.","In a self-supervised fashion, we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the nodes.","We then use those pseudo-labels to train a GNN embedding and a random forest classifier to propagate the labels to the remaining nodes.","In this way, GraSSRep combines sequencing features with pre-defined and learned graph features to achieve state-of-the-art performance in repeat detection.","We evaluate our method using simulated and synthetic metagenomic datasets.","The results on the simulated data highlight our GraSSRep's robustness to repeat attributes, demonstrating its effectiveness in handling the complexity of repeated sequences.","Additionally, our experiments with synthetic metagenomic datasets reveal that incorporating the graph structure and the GNN enhances our detection performance.","Finally, in comparative analyses, GraSSRep outperforms existing repeat detection tools with respect to precision and recall."],"url":"http://arxiv.org/abs/2402.09381v1","category":"cs.LG"}
{"created":"2024-02-14 18:24:41","title":"MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech","abstract":"Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \\url{https://mobilespeech.github.io/} .","sentences":["Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts.","However, all previous work has been developed for cloud-based systems.","Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness.","Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time.","Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process.","Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation.","2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD.","We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality.","MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices.","Audio samples are available at \\url{https://mobilespeech.github.io/} ."],"url":"http://arxiv.org/abs/2402.09378v1","category":"eess.AS"}
{"created":"2024-02-14 18:22:45","title":"Exactly solvable Hamiltonian fragments obtained from a direct sum of Lie algebras","abstract":"Exactly solvable Hamiltonians are useful in the study of quantum many-body systems using quantum computers. In the variational quantum eigensolver, a decomposition of the target Hamiltonian into exactly solvable fragments can be used for evaluation of the energies via repeated quantum measurements. In this work, we apply more general classes of exactly solvable qubit Hamiltonians than previously considered to address the Hamiltonian measurement problem. The most general exactly solvable Hamiltonians are defined by the condition that, within each simultaneous eigenspace of a set of Pauli symmetries, the Hamiltonian acts effectively as an element of a direct sum of so(N) Lie algebras, and can therefore be measured using a combination of unitaries in the associated Lie group, Clifford unitaries, and mid-circuit measurements. Application of such Hamiltonians to decomposing molecular electronic Hamiltonians via graph partitioning techniques shows a reduction in the total number of measurements required to estimate the expectation value compared with previously used exactly solvable qubit Hamiltonians.","sentences":["Exactly solvable Hamiltonians are useful in the study of quantum many-body systems using quantum computers.","In the variational quantum eigensolver, a decomposition of the target Hamiltonian into exactly solvable fragments can be used for evaluation of the energies via repeated quantum measurements.","In this work, we apply more general classes of exactly solvable qubit Hamiltonians than previously considered to address the Hamiltonian measurement problem.","The most general exactly solvable Hamiltonians are defined by the condition that, within each simultaneous eigenspace of a set of Pauli symmetries, the Hamiltonian acts effectively as an element of a direct sum of so(N) Lie algebras, and can therefore be measured using a combination of unitaries in the associated Lie group, Clifford unitaries, and mid-circuit measurements.","Application of such Hamiltonians to decomposing molecular electronic Hamiltonians via graph partitioning techniques shows a reduction in the total number of measurements required to estimate the expectation value compared with previously used exactly solvable qubit Hamiltonians."],"url":"http://arxiv.org/abs/2402.09376v1","category":"quant-ph"}
{"created":"2024-02-14 18:21:19","title":"Local work-function manipulation by external optical stimulation","abstract":"Strongly differing static dipole moments of the trans and cis isomers of photochromic azobenzene allow for optical switching the work function of azobenzene-functionalized self-assembled monolayers (SAMs). We apply these properties in a fundamental experiment to manipulate the area size of the switched SAM. Azobenzene molecules were excited by ultraviolet laser illumination and the transient isomerization profile of the SAM was spatially resolved recording photoemission electron microscopy (PEEM) images. Thereby we demonstrate the spatial tuning of the SAM work-function and discuss the role of the laser spot-profile in generating sharp edges or gradual changes of the work function.","sentences":["Strongly differing static dipole moments of the trans and cis isomers of photochromic azobenzene allow for optical switching the work function of azobenzene-functionalized self-assembled monolayers (SAMs).","We apply these properties in a fundamental experiment to manipulate the area size of the switched SAM.","Azobenzene molecules were excited by ultraviolet laser illumination and the transient isomerization profile of the SAM was spatially resolved recording photoemission electron microscopy (PEEM) images.","Thereby we demonstrate the spatial tuning of the SAM work-function and discuss the role of the laser spot-profile in generating sharp edges or gradual changes of the work function."],"url":"http://arxiv.org/abs/2402.09375v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 18:18:33","title":"Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge","abstract":"Rib fractures are a common and potentially severe injury that can be challenging and labor-intensive to detect in CT scans. While there have been efforts to address this field, the lack of large-scale annotated datasets and evaluation benchmarks has hindered the development and validation of deep learning algorithms. To address this issue, the RibFrac Challenge was introduced, providing a benchmark dataset of over 5,000 rib fractures from 660 CT scans, with voxel-level instance mask annotations and diagnosis labels for four clinical categories (buckle, nondisplaced, displaced, or segmental). The challenge includes two tracks: a detection (instance segmentation) track evaluated by an FROC-style metric and a classification track evaluated by an F1-style metric. During the MICCAI 2020 challenge period, 243 results were evaluated, and seven teams were invited to participate in the challenge summary. The analysis revealed that several top rib fracture detection solutions achieved performance comparable or even better than human experts. Nevertheless, the current rib fracture classification solutions are hardly clinically applicable, which can be an interesting area in the future. As an active benchmark and research resource, the data and online evaluation of the RibFrac Challenge are available at the challenge website. As an independent contribution, we have also extended our previous internal baseline by incorporating recent advancements in large-scale pretrained networks and point-based rib segmentation techniques. The resulting FracNet+ demonstrates competitive performance in rib fracture detection, which lays a foundation for further research and development in AI-assisted rib fracture detection and diagnosis.","sentences":["Rib fractures are a common and potentially severe injury that can be challenging and labor-intensive to detect in CT scans.","While there have been efforts to address this field, the lack of large-scale annotated datasets and evaluation benchmarks has hindered the development and validation of deep learning algorithms.","To address this issue, the RibFrac Challenge was introduced, providing a benchmark dataset of over 5,000 rib fractures from 660 CT scans, with voxel-level instance mask annotations and diagnosis labels for four clinical categories (buckle, nondisplaced, displaced, or segmental).","The challenge includes two tracks: a detection (instance segmentation) track evaluated by an FROC-style metric and a classification track evaluated by an F1-style metric.","During the MICCAI 2020 challenge period, 243 results were evaluated, and seven teams were invited to participate in the challenge summary.","The analysis revealed that several top rib fracture detection solutions achieved performance comparable or even better than human experts.","Nevertheless, the current rib fracture classification solutions are hardly clinically applicable, which can be an interesting area in the future.","As an active benchmark and research resource, the data and online evaluation of the RibFrac Challenge are available at the challenge website.","As an independent contribution, we have also extended our previous internal baseline by incorporating recent advancements in large-scale pretrained networks and point-based rib segmentation techniques.","The resulting FracNet+ demonstrates competitive performance in rib fracture detection, which lays a foundation for further research and development in AI-assisted rib fracture detection and diagnosis."],"url":"http://arxiv.org/abs/2402.09372v1","category":"eess.IV"}
{"created":"2024-02-14 18:18:29","title":"Transformers Can Achieve Length Generalization But Not Robustly","abstract":"Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.","sentences":["Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models.","This issue persists even with large-scale Transformers handling relatively straightforward tasks.","In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers.","We show that the success of length generalization is intricately linked to the data format and the type of position encoding.","Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length.","Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds."],"url":"http://arxiv.org/abs/2402.09371v1","category":"cs.LG"}
{"created":"2024-02-14 18:17:45","title":"Pseudorandom Error-Correcting Codes","abstract":"We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationally indistinguishable from text output by the original model. This is the first undetectable watermarking scheme that can tolerate a constant rate of errors.   Our second application is to steganography, where a secret message is hidden in innocent-looking content. We present a constant-rate stateless steganography scheme with robustness to a constant rate of substitutions. Ours is the first stateless steganography scheme with provable steganographic security and any robustness to errors.","sentences":["We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary.","Efficient decoding of corrupted codewords is possible with the help of a decoding key.   ","We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions.","Specifically, pseudorandomness is based on either $2^{O(\\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   ","As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions.","The watermark is undetectable in the sense that any number of samples of watermarked text are computationally indistinguishable from text output by the original model.","This is the first undetectable watermarking scheme that can tolerate a constant rate of errors.   ","Our second application is to steganography, where a secret message is hidden in innocent-looking content.","We present a constant-rate stateless steganography scheme with robustness to a constant rate of substitutions.","Ours is the first stateless steganography scheme with provable steganographic security and any robustness to errors."],"url":"http://arxiv.org/abs/2402.09370v1","category":"cs.CR"}
{"created":"2024-02-14 18:13:51","title":"Magic-Me: Identity-Specific Video Customized Diffusion","abstract":"Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at https://github.com/Zhen-Dong/Magic-Me.","sentences":["Creating content for a specific identity (ID) has shown significant interest in the field of generative models.","In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable.","However, extending it to video generation is not well explored.","In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD).","With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent.","To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   ","Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines.","Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability.","The codes are available at https://github.com/Zhen-Dong/Magic-Me."],"url":"http://arxiv.org/abs/2402.09368v1","category":"cs.CV"}
{"created":"2024-02-14 18:12:52","title":"A Note on the Origin of Inertia","abstract":"The question of where the inertial properties of matter come from has been open for a long time. Isaac Newton considered inertia an intrinsic property of matter. Ernst Mach held a different view whereby the inertia of a body comes from its interaction with the rest of the universe. This idea is known today as Mach's principle. We discuss Mach's principle based on transactional gravity, the recently developed completion of the entropic gravity program by the physics of quantum events induced by transactions. A consequence of the analysis is a fundamental relation between the gravitational constant G and the total mass in the causal universe, derived by means of entropic principles.","sentences":["The question of where the inertial properties of matter come from has been open for a long time.","Isaac Newton considered inertia an intrinsic property of matter.","Ernst Mach held a different view whereby the inertia of a body comes from its interaction with the rest of the universe.","This idea is known today as Mach's principle.","We discuss Mach's principle based on transactional gravity, the recently developed completion of the entropic gravity program by the physics of quantum events induced by transactions.","A consequence of the analysis is a fundamental relation between the gravitational constant G and the total mass in the causal universe, derived by means of entropic principles."],"url":"http://arxiv.org/abs/2402.09365v1","category":"gr-qc"}
{"created":"2024-02-14 18:11:42","title":"Performance-Complexity-Latency Trade-offs of Concatenated RS-BCH Codes","abstract":"Using a generating function approach, a computationally tractable expression is derived to predict the frame error rate arising at the output of the binary symmetric channel when a number of outer Reed-Solomon codes are concatenated with a number of inner Bose-Ray-Chaudhuri-Hocquenghem codes, thereby obviating the need for time-consuming Monte Carlo simulations. Measuring (a) code performance via the gap to the Shannon limit, (b) decoding complexity via an estimate of the number of operations per decoded bit, and (c) decoding latency by the overall frame length, a code search is performed to determine the Pareto frontier for performance-complexity-latency trade-offs.","sentences":["Using a generating function approach, a computationally tractable expression is derived to predict the frame error rate arising at the output of the binary symmetric channel when a number of outer Reed-Solomon codes are concatenated with a number of inner Bose-Ray-Chaudhuri-Hocquenghem codes, thereby obviating the need for time-consuming Monte Carlo simulations.","Measuring (a) code performance via the gap to the Shannon limit, (b) decoding complexity via an estimate of the number of operations per decoded bit, and (c) decoding latency by the overall frame length, a code search is performed to determine the Pareto frontier for performance-complexity-latency trade-offs."],"url":"http://arxiv.org/abs/2402.09364v1","category":"cs.IT"}
{"created":"2024-02-14 18:08:38","title":"Small instanton-induced flavor invariants and the axion potential","abstract":"Small instantons which increase the axion mass due to an appropriate modification of QCD at a UV scale $\\Lambda_{\\rm SI}$, can also enhance the effect of CP-violating operators to shift the axion potential minimum by an amount, $\\theta_{\\rm ind}$, proportional to the flavorful couplings in the SMEFT. Since physical observables must be flavor basis independent, we construct a basis of determinant-like flavor invariants that arise from instanton calculations containing the effects of dimension-six CP-odd operators at the scale $\\require{cancel}\\Lambda_{\\cancel{\\rm CP}}$. This new basis provides a more reliable estimate of the shift $\\theta_{\\rm ind}$, that is severely constrained by neutron electric dipole moment experiments. In particular, for the case of four-quark, semi-leptonic and gluon dipole operators, these invariants are then used to provide improved limits on the ratio of scales $\\require{cancel}\\Lambda_{\\rm SI}/\\Lambda_{\\cancel{\\rm CP}}$ for different flavor scenarios. The CP-odd flavor invariants also provide a classification of the leading effects from Wilson coefficients, and as an example, we show that a semi-leptonic four-fermion operator is subdominant compared to the four-quark operators. More generally, the flavor invariants, together with an instanton NDA, can be used to more accurately estimate small instanton effects in the axion potential that arise from any SMEFT operator.","sentences":["Small instantons which increase the axion mass due to an appropriate modification of QCD at a UV scale $\\Lambda_{\\rm SI}$, can also enhance the effect of CP-violating operators to shift the axion potential minimum by an amount, $\\theta_{\\rm ind}$, proportional to the flavorful couplings in the SMEFT.","Since physical observables must be flavor basis independent, we construct a basis of determinant-like flavor invariants that arise from instanton calculations containing the effects of dimension-six CP-odd operators at the scale $\\require{cancel}\\Lambda_{\\cancel{\\rm CP}}$. This new basis provides a more reliable estimate of the shift $\\theta_{\\rm ind}$, that is severely constrained by neutron electric dipole moment experiments.","In particular, for the case of four-quark, semi-leptonic and gluon dipole operators, these invariants are then used to provide improved limits on the ratio of scales $\\require{cancel}\\Lambda_{\\rm SI}/\\Lambda_{\\cancel{\\rm CP}}$ for different flavor scenarios.","The CP-odd flavor invariants also provide a classification of the leading effects from Wilson coefficients, and as an example, we show that a semi-leptonic four-fermion operator is subdominant compared to the four-quark operators.","More generally, the flavor invariants, together with an instanton NDA, can be used to more accurately estimate small instanton effects in the axion potential that arise from any SMEFT operator."],"url":"http://arxiv.org/abs/2402.09361v1","category":"hep-ph"}
{"created":"2024-02-14 18:04:36","title":"HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference","abstract":"Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \\approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top-$k$ rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate top-$k$ operator. We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by $1.47\\times$ on a single TPUv5e device.","sentences":["Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache.","On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where","$k \\approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency.","However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains.","To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation).","HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top-$k$ rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate top-$k$ operator.","We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by $1.47\\times$ on a single TPUv5e device."],"url":"http://arxiv.org/abs/2402.09360v1","category":"cs.LG"}
{"created":"2024-02-14 18:03:58","title":"Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy","abstract":"Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns. ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream. Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes. ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually. Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements. Herein, we propose to use sparse tensor neural networks to reduce memory usage in 2D and to improve the scaling of the memory requirement for the extension of deep learning architecture to 3D. We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information. When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks. In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings. We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time.","sentences":["Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns.","ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream.","Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes.","ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually.","Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements.","Herein, we propose to use sparse tensor neural networks to reduce memory usage in 2D and to improve the scaling of the memory requirement for the extension of deep learning architecture to 3D.","We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information.","When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks.","In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings.","We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time."],"url":"http://arxiv.org/abs/2402.09359v1","category":"eess.IV"}
{"created":"2024-02-14 18:02:24","title":"Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis","abstract":"This study demonstrates the first in-hospital adaptation of a cloud-based AI, similar to ChatGPT, into a secure model for analyzing radiology reports, prioritizing patient data privacy. By employing a unique sentence-level knowledge distillation method through contrastive learning, we achieve over 95% accuracy in detecting anomalies. The model also accurately flags uncertainties in its predictions, enhancing its reliability and interpretability for physicians with certainty indicators. These advancements represent significant progress in developing secure and efficient AI tools for healthcare, suggesting a promising future for in-hospital AI applications with minimal supervision.","sentences":["This study demonstrates the first in-hospital adaptation of a cloud-based AI, similar to ChatGPT, into a secure model for analyzing radiology reports, prioritizing patient data privacy.","By employing a unique sentence-level knowledge distillation method through contrastive learning, we achieve over 95% accuracy in detecting anomalies.","The model also accurately flags uncertainties in its predictions, enhancing its reliability and interpretability for physicians with certainty indicators.","These advancements represent significant progress in developing secure and efficient AI tools for healthcare, suggesting a promising future for in-hospital AI applications with minimal supervision."],"url":"http://arxiv.org/abs/2402.09358v1","category":"cs.AI"}
{"created":"2024-02-14 18:01:28","title":"On the Impact of Spatial Covariance Matrix Ordering on Tile Low-Rank Estimation of Mat\u00e9rn Parameters","abstract":"Spatial statistical modeling and prediction involve generating and manipulating an n*n symmetric positive definite covariance matrix, where n denotes the number of spatial locations. However, when n is large, processing this covariance matrix using traditional methods becomes prohibitive. Thus, coupling parallel processing with approximation can be an elegant solution to this challenge by relying on parallel solvers that deal with the matrix as a set of small tiles instead of the full structure. Each processing unit can process a single tile, allowing better performance. The approximation can also be performed at the tile level for better compression and faster execution. The Tile Low-Rank (TLR) approximation, a tile-based approximation algorithm, has recently been used in spatial statistics applications. However, the quality of TLR algorithms mainly relies on ordering the matrix elements. This order can impact the compression quality and, therefore, the efficiency of the underlying linear solvers, which highly depends on the individual ranks of each tile. Thus, herein, we aim to investigate the accuracy and performance of some existing ordering algorithms that are used to order the geospatial locations before generating the spatial covariance matrix. Furthermore, we highlight the pros and cons of each ordering algorithm in the context of spatial statistics applications and give hints to practitioners on how to choose the ordering algorithm carefully. We assess the quality of the compression and the accuracy of the statistical parameter estimates of the Mat\\'ern covariance function using TLR approximation under various ordering algorithms and settings of correlations.","sentences":["Spatial statistical modeling and prediction involve generating and manipulating an n*n symmetric positive definite covariance matrix, where n denotes the number of spatial locations.","However, when n is large, processing this covariance matrix using traditional methods becomes prohibitive.","Thus, coupling parallel processing with approximation can be an elegant solution to this challenge by relying on parallel solvers that deal with the matrix as a set of small tiles instead of the full structure.","Each processing unit can process a single tile, allowing better performance.","The approximation can also be performed at the tile level for better compression and faster execution.","The Tile Low-Rank (TLR) approximation, a tile-based approximation algorithm, has recently been used in spatial statistics applications.","However, the quality of TLR algorithms mainly relies on ordering the matrix elements.","This order can impact the compression quality and, therefore, the efficiency of the underlying linear solvers, which highly depends on the individual ranks of each tile.","Thus, herein, we aim to investigate the accuracy and performance of some existing ordering algorithms that are used to order the geospatial locations before generating the spatial covariance matrix.","Furthermore, we highlight the pros and cons of each ordering algorithm in the context of spatial statistics applications and give hints to practitioners on how to choose the ordering algorithm carefully.","We assess the quality of the compression and the accuracy of the statistical parameter estimates of the Mat\\'ern covariance function using TLR approximation under various ordering algorithms and settings of correlations."],"url":"http://arxiv.org/abs/2402.09356v1","category":"stat.CO"}
{"created":"2024-02-14 17:59:47","title":"Single-Reset Divide & Conquer Imitation Learning","abstract":"Demonstrations are commonly used to speed up the learning process of Deep Reinforcement Learning algorithms. To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration. In particular, the Divide & Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration. The latest version, DCIL-II demonstrates remarkable sample efficiency. This novel method operates within an extended Goal-Conditioned Reinforcement Learning framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration. However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems. In response, we introduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome this constraint by relying on a single initial state reset rather than sequential resets. To address this more challenging setting, we integrate two mechanisms inspired by the Learning from Demonstrations literature, including a Demo-Buffer and Value Cloning, to guide the agent toward compatible success states. In addition, we introduce Approximate Goal Switching to facilitate training to reach goals distant from the reset state. Our paper makes several contributions, highlighting the importance of the reset assumption in DCIL-II, presenting the mechanisms of SR-DCIL variants and evaluating their performance in challenging robotic tasks compared to DCIL-II. In summary, this work offers insights into the significance of reset assumptions in the framework of DCIL and proposes SR-DCIL, a first step toward a versatile algorithm capable of learning control policies under a weaker reset assumption.","sentences":["Demonstrations are commonly used to speed up the learning process of Deep Reinforcement Learning algorithms.","To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration.","In particular, the Divide & Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration.","The latest version, DCIL-II demonstrates remarkable sample efficiency.","This novel method operates within an extended Goal-Conditioned Reinforcement Learning framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration.","However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems.","In response, we introduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome this constraint by relying on a single initial state reset rather than sequential resets.","To address this more challenging setting, we integrate two mechanisms inspired by the Learning from Demonstrations literature, including a Demo-Buffer and Value Cloning, to guide the agent toward compatible success states.","In addition, we introduce Approximate Goal Switching to facilitate training to reach goals distant from the reset state.","Our paper makes several contributions, highlighting the importance of the reset assumption in DCIL-II, presenting the mechanisms of SR-DCIL variants and evaluating their performance in challenging robotic tasks compared to DCIL-II.","In summary, this work offers insights into the significance of reset assumptions in the framework of DCIL and proposes SR-DCIL, a first step toward a versatile algorithm capable of learning control policies under a weaker reset assumption."],"url":"http://arxiv.org/abs/2402.09355v1","category":"cs.RO"}
{"created":"2024-02-14 17:55:21","title":"Extensions and paracanonical curves of genus 6","abstract":"In this note we give a computationally easy to use method to compute a maximal extension of certain varieties. As a application we prove that a general paracanonical curve C genus 6 as a codimension three subvarieties of P^4 extend to precisely 26 families of surfaces Y in P^5.","sentences":["In this note we give a computationally easy to use method to compute a maximal extension of certain varieties.","As a application we prove that a general paracanonical curve C genus 6 as a codimension three subvarieties of P^4 extend to precisely 26 families of surfaces Y in P^5."],"url":"http://arxiv.org/abs/2402.09351v1","category":"math.AG"}
{"created":"2024-02-14 17:50:30","title":"Irreducible representations of the crystallisation of the $C^{*}$-algebra $C(SU_{q}(n+1))$","abstract":"Crystallisation of the $C^*$-algebras $C(SU_{q}(n+1))$ was introduced in arXiv:2203.14665 [math.QA] as a $C^*$-algebra $A_{n}(0)$ given by a finite set of generators and relations. Here we study the irreducible representations of the $C^*$-algebra $A_{n}(0)$ and prove a factorisation theorem for its irreducible representations. This leads to a complete classification of all irreducible representations of $A_{n}(0)$. As an important consequence, we prove that all the irreducible representations arise exactly as $q\\to 0+$ limits of the irreducible representations of $C(SU_{q}(n+1))$ given by a result of Soibelman. We also prove that $A_{n}(0)$ is a type I $C^{*}$-algebra.","sentences":["Crystallisation of the $C^*$-algebras $C(SU_{q}(n+1))$ was introduced in arXiv:2203.14665 [math.","QA] as a $C^*$-algebra $A_{n}(0)$ given by a finite set of generators and relations.","Here we study the irreducible representations of the $C^*$-algebra $A_{n}(0)$ and prove a factorisation theorem for its irreducible representations.","This leads to a complete classification of all irreducible representations of $A_{n}(0)$. As an important consequence, we prove that all the irreducible representations arise exactly as $q\\to 0+$ limits of the irreducible representations of $C(SU_{q}(n+1))$ given by a result of Soibelman.","We also prove that $A_{n}(0)$ is a type I $C^{*}$-algebra."],"url":"http://arxiv.org/abs/2402.09347v1","category":"math.OA"}
{"created":"2024-02-14 17:49:31","title":"Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop","abstract":"As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is desirable, it is far from being easy or solved. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes. Experiments on a set of questions from TruthfulQA dataset show that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. The criteria for generating and applying auditing probes is generalizable to various LLMs regardless of the underlying structure or training mechanism.","sentences":["As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential.","Examples include bias, inconsistencies, and hallucination.","Although auditing the LLM for these problems is desirable, it is far from being easy or solved.","An effective method is to probe the LLM using different versions of the same question.","This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination.","However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically.","In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop.","This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability.","Specifically, we present a novel methodology with two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes.","Experiments on a set of questions from TruthfulQA dataset show that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM.","The criteria for generating and applying auditing probes is generalizable to various LLMs regardless of the underlying structure or training mechanism."],"url":"http://arxiv.org/abs/2402.09346v1","category":"cs.AI"}
{"created":"2024-02-14 17:49:07","title":"Mitigating Reward Hacking via Information-Theoretic Reward Modeling","abstract":"Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies. Extensive experiments on a wide range of settings and model scales (70M, 440M, 1.4B, and 7B) support the effectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimization detection mechanism is effective, potentially signifying a notable advancement in the field of RLHF. Code will be released upon acceptance.","sentences":["Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset.","In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation.","Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization.","Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies.","Extensive experiments on a wide range of settings and model scales (70M, 440M, 1.4B, and 7B) support the effectiveness of InfoRM.","Further analyses reveal that InfoRM's overoptimization detection mechanism is effective, potentially signifying a notable advancement in the field of RLHF.","Code will be released upon acceptance."],"url":"http://arxiv.org/abs/2402.09345v1","category":"cs.LG"}
{"created":"2024-02-14 17:46:46","title":"Generating Diverse Translation with Perturbed kNN-MT","abstract":"Generating multiple translation candidates would enable users to choose the one that satisfies their needs. Although there has been work on diversified generation, there exists room for improving the diversity mainly because the previous methods do not address the overcorrection problem -- the model underestimates a prediction that is largely different from the training data, even if that prediction is likely. This paper proposes methods that generate more diverse translations by introducing perturbed k-nearest neighbor machine translation (kNN-MT). Our methods expand the search space of kNN-MT and help incorporate diverse words into candidates by addressing the overcorrection problem. Our experiments show that the proposed methods drastically improve candidate diversity and control the degree of diversity by tuning the perturbation's magnitude.","sentences":["Generating multiple translation candidates would enable users to choose the one that satisfies their needs.","Although there has been work on diversified generation, there exists room for improving the diversity mainly because the previous methods do not address the overcorrection problem -- the model underestimates a prediction that is largely different from the training data, even if that prediction is likely.","This paper proposes methods that generate more diverse translations by introducing perturbed k-nearest neighbor machine translation (kNN-MT).","Our methods expand the search space of kNN-MT and help incorporate diverse words into candidates by addressing the overcorrection problem.","Our experiments show that the proposed methods drastically improve candidate diversity and control the degree of diversity by tuning the perturbation's magnitude."],"url":"http://arxiv.org/abs/2402.09344v1","category":"cs.CL"}
{"created":"2024-02-14 17:43:50","title":"Registration of Longitudinal Spine CTs for Monitoring Lesion Growth","abstract":"Accurate and reliable registration of longitudinal spine images is essential for assessment of disease progression and surgical outcome. Implementing a fully automatic and robust registration is crucial for clinical use, however, it is challenging due to substantial change in shape and appearance due to lesions. In this paper we present a novel method to automatically align longitudinal spine CTs and accurately assess lesion progression. Our method follows a two-step pipeline where vertebrae are first automatically localized, labeled and 3D surfaces are generated using a deep learning model, then longitudinally aligned using a Gaussian mixture model surface registration. We tested our approach on 37 vertebrae, from 5 patients, with baseline CTs and 3, 6, and 12 months follow-ups leading to 111 registrations. Our experiment showed accurate registration with an average Hausdorff distance of 0.65 mm and average Dice score of 0.92.","sentences":["Accurate and reliable registration of longitudinal spine images is essential for assessment of disease progression and surgical outcome.","Implementing a fully automatic and robust registration is crucial for clinical use, however, it is challenging due to substantial change in shape and appearance due to lesions.","In this paper we present a novel method to automatically align longitudinal spine CTs and accurately assess lesion progression.","Our method follows a two-step pipeline where vertebrae are first automatically localized, labeled and 3D surfaces are generated using a deep learning model, then longitudinally aligned using a Gaussian mixture model surface registration.","We tested our approach on 37 vertebrae, from 5 patients, with baseline CTs and 3, 6, and 12 months follow-ups leading to 111 registrations.","Our experiment showed accurate registration with an average Hausdorff distance of 0.65 mm and average Dice score of 0.92."],"url":"http://arxiv.org/abs/2402.09341v1","category":"eess.IV"}
{"created":"2024-02-14 17:42:24","title":"Neural Networks asymptotic behaviours suitable for the resolution of inverse problems","abstract":"In this paper, we perform a study on the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems. We consider NN's asymptotic limits, corresponding to Gaussian Processes (GPs), where parameter non-linearities are lost. Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice. A scenario with a known analytical solution. Our findings indicate that solving the deconvolution inverse problem with a fully connected NN yields less performing results than those obtained using the GPs derived from NN's asymptotic limits. Furthermore, we observe the trained NN's accuracy approaching that of GPs with increasing layer width. Notably, one of these GPs defies interpretation as a probabilistic model, offering a novel perspective compared to established methods in the literature. Additionally, the NNs, in their asymptotic limit, provide cost-effective analytical solutions.","sentences":["In this paper, we perform a study on the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems.","We consider NN's asymptotic limits, corresponding to Gaussian Processes (GPs), where parameter non-linearities are lost.","Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice.","A scenario with a known analytical solution.","Our findings indicate that solving the deconvolution inverse problem with a fully connected NN yields less performing results than those obtained using the GPs derived from NN's asymptotic limits.","Furthermore, we observe the trained NN's accuracy approaching that of GPs with increasing layer width.","Notably, one of these GPs defies interpretation as a probabilistic model, offering a novel perspective compared to established methods in the literature.","Additionally, the NNs, in their asymptotic limit, provide cost-effective analytical solutions."],"url":"http://arxiv.org/abs/2402.09338v1","category":"physics.comp-ph"}
{"created":"2024-02-14 17:31:04","title":"AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach","abstract":"As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. This may require probing or auditing them. Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. However, a tool for performing such audits with simple workflow and low technical threshold is lacking. In this demo, we introduce \"AuditLLM,\" a novel tool designed to evaluate the performance of various LLMs in a methodical way. AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation. A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by different people. Based on this assumption, AuditLLM produces easily interpretable results regarding the LLM's consistencies from a single question that the user enters. A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues. One could then use the output of AuditLLM to further investigate issues with the aforementioned LLM. To facilitate demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode which allows instant auditing of LLMs by analyzing responses to real-time queries; (2) Batch mode which facilitates comprehensive LLM auditing by processing multiple queries at once for in-depth analysis. This tool is beneficial for both researchers and general users, as it enhances our understanding of LLMs' capabilities in generating responses, using a standardized auditing platform.","sentences":["As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand.","This may require probing or auditing them.","Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality.","However, a tool for performing such audits with simple workflow and low technical threshold is lacking.","In this demo, we introduce \"AuditLLM,\" a novel tool designed to evaluate the performance of various LLMs in a methodical way.","AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation.","A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by different people.","Based on this assumption, AuditLLM produces easily interpretable results regarding the LLM's consistencies from a single question that the user enters.","A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues.","One could then use the output of AuditLLM to further investigate issues with the aforementioned LLM.","To facilitate demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode which allows instant auditing of LLMs by analyzing responses to real-time queries; (2) Batch mode which facilitates comprehensive LLM auditing by processing multiple queries at once for in-depth analysis.","This tool is beneficial for both researchers and general users, as it enhances our understanding of LLMs' capabilities in generating responses, using a standardized auditing platform."],"url":"http://arxiv.org/abs/2402.09334v1","category":"cs.AI"}
{"created":"2024-02-14 17:28:10","title":"Bosonic Pauli+: Efficient Simulation of Concatenated Gottesman-Kitaev-Preskill Codes","abstract":"A promising route towards fault-tolerant quantum error correction is the concatenation of a Gottesman-Kitaev-Preskill (GKP) code with a qubit code. Development of such concatenated codes requires simulation tools which realistically model noise, while being able to simulate the dynamics of many modes. However, so far, large-scale simulation tools for concatenated GKP codes have been limited to idealized noise models and GKP code implementations. Here, we introduce the Bosonic Pauli+ model (BP+), which can be simulated efficiently for a large number of modes, while capturing the rich dynamics in the bosonic multi-mode Hilbert space. We demonstrate the method by simulating a hybrid surface code, where the data qubits are finite-energy GKP qubits stabilized using the small-Big-small (sBs) protocol, and the syndrome qubits are standard two-level systems. Using BP+, we present logical error rates of such an implementation. Confidence in the accuracy of the method is gained by comparing its predictions with full time evolution simulations for several relevant quantum circuits. While developed specifically for GKP qubits stabilized using the sBs protocol, the mathematical structure of BP+ is generic and may be applicable also to the simulation of concatenations using other bosonic codes.","sentences":["A promising route towards fault-tolerant quantum error correction is the concatenation of a Gottesman-Kitaev-Preskill (GKP) code with a qubit code.","Development of such concatenated codes requires simulation tools which realistically model noise, while being able to simulate the dynamics of many modes.","However, so far, large-scale simulation tools for concatenated GKP codes have been limited to idealized noise models and GKP code implementations.","Here, we introduce the Bosonic Pauli+ model (BP+), which can be simulated efficiently for a large number of modes, while capturing the rich dynamics in the bosonic multi-mode Hilbert space.","We demonstrate the method by simulating a hybrid surface code, where the data qubits are finite-energy GKP qubits stabilized using the small-Big-small (sBs) protocol, and the syndrome qubits are standard two-level systems.","Using BP+, we present logical error rates of such an implementation.","Confidence in the accuracy of the method is gained by comparing its predictions with full time evolution simulations for several relevant quantum circuits.","While developed specifically for GKP qubits stabilized using the sBs protocol, the mathematical structure of BP+ is generic and may be applicable also to the simulation of concatenations using other bosonic codes."],"url":"http://arxiv.org/abs/2402.09333v1","category":"quant-ph"}
{"created":"2024-02-14 17:17:30","title":"Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization","abstract":"In this work, we investigate the interplay between memorization and learning in the context of \\emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\varepsilon$ has CMI bounded below by $\\Omega(1/\\varepsilon^2)$ and $\\Omega(1/\\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems.","sentences":["In this work, we investigate the interplay between memorization and learning in the context of \\emph{stochastic convex optimization} (SCO).","We define memorization via the information a learning algorithm reveals about its training data points.","We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020).","Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023).","We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\varepsilon$ has CMI bounded below by $\\Omega(1/\\varepsilon^2)$ and $\\Omega(1/\\varepsilon)$, respectively.","We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems.","Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems."],"url":"http://arxiv.org/abs/2402.09327v1","category":"cs.LG"}
{"created":"2024-02-14 17:15:59","title":"Development of a gyrokinetic-MHD energetic particle simulation code Part I: MHD version","abstract":"A new magnetohydrodynamics (MHD) code based on initial value approach, GMEC_I, has been developed for simulating various MHD physics in tokamak plasmas, as the MHD foundation of the gyrokinetic-MHD energetic particle simulation code (GMEC) family. GMEC_I solves multi-level reduced-MHD models that form a hierarchy of physics complexity, which provide conveniences for the cross-code verification and the identification of key physics effect in tokamak geometry. The field-aligned coordinates are used to represent mode structure efficiently. High-order finite difference methods are used for spatial discretization. The shifted metric methods are used for numerical stability. The discrete expansion forms of physics equations in the code are generated symbolically using the compile-time symbolic solver (CSS), which is specifically developed to reduce the complexity of the high-order finite difference form of the MHD equations. Advanced computational techniques have been implemented for optimizing memory access and code parallelization that show a good efficiency using both Thread Building Block (TBB) and Message Passing Interface (MPI). Benchmarks between GMEC_I and the eigenvalue code MAS are presented for ballooning modes without and with diamagnetic drift effects, and tearing modes, which show excellent agreements.","sentences":["A new magnetohydrodynamics (MHD) code based on initial value approach, GMEC_I, has been developed for simulating various MHD physics in tokamak plasmas, as the MHD foundation of the gyrokinetic-MHD energetic particle simulation code (GMEC) family.","GMEC_I solves multi-level reduced-MHD models that form a hierarchy of physics complexity, which provide conveniences for the cross-code verification and the identification of key physics effect in tokamak geometry.","The field-aligned coordinates are used to represent mode structure efficiently.","High-order finite difference methods are used for spatial discretization.","The shifted metric methods are used for numerical stability.","The discrete expansion forms of physics equations in the code are generated symbolically using the compile-time symbolic solver (CSS), which is specifically developed to reduce the complexity of the high-order finite difference form of the MHD equations.","Advanced computational techniques have been implemented for optimizing memory access and code parallelization that show a good efficiency using both Thread Building Block (TBB) and Message Passing Interface (MPI).","Benchmarks between GMEC_I and the eigenvalue code MAS are presented for ballooning modes without and with diamagnetic drift effects, and tearing modes, which show excellent agreements."],"url":"http://arxiv.org/abs/2402.09324v1","category":"physics.plasm-ph"}
{"created":"2024-02-14 17:14:34","title":"ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization","abstract":"Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL). Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA. We also conduct detailed analyses to offer comprehensive insights into ICDPO.","sentences":["Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content.","Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods.","However, these methods do not essentially enhance the LLM itself.","In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL).","Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO).","It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance.","ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits.","Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA.","We also conduct detailed analyses to offer comprehensive insights into ICDPO."],"url":"http://arxiv.org/abs/2402.09320v1","category":"cs.CL"}
{"created":"2024-02-14 17:14:24","title":"Eulerian Formulation of the Tensor-Based Morphology Equations for Strain-Based Blood Damage Modeling","abstract":"The development of blood-handling medical devices, such as ventricular assist devices, requires the analysis of their biocompatibility. Among other aspects, this includes hemolysis, i.e., red blood cell damage. For this purpose, computational fluid dynamics (CFD) methods are employed to predict blood flow in prototypes. The most basic hemolysis models directly estimate red blood cell damage from fluid stress in the resulting flow field. More advanced models explicitly resolve cell deformation. On the downside, these models are typically written in a Lagrangian formulation, i.e., they require pathline tracking. We present a new Eulerian description of cell deformation, enabling the evaluation of the solution across the whole domain. The resulting hemolysis model can be applied to any converged CFD simulation due to one-way coupling with the fluid velocity field. We discuss the efficient numerical treatment of the model equations in a stabilized finite element context. We validate the model by comparison to the original Lagrangian formulation in selected benchmark flows. Two more complex test cases demonstrate the method's capabilities in real-world applications. The results highlight the advantages over previous hemolysis models. In conclusion, the model holds great potential for the design process of future generations of medical devices.","sentences":["The development of blood-handling medical devices, such as ventricular assist devices, requires the analysis of their biocompatibility.","Among other aspects, this includes hemolysis, i.e., red blood cell damage.","For this purpose, computational fluid dynamics (CFD) methods are employed to predict blood flow in prototypes.","The most basic hemolysis models directly estimate red blood cell damage from fluid stress in the resulting flow field.","More advanced models explicitly resolve cell deformation.","On the downside, these models are typically written in a Lagrangian formulation, i.e., they require pathline tracking.","We present a new Eulerian description of cell deformation, enabling the evaluation of the solution across the whole domain.","The resulting hemolysis model can be applied to any converged CFD simulation due to one-way coupling with the fluid velocity field.","We discuss the efficient numerical treatment of the model equations in a stabilized finite element context.","We validate the model by comparison to the original Lagrangian formulation in selected benchmark flows.","Two more complex test cases demonstrate the method's capabilities in real-world applications.","The results highlight the advantages over previous hemolysis models.","In conclusion, the model holds great potential for the design process of future generations of medical devices."],"url":"http://arxiv.org/abs/2402.09319v1","category":"physics.flu-dyn"}
{"created":"2024-02-14 17:13:36","title":"Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio","abstract":"We present PECMAE, an interpretable model for music audio classification based on prototype learning. Our model is based on a previous method, APNet, which jointly learns an autoencoder and a prototypical network. Instead, we propose to decouple both training processes. This enables us to leverage existing self-supervised autoencoders pre-trained on much larger data (EnCodecMAE), providing representations with better generalization. APNet allows prototypes' reconstruction to waveforms for interpretability relying on the nearest training data samples. In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency. We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before. We find that the prototype-based models preserve most of the performance achieved with the autoencoder embeddings, while the sonification of prototypes benefits understanding the behavior of the classifier.","sentences":["We present PECMAE, an interpretable model for music audio classification based on prototype learning.","Our model is based on a previous method, APNet, which jointly learns an autoencoder and a prototypical network.","Instead, we propose to decouple both training processes.","This enables us to leverage existing self-supervised autoencoders pre-trained on much larger data (EnCodecMAE), providing representations with better generalization.","APNet allows prototypes' reconstruction to waveforms for interpretability relying on the nearest training data samples.","In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency.","We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before.","We find that the prototype-based models preserve most of the performance achieved with the autoencoder embeddings, while the sonification of prototypes benefits understanding the behavior of the classifier."],"url":"http://arxiv.org/abs/2402.09318v1","category":"cs.SD"}
{"created":"2024-02-14 17:13:35","title":"Extended mean-field games with multi-dimensional singular controls and non-linear jump impact","abstract":"We establish a probabilistic framework for analysing extended mean-field games with multi-dimensional singular controls and state-dependent jump dynamics and costs. Two key challenges arise when analysing such games: the state dynamics may not depend continuously on the control and the reward function may not be u.s.c.~Both problems can be overcome by restricting the set of admissible singular controls to controls that can be approximated by continuous ones. We prove that the corresponding set of admissible weak controls is given by the weak solutions to a Marcus-type SDE and provide an explicit characterisation of the reward function. The reward function will in general only be u.s.c.~To address the lack of continuity we introduce a novel class of MFGs with a broader set of admissible controls, called MFGs of parametrisations. Parametrisations are laws of state/control processes that continuously interpolate jumps. We prove that the reward functional is continuous on the set of parametrisations, establish the existence of equilibria in MFGs of parametrisations, and show that the set of Nash equilibria in MFGs of parametrisations and in the underlying MFG with singular controls coincide. This shows that MFGs of parametrisations provide a canonical framework for analysing MFGs with singular controls and non-linear jump impact.","sentences":["We establish a probabilistic framework for analysing extended mean-field games with multi-dimensional singular controls and state-dependent jump dynamics and costs.","Two key challenges arise when analysing such games: the state dynamics may not depend continuously on the control and the reward function may not be u.s.c.~Both problems can be overcome by restricting the set of admissible singular controls to controls that can be approximated by continuous ones.","We prove that the corresponding set of admissible weak controls is given by the weak solutions to a Marcus-type SDE and provide an explicit characterisation of the reward function.","The reward function will in general only be u.s.c.~To address the lack of continuity we introduce a novel class of MFGs with a broader set of admissible controls, called MFGs of parametrisations.","Parametrisations are laws of state/control processes that continuously interpolate jumps.","We prove that the reward functional is continuous on the set of parametrisations, establish the existence of equilibria in MFGs of parametrisations, and show that the set of Nash equilibria in MFGs of parametrisations and in the underlying MFG with singular controls coincide.","This shows that MFGs of parametrisations provide a canonical framework for analysing MFGs with singular controls and non-linear jump impact."],"url":"http://arxiv.org/abs/2402.09317v1","category":"math.OC"}
{"created":"2024-02-14 17:11:52","title":"Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models","abstract":"Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification. Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.","sentences":["Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical.","Image data, if not protected, can be exploited to infer personal or contextual information.","Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans.","Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation.","This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps.","The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification.","Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively."],"url":"http://arxiv.org/abs/2402.09316v1","category":"cs.CV"}
{"created":"2024-02-14 16:59:06","title":"Topologies of maximally extended non-Hausdorff Misner Space","abstract":"Misner (1967) space is a portion of 2-dimensional Minkowski spacetime, identified under a boost $\\mathcal B$. It is well known that the maximal analytic extension of Misner space that is Hausdorff consists of one half of Minkowski spacetime, identified under $\\mathcal B$; and Hawking and Ellis (1973) have shown that the maximal analytic extension that is non-Hausdorff is equal to the full Minkowski spacetime with the point $Q$ at the origin removed, identifed under $\\mathcal B$. In this paper I show that, in fact, there is an infinite set of non-Hausdorff maximal analytic extensions, each with a different causal structure. The extension constructed by Hawking and Ellis is the simplest of these. Another extension is obtained by wrapping an infinite number of copies of Minkowski spacetime around the removed $Q$ as a helicoid or Riemann surface and then identifying events under the boost $\\mathcal B$. The other extensions are obtained by wrapping some number $n$ of successive copies of Minkowski spacetime around the missing $Q$ as a helicoid, then identifying the end of the $n$'th copy with the beginning of the initial copy, and then identifying events under $\\mathcal B$. I discuss the causal structure and covering spaces of each of these extensions.","sentences":["Misner (1967) space is a portion of 2-dimensional Minkowski spacetime, identified under a boost $\\mathcal B$.","It is well known that the maximal analytic extension of Misner space that is Hausdorff consists of one half of Minkowski spacetime, identified under $\\mathcal B$; and Hawking and Ellis (1973) have shown that the maximal analytic extension that is non-Hausdorff is equal to the full Minkowski spacetime with the point $Q$ at the origin removed, identifed under $\\mathcal B$. In this paper I show that, in fact, there is an infinite set of non-Hausdorff maximal analytic extensions, each with a different causal structure.","The extension constructed by Hawking and Ellis is the simplest of these.","Another extension is obtained by wrapping an infinite number of copies of Minkowski spacetime around the removed $Q$ as a helicoid or Riemann surface and then identifying events under the boost $\\mathcal B$.","The other extensions are obtained by wrapping some number $n$ of successive copies of Minkowski spacetime around the missing $Q$ as a helicoid, then identifying the end of the $n$'th copy with the beginning of the initial copy, and then identifying events under $\\mathcal B$. I discuss the causal structure and covering spaces of each of these extensions."],"url":"http://arxiv.org/abs/2402.09312v1","category":"gr-qc"}
{"created":"2024-02-14 16:56:26","title":"Phase transition catalyzed by primordial black holes","abstract":"We investigate the first-order phase transition catalyzed by primordial black holes~(PBHs) in the early Universe. We find that super-horizon curvature perturbations generated in this scenario lead to the production of gravitational waves when the scalar modes re-enter the horizon. If PBHs with masses about $10^{-13}M_{\\odot}$ constitute all dark matter, the first-order electroweak phase transition catalyzed by PBHs can explain the gravitational wave signal observed by pulsar timing array collaborations without the overproduction of PBHs.","sentences":["We investigate the first-order phase transition catalyzed by primordial black holes~(PBHs) in the early Universe.","We find that super-horizon curvature perturbations generated in this scenario lead to the production of gravitational waves when the scalar modes re-enter the horizon.","If PBHs with masses about $10^{-13}M_{\\odot}$ constitute all dark matter, the first-order electroweak phase transition catalyzed by PBHs can explain the gravitational wave signal observed by pulsar timing array collaborations without the overproduction of PBHs."],"url":"http://arxiv.org/abs/2402.09310v1","category":"astro-ph.CO"}
{"created":"2024-02-14 16:55:06","title":"On Betti numbers for symmetric powers of modules","abstract":"Let $M$ be a finitely generated module over a local ring $(R,\\mathfrak{m})$. By $\\mathcal{S}_j(M)$, we denote the $j$th symmetric power of $M$ ($j$th graded component of the symmetric algebra $\\mathcal{S}_R(M)$). The purpose of this paper is to investigate the minimal free resolutions $\\mathcal{S}_j(M)$ as $R$-module for each $j\\geq 2$ and determine the Betti numbers of $\\mathcal{S}_j(M)$ in terms of the Betti numbers of $M$. This has some applications, for example for linear type ideals $I$, we obtain formulas of the Betti numbers $I^j$ in terms of the Betti numbers of $I$. In addition, we establish upper and lower bounds of Betti numbers of $\\mathcal{S}_j(M)$ in terms of Betti numbers of $M$. In particular, obtain some applications of the famous Buchsbaum-Eisenbud-Horrocks conjecture.","sentences":["Let $M$ be a finitely generated module over a local ring $(R,\\mathfrak{m})$. By $\\mathcal{S}_j(M)$, we denote the $j$th symmetric power of $M$ ($j$th graded component of the symmetric algebra $\\mathcal{S}_R(M)$).","The purpose of this paper is to investigate the minimal free resolutions $\\mathcal{S}_j(M)$ as $R$-module for each $j\\geq 2$ and determine the Betti numbers of $\\mathcal{S}_j(M)$ in terms of the Betti numbers of $M$. This has some applications, for example for linear type ideals $I$, we obtain formulas of the Betti numbers $I^j$ in terms of the Betti numbers of $I$. In addition, we establish upper and lower bounds of Betti numbers of $\\mathcal{S}_j(M)$ in terms of Betti numbers of $M$. In particular, obtain some applications of the famous Buchsbaum-Eisenbud-Horrocks conjecture."],"url":"http://arxiv.org/abs/2402.09309v1","category":"math.AC"}
{"created":"2024-02-14 16:51:03","title":"CMOS photonic integrated source of ultrabroadband polarization-entangled photons","abstract":"We showcase a fully on-chip CMOS-fabricated silicon photonic integrated circuit employing a bidirectionally pumped microring and polarization splitter-rotators tailored for the generation of ultrabroadband ($>$9 THz), high-fidelity (90-98%) polarization-entangled photons. Spanning the optical C+L-band and producing over 116 frequency-bin pairs on a 38.4 GHz-spaced grid, this source is ideal for flex-grid wavelength-multiplexed entanglement distribution in multiuser networks.","sentences":["We showcase a fully on-chip CMOS-fabricated silicon photonic integrated circuit employing a bidirectionally pumped microring and polarization splitter-rotators tailored for the generation of ultrabroadband ($>$9 THz), high-fidelity (90-98%) polarization-entangled photons.","Spanning the optical C+L-band and producing over 116 frequency-bin pairs on a 38.4 GHz-spaced grid, this source is ideal for flex-grid wavelength-multiplexed entanglement distribution in multiuser networks."],"url":"http://arxiv.org/abs/2402.09307v1","category":"physics.optics"}
{"created":"2024-02-14 16:49:13","title":"Embracing the black box: Heading towards foundation models for causal discovery from time series data","abstract":"Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner. Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We argue that this hints at the possibility of a foundation model for causal discovery.","sentences":["Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques.","However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning.","To address this gap, we explore what we call Causal Pretraining.","A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner.","Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics.","More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics.","Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits.","We argue that this hints at the possibility of a foundation model for causal discovery."],"url":"http://arxiv.org/abs/2402.09305v1","category":"cs.LG"}
{"created":"2024-02-14 16:47:27","title":"A pulsar-like swing in the polarisation position angle of a nearby fast radio burst","abstract":"Fast radio bursts (FRBs) last for milliseconds and arrive at Earth from cosmological distances. While their origin(s) and emission mechanism(s) are presently unknown, their signals bear similarities with the much less luminous radio emission generated by pulsars within our Galaxy and several lines of evidence point toward neutron star origins. For pulsars, the linear polarisation position angle (PA) often exhibits evolution over the pulse phase that is interpreted within a geometric framework known as the rotating vector model (RVM). Here, we report on a fast radio burst, FRB 20221022A, detected by the Canadian Hydrogen Intensity Mapping Experiment (CHIME) and localized to a nearby host galaxy ($\\sim 65\\; \\rm{Mpc}$), MCG+14-02-011. This one-off FRB displays a $\\sim 130$ degree rotation of its PA over its $\\sim 2.5\\; \\rm{ms}$ burst duration, closely resembling the \"S\"-shaped PA evolution commonly seen from pulsars and some radio magnetars. The PA evolution disfavours emission models involving shocks far from the source and instead suggests magnetospheric origins for this source which places the emission region close to the FRB central engine, echoing similar conclusions drawn from tempo-polarimetric studies of some repeating sources. This FRB's PA evolution is remarkably well-described by the RVM and, although we cannot determine the inclination and magnetic obliquity due to the unknown period/duty cycle of the source, we can dismiss extremely short-period pulsars (e.g., recycled millisecond pulsars) as potential progenitors. RVM-fitting appears to favour a source occupying a unique position in the period/duty cycle phase space that implies tight opening angles for the beamed emission, significantly reducing burst energy requirements of the source.","sentences":["Fast radio bursts (FRBs) last for milliseconds and arrive at Earth from cosmological distances.","While their origin(s) and emission mechanism(s) are presently unknown, their signals bear similarities with the much less luminous radio emission generated by pulsars within our Galaxy and several lines of evidence point toward neutron star origins.","For pulsars, the linear polarisation position angle (PA) often exhibits evolution over the pulse phase that is interpreted within a geometric framework known as the rotating vector model (RVM).","Here, we report on a fast radio burst, FRB 20221022A, detected by the Canadian Hydrogen Intensity Mapping Experiment (CHIME) and localized to a nearby host galaxy ($\\sim 65\\; \\rm{Mpc}$), MCG+14-02-011.","This one-off FRB displays a $\\sim 130$ degree rotation of its PA over its $\\sim 2.5\\; \\rm{ms}$ burst duration, closely resembling the \"S\"-shaped PA evolution commonly seen from pulsars and some radio magnetars.","The PA evolution disfavours emission models involving shocks far from the source and instead suggests magnetospheric origins for this source which places the emission region close to the FRB central engine, echoing similar conclusions drawn from tempo-polarimetric studies of some repeating sources.","This FRB's PA evolution is remarkably well-described by the RVM and, although we cannot determine the inclination and magnetic obliquity due to the unknown period/duty cycle of the source, we can dismiss extremely short-period pulsars (e.g., recycled millisecond pulsars) as potential progenitors.","RVM-fitting appears to favour a source occupying a unique position in the period/duty cycle phase space that implies tight opening angles for the beamed emission, significantly reducing burst energy requirements of the source."],"url":"http://arxiv.org/abs/2402.09304v1","category":"astro-ph.HE"}
{"created":"2024-02-14 16:47:20","title":"Immediate generalisation in humans but a generalisation lag in deep neural networks$\\unicode{x2014}$evidence for representational divergence?","abstract":"Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate and compare how well learned representations can be generalized to previously unseen test data.   Our findings indicate that in terms of absolute classification performance DNNs demonstrate a level of data efficiency comparable to$\\unicode{x2014}$and sometimes even exceeding that$\\unicode{x2014}$of human learners, challenging some prevailing assumptions in the field. However, comparisons across the entire learning process reveal significant representational differences: while DNNs' learning is characterized by a pronounced generalisation lag, humans appear to immediately acquire generalizable representations without a preliminary phase of learning training set-specific information that is only later transferred to novel data.","sentences":["Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification.","Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed.","However, the process of how these representations emerge$\\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\\unicode{x2014}$is less often directly and empirically compared.   ","Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs.","We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided.","Across the whole learning process we evaluate and compare how well learned representations can be generalized to previously unseen test data.   ","Our findings indicate that in terms of absolute classification performance DNNs demonstrate a level of data efficiency comparable to$\\unicode{x2014}$and sometimes even exceeding that$\\unicode{x2014}$of human learners, challenging some prevailing assumptions in the field.","However, comparisons across the entire learning process reveal significant representational differences: while DNNs' learning is characterized by a pronounced generalisation lag, humans appear to immediately acquire generalizable representations without a preliminary phase of learning training set-specific information that is only later transferred to novel data."],"url":"http://arxiv.org/abs/2402.09303v1","category":"cs.CV"}
{"created":"2024-02-14 16:30:30","title":"The impact of load placement on grid resonances during grid restoration","abstract":"As inverter-based generation is being massively deployed in the grid, these type of units have to take over the current roles of conventional generation, including the capability of restoring the grid. In this context, the resonances of the grid during the first steps of a black start can be concerning, given that the grid is lightly loaded. Especially relevant are the low frequency resonances, that may be excited by the harmonic components of the inverter. A typical strategy to avoid or minimize the effect of such resonances relies on connecting load banks. This was fairly feasible with conventional generation, but given the limited ratings of inverters, the amount of load that can be connected at the beginning is very limited. In this paper we consider the energization of a transmission line, and investigate the optimal location of a load along a line in order to maximize the damping in the system. By analysing the spectral properties as a function of the load location, we formally prove that placing the load in the middle of the transmission line maximizes the damping ratio of the first resonance of the system.","sentences":["As inverter-based generation is being massively deployed in the grid, these type of units have to take over the current roles of conventional generation, including the capability of restoring the grid.","In this context, the resonances of the grid during the first steps of a black start can be concerning, given that the grid is lightly loaded.","Especially relevant are the low frequency resonances, that may be excited by the harmonic components of the inverter.","A typical strategy to avoid or minimize the effect of such resonances relies on connecting load banks.","This was fairly feasible with conventional generation, but given the limited ratings of inverters, the amount of load that can be connected at the beginning is very limited.","In this paper we consider the energization of a transmission line, and investigate the optimal location of a load along a line in order to maximize the damping in the system.","By analysing the spectral properties as a function of the load location, we formally prove that placing the load in the middle of the transmission line maximizes the damping ratio of the first resonance of the system."],"url":"http://arxiv.org/abs/2402.09294v1","category":"eess.SY"}
{"created":"2024-02-14 16:30:11","title":"On the monogenity of totally complex pure octic fields","abstract":"Let $0,1\\ne m\\in Z$ and $\\alpha=\\sqrt[8]{m}$. According to the results of I. Ga\\'al and L. El Fadil, $\\alpha$ generates a power integral basis in $K=Q(\\alpha)$, if and only if $m$ is square-free and $m\\not\\equiv 1\\;(\\bmod\\; 4)$. In the present paper we consider totally complex pure octic fields, that is the case $m<0$, with $m$ satisfiying the above property. In this case $(1,\\alpha,\\alpha^2,\\ldots,\\alpha^7)$ is an integral basis. Our purpose is to investigate whether $K$ admits any other generators of power integral bases, inequivalent to $\\alpha$. We present an efficient method to calculate generators of power integral bases in this type of fields with coefficients $<10^{200}$ in the above integral basis. We report on the results of our calculation for this type of fields with $0>m>-5000$, which yields 2024 fields.","sentences":["Let $0,1\\ne m\\in Z$ and $\\alpha=\\sqrt[8]{m}$. According to the results of I. Ga\\'al and L. El Fadil, $\\alpha$ generates a power integral basis in $K=Q(\\alpha)$, if and only if $m$ is square-free and $m\\not\\equiv 1\\;(\\bmod\\; 4)$.","In the present paper we consider totally complex pure octic fields, that is the case $m<0$, with $m$ satisfiying the above property.","In this case $(1,\\alpha,\\alpha^2,\\ldots,\\alpha^7)$ is an integral basis.","Our purpose is to investigate whether $K$ admits any other generators of power integral bases, inequivalent to $\\alpha$. We present an efficient method to calculate generators of power integral bases in this type of fields with coefficients $<10^{200}$ in the above integral basis.","We report on the results of our calculation for this type of fields with $0>m>-5000$, which yields 2024 fields."],"url":"http://arxiv.org/abs/2402.09293v1","category":"math.NT"}
{"created":"2024-02-14 16:24:18","title":"Rapid spin changes around a magnetar fast radio burst","abstract":"Magnetars are neutron stars with extremely high magnetic fields that exhibit various X-ray phenomena such as sporadic sub-second bursts, long-term persistent flux enhancements, and variable rates of rotation period change. In 2020, a fast radio burst (FRB), akin to cosmological millisecond-duration radio bursts, was detected from the Galactic magnetar SGR 1935+2154, confirming the long-suspected association between some FRBs and magnetars. However, the mechanism for FRB generation in magnetars remains unclear. Here we report the X-ray discovery of an unprecedented double glitch in SGR 1935+2154 within a time interval of approximately nine hours, bracketing an FRB that occurred on October 14, 2022. Each glitch involved a significant increase in the magnetar's spin frequency, being among the largest abrupt changes in neutron star rotation ever observed. Between the glitches, the magnetar exhibited a rapid spin-down phase, accompanied by a profound increase and subsequent decline in its persistent X-ray emission and burst rate. We postulate that a strong, ephemeral, magnetospheric wind provides the torque that rapidly slows the star's rotation. The trigger for the first glitch couples the star's crust to its magnetosphere, enhances the various X-ray signals, and spawns the wind that alters magnetospheric conditions that might produce the FRB.","sentences":["Magnetars are neutron stars with extremely high magnetic fields that exhibit various X-ray phenomena such as sporadic sub-second bursts, long-term persistent flux enhancements, and variable rates of rotation period change.","In 2020, a fast radio burst (FRB), akin to cosmological millisecond-duration radio bursts, was detected from the Galactic magnetar SGR 1935+2154, confirming the long-suspected association between some FRBs and magnetars.","However, the mechanism for FRB generation in magnetars remains unclear.","Here we report the X-ray discovery of an unprecedented double glitch in SGR 1935+2154 within a time interval of approximately nine hours, bracketing an FRB that occurred on October 14, 2022.","Each glitch involved a significant increase in the magnetar's spin frequency, being among the largest abrupt changes in neutron star rotation ever observed.","Between the glitches, the magnetar exhibited a rapid spin-down phase, accompanied by a profound increase and subsequent decline in its persistent X-ray emission and burst rate.","We postulate that a strong, ephemeral, magnetospheric wind provides the torque that rapidly slows the star's rotation.","The trigger for the first glitch couples the star's crust to its magnetosphere, enhances the various X-ray signals, and spawns the wind that alters magnetospheric conditions that might produce the FRB."],"url":"http://arxiv.org/abs/2402.09291v1","category":"astro-ph.HE"}
{"created":"2024-02-14 16:23:23","title":"Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning","abstract":"Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two-the semantic state and the latent state-are then fused and utilized as inputs to a policy network. This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing supervised state information to integrating richer, latent insights. Extensive experimental results indicate that by merging these dual representations, PSRL offers a potent balance, enhancing model interpretability while preserving, and often significantly outperforming, the performance benchmarks set by traditional methods in terms of reward and convergence speed.","sentences":["Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery.","Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state.","We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework.","At the heart of PSRL is the fusion of both supervised and unsupervised learning.","The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time.","This yields more interpretable policies that compose state predictions with control.","In parallel, it captures an unsupervised latent representation.","These two-the semantic state and the latent state-are then fused and utilized as inputs to a policy network.","This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing supervised state information to integrating richer, latent insights.","Extensive experimental results indicate that by merging these dual representations, PSRL offers a potent balance, enhancing model interpretability while preserving, and often significantly outperforming, the performance benchmarks set by traditional methods in terms of reward and convergence speed."],"url":"http://arxiv.org/abs/2402.09290v1","category":"cs.LG"}
{"created":"2024-02-14 16:22:45","title":"Study of quasi-projectile properties at Fermi energies in 48Ca projectile systems","abstract":"The emission of the pre-equilibrium particles during nuclear collisions at moderate beam energies is still an open question. This influences the properties of the compound nucleus but also changes the interpretation of the quasi-fission process. A systematic analysis of the data obtained by the FAZIA collaboration during a recent experiment with a neutron rich projectile is presented. The full range of charged particles detected in the experiment is within the limit of isotopic resolution of the FAZIA detector. Quasi-projectile (QP) fragments were detected in majority thanks to the forward angular acceptance of the experimental setup which was confirmed by introducing cuts based on the HIPSE event generator calculations. The main goal was to compare the experimental results with the HIPSE simulations after introducing these cuts to investigate the influence of the n-rich entrance channel on the QP fragment properties. More specifically, the lowering of N/Z of QP fragments with beam energy was found to be present since the initial phase of the reaction. Thus, pre-equilibrium emissions might be a possible candidate to explain such an effect.","sentences":["The emission of the pre-equilibrium particles during nuclear collisions at moderate beam energies is still an open question.","This influences the properties of the compound nucleus but also changes the interpretation of the quasi-fission process.","A systematic analysis of the data obtained by the FAZIA collaboration during a recent experiment with a neutron rich projectile is presented.","The full range of charged particles detected in the experiment is within the limit of isotopic resolution of the FAZIA detector.","Quasi-projectile (QP) fragments were detected in majority thanks to the forward angular acceptance of the experimental setup which was confirmed by introducing cuts based on the HIPSE event generator calculations.","The main goal was to compare the experimental results with the HIPSE simulations after introducing these cuts to investigate the influence of the n-rich entrance channel on the QP fragment properties.","More specifically, the lowering of N/Z of QP fragments with beam energy was found to be present since the initial phase of the reaction.","Thus, pre-equilibrium emissions might be a possible candidate to explain such an effect."],"url":"http://arxiv.org/abs/2402.09289v1","category":"nucl-ex"}
{"created":"2024-02-14 16:19:09","title":"Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into Practice in Gun Violence Research","abstract":"Objective: Firearm injury research necessitates using data from often-exploited vulnerable populations of Black and Brown Americans. In order to minimize distrust, this study provides a framework for establishing AI trust and transparency with the general population. Methods: We propose a Model Facts template that is easily extendable and decomposes accuracy and demographics into standardized and minimally complex values. This framework allows general users to assess the validity and biases of a model without diving into technical model documentation. Examples: We apply the Model Facts template on two previously published models, a violence risk identification model and a suicide risk prediction model. We demonstrate the ease of accessing the appropriate information when the data is structured appropriately. Discussion: The Model Facts template is limited in its current form to human based data and biases. Like nutrition facts, it also will require some educational resources for users to grasp its full utility. Human computer interaction experiments should be conducted to ensure that the interaction between user interface and model interface is as desired. Conclusion: The Model Facts label is the first framework dedicated to establishing trust with end users and general population consumers. Implementation of Model Facts into firearm injury research will provide public health practitioners and those impacted by firearm injury greater faith in the tools the research provides.","sentences":["Objective: Firearm injury research necessitates using data from often-exploited vulnerable populations of Black and Brown Americans.","In order to minimize distrust, this study provides a framework for establishing AI trust and transparency with the general population.","Methods: We propose a Model Facts template that is easily extendable and decomposes accuracy and demographics into standardized and minimally complex values.","This framework allows general users to assess the validity and biases of a model without diving into technical model documentation.","Examples: We apply the Model Facts template on two previously published models, a violence risk identification model and a suicide risk prediction model.","We demonstrate the ease of accessing the appropriate information when the data is structured appropriately.","Discussion:","The Model Facts template is limited in its current form to human based data and biases.","Like nutrition facts, it also will require some educational resources for users to grasp its full utility.","Human computer interaction experiments should be conducted to ensure that the interaction between user interface and model interface is as desired.","Conclusion: The Model Facts label is the first framework dedicated to establishing trust with end users and general population consumers.","Implementation of Model Facts into firearm injury research will provide public health practitioners and those impacted by firearm injury greater faith in the tools the research provides."],"url":"http://arxiv.org/abs/2402.09286v1","category":"cs.AI"}
{"created":"2024-02-14 16:19:04","title":"GraphiQ: Quantum circuit design for photonic graph states","abstract":"GraphiQ is a versatile open-source framework for designing photonic graph state generation schemes, with a particular emphasis on photon-emitter hybrid circuits. Built in Python, GraphiQ consists of a suite of design tools, including multiple simulation backends and optimization methods. The library supports scheme optimization in the presence of circuit imperfections, as well as user-defined optimization goals. Our framework thus represents a valuable tool for the development of practical schemes adhering to experimentally-relevant constraints. As graph states are a key resource for measurement-based quantum computing, all-photonic quantum repeaters, and robust quantum metrology, among others, we envision GraphiQ's broad impact for advancing quantum technologies.","sentences":["GraphiQ is a versatile open-source framework for designing photonic graph state generation schemes, with a particular emphasis on photon-emitter hybrid circuits.","Built in Python, GraphiQ consists of a suite of design tools, including multiple simulation backends and optimization methods.","The library supports scheme optimization in the presence of circuit imperfections, as well as user-defined optimization goals.","Our framework thus represents a valuable tool for the development of practical schemes adhering to experimentally-relevant constraints.","As graph states are a key resource for measurement-based quantum computing, all-photonic quantum repeaters, and robust quantum metrology, among others, we envision GraphiQ's broad impact for advancing quantum technologies."],"url":"http://arxiv.org/abs/2402.09285v1","category":"quant-ph"}
{"created":"2024-02-14 16:14:03","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","abstract":"Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.","sentences":["Large Language Models (LLMs) are now commonplace in conversation applications.","However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety.","Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations.","Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject.","For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety."],"url":"http://arxiv.org/abs/2402.09283v1","category":"cs.CL"}
{"created":"2024-02-14 16:10:42","title":"Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification","abstract":"Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our method outperforms established methods. Our method stands out by addressing both LDA criteria, unlike PCA and the Hessian method, which predominantly emphasize one criterion each. This comprehensive approach captures intricate patterns and relationships, enhancing classification performance. Furthermore, through the utilization of both LDA criteria, our method outperforms LDA itself by leveraging higher-dimensional feature spaces, in accordance with Cover's theorem, which favors linear separability in higher dimensions. Our method also surpasses kernel-based methods and manifold learning techniques in performance. Additionally, our approach sheds light on complex DNN decision-making, rendering them comprehensible within a 2D space.","sentences":["Covariance and Hessian matrices have been analyzed separately in the literature for classification problems.","However, integrating these matrices has the potential to enhance their combined power in improving classification performance.","We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks.","Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances.","By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria.","Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our method outperforms established methods.","Our method stands out by addressing both LDA criteria, unlike PCA and the Hessian method, which predominantly emphasize one criterion each.","This comprehensive approach captures intricate patterns and relationships, enhancing classification performance.","Furthermore, through the utilization of both LDA criteria, our method outperforms LDA itself by leveraging higher-dimensional feature spaces, in accordance with Cover's theorem, which favors linear separability in higher dimensions.","Our method also surpasses kernel-based methods and manifold learning techniques in performance.","Additionally, our approach sheds light on complex DNN decision-making, rendering them comprehensible within a 2D space."],"url":"http://arxiv.org/abs/2402.09281v1","category":"cs.LG"}
{"created":"2024-02-14 16:05:29","title":"Persistence of steady-states for dynamical systems on large networks","abstract":"The goal of this work is to identify steady-state solutions to dynamical systems defined on large, random families of networks. We do so by passing to a continuum limit where the adjacency matrix is replaced by a non-local operator with kernel called a graphon. This graphon equation is often more amenable to analysis and provides a single equation to study instead of the infinitely many variations of networks that lead to the limit. Our work establishes a rigorous connection between steady-states of the continuum and network systems. Precisely, we show that if the graphon equation has a steady-state solution whose linearization is invertible, there exists related steady-state solutions to the finite-dimensional networked dynamical system over all sufficiently large graphs converging to the graphon. The proof involves setting up a Newton--Kantorovich type iteration scheme which is shown to be a contraction on a suitable metric space. Interestingly, we show that the first iterate of our defined operator in general fails to be a contraction mapping, but the second iterate is proven to contract on the space. We extend our results to show that linear stability properties further carry over from the graphon system to the graph dynamical system. Our results are applied to twisted states in a Kuramoto model of coupled oscillators, steady-states in a model of neuronal network activity, and a Lotka--Volterra model of ecological interaction.","sentences":["The goal of this work is to identify steady-state solutions to dynamical systems defined on large, random families of networks.","We do so by passing to a continuum limit where the adjacency matrix is replaced by a non-local operator with kernel called a graphon.","This graphon equation is often more amenable to analysis and provides a single equation to study instead of the infinitely many variations of networks that lead to the limit.","Our work establishes a rigorous connection between steady-states of the continuum and network systems.","Precisely, we show that if the graphon equation has a steady-state solution whose linearization is invertible, there exists related steady-state solutions to the finite-dimensional networked dynamical system over all sufficiently large graphs converging to the graphon.","The proof involves setting up a Newton--Kantorovich type iteration scheme which is shown to be a contraction on a suitable metric space.","Interestingly, we show that the first iterate of our defined operator in general fails to be a contraction mapping, but the second iterate is proven to contract on the space.","We extend our results to show that linear stability properties further carry over from the graphon system to the graph dynamical system.","Our results are applied to twisted states in a Kuramoto model of coupled oscillators, steady-states in a model of neuronal network activity, and a Lotka--Volterra model of ecological interaction."],"url":"http://arxiv.org/abs/2402.09276v1","category":"math.DS"}
{"created":"2024-02-14 16:00:28","title":"Neutrino flavor transformation with moments: application to fast flavor instabilities in neutron star mergers","abstract":"Neutrino evolution, of great importance in environments such as neutron star mergers (NSMs) because of their impact on explosive nucleosynthesis, is still poorly understood due to the high complexity and variety of possible flavor conversion mechanisms. In this study, we focus on so-called \"fast flavor oscillations\", which can occur on timescales of nanoseconds and are connected to the existence of a crossing between the angular distributions of electron (anti)neutrinos. Based on the neutrino radiation field drawn from a three dimensional neutron star merger simulation, we use an extension of the two-moment formalism of neutrino quantum kinetics, and perform a linear stability analysis to determine the characteristics of fast flavor instabilities across the simulation. We compare the results to local (centimeter-scale) three-dimensional two-flavor simulations using either a moment method or a particle-in-cell architecture. We get generally good agreement in the instability growth rate and typical instability lengthscale, although the imperfections of the closure used in moment methods remain to be better understood.","sentences":["Neutrino evolution, of great importance in environments such as neutron star mergers (NSMs) because of their impact on explosive nucleosynthesis, is still poorly understood due to the high complexity and variety of possible flavor conversion mechanisms.","In this study, we focus on so-called \"fast flavor oscillations\", which can occur on timescales of nanoseconds and are connected to the existence of a crossing between the angular distributions of electron (anti)neutrinos.","Based on the neutrino radiation field drawn from a three dimensional neutron star merger simulation, we use an extension of the two-moment formalism of neutrino quantum kinetics, and perform a linear stability analysis to determine the characteristics of fast flavor instabilities across the simulation.","We compare the results to local (centimeter-scale) three-dimensional two-flavor simulations using either a moment method or a particle-in-cell architecture.","We get generally good agreement in the instability growth rate and typical instability lengthscale, although the imperfections of the closure used in moment methods remain to be better understood."],"url":"http://arxiv.org/abs/2402.09274v1","category":"astro-ph.HE"}
{"created":"2024-02-14 15:59:22","title":"Hybrid Machine Learning techniques in the management of harmful algal blooms impact","abstract":"Harmful algal blooms (HABs) are episodes of high concentrations of algae that are potentially toxic for human consumption. Mollusc farming can be affected by HABs because, as filter feeders, they can accumulate high concentrations of marine biotoxins in their tissues. To avoid the risk to human consumption, harvesting is prohibited when toxicity is detected. At present, the closure of production areas is based on expert knowledge and the existence of a predictive model would help when conditions are complex and sampling is not possible. Although the concentration of toxin in meat is the method most commonly used by experts in the control of shellfish production areas, it is rarely used as a target by automatic prediction models. This is largely due to the irregularity of the data due to the established sampling programs. As an alternative, the activity status of production areas has been proposed as a target variable based on whether mollusc meat has a toxicity level below or above the legal limit. This new option is the most similar to the actual functioning of the control of shellfish production areas. For this purpose, we have made a comparison between hybrid machine learning models like Neural-Network-Adding Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN) when estimating the state of production areas. The study has been carried out in several estuaries with different levels of complexity in the episodes of algal blooms to demonstrate the generalization capacity of the models in bloom detection. As a result, we could observe that, with an average recall value of 93.41% and without dropping below 90% in any of the estuaries, BAGNET outperforms the other models both in terms of results and robustness.","sentences":["Harmful algal blooms (HABs) are episodes of high concentrations of algae that are potentially toxic for human consumption.","Mollusc farming can be affected by HABs because, as filter feeders, they can accumulate high concentrations of marine biotoxins in their tissues.","To avoid the risk to human consumption, harvesting is prohibited when toxicity is detected.","At present, the closure of production areas is based on expert knowledge and the existence of a predictive model would help when conditions are complex and sampling is not possible.","Although the concentration of toxin in meat is the method most commonly used by experts in the control of shellfish production areas, it is rarely used as a target by automatic prediction models.","This is largely due to the irregularity of the data due to the established sampling programs.","As an alternative, the activity status of production areas has been proposed as a target variable based on whether mollusc meat has a toxicity level below or above the legal limit.","This new option is the most similar to the actual functioning of the control of shellfish production areas.","For this purpose, we have made a comparison between hybrid machine learning models like Neural-Network-Adding Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN) when estimating the state of production areas.","The study has been carried out in several estuaries with different levels of complexity in the episodes of algal blooms to demonstrate the generalization capacity of the models in bloom detection.","As a result, we could observe that, with an average recall value of 93.41% and without dropping below 90% in any of the estuaries, BAGNET outperforms the other models both in terms of results and robustness."],"url":"http://arxiv.org/abs/2402.09271v1","category":"cs.LG"}
{"created":"2024-02-14 15:55:30","title":"Personalized Large Language Models","abstract":"Large language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years. However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots. This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks. Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models. Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures. These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks.","sentences":["Large language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years.","However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots.","This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks.","Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models.","Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures.","These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks."],"url":"http://arxiv.org/abs/2402.09269v1","category":"cs.CL"}
{"created":"2024-02-14 15:52:42","title":"Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation","abstract":"Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. \"hallucinations\", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.","sentences":["Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. \"hallucinations\", even when they hold relevant knowledge.","To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations.","In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality.","Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge.","Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration.","We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm.","We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN."],"url":"http://arxiv.org/abs/2402.09267v1","category":"cs.CL"}
{"created":"2024-02-14 15:51:58","title":"Machine Learning in management of precautionary closures caused by lipophilic biotoxins","abstract":"Mussel farming is one of the most important aquaculture industries. The main risk to mussel farming is harmful algal blooms (HABs), which pose a risk to human consumption. In Galicia, the Spanish main producer of cultivated mussels, the opening and closing of the production areas is controlled by a monitoring program. In addition to the closures resulting from the presence of toxicity exceeding the legal threshold, in the absence of a confirmatory sampling and the existence of risk factors, precautionary closures may be applied. These decisions are made by experts without the support or formalisation of the experience on which they are based. Therefore, this work proposes a predictive model capable of supporting the application of precautionary closures. Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and 0.75 respectively, the kNN algorithm has provided the best results. This allows the creation of a system capable of helping in complex situations where forecast errors are more common.","sentences":["Mussel farming is one of the most important aquaculture industries.","The main risk to mussel farming is harmful algal blooms (HABs), which pose a risk to human consumption.","In Galicia, the Spanish main producer of cultivated mussels, the opening and closing of the production areas is controlled by a monitoring program.","In addition to the closures resulting from the presence of toxicity exceeding the legal threshold, in the absence of a confirmatory sampling and the existence of risk factors, precautionary closures may be applied.","These decisions are made by experts without the support or formalisation of the experience on which they are based.","Therefore, this work proposes a predictive model capable of supporting the application of precautionary closures.","Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and 0.75 respectively, the kNN algorithm has provided the best results.","This allows the creation of a system capable of helping in complex situations where forecast errors are more common."],"url":"http://arxiv.org/abs/2402.09266v1","category":"cs.AI"}
{"created":"2024-02-14 15:51:55","title":"Computational Complexity of Preferred Subset Repairs on Data-Graphs","abstract":"The problem of repairing inconsistent knowledge bases has a long history within the communities of database theory and knowledge representation and reasoning, especially from the perspective of structured data. However, as the data available in real-world domains becomes more complex and interconnected, the need naturally arises for developing new types of repositories, representation languages, and semantics, to allow for more suitable ways to query and reason about it. Graph databases provide an effective way to represent relationships among semi-structured data, and allow processing and querying these connections efficiently. In this work, we focus on the problem of computing prioritized repairs over graph databases with data values, using a notion of consistency based on Reg-GXPath expressions as integrity constraints. We present several preference criteria based on the standard subset repair semantics, incorporating weights, multisets, and set-based priority levels. We study the most common repairing tasks, showing that it is possible to maintain the same computational complexity as in the case where no preference criterion is available for exploitation. To complete the picture, we explore the complexity of consistent query answering in this setting and obtain tight lower and upper bounds for all the preference criteria introduced.","sentences":["The problem of repairing inconsistent knowledge bases has a long history within the communities of database theory and knowledge representation and reasoning, especially from the perspective of structured data.","However, as the data available in real-world domains becomes more complex and interconnected, the need naturally arises for developing new types of repositories, representation languages, and semantics, to allow for more suitable ways to query and reason about it.","Graph databases provide an effective way to represent relationships among semi-structured data, and allow processing and querying these connections efficiently.","In this work, we focus on the problem of computing prioritized repairs over graph databases with data values, using a notion of consistency based on Reg-GXPath expressions as integrity constraints.","We present several preference criteria based on the standard subset repair semantics, incorporating weights, multisets, and set-based priority levels.","We study the most common repairing tasks, showing that it is possible to maintain the same computational complexity as in the case where no preference criterion is available for exploitation.","To complete the picture, we explore the complexity of consistent query answering in this setting and obtain tight lower and upper bounds for all the preference criteria introduced."],"url":"http://arxiv.org/abs/2402.09265v1","category":"cs.DB"}
{"created":"2024-02-14 15:51:28","title":"UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers","abstract":"Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection and reliable uncertainty estimation; (ii) introduce a cascade ML framework to achieve efficient model inference via early exits, by sharing shallower model layers among different event models; (iii) optimize the deployment of the model and MCU library for system efficiency. We conducted extensive experiments and compared UR2M to traditional uncertainty baselines using three wearable datasets. Our results demonstrate that UR2M achieves up to 864% faster inference speed, 857% energy-saving for uncertainty estimation, 55% memory saving on two popular MCUs, and a 22% improvement in uncertainty quantification performance.   UR2M can be deployed on a wide range of MCUs, significantly expanding real-time and reliable WED applications.","sentences":["Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases.","This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare.","Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output.","However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs).","This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   ","In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs.","Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection and reliable uncertainty estimation; (ii) introduce a cascade ML framework to achieve efficient model inference via early exits, by sharing shallower model layers among different event models; (iii) optimize the deployment of the model and MCU library for system efficiency.","We conducted extensive experiments and compared UR2M to traditional uncertainty baselines using three wearable datasets.","Our results demonstrate that UR2M achieves up to 864% faster inference speed, 857% energy-saving for uncertainty estimation, 55% memory saving on two popular MCUs, and a 22% improvement in uncertainty quantification performance.   ","UR2M can be deployed on a wide range of MCUs, significantly expanding real-time and reliable WED applications."],"url":"http://arxiv.org/abs/2402.09264v1","category":"cs.LG"}
{"created":"2024-02-14 15:51:01","title":"Uncertainty-Aware Transient Stability-Constrained Preventive Redispatch: A Distributional Reinforcement Learning Approach","abstract":"Transient stability-constrained preventive redispatch plays a crucial role in ensuring power system security and stability. Since redispatch strategies need to simultaneously satisfy complex transient constraints and the economic need, model-based formulation and optimization become extremely challenging. In addition, the increasing uncertainty and variability introduced by renewable sources start to drive the system stability consideration from deterministic to probabilistic, which further exaggerates the complexity. In this paper, a Graph neural network guided Distributional Deep Reinforcement Learning (GD2RL) method is proposed, for the first time, to solve the uncertainty-aware transient stability-constrained preventive redispatch problem. First, a graph neural network-based transient simulator is trained by supervised learning to efficiently generate post-contingency rotor angle curves with the steady-state and contingency as inputs, which serves as a feature extractor for operating states and a surrogate time-domain simulator during the environment interaction for reinforcement learning. Distributional deep reinforcement learning with explicit uncertainty distribution of system operational conditions is then applied to generate the redispatch strategy to balance the user-specified probabilistic stability performance and economy preferences. The full distribution of the post-control transient stability index is directly provided as the output. Case studies on the modified New England 39-bus system validate the proposed method.","sentences":["Transient stability-constrained preventive redispatch plays a crucial role in ensuring power system security and stability.","Since redispatch strategies need to simultaneously satisfy complex transient constraints and the economic need, model-based formulation and optimization become extremely challenging.","In addition, the increasing uncertainty and variability introduced by renewable sources start to drive the system stability consideration from deterministic to probabilistic, which further exaggerates the complexity.","In this paper, a Graph neural network guided Distributional Deep Reinforcement Learning (GD2RL) method is proposed, for the first time, to solve the uncertainty-aware transient stability-constrained preventive redispatch problem.","First, a graph neural network-based transient simulator is trained by supervised learning to efficiently generate post-contingency rotor angle curves with the steady-state and contingency as inputs, which serves as a feature extractor for operating states and a surrogate time-domain simulator during the environment interaction for reinforcement learning.","Distributional deep reinforcement learning with explicit uncertainty distribution of system operational conditions is then applied to generate the redispatch strategy to balance the user-specified probabilistic stability performance and economy preferences.","The full distribution of the post-control transient stability index is directly provided as the output.","Case studies on the modified New England 39-bus system validate the proposed method."],"url":"http://arxiv.org/abs/2402.09263v1","category":"eess.SY"}
{"created":"2024-02-14 15:45:56","title":"SyntaxShap: Syntax-aware Explainability Method for Text Generation","abstract":"To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the model. We show that our syntax-aware method produces explanations that help build more faithful, coherent, and interpretable explanations for predictions by autoregressive models.","sentences":["To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions.","However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data.","This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data.","The presented work extends Shapley values to account for parsing-based syntactic dependencies.","Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree.","We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the model.","We show that our syntax-aware method produces explanations that help build more faithful, coherent, and interpretable explanations for predictions by autoregressive models."],"url":"http://arxiv.org/abs/2402.09259v1","category":"cs.CL"}
{"created":"2024-02-14 15:41:01","title":"Celestial Conformal Primaries in Effective Field Theories","abstract":"Scattering amplitudes in $d+2$ dimensions can be recast as correlators of conformal primary operators in a putative holographic CFT$_d$ by working in a basis of boost eigenstates instead of momentum eigenstates. It has been shown previously that conformal primary operators with $\\Delta \\in \\frac{d}{2} + i {\\mathbb R}$ form a basis for massless one-particle representations. In this paper, we consider more general conformal primary operators with $\\Delta \\in {\\mathbb C}$ and show that completeness, normalizability, and consistency with CPT implies that we must restrict the scaling dimensions to either $\\Delta \\in \\frac{d}{2} + i {\\mathbb R}$ or $\\Delta \\in {\\mathbb R}$. Unlike those with $\\Delta \\in \\frac{d}{2} + i {\\mathbb R}$, the conformal primaries with $\\Delta \\in {\\mathbb R}$ can be constructed without knowledge of the UV and can therefore be defined in effective field theories. With additional analyticity assumptions, we can restrict $\\Delta \\in 2 - {\\mathbb Z}_{\\geq0}$ or $\\Delta \\in \\frac{1}{2}-{\\mathbb Z}_{\\geq0}$ for bosonic or fermionic operators, respectively.","sentences":["Scattering amplitudes in $d+2$ dimensions can be recast as correlators of conformal primary operators in a putative holographic CFT$_d$ by working in a basis of boost eigenstates instead of momentum eigenstates.","It has been shown previously that conformal primary operators with $\\Delta \\in \\frac{d}{2} + i {\\mathbb R}$ form a basis for massless one-particle representations.","In this paper, we consider more general conformal primary operators with $\\Delta \\in {\\mathbb C}$ and show that completeness, normalizability, and consistency with CPT implies that we must restrict the scaling dimensions to either $\\Delta \\in \\frac{d}{2} + i {\\mathbb R}$ or $\\Delta \\in {\\mathbb R}$. Unlike those with $\\Delta \\in \\frac{d}{2} + i {\\mathbb R}$, the conformal primaries with $\\Delta \\in {\\mathbb R}$ can be constructed without knowledge of the UV and can therefore be defined in effective field theories.","With additional analyticity assumptions, we can restrict $\\Delta \\in 2 - {\\mathbb Z}_{\\geq0}$ or $\\Delta \\in \\frac{1}{2}-{\\mathbb Z}_{\\geq0}$ for bosonic or fermionic operators, respectively."],"url":"http://arxiv.org/abs/2402.09256v1","category":"hep-th"}
{"created":"2024-02-14 15:40:26","title":"Monochromatic $k$-connection of graphs","abstract":"An edge-coloured path is monochromatic if all of its edges have the same colour. For a $k$-connected graph $G$, the monochromatic $k$-connection number of $G$, denoted by $mc_k(G)$, is the maximum number of colours in an edge-colouring of $G$ such that, any two vertices are connected by $k$ internally vertex-disjoint monochromatic paths. In this paper, we shall study the parameter $mc_k(G)$. We obtain bounds for $mc_k(G)$, for general graphs $G$. We also compute $mc_k(G)$ exactly when $k$ is small, and $G$ is a graph on $n$ vertices, with a spanning $k$-connected subgraph having the minimum possible number of edges, namely $\\lceil\\frac{kn}{2}\\rceil$. We prove a similar result when $G$ is a bipartite graph.","sentences":["An edge-coloured path is monochromatic if all of its edges have the same colour.","For a $k$-connected graph $G$, the monochromatic $k$-connection number of $G$, denoted by $mc_k(G)$, is the maximum number of colours in an edge-colouring of $G$ such that, any two vertices are connected by $k$ internally vertex-disjoint monochromatic paths.","In this paper, we shall study the parameter $mc_k(G)$. We obtain bounds for $mc_k(G)$, for general graphs $G$.","We also compute $mc_k(G)$ exactly when $k$ is small, and $G$ is a graph on $n$ vertices, with a spanning $k$-connected subgraph having the minimum possible number of edges, namely $\\lceil\\frac{kn}{2}\\rceil$. We prove a similar result when $G$ is a bipartite graph."],"url":"http://arxiv.org/abs/2402.09254v1","category":"math.CO"}
{"created":"2024-02-14 15:39:25","title":"An asymptotic-preserving scheme for Euler equations I: non-ideal gases","abstract":"{We analyze a general Implicit-Explicit (IMEX) time discretization for the compressible Euler equations of gas dynamics, showing that they are asymptotic-preserving (AP) in the low Mach number limit. The analysis is carried out for a general equation of state (EOS). We consider both a single asymptotic length scale and two length scales. We then show that, when coupling these time discretizations with a Discontinuous Galerkin (DG) space discretization with appropriate fluxes, an all Mach number numerical method is obtained. A number of relevant benchmarks for ideal gases and their non-trivial extension to non-ideal EOS validate the performed analysis.","sentences":["{We analyze a general Implicit-Explicit (IMEX) time discretization for the compressible Euler equations of gas dynamics, showing that they are asymptotic-preserving (AP) in the low Mach number limit.","The analysis is carried out for a general equation of state (EOS).","We consider both a single asymptotic length scale and two length scales.","We then show that, when coupling these time discretizations with a Discontinuous Galerkin (DG) space discretization with appropriate fluxes, an all Mach number numerical method is obtained.","A number of relevant benchmarks for ideal gases and their non-trivial extension to non-ideal EOS validate the performed analysis."],"url":"http://arxiv.org/abs/2402.09252v1","category":"math.NA"}
{"created":"2024-02-14 15:38:56","title":"Universal Machine Learning Kohn-Sham Hamiltonian for Materials","abstract":"While density functional theory (DFT) serves as a prevalent computational approach in electronic structure calculations, its computational demands and scalability limitations persist. Recently, leveraging neural networks to parameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue for accelerating electronic structure computations. Despite advancements, challenges such as the necessity for computing extensive DFT training data to explore new systems and the complexity of establishing accurate ML models for multi-elemental materials still exist. Addressing these hurdles, this study introduces a universal electronic Hamiltonian model trained on Hamiltonian matrices obtained from first-principles DFT calculations of nearly all crystal structures on the Materials Project. We demonstrate its generality in predicting electronic structures across the whole periodic table, including complex multi-elemental systems. By offering a reliable efficient framework for computing electronic properties, this universal Hamiltonian model lays the groundwork for advancements in diverse fields related to electronic structures.","sentences":["While density functional theory (DFT) serves as a prevalent computational approach in electronic structure calculations, its computational demands and scalability limitations persist.","Recently, leveraging neural networks to parameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue for accelerating electronic structure computations.","Despite advancements, challenges such as the necessity for computing extensive DFT training data to explore new systems and the complexity of establishing accurate ML models for multi-elemental materials still exist.","Addressing these hurdles, this study introduces a universal electronic Hamiltonian model trained on Hamiltonian matrices obtained from first-principles DFT calculations of nearly all crystal structures on the Materials Project.","We demonstrate its generality in predicting electronic structures across the whole periodic table, including complex multi-elemental systems.","By offering a reliable efficient framework for computing electronic properties, this universal Hamiltonian model lays the groundwork for advancements in diverse fields related to electronic structures."],"url":"http://arxiv.org/abs/2402.09251v1","category":"physics.comp-ph"}
{"created":"2024-02-14 15:38:05","title":"Validity of using Els\u00e4sser variables to study the interaction of compressible solar wind fluctuations with a coronal mass ejection","abstract":"Alfv\\'enic fluctuations, as modelled by the non-linear interactions of Alfv\\'en waves of various scales, are seen to dominate solar wind turbulence. However, there is also a non-negligible component of non-Alfv\\'enic fluctuations. The Els\\\"asser formalism, which is central to the study of Alfv\\'enic turbulence due to its ability to differentiate between parallel and anti-parallel Alfv\\'en waves, cannot strictly separate wavemodes in the presence of compressive magnetoacoustic waves. In this study, we analyse the deviations generated in the Els\\\"asser formalism as density fluctuations are naturally generated through the propagation of a linearly polarised Alfv\\'en wave. The study was performed in the context of a coronal mass ejection (CME) propagating through the solar wind, which enables the creation of two solar wind regimes, pristine wind and a shocked CME sheath, where the Els\\\"asser formalism can be evaluated. In these two regimes we studied the deviations of the Els\\\"asser formalism in separating parallel and anti-parallel components of Alfv\\'enic solar wind perturbations generated by small-amplitude density fluctuations. We used an ideal 2.5D magnetohydrodynamic (MHD) model with an adiabatic equation of state. An Alfv\\'en pump wave was injected into the quiet solar wind by perturbing the transverse magnetic field and velocity components. This wave subsequently generates density fluctuations through the ponderomotive force. A CME was injected by inserting a flux-rope modelled as a magnetic island into the quasi-steady solar wind. The presence of density perturbations creates an approximately 10% deviation in the Els\\\"asser variables and reflection coefficient for the Alfv\\'en waves as well as a deviation of approximately 0.1 in the cross helicity in regions containing both parallel and anti-parallel fluctuations.","sentences":["Alfv\\'enic fluctuations, as modelled by the non-linear interactions of Alfv\\'en waves of various scales, are seen to dominate solar wind turbulence.","However, there is also a non-negligible component of non-Alfv\\'enic fluctuations.","The Els\\\"asser formalism, which is central to the study of Alfv\\'enic turbulence due to its ability to differentiate between parallel and anti-parallel Alfv\\'en waves, cannot strictly separate wavemodes in the presence of compressive magnetoacoustic waves.","In this study, we analyse the deviations generated in the Els\\\"asser formalism as density fluctuations are naturally generated through the propagation of a linearly polarised Alfv\\'en wave.","The study was performed in the context of a coronal mass ejection (CME) propagating through the solar wind, which enables the creation of two solar wind regimes, pristine wind and a shocked CME sheath, where the Els\\\"asser formalism can be evaluated.","In these two regimes we studied the deviations of the Els\\\"asser formalism in separating parallel and anti-parallel components of Alfv\\'enic solar wind perturbations generated by small-amplitude density fluctuations.","We used an ideal 2.5D magnetohydrodynamic (MHD) model with an adiabatic equation of state.","An Alfv\\'en pump wave was injected into the quiet solar wind by perturbing the transverse magnetic field and velocity components.","This wave subsequently generates density fluctuations through the ponderomotive force.","A CME was injected by inserting a flux-rope modelled as a magnetic island into the quasi-steady solar wind.","The presence of density perturbations creates an approximately 10% deviation in the Els\\\"asser variables and reflection coefficient for the Alfv\\'en waves as well as a deviation of approximately 0.1 in the cross helicity in regions containing both parallel and anti-parallel fluctuations."],"url":"http://arxiv.org/abs/2402.09250v1","category":"astro-ph.SR"}
{"created":"2024-02-14 15:37:58","title":"Exploring the Relationship: Transformative Adaptive Activation Functions in Comparison to Other Activation Functions","abstract":"Neural networks are the state-of-the-art approach for many tasks and the activation function is one of the main building blocks that allow such performance. Recently, a novel transformative adaptive activation function (TAAF) allowing for any vertical and horizontal translation and scaling was proposed. This work sets the TAAF into the context of other activation functions. It shows that the TAAFs generalize over 50 existing activation functions and utilize similar concepts as over 70 other activation functions, underscoring the versatility of TAAFs. This comprehensive exploration positions TAAFs as a promising and adaptable addition to neural networks.","sentences":["Neural networks are the state-of-the-art approach for many tasks and the activation function is one of the main building blocks that allow such performance.","Recently, a novel transformative adaptive activation function (TAAF) allowing for any vertical and horizontal translation and scaling was proposed.","This work sets the TAAF into the context of other activation functions.","It shows that the TAAFs generalize over 50 existing activation functions and utilize similar concepts as over 70 other activation functions, underscoring the versatility of TAAFs.","This comprehensive exploration positions TAAFs as a promising and adaptable addition to neural networks."],"url":"http://arxiv.org/abs/2402.09249v1","category":"cs.LG"}
{"created":"2024-02-14 15:36:29","title":"Constants of motion characterizing continuous symmetry-broken phases","abstract":"We present a theory characterizing the phases emerging as a consequence of continuous symmetry-breaking in quantum and classical systems. In symmetry-breaking phases, dynamics is restricted due to the existence of a set of conserved charges derived from the order parameter of the phase transition. Their expectation values are determined by the privileged direction appearing in the ordered phase as a consequence of symmetry breaking, and thus they can be used to determine whether this direction is well defined or it has quantum fluctuations. Our theory is numerically exemplified via the two-dimensional limit of the vibron model, a fully connected system invariant under a rotation operator which generates the continuous symmetry-breaking.","sentences":["We present a theory characterizing the phases emerging as a consequence of continuous symmetry-breaking in quantum and classical systems.","In symmetry-breaking phases, dynamics is restricted due to the existence of a set of conserved charges derived from the order parameter of the phase transition.","Their expectation values are determined by the privileged direction appearing in the ordered phase as a consequence of symmetry breaking, and thus they can be used to determine whether this direction is well defined or it has quantum fluctuations.","Our theory is numerically exemplified via the two-dimensional limit of the vibron model, a fully connected system invariant under a rotation operator which generates the continuous symmetry-breaking."],"url":"http://arxiv.org/abs/2402.09248v1","category":"quant-ph"}
{"created":"2024-02-14 15:34:38","title":"Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many Robots","abstract":"We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play's permutations. To solve the problem, we introduce Branch and Play (B&P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&P consistently outperforms various baselines, and computes the socially optimal equilibrium.","sentences":["We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game.","We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play's permutations.","To solve the problem, we introduce Branch and Play (B&P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium.","As a subroutine for B&P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play.","We demonstrate the practical utility of B&P to coordinate air traffic control, swarm formation, and delivery vehicle fleets.","We find that B&P consistently outperforms various baselines, and computes the socially optimal equilibrium."],"url":"http://arxiv.org/abs/2402.09246v1","category":"cs.RO"}
{"created":"2024-02-14 15:34:28","title":"Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality","abstract":"The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP 2023 is to promote and support collaborative research on machine learning for 3D audio signal processing, with a specific emphasis on 3D speech enhancement and 3D Sound Event Localization and Detection in Extended Reality applications. As part of our latest competition, we provide a brand-new dataset, which maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets, but with first-order Ambisonics recordings from multiple reverberant simulated environments. Moreover, we start exploring an audio-visual scenario by providing images of these environments, as perceived by the different microphone positions and orientations. We also propose updated baseline models for both tasks that can now support audio-image couples as input and a supporting API to replicate our results. Finally, we present the results of the participants. Further details about the challenge are available at https://www.l3das.com/icassp2023.","sentences":["The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP 2023 is to promote and support collaborative research on machine learning for 3D audio signal processing, with a specific emphasis on 3D speech enhancement and 3D Sound Event Localization and Detection in Extended Reality applications.","As part of our latest competition, we provide a brand-new dataset, which maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets, but with first-order Ambisonics recordings from multiple reverberant simulated environments.","Moreover, we start exploring an audio-visual scenario by providing images of these environments, as perceived by the different microphone positions and orientations.","We also propose updated baseline models for both tasks that can now support audio-image couples as input and a supporting API to replicate our results.","Finally, we present the results of the participants.","Further details about the challenge are available at https://www.l3das.com/icassp2023."],"url":"http://arxiv.org/abs/2402.09245v1","category":"eess.AS"}
{"created":"2024-02-14 15:33:28","title":"Zero-energy Devices for 6G: Technical Enablers at a Glance","abstract":"Low-cost, resource-constrained, maintenance-free, and energy-harvesting (EH) Internet of Things (IoT) devices, referred to as zero-energy devices (ZEDs), are rapidly attracting attention from industry and academia due to their myriad of applications. To date, such devices remain primarily unsupported by modern IoT connectivity solutions due to their intrinsic fabrication, hardware, deployment, and operation limitations, while lacking clarity on their key technical enablers and prospects. Herein, we address this by discussing the main characteristics and enabling technologies of ZEDs within the next generation of mobile networks, specifically focusing on unconventional EH sources, multi-source EH, power management, energy storage solutions, manufacturing material and practices, backscattering, and low-complexity receivers. Moreover, we highlight the need for lightweight and energy-aware computing, communication, and scheduling protocols, while discussing potential approaches related to TinyML, duty cycling, and infrastructure enablers like radio frequency wireless power transfer and wake-up protocols. Challenging aspects and open research directions are identified and discussed in all the cases. Finally, we showcase an experimental ZED proof-of-concept related to ambient cellular backscattering.","sentences":["Low-cost, resource-constrained, maintenance-free, and energy-harvesting (EH) Internet of Things (IoT) devices, referred to as zero-energy devices (ZEDs), are rapidly attracting attention from industry and academia due to their myriad of applications.","To date, such devices remain primarily unsupported by modern IoT connectivity solutions due to their intrinsic fabrication, hardware, deployment, and operation limitations, while lacking clarity on their key technical enablers and prospects.","Herein, we address this by discussing the main characteristics and enabling technologies of ZEDs within the next generation of mobile networks, specifically focusing on unconventional EH sources, multi-source EH, power management, energy storage solutions, manufacturing material and practices, backscattering, and low-complexity receivers.","Moreover, we highlight the need for lightweight and energy-aware computing, communication, and scheduling protocols, while discussing potential approaches related to TinyML, duty cycling, and infrastructure enablers like radio frequency wireless power transfer and wake-up protocols.","Challenging aspects and open research directions are identified and discussed in all the cases.","Finally, we showcase an experimental ZED proof-of-concept related to ambient cellular backscattering."],"url":"http://arxiv.org/abs/2402.09244v1","category":"eess.SP"}
{"created":"2024-02-14 15:32:35","title":"Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection","abstract":"Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health. As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants. Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations. Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable. The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories. To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes. Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features. Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion. Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector. Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS. Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD. Code and dataset are available at https://github.com/LanceZPF/KEFS.","sentences":["Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health.","As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants.","Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations.","Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable.","The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories.","To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes.","Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features.","Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion.","Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector.","Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS.","Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD.","Code and dataset are available at https://github.com/LanceZPF/KEFS."],"url":"http://arxiv.org/abs/2402.09242v1","category":"cs.CV"}
{"created":"2024-02-14 15:28:42","title":"Switch EMA: A Free Lunch for Better Flatness and Sharpness","abstract":"Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.","sentences":["Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization.","Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations.","This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA).","From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness.","To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling.","Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds."],"url":"http://arxiv.org/abs/2402.09240v1","category":"cs.LG"}
{"created":"2024-02-14 15:27:30","title":"Mean ergodic and related properties of generalized Ces\u00e0ro operators in BK-sequence spaces","abstract":"Recent results concerning the linear dynamics and mean ergodicity of compact operators in Banach spaces, together with additional new results, are employed to investigate various spectral properties of generalized Ces\\`aro operators acting in large classes of classical BK-sequence spaces. Of particular interest is to determine the eigenvalues and the corresponding eigenvectors of such operators and to decide whether (or not) the operators are power bounded, mean ergodic and supercyclic.","sentences":["Recent results concerning the linear dynamics and mean ergodicity of compact operators in Banach spaces, together with additional new results, are employed to investigate various spectral properties of generalized Ces\\`aro operators acting in large classes of classical BK-sequence spaces.","Of particular interest is to determine the eigenvalues and the corresponding eigenvectors of such operators and to decide whether (or not)","the operators are power bounded, mean ergodic and supercyclic."],"url":"http://arxiv.org/abs/2402.09238v1","category":"math.FA"}
{"created":"2024-02-14 15:24:20","title":"Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency","abstract":"State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we improve this retrieval step and tailor it to the final localization task. Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from generative text-to-image models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization. After expanding the training set, we propose a training approach that leverages the specificities and the underlying geometry of this mix of real and synthetic images. We experimentally show that those changes translate into large improvements for the most challenging visual localization datasets. Project page: https://europe.naverlabs.com/ret4loc","sentences":["State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial.","Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy.","In this paper, we improve this retrieval step and tailor it to the final localization task.","Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from generative text-to-image models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization.","After expanding the training set, we propose a training approach that leverages the specificities and the underlying geometry of this mix of real and synthetic images.","We experimentally show that those changes translate into large improvements for the most challenging visual localization datasets.","Project page: https://europe.naverlabs.com/ret4loc"],"url":"http://arxiv.org/abs/2402.09237v1","category":"cs.CV"}
{"created":"2024-02-14 15:23:59","title":"Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models","abstract":"To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.","sentences":["To build intelligent machine learning systems, there are two broad approaches.","One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning.","The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work.","In this work, we relate these two approaches and study how to learn human-interpretable concepts from data.","Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data.","Experiments on synthetic data and large language models show the utility of our unified approach."],"url":"http://arxiv.org/abs/2402.09236v1","category":"cs.LG"}
{"created":"2024-02-14 15:22:24","title":"Design and Realization of a Benchmarking Testbed for Evaluating Autonomous Platooning Algorithms","abstract":"Autonomous vehicle platoons present near- and long-term opportunities to enhance operational efficiencies and save lives. The past 30 years have seen rapid development in the autonomous driving space, enabling new technologies that will alleviate the strain placed on human drivers and reduce vehicle emissions. This paper introduces a testbed for evaluating and benchmarking platooning algorithms on 1/10th scale vehicles with onboard sensors. To demonstrate the testbed's utility, we evaluate three algorithms, linear feedback and two variations of distributed model predictive control, and compare their results on a typical platooning scenario where the lead vehicle tracks a reference trajectory that changes speed multiple times. We validate our algorithms in simulation to analyze the performance as the platoon size increases, and find that the distributed model predictive control algorithms outperform linear feedback on hardware and in simulation.","sentences":["Autonomous vehicle platoons present near- and long-term opportunities to enhance operational efficiencies and save lives.","The past 30 years have seen rapid development in the autonomous driving space, enabling new technologies that will alleviate the strain placed on human drivers and reduce vehicle emissions.","This paper introduces a testbed for evaluating and benchmarking platooning algorithms on 1/10th scale vehicles with onboard sensors.","To demonstrate the testbed's utility, we evaluate three algorithms, linear feedback and two variations of distributed model predictive control, and compare their results on a typical platooning scenario where the lead vehicle tracks a reference trajectory that changes speed multiple times.","We validate our algorithms in simulation to analyze the performance as the platoon size increases, and find that the distributed model predictive control algorithms outperform linear feedback on hardware and in simulation."],"url":"http://arxiv.org/abs/2402.09233v1","category":"cs.RO"}
{"created":"2024-02-14 15:21:37","title":"Iterated Straight-Line Programs","abstract":"We explore an extension to straight-line programs (SLPs) that outperforms, for some text families, the measure $\\delta$ based on substring complexity, a lower bound for most measures and compressors exploiting repetitiveness (which are crucial in areas like Bioinformatics). The extension, called iterated SLPs (ISLPs), allows rules of the form $A \\rightarrow \\Pi_{i=k_1}^{k_2} B_1^{i^{c_1}}\\cdots B_t^{i^{c_t}}$, for which we show how to extract any substring of length $\\lambda$, from the represented text $T[1.. n]$, in time $O(\\lambda + \\log^2 n\\log\\log n)$. This is the first compressed representation for repetitive texts breaking $\\delta$ while, at the same time, supporting direct access to arbitrary text symbols in polylogarithmic time. As a byproduct, we extend Ganardi et al.'s technique to balance any SLP (so it has a derivation tree of logarithmic height) to a wide generalization of SLPs, including ISLPs.","sentences":["We explore an extension to straight-line programs (SLPs) that outperforms, for some text families, the measure $\\delta$ based on substring complexity, a lower bound for most measures and compressors exploiting repetitiveness (which are crucial in areas like Bioinformatics).","The extension, called iterated SLPs (ISLPs), allows rules of the form $A \\rightarrow \\Pi_{i=k_1}^{k_2} B_1^{i^{c_1}}\\cdots B_t^{i^{c_t}}$, for which we show how to extract any substring of length $\\lambda$, from the represented text $T[1.. n]$, in time $O(\\lambda + \\log^2 n\\log\\log","n)$. This is the first compressed representation for repetitive texts breaking $\\delta$ while, at the same time, supporting direct access to arbitrary text symbols in polylogarithmic time.","As a byproduct, we extend Ganardi et al.'s technique to balance any SLP (so it has a derivation tree of logarithmic height) to a wide generalization of SLPs, including ISLPs."],"url":"http://arxiv.org/abs/2402.09232v1","category":"cs.DS"}
{"created":"2024-02-14 15:17:37","title":"Context Composing for Full Line Code Completion","abstract":"Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer. Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users. The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation. In addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area.","sentences":["Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer.","Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks.","This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself.","At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer.","We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users.","The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation.","In addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area."],"url":"http://arxiv.org/abs/2402.09230v1","category":"cs.SE"}
{"created":"2024-02-14 15:13:09","title":"Efficient Terahertz Generation from CoPt-based Terahertz Emitters via Orbital-to-Charge Conversion","abstract":"Orbitronics devices operate by manipulating orbitally-polarized currents. Recent studies have shown that these orbital currents can be excited by femtosecond laser pulses in ferromagnet as Ni and converted into ultrafast charge current via orbital-to-charge conversion. However, the terahertz emission from orbitronic terahertz emitter based on Ni is still much weaker than the typical spintronic terahertz emitter. Here, we report more efficient light-induced generation of orbital current from CoPt alloy and the orbitronic terahertz emission by CoPt/Cu/MgO shows terahertz radiation comparable to that of efficient spintronic terahertz emitters. By varying the concentration of CoPt alloy, the thickness of Cu, and the capping layer, we confirm that THz emission primarily originates from the orbital accumulation generated within CoPt, propagating through Cu and followed by the subsequent orbital-to-charge conversion from the inverse orbital Rashba-Edelstein effect at the Cu/MgO interface. This study provides strong evidence for the very efficient orbital current generation in CoPt alloy, paving the way to efficient orbital terahertz emitters.","sentences":["Orbitronics devices operate by manipulating orbitally-polarized currents.","Recent studies have shown that these orbital currents can be excited by femtosecond laser pulses in ferromagnet as Ni and converted into ultrafast charge current via orbital-to-charge conversion.","However, the terahertz emission from orbitronic terahertz emitter based on Ni is still much weaker than the typical spintronic terahertz emitter.","Here, we report more efficient light-induced generation of orbital current from CoPt alloy and the orbitronic terahertz emission by CoPt/Cu/MgO shows terahertz radiation comparable to that of efficient spintronic terahertz emitters.","By varying the concentration of CoPt alloy, the thickness of Cu, and the capping layer, we confirm that THz emission primarily originates from the orbital accumulation generated within CoPt, propagating through Cu and followed by the subsequent orbital-to-charge conversion from the inverse orbital Rashba-Edelstein effect at the Cu/MgO interface.","This study provides strong evidence for the very efficient orbital current generation in CoPt alloy, paving the way to efficient orbital terahertz emitters."],"url":"http://arxiv.org/abs/2402.09228v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 15:09:01","title":"Is my Data in your AI Model? Membership Inference Test with Application to Face Images","abstract":"This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data.","sentences":["This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models.","Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process.","The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs).","The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models.","Experiments are carried out using six publicly available databases, comprising over 22 million face images in total.","Also, different experimental scenarios are considered depending on the context available of the AI model to test.","Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data."],"url":"http://arxiv.org/abs/2402.09225v1","category":"cs.CV"}
{"created":"2024-02-14 15:01:07","title":"Spectral Filters, Dark Signals, and Attention Sinks","abstract":"Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation. We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum.","sentences":["Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens.","We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands.","We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation.","We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved.","Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum."],"url":"http://arxiv.org/abs/2402.09221v1","category":"cs.AI"}
{"created":"2024-02-14 14:56:05","title":"The Mumford Dynamical System and Hyperelliptic Kleinian Functions","abstract":"We establish differential-algebraic theory of the Mumford dynamical system. In the framework of this theory, we introduce the $(P,Q)$-recursion, which defines a sequence of functions $P_1,P_2,\\ldots$ given the first function of this sequence $P_1$ and a sequence of parameters $h_1,h_2,\\ldots$. The general solution of the $(P,Q)$-recursion is shown to give a solution for the parametric graded Korteweg--de Vries hierarchy. We prove that all solutions of the Mumford dynamical $g$-system are determined by the $(P,Q)$-recursion under the condition $P_{g+1} = 0$, which is equivalent to an ordinary nonlinear differential equation of order $2g$ for the function $P_1$. Reduction of the $g$-system of Mumford to the Buchstaber--Enolskii--Leykin dynamical system is described explicitly, and its explicit $2g$-parameter solution in hyperelliptic Klein functions is presented.","sentences":["We establish differential-algebraic theory of the Mumford dynamical system.","In the framework of this theory, we introduce the $(P,Q)$-recursion, which defines a sequence of functions $P_1,P_2,\\ldots$ given the first function of this sequence $P_1$ and a sequence of parameters $h_1,h_2,\\ldots$. The general solution of the $(P,Q)$-recursion is shown to give a solution for the parametric graded Korteweg--de Vries hierarchy.","We prove that all solutions of the Mumford dynamical $g$-system are determined by the $(P,Q)$-recursion under the condition $P_{g+1} = 0$, which is equivalent to an ordinary nonlinear differential equation of order $2g$ for the function $P_1$. Reduction of the $g$-system of Mumford to the Buchstaber--Enolskii--Leykin dynamical system is described explicitly, and its explicit $2g$-parameter solution in hyperelliptic Klein functions is presented."],"url":"http://arxiv.org/abs/2402.09218v1","category":"nlin.SI"}
{"created":"2024-02-14 14:53:56","title":"Scaling the Authoring of AutoTutors with Large Language Models","abstract":"Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees. We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results. Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer. This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility of LLM-based approaches. Through a human evaluation study on two datasets based on math word problems, we show that our hybrid approach achieves a better overall tutoring score than an instructed, but otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the scope for the community to improve its performance by improving individual modules or using different teaching strategies that it can follow","sentences":["Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation.","In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems.","A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees.","We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results.","Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer.","This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility of LLM-based approaches.","Through a human evaluation study on two datasets based on math word problems, we show that our hybrid approach achieves a better overall tutoring score than an instructed, but otherwise free-form, GPT-4.","MWPTutor is completely modular and opens up the scope for the community to improve its performance by improving individual modules or using different teaching strategies that it can follow"],"url":"http://arxiv.org/abs/2402.09216v1","category":"cs.CL"}
{"created":"2024-02-14 14:53:46","title":"Modeling of groundwater flow in porous medium layered over inclined impermeable bed","abstract":"We propose a new mathematical model of groundwater flow in porous medium layered over inclined impermeable bed. In its full generality, this is a free-surface problem. To obtain analytically tractable model, we use generalized Dupuit-Forchheimer assumption for inclined impermeable bed. In this way, we arrive at parabolic partial differential equation which is a generalization of the classical Boussinesq equation. Novelty of our approach consists in considering nonlinear constitutive law of the power type. Thus introducing $p$-Laplacian-like differential operator into the Boussinesq equation. Unlike in the classical case of the Boussinesq equation, the convective term cannot be set aside from the main part of the diffusive term and remains incorporated within it.   In the sequel of the paper, we analyze qualitative properties of the stationary solutions of our model. In particular, we study existence and regularity of weak solutions for the following boundary value problem \\begin{equation*} \\begin{aligned}   &   -   \\frac{\\rm d}{{\\rm d} x}   \\left[   (u(x) + H) \\left|\\frac{{\\rm d} u}{{\\rm d} x}(x) \\cos(\\varphi) + \\sin(\\varphi) \\right|^{p - 2}   \\left(\\frac{{\\rm d} u}{{\\rm d} x}(x) \\cos(\\varphi) + \\sin(\\varphi)\\right)   \\right]   &   \\begin{aligned}   &   =   f(x)\\,, & \\qquad\\qquad x \\in (-1,1)\\,,   &   u(-1) = u(1) = 0\\,,&   \\end{aligned} \\end{aligned} \\end{equation*} where $p>1$, $H>0$, $\\varphi\\in (0, \\pi/2)$, $f\\geq 0$, $f\\in L^{1}(-1,1)$. In the case of $p>2$, we study validity of Weak and Strong Maximum Principles as well. We use methods based on the linearization of the $p$-Laplacian-type problems in the vicinity of known solution, error estimates, and analysis of Green's function of the linearized problem.","sentences":["We propose a new mathematical model of groundwater flow in porous medium layered over inclined impermeable bed.","In its full generality, this is a free-surface problem.","To obtain analytically tractable model, we use generalized Dupuit-Forchheimer assumption for inclined impermeable bed.","In this way, we arrive at parabolic partial differential equation which is a generalization of the classical Boussinesq equation.","Novelty of our approach consists in considering nonlinear constitutive law of the power type.","Thus introducing $p$-Laplacian-like differential operator into the Boussinesq equation.","Unlike in the classical case of the Boussinesq equation, the convective term cannot be set aside from the main part of the diffusive term and remains incorporated within it.   ","In the sequel of the paper, we analyze qualitative properties of the stationary solutions of our model.","In particular, we study existence and regularity of weak solutions for the following boundary value problem \\begin{equation*} \\begin{aligned}   &   -   \\frac{\\rm d}{{\\rm d} x}   \\left[   (u(x)","+ H) \\left|\\frac{{\\rm d} u}{{\\rm d} x}(x) \\cos(\\varphi)","+ \\sin(\\varphi) \\right|^{p - 2}   \\left(\\frac{{\\rm d} u}{{\\rm d} x}(x) \\cos(\\varphi) +","\\sin(\\varphi)\\right)   \\right]   &   \\begin{aligned}   &   =   f(x)\\,, & \\qquad\\qquad x \\in (-1,1)\\,,   &   u(-1) = u(1) = 0\\,,&   \\end{aligned} \\end{aligned} \\end{equation*} where $p>1$, $H>0$, $\\varphi\\in (0, \\pi/2)$, $f\\geq 0$, $f\\in L^{1}(-1,1)$. In the case of $p>2$, we study validity of Weak and Strong Maximum Principles as well.","We use methods based on the linearization of the $p$-Laplacian-type problems in the vicinity of known solution, error estimates, and analysis of Green's function of the linearized problem."],"url":"http://arxiv.org/abs/2402.09215v1","category":"math.AP"}
{"created":"2024-02-14 14:46:10","title":"Machine classification of quantum correlations for entanglement distribution networks","abstract":"The paper suggest employing machine learning for resource-efficient classification of quantum correlations in entanglement distribution networks. Specifically, artificial neural networks (ANN) are utilized to classify quantum correlations based on collective measurements conducted in the geometry of entanglement swapping. ANNs are trained to categorize two-qubit quantum states into five mutually exclusive classes depending on the strength of quantum correlations exhibited by the states. The precision and recall of the ANN models are analyzed as functions of the quantum resources consumed, i.e. the number of collective measurements performed.","sentences":["The paper suggest employing machine learning for resource-efficient classification of quantum correlations in entanglement distribution networks.","Specifically, artificial neural networks (ANN) are utilized to classify quantum correlations based on collective measurements conducted in the geometry of entanglement swapping.","ANNs are trained to categorize two-qubit quantum states into five mutually exclusive classes depending on the strength of quantum correlations exhibited by the states.","The precision and recall of the ANN models are analyzed as functions of the quantum resources consumed, i.e. the number of collective measurements performed."],"url":"http://arxiv.org/abs/2402.09212v1","category":"quant-ph"}
{"created":"2024-02-14 14:46:03","title":"DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced Three-Point Trackers","abstract":"Full-body avatar presence is crucial for immersive social and environmental interactions in digital reality. However, current devices only provide three six degrees of freedom (DOF) poses from the headset and two controllers (i.e. three-point trackers). Because it is a highly under-constrained problem, inferring full-body pose from these inputs is challenging, especially when supporting the full range of body proportions and use cases represented by the general population. In this paper, we propose a deep learning framework, DivaTrack, which outperforms existing methods when applied to diverse body sizes and activities. We augment the sparse three-point inputs with linear accelerations from Inertial Measurement Units (IMU) to improve foot contact prediction. We then condition the otherwise ambiguous lower-body pose with the predictions of foot contact and upper-body pose in a two-stage model. We further stabilize the inferred full-body pose in a wide range of configurations by learning to blend predictions that are computed in two reference frames, each of which is designed for different types of motions. We demonstrate the effectiveness of our design on a large dataset that captures 22 subjects performing challenging locomotion for three-point tracking, including lunges, hula-hooping, and sitting. As shown in a live demo using the Meta VR headset and Xsens IMUs, our method runs in real-time while accurately tracking a user's motion when they perform a diverse set of movements.","sentences":["Full-body avatar presence is crucial for immersive social and environmental interactions in digital reality.","However, current devices only provide three six degrees of freedom (DOF) poses from the headset and two controllers (i.e. three-point trackers).","Because it is a highly under-constrained problem, inferring full-body pose from these inputs is challenging, especially when supporting the full range of body proportions and use cases represented by the general population.","In this paper, we propose a deep learning framework, DivaTrack, which outperforms existing methods when applied to diverse body sizes and activities.","We augment the sparse three-point inputs with linear accelerations from Inertial Measurement Units (IMU) to improve foot contact prediction.","We then condition the otherwise ambiguous lower-body pose with the predictions of foot contact and upper-body pose in a two-stage model.","We further stabilize the inferred full-body pose in a wide range of configurations by learning to blend predictions that are computed in two reference frames, each of which is designed for different types of motions.","We demonstrate the effectiveness of our design on a large dataset that captures 22 subjects performing challenging locomotion for three-point tracking, including lunges, hula-hooping, and sitting.","As shown in a live demo using the Meta VR headset and Xsens IMUs, our method runs in real-time while accurately tracking a user's motion when they perform a diverse set of movements."],"url":"http://arxiv.org/abs/2402.09211v1","category":"cs.CV"}
{"created":"2024-02-14 14:43:59","title":"A general mechanism for enhancer-insulator pairing reveals heterogeneous dynamics in long-distant 3D gene regulation","abstract":"Cells regulate fates and complex body plans using spatiotemporal signaling cascades that alter gene expression. Enhancers, short DNA sequences (50-150 base pairs), help coordinate these cascades by attracting regulatory proteins to enhance the transcription of distal genes by binding to promoters. In humans, there are hundreds of thousands of enhancers dispersed across the genome, which poses a challenging coordination task to prevent unintended gene activation. To mitigate this problem, the genome contains additional DNA elements, insulators, that block enhancer-promoter interactions. However, there is an open problem with how the insulation works, especially as enhancer-insulator pairs may be separated by millions of base pairs. Based on recent empirical data from Hi-C experiments, this paper proposes a new mechanism that challenges the common paradigm that rests on specific insulator-insulator interactions. Instead, this paper introduces a stochastic looping model where enhancers bind weakly to surrounding chromatin. After calibrating the model to experimental data, we use simulations to study the broad distribution of hitting times between an enhancer and a promoter when there are blocking insulators. In some cases, there is a large difference between average and most probable hitting times, making it difficult to assign a typical time scale, hinting at highly defocused regulation times. We also map our computational model onto a resetting problem that allows us to derive several analytical results. Besides offering new insights into enhancer-insulator interactions, our paper advances the understanding of gene regulatory networks and causal connections between genome folding and gene activation.","sentences":["Cells regulate fates and complex body plans using spatiotemporal signaling cascades that alter gene expression.","Enhancers, short DNA sequences (50-150 base pairs), help coordinate these cascades by attracting regulatory proteins to enhance the transcription of distal genes by binding to promoters.","In humans, there are hundreds of thousands of enhancers dispersed across the genome, which poses a challenging coordination task to prevent unintended gene activation.","To mitigate this problem, the genome contains additional DNA elements, insulators, that block enhancer-promoter interactions.","However, there is an open problem with how the insulation works, especially as enhancer-insulator pairs may be separated by millions of base pairs.","Based on recent empirical data from Hi-C experiments, this paper proposes a new mechanism that challenges the common paradigm that rests on specific insulator-insulator interactions.","Instead, this paper introduces a stochastic looping model where enhancers bind weakly to surrounding chromatin.","After calibrating the model to experimental data, we use simulations to study the broad distribution of hitting times between an enhancer and a promoter when there are blocking insulators.","In some cases, there is a large difference between average and most probable hitting times, making it difficult to assign a typical time scale, hinting at highly defocused regulation times.","We also map our computational model onto a resetting problem that allows us to derive several analytical results.","Besides offering new insights into enhancer-insulator interactions, our paper advances the understanding of gene regulatory networks and causal connections between genome folding and gene activation."],"url":"http://arxiv.org/abs/2402.09209v1","category":"q-bio.MN"}
{"created":"2024-02-14 14:40:29","title":"Monoidal model structures on filtered chain complexes relating to spectral sequences","abstract":"We establish monoidal model structures on model categories of filtered chain complexes constructed by Cirici, Egas Santander, Livernet and Whitehouse whose weak equivalences are the quasi-isomorphisms on the $r$-page of the associated spectral sequences. In doing so we provide a partial classification of cofibrant objects and cofibrations of the model structures involving a boundedness restriction on the filtration. As a consequence we also obtain, by results of Schwede and Shipley, cofibrantly generated model structures on the categories of filtered differential graded algebras as well as their modules.","sentences":["We establish monoidal model structures on model categories of filtered chain complexes constructed by Cirici, Egas Santander, Livernet and Whitehouse whose weak equivalences are the quasi-isomorphisms on the $r$-page of the associated spectral sequences.","In doing so we provide a partial classification of cofibrant objects and cofibrations of the model structures involving a boundedness restriction on the filtration.","As a consequence we also obtain, by results of Schwede and Shipley, cofibrantly generated model structures on the categories of filtered differential graded algebras as well as their modules."],"url":"http://arxiv.org/abs/2402.09207v1","category":"math.AT"}
{"created":"2024-02-14 14:36:30","title":"Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents","abstract":"Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency. All the data and codes are released.","sentences":["Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions.","Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions.","To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries.","Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction.","Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution.","Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency.","All the data and codes are released."],"url":"http://arxiv.org/abs/2402.09205v1","category":"cs.CL"}
{"created":"2024-02-14 14:33:56","title":"A universal scaling limit for diffusive amnesic step-reinforced random walks","abstract":"We introduce a variation of the step-reinforced random walk with general memory. For the diffusive regime, we establish a functional invariance principle and show that, given suitable conditions on the memory sequence, the arising limiting processes are always the sum of a noise reinforced Brownian motion and a (not independent) Brownian motion.","sentences":["We introduce a variation of the step-reinforced random walk with general memory.","For the diffusive regime, we establish a functional invariance principle and show that, given suitable conditions on the memory sequence, the arising limiting processes are always the sum of a noise reinforced Brownian motion and a (not independent) Brownian motion."],"url":"http://arxiv.org/abs/2402.09202v1","category":"math.PR"}
{"created":"2024-02-14 14:33:39","title":"Better-than-KL PAC-Bayes Bounds","abstract":"Let $f(\\theta, X_1),$ $ \\dots,$ $ f(\\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \\dots, X_n$ are independent random variables (data), and $\\theta$ is a random parameter distributed according to some data-dependent posterior distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function. Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem. Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto standard choice is the KL divergence. However, the tightness of this choice has rarely been questioned.   In this paper, we challenge the tightness of the KL-divergence-based bounds by showing that it is possible to achieve a strictly tighter bound. In particular, we demonstrate new high-probability PAC-Bayes bounds with a novel and better-than-KL divergence that is inspired by Zhang et al. (2022). Our proof is inspired by recent advances in regret analysis of gambling algorithms, and its use to derive concentration inequalities. Our result is first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are not known to be strictly better than KL. Thus, we believe our work marks the first step towards identifying optimal rates of PAC-Bayes bounds.","sentences":["Let $f(\\theta, X_1),$ $ \\dots,$ $ f(\\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \\dots, X_n$ are independent random variables (data), and $\\theta$ is a random parameter distributed according to some data-dependent posterior distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence.","An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function.","Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem.","Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto standard choice is the KL divergence.","However, the tightness of this choice has rarely been questioned.   ","In this paper, we challenge the tightness of the KL-divergence-based bounds by showing that it is possible to achieve a strictly tighter bound.","In particular, we demonstrate new high-probability PAC-Bayes bounds with a novel and better-than-KL divergence that is inspired by Zhang et al. (2022).","Our proof is inspired by recent advances in regret analysis of gambling algorithms, and its use to derive concentration inequalities.","Our result is first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are not known to be strictly better than KL.","Thus, we believe our work marks the first step towards identifying optimal rates of PAC-Bayes bounds."],"url":"http://arxiv.org/abs/2402.09201v1","category":"cs.LG"}
{"created":"2024-02-14 14:33:17","title":"Discovering Command and Control (C2) Channels on Tor and Public Networks Using Reinforcement Learning","abstract":"Command and control (C2) channels are an essential component of many types of cyber attacks, as they enable attackers to remotely control their malware-infected machines and execute harmful actions, such as propagating malicious code across networks, exfiltrating confidential data, or initiating distributed denial of service (DDoS) attacks. Identifying these C2 channels is therefore crucial in helping to mitigate and prevent cyber attacks. However, identifying C2 channels typically involves a manual process, requiring deep knowledge and expertise in cyber operations. In this paper, we propose a reinforcement learning (RL) based approach to automatically emulate C2 attack campaigns using both the normal (public) and the Tor networks. In addition, payload size and network firewalls are configured to simulate real-world attack scenarios. Results on a typical network configuration show that the RL agent can automatically discover resilient C2 attack paths utilizing both Tor-based and conventional communication channels, while also bypassing network firewalls.","sentences":["Command and control (C2) channels are an essential component of many types of cyber attacks, as they enable attackers to remotely control their malware-infected machines and execute harmful actions, such as propagating malicious code across networks, exfiltrating confidential data, or initiating distributed denial of service (DDoS) attacks.","Identifying these C2 channels is therefore crucial in helping to mitigate and prevent cyber attacks.","However, identifying C2 channels typically involves a manual process, requiring deep knowledge and expertise in cyber operations.","In this paper, we propose a reinforcement learning (RL) based approach to automatically emulate C2 attack campaigns using both the normal (public) and the Tor networks.","In addition, payload size and network firewalls are configured to simulate real-world attack scenarios.","Results on a typical network configuration show that the RL agent can automatically discover resilient C2 attack paths utilizing both Tor-based and conventional communication channels, while also bypassing network firewalls."],"url":"http://arxiv.org/abs/2402.09200v1","category":"cs.CR"}
{"created":"2024-02-14 14:32:16","title":"Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling","abstract":"With the rapidly increasing application of large language models (LLMs), their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution. This makes AI-generated text (AIGT) detection of great importance. Among existing methods, white-box methods are generally superior to black-box methods in terms of performance and generalizability, but they require access to LLMs' internal states and are not applicable to black-box settings. In this paper, we propose to estimate word generation probabilities as pseudo white-box features via multiple re-sampling to help improve AIGT detection under the black-box setting. Specifically, we design POGER, a proxy-guided efficient re-sampling method, which selects a small subset of representative words (e.g., 10 words) for performing multiple re-sampling in black-box AIGT detection. Experiments on datasets containing texts from humans and seven LLMs show that POGER outperforms all baselines in macro F1 under black-box, partial white-box, and out-of-distribution settings and maintains lower re-sampling costs than its existing counterparts.","sentences":["With the rapidly increasing application of large language models (LLMs), their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution.","This makes AI-generated text (AIGT) detection of great importance.","Among existing methods, white-box methods are generally superior to black-box methods in terms of performance and generalizability, but they require access to LLMs' internal states and are not applicable to black-box settings.","In this paper, we propose to estimate word generation probabilities as pseudo white-box features via multiple re-sampling to help improve AIGT detection under the black-box setting.","Specifically, we design POGER, a proxy-guided efficient re-sampling method, which selects a small subset of representative words (e.g., 10 words) for performing multiple re-sampling in black-box AIGT detection.","Experiments on datasets containing texts from humans and seven LLMs show that POGER outperforms all baselines in macro F1 under black-box, partial white-box, and out-of-distribution settings and maintains lower re-sampling costs than its existing counterparts."],"url":"http://arxiv.org/abs/2402.09199v1","category":"cs.CL"}
{"created":"2024-02-14 14:28:22","title":"Same-diff? Conceptual similarities between gauge transformations and diffeomorphisms. Part III: Representational conventions and relationism","abstract":"The following questions are germane to our understanding of gauge-(in)variant quantities and physical possibility: in which ways are gauge transformations and spacetime diffeomorphisms similar, and in which are they different? Sophistication is the most popular attitude towards some of these questions: roughly, it takes models related by these symmetries to represent the same physical possibility. In the previous paper in this series, I discussed obstacles to sophistication and then showed how these obstacles are overcome by theories that fulfill three Desiderata (i-iii). But this resolution still leaves open two main worries about sophistication: (a) it allows the individuation of structure-tokens to remain intractably prolix and thus of limited use, which is why practising physicists frequently invoke 'relational, symmetry-invariant observables'; and (b) it leaves us with no formal framework for expressing counterfactual statements about the world. Here I will show that a third Desideratum, (iii), answers these worries. The new Desideratum requires a `relational' understanding \\emph{of coordinates} (or frames, etc).","sentences":["The following questions are germane to our understanding of gauge-(in)variant quantities and physical possibility: in which ways are gauge transformations and spacetime diffeomorphisms similar, and in which are they different?","Sophistication is the most popular attitude towards some of these questions: roughly, it takes models related by these symmetries to represent the same physical possibility.","In the previous paper in this series, I discussed obstacles to sophistication and then showed how these obstacles are overcome by theories that fulfill three Desiderata (i-iii).","But this resolution still leaves open two main worries about sophistication: (a) it allows the individuation of structure-tokens to remain intractably prolix and thus of limited use, which is why practising physicists frequently invoke 'relational, symmetry-invariant observables'; and (b) it leaves us with no formal framework for expressing counterfactual statements about the world.","Here I will show that a third Desideratum, (iii), answers these worries.","The new Desideratum requires a `relational' understanding \\emph{of coordinates} (or frames, etc)."],"url":"http://arxiv.org/abs/2402.09198v1","category":"physics.hist-ph"}
{"created":"2024-02-14 14:27:52","title":"Implementing local-explainability in Gradient Boosting Trees: Feature Contribution","abstract":"Gradient Boost Decision Trees (GBDT) is a powerful additive model based on tree ensembles. Its nature makes GBDT a black-box model even though there are multiple explainable artificial intelligence (XAI) models obtaining information by reinterpreting the model globally and locally. Each tree of the ensemble is a transparent model itself but the final outcome is the result of a sum of these trees and it is not easy to clarify.   In this paper, a feature contribution method for GBDT is developed. The proposed method takes advantage of the GBDT architecture to calculate the contribution of each feature using the residue of each node. This algorithm allows to calculate the sequence of node decisions given a prediction.   Theoretical proofs and multiple experiments have been carried out to demonstrate the performance of our method which is not only a local explicability model for the GBDT algorithm but also a unique option that reflects GBDTs internal behavior. The proposal is aligned to the contribution of characteristics having impact in some artificial intelligence problems such as ethical analysis of Artificial Intelligence (AI) and comply with the new European laws such as the General Data Protection Regulation (GDPR) about the right to explain and nondiscrimination.","sentences":["Gradient Boost Decision Trees (GBDT) is a powerful additive model based on tree ensembles.","Its nature makes GBDT a black-box model even though there are multiple explainable artificial intelligence (XAI) models obtaining information by reinterpreting the model globally and locally.","Each tree of the ensemble is a transparent model itself but the final outcome is the result of a sum of these trees and it is not easy to clarify.   ","In this paper, a feature contribution method for GBDT is developed.","The proposed method takes advantage of the GBDT architecture to calculate the contribution of each feature using the residue of each node.","This algorithm allows to calculate the sequence of node decisions given a prediction.   ","Theoretical proofs and multiple experiments have been carried out to demonstrate the performance of our method which is not only a local explicability model for the GBDT algorithm but also a unique option that reflects GBDTs internal behavior.","The proposal is aligned to the contribution of characteristics having impact in some artificial intelligence problems such as ethical analysis of Artificial Intelligence (AI) and comply with the new European laws such as the General Data Protection Regulation (GDPR) about the right to explain and nondiscrimination."],"url":"http://arxiv.org/abs/2402.09197v1","category":"cs.LG"}
{"created":"2024-02-14 14:23:57","title":"Complete complementarity relations in tree level QED processes","abstract":"We exploit complete complementarity relations to fully characterize various aspects of quantumness in a Bhabha scattering process $(e^-e^+ \\rightarrow e^-e^+)$ at tree level. For illustrative purposes, we consider three different situations: in the first one the initial electron A and positron B are described by a factorized state; in the second one, the incoming particles are described by local superpositions and the total state is factorized; finally, we consider the more general initial state in which A and B can be entangled. We find that the QED scattering process generates and distributes quantum information in a non-trivial way among the particles, with CCR being fulfilled both for initial and final states.","sentences":["We exploit complete complementarity relations to fully characterize various aspects of quantumness in a Bhabha scattering process $(e^-e^+ \\rightarrow e^-e^+)$ at tree level.","For illustrative purposes, we consider three different situations: in the first one the initial electron A and positron B are described by a factorized state; in the second one, the incoming particles are described by local superpositions and the total state is factorized; finally, we consider the more general initial state in which A and B can be entangled.","We find that the QED scattering process generates and distributes quantum information in a non-trivial way among the particles, with CCR being fulfilled both for initial and final states."],"url":"http://arxiv.org/abs/2402.09195v1","category":"quant-ph"}
{"created":"2024-02-14 14:17:21","title":"(Ir)rationality and Cognitive Biases in Large Language Models","abstract":"Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.","sentences":["Do large language models (LLMs) display rational reasoning?","LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear.","In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature.","We find that, like humans, LLMs display irrationality in these tasks.","However, the way this irrationality is displayed does not reflect that shown by humans.","When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases.","On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses.","Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning."],"url":"http://arxiv.org/abs/2402.09193v1","category":"cs.CL"}
{"created":"2024-02-14 14:15:21","title":"Cyber Deception Reactive: TCP Stealth Redirection to On-Demand Honeypots","abstract":"Cybersecurity is developing rapidly, and new methods of defence against attackers are appearing, such as Cyber Deception (CYDEC). CYDEC consists of deceiving the enemy who performs actions without realising that he/she is being deceived. This article proposes designing, implementing, and evaluating a deception mechanism based on the stealthy redirection of TCP communications to an on-demand honey server with the same characteristics as the victim asset, i.e., it is a clone. Such a mechanism ensures that the defender fools the attacker, thanks to stealth redirection. In this situation, the attacker will focus on attacking the honey server while enabling the recollection of relevant information to generate threat intelligence. The experiments in different scenarios show how the proposed solution can effectively redirect an attacker to a copied asset on demand, thus protecting the real asset. Finally, the results obtained by evaluating the latency times ensure that the redirection is undetectable by humans and very difficult to detect by a machine.","sentences":["Cybersecurity is developing rapidly, and new methods of defence against attackers are appearing, such as Cyber Deception (CYDEC).","CYDEC consists of deceiving the enemy who performs actions without realising that he/she is being deceived.","This article proposes designing, implementing, and evaluating a deception mechanism based on the stealthy redirection of TCP communications to an on-demand honey server with the same characteristics as the victim asset, i.e., it is a clone.","Such a mechanism ensures that the defender fools the attacker, thanks to stealth redirection.","In this situation, the attacker will focus on attacking the honey server while enabling the recollection of relevant information to generate threat intelligence.","The experiments in different scenarios show how the proposed solution can effectively redirect an attacker to a copied asset on demand, thus protecting the real asset.","Finally, the results obtained by evaluating the latency times ensure that the redirection is undetectable by humans and very difficult to detect by a machine."],"url":"http://arxiv.org/abs/2402.09191v1","category":"cs.CR"}
{"created":"2024-02-14 14:10:02","title":"Invariants of persistence modules defined by order-embeddings","abstract":"One of the main objectives of topological data analysis is the study of discrete invariants for persistence modules, in particular when dealing with multiparameter persistence modules. In many cases, the invariants studied for these non-totally ordered posets $P$ can be obtained from restricting a given module to a subposet $X$ of $P$ that is totally ordered (or more generally, of finite representation type), and then computing the barcode (or the general direct sum decomposition) over $X$.   We consider in this paper general order-preserving embeddings of representation-finite subposets $X$ into $P$ and study systematically the invariants obtained by decomposing the restriction of a given $P$-module $M$ to $X$ into its indecomposable summands. The restriction functor from $\\mathrm{mod} \\ P$ to $\\mathrm{mod} \\ X$ is well-studied, and it is known to be exact and admits both left and right adjoint functors, known as induction and co-induction functors. This allows us to obtain new homological insights, and also to re-interpret previous results. We use this approach also to determine bases, thus generalizing the concept of signed barcodes which is considered in the literature in relation to stability results.   It turns out that considering only order-embeddings of one fixed poset $X$ into the poset $P$, and studying the set of all indecomposables obtained from $X$ introduces a lot of redundancy. We therefore also study iterated embeddings of several posets of increasing sizes, while limiting attention to only some indecomposables (that have not been obtained from embedding of smaller posets previously).","sentences":["One of the main objectives of topological data analysis is the study of discrete invariants for persistence modules, in particular when dealing with multiparameter persistence modules.","In many cases, the invariants studied for these non-totally ordered posets $P$ can be obtained from restricting a given module to a subposet $X$ of $P$ that is totally ordered (or more generally, of finite representation type), and then computing the barcode (or the general direct sum decomposition) over $X$.   We consider in this paper general order-preserving embeddings of representation-finite subposets $X$ into $P$ and study systematically the invariants obtained by decomposing the restriction of a given $P$-module $M$ to $X$ into its indecomposable summands.","The restriction functor from $\\mathrm{mod} \\ P$ to $\\mathrm{mod} \\ X$ is well-studied, and it is known to be exact and admits both left and right adjoint functors, known as induction and co-induction functors.","This allows us to obtain new homological insights, and also to re-interpret previous results.","We use this approach also to determine bases, thus generalizing the concept of signed barcodes which is considered in the literature in relation to stability results.   ","It turns out that considering only order-embeddings of one fixed poset $X$ into the poset $P$, and studying the set of all indecomposables obtained from $X$ introduces a lot of redundancy.","We therefore also study iterated embeddings of several posets of increasing sizes, while limiting attention to only some indecomposables (that have not been obtained from embedding of smaller posets previously)."],"url":"http://arxiv.org/abs/2402.09190v1","category":"math.AT"}
{"created":"2024-02-14 14:07:27","title":"Perazzo hypersurfaces and the weak Lefschetz property","abstract":"We deal with Perazzo hypersurfaces $X=V(f)$ in $\\PP^{n+2}$ defined by a homogeneous polynomial $f(x_0,x_1,\\dots,x_n,u,v)=p_0(u,v)x_0+p_1(u,v)x_1+\\cdots +p_n(u,v)x_n+g(u,v)$, where $p_0,p_1,\\dots ,p_n$ are algebraically dependent but linearly independent forms of degree $d-1$ in $K[u,v]$ and $g$ is a form in $K[u,v]$ of degree $d$. Perazzo hypersurfaces have vanishing hessian and, hence, the associated graded artinian Gorenstein algebra $A_f$ fails the strong Lefschetz property. In this paper, we first determine the maximum and minimum Hilbert function of $A_f$, we prove that the Hilbert function of $A_f$ is always unimodal and we determine when $A_f$ satisfies the weak Lefschetz property. We illustrate our results with many examples and we show that our results do not generalize to Perazzo hypersurfaces $X=V(f)$ in $\\PP^{n+3}$ defined by a homogeneous polynomial $f(x_0,x_1,\\dots,x_{n},u,v,w)=p_0(u,v,w)x_0+p_1(u,v,w)x_1+\\cdots +p_{n}(u,v,w)x_{n}+g(u,v,w)$, where $p_0,p_1,\\dots ,p_{n}$ are algebraically dependent but linearly independent forms of degree $d-1$ in $K[u,v,w]$ and $g$ is a form in $K[u,v,w]$ of degree $d$.","sentences":["We deal with Perazzo hypersurfaces $X=V(f)$ in $\\PP^{n+2}$ defined by a homogeneous polynomial $f(x_0,x_1,\\dots,x_n,u,v)=p_0(u,v)x_0+p_1(u,v)x_1+\\cdots +p_n(u,v)x_n+g(u,v)$, where $p_0,p_1,\\dots ,p_n$ are algebraically dependent but linearly independent forms of degree $d-1$ in $K[u,v]$ and $g$ is a form in $K[u,v]$ of degree $d$. Perazzo hypersurfaces have vanishing hessian and, hence, the associated graded artinian Gorenstein algebra $A_f$ fails the strong Lefschetz property.","In this paper, we first determine the maximum and minimum Hilbert function of $A_f$, we prove that the Hilbert function of $A_f$ is always unimodal","and we determine when $A_f$ satisfies the weak Lefschetz property.","We illustrate our results with many examples and we show that our results do not generalize to Perazzo hypersurfaces $X=V(f)$ in $\\PP^{n+3}$ defined by a homogeneous polynomial $f(x_0,x_1,\\dots,x_{n},u,v,w)=p_0(u,v,w)x_0+p_1(u,v,w)x_1+\\cdots +p_{n}(u,v,w)x_{n}+g(u,v,w)$, where $p_0,p_1,\\dots ,p_{n}$ are algebraically dependent but linearly independent forms of degree $d-1$ in $K[u,v,w]$ and $g$ is a form in $K[u,v,w]$ of degree $d$."],"url":"http://arxiv.org/abs/2402.09188v1","category":"math.AG"}
{"created":"2024-02-14 14:06:41","title":"Identifying Tractable Quantified Temporal Constraints within Ord-Horn","abstract":"The constraint satisfaction problem, parameterized by a relational structure, provides a general framework for expressing computational decision problems. Already the restriction to the class of all finite structures forms an interesting microcosm on its own, but to express decision problems in temporal reasoning one has to take a step beyond the finite-domain realm. An important class of templates used in this context are temporal structures, i.e., structures over $\\mathbb{Q}$ whose relations are first-order definable using the usual countable dense linear order without endpoints. In the standard setting, which allows only existential quantification over input variables, the complexity of finite and temporal constraints has been fully classified. In the quantified setting, i.e., when one also allows universal quantifiers, there is only a handful of partial classification results and many concrete cases of unknown complexity. This paper presents a significant progress towards understanding the complexity of the quantified constraint satisfaction problem for temporal structures. We provide a complexity dichotomy for quantified constraints over the Ord-Horn fragment, which played an important role in understanding the complexity of constraints both over temporal structures and in Allen's interval algebra. We show that all problems under consideration are in P or coNP-hard. In particular, we determine the complexity of the quantified constraint satisfaction problem for $(\\mathbb{Q};x=y\\Rightarrow x\\geq z)$, hereby settling a question open for more than ten years.","sentences":["The constraint satisfaction problem, parameterized by a relational structure, provides a general framework for expressing computational decision problems.","Already the restriction to the class of all finite structures forms an interesting microcosm on its own, but to express decision problems in temporal reasoning one has to take a step beyond the finite-domain realm.","An important class of templates used in this context are temporal structures, i.e., structures over $\\mathbb{Q}$ whose relations are first-order definable using the usual countable dense linear order without endpoints.","In the standard setting, which allows only existential quantification over input variables, the complexity of finite and temporal constraints has been fully classified.","In the quantified setting, i.e., when one also allows universal quantifiers, there is only a handful of partial classification results and many concrete cases of unknown complexity.","This paper presents a significant progress towards understanding the complexity of the quantified constraint satisfaction problem for temporal structures.","We provide a complexity dichotomy for quantified constraints over the Ord-Horn fragment, which played an important role in understanding the complexity of constraints both over temporal structures and in Allen's interval algebra.","We show that all problems under consideration are in P or coNP-hard.","In particular, we determine the complexity of the quantified constraint satisfaction problem for $(\\mathbb{Q};x=y\\Rightarrow x\\geq z)$, hereby settling a question open for more than ten years."],"url":"http://arxiv.org/abs/2402.09187v1","category":"cs.LO"}
{"created":"2024-02-14 13:51:56","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM","abstract":"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks. However, their potential in the medical domain remains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications. To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical Visual Question Answering (VQA) benchmark. This benchmark is collected from 75 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions. Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs. Through our extensive experiments, we have found that existing LVLMs struggle to address these medical VQA problems effectively. Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field. The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset's significance. Our dataset will be made publicly available.","sentences":["Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks.","However, their potential in the medical domain remains largely unexplored.","A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications.","To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical Visual Question Answering (VQA) benchmark.","This benchmark is collected from 75 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions.","Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs.","Through our extensive experiments, we have found that existing LVLMs struggle to address these medical VQA problems effectively.","Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field.","The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset's significance.","Our dataset will be made publicly available."],"url":"http://arxiv.org/abs/2402.09181v1","category":"eess.IV"}
{"created":"2024-02-14 13:47:18","title":"Generalized Portrait Quality Assessment","abstract":"Automated and robust portrait quality assessment (PQA) is of paramount importance in high-impact applications such as smartphone photography. This paper presents FHIQA, a learning-based approach to PQA that introduces a simple but effective quality score rescaling method based on image semantics, to enhance the precision of fine-grained image quality metrics while ensuring robust generalization to various scene settings beyond the training dataset. The proposed approach is validated by extensive experiments on the PIQ23 benchmark and comparisons with the current state of the art. The source code of FHIQA will be made publicly available on the PIQ23 GitHub repository at https://github.com/DXOMARK-Research/PIQ2023.","sentences":["Automated and robust portrait quality assessment (PQA) is of paramount importance in high-impact applications such as smartphone photography.","This paper presents FHIQA, a learning-based approach to PQA that introduces a simple but effective quality score rescaling method based on image semantics, to enhance the precision of fine-grained image quality metrics while ensuring robust generalization to various scene settings beyond the training dataset.","The proposed approach is validated by extensive experiments on the PIQ23 benchmark and comparisons with the current state of the art.","The source code of FHIQA will be made publicly available on the PIQ23 GitHub repository at https://github.com/DXOMARK-Research/PIQ2023."],"url":"http://arxiv.org/abs/2402.09178v1","category":"cs.CV"}
{"created":"2024-02-14 13:45:19","title":"Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks","abstract":"Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in LLMs. We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM. By doing so, we guide the responses of the model toward revealing the 'desired' harmful information. We conduct experiments on four different LLMs and demonstrate the efficacy of this attack, which is black-box and can also transfer across LLMs. We believe this can lead to further developments and understanding of the context vector in LLMs.","sentences":["Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query.","As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks.","In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack.","The idea relies on the autoregressive nature of the generation process in LLMs.","We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks.","Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM.","By doing so, we guide the responses of the model toward revealing the 'desired' harmful information.","We conduct experiments on four different LLMs and demonstrate the efficacy of this attack, which is black-box and can also transfer across LLMs.","We believe this can lead to further developments and understanding of the context vector in LLMs."],"url":"http://arxiv.org/abs/2402.09177v1","category":"cs.LG"}
{"created":"2024-02-14 13:45:06","title":"Large Language Model Interaction Simulator for Cold-Start Item Recommendation","abstract":"Recommending cold items is a long-standing challenge for collaborative filtering models because these cold items lack historical user interactions to model their collaborative features. The gap between the content of cold items and their behavior patterns makes it difficult to generate accurate behavioral embeddings for cold items. Existing cold-start models use mapping functions to generate fake behavioral embeddings based on the content feature of cold items. However, these generated embeddings have significant differences from the real behavioral embeddings, leading to a negative impact on cold recommendation performance. To address this challenge, we propose an LLM Interaction Simulator (LLM-InS) to model users' behavior patterns based on the content aspect. This simulator allows recommender systems to simulate vivid interactions for each cold item and transform them from cold to warm items directly. Specifically, we outline the designing and training process of a tailored LLM-simulator that can simulate the behavioral patterns of users and items. Additionally, we introduce an efficient \"filtering-and-refining\" approach to take full advantage of the simulation power of the LLMs. Finally, we propose an updating method to update the embeddings of the items. we unified trains for both cold and warm items within a recommender model based on the simulated and real interactions. Extensive experiments using real behavioral embeddings demonstrate that our proposed model, LLM-InS, outperforms nine state-of-the-art cold-start methods and three LLM models in cold-start item recommendations.","sentences":["Recommending cold items is a long-standing challenge for collaborative filtering models because these cold items lack historical user interactions to model their collaborative features.","The gap between the content of cold items and their behavior patterns makes it difficult to generate accurate behavioral embeddings for cold items.","Existing cold-start models use mapping functions to generate fake behavioral embeddings based on the content feature of cold items.","However, these generated embeddings have significant differences from the real behavioral embeddings, leading to a negative impact on cold recommendation performance.","To address this challenge, we propose an LLM Interaction Simulator (LLM-InS) to model users' behavior patterns based on the content aspect.","This simulator allows recommender systems to simulate vivid interactions for each cold item and transform them from cold to warm items directly.","Specifically, we outline the designing and training process of a tailored LLM-simulator that can simulate the behavioral patterns of users and items.","Additionally, we introduce an efficient \"filtering-and-refining\" approach to take full advantage of the simulation power of the LLMs.","Finally, we propose an updating method to update the embeddings of the items.","we unified trains for both cold and warm items within a recommender model based on the simulated and real interactions.","Extensive experiments using real behavioral embeddings demonstrate that our proposed model, LLM-InS, outperforms nine state-of-the-art cold-start methods and three LLM models in cold-start item recommendations."],"url":"http://arxiv.org/abs/2402.09176v1","category":"cs.IR"}
{"created":"2024-02-14 13:43:14","title":"Automated Unit Test Improvement using Large Language Models at Meta","abstract":"This paper describes Meta's TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM's test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta's Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.","sentences":["This paper describes Meta's TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests.","TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination.","We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms.","In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM's test cases built correctly, 57% passed reliably, and 25% increased coverage.","During Meta's Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers.","We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement."],"url":"http://arxiv.org/abs/2402.09171v1","category":"cs.SE"}
{"created":"2024-02-14 13:43:10","title":"Permittivity Estimation in Ray-tracing Using Path Loss Data based on GAMP","abstract":"In this paper, we propose a modified Generalized Approximate Message Passing (GAMP) algorithm to estimate permittivity parameters using path loss data in ray-tracing model.","sentences":["In this paper, we propose a modified Generalized Approximate Message Passing (GAMP) algorithm to estimate permittivity parameters using path loss data in ray-tracing model."],"url":"http://arxiv.org/abs/2402.09170v1","category":"eess.SP"}
{"created":"2024-02-14 13:31:53","title":"Unifying Invariance and Spuriousity for Graph Out-of-Distribution via Probability of Necessity and Sufficiency","abstract":"Graph Out-of-Distribution (OOD), requiring that models trained on biased data generalize to the unseen test data, has a massive of real-world applications. One of the most mainstream methods is to extract the invariant subgraph by aligning the original and augmented data with the help of environment augmentation. However, these solutions might lead to the loss or redundancy of semantic subgraph and further result in suboptimal generalization. To address this challenge, we propose a unified framework to exploit the Probability of Necessity and Sufficiency to extract the Invariant Substructure (PNSIS). Beyond that, this framework further leverages the spurious subgraph to boost the generalization performance in an ensemble manner to enhance the robustness on the noise data. Specificially, we first consider the data generation process for graph data. Under mild conditions, we show that the invariant subgraph can be extracted by minimizing an upper bound, which is built on the theoretical advance of probability of necessity and sufficiency. To further bridge the theory and algorithm, we devise the PNSIS model, which involves an invariant subgraph extractor for invariant graph learning as well invariant and spurious subgraph classifiers for generalization enhancement. Experimental results demonstrate that our \\textbf{PNSIS} model outperforms the state-of-the-art techniques on graph OOD on several benchmarks, highlighting the effectiveness in real-world scenarios.","sentences":["Graph Out-of-Distribution (OOD), requiring that models trained on biased data generalize to the unseen test data, has a massive of real-world applications.","One of the most mainstream methods is to extract the invariant subgraph by aligning the original and augmented data with the help of environment augmentation.","However, these solutions might lead to the loss or redundancy of semantic subgraph and further result in suboptimal generalization.","To address this challenge, we propose a unified framework to exploit the Probability of Necessity and Sufficiency to extract the Invariant Substructure (PNSIS).","Beyond that, this framework further leverages the spurious subgraph to boost the generalization performance in an ensemble manner to enhance the robustness on the noise data.","Specificially, we first consider the data generation process for graph data.","Under mild conditions, we show that the invariant subgraph can be extracted by minimizing an upper bound, which is built on the theoretical advance of probability of necessity and sufficiency.","To further bridge the theory and algorithm, we devise the PNSIS model, which involves an invariant subgraph extractor for invariant graph learning as well invariant and spurious subgraph classifiers for generalization enhancement.","Experimental results demonstrate that our \\textbf{PNSIS} model outperforms the state-of-the-art techniques on graph OOD on several benchmarks, highlighting the effectiveness in real-world scenarios."],"url":"http://arxiv.org/abs/2402.09165v1","category":"cs.LG"}
{"created":"2024-02-14 13:30:02","title":"Less is More: Fewer Interpretable Region via Submodular Subset Selection","abstract":"Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets. Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular. Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011). For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9% and 2.5% gain relative to HSIC-Attribution. For incorrectly predicted samples, our method achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively. The code is released at https://github.com/RuoyuChen10/SMDL-Attribution.","sentences":["Image attribution algorithms aim to identify important regions that are highly relevant to model decisions.","Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions.","To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions.","To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions.","To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets.","Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular.","Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011).","For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9% and 2.5% gain relative to HSIC-Attribution.","For incorrectly predicted samples, our method achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively.","The code is released at https://github.com/RuoyuChen10/SMDL-Attribution."],"url":"http://arxiv.org/abs/2402.09164v1","category":"cs.CV"}
{"created":"2024-02-14 13:24:21","title":"Role-Playing Simulation Games using ChatGPT","abstract":"Since the COVID-19 pandemic, educational institutions have embarked on digital transformation projects. The success of these projects depends on integrating new technologies and understanding the needs of digitally literate students. The \"learning by doing\" approach suggests that real success in learning new skills is achieved when students can try out and practise these skills. In this article, we demonstrate how Large Language Models (LLMs) can enhance the quality of teaching by using ChatGPT in a role-playing simulation game scenario to promote active learning. Moreover, we discuss how LLMs can boost students' interest in learning by allowing them to practice real-life scenarios using ChatGPT.","sentences":["Since the COVID-19 pandemic, educational institutions have embarked on digital transformation projects.","The success of these projects depends on integrating new technologies and understanding the needs of digitally literate students.","The \"learning by doing\" approach suggests that real success in learning new skills is achieved when students can try out and practise these skills.","In this article, we demonstrate how Large Language Models (LLMs) can enhance the quality of teaching by using ChatGPT in a role-playing simulation game scenario to promote active learning.","Moreover, we discuss how LLMs can boost students' interest in learning by allowing them to practice real-life scenarios using ChatGPT."],"url":"http://arxiv.org/abs/2402.09161v1","category":"cs.AI"}
{"created":"2024-02-14 13:22:02","title":"On the quotient of affine semigroups by a positive integer","abstract":"This work delves into the {\\it quotient of an affine semigroup by a positive integer}, exploring its intricate properties and broader implications. We unveil an {\\it associated tree} that serves as a valuable tool for further analysis. Moreover, we successfully generalize several key irreducibility results, extending their applicability to the more general class of $\\mathcal C$-semigroup quotients. To shed light on these concepts, we introduce the novel notion of an {\\it arithmetic variety of affine semigroups}, accompanied by illuminating examples that showcase its power.","sentences":["This work delves into the {\\it quotient of an affine semigroup by a positive integer}, exploring its intricate properties and broader implications.","We unveil an {\\it associated tree} that serves as a valuable tool for further analysis.","Moreover, we successfully generalize several key irreducibility results, extending their applicability to the more general class of $\\mathcal C$-semigroup quotients.","To shed light on these concepts, we introduce the novel notion of an {\\it arithmetic variety of affine semigroups}, accompanied by illuminating examples that showcase its power."],"url":"http://arxiv.org/abs/2402.09159v1","category":"math.AC"}
{"created":"2024-02-14 13:08:25","title":"Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis","abstract":"In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language model, we performed adaptive training to develop a model specialized for the psychological domain. We assessed our model's effectiveness across four public benchmarks, where it not only surpassed the performance of standard pre-trained models but also showed a inclination for making psychologically relevant predictions. Due to concerns regarding data privacy, the dataset will not be made publicly available. However, we have made the pre-trained models and codes publicly accessible to the community via: https://github.com/zwzzzQAQ/Chinese-MentalBERT.","sentences":["In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings.","This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations.","There is a recognized need for models capable of efficient analysis.","While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology.","To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries.","To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism.","Building on an existing Chinese language model, we performed adaptive training to develop a model specialized for the psychological domain.","We assessed our model's effectiveness across four public benchmarks, where it not only surpassed the performance of standard pre-trained models but also showed a inclination for making psychologically relevant predictions.","Due to concerns regarding data privacy, the dataset will not be made publicly available.","However, we have made the pre-trained models and codes publicly accessible to the community via: https://github.com/zwzzzQAQ/Chinese-MentalBERT."],"url":"http://arxiv.org/abs/2402.09151v1","category":"cs.CL"}
{"created":"2024-02-14 12:57:59","title":"Static, Cylindrically Symmetric Spacetimes in the Coincident f(Q) Gravity","abstract":"In this paper we consider a static, cylindrically symmetric spacetime with coincident f(Q) gravity. Since the field equation of this spacetime in symmetric teleparallel gravity is suitable for choosing the function of f(Q) in the form of power series and exponential forms, perfect fluid solutions of these forms are discussed. Energy densities, directional pressures and energy conditions are plotted and analysed for a few different metric potentials. Although cosmic strings violate all energy conditions in both f(Q) functions, the Levi-Civita solution violates all energy conditions in the power law function of f(Q), but for exponential f(Q) gravity they are satisfied in small regions.","sentences":["In this paper we consider a static, cylindrically symmetric spacetime with coincident f(Q) gravity.","Since the field equation of this spacetime in symmetric teleparallel gravity is suitable for choosing the function of f(Q) in the form of power series and exponential forms, perfect fluid solutions of these forms are discussed.","Energy densities, directional pressures and energy conditions are plotted and analysed for a few different metric potentials.","Although cosmic strings violate all energy conditions in both f(Q) functions, the Levi-Civita solution violates all energy conditions in the power law function of f(Q), but for exponential f(Q) gravity they are satisfied in small regions."],"url":"http://arxiv.org/abs/2402.09149v1","category":"gr-qc"}
{"created":"2024-02-14 12:56:58","title":"Into the Unknown: Self-Learning Large Language Models","abstract":"We address the main problem of self-learning LLM: the question of what to learn. We propose a self-learning LLM framework that enables an LLM to independently learn previously unknown knowledge through self-assessment of their own hallucinations. Using the hallucination score, we introduce a new concept of Points in The Unknown (PiUs), along with one extrinsic and three intrinsic methods for automatic PiUs identification. It facilitates the creation of a self-learning loop that focuses exclusively on the knowledge gap in Points in The Unknown, resulting in a reduced hallucination score. We also developed evaluation metrics for gauging an LLM's self-learning capability. Our experiments revealed that 7B-Mistral models that have been finetuned or aligned are capable of self-learning considerably well. Our self-learning concept allows more efficient LLM updates and opens new perspectives for knowledge exchange. It may also increase public trust in AI.","sentences":["We address the main problem of self-learning LLM: the question of what to learn.","We propose a self-learning LLM framework that enables an LLM to independently learn previously unknown knowledge through self-assessment of their own hallucinations.","Using the hallucination score, we introduce a new concept of Points in The Unknown (PiUs), along with one extrinsic and three intrinsic methods for automatic PiUs identification.","It facilitates the creation of a self-learning loop that focuses exclusively on the knowledge gap in Points in The Unknown, resulting in a reduced hallucination score.","We also developed evaluation metrics for gauging an LLM's self-learning capability.","Our experiments revealed that 7B-Mistral models that have been finetuned or aligned are capable of self-learning considerably well.","Our self-learning concept allows more efficient LLM updates and opens new perspectives for knowledge exchange.","It may also increase public trust in AI."],"url":"http://arxiv.org/abs/2402.09147v1","category":"cs.AI"}
{"created":"2024-02-14 12:51:02","title":"A comparison of next-generation turbulence profiling instruments at Paranal","abstract":"A six-night optical turbulence monitoring campaign has been carried at Cerro Paranal observatory in February and March, 2023 to facilitate the development and characterisation of two novel atmospheric site monitoring instruments - the ring-image next generation scintillation sensor (RINGSS) and 24-hour Shack Hartmann image motion monitor (24hSHIMM) in the context of providing optical turbulence monitoring support for upcoming 20-40m telescopes. Alongside these two instruments, the well-characterised Stereo-SCIDAR and 2016-MASS-DIMM were operated throughout the campaign to provide data for comparison. All instruments obtain estimates of optical turbulence profiles through statistical analysis of intensity and wavefront angle-of-arrival fluctuations from observations of stars. Contemporaneous measurements of the integrated turbulence parameters are compared and the ratios, bias, unbiased root mean square error and correlation of results from each instrument assessed. Strong agreement was observed in measurements of seeing, free atmosphere seeing and coherence time. Less correlation is seen for isoplanatic angle, although the median values agree well. Median turbulence parameters are further compared against long-term monitoring data from Paranal instruments. Profiles from the three small-telescope instruments are compared with the 100-layer profile from the stereo-SCIDAR. It is found that the RINGSS and SHIMM offer improved accuracy in characterisation of the vertical optical turbulence profile over the MASS-DIMM. Finally, the first results of continuous optical turbulence monitoring at Paranal are presented which show a strong diurnal variation and predictable trend in the seeing. A value of 2.65\" is found for the median daytime seeing.","sentences":["A six-night optical turbulence monitoring campaign has been carried at Cerro Paranal observatory in February and March, 2023 to facilitate the development and characterisation of two novel atmospheric site monitoring instruments - the ring-image next generation scintillation sensor (RINGSS) and 24-hour Shack Hartmann image motion monitor (24hSHIMM) in the context of providing optical turbulence monitoring support for upcoming 20-40m telescopes.","Alongside these two instruments, the well-characterised Stereo-SCIDAR and 2016-MASS-DIMM were operated throughout the campaign to provide data for comparison.","All instruments obtain estimates of optical turbulence profiles through statistical analysis of intensity and wavefront angle-of-arrival fluctuations from observations of stars.","Contemporaneous measurements of the integrated turbulence parameters are compared and the ratios, bias, unbiased root mean square error and correlation of results from each instrument assessed.","Strong agreement was observed in measurements of seeing, free atmosphere seeing and coherence time.","Less correlation is seen for isoplanatic angle, although the median values agree well.","Median turbulence parameters are further compared against long-term monitoring data from Paranal instruments.","Profiles from the three small-telescope instruments are compared with the 100-layer profile from the stereo-SCIDAR.","It is found that the RINGSS and SHIMM offer improved accuracy in characterisation of the vertical optical turbulence profile over the MASS-DIMM.","Finally, the first results of continuous optical turbulence monitoring at Paranal are presented which show a strong diurnal variation and predictable trend in the seeing.","A value of 2.65\" is found for the median daytime seeing."],"url":"http://arxiv.org/abs/2402.09144v1","category":"astro-ph.IM"}
{"created":"2024-02-14 12:48:27","title":"On groups and fields interpretable in differentially closed valued fields and in various $\\mathrm{NTP_2}$ fields","abstract":"This paper aims at developing model-theoretic tools to study interpretable fields and definably amenable groups, mainly in $\\mathrm{NIP}$ or $\\mathrm{NTP_2}$ settings. An abstract theorem constructing definable group homomorphisms from generic data is proved. It relies heavily on a stabilizer theorem of Montenegro, Onshuus and Simon. The main application is a structure theorem for definably amenable groups that are interpretable in algebraically bounded perfect $\\mathrm{NTP_2}$ fields with bounded Galois group (under some mild assumption on the imaginaries involved), or in algebraically bounded theories of (differential) NIP fields. These imply a classification of the fields interpretable in differentially closed valued fields, and structure theorems for fields interpretable in finitely ramified henselian valued fields of characteristic $0$, or in NIP algebraically bounded differential fields.","sentences":["This paper aims at developing model-theoretic tools to study interpretable fields and definably amenable groups, mainly in $\\mathrm{NIP}$ or $\\mathrm{NTP_2}$ settings.","An abstract theorem constructing definable group homomorphisms from generic data is proved.","It relies heavily on a stabilizer theorem of Montenegro, Onshuus and Simon.","The main application is a structure theorem for definably amenable groups that are interpretable in algebraically bounded perfect $\\mathrm{NTP_2}$ fields with bounded Galois group (under some mild assumption on the imaginaries involved), or in algebraically bounded theories of (differential) NIP fields.","These imply a classification of the fields interpretable in differentially closed valued fields, and structure theorems for fields interpretable in finitely ramified henselian valued fields of characteristic $0$, or in NIP algebraically bounded differential fields."],"url":"http://arxiv.org/abs/2402.09143v1","category":"math.LO"}
{"created":"2024-02-14 12:41:09","title":"Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies","abstract":"This study conducts a thorough evaluation of text augmentation techniques across a variety of datasets and natural language processing (NLP) tasks to address the lack of reliable, generalized evidence for these methods. It examines the effectiveness of these techniques in augmenting training sets to improve performance in tasks such as topic classification, sentiment analysis, and offensive language detection. The research emphasizes not only the augmentation methods, but also the strategic order in which real and augmented instances are introduced during training. A major contribution is the development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for augmented datasets, which represents a novel approach in the field. Results show that specific augmentation methods, especially when integrated with MCCL, significantly outperform traditional training approaches in NLP model performance. These results underscore the need for careful selection of augmentation techniques and sequencing strategies to optimize the balance between speed and quality improvement in various NLP tasks. The study concludes that the use of augmentation methods, especially in conjunction with MCCL, leads to improved results in various classification tasks, providing a foundation for future advances in text augmentation strategies in NLP.","sentences":["This study conducts a thorough evaluation of text augmentation techniques across a variety of datasets and natural language processing (NLP) tasks to address the lack of reliable, generalized evidence for these methods.","It examines the effectiveness of these techniques in augmenting training sets to improve performance in tasks such as topic classification, sentiment analysis, and offensive language detection.","The research emphasizes not only the augmentation methods, but also the strategic order in which real and augmented instances are introduced during training.","A major contribution is the development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for augmented datasets, which represents a novel approach in the field.","Results show that specific augmentation methods, especially when integrated with MCCL, significantly outperform traditional training approaches in NLP model performance.","These results underscore the need for careful selection of augmentation techniques and sequencing strategies to optimize the balance between speed and quality improvement in various NLP tasks.","The study concludes that the use of augmentation methods, especially in conjunction with MCCL, leads to improved results in various classification tasks, providing a foundation for future advances in text augmentation strategies in NLP."],"url":"http://arxiv.org/abs/2402.09141v1","category":"cs.CL"}
{"created":"2024-02-14 12:38:04","title":"Semi-Supervised Diffusion Model for Brain Age Prediction","abstract":"Brain age prediction models have succeeded in predicting clinical outcomes in neurodegenerative diseases, but can struggle with tasks involving faster progressing diseases and low quality data. To enhance their performance, we employ a semi-supervised diffusion model, obtaining a 0.83(p<0.01) correlation between chronological and predicted age on low quality T1w MR images. This was competitive with state-of-the-art non-generative methods. Furthermore, the predictions produced by our model were significantly associated with survival length (r=0.24, p<0.05) in Amyotrophic Lateral Sclerosis. Thus, our approach demonstrates the value of diffusion-based architectures for the task of brain age prediction.","sentences":["Brain age prediction models have succeeded in predicting clinical outcomes in neurodegenerative diseases, but can struggle with tasks involving faster progressing diseases and low quality data.","To enhance their performance, we employ a semi-supervised diffusion model, obtaining a 0.83(p<0.01) correlation between chronological and predicted age on low quality T1w MR images.","This was competitive with state-of-the-art non-generative methods.","Furthermore, the predictions produced by our model were significantly associated with survival length (r=0.24, p<0.05) in Amyotrophic Lateral Sclerosis.","Thus, our approach demonstrates the value of diffusion-based architectures for the task of brain age prediction."],"url":"http://arxiv.org/abs/2402.09137v1","category":"eess.IV"}
{"created":"2024-02-14 12:34:58","title":"DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning","abstract":"Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.","sentences":["Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks.","Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs.","In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation.","It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability.","Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work.","Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs.","(2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it."],"url":"http://arxiv.org/abs/2402.09136v1","category":"cs.CL"}
{"created":"2024-02-14 12:34:38","title":"Unconventional Computing based on Four Wave Mixing in Highly Nonlinear Waveguides","abstract":"In this work we numerically analyze a photonic unconventional accelerator based on the four-wave mixing effect in highly nonlinear waveguides. The proposed scheme can act as a fully analogue system for nonlinear signal processing directly in the optical domain. By exploiting the rich Kerr-induced nonlinearities, multiple nonlinear transformations of an input signal can be generated and used for solving complex nonlinear tasks. We first evaluate the performance of our scheme in the Santa-Fe chaotic time-series prediction. The true power of this processor is revealed in the all-optical nonlinearity compensation in an optical communication scenario where we provide results superior to those offered by strong machine learning algorithms with reduced power consumption and computational complexity. Finally, we showcase how the FWM module can be used as a reconfigurable nonlinear activation module being capable of reproducing characteristic functions such as sigmoid or rectified linear unit.","sentences":["In this work we numerically analyze a photonic unconventional accelerator based on the four-wave mixing effect in highly nonlinear waveguides.","The proposed scheme can act as a fully analogue system for nonlinear signal processing directly in the optical domain.","By exploiting the rich Kerr-induced nonlinearities, multiple nonlinear transformations of an input signal can be generated and used for solving complex nonlinear tasks.","We first evaluate the performance of our scheme in the Santa-Fe chaotic time-series prediction.","The true power of this processor is revealed in the all-optical nonlinearity compensation in an optical communication scenario where we provide results superior to those offered by strong machine learning algorithms with reduced power consumption and computational complexity.","Finally, we showcase how the FWM module can be used as a reconfigurable nonlinear activation module being capable of reproducing characteristic functions such as sigmoid or rectified linear unit."],"url":"http://arxiv.org/abs/2402.09135v1","category":"physics.optics"}
{"created":"2024-02-14 12:28:38","title":"Exploring the Adversarial Capabilities of Large Language Models","abstract":"The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)autonomous systems relying on LLMs, highlighting potential challenges in their interaction with existing systems and safety measures.","sentences":["The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research.","While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored.","Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks.","More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails.","Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems.","Our findings carry significant implications for (semi-)autonomous systems relying on LLMs, highlighting potential challenges in their interaction with existing systems and safety measures."],"url":"http://arxiv.org/abs/2402.09132v1","category":"cs.AI"}
{"created":"2024-02-14 12:28:26","title":"General penny graphs are at most 43/18-dense","abstract":"We prove that among $n$ points in the plane in general position, the shortest distance occurs at most $43n/18$ times, improving upon the upper bound of $17n/7$ obtained by T\\'oth in 1997.","sentences":["We prove that among $n$ points in the plane in general position, the shortest distance occurs at most $43n/18$ times, improving upon the upper bound of $17n/7$ obtained by T\\'oth in 1997."],"url":"http://arxiv.org/abs/2402.09131v1","category":"math.CO"}
{"created":"2024-02-14 12:27:54","title":"Optimal Automated Market Makers: Differentiable Economics and Strong Duality","abstract":"The role of a market maker is to simultaneously offer to buy and sell quantities of goods, often a financial asset such as a share, at specified prices. An automated market maker (AMM) is a mechanism that offers to trade according to some predetermined schedule; the best choice of this schedule depends on the market maker's goals. The literature on the design of AMMs has mainly focused on prediction markets with the goal of information elicitation. More recent work motivated by DeFi has focused instead on the goal of profit maximization, but considering only a single type of good (traded with a numeraire), including under adverse selection (Milionis et al. 2022). Optimal market making in the presence of multiple goods, including the possibility of complex bundling behavior, is not well understood. In this paper, we show that finding an optimal market maker is dual to an optimal transport problem, with specific geometric constraints on the transport plan in the dual. We show that optimal mechanisms for multiple goods and under adverse selection can take advantage of bundling, both improved prices for bundled purchases and sales as well as sometimes accepting payment \"in kind.\" We present conjectures of optimal mechanisms in additional settings which show further complex behavior. From a methodological perspective, we make essential use of the tools of differentiable economics to generate conjectures of optimal mechanisms, and give a proof-of-concept for the use of such tools in guiding theoretical investigations.","sentences":["The role of a market maker is to simultaneously offer to buy and sell quantities of goods, often a financial asset such as a share, at specified prices.","An automated market maker (AMM) is a mechanism that offers to trade according to some predetermined schedule; the best choice of this schedule depends on the market maker's goals.","The literature on the design of AMMs has mainly focused on prediction markets with the goal of information elicitation.","More recent work motivated by DeFi has focused instead on the goal of profit maximization, but considering only a single type of good (traded with a numeraire), including under adverse selection (Milionis et al. 2022).","Optimal market making in the presence of multiple goods, including the possibility of complex bundling behavior, is not well understood.","In this paper, we show that finding an optimal market maker is dual to an optimal transport problem, with specific geometric constraints on the transport plan in the dual.","We show that optimal mechanisms for multiple goods and under adverse selection can take advantage of bundling, both improved prices for bundled purchases and sales as well as sometimes accepting payment \"in kind.\"","We present conjectures of optimal mechanisms in additional settings which show further complex behavior.","From a methodological perspective, we make essential use of the tools of differentiable economics to generate conjectures of optimal mechanisms, and give a proof-of-concept for the use of such tools in guiding theoretical investigations."],"url":"http://arxiv.org/abs/2402.09129v1","category":"cs.GT"}
{"created":"2024-02-14 12:27:46","title":"Machine learning assisted prediction of organic salt structure properties","abstract":"We demonstrate a machine learning-based approach which predicts the properties of crystal structures following relaxation based on the unrelaxed structure. Use of crystal graph singular values reduces the number of features required to describe a crystal by more than an order of magnitude compared to the full crystal graph representation. We construct machine learning models using the crystal graph singular value representations in order to predict the volume, enthalpy per atom, and metal versus semiconducting phase of DFT-relaxed organic salt crystals based on randomly generated unrelaxed crystal structures. Initial base models are trained to relate 89,949 randomly generated structures of salts formed by varying ratios of 1,3,5-triazine and HCl with the corresponding volumes, enthalpies per atom, and phase of the DFT-relaxed structures. We further demonstrate that the base model is able to extrapolate to new chemical systems with the inclusion of 2,000 to 10,000 crystal structures from the new system. After training a single model with a large number of data points, extension can be done at significantly lower cost. The constructed machine learning models can be used to rapidly screen large sets of randomly generated organic salt crystal structures and efficiently downselect the structures most likely to be experimentally realizable. The models can be used either as a stand-alone crystal structure predictor or incorporated into more sophisticated workflows as a filtering step.","sentences":["We demonstrate a machine learning-based approach which predicts the properties of crystal structures following relaxation based on the unrelaxed structure.","Use of crystal graph singular values reduces the number of features required to describe a crystal by more than an order of magnitude compared to the full crystal graph representation.","We construct machine learning models using the crystal graph singular value representations in order to predict the volume, enthalpy per atom, and metal versus semiconducting phase of DFT-relaxed organic salt crystals based on randomly generated unrelaxed crystal structures.","Initial base models are trained to relate 89,949 randomly generated structures of salts formed by varying ratios of 1,3,5-triazine and HCl with the corresponding volumes, enthalpies per atom, and phase of the DFT-relaxed structures.","We further demonstrate that the base model is able to extrapolate to new chemical systems with the inclusion of 2,000 to 10,000 crystal structures from the new system.","After training a single model with a large number of data points, extension can be done at significantly lower cost.","The constructed machine learning models can be used to rapidly screen large sets of randomly generated organic salt crystal structures and efficiently downselect the structures most likely to be experimentally realizable.","The models can be used either as a stand-alone crystal structure predictor or incorporated into more sophisticated workflows as a filtering step."],"url":"http://arxiv.org/abs/2402.09128v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-14 12:24:21","title":"MPIrigen: MPI Code Generation through Domain-Specific Language Models","abstract":"The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context. Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions up to 0.8 accuracy in location and function predictions, and with more than 0.9 accuracy for argument predictions. The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools. The sources of this work are available at our GitHub MPIrigen repository: https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen","sentences":["The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration.","The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored.","This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs.","Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs.","In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models.","Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI.","We call the resulting model as MPIrigen.","We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context.","Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions up to 0.8 accuracy in location and function predictions, and with more than 0.9 accuracy for argument predictions.","The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools.","The sources of this work are available at our GitHub MPIrigen repository: https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen"],"url":"http://arxiv.org/abs/2402.09126v1","category":"cs.DC"}
{"created":"2024-02-14 12:23:09","title":"Database for the meta-analysis of the social cost of carbon (v2024.0)","abstract":"A new version of the database for the meta-analysis of estimates of the social cost of carbon is presented. New records were added, and new fields on the impact of climate change and the shape of the welfare function. The database was extended to co-author and citation networks.","sentences":["A new version of the database for the meta-analysis of estimates of the social cost of carbon is presented.","New records were added, and new fields on the impact of climate change and the shape of the welfare function.","The database was extended to co-author and citation networks."],"url":"http://arxiv.org/abs/2402.09125v1","category":"econ.GN"}
{"created":"2024-02-14 12:15:05","title":"Classical and generalized solutions of an alarm-taxis model","abstract":"In bounded, spatially two-dimensional domains, the system \\begin{equation*}   \\left\\lbrace\\begin{alignedat}{3}   u_t &= d_1 \\Delta u && &&+ u(\\lambda_1 - \\mu_1 u - a_1 v - a_2 w), \\\\   v_t &= d_2 \\Delta v &&- \\xi \\nabla \\cdot (v \\nabla u) &&+ v(\\lambda_2 - \\mu_2 v + b_1 u - a_3 w),\\\\   w_t &= d_3 \\Delta w &&- \\chi \\nabla \\cdot (w \\nabla (uv)) &&+ w(\\lambda_3 - \\mu_3 w + b_2 u + b_3 v),   \\end{alignedat}\\right. \\end{equation*} complemented with initial and homogeneous Neumann boundary conditions, models the interaction between prey (with density $u$), predator (with density $v$) and superpredator (with density $w$), which preys on both other populations. Apart from random motion and prey-tactical behavior of the primary predator, the key aspect of this system is that the secondary predator reacts to alarm calls of the prey, issued by the latter whenever attacked by the primary predator.   We first show in the pure alarm-taxis model, i.e. if $\\xi = 0$, that global classical solutions exist.   For the full model (with $\\xi > 0$), the taxis terms and the presence of the term $-a_2 uw$ in the first equation apparently hinder certain bootstrap procedures, meaning that the available regularity information is rather limited. Nonetheless, we are able to obtain global generalized solutions. An important technical challenge is to guarantee strong convergence of (weighted) gradients of the first two solution components in order to conclude that approximate solutions converge to a generalized solution of the limit problem.","sentences":["In bounded, spatially two-dimensional domains, the system \\begin{equation*}   \\left\\lbrace\\begin{alignedat}{3}   u_t &= d_1 \\Delta u && &&+ u(\\lambda_1 - \\mu_1 u - a_1 v - a_2 w), \\\\   v_t &= d_2 \\Delta v &&- \\xi \\nabla \\cdot (v \\nabla u) &&+ v(\\lambda_2 - \\mu_2 v + b_1 u - a_3","w),\\\\   w_t &= d_3 \\Delta w &&- \\chi \\nabla \\cdot (w \\nabla (uv)) &&+ w(\\lambda_3 - \\mu_3 w + b_2 u + b_3 v),   \\end{alignedat}\\right.","\\end{equation*} complemented with initial and homogeneous Neumann boundary conditions, models the interaction between prey (with density $u$), predator (with density $v$) and superpredator (with density $w$), which preys on both other populations.","Apart from random motion and prey-tactical behavior of the primary predator, the key aspect of this system is that the secondary predator reacts to alarm calls of the prey, issued by the latter whenever attacked by the primary predator.   ","We first show in the pure alarm-taxis model, i.e. if $\\xi = 0$, that global classical solutions exist.   ","For the full model (with $\\xi > 0$), the taxis terms and the presence of the term $-a_2 uw$ in the first equation apparently hinder certain bootstrap procedures, meaning that the available regularity information is rather limited.","Nonetheless, we are able to obtain global generalized solutions.","An important technical challenge is to guarantee strong convergence of (weighted) gradients of the first two solution components in order to conclude that approximate solutions converge to a generalized solution of the limit problem."],"url":"http://arxiv.org/abs/2402.09119v1","category":"math.AP"}
{"created":"2024-02-14 12:03:55","title":"The Hausdorff-integral on h-measure spaces","abstract":"We are going to widen the scope of the previously defined Hausdorff-integral in the sense that we develop the theory of the integral on some naturally generalized measure spaces. In all our intentions, we follow the same attitude that we had in our previous investigations, i.e. we work in the realm of Hausdorff dimension and measure.","sentences":["We are going to widen the scope of the previously defined Hausdorff-integral in the sense that we develop the theory of the integral on some naturally generalized measure spaces.","In all our intentions, we follow the same attitude that we had in our previous investigations, i.e. we work in the realm of Hausdorff dimension and measure."],"url":"http://arxiv.org/abs/2402.09118v1","category":"math.CA"}
{"created":"2024-02-14 11:59:30","title":"Deterministic identification over channels with finite output: a dimensional perspective on superlinear rates","abstract":"Following initial work by JaJa and Ahlswede/Cai, and inspired by a recent renewed surge in interest in deterministic identification via noisy channels, we consider the problem in its generality for memoryless channels with finite output, but arbitrary input alphabets.   Such a channel is essentially given by (the closure of) the subset of its output distributions in the probability simplex. Our main findings are that the maximum number of messages thus identifiable scales super-exponentially as $2^{R\\,n\\log n}$ with the block length $n$, and that the optimal rate $R$ is upper and lower bounded in terms of the covering (aka Minkowski, or Kolmogorov, or entropy) dimension $d$ of the output set: $\\frac14 d \\leq R \\leq d$. Leading up to the general case, we treat the important special case of the so-called Bernoulli channel with input alphabet $[0;1]$ and binary output, which has $d=1$, to gain intuition. Along the way, we show a certain Hypothesis Testing Lemma (generalising an earlier insight of Ahlswede regarding the intersection of typical sets) that implies that for the construction of a deterministic identification code, it is sufficient to ensure pairwise reliable distinguishability of the output distributions.   These results are then shown to generalise directly to classical-quantum channels with finite-dimensional output quantum system (but arbitrary input alphabet), and in particular to quantum channels on finite-dimensional quantum systems under the constraint that the identification code can only use tensor product inputs.","sentences":["Following initial work by JaJa and Ahlswede/Cai, and inspired by a recent renewed surge in interest in deterministic identification via noisy channels, we consider the problem in its generality for memoryless channels with finite output, but arbitrary input alphabets.   ","Such a channel is essentially given by (the closure of) the subset of its output distributions in the probability simplex.","Our main findings are that the maximum number of messages thus identifiable scales super-exponentially as $2^{R\\,n\\log n}$ with the block length $n$, and that the optimal rate $R$ is upper and lower bounded in terms of the covering (aka Minkowski, or Kolmogorov, or entropy) dimension $d$ of the output set: $\\frac14 d \\leq R \\leq d$. Leading up to the general case, we treat the important special case of the so-called Bernoulli channel with input alphabet $[0;1]$ and binary output, which has $d=1$, to gain intuition.","Along the way, we show a certain Hypothesis Testing Lemma (generalising an earlier insight of Ahlswede regarding the intersection of typical sets) that implies that for the construction of a deterministic identification code, it is sufficient to ensure pairwise reliable distinguishability of the output distributions.   ","These results are then shown to generalise directly to classical-quantum channels with finite-dimensional output quantum system (but arbitrary input alphabet), and in particular to quantum channels on finite-dimensional quantum systems under the constraint that the identification code can only use tensor product inputs."],"url":"http://arxiv.org/abs/2402.09117v1","category":"cs.IT"}
{"created":"2024-02-14 11:59:23","title":"Zero-entropy encoders and simultaneous decoders in identification via quantum channels","abstract":"Motivated by deterministic identification via (classical) channels, where the encoder is not allowed to use randomization, we revisit the problem of identification via quantum channels but now with the additional restriction that the message encoding must use pure quantum states, rather than general mixed states. Together with the previously considered distinction between simultaneous and general decoders, this suggests a two-dimensional spectrum of different identification capacities, whose behaviour could a priori be very different.   We demonstrate two new results as our main findings: first, we show that all four combinations (pure/mixed encoder, simultaneous/general decoder) have a double-exponentially growing code size, and that indeed the corresponding identification capacities are lower bounded by the classical transmission capacity for a general quantum channel, which is given by the Holevo-Schumacher-Westmoreland Theorem. Secondly, we show that the simultaneous identification capacity of a quantum channel equals the simultaneous identification capacity with pure state encodings, thus leaving three linearly ordered identification capacities. By considering some simple examples, we finally show that these three are all different: general identification capacity can be larger than pure-state-encoded identification capacity, which in turn can be larger than pure-state-encoded simultaneous identification capacity.","sentences":["Motivated by deterministic identification via (classical) channels, where the encoder is not allowed to use randomization, we revisit the problem of identification via quantum channels but now with the additional restriction that the message encoding must use pure quantum states, rather than general mixed states.","Together with the previously considered distinction between simultaneous and general decoders, this suggests a two-dimensional spectrum of different identification capacities, whose behaviour could a priori be very different.   ","We demonstrate two new results as our main findings: first, we show that all four combinations (pure/mixed encoder, simultaneous/general decoder) have a double-exponentially growing code size, and that indeed the corresponding identification capacities are lower bounded by the classical transmission capacity for a general quantum channel, which is given by the Holevo-Schumacher-Westmoreland Theorem.","Secondly, we show that the simultaneous identification capacity of a quantum channel equals the simultaneous identification capacity with pure state encodings, thus leaving three linearly ordered identification capacities.","By considering some simple examples, we finally show that these three are all different: general identification capacity can be larger than pure-state-encoded identification capacity, which in turn can be larger than pure-state-encoded simultaneous identification capacity."],"url":"http://arxiv.org/abs/2402.09116v1","category":"quant-ph"}
{"created":"2024-02-14 11:57:13","title":"Integrated Topology and Traffic Engineering for Reconfigurable Datacenter Networks","abstract":"The state-of-the-art topologies of datacenter networks are fixed, based on electrical switching technology, and by now, we understand their throughput and cost well. For the past years, researchers have been developing novel optical switching technologies that enable the emergence of reconfigurable datacenter networks (RDCNs) that support dynamic psychical topologies. The art of network design of dynamic topologies, i.e., 'Topology Engineering,' is still in its infancy. Different designs offer distinct advantages, such as faster switch reconfiguration times or demand-aware topologies, and to date, it is yet unclear what design maximizes the throughput.   This paper aims to improve our analytical understanding and formally studies the throughput of reconfigurable networks by presenting a general and unifying model for dynamic networks and their topology and traffic engineering. We use our model to study demand-oblivious and demand-aware systems and prove new upper bounds for the throughput of a system as a function of its topology and traffic schedules.   Next, we offer a novel system design that combines both demand-oblivious and demand-aware schedules, and we prove its throughput supremacy under a large family of demand matrices. We evaluate our design numerically for sparse and dense traffic and show that our approach can outperform other designs by up to 25% using common network parameters.","sentences":["The state-of-the-art topologies of datacenter networks are fixed, based on electrical switching technology, and by now, we understand their throughput and cost well.","For the past years, researchers have been developing novel optical switching technologies that enable the emergence of reconfigurable datacenter networks (RDCNs) that support dynamic psychical topologies.","The art of network design of dynamic topologies, i.e., 'Topology Engineering,' is still in its infancy.","Different designs offer distinct advantages, such as faster switch reconfiguration times or demand-aware topologies, and to date, it is yet unclear what design maximizes the throughput.   ","This paper aims to improve our analytical understanding and formally studies the throughput of reconfigurable networks by presenting a general and unifying model for dynamic networks and their topology and traffic engineering.","We use our model to study demand-oblivious and demand-aware systems and prove new upper bounds for the throughput of a system as a function of its topology and traffic schedules.   ","Next, we offer a novel system design that combines both demand-oblivious and demand-aware schedules, and we prove its throughput supremacy under a large family of demand matrices.","We evaluate our design numerically for sparse and dense traffic and show that our approach can outperform other designs by up to 25% using common network parameters."],"url":"http://arxiv.org/abs/2402.09115v1","category":"cs.NI"}
{"created":"2024-02-14 11:55:16","title":"Regression graphs and sparsity-inducing reparametrizations","abstract":"Motivated by the important statistical role of sparsity, the paper uncovers four reparametrizations for covariance matrices in which sparsity is associated with conditional independence graphs in a notional Gaussian model. The intimate relationship between the Iwasawa decomposition of the general linear group and the open cone of positive definite matrices allows a unifying perspective. Specifically, the positive definite cone can be reconstructed without loss or redundancy from the exponential map applied to four Lie subalgebras determined by the Iwasawa decomposition of the general linear group. This accords geometric interpretations to the reparametrizations and the corresponding notion of sparsity. Conditions that ensure legitimacy of the reparametrizations for statistical models are identified. While the focus of this work is on understanding population-level structure, there are strong methodological implications. In particular, since the population-level sparsity manifests in a vector space, imposition of sparsity on relevant sample quantities produces a covariance estimate that respects the positive definite cone constraint.","sentences":["Motivated by the important statistical role of sparsity, the paper uncovers four reparametrizations for covariance matrices in which sparsity is associated with conditional independence graphs in a notional Gaussian model.","The intimate relationship between the Iwasawa decomposition of the general linear group and the open cone of positive definite matrices allows a unifying perspective.","Specifically, the positive definite cone can be reconstructed without loss or redundancy from the exponential map applied to four Lie subalgebras determined by the Iwasawa decomposition of the general linear group.","This accords geometric interpretations to the reparametrizations and the corresponding notion of sparsity.","Conditions that ensure legitimacy of the reparametrizations for statistical models are identified.","While the focus of this work is on understanding population-level structure, there are strong methodological implications.","In particular, since the population-level sparsity manifests in a vector space, imposition of sparsity on relevant sample quantities produces a covariance estimate that respects the positive definite cone constraint."],"url":"http://arxiv.org/abs/2402.09112v1","category":"math.ST"}
{"created":"2024-02-14 11:54:20","title":"On-shell Bootstrap for n-gluons and gravitons scattering in (A)dS, Unitarity and Soft limit","abstract":"We propose an algorithm to recursively bootstrap $n$-point gluon and graviton Mellin-Momentum amplitudes in (A)dS spacetime using only three-point amplitude. We discover that gluon amplitudes are simply determined by factorization for $n\\geq 5$. The same principle applies to $n$-point graviton amplitudes, but additional constraints such as flat space and soft limits are needed to fix contact terms. Furthermore, we establish a mapping from $n$-point Mellin-Momentum amplitudes to $n$-point cosmological correlators. We efficiently compute explicit examples up to five points. This leads to the first five-graviton amplitude in $AdS_{d+1}$.","sentences":["We propose an algorithm to recursively bootstrap $n$-point gluon and graviton Mellin-Momentum amplitudes in (A)dS spacetime using only three-point amplitude.","We discover that gluon amplitudes are simply determined by factorization for $n\\geq 5$.","The same principle applies to $n$-point graviton amplitudes, but additional constraints such as flat space and soft limits are needed to fix contact terms.","Furthermore, we establish a mapping from $n$-point Mellin-Momentum amplitudes to $n$-point cosmological correlators.","We efficiently compute explicit examples up to five points.","This leads to the first five-graviton amplitude in $AdS_{d+1}$."],"url":"http://arxiv.org/abs/2402.09111v1","category":"hep-th"}
{"created":"2024-02-14 11:47:19","title":"Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks","abstract":"Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers. We demonstrate that our approach can achieve high classification accuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\\%$). We estimate that the proposed SC approach can lead to over $6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design through an FPGA implementation, which is shown to achieve $48\\times$ lower latency as compared to a GPU implementation, while consuming $15\\times$ less power.","sentences":["Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency.","Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient.","In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers.","We demonstrate that our approach can achieve high classification accuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\\%$).","We estimate that the proposed SC approach can lead to over $6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory access costs for a digital CMOS-based ASIC design.","We experimentally validate our stochastic attention block design through an FPGA implementation, which is shown to achieve $48\\times$ lower latency as compared to a GPU implementation, while consuming $15\\times$ less power."],"url":"http://arxiv.org/abs/2402.09109v1","category":"cs.AR"}
{"created":"2024-02-14 11:31:36","title":"Generalized Double Affine Hecke Algebra for Double Torus","abstract":"We propose a generalization of the double affine Hecke algebra of type-C C1 at specific parameters by introducing a ``Heegaard dual'' of the Hecke operators. Shown is a relationship with the skein algebra on double torus. We give automorphisms of the algebra associated with the Dehn twists on the double torus.","sentences":["We propose a generalization of the double affine Hecke algebra of type-C C1 at specific parameters by introducing a ``Heegaard dual'' of the Hecke operators.","Shown is a relationship with the skein algebra on double torus.","We give automorphisms of the algebra associated with the Dehn twists on the double torus."],"url":"http://arxiv.org/abs/2402.09106v1","category":"math.QA"}
{"created":"2024-02-14 11:22:20","title":"DestripeCycleGAN: Stripe Simulation CycleGAN for Unsupervised Infrared Image Destriping","abstract":"CycleGAN has been proven to be an advanced approach for unsupervised image restoration. This framework consists of two generators: a denoising one for inference and an auxiliary one for modeling noise to fulfill cycle-consistency constraints. However, when applied to the infrared destriping task, it becomes challenging for the vanilla auxiliary generator to consistently produce vertical noise under unsupervised constraints. This poses a threat to the effectiveness of the cycle-consistency loss, leading to stripe noise residual in the denoised image. To address the above issue, we present a novel framework for single-frame infrared image destriping, named DestripeCycleGAN. In this model, the conventional auxiliary generator is replaced with a priori stripe generation model (SGM) to introduce vertical stripe noise in the clean data, and the gradient map is employed to re-establish cycle-consistency. Meanwhile, a Haar wavelet background guidance module (HBGM) has been designed to minimize the divergence of background details between the different domains. To preserve vertical edges, a multi-level wavelet U-Net (MWUNet) is proposed as the denoising generator, which utilizes the Haar wavelet transform as the sampler to decline directional information loss. Moreover, it incorporates the group fusion block (GFB) into skip connections to fuse the multi-scale features and build the context of long-distance dependencies. Extensive experiments on real and synthetic data demonstrate that our DestripeCycleGAN surpasses the state-of-the-art methods in terms of visual quality and quantitative evaluation. Our code will be made public at https://github.com/0wuji/DestripeCycleGAN.","sentences":["CycleGAN has been proven to be an advanced approach for unsupervised image restoration.","This framework consists of two generators: a denoising one for inference and an auxiliary one for modeling noise to fulfill cycle-consistency constraints.","However, when applied to the infrared destriping task, it becomes challenging for the vanilla auxiliary generator to consistently produce vertical noise under unsupervised constraints.","This poses a threat to the effectiveness of the cycle-consistency loss, leading to stripe noise residual in the denoised image.","To address the above issue, we present a novel framework for single-frame infrared image destriping, named DestripeCycleGAN.","In this model, the conventional auxiliary generator is replaced with a priori stripe generation model (SGM) to introduce vertical stripe noise in the clean data, and the gradient map is employed to re-establish cycle-consistency.","Meanwhile, a Haar wavelet background guidance module (HBGM) has been designed to minimize the divergence of background details between the different domains.","To preserve vertical edges, a multi-level wavelet U-Net (MWUNet) is proposed as the denoising generator, which utilizes the Haar wavelet transform as the sampler to decline directional information loss.","Moreover, it incorporates the group fusion block (GFB) into skip connections to fuse the multi-scale features and build the context of long-distance dependencies.","Extensive experiments on real and synthetic data demonstrate that our DestripeCycleGAN surpasses the state-of-the-art methods in terms of visual quality and quantitative evaluation.","Our code will be made public at https://github.com/0wuji/DestripeCycleGAN."],"url":"http://arxiv.org/abs/2402.09101v1","category":"eess.IV"}
{"created":"2024-02-14 11:20:47","title":"Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs","abstract":"Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup. This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions. In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence. Our study introduces a network designed for expression-based video inpainting, employing generative adversarial networks (GANs) to handle static and moving occlusions across all frames. By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user's identity consistently across frames. We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs. Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results.","sentences":["Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup.","This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions.","In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence.","Our study introduces a network designed for expression-based video inpainting, employing generative adversarial networks (GANs) to handle static and moving occlusions across all frames.","By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user's identity consistently across frames.","We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs.","Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results."],"url":"http://arxiv.org/abs/2402.09100v1","category":"cs.CV"}
{"created":"2024-02-14 11:20:09","title":"Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective","abstract":"Prior studies on the emergence in large models have primarily focused on how the functional capabilities of large language models (LLMs) scale with model size. Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within LLMs by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process. By introducing the concepts of \"self-organization\" and \"multifractal analysis,\" we explore how neuron interactions dynamically evolve during training, leading to \"emergence,\" mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors. To quantitatively analyze the continuously evolving interactions among neurons in large models during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA). Utilizing NeuroMFA, we conduct a comprehensive examination of the emergent behavior in LLMs through the lens of both model size and training process, paving new avenues for research into the emergence in large models.","sentences":["Prior studies on the emergence in large models have primarily focused on how the functional capabilities of large language models (LLMs) scale with model size.","Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within LLMs by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process.","By introducing the concepts of \"self-organization\" and \"multifractal analysis,\" we explore how neuron interactions dynamically evolve during training, leading to \"emergence,\" mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors.","To quantitatively analyze the continuously evolving interactions among neurons in large models during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA).","Utilizing NeuroMFA, we conduct a comprehensive examination of the emergent behavior in LLMs through the lens of both model size and training process, paving new avenues for research into the emergence in large models."],"url":"http://arxiv.org/abs/2402.09099v1","category":"cs.AI"}
{"created":"2024-02-14 11:18:48","title":"Multiple-output composite quantile regression through an optimal transport lens","abstract":"Composite quantile regression has been used to obtain robust estimators of regression coefficients in linear models with good statistical efficiency. By revealing an intrinsic link between the composite quantile regression loss function and the Wasserstein distance from the residuals to the set of quantiles, we establish a generalization of the composite quantile regression to the multiple-output settings. Theoretical convergence rates of the proposed estimator are derived both under the setting where the additive error possesses only a finite $\\ell$-th moment (for $\\ell > 2$) and where it exhibits a sub-Weibull tail. In doing so, we develop novel techniques for analyzing the M-estimation problem that involves Wasserstein-distance in the loss. Numerical studies confirm the practical effectiveness of our proposed procedure.","sentences":["Composite quantile regression has been used to obtain robust estimators of regression coefficients in linear models with good statistical efficiency.","By revealing an intrinsic link between the composite quantile regression loss function and the Wasserstein distance from the residuals to the set of quantiles, we establish a generalization of the composite quantile regression to the multiple-output settings.","Theoretical convergence rates of the proposed estimator are derived both under the setting where the additive error possesses only a finite $\\ell$-th moment (for $\\ell > 2$) and where it exhibits a sub-Weibull tail.","In doing so, we develop novel techniques for analyzing the M-estimation problem that involves Wasserstein-distance in the loss.","Numerical studies confirm the practical effectiveness of our proposed procedure."],"url":"http://arxiv.org/abs/2402.09098v1","category":"math.ST"}
{"created":"2024-02-14 11:17:14","title":"A Digital Twin prototype for traffic sign recognition of a learning-enabled autonomous vehicle","abstract":"In this paper, we present a novel digital twin prototype for a learning-enabled self-driving vehicle. The primary objective of this digital twin is to perform traffic sign recognition and lane keeping. The digital twin architecture relies on co-simulation and uses the Functional Mock-up Interface and SystemC Transaction Level Modeling standards. The digital twin consists of four clients, i) a vehicle model that is designed in Amesim tool, ii) an environment model developed in Prescan, iii) a lane-keeping controller designed in Robot Operating System, and iv) a perception and speed control module developed in the formal modeling language of BIP (Behavior, Interaction, Priority). These clients interface with the digital twin platform, PAVE360-Veloce System Interconnect (PAVE360-VSI). PAVE360-VSI acts as the co-simulation orchestrator and is responsible for synchronization, interconnection, and data exchange through a server. The server establishes connections among the different clients and also ensures adherence to the Ethernet protocol. We conclude with illustrative digital twin simulations and recommendations for future work.","sentences":["In this paper, we present a novel digital twin prototype for a learning-enabled self-driving vehicle.","The primary objective of this digital twin is to perform traffic sign recognition and lane keeping.","The digital twin architecture relies on co-simulation and uses the Functional Mock-up Interface and SystemC Transaction Level Modeling standards.","The digital twin consists of four clients, i) a vehicle model that is designed in Amesim tool, ii) an environment model developed in Prescan, iii) a lane-keeping controller designed in Robot Operating System, and iv) a perception and speed control module developed in the formal modeling language of BIP (Behavior, Interaction, Priority).","These clients interface with the digital twin platform, PAVE360-Veloce System Interconnect (PAVE360-VSI).","PAVE360-VSI acts as the co-simulation orchestrator and is responsible for synchronization, interconnection, and data exchange through a server.","The server establishes connections among the different clients and also ensures adherence to the Ethernet protocol.","We conclude with illustrative digital twin simulations and recommendations for future work."],"url":"http://arxiv.org/abs/2402.09097v1","category":"cs.RO"}
{"created":"2024-02-14 11:16:59","title":"Ambipolar Ion Pumping with Ratchet Driven Active Membranes","abstract":"In recent years there has been significant progress in the development of artificial ion pumping membranes. Ion pumps based on asymmetric nano-pores have been shown to operate as ionic current rectifiers, thus pumping a net ion flux against a concentration gradient even when driven with unbiased ac signals. However, since ion transport relies on charged nano-pores, it is selective to either cations or anions, and thus cannot pump both cations and anions simultaneously. In this paper, we present a model for an electronically active membrane which is based on a flashing ratchet mechanism. The model includes adjacent electrolyte reservoirs and ion-ion interactions, which were not accounted for in prior similar models, and thus provides a better understanding of the driving mechanism and potential capabilities and limitations. It is shown that unlike most other proposed ion pumps, the ratchet-based ion pump (RBIP) drives both cations and anions in the same direction and up a concentration gradient. This process, referred to as ambipolar ion pumping, is shown to be highly robust for many electrolyte compositions and input signals. The membrane is composed of alternating conductive thin layers (electrodes), separated by insulating layers in an asymmetric design. With insulating layers thickness of 70 and 30 nm and an input signal amplitude of 0.5 V, the device drives a salt flux of 0.03 mol/m^2 s in a mildly saline solution (10 mM). Thus, RBIPs may pave the way for many exciting future applications involving ambipolar ion pumping, most notably for desalination.","sentences":["In recent years there has been significant progress in the development of artificial ion pumping membranes.","Ion pumps based on asymmetric nano-pores have been shown to operate as ionic current rectifiers, thus pumping a net ion flux against a concentration gradient even when driven with unbiased ac signals.","However, since ion transport relies on charged nano-pores, it is selective to either cations or anions, and thus cannot pump both cations and anions simultaneously.","In this paper, we present a model for an electronically active membrane which is based on a flashing ratchet mechanism.","The model includes adjacent electrolyte reservoirs and ion-ion interactions, which were not accounted for in prior similar models, and thus provides a better understanding of the driving mechanism and potential capabilities and limitations.","It is shown that unlike most other proposed ion pumps, the ratchet-based ion pump (RBIP) drives both cations and anions in the same direction and up a concentration gradient.","This process, referred to as ambipolar ion pumping, is shown to be highly robust for many electrolyte compositions and input signals.","The membrane is composed of alternating conductive thin layers (electrodes), separated by insulating layers in an asymmetric design.","With insulating layers thickness of 70 and 30 nm and an input signal amplitude of 0.5 V, the device drives a salt flux of 0.03 mol/m^2 s in a mildly saline solution (10 mM).","Thus, RBIPs may pave the way for many exciting future applications involving ambipolar ion pumping, most notably for desalination."],"url":"http://arxiv.org/abs/2402.09096v1","category":"physics.app-ph"}
{"created":"2024-02-14 11:11:51","title":"Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues","abstract":"With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense strategy and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of ''When unable to attack, defend'' from Sun Tzu's Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. Extensive experimental results show that Puzzler achieves a query success rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than baselines. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines.","sentences":["With the development of LLMs, the security threats of LLMs are getting more and more attention.","Numerous jailbreak attacks have been proposed to assess the security defense of LLMs.","Current jailbreak attacks primarily utilize scenario camouflage techniques.","However their explicitly mention of malicious intent will be easily recognized and defended by LLMs.","In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense strategy and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query.","In addition, inspired by the wisdom of ''When unable to attack, defend'' from Sun Tzu's Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs.","Extensive experimental results show that Puzzler achieves a query success rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than baselines.","Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines."],"url":"http://arxiv.org/abs/2402.09091v1","category":"cs.CR"}
{"created":"2024-02-14 11:08:04","title":"The Vienna Architecture Description Language","abstract":"The Vienna Architecture Description Language (VADL) is a powerful processor description language (PDL) that enables the concise formal specification of processor architectures. By utilizing a single VADL processor specification, the VADL system exhibits the capability to automatically generate a range of artifacts necessary for rapid design space exploration. These include assemblers, compilers, linkers, functional instruction set simulators, cycle-accurate instruction set simulators, synthesizable specifications in a hardware description language, as well as test cases and documentation. One distinctive feature of VADL lies in its separation of the instruction set architecture (ISA) specification and the microarchitecture (MiA) specification. This segregation allows users the flexibility to combine various ISAs with different MiAs, providing a versatile approach to processor design. In contrast to existing PDLs, VADL's MiA specification operates at a higher level of abstraction, enhancing the clarity and simplicity of the design process. Notably, with a single ISA specification, VADL streamlines compiler generation and maintenance by eliminating the need for intricate compiler-specific knowledge. This article introduces VADL, describes the generator techniques in detail and demonstrates the power of the language and the performance of the generators in an empirical evaluation. The evaluation shows the expressiveness and conciseness of VADL and the efficiency of the generated artifacts.","sentences":["The Vienna Architecture Description Language (VADL) is a powerful processor description language (PDL) that enables the concise formal specification of processor architectures.","By utilizing a single VADL processor specification, the VADL system exhibits the capability to automatically generate a range of artifacts necessary for rapid design space exploration.","These include assemblers, compilers, linkers, functional instruction set simulators, cycle-accurate instruction set simulators, synthesizable specifications in a hardware description language, as well as test cases and documentation.","One distinctive feature of VADL lies in its separation of the instruction set architecture (ISA) specification and the microarchitecture (MiA) specification.","This segregation allows users the flexibility to combine various ISAs with different MiAs, providing a versatile approach to processor design.","In contrast to existing PDLs, VADL's MiA specification operates at a higher level of abstraction, enhancing the clarity and simplicity of the design process.","Notably, with a single ISA specification, VADL streamlines compiler generation and maintenance by eliminating the need for intricate compiler-specific knowledge.","This article introduces VADL, describes the generator techniques in detail and demonstrates the power of the language and the performance of the generators in an empirical evaluation.","The evaluation shows the expressiveness and conciseness of VADL and the efficiency of the generated artifacts."],"url":"http://arxiv.org/abs/2402.09087v1","category":"cs.PL"}
{"created":"2024-02-14 11:02:04","title":"Polynomial Semantics of Tractable Probabilistic Circuits","abstract":"Probabilistic circuits compute multilinear polynomials that represent probability distributions. They are tractable models that support efficient marginal inference. However, various polynomial semantics have been considered in the literature (e.g., network polynomials, likelihood polynomials, generating functions, Fourier transforms, and characteristic polynomials). The relationships between these polynomial encodings of distributions is largely unknown. In this paper, we prove that for binary distributions, each of these probabilistic circuit models is equivalent in the sense that any circuit for one of them can be transformed into a circuit for any of the others with only a polynomial increase in size. They are therefore all tractable for marginal inference on the same class of distributions. Finally, we explore the natural extension of one such polynomial semantics, called probabilistic generating circuits, to categorical random variables, and establish that marginal inference becomes #P-hard.","sentences":["Probabilistic circuits compute multilinear polynomials that represent probability distributions.","They are tractable models that support efficient marginal inference.","However, various polynomial semantics have been considered in the literature (e.g., network polynomials, likelihood polynomials, generating functions, Fourier transforms, and characteristic polynomials).","The relationships between these polynomial encodings of distributions is largely unknown.","In this paper, we prove that for binary distributions, each of these probabilistic circuit models is equivalent in the sense that any circuit for one of them can be transformed into a circuit for any of the others with only a polynomial increase in size.","They are therefore all tractable for marginal inference on the same class of distributions.","Finally, we explore the natural extension of one such polynomial semantics, called probabilistic generating circuits, to categorical random variables, and establish that marginal inference becomes #P-hard."],"url":"http://arxiv.org/abs/2402.09085v1","category":"cs.AI"}
{"created":"2024-02-14 10:57:29","title":"Sobolev Training for Operator Learning","abstract":"This study investigates the impact of Sobolev Training on operator learning frameworks for improving model performance. Our research reveals that integrating derivative information into the loss function enhances the training process, and we propose a novel framework to approximate derivatives on irregular meshes in operator learning. Our findings are supported by both experimental evidence and theoretical analysis. This demonstrates the effectiveness of Sobolev Training in approximating the solution operators between infinite-dimensional spaces.","sentences":["This study investigates the impact of Sobolev Training on operator learning frameworks for improving model performance.","Our research reveals that integrating derivative information into the loss function enhances the training process, and we propose a novel framework to approximate derivatives on irregular meshes in operator learning.","Our findings are supported by both experimental evidence and theoretical analysis.","This demonstrates the effectiveness of Sobolev Training in approximating the solution operators between infinite-dimensional spaces."],"url":"http://arxiv.org/abs/2402.09084v1","category":"cs.LG"}
{"created":"2024-02-14 10:46:27","title":"Science Opportunities for IMAP-Lo Observations of Interstellar Neutral Hydrogen and Deuterium During a Maximum of Solar Activity","abstract":"Direct-sampling observations of interstellar neutral gas, including hydrogen and deuterium, have been performed for more than one cycle of solar activity by IBEX. IBEX viewing is restricted to directions perpendicular to the spacecraft--Sun line, which limits the observations to several months each year. This restriction is removed in a forthcoming mission Interstellar Mapping and Acceleration Probe. The IMAP-Lo instrument will have a capability of adjusting the angle of its boresight with the spacecraft rotation axis. We continue a series of studies of resulting science opportunities. We adopt a schedule of adjusting the boresight angle suggested by Kubiak et al. 2023 and focus on interstellar hydrogen and deuterium during solar maximum epoch. Based on extensive set of simulations, we identify the times during calendar year and elongation angles of the boresight needed to measure the abundance of D/H at the termination shock and unambiguously observe interstellar H without contribution from interstellar He. Furthermore, IMAP-Lo will be able to resolve the primary and secondary populations, in particular to view the secondary population with little contribution from the primary. We show that the expected signal is sensitive to details of radiation pressure, particularly its dependence on radial speed of the atoms, and to details of the behavior of the distribution function of the primary and secondary populations at the heliopause. Therefore, IMAP-Lo will be able to provide observations needed to address compelling questions of the heliospheric physics, and even general astrophysics.","sentences":["Direct-sampling observations of interstellar neutral gas, including hydrogen and deuterium, have been performed for more than one cycle of solar activity by IBEX.","IBEX viewing is restricted to directions perpendicular to the spacecraft--Sun line, which limits the observations to several months each year.","This restriction is removed in a forthcoming mission Interstellar Mapping and Acceleration Probe.","The IMAP-Lo instrument will have a capability of adjusting the angle of its boresight with the spacecraft rotation axis.","We continue a series of studies of resulting science opportunities.","We adopt a schedule of adjusting the boresight angle suggested by Kubiak et al. 2023 and focus on interstellar hydrogen and deuterium during solar maximum epoch.","Based on extensive set of simulations, we identify the times during calendar year and elongation angles of the boresight needed to measure the abundance of D/H at the termination shock and unambiguously observe interstellar H without contribution from interstellar He.","Furthermore, IMAP-Lo will be able to resolve the primary and secondary populations, in particular to view the secondary population with little contribution from the primary.","We show that the expected signal is sensitive to details of radiation pressure, particularly its dependence on radial speed of the atoms, and to details of the behavior of the distribution function of the primary and secondary populations at the heliopause.","Therefore, IMAP-Lo will be able to provide observations needed to address compelling questions of the heliospheric physics, and even general astrophysics."],"url":"http://arxiv.org/abs/2402.09080v1","category":"astro-ph.SR"}
{"created":"2024-02-14 10:44:15","title":"Radial symmetry and sharp asymptotic behaviors of nonnegative solutions to $D^{1,p}$-critical quasi-linear static Schr\u00f6dinger-Hartree equation involving $p$-Laplacian $-\u0394_{p}$","abstract":"In this paper, we mainly consider nonnegative weak solution to the $D^{1,p}(\\R^{N})$-critical quasi-linear static Schr\\\"{o}dinger-Hartree equation with $p$-Laplacian $-\\Delta_{p}$ and nonlocal nonlinearity: \\begin{align*} -\\Delta_p u =\\left(|x|^{-2p}\\ast |u|^{p}\\right)|u|^{p-2}u \\qquad &\\mbox{in} \\,\\, \\mathbb{R}^N, \\end{align*} where $1<p<\\frac{N}{2}$, $N\\geq3$ and $u\\in D^{1,p}(\\R^N)$. Being different to the $D^{1,p}(\\R^{N})$-critical local nonlinear term $u^{p^{\\star}-1}$ with $p^{\\star}:=\\frac{Np}{N-p}$ investigated in \\cite{LDBS,BS16,VJ16} etc., since the nonlocal convolution $|x|^{-2p}*u^p$ appears in the Hartree type nonlinearity, it is impossible for us to use the scaling arguments and the Doubling Lemma as in \\cite{VJ16} to get preliminary estimates on upper bounds of asymptotic behaviors for any positive solutions $u$. Moreover, it is also quite difficult to obtain the boundedness of the quasi-norm $\\|u \\|_{L^{s,\\infty}(\\R^N)}$ and hence derive the sharp estimates on upper bounds of asymptotic behaviors from the preliminary estimates as in \\cite{VJ16}. Fortunately, by showing a better preliminary estimates on upper bounds of asymptotic behaviors through the De Giorgi-Moser-Nash iteration method and combining the result from \\cite{XCL}, we are able to overcome these difficulties and establish regularity and the sharp estimates on both upper and lower bounds of asymptotic behaviors for any positive solution $u$ to more general equation $-\\Delta_p u=V(x)u^{p-1}$ with $V\\in L^{\\frac{N}{p}}(\\mathbb{R}^{N})$. Then, by using the arguments from \\cite{BS16,VJ16}, we can deduce the sharp estimates on both upper and lower bounds for the decay rate of $|\\nabla u|$. Finally, as a consequence, we can apply the method of moving planes to prove that all the nonnegative solutions are radially symmetric and strictly decreasing about some point $x_0\\in\\R^N$.","sentences":["In this paper, we mainly consider nonnegative weak solution to the $D^{1,p}(\\R^{N})$-critical quasi-linear static Schr\\\"{o}dinger-Hartree equation with $p$-Laplacian $-\\Delta_{p}$ and nonlocal nonlinearity: \\begin{align*} -\\Delta_p u =\\left(|x|^{-2p}\\ast |u|^{p}\\right)|u|^{p-2}u \\qquad &\\mbox{in} \\,\\, \\mathbb{R}^N, \\end{align*} where $1<p<\\frac{N}{2}$, $N\\geq3$ and $u\\in D^{1,p}(\\R^N)$. Being different to the $D^{1,p}(\\R^{N})$-critical local nonlinear term $u^{p^{\\star}-1}$ with $p^{\\star}:=\\frac{Np}{N-p}$ investigated in \\cite{LDBS,BS16,VJ16} etc., since the nonlocal convolution $|x|^{-2p}*u^p$ appears in the Hartree type nonlinearity, it is impossible for us to use the scaling arguments and the Doubling Lemma as in \\cite{VJ16} to get preliminary estimates on upper bounds of asymptotic behaviors for any positive solutions $u$. Moreover, it is also quite difficult to obtain the boundedness of the quasi-norm $\\|u \\|_{L^{s,\\infty}(\\R^N)}$ and hence derive the sharp estimates on upper bounds of asymptotic behaviors from the preliminary estimates as in \\cite{VJ16}.","Fortunately, by showing a better preliminary estimates on upper bounds of asymptotic behaviors through the De Giorgi-Moser-Nash iteration method and combining the result from \\cite{XCL}, we are able to overcome these difficulties and establish regularity and the sharp estimates on both upper and lower bounds of asymptotic behaviors for any positive solution $u$ to more general equation $-\\Delta_p u=V(x)u^{p-1}$ with $V\\in L^{\\frac{N}{p}}(\\mathbb{R}^{N})$.","Then, by using the arguments from \\cite{BS16,VJ16}, we can deduce the sharp estimates on both upper and lower bounds for the decay rate of $|\\nabla u|$. Finally, as a consequence, we can apply the method of moving planes to prove that all the nonnegative solutions are radially symmetric and strictly decreasing about some point $x_0\\in\\R^N$."],"url":"http://arxiv.org/abs/2402.09079v1","category":"math.AP"}
{"created":"2024-02-14 10:44:03","title":"Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods","abstract":"This paper introduces innovative methods in Reinforcement Learning (RL), focusing on addressing and exploiting estimation biases in Actor-Critic methods for continuous control tasks, using Deep Double Q-Learning. We propose two novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3). ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a balance between computational efficiency and performance, while BE-TD3 is designed to dynamically select the most advantageous estimation bias during training. Our extensive experiments across various continuous control tasks demonstrate the effectiveness of our approaches. We show that these algorithms can either match or surpass existing methods like TD3, particularly in environments where estimation biases significantly impact learning. The results underline the importance of bias exploitation in improving policy learning in RL.","sentences":["This paper introduces innovative methods in Reinforcement Learning (RL), focusing on addressing and exploiting estimation biases in Actor-Critic methods for continuous control tasks, using Deep Double Q-Learning.","We propose two novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3).","ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a balance between computational efficiency and performance, while BE-TD3 is designed to dynamically select the most advantageous estimation bias during training.","Our extensive experiments across various continuous control tasks demonstrate the effectiveness of our approaches.","We show that these algorithms can either match or surpass existing methods like TD3, particularly in environments where estimation biases significantly impact learning.","The results underline the importance of bias exploitation in improving policy learning in RL."],"url":"http://arxiv.org/abs/2402.09078v1","category":"cs.LG"}
{"created":"2024-02-14 10:39:16","title":"Preserving system activity while controlling epidemic spreading in adaptive temporal networks","abstract":"Human behaviour strongly influences the spread of infectious diseases: understanding the interplay between epidemic dynamics and adaptive behaviours is essential to improve response strategies to epidemics, with the goal of containing the epidemic while preserving a sufficient level of operativeness in the population. Through activity-driven temporal networks, we formulate a general framework which models a wide range of adaptive behaviours and mitigation strategies, observed in real populations. We analytically derive the conditions for a widespread diffusion of epidemics in the presence of arbitrary adaptive behaviours, highlighting the crucial role of correlations between agents behaviour in the infected and in the susceptible state. We focus on the effects of sick-leave, comparing the effectiveness of different strategies in reducing the impact of the epidemic and preserving the system operativeness. We show the critical relevance of heterogeneity in individual behavior: in homogeneous networks, all sick-leave strategies are equivalent and poorly effective, while in heterogeneous networks, strategies targeting the most vulnerable nodes are able to effectively mitigate the epidemic, also avoiding a deterioration in system activity and maintaining a low level of absenteeism. Interestingly, with targeted strategies both the minimum of population activity and the maximum of absenteeism anticipate the infection peak, which is effectively flattened and delayed, so that full operativeness is almost restored when the infection peak arrives. We also provide realistic estimates of the model parameters for influenza-like illness, thereby suggesting strategies for managing epidemics and absenteeism in realistic populations.","sentences":["Human behaviour strongly influences the spread of infectious diseases: understanding the interplay between epidemic dynamics and adaptive behaviours is essential to improve response strategies to epidemics, with the goal of containing the epidemic while preserving a sufficient level of operativeness in the population.","Through activity-driven temporal networks, we formulate a general framework which models a wide range of adaptive behaviours and mitigation strategies, observed in real populations.","We analytically derive the conditions for a widespread diffusion of epidemics in the presence of arbitrary adaptive behaviours, highlighting the crucial role of correlations between agents behaviour in the infected and in the susceptible state.","We focus on the effects of sick-leave, comparing the effectiveness of different strategies in reducing the impact of the epidemic and preserving the system operativeness.","We show the critical relevance of heterogeneity in individual behavior: in homogeneous networks, all sick-leave strategies are equivalent and poorly effective, while in heterogeneous networks, strategies targeting the most vulnerable nodes are able to effectively mitigate the epidemic, also avoiding a deterioration in system activity and maintaining a low level of absenteeism.","Interestingly, with targeted strategies both the minimum of population activity and the maximum of absenteeism anticipate the infection peak, which is effectively flattened and delayed, so that full operativeness is almost restored when the infection peak arrives.","We also provide realistic estimates of the model parameters for influenza-like illness, thereby suggesting strategies for managing epidemics and absenteeism in realistic populations."],"url":"http://arxiv.org/abs/2402.09076v1","category":"physics.soc-ph"}
{"created":"2024-02-14 10:33:17","title":"Trace Ratio Based Manifold Learning with Tensor Data","abstract":"In this paper, we propose an extension of trace ratio based Manifold learning methods to deal with multidimensional data sets. Based on recent progress on the tensor-tensor product, we present a generalization of the trace ratio criterion by using the properties of the t-product. This will conduct us to introduce some new concepts such as Laplacian tensor and we will study formally the trace ratio problem by discuting the conditions for the exitence of solutions and optimality. Next, we will present a tensor Newton QR decomposition algorithm for solving the trace ratio problem. Manifold learning methods such as Laplacian eigenmaps, linear discriminant analysis and locally linear embedding will be formulated in a tensor representation and optimized by the proposed algorithm. Lastly, we will evaluate the performance of the different studied dimension reduction methods on several synthetic and real world data sets.","sentences":["In this paper, we propose an extension of trace ratio based Manifold learning methods to deal with multidimensional data sets.","Based on recent progress on the tensor-tensor product, we present a generalization of the trace ratio criterion by using the properties of the t-product.","This will conduct us to introduce some new concepts such as Laplacian tensor and we will study formally the trace ratio problem by discuting the conditions for the exitence of solutions and optimality.","Next, we will present a tensor Newton QR decomposition algorithm for solving the trace ratio problem.","Manifold learning methods such as Laplacian eigenmaps, linear discriminant analysis and locally linear embedding will be formulated in a tensor representation and optimized by the proposed algorithm.","Lastly, we will evaluate the performance of the different studied dimension reduction methods on several synthetic and real world data sets."],"url":"http://arxiv.org/abs/2402.09072v1","category":"math.NA"}
{"created":"2024-02-14 10:32:58","title":"Affine transformation estimation improves visual self-supervised learning","abstract":"The standard approach to modern self-supervised learning is to generate random views through data augmentations and minimise a loss computed from the representations of these views. This inherently encourages invariance to the transformations that comprise the data augmentation function. In this work, we show that adding a module to constrain the representations to be predictive of an affine transformation improves the performance and efficiency of the learning process. The module is agnostic to the base self-supervised model and manifests in the form of an additional loss term that encourages an aggregation of the encoder representations to be predictive of an affine transformation applied to the input images. We perform experiments in various modern self-supervised models and see a performance improvement in all cases. Further, we perform an ablation study on the components of the affine transformation to understand which of them is affecting performance the most, as well as on key architectural design decisions.","sentences":["The standard approach to modern self-supervised learning is to generate random views through data augmentations and minimise a loss computed from the representations of these views.","This inherently encourages invariance to the transformations that comprise the data augmentation function.","In this work, we show that adding a module to constrain the representations to be predictive of an affine transformation improves the performance and efficiency of the learning process.","The module is agnostic to the base self-supervised model and manifests in the form of an additional loss term that encourages an aggregation of the encoder representations to be predictive of an affine transformation applied to the input images.","We perform experiments in various modern self-supervised models and see a performance improvement in all cases.","Further, we perform an ablation study on the components of the affine transformation to understand which of them is affecting performance the most, as well as on key architectural design decisions."],"url":"http://arxiv.org/abs/2402.09071v1","category":"cs.CV"}
{"created":"2024-02-14 10:24:04","title":"Solid Waste Detection in Remote Sensing Images: A Survey","abstract":"The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locations for new landfills. This review aims to provide a detailed illustration of the most relevant proposals for the detection and monitoring of solid waste sites by describing and comparing the approaches, the implemented techniques, and the employed data. Furthermore, since the data sources are of the utmost importance for developing an effective solid waste detection model, a comprehensive overview of the satellites and publicly available data sets is presented. Finally, this paper identifies the open issues in the state-of-the-art and discusses the relevant research directions for reducing the costs and improving the effectiveness of novel solid waste detection methods.","sentences":["The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards.","Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans.","Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive.","Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time.","Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades.","Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locations for new landfills.","This review aims to provide a detailed illustration of the most relevant proposals for the detection and monitoring of solid waste sites by describing and comparing the approaches, the implemented techniques, and the employed data.","Furthermore, since the data sources are of the utmost importance for developing an effective solid waste detection model, a comprehensive overview of the satellites and publicly available data sets is presented.","Finally, this paper identifies the open issues in the state-of-the-art and discusses the relevant research directions for reducing the costs and improving the effectiveness of novel solid waste detection methods."],"url":"http://arxiv.org/abs/2402.09066v1","category":"cs.CV"}
{"created":"2024-02-14 10:20:03","title":"Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space","abstract":"Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs. Trigger Warning: the appendix contains LLM-generated text with violence and harassment.","sentences":["Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models.","However, this approach neglects the steady progression of open-source models.","As open-source models advance in capability, ensuring their safety also becomes increasingly imperative.","Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored.","We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens.","We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning.","Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models.","Our findings highlight embedding space attacks as an important threat model in open-source LLMs.","Trigger Warning: the appendix contains LLM-generated text with violence and harassment."],"url":"http://arxiv.org/abs/2402.09063v1","category":"cs.LG"}
{"created":"2024-02-14 10:15:43","title":"I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption","abstract":"In today's machine learning landscape, fine-tuning pretrained transformer models has emerged as an essential technique, particularly in scenarios where access to task-aligned training data is limited. However, challenges surface when data sharing encounters obstacles due to stringent privacy regulations or user apprehension regarding personal information disclosure. Earlier works based on secure multiparty computation (SMC) and fully homomorphic encryption (FHE) for privacy-preserving machine learning (PPML) focused more on privacy-preserving inference than privacy-preserving training. In response, we introduce BlindTuner, a privacy-preserving fine-tuning system that enables transformer training exclusively on homomorphically encrypted data for image classification. Our extensive experimentation validates BlindTuner's effectiveness by demonstrating comparable accuracy to non-encrypted models. Notably, our findings highlight a substantial speed enhancement of 1.5x to 600x over previous work in this domain.","sentences":["In today's machine learning landscape, fine-tuning pretrained transformer models has emerged as an essential technique, particularly in scenarios where access to task-aligned training data is limited.","However, challenges surface when data sharing encounters obstacles due to stringent privacy regulations or user apprehension regarding personal information disclosure.","Earlier works based on secure multiparty computation (SMC) and fully homomorphic encryption (FHE) for privacy-preserving machine learning (PPML) focused more on privacy-preserving inference than privacy-preserving training.","In response, we introduce BlindTuner, a privacy-preserving fine-tuning system that enables transformer training exclusively on homomorphically encrypted data for image classification.","Our extensive experimentation validates BlindTuner's effectiveness by demonstrating comparable accuracy to non-encrypted models.","Notably, our findings highlight a substantial speed enhancement of 1.5x to 600x over previous work in this domain."],"url":"http://arxiv.org/abs/2402.09059v1","category":"cs.LG"}
{"created":"2024-02-14 10:14:54","title":"Stellar Population Astrophysics (SPA) with TNG, Fluorine abundances in seven open clusters","abstract":"The age, evolution, and chemical properties of the Galactic disk can be effectively ascertained using open clusters. Within the large program Stellar Populations Astrophysics at the Telescopio Nazionale Galileo, we specifically focused on stars in open clusters, to investigate various astrophysical topics, from the chemical content of very young systems to the abundance patterns of lesser studied intermediate-age and old open clusters. We investigate the astrophysically interesting element fluorine (F), which has an uncertain and intriguing cosmic origin. We also determine the abundance of cerium (Ce), as F abundance is expected to correlate with the s-process elements. High-resolution near-infrared spectra were obtained using the GIANO-B spectrograph. The Python version of Spectroscopy Made Easy (PySME), was used to derive atmospheric parameters and abundances. The stellar parameters were determined using OH, CN, and CO molecular lines along with Fe I lines. This paper presents the first F Galactic radial abundance gradient. Our results are also compared with literature estimates and with Galactic chemical evolution models that have been generated using different F production channels. Our results indicate a constant, solar pattern in the [F/Fe] ratios across clusters of different ages, supporting the latest findings that fluorine levels do not exhibit any secondary behavior for stars with solar or above-solar metallicity. By comparing our sample stars with the predictions of Galactic chemical evolution models, we came to the conclusion that both asymptotic giant branch stars and massive stars, including a fraction of fast rotators that increase with decreasing metallicity, are needed to explain the cosmic origin of F.","sentences":["The age, evolution, and chemical properties of the Galactic disk can be effectively ascertained using open clusters.","Within the large program Stellar Populations Astrophysics at the Telescopio Nazionale Galileo, we specifically focused on stars in open clusters, to investigate various astrophysical topics, from the chemical content of very young systems to the abundance patterns of lesser studied intermediate-age and old open clusters.","We investigate the astrophysically interesting element fluorine (F), which has an uncertain and intriguing cosmic origin.","We also determine the abundance of cerium (Ce), as F abundance is expected to correlate with the s-process elements.","High-resolution near-infrared spectra were obtained using the GIANO-B spectrograph.","The Python version of Spectroscopy Made Easy (PySME), was used to derive atmospheric parameters and abundances.","The stellar parameters were determined using OH, CN, and CO molecular lines along with Fe I lines.","This paper presents the first F Galactic radial abundance gradient.","Our results are also compared with literature estimates and with Galactic chemical evolution models that have been generated using different F production channels.","Our results indicate a constant, solar pattern in the [F/Fe] ratios across clusters of different ages, supporting the latest findings that fluorine levels do not exhibit any secondary behavior for stars with solar or above-solar metallicity.","By comparing our sample stars with the predictions of Galactic chemical evolution models, we came to the conclusion that both asymptotic giant branch stars and massive stars, including a fraction of fast rotators that increase with decreasing metallicity, are needed to explain the cosmic origin of F."],"url":"http://arxiv.org/abs/2402.09058v1","category":"astro-ph.GA"}
{"created":"2024-02-14 10:07:05","title":"Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?","abstract":"Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-order loss minimization, and the relative (rather than absolute) nature of epistemic uncertainty measures.","sentences":["Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty.","Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years.","The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted.","This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures.","With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-order loss minimization, and the relative (rather than absolute) nature of epistemic uncertainty measures."],"url":"http://arxiv.org/abs/2402.09056v1","category":"cs.AI"}
{"created":"2024-02-14 10:05:19","title":"Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection","abstract":"The growing importance of multi-modal humor detection within affective computing correlates with the expanding influence of short-form video sharing on social media platforms. In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal contrastive pre-training. Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space. The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches. Our dataset, code and model release at https://github.com/yliu-cs/CVLA.","sentences":["The growing importance of multi-modal humor detection within affective computing correlates with the expanding influence of short-form video sharing on social media platforms.","In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal contrastive pre-training.","Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space.","The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches.","Our dataset, code and model release at https://github.com/yliu-cs/CVLA."],"url":"http://arxiv.org/abs/2402.09055v1","category":"cs.CV"}
{"created":"2024-02-14 09:51:05","title":"L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects","abstract":"Diffusion-based image generation models such as DALL-E 3 and Stable Diffusion-XL demonstrate remarkable capabilities in generating images with realistic and unique compositions. Yet, these models are not robust in precisely reasoning about physical and spatial configurations of objects, especially when instructed with unconventional, thereby out-of-distribution descriptions, such as \"a chair with five legs\". In this paper, we propose a language agent with chain-of-3D-thoughts (L3GO), an inference-time approach that can reason about part-based 3D mesh generation of unconventional objects that current data-driven diffusion models struggle with. More concretely, we use large language models as agents to compose a desired object via trial-and-error within the 3D simulation environment. To facilitate our investigation, we develop a new benchmark, Unconventionally Feasible Objects (UFO), as well as SimpleBlenv, a wrapper environment built on top of Blender where language agents can build and compose atomic building blocks via API calls. Human and automatic GPT-4V evaluations show that our approach surpasses the standard GPT-4 and other language agents (e.g., ReAct and Reflexion) for 3D mesh generation on ShapeNet. Moreover, when tested on our UFO benchmark, our approach outperforms other state-of-the-art text-to-2D image and text-to-3D models based on human evaluation.","sentences":["Diffusion-based image generation models such as DALL-E 3 and Stable Diffusion-XL demonstrate remarkable capabilities in generating images with realistic and unique compositions.","Yet, these models are not robust in precisely reasoning about physical and spatial configurations of objects, especially when instructed with unconventional, thereby out-of-distribution descriptions, such as \"a chair with five legs\".","In this paper, we propose a language agent with chain-of-3D-thoughts (L3GO), an inference-time approach that can reason about part-based 3D mesh generation of unconventional objects that current data-driven diffusion models struggle with.","More concretely, we use large language models as agents to compose a desired object via trial-and-error within the 3D simulation environment.","To facilitate our investigation, we develop a new benchmark, Unconventionally Feasible Objects (UFO), as well as SimpleBlenv, a wrapper environment built on top of Blender where language agents can build and compose atomic building blocks via API calls.","Human and automatic GPT-4V evaluations show that our approach surpasses the standard GPT-4 and other language agents (e.g., ReAct and Reflexion) for 3D mesh generation on ShapeNet.","Moreover, when tested on our UFO benchmark, our approach outperforms other state-of-the-art text-to-2D image and text-to-3D models based on human evaluation."],"url":"http://arxiv.org/abs/2402.09052v1","category":"cs.AI"}
{"created":"2024-02-14 09:48:39","title":"FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning","abstract":"The human-like automatic deductive reasoning has always been one of the most challenging open problems in the interdiscipline of mathematics and artificial intelligence. This paper is the third in a series of our works. We built a neural-symbolic system, called FGeoDRL, to automatically perform human-like geometric deductive reasoning. The neural part is an AI agent based on reinforcement learning, capable of autonomously learning problem-solving methods from the feedback of a formalized environment, without the need for human supervision. It leverages a pre-trained natural language model to establish a policy network for theorem selection and employ Monte Carlo Tree Search for heuristic exploration. The symbolic part is a reinforcement learning environment based on geometry formalization theory and FormalGeo\\cite{FormalGeo}, which models GPS as a Markov Decision Process\\cite{MDP}. In this formal symbolic system, the known conditions and objectives of the problem form the state space, while the set of theorems forms the action space. Leveraging FGeoDRL, we have achieved readable and verifiable automated solutions to geometric problems. Experiments conducted on the formalgeo7k dataset have achieved a problem-solving success rate of 86.40\\%. The project is available at https://github.com/PersonNoName/FGeoDRL.","sentences":["The human-like automatic deductive reasoning has always been one of the most challenging open problems in the interdiscipline of mathematics and artificial intelligence.","This paper is the third in a series of our works.","We built a neural-symbolic system, called FGeoDRL, to automatically perform human-like geometric deductive reasoning.","The neural part is an AI agent based on reinforcement learning, capable of autonomously learning problem-solving methods from the feedback of a formalized environment, without the need for human supervision.","It leverages a pre-trained natural language model to establish a policy network for theorem selection and employ Monte Carlo Tree Search for heuristic exploration.","The symbolic part is a reinforcement learning environment based on geometry formalization theory and FormalGeo\\cite{FormalGeo}, which models GPS as a Markov Decision Process\\cite{MDP}.","In this formal symbolic system, the known conditions and objectives of the problem form the state space, while the set of theorems forms the action space.","Leveraging FGeoDRL, we have achieved readable and verifiable automated solutions to geometric problems.","Experiments conducted on the formalgeo7k dataset have achieved a problem-solving success rate of 86.40\\%.","The project is available at https://github.com/PersonNoName/FGeoDRL."],"url":"http://arxiv.org/abs/2402.09051v1","category":"cs.AI"}
{"created":"2024-02-14 09:44:28","title":"FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems","abstract":"The application of contemporary artificial intelligence techniques to address geometric problems and automated deductive proof has always been a grand challenge to the interdiscipline field of mathematics and artificial Intelligence. This is the fourth article in a series of our works, in our previous work, we established of a geometric formalized system known as FormalGeo. Moreover we annotated approximately 7000 geometric problems, forming the FormalGeo7k dataset. Despite the FGPS (Formal Geometry Problem Solver) can achieve interpretable algebraic equation solving and human-like deductive reasoning, it often experiences timeouts due to the complexity of the search strategy. In this paper, we introduced FGeo-TP (Theorem Predictor), which utilizes the language model to predict theorem sequences for solving geometry problems. We compared the effectiveness of various Transformer architectures, such as BART or T5, in theorem prediction, implementing pruning in the search process of FGPS, thereby improving its performance in solving geometry problems. Our results demonstrate a significant increase in the problem-solving rate of the language model-enhanced FGeo-TP on the FormalGeo7k dataset, rising from 39.7% to 80.86%. Furthermore, FGeo-TP exhibits notable reductions in solving time and search steps across problems of varying difficulty levels.","sentences":["The application of contemporary artificial intelligence techniques to address geometric problems and automated deductive proof has always been a grand challenge to the interdiscipline field of mathematics and artificial Intelligence.","This is the fourth article in a series of our works, in our previous work, we established of a geometric formalized system known as FormalGeo.","Moreover we annotated approximately 7000 geometric problems, forming the FormalGeo7k dataset.","Despite the FGPS (Formal Geometry Problem Solver) can achieve interpretable algebraic equation solving and human-like deductive reasoning, it often experiences timeouts due to the complexity of the search strategy.","In this paper, we introduced FGeo-TP (Theorem Predictor), which utilizes the language model to predict theorem sequences for solving geometry problems.","We compared the effectiveness of various Transformer architectures, such as BART or T5, in theorem prediction, implementing pruning in the search process of FGPS, thereby improving its performance in solving geometry problems.","Our results demonstrate a significant increase in the problem-solving rate of the language model-enhanced FGeo-TP on the FormalGeo7k dataset, rising from 39.7% to 80.86%.","Furthermore, FGeo-TP exhibits notable reductions in solving time and search steps across problems of varying difficulty levels."],"url":"http://arxiv.org/abs/2402.09047v1","category":"cs.AI"}
{"created":"2024-02-14 09:43:35","title":"Inference of Abstraction for a Unified Account of Reasoning and Learning","abstract":"Inspired by Bayesian approaches to brain function in neuroscience, we give a simple theory of probabilistic inference for a unified account of reasoning and learning. We simply model how data cause symbolic knowledge in terms of its satisfiability in formal logic. The underlying idea is that reasoning is a process of deriving symbolic knowledge from data via abstraction, i.e., selective ignorance. The logical consequence relation is discussed for its proof-based theoretical correctness. The MNIST dataset is discussed for its experiment-based empirical correctness.","sentences":["Inspired by Bayesian approaches to brain function in neuroscience, we give a simple theory of probabilistic inference for a unified account of reasoning and learning.","We simply model how data cause symbolic knowledge in terms of its satisfiability in formal logic.","The underlying idea is that reasoning is a process of deriving symbolic knowledge from data via abstraction, i.e., selective ignorance.","The logical consequence relation is discussed for its proof-based theoretical correctness.","The MNIST dataset is discussed for its experiment-based empirical correctness."],"url":"http://arxiv.org/abs/2402.09046v1","category":"cs.AI"}
{"created":"2024-02-14 09:39:04","title":"Non-existence of short hairs for static black holes","abstract":"The black hole no-short hair theorem establishes a universal lower bound on the extension of hairs outside any 4-dimensional spherically symmetric black hole solutions. We generalise this theorem beyond spherical symmetry, specifically for static, axisymmetric hairy black hole solutions and prove that the ``hairosphere'' must extend beyond the radial extent of the innermost light ring.","sentences":["The black hole no-short hair theorem establishes a universal lower bound on the extension of hairs outside any 4-dimensional spherically symmetric black hole solutions.","We generalise this theorem beyond spherical symmetry, specifically for static, axisymmetric hairy black hole solutions and prove that the ``hairosphere'' must extend beyond the radial extent of the innermost light ring."],"url":"http://arxiv.org/abs/2402.09044v1","category":"gr-qc"}
{"created":"2024-02-14 09:35:29","title":"A new approach in two-dimensional heavy-tailed distributions","abstract":"We consider a new approach in the definition of two-dimensional heavy-tailed distributions. Namely, we introduce the classes of two-dimensional long-tailed, of twodimensional dominatedly varying and of two-dimensional consistently varying distributions. Next, we define the closure property with respect to two-dimensional convolution and to joint max-sum equivalence in order to study if they are satisfied by these classes. Further we examine the joint behavior of two random sums, under generalized tail asymptotic independence. Afterward we study the closure property under scalar product and two dimensional product convolution and by these results we extended our main result in the case of jointly randomly weighted sums. Our results contained some applications where we establish the asymptotic expression of the ruin probability in a two-dimensional discrete-time risk model.","sentences":["We consider a new approach in the definition of two-dimensional heavy-tailed distributions.","Namely, we introduce the classes of two-dimensional long-tailed, of twodimensional dominatedly varying and of two-dimensional consistently varying distributions.","Next, we define the closure property with respect to two-dimensional convolution and to joint max-sum equivalence in order to study if they are satisfied by these classes.","Further we examine the joint behavior of two random sums, under generalized tail asymptotic independence.","Afterward we study the closure property under scalar product and two dimensional product convolution and by these results we extended our main result in the case of jointly randomly weighted sums.","Our results contained some applications where we establish the asymptotic expression of the ruin probability in a two-dimensional discrete-time risk model."],"url":"http://arxiv.org/abs/2402.09040v1","category":"math.PR"}
{"created":"2024-02-14 09:34:45","title":"Tunneling of quantum geometries in spinfoams","abstract":"Quantum gravitational tunneling effects are expected to give rise to a number of interesting observable phenomena, including, in particular, the evolution of black holes at the end of their existence or the emergence of the early universe from a quantum phase. Covariant Loop Quantum Gravity provides a framework to study these phenomena, yet a precise identification of tunneling processes is still not known. Motivated by this question, we consider a related, simpler case, that of Ponzano-Regge amplitudes: we find a surprising and detailed analogy of a class of simple transition amplitudes with tunneling processes in non-relativistic quantum mechanics.","sentences":["Quantum gravitational tunneling effects are expected to give rise to a number of interesting observable phenomena, including, in particular, the evolution of black holes at the end of their existence or the emergence of the early universe from a quantum phase.","Covariant Loop Quantum Gravity provides a framework to study these phenomena, yet a precise identification of tunneling processes is still not known.","Motivated by this question, we consider a related, simpler case, that of Ponzano-Regge amplitudes: we find a surprising and detailed analogy of a class of simple transition amplitudes with tunneling processes in non-relativistic quantum mechanics."],"url":"http://arxiv.org/abs/2402.09038v1","category":"gr-qc"}
{"created":"2024-02-14 09:21:00","title":"Can Text-to-image Model Assist Multi-modal Learning for Visual Recognition with Visual Modality Missing?","abstract":"Multi-modal learning has emerged as an increasingly promising avenue in vision recognition, driving innovations across diverse domains ranging from media and education to healthcare and transportation. Despite its success, the robustness of multi-modal learning for visual recognition is often challenged by the unavailability of a subset of modalities, especially the visual modality. Conventional approaches to mitigate missing modalities in multi-modal learning rely heavily on algorithms and modality fusion schemes. In contrast, this paper explores the use of text-to-image models to assist multi-modal learning. Specifically, we propose a simple but effective multi-modal learning framework GTI-MM to enhance the data efficiency and model robustness against missing visual modality by imputing the missing data with generative transformers. Using multiple multi-modal datasets with visual recognition tasks, we present a comprehensive analysis of diverse conditions involving missing visual modality in data, including model training. Our findings reveal that synthetic images benefit training data efficiency with visual data missing in training and improve model robustness with visual data missing involving training and testing. Moreover, we demonstrate GTI-MM is effective with lower generation quantity and simple prompt techniques.","sentences":["Multi-modal learning has emerged as an increasingly promising avenue in vision recognition, driving innovations across diverse domains ranging from media and education to healthcare and transportation.","Despite its success, the robustness of multi-modal learning for visual recognition is often challenged by the unavailability of a subset of modalities, especially the visual modality.","Conventional approaches to mitigate missing modalities in multi-modal learning rely heavily on algorithms and modality fusion schemes.","In contrast, this paper explores the use of text-to-image models to assist multi-modal learning.","Specifically, we propose a simple but effective multi-modal learning framework GTI-MM to enhance the data efficiency and model robustness against missing visual modality by imputing the missing data with generative transformers.","Using multiple multi-modal datasets with visual recognition tasks, we present a comprehensive analysis of diverse conditions involving missing visual modality in data, including model training.","Our findings reveal that synthetic images benefit training data efficiency with visual data missing in training and improve model robustness with visual data missing involving training and testing.","Moreover, we demonstrate GTI-MM is effective with lower generation quantity and simple prompt techniques."],"url":"http://arxiv.org/abs/2402.09036v1","category":"cs.CV"}
{"created":"2024-02-14 09:20:22","title":"Using storytelling to foster the teaching and learning of gravitational waves physics at high-school","abstract":"Studies in Physics Education Research show that interdisciplinary approaches in education foster students' motivation, creativity, curiosity, and interest in physics. We discuss their features and potential role in bringing contemporary physics topics to high school, and how to use them to integrate formal educational programs. We make an explicit example of the use of storytelling and theatrical techniques to introduce secondary school students to black holes and gravitational waves topics. The activity has been designed by the Educational Division of the Physics Department at the University of Cagliari. Participants were 200 high-school students (17 to 19 years old) from five schools (scientific, humanities) in Sardinia. A measure of the efficacy in the use of artistic tools to communicate and teach the proposed subjects has been done utilizing a research questionnaire. We collected 76 answers. Results show that our methodology is useful to introduce students to contemporary physics themes, fostering their interest and learning of such contents. Students from humanities significantly appreciated more the use of poetry and artistic tools than their scientific peers. Finally, we discuss the potentiality of our approach in orientating students towards a STEAM (STEM and Arts) career.","sentences":["Studies in Physics Education Research show that interdisciplinary approaches in education foster students' motivation, creativity, curiosity, and interest in physics.","We discuss their features and potential role in bringing contemporary physics topics to high school, and how to use them to integrate formal educational programs.","We make an explicit example of the use of storytelling and theatrical techniques to introduce secondary school students to black holes and gravitational waves topics.","The activity has been designed by the Educational Division of the Physics Department at the University of Cagliari.","Participants were 200 high-school students (17 to 19 years old) from five schools (scientific, humanities) in Sardinia.","A measure of the efficacy in the use of artistic tools to communicate and teach the proposed subjects has been done utilizing a research questionnaire.","We collected 76 answers.","Results show that our methodology is useful to introduce students to contemporary physics themes, fostering their interest and learning of such contents.","Students from humanities significantly appreciated more the use of poetry and artistic tools than their scientific peers.","Finally, we discuss the potentiality of our approach in orientating students towards a STEAM (STEM and Arts) career."],"url":"http://arxiv.org/abs/2402.09035v1","category":"physics.ed-ph"}
{"created":"2024-02-14 09:20:13","title":"Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints","abstract":"Activation functions enable neural networks to learn complex representations by introducing non-linearities. While feedforward models commonly use rectified linear units, sequential models like recurrent neural networks, long short-term memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH activation functions. However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies. To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints. SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse applications, such as sign language recognition, regression, and time-series classification tasks, where the dataset is limited. Our experiments demonstrate that SST models consistently outperform RNN-based models with baseline activations, exhibiting improved test accuracy.","sentences":["Activation functions enable neural networks to learn complex representations by introducing non-linearities.","While feedforward models commonly use rectified linear units, sequential models like recurrent neural networks, long short-term memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH activation functions.","However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies.","To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints.","SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering.","We evaluate SST-powered LSTMs and GRUs for diverse applications, such as sign language recognition, regression, and time-series classification tasks, where the dataset is limited.","Our experiments demonstrate that SST models consistently outperform RNN-based models with baseline activations, exhibiting improved test accuracy."],"url":"http://arxiv.org/abs/2402.09034v1","category":"cs.LG"}
{"created":"2024-02-14 09:08:00","title":"Awareness in robotics: An early perspective from the viewpoint of the EIC Pathfinder Challenge \"Awareness Inside''","abstract":"Consciousness has been historically a heavily debated topic in engineering, science, and philosophy. On the contrary, awareness had less success in raising the interest of scholars in the past. However, things are changing as more and more researchers are getting interested in answering questions concerning what awareness is and how it can be artificially generated. The landscape is rapidly evolving, with multiple voices and interpretations of the concept being conceived and techniques being developed. The goal of this paper is to summarize and discuss the ones among these voices connected with projects funded by the EIC Pathfinder Challenge called ``Awareness Inside'', a nonrecurring call for proposals within Horizon Europe designed specifically for fostering research on natural and synthetic awareness. In this perspective, we dedicate special attention to challenges and promises of applying synthetic awareness in robotics, as the development of mature techniques in this new field is expected to have a special impact on generating more capable and trustworthy embodied systems.","sentences":["Consciousness has been historically a heavily debated topic in engineering, science, and philosophy.","On the contrary, awareness had less success in raising the interest of scholars in the past.","However, things are changing as more and more researchers are getting interested in answering questions concerning what awareness is and how it can be artificially generated.","The landscape is rapidly evolving, with multiple voices and interpretations of the concept being conceived and techniques being developed.","The goal of this paper is to summarize and discuss the ones among these voices connected with projects funded by the EIC Pathfinder Challenge called ``Awareness Inside'', a nonrecurring call for proposals within Horizon Europe designed specifically for fostering research on natural and synthetic awareness.","In this perspective, we dedicate special attention to challenges and promises of applying synthetic awareness in robotics, as the development of mature techniques in this new field is expected to have a special impact on generating more capable and trustworthy embodied systems."],"url":"http://arxiv.org/abs/2402.09030v1","category":"cs.RO"}
{"created":"2024-02-14 08:56:41","title":"Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems","abstract":"Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks. Understanding attack tactics helps improve the robustness of RSs. We intend to develop efficient attack methods that use limited resources to generate high-quality fake user profiles to achieve 1) transferability among black-box RSs 2) and imperceptibility among detectors. In order to achieve these goals, we introduce textual reviews of products to enhance the generation quality of the profiles. Specifically, we propose a novel attack framework named R-Trojan, which formulates the attack objectives as an optimization problem and adopts a tailored transformer-based generative adversarial network (GAN) to solve it so that high-quality attack profiles can be produced. Comprehensive experiments on real-world datasets demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods on various victim RSs under black-box settings and show its good imperceptibility.","sentences":["Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks.","Understanding attack tactics helps improve the robustness of RSs.","We intend to develop efficient attack methods that use limited resources to generate high-quality fake user profiles to achieve 1) transferability among black-box RSs 2) and imperceptibility among detectors.","In order to achieve these goals, we introduce textual reviews of products to enhance the generation quality of the profiles.","Specifically, we propose a novel attack framework named R-Trojan, which formulates the attack objectives as an optimization problem and adopts a tailored transformer-based generative adversarial network (GAN) to solve it so that high-quality attack profiles can be produced.","Comprehensive experiments on real-world datasets demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods on various victim RSs under black-box settings and show its good imperceptibility."],"url":"http://arxiv.org/abs/2402.09023v1","category":"cs.CR"}
{"created":"2024-02-14 08:52:45","title":"Assessing AI-Based Code Assistants in Method Generation Tasks","abstract":"AI-based code assistants are increasingly popular as a means to enhance productivity and improve code quality. This study compares four AI-based code assistants, GitHub Copilot, Tabnine, ChatGPT, and Google Bard, in method generation tasks, assessing their ability to produce accurate, correct, and efficient code. Results show that code assistants are useful, with complementary capabilities, although they rarely generate ready-to-use correct code.","sentences":["AI-based code assistants are increasingly popular as a means to enhance productivity and improve code quality.","This study compares four AI-based code assistants, GitHub Copilot, Tabnine, ChatGPT, and Google Bard, in method generation tasks, assessing their ability to produce accurate, correct, and efficient code.","Results show that code assistants are useful, with complementary capabilities, although they rarely generate ready-to-use correct code."],"url":"http://arxiv.org/abs/2402.09022v1","category":"cs.SE"}
{"created":"2024-02-14 08:51:05","title":"NMR contributions to the study of water transfer in proton exchange membranes for fuel cells","abstract":"As programs to support efficient and sustainable energy sources are expanding, research into the potential applications of the hydrogen vector is accelerating. Proton exchange membrane fuel cells are electrochemical converters that transform the chemical energy of hydrogen into electrical energy. These devices are used today for low- and medium-power stationary applications and for mobility, in trains, cars, bicycles, etc. Proton exchange membrane fuel cells use a polymer membrane as the electrolyte. The role of the membrane is multiple: it must separate gases, be an electronic insulator and a very good ionic conductor. In addition, it must resist free-radical chemical attack and have good mechanical strength. Nafion-type perfluorinated membranes have all these properties: the fluorinated backbone is naturally hydrophobic, but the hydrophilic ionic groups give the material excellent water sorption properties. The water adsorbed in the structure is extremely mobile, acting as a transport medium for the protons generated at the anode. Although it has been studied for a long time and has been the subject of a large number of papers perfluorinated membranes are still the reference membranes today. This article reviews some contributions of Nuclear Magnetic Resonance methods in liquid state to the study of water properties in the structure of Nafion-type perfluorinated membranes.","sentences":["As programs to support efficient and sustainable energy sources are expanding, research into the potential applications of the hydrogen vector is accelerating.","Proton exchange membrane fuel cells are electrochemical converters that transform the chemical energy of hydrogen into electrical energy.","These devices are used today for low- and medium-power stationary applications and for mobility, in trains, cars, bicycles, etc.","Proton exchange membrane fuel cells use a polymer membrane as the electrolyte.","The role of the membrane is multiple: it must separate gases, be an electronic insulator and a very good ionic conductor.","In addition, it must resist free-radical chemical attack and have good mechanical strength.","Nafion-type perfluorinated membranes have all these properties: the fluorinated backbone is naturally hydrophobic, but the hydrophilic ionic groups give the material excellent water sorption properties.","The water adsorbed in the structure is extremely mobile, acting as a transport medium for the protons generated at the anode.","Although it has been studied for a long time and has been the subject of a large number of papers perfluorinated membranes are still the reference membranes today.","This article reviews some contributions of Nuclear Magnetic Resonance methods in liquid state to the study of water properties in the structure of Nafion-type perfluorinated membranes."],"url":"http://arxiv.org/abs/2402.09019v1","category":"cond-mat.soft"}
{"created":"2024-02-14 08:50:14","title":"Neural Operators Meet Energy-based Theory: Operator Learning for Hamiltonian and Dissipative PDEs","abstract":"The operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces. Prior works have proposed deep neural networks (DNNs) for learning such a mapping, enabling the learning of solution operators of partial differential equations (PDEs). However, these works still struggle to learn dynamics that obeys the laws of physics. This paper proposes Energy-consistent Neural Operators (ENOs), a general framework for learning solution operators of PDEs that follows the energy conservation or dissipation law from observed solution trajectories. We introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the energy functional is modeled by another DNN, allowing one to bias the outputs of the DNN-based solution operators to ensure energetic consistency without explicit PDEs. Experiments on multiple physical systems show that ENO outperforms existing DNN models in predicting solutions from data, especially in super-resolution settings.","sentences":["The operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces.","Prior works have proposed deep neural networks (DNNs) for learning such a mapping, enabling the learning of solution operators of partial differential equations (PDEs).","However, these works still struggle to learn dynamics that obeys the laws of physics.","This paper proposes Energy-consistent Neural Operators (ENOs), a general framework for learning solution operators of PDEs that follows the energy conservation or dissipation law from observed solution trajectories.","We introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the energy functional is modeled by another DNN, allowing one to bias the outputs of the DNN-based solution operators to ensure energetic consistency without explicit PDEs.","Experiments on multiple physical systems show that ENO outperforms existing DNN models in predicting solutions from data, especially in super-resolution settings."],"url":"http://arxiv.org/abs/2402.09018v1","category":"stat.ML"}
{"created":"2024-02-14 08:46:18","title":"Pyramid Attention Network for Medical Image Registration","abstract":"The advent of deep-learning-based registration networks has addressed the time-consuming challenge in traditional iterative methods.However, the potential of current registration networks for comprehensively capturing spatial relationships has not been fully explored, leading to inadequate performance in large-deformation image registration.The pure convolutional neural networks (CNNs) neglect feature enhancement, while current Transformer-based networks are susceptible to information redundancy.To alleviate these issues, we propose a pyramid attention network (PAN) for deformable medical image registration.Specifically, the proposed PAN incorporates a dual-stream pyramid encoder with channel-wise attention to boost the feature representation.Moreover, a multi-head local attention Transformer is introduced as decoder to analyze motion patterns and generate deformation fields.Extensive experiments on two public brain magnetic resonance imaging (MRI) datasets and one abdominal MRI dataset demonstrate that our method achieves favorable registration performance, while outperforming several CNN-based and Transformer-based registration networks.Our code is publicly available at https://github.com/JuliusWang-7/PAN.","sentences":["The advent of deep-learning-based registration networks has addressed the time-consuming challenge in traditional iterative methods.","However, the potential of current registration networks for comprehensively capturing spatial relationships has not been fully explored, leading to inadequate performance in large-deformation image registration.","The pure convolutional neural networks (CNNs) neglect feature enhancement, while current Transformer-based networks are susceptible to information redundancy.","To alleviate these issues, we propose a pyramid attention network (PAN) for deformable medical image registration.","Specifically, the proposed PAN incorporates a dual-stream pyramid encoder with channel-wise attention to boost the feature representation.","Moreover, a multi-head local attention Transformer is introduced as decoder to analyze motion patterns and generate deformation fields.","Extensive experiments on two public brain magnetic resonance imaging (MRI) datasets and one abdominal MRI dataset demonstrate that our method achieves favorable registration performance, while outperforming several CNN-based and Transformer-based registration networks.","Our code is publicly available at https://github.com/JuliusWang-7/PAN."],"url":"http://arxiv.org/abs/2402.09016v1","category":"cs.CV"}
{"created":"2024-02-14 08:46:15","title":"Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications","abstract":"The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of quantifier's work.","sentences":["The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks.","However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency.","This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs.","We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application.","This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria.","We present a comprehensive analysis of the robustness of quantifier's work."],"url":"http://arxiv.org/abs/2402.09015v1","category":"cs.CL"}
{"created":"2024-02-14 08:28:04","title":"A Practical and Online Trajectory Planner for Autonomous Ships' Berthing, Incorporating Speed Control","abstract":"Autonomous ships are essentially designed and equipped to perceive their internal and external environment and subsequently perform appropriate actions depending on the predetermined objective(s) without human intervention. Consequently, trajectory planning algorithms for autonomous berthing must consider factors such as system dynamics, ship actuators, environmental disturbances, and the safety of the ship, other ships, and port structures, among others. In this study, basing the ship dynamics on the low-speed MMG model, trajectory planning for an autonomous ship is modeled as an optimal control problem (OCP) that is transcribed into a nonlinear programming problem (NLP) using the direct multiple shooting technique. To enhance berthing safety, besides considering wind disturbances, speed control, actuators' limitations, and collision avoidance features are incorporated as constraints in the NLP, which is then solved using the Sequential Quadratic Programming (SQP) algorithm in MATLAB. Finally, the performance of the proposed planner is evaluated through (i) comparison with solutions obtained using CMA-ES for two different model ships, (ii) trajectory planning for different harbor entry and berth approach scenarios, and (iii) feasibility study using stochastically generated initial conditions and positions within the port boundaries. Simulation results indicate enhanced berthing safety as well as practical and computational feasibility making the planner suitable for real-time applications.","sentences":["Autonomous ships are essentially designed and equipped to perceive their internal and external environment and subsequently perform appropriate actions depending on the predetermined objective(s) without human intervention.","Consequently, trajectory planning algorithms for autonomous berthing must consider factors such as system dynamics, ship actuators, environmental disturbances, and the safety of the ship, other ships, and port structures, among others.","In this study, basing the ship dynamics on the low-speed MMG model, trajectory planning for an autonomous ship is modeled as an optimal control problem (OCP) that is transcribed into a nonlinear programming problem (NLP) using the direct multiple shooting technique.","To enhance berthing safety, besides considering wind disturbances, speed control, actuators' limitations, and collision avoidance features are incorporated as constraints in the NLP, which is then solved using the Sequential Quadratic Programming (SQP) algorithm in MATLAB.","Finally, the performance of the proposed planner is evaluated through (i) comparison with solutions obtained using CMA-ES for two different model ships, (ii) trajectory planning for different harbor entry and berth approach scenarios, and (iii) feasibility study using stochastically generated initial conditions and positions within the port boundaries.","Simulation results indicate enhanced berthing safety as well as practical and computational feasibility making the planner suitable for real-time applications."],"url":"http://arxiv.org/abs/2402.09009v1","category":"eess.SY"}
{"created":"2024-02-14 08:20:49","title":"DRL-Based Orchestration of Multi-User MISO Systems with Stacked Intelligent Metasurfaces","abstract":"Stacked intelligent metasurfaces (SIM) represents an advanced signal processing paradigm that enables over-the-air processing of electromagnetic waves at the speed of light. Its multi-layer structure exhibits customizable increased computational capability compared to conventional single-layer reconfigurable intelligent surfaces and metasurface lenses. In this paper, we deploy SIM to improve the performance of multi-user multiple-input single-output (MISO) wireless systems with low complexity transmit radio frequency (RF) chains. In particular, an optimization formulation for the joint design of the SIM phase shifts and the transmit power allocation is presented, which is efficiently solved via a customized deep reinforcement learning (DRL) approach that continuously observes pre-designed states of the SIM-parametrized smart wireless environment. The presented performance evaluation results showcase the proposed method's capability to effectively learn from the wireless environment while outperforming conventional precoding schemes under low transmit power conditions. Finally, a whitening process is presented to further augment the robustness of the proposed scheme.","sentences":["Stacked intelligent metasurfaces (SIM) represents an advanced signal processing paradigm that enables over-the-air processing of electromagnetic waves at the speed of light.","Its multi-layer structure exhibits customizable increased computational capability compared to conventional single-layer reconfigurable intelligent surfaces and metasurface lenses.","In this paper, we deploy SIM to improve the performance of multi-user multiple-input single-output (MISO) wireless systems with low complexity transmit radio frequency (RF) chains.","In particular, an optimization formulation for the joint design of the SIM phase shifts and the transmit power allocation is presented, which is efficiently solved via a customized deep reinforcement learning (DRL) approach that continuously observes pre-designed states of the SIM-parametrized smart wireless environment.","The presented performance evaluation results showcase the proposed method's capability to effectively learn from the wireless environment while outperforming conventional precoding schemes under low transmit power conditions.","Finally, a whitening process is presented to further augment the robustness of the proposed scheme."],"url":"http://arxiv.org/abs/2402.09006v1","category":"eess.SP"}
{"created":"2024-02-14 08:18:00","title":"Excitation signatures of isochorically heated electrons in solids at finite wavenumber explored from first principles","abstract":"Ultrafast heating of solids with modern X-ray free electron lasers (XFELs) leads to a unique set of conditions that is characterized by the simultaneous presence of heated electrons in a cold ionic lattice. In this work, we analyze the effect of electronic heating on the dynamic structure factor (DSF) in bulk Aluminium (Al) with a face-centered cubic lattice and in silicon (Si) with a crystal diamond structure using first-principles linear-response time-dependent density functional theory simulations. We find a thermally induced red shift of the collective plasmon excitation in both materials. In addition, we show that the heating of the electrons in Al can lead to the formation of a double-plasmon peak due to the extension of the Landau damping region to smaller wavenumbers. Finally, we demonstrate that thermal effects generate a measurable and distinct signature (peak-valley structure) in the DSF of Si at small frequencies. Our simulations indicate that there is a variety of new features in the spectrum of X-ray-driven solids, specifically at finite momentum transfer, which can probed in upcoming X-ray Thomson scattering (XRTS) experiments at various XFEL facilities.","sentences":["Ultrafast heating of solids with modern X-ray free electron lasers (XFELs) leads to a unique set of conditions that is characterized by the simultaneous presence of heated electrons in a cold ionic lattice.","In this work, we analyze the effect of electronic heating on the dynamic structure factor (DSF) in bulk Aluminium (Al) with a face-centered cubic lattice and in silicon (Si) with a crystal diamond structure using first-principles linear-response time-dependent density functional theory simulations.","We find a thermally induced red shift of the collective plasmon excitation in both materials.","In addition, we show that the heating of the electrons in Al can lead to the formation of a double-plasmon peak due to the extension of the Landau damping region to smaller wavenumbers.","Finally, we demonstrate that thermal effects generate a measurable and distinct signature (peak-valley structure) in the DSF of Si at small frequencies.","Our simulations indicate that there is a variety of new features in the spectrum of X-ray-driven solids, specifically at finite momentum transfer, which can probed in upcoming X-ray Thomson scattering (XRTS) experiments at various XFEL facilities."],"url":"http://arxiv.org/abs/2402.09005v1","category":"physics.comp-ph"}
{"created":"2024-02-14 07:52:28","title":"Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data","abstract":"Standardising structure volume names in radiotherapy (RT) data is necessary to enable data mining and analyses, especially across multi-institutional centres. This process is time and resource intensive, which highlights the need for new automated and efficient approaches to handle the task. Several machine learning-based methods have been proposed and evaluated to standardise nomenclature. However, no studies have considered that RT patient records are distributed across multiple data centres. This paper introduces a method that emulates real-world environments to establish standardised nomenclature. This is achieved by integrating decentralised real-time data and federated learning (FL). A multimodal deep artificial neural network was proposed to standardise RT data in federated settings. Three types of possible attributes were extracted from the structures to train the deep learning models: tabular, visual, and volumetric. Simulated experiments were carried out to train the models across several scenarios including multiple data centres, input modalities, and aggregation strategies. The models were compared against models developed with single modalities in federated settings, in addition to models trained in centralised settings. Categorical classification accuracy was calculated on hold-out samples to inform the models performance. Our results highlight the need for fusing multiple modalities when training such models, with better performance reported with tabular-volumetric models. In addition, we report comparable accuracy compared to models built in centralised settings. This demonstrates the suitability of FL for handling the standardization task. Additional ablation analyses showed that the total number of samples in the data centres and the number of data centres highly affects the training process and should be carefully considered when building standardisation models.","sentences":["Standardising structure volume names in radiotherapy (RT) data is necessary to enable data mining and analyses, especially across multi-institutional centres.","This process is time and resource intensive, which highlights the need for new automated and efficient approaches to handle the task.","Several machine learning-based methods have been proposed and evaluated to standardise nomenclature.","However, no studies have considered that RT patient records are distributed across multiple data centres.","This paper introduces a method that emulates real-world environments to establish standardised nomenclature.","This is achieved by integrating decentralised real-time data and federated learning (FL).","A multimodal deep artificial neural network was proposed to standardise RT data in federated settings.","Three types of possible attributes were extracted from the structures to train the deep learning models: tabular, visual, and volumetric.","Simulated experiments were carried out to train the models across several scenarios including multiple data centres, input modalities, and aggregation strategies.","The models were compared against models developed with single modalities in federated settings, in addition to models trained in centralised settings.","Categorical classification accuracy was calculated on hold-out samples to inform the models performance.","Our results highlight the need for fusing multiple modalities when training such models, with better performance reported with tabular-volumetric models.","In addition, we report comparable accuracy compared to models built in centralised settings.","This demonstrates the suitability of FL for handling the standardization task.","Additional ablation analyses showed that the total number of samples in the data centres and the number of data centres highly affects the training process and should be carefully considered when building standardisation models."],"url":"http://arxiv.org/abs/2402.08999v1","category":"cs.LG"}
{"created":"2024-02-14 07:48:16","title":"AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems","abstract":"Recently, Large Language Model based Autonomous system(LLMAS) has gained great popularity for its potential to simulate complicated behaviors of human societies. One of its main challenges is to present and analyze the dynamic events evolution of LLMAS. In this work, we present a visualization approach to explore detailed statuses and agents' behavior within LLMAS. We propose a general pipeline that establishes a behavior structure from raw LLMAS execution events, leverages a behavior summarization algorithm to construct a hierarchical summary of the entire structure in terms of time sequence, and a cause trace method to mine the causal relationship between agent behaviors. We then develop AgentLens, a visual analysis system that leverages a hierarchical temporal visualization for illustrating the evolution of LLMAS, and supports users to interactively investigate details and causes of agents' behaviors. Two usage scenarios and a user study demonstrate the effectiveness and usability of our AgentLens.","sentences":["Recently, Large Language Model based Autonomous system(LLMAS) has gained great popularity for its potential to simulate complicated behaviors of human societies.","One of its main challenges is to present and analyze the dynamic events evolution of LLMAS.","In this work, we present a visualization approach to explore detailed statuses and agents' behavior within LLMAS.","We propose a general pipeline that establishes a behavior structure from raw LLMAS execution events, leverages a behavior summarization algorithm to construct a hierarchical summary of the entire structure in terms of time sequence, and a cause trace method to mine the causal relationship between agent behaviors.","We then develop AgentLens, a visual analysis system that leverages a hierarchical temporal visualization for illustrating the evolution of LLMAS, and supports users to interactively investigate details and causes of agents' behaviors.","Two usage scenarios and a user study demonstrate the effectiveness and usability of our AgentLens."],"url":"http://arxiv.org/abs/2402.08995v1","category":"cs.HC"}
{"created":"2024-02-14 07:41:48","title":"CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding","abstract":"The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters. Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token representations are used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method. Code is available at https://github.com/CLIP-MUSED/CLIP-MUSED.","sentences":["The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences.","Moreover, the limited availability of data from a single subject has a constraining impact on model performance.","Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli.","To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method.","Our method consists of a Transformer-based feature extractor to effectively model global neural representations.","It also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters.","Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli.","Finally, token representations are used for multi-subject semantic decoding.","Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets.","Visualization results provide insights into the effectiveness of our proposed method.","Code is available at https://github.com/CLIP-MUSED/CLIP-MUSED."],"url":"http://arxiv.org/abs/2402.08994v1","category":"cs.CV"}
{"created":"2024-02-14 07:40:48","title":"The polyhedral type of a polynomial map on the plane","abstract":"Two continuous maps $f, g : \\mathbb{C}^2\\to\\mathbb{C}^2$ are said to be topologically equivalent if there exist homeomorphisms $\\varphi,\\psi:\\mathbb{C}^2\\to\\mathbb{C}^2$ satisfying $\\psi\\circ f\\circ\\varphi = g$. It is known that there are finitely many topologically non-equivalent polynomial maps $\\mathbb{C}^2\\to\\mathbb{C}^2$ with any given degree $d$. The number $T(d)$ of these topological types is known only whenever $d=2$. In this paper, we describe the topology of generic complex polynomial maps on the plane using the corresponding pair of Newton polytopes and establish a method for constructing topologically non-equivalent maps of degree $d$. We furthermore provide a software implementation of the resulting algorithm, and present lower bounds on $T(d)$ whenever $d=3$ and $d=4$.","sentences":["Two continuous maps $f, g : \\mathbb{C}^2\\to\\mathbb{C}^2$ are said to be topologically equivalent if there exist homeomorphisms $\\varphi,\\psi:\\mathbb{C}^2\\to\\mathbb{C}^2$ satisfying $\\psi\\circ f\\circ\\varphi = g$. It is known that there are finitely many topologically non-equivalent polynomial maps $\\mathbb{C}^2\\to\\mathbb{C}^2$ with any given degree $d$.","The number $T(d)$ of these topological types is known only whenever $d=2$. In this paper, we describe the topology of generic complex polynomial maps on the plane using the corresponding pair of Newton polytopes and establish a method for constructing topologically non-equivalent maps of degree $d$. We furthermore provide a software implementation of the resulting algorithm, and present lower bounds on $T(d)$ whenever $d=3$ and $d=4$."],"url":"http://arxiv.org/abs/2402.08993v1","category":"math.AG"}
{"created":"2024-02-14 07:19:16","title":"Global controllability to harmonic maps of the heat flow from a circle to a sphere","abstract":"In this paper, we study the global controllability and stabilization problems of the harmonic map heat flow from a circle to a sphere. Combining ideas from control theory, heat flow, differential geometry, and asymptotic analysis, we obtain several important properties, such as small-time local controllability, local quantitative rapid stabilization, obstruction to semi-global asymptotic stabilization, and global controllability to geodesics. Surprisingly, due to the geometric feature of the equation we also discover the small-time global controllability between harmonic maps within the same homotopy class for general compact Riemannian manifold targets, which is to be compared with the analogous but longstanding problem for the nonlinear heat equations.","sentences":["In this paper, we study the global controllability and stabilization problems of the harmonic map heat flow from a circle to a sphere.","Combining ideas from control theory, heat flow, differential geometry, and asymptotic analysis, we obtain several important properties, such as small-time local controllability, local quantitative rapid stabilization, obstruction to semi-global asymptotic stabilization, and global controllability to geodesics.","Surprisingly, due to the geometric feature of the equation we also discover the small-time global controllability between harmonic maps within the same homotopy class for general compact Riemannian manifold targets, which is to be compared with the analogous but longstanding problem for the nonlinear heat equations."],"url":"http://arxiv.org/abs/2402.08990v1","category":"math.AP"}
{"created":"2024-02-14 07:10:57","title":"An Algorithmic Meta Theorem for Homomorphism Indistinguishability","abstract":"Two graphs $G$ and $H$ are homomorphism indistinguishable over a family of graphs $\\mathcal{F}$ if for all graphs $F \\in \\mathcal{F}$ the number of homomorphisms from $F$ to $G$ is equal to the number of homomorphism from $F$ to $H$. Many natural equivalence relations comparing graphs such as (quantum) isomorphism, cospectrality, and logical equivalences can be characterised as homomorphism indistinguishability relations over various graph classes.   For a fixed graph class $\\mathcal{F}$, the decision problem HomInd($\\mathcal{F}$) asks to determine whether two input graphs $G$ and $H$ are homomorphism indistinguishable over $\\mathcal{F}$. The problem HomInd($\\mathcal{F}$) is known to be decidable only for few graph classes $\\mathcal{F}$. We show that HomInd($\\mathcal{F}$) admits a randomised polynomial-time algorithm for every graph class $\\mathcal{F}$ of bounded treewidth which is definable in counting monadic second-order logic CMSO2. Thereby, we give the first general algorithm for deciding homomorphism indistinguishability.   This result extends to a version of HomInd where the graph class $\\mathcal{F}$ is specified by a CMSO2-sentence and a bound $k$ on the treewidth, which are given as input. For fixed $k$, this problem is randomised fixed-parameter tractable. If $k$ is part of the input then it is coNP- and coW[1]-hard. Addressing a problem posed by Berkholz (2012), we show coNP-hardness by establishing that deciding indistinguishability under the $k$-dimensional Weisfeiler--Leman algorithm is coNP-hard when $k$ is part of the input.","sentences":["Two graphs $G$ and $H$ are homomorphism indistinguishable over a family of graphs $\\mathcal{F}$ if for all graphs $F \\in \\mathcal{F}$ the number of homomorphisms from $F$ to $G$ is equal to the number of homomorphism from $F$ to $H$. Many natural equivalence relations comparing graphs such as (quantum) isomorphism, cospectrality, and logical equivalences can be characterised as homomorphism indistinguishability relations over various graph classes.   ","For a fixed graph class $\\mathcal{F}$, the decision problem HomInd($\\mathcal{F}$) asks to determine whether two input graphs $G$ and $H$ are homomorphism indistinguishable over $\\mathcal{F}$. The problem HomInd($\\mathcal{F}$) is known to be decidable only for few graph classes $\\mathcal{F}$. We show that HomInd($\\mathcal{F}$) admits a randomised polynomial-time algorithm for every graph class $\\mathcal{F}$ of bounded treewidth which is definable in counting monadic second-order logic CMSO2.","Thereby, we give the first general algorithm for deciding homomorphism indistinguishability.   ","This result extends to a version of HomInd where the graph class $\\mathcal{F}$ is specified by a CMSO2-sentence and a bound $k$ on the treewidth, which are given as input.","For fixed $k$, this problem is randomised fixed-parameter tractable.","If $k$ is part of the input then it is coNP- and coW[1]-hard.","Addressing a problem posed by Berkholz (2012), we show coNP-hardness by establishing that deciding indistinguishability under the $k$-dimensional Weisfeiler--Leman algorithm is coNP-hard when $k$ is part of the input."],"url":"http://arxiv.org/abs/2402.08989v1","category":"cs.LO"}
{"created":"2024-02-14 07:06:30","title":"Multi-modality transrectal ultrasound vudei classification for identification of clinically significant prostate cancer","abstract":"Prostate cancer is the most common noncutaneous cancer in the world. Recently, multi-modality transrectal ultrasound (TRUS) has increasingly become an effective tool for the guidance of prostate biopsies. With the aim of effectively identifying prostate cancer, we propose a framework for the classification of clinically significant prostate cancer (csPCa) from multi-modality TRUS videos. The framework utilizes two 3D ResNet-50 models to extract features from B-mode images and shear wave elastography images, respectively. An adaptive spatial fusion module is introduced to aggregate two modalities' features. An orthogonal regularized loss is further used to mitigate feature redundancy. The proposed framework is evaluated on an in-house dataset containing 512 TRUS videos, and achieves favorable performance in identifying csPCa with an area under curve (AUC) of 0.84. Furthermore, the visualized class activation mapping (CAM) images generated from the proposed framework may provide valuable guidance for the localization of csPCa, thus facilitating the TRUS-guided targeted biopsy. Our code is publicly available at https://github.com/2313595986/ProstateTRUS.","sentences":["Prostate cancer is the most common noncutaneous cancer in the world.","Recently, multi-modality transrectal ultrasound (TRUS) has increasingly become an effective tool for the guidance of prostate biopsies.","With the aim of effectively identifying prostate cancer, we propose a framework for the classification of clinically significant prostate cancer (csPCa) from multi-modality TRUS videos.","The framework utilizes two 3D ResNet-50 models to extract features from B-mode images and shear wave elastography images, respectively.","An adaptive spatial fusion module is introduced to aggregate two modalities' features.","An orthogonal regularized loss is further used to mitigate feature redundancy.","The proposed framework is evaluated on an in-house dataset containing 512 TRUS videos, and achieves favorable performance in identifying csPCa with an area under curve (AUC) of 0.84.","Furthermore, the visualized class activation mapping (CAM) images generated from the proposed framework may provide valuable guidance for the localization of csPCa, thus facilitating the TRUS-guided targeted biopsy.","Our code is publicly available at https://github.com/2313595986/ProstateTRUS."],"url":"http://arxiv.org/abs/2402.08987v1","category":"eess.IV"}
{"created":"2024-02-14 06:54:31","title":"SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding","abstract":"As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their token probabilities, while simultaneously attenuating the probabilities of token sequences that are aligned with the objectives of jailbreak attacks. We perform extensive experiments on five LLMs using six state-of-the-art jailbreak attacks and four benchmark datasets. Our results show that SafeDecoding significantly reduces the attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries. SafeDecoding outperforms six defense methods.","sentences":["As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety.","Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat.","In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries.","Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order.","This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their token probabilities, while simultaneously attenuating the probabilities of token sequences that are aligned with the objectives of jailbreak attacks.","We perform extensive experiments on five LLMs using six state-of-the-art jailbreak attacks and four benchmark datasets.","Our results show that SafeDecoding significantly reduces the attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries.","SafeDecoding outperforms six defense methods."],"url":"http://arxiv.org/abs/2402.08983v1","category":"cs.CR"}
{"created":"2024-02-14 06:51:49","title":"MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional Feature Selection","abstract":"Feature selection is a crucial step in data mining to enhance model performance by reducing data dimensionality. However, the increasing dimensionality of collected data exacerbates the challenge known as the \"curse of dimensionality\", where computation grows exponentially with the number of dimensions. To tackle this issue, evolutionary computational (EC) approaches have gained popularity due to their simplicity and applicability. Unfortunately, the diverse designs of EC methods result in varying abilities to handle different data, often underutilizing and not sharing information effectively. In this paper, we propose a novel approach called PSO-based Multi-task Evolutionary Learning (MEL) that leverages multi-task learning to address these challenges. By incorporating information sharing between different feature selection tasks, MEL achieves enhanced learning ability and efficiency. We evaluate the effectiveness of MEL through extensive experiments on 22 high-dimensional datasets. Comparing against 24 EC approaches, our method exhibits strong competitiveness. Additionally, we have open-sourced our code on GitHub at https://github.com/wangxb96/MEL.","sentences":["Feature selection is a crucial step in data mining to enhance model performance by reducing data dimensionality.","However, the increasing dimensionality of collected data exacerbates the challenge known as the \"curse of dimensionality\", where computation grows exponentially with the number of dimensions.","To tackle this issue, evolutionary computational (EC) approaches have gained popularity due to their simplicity and applicability.","Unfortunately, the diverse designs of EC methods result in varying abilities to handle different data, often underutilizing and not sharing information effectively.","In this paper, we propose a novel approach called PSO-based Multi-task Evolutionary Learning (MEL) that leverages multi-task learning to address these challenges.","By incorporating information sharing between different feature selection tasks, MEL achieves enhanced learning ability and efficiency.","We evaluate the effectiveness of MEL through extensive experiments on 22 high-dimensional datasets.","Comparing against 24 EC approaches, our method exhibits strong competitiveness.","Additionally, we have open-sourced our code on GitHub at https://github.com/wangxb96/MEL."],"url":"http://arxiv.org/abs/2402.08982v1","category":"cs.LG"}
{"created":"2024-02-14 06:49:23","title":"Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing","abstract":"In smart manufacturing systems (SMSs), flexible job-shop scheduling with transportation constraints (FJSPT) is essential to optimize solutions for maximizing productivity, considering production flexibility based on automated guided vehicles (AGVs). Recent developments in deep reinforcement learning (DRL)-based methods for FJSPT have encountered a scale generalization challenge. These methods underperform when applied to environment at scales different from their training set, resulting in low-quality solutions. To address this, we introduce a novel graph-based DRL method, named the Heterogeneous Graph Scheduler (HGS). Our method leverages locally extracted relational knowledge among operations, machines, and vehicle nodes for scheduling, with a graph-structured decision-making framework that reduces encoding complexity and enhances scale generalization. Our performance evaluation, conducted with benchmark datasets, reveals that the proposed method outperforms traditional dispatching rules, meta-heuristics, and existing DRL-based approaches in terms of makespan performance, even on large-scale instances that have not been experienced during training.","sentences":["In smart manufacturing systems (SMSs), flexible job-shop scheduling with transportation constraints (FJSPT) is essential to optimize solutions for maximizing productivity, considering production flexibility based on automated guided vehicles (AGVs).","Recent developments in deep reinforcement learning (DRL)-based methods for FJSPT have encountered a scale generalization challenge.","These methods underperform when applied to environment at scales different from their training set, resulting in low-quality solutions.","To address this, we introduce a novel graph-based DRL method, named the Heterogeneous Graph Scheduler (HGS).","Our method leverages locally extracted relational knowledge among operations, machines, and vehicle nodes for scheduling, with a graph-structured decision-making framework that reduces encoding complexity and enhances scale generalization.","Our performance evaluation, conducted with benchmark datasets, reveals that the proposed method outperforms traditional dispatching rules, meta-heuristics, and existing DRL-based approaches in terms of makespan performance, even on large-scale instances that have not been experienced during training."],"url":"http://arxiv.org/abs/2402.08979v1","category":"eess.SY"}
{"created":"2024-02-14 06:47:30","title":"Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks","abstract":"Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.","sentences":["Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks.","However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge.","We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively.","Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation.","Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations.","Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts."],"url":"http://arxiv.org/abs/2402.08978v1","category":"cs.HC"}
{"created":"2024-02-14 06:43:44","title":"Derivative sampling expansions in shift-invariant spaces with error estimates covering discontinuous signals","abstract":"This paper is concerned with the problem of sampling and interpolation involving derivatives in shift-invariant spaces and the error analysis of the derivative sampling expansions for fundamentally large classes of functions. A new type of polynomials based on derivative samples is introduced, which is different from the Euler-Frobenius polynomials for the multiplicity $r>1$. A complete characterization of uniform sampling with derivatives is given using Laurent operators. The rate of approximation of a signal (not necessarily continuous) by the derivative sampling expansions in shift-invariant spaces generated by compactly supported functions is established in terms of $L^p$- average modulus of smoothness. Finally, several typical examples illustrating the various problems are discussed in detail.","sentences":["This paper is concerned with the problem of sampling and interpolation involving derivatives in shift-invariant spaces and the error analysis of the derivative sampling expansions for fundamentally large classes of functions.","A new type of polynomials based on derivative samples is introduced, which is different from the Euler-Frobenius polynomials for the multiplicity $r>1$. A complete characterization of uniform sampling with derivatives is given using Laurent operators.","The rate of approximation of a signal (not necessarily continuous) by the derivative sampling expansions in shift-invariant spaces generated by compactly supported functions is established in terms of $L^p$- average modulus of smoothness.","Finally, several typical examples illustrating the various problems are discussed in detail."],"url":"http://arxiv.org/abs/2402.08977v1","category":"math.FA"}
{"created":"2024-02-14 18:24:27","title":"Limitless FaaS: Overcoming serverless functions execution time limits with invoke driven architecture and memory checkpoints","abstract":"Function-as-a-Service (FaaS) allows to directly submit function code to a cloud provider without the burden of managing infrastructure resources. Each cloud provider establishes execution time limits to their FaaS offerings, which impose the risk of spending computation time without achieving partial results. In this work, a framework that enables limitless execution time in FaaS, with little to no modifications to the user-provided function code, is presented. After a thorough literature and theoretical framework review, Apache OpenWhisk Actions and the DMCTP checkpoint-and-restore (CR) tool were selected. With these, dependent successive serverless same-function invocations that exploit the persistence of partial results were implemented. The solution was submitted to the FaaSDom benchmark and time metrics were collected. Additionally, the solution was characterized in terms of the Serverless Trilemma. The resultant system, even at this proof-of-concept state, offers a lot of value to companies that rely heavily on serverless architecture.","sentences":["Function-as-a-Service (FaaS) allows to directly submit function code to a cloud provider without the burden of managing infrastructure resources.","Each cloud provider establishes execution time limits to their FaaS offerings, which impose the risk of spending computation time without achieving partial results.","In this work, a framework that enables limitless execution time in FaaS, with little to no modifications to the user-provided function code, is presented.","After a thorough literature and theoretical framework review, Apache OpenWhisk Actions and the DMCTP checkpoint-and-restore (CR) tool were selected.","With these, dependent successive serverless same-function invocations that exploit the persistence of partial results were implemented.","The solution was submitted to the FaaSDom benchmark and time metrics were collected.","Additionally, the solution was characterized in terms of the Serverless Trilemma.","The resultant system, even at this proof-of-concept state, offers a lot of value to companies that rely heavily on serverless architecture."],"url":"http://arxiv.org/abs/2402.09377v1","category":"cs.DC"}
{"created":"2024-02-14 18:16:54","title":"Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking","abstract":"Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages. Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models. Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain.","sentences":["Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions.","Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition.","Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages.","Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction.","Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models.","Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain."],"url":"http://arxiv.org/abs/2402.09369v1","category":"cs.CL"}
{"created":"2024-02-14 18:13:37","title":"Prediction of Activated Sludge Settling Characteristics from Microscopy Images with Deep Convolutional Neural Networks and Transfer Learning","abstract":"Microbial communities play a key role in biological wastewater treatment processes. Activated sludge settling characteristics, for example, are affected by microbial community composition, varying by changes in operating conditions and influent characteristics of wastewater treatment plants (WWTPs). Timely assessment and prediction of changes in microbial composition leading to settling problems, such as filamentous bulking (FB), can prevent operational challenges, reductions in treatment efficiency, and adverse environmental impacts. This study presents an innovative computer vision-based approach to assess activated sludge-settling characteristics based on the morphological properties of flocs and filaments in microscopy images. Implementing the transfer learning of deep convolutional neural network (CNN) models, this approach aims to overcome the limitations of existing quantitative image analysis techniques. The offline microscopy image dataset was collected over two years, with weekly sampling at a full-scale industrial WWTP in Belgium. Multiple data augmentation techniques were employed to enhance the generalizability of the CNN models. Various CNN architectures, including Inception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested to evaluate their performance in predicting sludge settling characteristics. The sludge volume index was used as the final prediction variable, but the method can easily be adjusted to predict any other settling metric of choice. The results showed that the suggested CNN-based approach provides less labour-intensive, objective, and consistent assessments, while transfer learning notably minimises the training phase, resulting in a generalizable system that can be employed in real-time applications.","sentences":["Microbial communities play a key role in biological wastewater treatment processes.","Activated sludge settling characteristics, for example, are affected by microbial community composition, varying by changes in operating conditions and influent characteristics of wastewater treatment plants (WWTPs).","Timely assessment and prediction of changes in microbial composition leading to settling problems, such as filamentous bulking (FB), can prevent operational challenges, reductions in treatment efficiency, and adverse environmental impacts.","This study presents an innovative computer vision-based approach to assess activated sludge-settling characteristics based on the morphological properties of flocs and filaments in microscopy images.","Implementing the transfer learning of deep convolutional neural network (CNN) models, this approach aims to overcome the limitations of existing quantitative image analysis techniques.","The offline microscopy image dataset was collected over two years, with weekly sampling at a full-scale industrial WWTP in Belgium.","Multiple data augmentation techniques were employed to enhance the generalizability of the CNN models.","Various CNN architectures, including Inception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested to evaluate their performance in predicting sludge settling characteristics.","The sludge volume index was used as the final prediction variable, but the method can easily be adjusted to predict any other settling metric of choice.","The results showed that the suggested CNN-based approach provides less labour-intensive, objective, and consistent assessments, while transfer learning notably minimises the training phase, resulting in a generalizable system that can be employed in real-time applications."],"url":"http://arxiv.org/abs/2402.09367v1","category":"cs.CV"}
{"created":"2024-02-14 16:41:35","title":"Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code","abstract":"Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM's training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.","sentences":["Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources.","The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing.","The dataset for training these models is mainly collected from publicly available sources.","This raises the issue of intellectual property infringement as developers' codes are already included in the dataset.","Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models.","Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement.","To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM's training dataset.","We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion.","In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM.","In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%.","In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets."],"url":"http://arxiv.org/abs/2402.09299v1","category":"cs.SE"}
{"created":"2024-02-14 16:09:34","title":"Nonreciprocal collective dynamics in a mixture of phoretic Janus colloids","abstract":"A multicomponent mixture of Janus colloids with distinct catalytic coats and phoretic mobilities is a promising theoretical system to explore the collective behavior arising from nonreciprocal interactions. An active colloid produces (or consumes) chemicals, self-propels, drifts along chemical gradients, and rotates its intrinsic polarity to align with a gradient. As a result the connection from microscopics to continuum theories through coarse-graining couples densities and polarization fields in unique ways. Focusing on a binary mixture, we show that these couplings render the unpatterned reference state unstable to small perturbations through a variety of instabilities including oscillatory ones which arise on crossing an exceptional point or through a Hopf bifurcation. For fast relaxation of the polar fields, they can be eliminated in favor of the density fields to obtain a microscopic realization of the Nonreciprocal Cahn-Hilliard model for two conserved species with two distinct sources of non-reciprocity, one in the interaction coefficient and the other in the interfacial tension. Our work establishes Janus colloids as a versatile model for a bottom-up approach to both scalar and polar active mixtures.","sentences":["A multicomponent mixture of Janus colloids with distinct catalytic coats and phoretic mobilities is a promising theoretical system to explore the collective behavior arising from nonreciprocal interactions.","An active colloid produces (or consumes) chemicals, self-propels, drifts along chemical gradients, and rotates its intrinsic polarity to align with a gradient.","As a result the connection from microscopics to continuum theories through coarse-graining couples densities and polarization fields in unique ways.","Focusing on a binary mixture, we show that these couplings render the unpatterned reference state unstable to small perturbations through a variety of instabilities including oscillatory ones which arise on crossing an exceptional point or through a Hopf bifurcation.","For fast relaxation of the polar fields, they can be eliminated in favor of the density fields to obtain a microscopic realization of the Nonreciprocal Cahn-Hilliard model for two conserved species with two distinct sources of non-reciprocity, one in the interaction coefficient and the other in the interfacial tension.","Our work establishes Janus colloids as a versatile model for a bottom-up approach to both scalar and polar active mixtures."],"url":"http://arxiv.org/abs/2402.09279v1","category":"cond-mat.soft"}
{"created":"2024-02-14 15:40:31","title":"On collection schemes and Gaifman's splitting theorem","abstract":"We study model theoretic characterizations of various collection schemes over $\\mathbf{PA}^-$ from the viewpoint of Gaifman's splitting theorem.","sentences":["We study model theoretic characterizations of various collection schemes over $\\mathbf{PA}^-$ from the viewpoint of Gaifman's splitting theorem."],"url":"http://arxiv.org/abs/2402.09255v1","category":"math.LO"}
{"created":"2024-02-14 14:57:25","title":"A case study of university student networks and the COVID-19 pandemic using a social network analysis approach in halls of residence","abstract":"The COVID-19 pandemic has meant that young university students have had to adapt their learning and have a reduced relational context. Adversity contexts build models of human behaviour based on relationships. However, there is a lack of studies that analyse the behaviour of university students based on their social structure in the context of a pandemic. This information could be useful in making decisions on how to plan collective responses to adversities. The Social Network Analysis (SNA) method has been chosen to address this structural perspective. The aim of our research is to describe the structural behaviour of students in university residences during the COVID-19 pandemic with a more in-depth analysis of student leaders. A descriptive cross-sectional study was carried out at one Spanish Public University, Le\\'on, from 23th October 2020 to 20th November 2020. The participation was of 93 students, from four halls of residence. The data were collected from a database created specifically at the university to \"track\" contacts in the COVID-19 pandemic, SiVeUle. We applied the SNA for the analysis of the data. The leadership on the university residence was measured using centrality measures. The top leaders were analyzed using the Egonetwork and an assessment of the key players. Students with higher social reputations experience higher levels of pandemic contagion in relation to COVID-19 infection. The results were statistically significant between the centrality in the network and the results of the COVID-19 infection. The most leading students showed a high degree of Betweenness, and three students had the key player structure in the network. Networking behaviour of university students in halls of residence could be related to contagion in the COVID-19 pandemic.","sentences":["The COVID-19 pandemic has meant that young university students have had to adapt their learning and have a reduced relational context.","Adversity contexts build models of human behaviour based on relationships.","However, there is a lack of studies that analyse the behaviour of university students based on their social structure in the context of a pandemic.","This information could be useful in making decisions on how to plan collective responses to adversities.","The Social Network Analysis (SNA) method has been chosen to address this structural perspective.","The aim of our research is to describe the structural behaviour of students in university residences during the COVID-19 pandemic with a more in-depth analysis of student leaders.","A descriptive cross-sectional study was carried out at one Spanish Public University, Le\\'on, from 23th October 2020 to 20th November 2020.","The participation was of 93 students, from four halls of residence.","The data were collected from a database created specifically at the university to \"track\" contacts in the COVID-19 pandemic, SiVeUle.","We applied the SNA for the analysis of the data.","The leadership on the university residence was measured using centrality measures.","The top leaders were analyzed using the Egonetwork and an assessment of the key players.","Students with higher social reputations experience higher levels of pandemic contagion in relation to COVID-19 infection.","The results were statistically significant between the centrality in the network and the results of the COVID-19 infection.","The most leading students showed a high degree of Betweenness, and three students had the key player structure in the network.","Networking behaviour of university students in halls of residence could be related to contagion in the COVID-19 pandemic."],"url":"http://arxiv.org/abs/2402.09219v1","category":"cs.CY"}
{"created":"2024-02-14 13:55:59","title":"Non-contact in situ multi-diagnostic NMR/dielectric spectroscopy","abstract":"Introduction of a dielectric material in an NMR probe head modifies the frequency response of the probe circuit, a phenomenon revealed by the detuning of the probe. For NMR spectroscopy, this detuning is corrected for by tuning and matching the probe head prior to the NMR measurement. The magnitude of the probe detuning - the dielectric shift - provides direct access to the dielectric properties of the sample, enabling NMR spectrometers to simultaneously perform both dielectric and NMR spectroscopy. By measuring sample permittivity as function of frequency, permittivity spectroscopy can be performed using the new methodology. As a proof concept, this was evaluated on methanol, ethanol, 1-propanol, 1-pentanol and 1-octanol using a commercial CPMAS NMR probe head. The results accurately match literature data collected by standard dielectric spectroscopy techniques. Subsequently, the method was also applied to investigate the solvent-surface interactions of water confined in the micropores of an MFI-type, hydrophilic zeolite with Si/Al ratio of 11.5. In the micropores, water adsorbs to Br{\\o}nsted acid sites and defect sites, resulting in a drastically decreased dielectric permittivity of the nano-confined water. A theoretical background for the new methodology is provided using an effective electric circuit model of a CPMAS probe head with solenoid coil, describing the detuning resulting from insertion of dielectric samples in the probe head.","sentences":["Introduction of a dielectric material in an NMR probe head modifies the frequency response of the probe circuit, a phenomenon revealed by the detuning of the probe.","For NMR spectroscopy, this detuning is corrected for by tuning and matching the probe head prior to the NMR measurement.","The magnitude of the probe detuning - the dielectric shift - provides direct access to the dielectric properties of the sample, enabling NMR spectrometers to simultaneously perform both dielectric and NMR spectroscopy.","By measuring sample permittivity as function of frequency, permittivity spectroscopy can be performed using the new methodology.","As a proof concept, this was evaluated on methanol, ethanol, 1-propanol, 1-pentanol and 1-octanol using a commercial CPMAS NMR probe head.","The results accurately match literature data collected by standard dielectric spectroscopy techniques.","Subsequently, the method was also applied to investigate the solvent-surface interactions of water confined in the micropores of an MFI-type, hydrophilic zeolite with Si/Al ratio of 11.5.","In the micropores, water adsorbs to Br{\\o}nsted acid sites and defect sites, resulting in a drastically decreased dielectric permittivity of the nano-confined water.","A theoretical background for the new methodology is provided using an effective electric circuit model of a CPMAS probe head with solenoid coil, describing the detuning resulting from insertion of dielectric samples in the probe head."],"url":"http://arxiv.org/abs/2402.09183v1","category":"physics.ins-det"}
{"created":"2024-02-14 12:28:14","title":"Recommendation Algorithm Based on Recommendation Sessions","abstract":"The enormous development of the Internet, both in the geographical scale and in the area of using its possibilities in everyday life, determines the creation and collection of huge amounts of data. Due to the scale, it is not possible to analyse them using traditional methods, therefore it makes a necessary to use modern methods and techniques. Such methods are provided, among others, by the area of recommendations. The aim of this study is to present a new algorithm in the area of recommendation systems, the algorithm based on data from various sets of information, both static (categories of objects, features of objects) and dynamic (user behaviour).","sentences":["The enormous development of the Internet, both in the geographical scale and in the area of using its possibilities in everyday life, determines the creation and collection of huge amounts of data.","Due to the scale, it is not possible to analyse them using traditional methods, therefore it makes a necessary to use modern methods and techniques.","Such methods are provided, among others, by the area of recommendations.","The aim of this study is to present a new algorithm in the area of recommendation systems, the algorithm based on data from various sets of information, both static (categories of objects, features of objects) and dynamic (user behaviour)."],"url":"http://arxiv.org/abs/2402.09130v1","category":"cs.IR"}
{"created":"2024-02-14 10:33:33","title":"Selective decision making and collective behavior of fish by the motion of visual attention","abstract":"Collective motion provides a spectacular example of self-organization in Nature. Visual information plays a crucial role among various types of information in determining interactions. Recently, experiments have revealed that organisms such as fish and insects selectively utilize a portion, rather than the entirety, of visual information. Here, focusing on fish, we propose an agent-based model where the direction of attention is guided by visual stimuli received from the images of nearby fish. Our model reproduces a branching phenomenon where a fish selectively follows a specific individual as the distance between two or three nearby fish increases. Furthermore, our model replicates various patterns of collective motion in a group of agents, such as vortex, polarized school, swarm, and turning. We also discuss the topological nature of visual interaction, as well as the positional distribution of nearby fish and the map of pairwise and three-body interactions induced by them. Through a comprehensive comparison with existing experimental results, we clarify the roles of visual interactions and issues to be resolved by other forms of interactions.","sentences":["Collective motion provides a spectacular example of self-organization in Nature.","Visual information plays a crucial role among various types of information in determining interactions.","Recently, experiments have revealed that organisms such as fish and insects selectively utilize a portion, rather than the entirety, of visual information.","Here, focusing on fish, we propose an agent-based model where the direction of attention is guided by visual stimuli received from the images of nearby fish.","Our model reproduces a branching phenomenon where a fish selectively follows a specific individual as the distance between two or three nearby fish increases.","Furthermore, our model replicates various patterns of collective motion in a group of agents, such as vortex, polarized school, swarm, and turning.","We also discuss the topological nature of visual interaction, as well as the positional distribution of nearby fish and the map of pairwise and three-body interactions induced by them.","Through a comprehensive comparison with existing experimental results, we clarify the roles of visual interactions and issues to be resolved by other forms of interactions."],"url":"http://arxiv.org/abs/2402.09073v1","category":"nlin.AO"}
{"created":"2024-02-14 09:06:50","title":"Understanding Stress, Burnout, and Behavioral Patterns in Medical Residents Using Large-scale Longitudinal Wearable Recordings","abstract":"Medical residency training is often associated with physically intense and emotionally demanding tasks, requiring them to engage in extended working hours providing complex clinical care. Residents are hence susceptible to negative psychological effects, including stress and anxiety, that can lead to decreased well-being, affecting them achieving desired training outcomes. Understanding the daily behavioral patterns of residents can guide the researchers to identify the source of stress in residency training, offering unique opportunities to improve residency programs. In this study, we investigate the workplace behavioral patterns of 43 medical residents across different stages of their training, using longitudinal wearable recordings collected over a 3-week rotation. Specifically, we explore their ambulatory patterns, the computer access, and the interactions with mentors of residents. Our analysis reveals that residents showed distinct working behaviors in walking movement patterns and computer usage compared to different years in the program. Moreover, we identify that interaction patterns with mentoring doctors indicate stress, burnout, and job satisfaction.","sentences":["Medical residency training is often associated with physically intense and emotionally demanding tasks, requiring them to engage in extended working hours providing complex clinical care.","Residents are hence susceptible to negative psychological effects, including stress and anxiety, that can lead to decreased well-being, affecting them achieving desired training outcomes.","Understanding the daily behavioral patterns of residents can guide the researchers to identify the source of stress in residency training, offering unique opportunities to improve residency programs.","In this study, we investigate the workplace behavioral patterns of 43 medical residents across different stages of their training, using longitudinal wearable recordings collected over a 3-week rotation.","Specifically, we explore their ambulatory patterns, the computer access, and the interactions with mentors of residents.","Our analysis reveals that residents showed distinct working behaviors in walking movement patterns and computer usage compared to different years in the program.","Moreover, we identify that interaction patterns with mentoring doctors indicate stress, burnout, and job satisfaction."],"url":"http://arxiv.org/abs/2402.09028v1","category":"cs.CY"}
{"created":"2024-02-14 06:39:54","title":"Research and application of Transformer based anomaly detection model: A literature review","abstract":"Transformer, as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of anomaly detection. To inspire research on Transformer-based anomaly detection, this review offers a fresh perspective on the concept of anomaly detection. We explore the current challenges of anomaly detection and provide detailed insights into the operating principles of Transformer and its variants in anomaly detection tasks. Additionally, we delineate various application scenarios for Transformer-based anomaly detection models and discuss the datasets and evaluation metrics employed. Furthermore, this review highlights the key challenges in Transformer-based anomaly detection research and conducts a comprehensive analysis of future research trends in this domain. The review includes an extensive compilation of over 100 core references related to Transformer-based anomaly detection. To the best of our knowledge, this is the first comprehensive review that focuses on the research related to Transformer in the context of anomaly detection. We hope that this paper can provide detailed technical information to researchers interested in Transformer-based anomaly detection tasks.","sentences":["Transformer, as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of anomaly detection.","To inspire research on Transformer-based anomaly detection, this review offers a fresh perspective on the concept of anomaly detection.","We explore the current challenges of anomaly detection and provide detailed insights into the operating principles of Transformer and its variants in anomaly detection tasks.","Additionally, we delineate various application scenarios for Transformer-based anomaly detection models and discuss the datasets and evaluation metrics employed.","Furthermore, this review highlights the key challenges in Transformer-based anomaly detection research and conducts a comprehensive analysis of future research trends in this domain.","The review includes an extensive compilation of over 100 core references related to Transformer-based anomaly detection.","To the best of our knowledge, this is the first comprehensive review that focuses on the research related to Transformer in the context of anomaly detection.","We hope that this paper can provide detailed technical information to researchers interested in Transformer-based anomaly detection tasks."],"url":"http://arxiv.org/abs/2402.08975v1","category":"cs.LG"}
{"created":"2024-02-14 06:38:40","title":"Examining the Unique Online Risk Experiences and Mental Health Outcomes of LGBTQ+ versus Heterosexual Youth","abstract":"We collected and analyzed Instagram direct messages (DMs) from 173 youth aged 13-21 (including 86 LGBTQ+ youth). We examined youth's risk-flagged social media trace data with their self-reported mental health outcomes to examine how the differing online experiences of LGBTQ+ youth compare with their heterosexual counterparts. We found that LGBTQ+ youth experienced significantly more high-risk online interactions compared to heterosexual youth. LGBTQ+ youth reported overall poorer mental health, with online harassment specifically amplifying Self-Harm and Injury. LGBTQ+ youth's mental well-being linked positively to sexual messages, unlike heterosexual youth. Qualitatively, we found that most of the risk-flagged messages of LGBTQ+ youth were sexually motivated; however, a silver lining was that they sought support for their sexual identity from peers on the platform. The study highlights the importance of tailored online safety and inclusive design for LGBTQ+ youth, with implications for CHI community advancements in fostering a supportive online environments.","sentences":["We collected and analyzed Instagram direct messages (DMs) from 173 youth aged 13-21 (including 86 LGBTQ+ youth).","We examined youth's risk-flagged social media trace data with their self-reported mental health outcomes to examine how the differing online experiences of LGBTQ+ youth compare with their heterosexual counterparts.","We found that LGBTQ+ youth experienced significantly more high-risk online interactions compared to heterosexual youth.","LGBTQ+ youth reported overall poorer mental health, with online harassment specifically amplifying Self-Harm and Injury.","LGBTQ+ youth's mental well-being linked positively to sexual messages, unlike heterosexual youth.","Qualitatively, we found that most of the risk-flagged messages of LGBTQ+ youth were sexually motivated; however, a silver lining was that they sought support for their sexual identity from peers on the platform.","The study highlights the importance of tailored online safety and inclusive design for LGBTQ+ youth, with implications for CHI community advancements in fostering a supportive online environments."],"url":"http://arxiv.org/abs/2402.08974v1","category":"cs.HC"}
{"created":"2024-02-14 06:25:50","title":"GrounDial: Human-norm Grounded Safe Dialog Response Generation","abstract":"Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses, agreeing to offensive user input or including toxic content. Previous research aimed to alleviate the toxicity, by fine-tuning LLM with manually annotated safe dialogue histories. However, the dependency on additional tuning requires substantial costs. To remove the dependency, we propose GrounDial, where response safety is achieved by grounding responses to commonsense social rules without requiring fine-tuning. A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning.","sentences":["Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses, agreeing to offensive user input or including toxic content.","Previous research aimed to alleviate the toxicity, by fine-tuning LLM with manually annotated safe dialogue histories.","However, the dependency on additional tuning requires substantial costs.","To remove the dependency, we propose GrounDial, where response safety is achieved by grounding responses to commonsense social rules without requiring fine-tuning.","A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning."],"url":"http://arxiv.org/abs/2402.08968v1","category":"cs.AI"}
{"created":"2024-02-14 06:09:36","title":"DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning","abstract":"Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources. On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information. To address class imbalances cost-efficiently, we propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory. The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances. We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its robustness and providing reliable results in downstream tasks. We also analyze the role of the DUEL policy in the training process through various metrics and visualizations.","sentences":["Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources.","On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information.","To address class imbalances cost-efficiently, we propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL).","This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory.","The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances.","We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its robustness and providing reliable results in downstream tasks.","We also analyze the role of the DUEL policy in the training process through various metrics and visualizations."],"url":"http://arxiv.org/abs/2402.08963v1","category":"cs.LG"}
{"created":"2024-02-14 06:05:37","title":"HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding","abstract":"Existing knowledge hypergraph embedding methods mainly focused on improving model performance, but their model structures are becoming more complex and redundant. Furthermore, due to the inherent complex semantic knowledge, the computation of knowledge hypergraph embedding models is often very expensive, leading to low efficiency. In this paper, we propose a feature interaction and extraction-enhanced 3D circular convolutional embedding model, HyCubE, which designs a novel 3D circular convolutional neural network and introduces the alternate mask stack strategy to achieve efficient n-ary knowledge hypergraph embedding. By adaptively adjusting the 3D circular convolution kernel size and uniformly embedding the entity position information, HyCubE improves the model performance with fewer parameters and reaches a better trade-off between model performance and efficiency. In addition, we use 1-N multilinear scoring based on the entity mask mechanism to further accelerate the model training efficiency. Finally, extensive experimental results on all datasets demonstrate that HyCubE consistently outperforms state-of-the-art baselines, with an average improvement of 4.08%-10.77% and a maximum improvement of 21.16% across all metrics. Commendably, HyCubE speeds up by an average of 7.55x and reduces memory usage by an average of 77.02% compared to the latest state-of-the-art baselines.","sentences":["Existing knowledge hypergraph embedding methods mainly focused on improving model performance, but their model structures are becoming more complex and redundant.","Furthermore, due to the inherent complex semantic knowledge, the computation of knowledge hypergraph embedding models is often very expensive, leading to low efficiency.","In this paper, we propose a feature interaction and extraction-enhanced 3D circular convolutional embedding model, HyCubE, which designs a novel 3D circular convolutional neural network and introduces the alternate mask stack strategy to achieve efficient n-ary knowledge hypergraph embedding.","By adaptively adjusting the 3D circular convolution kernel size and uniformly embedding the entity position information, HyCubE improves the model performance with fewer parameters and reaches a better trade-off between model performance and efficiency.","In addition, we use 1-N multilinear scoring based on the entity mask mechanism to further accelerate the model training efficiency.","Finally, extensive experimental results on all datasets demonstrate that HyCubE consistently outperforms state-of-the-art baselines, with an average improvement of 4.08%-10.77% and a maximum improvement of 21.16% across all metrics.","Commendably, HyCubE speeds up by an average of 7.55x and reduces memory usage by an average of 77.02% compared to the latest state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.08961v1","category":"cs.AI"}
{"created":"2024-02-14 06:01:44","title":"Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision","abstract":"Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding space. However, the inherent noise in the correspondence between masks and entities poses a significant challenge when obtaining reliable pairs. In light of this, we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities. Compared to text-only weakly-supervised methods, our Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K datasets, and even surpasses fully-supervised methods on the challenging PASCAL Context-459 dataset.","sentences":["Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios.","Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance.","In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively.","With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions.","Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding space.","However, the inherent noise in the correspondence between masks and entities poses a significant challenge when obtaining reliable pairs.","In light of this, we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities.","Compared to text-only weakly-supervised methods, our Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K datasets, and even surpasses fully-supervised methods on the challenging PASCAL Context-459 dataset."],"url":"http://arxiv.org/abs/2402.08960v1","category":"cs.CV"}
{"created":"2024-02-14 05:58:43","title":"Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers","abstract":"With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive experiments on various language models and complexity analysis, we demonstrate that aespa is accurate and efficient in quantizing Transformer models.","sentences":["With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs.","Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required.","As a cost-effective alternative, one-shot PTQ schemes have been proposed.","Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers.","In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency.","The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score.","Through extensive experiments on various language models and complexity analysis, we demonstrate that aespa is accurate and efficient in quantizing Transformer models."],"url":"http://arxiv.org/abs/2402.08958v1","category":"cs.LG"}
{"created":"2024-02-14 05:57:58","title":"MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data","abstract":"Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-wise formal solutions. (3) Lastly, the framework utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE with 5,866 valid data points. Each data point contains an informal statement, an informal proof, and a translated formal proof that passes the prover validation. We perform extensive analysis and demonstrate that MUSTARD generates validated high-quality step-by-step data. We further apply the MUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B achieves a 15.41% average relative performance gain in automated theorem proving, and 8.18% in math word problems. Codes and data are available at https://github.com/Eleanor-H/MUSTARD.","sentences":["Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving.","As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges.","Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance.","However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks.","To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity.","MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category.","(2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-wise formal solutions.","(3) Lastly, the framework utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs.","With the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE with 5,866 valid data points.","Each data point contains an informal statement, an informal proof, and a translated formal proof that passes the prover validation.","We perform extensive analysis and demonstrate that MUSTARD generates validated high-quality step-by-step data.","We further apply the MUSTARDSAUCE for fine-tuning smaller language models.","The fine-tuned Llama 2-7B achieves a 15.41% average relative performance gain in automated theorem proving, and 8.18% in math word problems.","Codes and data are available at https://github.com/Eleanor-H/MUSTARD."],"url":"http://arxiv.org/abs/2402.08957v1","category":"cs.AI"}
{"created":"2024-02-14 05:52:23","title":"Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models","abstract":"Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak, & Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of \"counterfactual\" variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.","sentences":["Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities.","However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data.","Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak, & Lu, 2023).","We take one set of analogy problems used to evaluate LLMs and create a set of \"counterfactual\" variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data.","We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set.","This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making."],"url":"http://arxiv.org/abs/2402.08955v1","category":"cs.AI"}
{"created":"2024-02-14 04:50:18","title":"Premise Order Matters in Reasoning with Large Language Models","abstract":"Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.","sentences":["Large language models (LLMs) have accomplished remarkable reasoning performance in various domains.","However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task.","In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps.","For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy.","We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%.","In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark."],"url":"http://arxiv.org/abs/2402.08939v1","category":"cs.AI"}
{"created":"2024-02-14 04:21:11","title":"Listening to Multi-talker Conversations: Modular and End-to-end Perspectives","abstract":"Since the first speech recognition systems were built more than 30 years ago, improvement in voice technology has enabled applications such as smart assistants and automated customer support. However, conversation intelligence of the future requires recognizing free-flowing multi-party conversations, which is a crucial and challenging component that still remains unsolved. In this dissertation, we focus on this problem of speaker-attributed multi-talker speech recognition, and propose two perspectives which result from its probabilistic formulation.   In the modular perspective, we build a pipeline of sub-tasks involving speaker diarization, target speaker extraction, and speech recognition. Our first contribution is a method to perform overlap-aware diarization by reformulating spectral clustering as a constrained optimization problem. We also describe an algorithm to ensemble diarization outputs, either to combine overlap-aware systems or to perform multi-channel diarization by late fusion. Once speaker segments are identified, we robustly extract single-speaker utterances from the mixture using a GPU-accelerated implementation of guided source separation, which allows us to use an off-the-shelf ASR system to obtain speaker-attributed transcripts.   Since the modular approach suffers from error propagation, we propose an alternate \"end-to-end\" perspective on the problem. For this, we describe the Streaming Unmixing and Recognition Transducer (SURT). We show how to train SURT models efficiently by carefully designing the network architecture, objective functions, and mixture simulation techniques. Finally, we add an auxiliary speaker branch to enable joint prediction of speaker labels synchronized with the speech tokens. We demonstrate that training on synthetic mixtures and adapting with real data helps these models transfer well for streaming transcription of real meeting sessions.","sentences":["Since the first speech recognition systems were built more than 30 years ago, improvement in voice technology has enabled applications such as smart assistants and automated customer support.","However, conversation intelligence of the future requires recognizing free-flowing multi-party conversations, which is a crucial and challenging component that still remains unsolved.","In this dissertation, we focus on this problem of speaker-attributed multi-talker speech recognition, and propose two perspectives which result from its probabilistic formulation.   ","In the modular perspective, we build a pipeline of sub-tasks involving speaker diarization, target speaker extraction, and speech recognition.","Our first contribution is a method to perform overlap-aware diarization by reformulating spectral clustering as a constrained optimization problem.","We also describe an algorithm to ensemble diarization outputs, either to combine overlap-aware systems or to perform multi-channel diarization by late fusion.","Once speaker segments are identified, we robustly extract single-speaker utterances from the mixture using a GPU-accelerated implementation of guided source separation, which allows us to use an off-the-shelf ASR system to obtain speaker-attributed transcripts.   ","Since the modular approach suffers from error propagation, we propose an alternate \"end-to-end\" perspective on the problem.","For this, we describe the Streaming Unmixing and Recognition Transducer (SURT).","We show how to train SURT models efficiently by carefully designing the network architecture, objective functions, and mixture simulation techniques.","Finally, we add an auxiliary speaker branch to enable joint prediction of speaker labels synchronized with the speech tokens.","We demonstrate that training on synthetic mixtures and adapting with real data helps these models transfer well for streaming transcription of real meeting sessions."],"url":"http://arxiv.org/abs/2402.08932v1","category":"eess.AS"}
{"created":"2024-02-14 03:56:27","title":"MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences","abstract":"Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed solution. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language models (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among human preferences. Our algorithm achieves an average improvement of more than 16% in win-rates over conventional RLHF algorithms and improves the win-rate (accuracy) for minority groups by over 33% without compromising the performance of majority groups, showcasing the robustness and fairness of our approach. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.","sentences":["Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data.","However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users.","In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences.","To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences.","We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed solution.","We present comprehensive experimental results on small-scale (GPT-2) and large-scale language models (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among human preferences.","Our algorithm achieves an average improvement of more than 16% in win-rates over conventional RLHF algorithms and improves the win-rate (accuracy) for minority groups by over 33% without compromising the performance of majority groups, showcasing the robustness and fairness of our approach.","We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general."],"url":"http://arxiv.org/abs/2402.08925v1","category":"cs.CL"}
{"created":"2024-02-14 03:41:50","title":"Enhancing ID and Text Fusion via Alternative Training in Session-based Recommendation","abstract":"Session-based recommendation has gained increasing attention in recent years, with its aim to offer tailored suggestions based on users' historical behaviors within sessions.   To advance this field, a variety of methods have been developed, with ID-based approaches typically demonstrating promising performance. However, these methods often face challenges with long-tail items and overlook other rich forms of information, notably valuable textual semantic information. To integrate text information, various methods have been introduced, mostly following a naive fusion framework. Surprisingly, we observe that fusing these two modalities does not consistently outperform the best single modality by following the naive fusion framework. Further investigation reveals an potential imbalance issue in naive fusion, where the ID dominates and text modality is undertrained. This suggests that the unexpected observation may stem from naive fusion's failure to effectively balance the two modalities, often over-relying on the stronger ID modality. This insight suggests that naive fusion might not be as effective in combining ID and text as previously expected. To address this, we propose a novel alternative training strategy AlterRec. It separates the training of ID and text, thereby avoiding the imbalance issue seen in naive fusion. Additionally, AlterRec designs a novel strategy to facilitate the interaction between the two modalities, enabling them to mutually learn from each other and integrate the text more effectively. Comprehensive experiments demonstrate the effectiveness of AlterRec in session-based recommendation. The implementation is available at https://github.com/Juanhui28/AlterRec.","sentences":["Session-based recommendation has gained increasing attention in recent years, with its aim to offer tailored suggestions based on users' historical behaviors within sessions.   ","To advance this field, a variety of methods have been developed, with ID-based approaches typically demonstrating promising performance.","However, these methods often face challenges with long-tail items and overlook other rich forms of information, notably valuable textual semantic information.","To integrate text information, various methods have been introduced, mostly following a naive fusion framework.","Surprisingly, we observe that fusing these two modalities does not consistently outperform the best single modality by following the naive fusion framework.","Further investigation reveals an potential imbalance issue in naive fusion, where the ID dominates and text modality is undertrained.","This suggests that the unexpected observation may stem from naive fusion's failure to effectively balance the two modalities, often over-relying on the stronger ID modality.","This insight suggests that naive fusion might not be as effective in combining ID and text as previously expected.","To address this, we propose a novel alternative training strategy AlterRec.","It separates the training of ID and text, thereby avoiding the imbalance issue seen in naive fusion.","Additionally, AlterRec designs a novel strategy to facilitate the interaction between the two modalities, enabling them to mutually learn from each other and integrate the text more effectively.","Comprehensive experiments demonstrate the effectiveness of AlterRec in session-based recommendation.","The implementation is available at https://github.com/Juanhui28/AlterRec."],"url":"http://arxiv.org/abs/2402.08921v1","category":"cs.IR"}
{"created":"2024-02-14 03:16:13","title":"Graph Inference Acceleration by Learning MLPs on Graphs without Supervision","abstract":"Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \\textbf{\\textsc{SimMLP}}, a \\textbf{\\textsc{Sim}}ple yet effective framework for learning \\textbf{\\textsc{MLP}}s on graphs without supervision, to enhance generalization. \\textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, we comprehensively analyze \\textsc{SimMLP} to demonstrate its equivalence to GNNs in the optimal case and its generalization capability. Empirically, \\textsc{SimMLP} outperforms state-of-the-art baselines, especially in settings with unseen nodes. In particular, it obtains significant performance gains {\\bf (7$\\sim$26\\%)} over MLPs and inference acceleration over GNNs {\\bf (90$\\sim$126$\\times$)} on large-scale graph datasets. Our codes are available at: \\url{https://github.com/Zehong-Wang/SimMLP}.","sentences":["Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection.","Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference.","However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications.","To this end, we present \\textbf{\\textsc{SimMLP}}, a \\textbf{\\textsc{Sim}}ple yet effective framework for learning \\textbf{\\textsc{MLP}}s on graphs without supervision, to enhance generalization.","\\textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions.","Theoretically, we comprehensively analyze \\textsc{SimMLP} to demonstrate its equivalence to GNNs in the optimal case and its generalization capability.","Empirically, \\textsc{SimMLP} outperforms state-of-the-art baselines, especially in settings with unseen nodes.","In particular, it obtains significant performance gains {\\bf (7$\\sim$26\\%)} over MLPs and inference acceleration over GNNs {\\bf (90$\\sim$126$\\times$)} on large-scale graph datasets.","Our codes are available at: \\url{https://github.com/Zehong-Wang/SimMLP}."],"url":"http://arxiv.org/abs/2402.08918v1","category":"cs.LG"}
{"created":"2024-02-14 02:46:47","title":"Tackling Negative Transfer on Graphs","abstract":"Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce two effective yet elegant methods, Subgraph Pooling (SP) and Subgraph Pooling++ (SP++), that transfer subgraph-level knowledge across graphs. We theoretically analyze the role of SP in reducing graph discrepancy and conduct extensive experiments to evaluate its superiority under various settings. Our code and datasets are available at: https://github.com/Zehong-Wang/Subgraph-Pooling.","sentences":["Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks.","However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer.","In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored.","We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities.","Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs.","To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal.","Building on this insight, we introduce two effective yet elegant methods, Subgraph Pooling (SP) and Subgraph Pooling++ (SP++), that transfer subgraph-level knowledge across graphs.","We theoretically analyze the role of SP in reducing graph discrepancy and conduct extensive experiments to evaluate its superiority under various settings.","Our code and datasets are available at: https://github.com/Zehong-Wang/Subgraph-Pooling."],"url":"http://arxiv.org/abs/2402.08907v1","category":"cs.LG"}
{"created":"2024-02-14 00:42:19","title":"DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling","abstract":"In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction. However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces. In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions. This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields. Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance. Moreover, the unlocked field's differentiability allows the accurate computation of essential topological properties such as normal directions and curvatures, pervasive in downstream tasks such as rendering. Through extensive experiments, we validate our approach across various data sets and against competitive baselines. The results demonstrate enhanced accuracy and up to an order of magnitude increase in speed compared to previous methods.","sentences":["In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction.","However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces.","In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions.","This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields.","Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance.","Moreover, the unlocked field's differentiability allows the accurate computation of essential topological properties such as normal directions and curvatures, pervasive in downstream tasks such as rendering.","Through extensive experiments, we validate our approach across various data sets and against competitive baselines.","The results demonstrate enhanced accuracy and up to an order of magnitude increase in speed compared to previous methods."],"url":"http://arxiv.org/abs/2402.08876v1","category":"cs.CV"}
{"created":"2024-02-14 00:30:18","title":"ScamSpot: Fighting Financial Fraud in Instagram Comments","abstract":"The long-standing problem of spam and fraudulent messages in the comment sections of Instagram pages in the financial sector claims new victims every day. Instagram's current spam filter proves inadequate, and existing research approaches are primarily confined to theoretical concepts. Practical implementations with evaluated results are missing. To solve this problem, we propose ScamSpot, a comprehensive system that includes a browser extension, a fine-tuned BERT model and a REST API. This approach ensures public accessibility of our results for Instagram users using the Chrome browser. Furthermore, we conduct a data annotation study, shedding light on the reasons and causes of the problem and evaluate the system through user feedback and comparison with existing models. ScamSpot is an open-source project and is publicly available at https://scamspot.github.io/.","sentences":["The long-standing problem of spam and fraudulent messages in the comment sections of Instagram pages in the financial sector claims new victims every day.","Instagram's current spam filter proves inadequate, and existing research approaches are primarily confined to theoretical concepts.","Practical implementations with evaluated results are missing.","To solve this problem, we propose ScamSpot, a comprehensive system that includes a browser extension, a fine-tuned BERT model and a REST API.","This approach ensures public accessibility of our results for Instagram users using the Chrome browser.","Furthermore, we conduct a data annotation study, shedding light on the reasons and causes of the problem and evaluate the system through user feedback and comparison with existing models.","ScamSpot is an open-source project and is publicly available at https://scamspot.github.io/."],"url":"http://arxiv.org/abs/2402.08869v1","category":"cs.AI"}
{"created":"2024-02-14 00:04:33","title":"Large Language Model with Graph Convolution for Recommendation","abstract":"In recent years, efforts have been made to use text information for better user profiling and item characterization in recommendations. However, text information can sometimes be of low quality, hindering its effectiveness for real-world applications. With knowledge and reasoning capabilities capsuled in Large Language Models (LLMs), utilizing LLMs emerges as a promising way for description improvement. However, existing ways of prompting LLMs with raw texts ignore structured knowledge of user-item interactions, which may lead to hallucination problems like inconsistent description generation. To this end, we propose a Graph-aware Convolutional LLM method to elicit LLMs to capture high-order relations in the user-item graph. To adapt text-based LLMs with structured graphs, We use the LLM as an aggregator in graph processing, allowing it to understand graph-based information step by step. Specifically, the LLM is required for description enhancement by exploring multi-hop neighbors layer by layer, thereby propagating information progressively in the graph. To enable LLMs to capture large-scale graph information, we break down the description task into smaller parts, which drastically reduces the context length of the token input with each step. Extensive experiments on three real-world datasets show that our method consistently outperforms state-of-the-art methods.","sentences":["In recent years, efforts have been made to use text information for better user profiling and item characterization in recommendations.","However, text information can sometimes be of low quality, hindering its effectiveness for real-world applications.","With knowledge and reasoning capabilities capsuled in Large Language Models (LLMs), utilizing LLMs emerges as a promising way for description improvement.","However, existing ways of prompting LLMs with raw texts ignore structured knowledge of user-item interactions, which may lead to hallucination problems like inconsistent description generation.","To this end, we propose a Graph-aware Convolutional LLM method to elicit LLMs to capture high-order relations in the user-item graph.","To adapt text-based LLMs with structured graphs, We use the LLM as an aggregator in graph processing, allowing it to understand graph-based information step by step.","Specifically, the LLM is required for description enhancement by exploring multi-hop neighbors layer by layer, thereby propagating information progressively in the graph.","To enable LLMs to capture large-scale graph information, we break down the description task into smaller parts, which drastically reduces the context length of the token input with each step.","Extensive experiments on three real-world datasets show that our method consistently outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.08859v1","category":"cs.AI"}
{"created":"2024-02-13 23:48:59","title":"GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency","abstract":"Large language models (LLMs) are becoming more prevalent and have found a ubiquitous use in providing different forms of writing assistance. However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering. We see design as one way to address these challenges and introduce GhostWriter, an AI-enhanced writing design probe where users can exercise enhanced agency and personalization. GhostWriter leverages LLMs to learn the user's intended writing style implicitly as they write, while allowing explicit teaching moments through manual style edits and annotations. We study 18 participants who use GhostWriter on two different writing tasks, observing that it helps users craft personalized text generations and empowers them by providing multiple ways to control the system's writing style. From this study, we present insights regarding people's relationship with AI-assisted writing and offer design recommendations for future work.","sentences":["Large language models (LLMs) are becoming more prevalent and have found a ubiquitous use in providing different forms of writing assistance.","However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering.","We see design as one way to address these challenges and introduce GhostWriter, an AI-enhanced writing design probe where users can exercise enhanced agency and personalization.","GhostWriter leverages LLMs to learn the user's intended writing style implicitly as they write, while allowing explicit teaching moments through manual style edits and annotations.","We study 18 participants who use GhostWriter on two different writing tasks, observing that it helps users craft personalized text generations and empowers them by providing multiple ways to control the system's writing style.","From this study, we present insights regarding people's relationship with AI-assisted writing and offer design recommendations for future work."],"url":"http://arxiv.org/abs/2402.08855v1","category":"cs.HC"}
{"created":"2024-02-13 23:29:09","title":"Hybrid Inverse Reinforcement Learning","abstract":"The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using hybrid RL -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formally, we derive a reduction from inverse RL to expert-competitive RL (rather than globally optimal RL) that allows us to dramatically reduce interaction during the inner policy search loop while maintaining the benefits of the IRL approach. This allows us to derive both model-free and model-based hybrid inverse RL algorithms with strong policy performance guarantees. Empirically, we find that our approaches are significantly more sample efficient than standard inverse RL and several other baselines on a suite of continuous control tasks.","sentences":["The inverse reinforcement learning approach to imitation learning is a double-edged sword.","On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches.","On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem.","Often, much of this computation is wasted searching over policies very dissimilar to the expert's.","In this work, we propose using hybrid RL -- training on a mixture of online and expert data -- to curtail unnecessary exploration.","Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy.","Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL.","More formally, we derive a reduction from inverse RL to expert-competitive RL (rather than globally optimal RL) that allows us to dramatically reduce interaction during the inner policy search loop while maintaining the benefits of the IRL approach.","This allows us to derive both model-free and model-based hybrid inverse RL algorithms with strong policy performance guarantees.","Empirically, we find that our approaches are significantly more sample efficient than standard inverse RL and several other baselines on a suite of continuous control tasks."],"url":"http://arxiv.org/abs/2402.08848v1","category":"cs.LG"}
{"created":"2024-02-13 23:25:04","title":"An Embarrassingly Simple Approach for LLM with Strong ASR Capacity","abstract":"In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data. Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment. We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community.","sentences":["In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM).","Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM.","We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task.","To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR.","The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained.","To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data.","Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment.","We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community."],"url":"http://arxiv.org/abs/2402.08846v1","category":"cs.CL"}
{"created":"2024-02-13 22:29:40","title":"Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties","abstract":"This study examines how artificial intelligence (AI), especially Reinforcement Learning (RL), can be used in farming to boost crop yields, fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate change and limited agricultural knowledge, we use Partially Observable Markov Decision Processes (POMDPs) with a crop simulator to model AI agents' interactions with farming environments. We apply deep Q-learning with Recurrent Neural Network (RNN)-based Q networks for training agents on optimal actions. Also, we develop Machine Learning (ML) models to predict N$_2$O emissions, integrating these predictions into the simulator. Our research tackles uncertainties in N$_2$O emission estimates with a probabilistic ML approach and climate variability through a stochastic weather model, offering a range of emission outcomes to improve forecast reliability and decision-making. By incorporating climate change effects, we enhance agents' climate adaptability, aiming for resilient agricultural practices. Results show these agents can align crop productivity with environmental concerns by penalizing N$_2$O emissions, adapting effectively to climate shifts like warmer temperatures and less rain. This strategy improves farm management under climate change, highlighting AI's role in sustainable agriculture.","sentences":["This study examines how artificial intelligence (AI), especially Reinforcement Learning (RL), can be used in farming to boost crop yields, fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil.","Facing climate change and limited agricultural knowledge, we use Partially Observable Markov Decision Processes (POMDPs) with a crop simulator to model AI agents' interactions with farming environments.","We apply deep Q-learning with Recurrent Neural Network (RNN)-based Q networks for training agents on optimal actions.","Also, we develop Machine Learning (ML) models to predict N$_2$O emissions, integrating these predictions into the simulator.","Our research tackles uncertainties in N$_2$O emission estimates with a probabilistic ML approach and climate variability through a stochastic weather model, offering a range of emission outcomes to improve forecast reliability and decision-making.","By incorporating climate change effects, we enhance agents' climate adaptability, aiming for resilient agricultural practices.","Results show these agents can align crop productivity with environmental concerns by penalizing N$_2$O emissions, adapting effectively to climate shifts like warmer temperatures and less rain.","This strategy improves farm management under climate change, highlighting AI's role in sustainable agriculture."],"url":"http://arxiv.org/abs/2402.08832v1","category":"cs.LG"}
{"created":"2024-02-13 22:26:24","title":"eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data","abstract":"With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM exhibits excellent generalizability to out-of-domain settings, including unseen products and unseen instructions, highlighting its superiority as a generalist e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective LLMs for e-commerce. ECInstruct and eCeLLM models are publicly accessible through https://ninglab.github.io/eCeLLM.","sentences":["With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge.","Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields.","Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce.","Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs.","Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain evaluation.","Moreover, eCeLLM exhibits excellent generalizability to out-of-domain settings, including unseen products and unseen instructions, highlighting its superiority as a generalist e-commerce model.","Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective LLMs for e-commerce.","ECInstruct and eCeLLM models are publicly accessible through https://ninglab.github.io/eCeLLM."],"url":"http://arxiv.org/abs/2402.08831v1","category":"cs.CL"}
{"created":"2024-02-13 22:10:50","title":"Maximizing Throughput with Routing Interference Avoidance in RIS-Assisted Relay Mesh Networks","abstract":"In the modern landscape of wireless communications, multi-hop, high-bandwidth, indoor Terahertz (THz) wireless communications are gaining significant attention. These systems couple Reconfigurable Intelligent Surface (RIS) and relay devices within the emerging 6G network framework, offering promising solutions for creating cell-less, indoor, and on-demand mesh networks. RIS devices are especially attractive, constructed by an array of reflecting elements that can phase shifts, such that the reflecting signals can be focused, steered, and the power of the signal enhanced towards the destination. This paper presents an in-depth, analytical examination of how path allocation impacts interference within such networks. We develop the first model which analyzes interference based on the geometric parameters of beams (conic, cylindrical) as they interact with RIS, User Equipment (UE), and relay devices. We introduce a transmission scheduling heuristic designed to mitigate interference, alongside an efficient optimization method to maximize throughput. Our performance results elucidate the interference's effect on communication path quality and highlight effective path selection strategies with throughput maximization.","sentences":["In the modern landscape of wireless communications, multi-hop, high-bandwidth, indoor Terahertz (THz) wireless communications are gaining significant attention.","These systems couple Reconfigurable Intelligent Surface (RIS) and relay devices within the emerging 6G network framework, offering promising solutions for creating cell-less, indoor, and on-demand mesh networks.","RIS devices are especially attractive, constructed by an array of reflecting elements that can phase shifts, such that the reflecting signals can be focused, steered, and the power of the signal enhanced towards the destination.","This paper presents an in-depth, analytical examination of how path allocation impacts interference within such networks.","We develop the first model which analyzes interference based on the geometric parameters of beams (conic, cylindrical) as they interact with RIS, User Equipment (UE), and relay devices.","We introduce a transmission scheduling heuristic designed to mitigate interference, alongside an efficient optimization method to maximize throughput.","Our performance results elucidate the interference's effect on communication path quality and highlight effective path selection strategies with throughput maximization."],"url":"http://arxiv.org/abs/2402.08825v1","category":"cs.NI"}
{"created":"2024-02-13 21:53:43","title":"An Adaptive System Architecture for Multimodal Intelligent Transportation Systems","abstract":"Multimodal intelligent transportation systems (M-ITS) encompass a range of transportation services that utilise various modes of transport and incorporate intelligent technologies for enhanced efficiency and user experience. There are several challenges in M-ITS including data integration, Interoperability, scalability, user experience, etc. To address these challenges, such a system requires an adaptive system architecture that enables M-ITS to operate as an integrated ecosystem. In this paper, we provide an adaptive, user-centric, and layered architecture for multimodal transportation systems. The proposed architecture ensures scalability for seamless interactions of various subcomponents, that are often managed by different stakeholders. Concurrently, the data architecture is detailed, covering diverse data sources, advanced analytics, and stringent governance, providing a robust basis for intelligent decision-making. We provide two example use cases of the proposed architecture, showing how the data architecture and the system architecture can be fused and serve multimodal intelligent transport services.","sentences":["Multimodal intelligent transportation systems (M-ITS) encompass a range of transportation services that utilise various modes of transport and incorporate intelligent technologies for enhanced efficiency and user experience.","There are several challenges in M-ITS including data integration, Interoperability, scalability, user experience, etc.","To address these challenges, such a system requires an adaptive system architecture that enables M-ITS to operate as an integrated ecosystem.","In this paper, we provide an adaptive, user-centric, and layered architecture for multimodal transportation systems.","The proposed architecture ensures scalability for seamless interactions of various subcomponents, that are often managed by different stakeholders.","Concurrently, the data architecture is detailed, covering diverse data sources, advanced analytics, and stringent governance, providing a robust basis for intelligent decision-making.","We provide two example use cases of the proposed architecture, showing how the data architecture and the system architecture can be fused and serve multimodal intelligent transport services."],"url":"http://arxiv.org/abs/2402.08817v1","category":"eess.SY"}
{"created":"2024-02-13 21:33:12","title":"Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis through Rapid Prototyping, Iteration and Curation","abstract":"Complex data analysis inherently seeks unexpected insights through exploratory \\re{visual analysis} methods, transcending logical, step-by-step processing. However, \\re{existing interfaces such as notebooks and dashboards have limitations in exploration and comparison for visual data analysis}. Addressing these limitations, we introduce a \"design-like\" intelligent canvas environment integrating generative AI into data analysis, offering rapid prototyping, iteration, and comparative visualization management. Our dual contributions include the integration of generative AI components into a canvas interface, and empirical findings from a user study (N=10) evaluating the effectiveness of the canvas interface.","sentences":["Complex data analysis inherently seeks unexpected insights through exploratory \\re{visual analysis} methods, transcending logical, step-by-step processing.","However, \\re{existing interfaces such as notebooks and dashboards have limitations in exploration and comparison for visual data analysis}.","Addressing these limitations, we introduce a \"design-like\" intelligent canvas environment integrating generative AI into data analysis, offering rapid prototyping, iteration, and comparative visualization management.","Our dual contributions include the integration of generative AI components into a canvas interface, and empirical findings from a user study (N=10) evaluating the effectiveness of the canvas interface."],"url":"http://arxiv.org/abs/2402.08812v1","category":"cs.HC"}
{"created":"2024-02-13 21:24:21","title":"Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy","abstract":"Background: Large language models (LLMs) such as OpenAI's GPT-4 or Google's PaLM 2 are proposed as viable diagnostic support tools or even spoken of as replacements for \"curbside consults\". However, even LLMs specifically trained on medical topics may lack sufficient diagnostic accuracy for real-life applications.   Methods: Using collective intelligence methods and a dataset of 200 clinical vignettes of real-life cases, we assessed and compared the accuracy of differential diagnoses obtained by asking individual commercial LLMs (OpenAI GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of differential diagnoses synthesized by aggregating responses from combinations of the same LLMs.   Results: We find that aggregating responses from multiple, various LLMs leads to more accurate differential diagnoses (average accuracy for 3 LLMs: $75.3\\%\\pm 1.6pp$) compared to the differential diagnoses produced by single LLMs (average accuracy for single LLMs: $59.0\\%\\pm 6.1pp$).   Discussion: The use of collective intelligence methods to synthesize differential diagnoses combining the responses of different LLMs achieves two of the necessary steps towards advancing acceptance of LLMs as a diagnostic support tool: (1) demonstrate high diagnostic accuracy and (2) eliminate dependence on a single commercial vendor.","sentences":["Background: Large language models (LLMs) such as OpenAI's GPT-4 or Google's PaLM 2 are proposed as viable diagnostic support tools or even spoken of as replacements for \"curbside consults\".","However, even LLMs specifically trained on medical topics may lack sufficient diagnostic accuracy for real-life applications.   ","Methods: Using collective intelligence methods and a dataset of 200 clinical vignettes of real-life cases, we assessed and compared the accuracy of differential diagnoses obtained by asking individual commercial LLMs (OpenAI GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of differential diagnoses synthesized by aggregating responses from combinations of the same LLMs.   ","Results: We find that aggregating responses from multiple, various LLMs leads to more accurate differential diagnoses (average accuracy for 3 LLMs: $75.3\\%\\pm 1.6pp$) compared to the differential diagnoses produced by single LLMs (average accuracy for single LLMs: $59.0\\%\\pm 6.1pp$).   ","Discussion: The use of collective intelligence methods to synthesize differential diagnoses combining the responses of different LLMs achieves two of the necessary steps towards advancing acceptance of LLMs as a diagnostic support tool: (1) demonstrate high diagnostic accuracy and (2) eliminate dependence on a single commercial vendor."],"url":"http://arxiv.org/abs/2402.08806v1","category":"cs.AI"}
{"created":"2024-02-13 21:15:33","title":"ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions","abstract":"Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the premier platform for developers' queries on programming and software development. Demonstrating an ability to generate instant, human-like responses to technical questions, ChatGPT has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative AI. Two months after ChatGPT's release, Meta released its answer with its own Large Language Model (LLM) called LLaMA: the race was on. We conducted an empirical study analyzing questions from Stack Overflow and using these LLMs to address them. This way, we aim to (ii) measure user engagement evolution with Stack Overflow over time; (ii) quantify the reliability of LLMs' answers and their potential to replace Stack Overflow in the long term; (iii) identify and understand why LLMs fails; and (iv) compare LLMs together. Our empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. Furthermore, we also discuss the impact of our findings regarding the usage and development of new LLMs.","sentences":["Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the premier platform for developers' queries on programming and software development.","Demonstrating an ability to generate instant, human-like responses to technical questions, ChatGPT has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative AI.","Two months after ChatGPT's release, Meta released its answer with its own Large Language Model (LLM) called LLaMA: the race was on.","We conducted an empirical study analyzing questions from Stack Overflow and using these LLMs to address them.","This way, we aim to (ii) measure user engagement evolution with Stack Overflow over time; (ii) quantify the reliability of LLMs' answers and their potential to replace Stack Overflow in the long term; (iii) identify and understand why LLMs fails; and (iv) compare LLMs together.","Our empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed.","Furthermore, we also discuss the impact of our findings regarding the usage and development of new LLMs."],"url":"http://arxiv.org/abs/2402.08801v1","category":"cs.SE"}
{"created":"2024-02-13 21:10:21","title":"Computing Power and the Governance of Artificial Intelligence","abstract":"Computing power, or \"compute,\" is crucial for the development and deployment of artificial intelligence (AI) capabilities. As a result, governments and companies have started to leverage compute as a means to govern AI. For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors. However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment. Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage. However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation. Some ideas are currently being piloted, while others are hindered by the need for fundamental research. Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power. We end by suggesting guardrails to minimize these risks from compute governance.","sentences":["Computing power, or \"compute,\" is crucial for the development and deployment of artificial intelligence (AI) capabilities.","As a result, governments and companies have started to leverage compute as a means to govern AI.","For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors.","However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment.","Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain.","These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI.","More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage.","However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation.","Some ideas are currently being piloted, while others are hindered by the need for fundamental research.","Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power.","We end by suggesting guardrails to minimize these risks from compute governance."],"url":"http://arxiv.org/abs/2402.08797v1","category":"cs.CY"}
{"created":"2024-02-13 21:09:48","title":"Noble Gas Planetology and the Xenon Clouds of Uranus","abstract":"Noble gases provide tracers of cosmic provenance that are accessible to a future Uranus Atmospheric Probe. Argon and krypton are expected to be well-mixed on Uranus with respect to H$_2$ and He, although condensation at the winter pole may be possible. The Ar/H$_2$ and Ar/Kr ratios address whether the materials accreted by Uranus resembled the extremely cold materials accreted by Jupiter's atmosphere, or whether they were warmer like comet 67P/Churyumov-Gerasimenko, or whether Uranus is like neither. Xenon condenses as an ice, probably on methane ice, in Uranus's upper troposphere. Condensation may complicate interpretation of Xe/H$_2$, but it also presents an opportunity to collect concentrated xenon samples suitable for measuring isotopes. Solar System Xe tracks three distinct nucleosynthetic xenon reservoirs, one evident in the Sun and in chondritic meteorites, a second in refractory presolar grains, and a third evident in comet 67P/C-G and in Earth's air. The first and third reservoirs appear to have been captured from different clouds of gas. The two gases do not appear to have been well-mixed; moreover, the high $^{129}$Xe/$^{132}$Xe ratio in 67P/C-G implies that the gas was captured before the initial nucleosynthetic complement of $^{129}$I (15.7 Myr half-life) had decayed. Xenon's isotopic peculiarities, if seen in Uranus, could usefully upset our understanding of planetary origins. Krypton's isotopic anomalies are more subtle and may prove hard to measure. There is a slight chance that neon and helium fractionations can be used to constrain how Uranus acquired its nebular envelope.","sentences":["Noble gases provide tracers of cosmic provenance that are accessible to a future Uranus Atmospheric Probe.","Argon and krypton are expected to be well-mixed on Uranus with respect to H$_2$ and He, although condensation at the winter pole may be possible.","The Ar/H$_2$ and Ar/Kr ratios address whether the materials accreted by Uranus resembled the extremely cold materials accreted by Jupiter's atmosphere, or whether they were warmer like comet 67P/Churyumov-Gerasimenko, or whether Uranus is like neither.","Xenon condenses as an ice, probably on methane ice, in Uranus's upper troposphere.","Condensation may complicate interpretation of Xe/H$_2$, but it also presents an opportunity to collect concentrated xenon samples suitable for measuring isotopes.","Solar System Xe tracks three distinct nucleosynthetic xenon reservoirs, one evident in the Sun and in chondritic meteorites, a second in refractory presolar grains, and a third evident in comet 67P/C-G and in Earth's air.","The first and third reservoirs appear to have been captured from different clouds of gas.","The two gases do not appear to have been well-mixed; moreover, the high $^{129}$Xe/$^{132}$Xe ratio in 67P/C-G implies that the gas was captured before the initial nucleosynthetic complement of $^{129}$I (15.7 Myr half-life) had decayed.","Xenon's isotopic peculiarities, if seen in Uranus, could usefully upset our understanding of planetary origins.","Krypton's isotopic anomalies are more subtle and may prove hard to measure.","There is a slight chance that neon and helium fractionations can be used to constrain how Uranus acquired its nebular envelope."],"url":"http://arxiv.org/abs/2402.08795v1","category":"astro-ph.EP"}
{"created":"2024-02-13 20:54:55","title":"Leveraging cough sounds to optimize chest x-ray usage in low-resource settings","abstract":"Chest X-ray is a commonly used tool during triage, diagnosis and management of respiratory diseases. In resource-constricted settings, optimizing this resource can lead to valuable cost savings for the health care system and the patients as well as to and improvement in consult time. We used prospectively-collected data from 137 patients referred for chest X-ray at the Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each patient provided at least five coughs while awaiting radiography. Collected cough sounds were analyzed using acoustic AI methods. Cross-validation was done on temporal and spectral features on the cough sounds of each patient. Features were summarized using standard statistical approaches. Three models were developed, tested and compared in their capacity to predict an abnormal result in the chest X-ray. All three methods yielded models that could discriminate to some extent between normal and abnormal with the logistic regression performing best with an area under the receiver operating characteristic curves ranging from 0.7 to 0.78. Despite limitations and its relatively small sample size, this study shows that AI-enabled algorithms can use cough sounds to predict which individuals presenting for chest radiographic examination will have a normal or abnormal results. These results call for expanding this research given the potential optimization of limited health care resources in low- and middle-income countries.","sentences":["Chest X-ray is a commonly used tool during triage, diagnosis and management of respiratory diseases.","In resource-constricted settings, optimizing this resource can lead to valuable cost savings for the health care system and the patients as well as to and improvement in consult time.","We used prospectively-collected data from 137 patients referred for chest X-ray at the Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India.","Each patient provided at least five coughs while awaiting radiography.","Collected cough sounds were analyzed using acoustic AI methods.","Cross-validation was done on temporal and spectral features on the cough sounds of each patient.","Features were summarized using standard statistical approaches.","Three models were developed, tested and compared in their capacity to predict an abnormal result in the chest X-ray.","All three methods yielded models that could discriminate to some extent between normal and abnormal with the logistic regression performing best with an area under the receiver operating characteristic curves ranging from 0.7 to 0.78.","Despite limitations and its relatively small sample size, this study shows that AI-enabled algorithms can use cough sounds to predict which individuals presenting for chest radiographic examination will have a normal or abnormal results.","These results call for expanding this research given the potential optimization of limited health care resources in low- and middle-income countries."],"url":"http://arxiv.org/abs/2402.08789v1","category":"eess.AS"}
{"created":"2024-02-13 20:29:36","title":"Enhanced Deep Q-Learning for 2D Self-Driving Cars: Implementation and Evaluation on a Custom Track Environment","abstract":"This research project presents the implementation of a Deep Q-Learning Network (DQN) for a self-driving car on a 2-dimensional (2D) custom track, with the objective of enhancing the DQN network's performance. It encompasses the development of a custom driving environment using Pygame on a track surrounding the University of Memphis map, as well as the design and implementation of the DQN model. The algorithm utilizes data from 7 sensors installed in the car, which measure the distance between the car and the track. These sensors are positioned in front of the vehicle, spaced 20 degrees apart, enabling them to sense a wide area ahead. We successfully implemented the DQN and also a modified version of the DQN with a priority-based action selection mechanism, which we refer to as modified DQN. The model was trained over 1000 episodes, and the average reward received by the agent was found to be around 40, which is approximately 60% higher than the original DQN and around 50% higher than the vanilla neural network.","sentences":["This research project presents the implementation of a Deep Q-Learning Network (DQN) for a self-driving car on a 2-dimensional (2D) custom track, with the objective of enhancing the DQN network's performance.","It encompasses the development of a custom driving environment using Pygame on a track surrounding the University of Memphis map, as well as the design and implementation of the DQN model.","The algorithm utilizes data from 7 sensors installed in the car, which measure the distance between the car and the track.","These sensors are positioned in front of the vehicle, spaced 20 degrees apart, enabling them to sense a wide area ahead.","We successfully implemented the DQN and also a modified version of the DQN with a priority-based action selection mechanism, which we refer to as modified DQN.","The model was trained over 1000 episodes, and the average reward received by the agent was found to be around 40, which is approximately 60% higher than the original DQN and around 50% higher than the vanilla neural network."],"url":"http://arxiv.org/abs/2402.08780v1","category":"cs.AI"}
{"created":"2024-02-13 20:21:29","title":"DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models","abstract":"Effective DNA embedding remains crucial in genomic analysis, particularly in scenarios lacking labeled data for model fine-tuning, despite the significant advancements in genome foundation models. A prime example is metagenomics binning, a critical process in microbiome research that aims to group DNA sequences by their species from a complex mixture of DNA sequences derived from potentially thousands of distinct, often uncharacterized species. To fill the lack of effective DNA embedding models, we introduce DNABERT-S, a genome foundation model that specializes in creating species-aware DNA embeddings. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enhance it with the proposed Curriculum Contrastive Learning (C$^2$LR) strategy. Empirical results on 18 diverse datasets showed DNABERT-S's remarkable performance. It outperforms the top baseline's performance in 10-shot species classification with just a 2-shot training while doubling the Adjusted Rand Index (ARI) in species clustering and substantially increasing the number of correctly identified species in metagenomics binning. The code, data, and pre-trained model are publicly available at https://github.com/Zhihan1996/DNABERT_S.","sentences":["Effective DNA embedding remains crucial in genomic analysis, particularly in scenarios lacking labeled data for model fine-tuning, despite the significant advancements in genome foundation models.","A prime example is metagenomics binning, a critical process in microbiome research that aims to group DNA sequences by their species from a complex mixture of DNA sequences derived from potentially thousands of distinct, often uncharacterized species.","To fill the lack of effective DNA embedding models, we introduce DNABERT-S, a genome foundation model that specializes in creating species-aware DNA embeddings.","To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer.","We further enhance it with the proposed Curriculum Contrastive Learning (C$^2$LR) strategy.","Empirical results on 18 diverse datasets showed DNABERT-S's remarkable performance.","It outperforms the top baseline's performance in 10-shot species classification with just a 2-shot training while doubling the Adjusted Rand Index (ARI) in species clustering and substantially increasing the number of correctly identified species in metagenomics binning.","The code, data, and pre-trained model are publicly available at https://github.com/Zhihan1996/DNABERT_S."],"url":"http://arxiv.org/abs/2402.08777v1","category":"q-bio.GN"}
{"created":"2024-02-13 20:07:58","title":"Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints","abstract":"The Multi-Agent Path Finding (MAPF) problem entails finding collision-free paths for a set of agents, guiding them from their start to goal locations. However, MAPF does not account for several practical task-related constraints. For example, agents may need to perform actions at goal locations with specific execution times, adhering to predetermined orders and timeframes. Moreover, goal assignments may not be predefined for agents, and the optimization objective may lack an explicit definition. To incorporate task assignment, path planning, and a user-defined objective into a coherent framework, this paper examines the Task Assignment and Path Finding with Precedence and Temporal Constraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) to simultaneously generate task assignments and collision-free paths that adhere to precedence and temporal constraints, maximizing an objective quantified by the return from a user-defined reward function in reinforcement learning (RL). Experimentally, we demonstrate that our algorithm, CBS-TA-PTC, can solve highly challenging bomb-defusing tasks with precedence and temporal constraints efficiently relative to MARL and adapted Target Assignment and Path Finding (TAPF) methods.","sentences":["The Multi-Agent Path Finding (MAPF) problem entails finding collision-free paths for a set of agents, guiding them from their start to goal locations.","However, MAPF does not account for several practical task-related constraints.","For example, agents may need to perform actions at goal locations with specific execution times, adhering to predetermined orders and timeframes.","Moreover, goal assignments may not be predefined for agents, and the optimization objective may lack an explicit definition.","To incorporate task assignment, path planning, and a user-defined objective into a coherent framework, this paper examines the Task Assignment and Path Finding with Precedence and Temporal Constraints (TAPF-PTC) problem.","We augment Conflict-Based Search (CBS) to simultaneously generate task assignments and collision-free paths that adhere to precedence and temporal constraints, maximizing an objective quantified by the return from a user-defined reward function in reinforcement learning (RL).","Experimentally, we demonstrate that our algorithm, CBS-TA-PTC, can solve highly challenging bomb-defusing tasks with precedence and temporal constraints efficiently relative to MARL and adapted Target Assignment and Path Finding (TAPF) methods."],"url":"http://arxiv.org/abs/2402.08772v1","category":"cs.AI"}
{"created":"2024-02-13 20:02:34","title":"Adversarially Robust Feature Learning for Breast Cancer Diagnosis","abstract":"Adversarial data can lead to malfunction of deep learning applications. It is essential to develop deep learning models that are robust to adversarial data while accurate on standard, clean data. In this study, we proposed a novel adversarially robust feature learning (ARFL) method for a real-world application of breast cancer diagnosis. ARFL facilitates adversarial training using both standard data and adversarial data, where a feature correlation measure is incorporated as an objective function to encourage learning of robust features and restrain spurious features. To show the effects of ARFL in breast cancer diagnosis, we built and evaluated diagnosis models using two independent clinically collected breast imaging datasets, comprising a total of 9,548 mammogram images. We performed extensive experiments showing that our method outperformed several state-of-the-art methods and that our method can enhance safer breast cancer diagnosis against adversarial attacks in clinical settings.","sentences":["Adversarial data can lead to malfunction of deep learning applications.","It is essential to develop deep learning models that are robust to adversarial data while accurate on standard, clean data.","In this study, we proposed a novel adversarially robust feature learning (ARFL) method for a real-world application of breast cancer diagnosis.","ARFL facilitates adversarial training using both standard data and adversarial data, where a feature correlation measure is incorporated as an objective function to encourage learning of robust features and restrain spurious features.","To show the effects of ARFL in breast cancer diagnosis, we built and evaluated diagnosis models using two independent clinically collected breast imaging datasets, comprising a total of 9,548 mammogram images.","We performed extensive experiments showing that our method outperformed several state-of-the-art methods and that our method can enhance safer breast cancer diagnosis against adversarial attacks in clinical settings."],"url":"http://arxiv.org/abs/2402.08768v1","category":"eess.IV"}
{"created":"2024-02-13 19:58:24","title":"A Dataset for the Detection of Dehumanizing Language","abstract":"Dehumanization is a mental process that enables the exclusion and ill treatment of a group of people. In this paper, we present two data sets of dehumanizing text, a large, automatically collected corpus and a smaller, manually annotated data set. Both data sets include a combination of political discourse and dialogue from movie subtitles. Our methods give us a broad and varied amount of dehumanization data to work with, enabling further exploratory analysis and automatic classification of dehumanization patterns. Both data sets will be publicly released.","sentences":["Dehumanization is a mental process that enables the exclusion and ill treatment of a group of people.","In this paper, we present two data sets of dehumanizing text, a large, automatically collected corpus and a smaller, manually annotated data set.","Both data sets include a combination of political discourse and dialogue from movie subtitles.","Our methods give us a broad and varied amount of dehumanization data to work with, enabling further exploratory analysis and automatic classification of dehumanization patterns.","Both data sets will be publicly released."],"url":"http://arxiv.org/abs/2402.08764v1","category":"cs.CL"}
{"created":"2024-02-13 19:54:29","title":"JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models","abstract":"The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency.   We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM's APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement. The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility. Experimental results demonstrate that our approach based on GPT2-XL outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against GPT3.5 175B, a propriety model that is two orders of magnitudes larger.","sentences":["The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums.","In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency.   ","We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship.","Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM's APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement.","The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility.","Experimental results demonstrate that our approach based on GPT2-XL outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against GPT3.5 175B, a propriety model that is two orders of magnitudes larger."],"url":"http://arxiv.org/abs/2402.08761v1","category":"cs.CL"}
{"created":"2024-02-13 19:46:39","title":"LLM-driven Imitation of Subrational Behavior : Illusion or Reality?","abstract":"Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects. Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment. To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios. We conclude by discussing the potential benefits, challenges and limitations of our framework.","sentences":["Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects.","Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct.","In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning.","We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion).","We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment.","To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios.","We conclude by discussing the potential benefits, challenges and limitations of our framework."],"url":"http://arxiv.org/abs/2402.08755v1","category":"cs.AI"}
{"created":"2024-02-13 19:39:11","title":"Forecasting for Swap Regret for All Downstream Agents","abstract":"We study the problem of making predictions so that downstream agents who best respond to them will be guaranteed diminishing swap regret, no matter what their utility functions are. It has been known since Foster and Vohra (1997) that agents who best-respond to calibrated forecasts have no swap regret. Unfortunately, the best known algorithms for guaranteeing calibrated forecasts in sequential adversarial environments do so at rates that degrade exponentially with the dimension of the prediction space. In this work, we show that by making predictions that are not calibrated, but are unbiased subject to a carefully selected collection of events, we can guarantee arbitrary downstream agents diminishing swap regret at rates that substantially improve over the rates that result from calibrated forecasts -- while maintaining the appealing property that our forecasts give guarantees for any downstream agent, without our forecasting algorithm needing to know their utility function.   We give separate results in the ``low'' (1 or 2) dimensional setting and the ``high'' ($> 2$) dimensional setting. In the low dimensional setting, we show how to make predictions such that all agents who best respond to our predictions have diminishing swap regret -- in 1 dimension, at the optimal $O(\\sqrt{T})$ rate. In the high dimensional setting we show how to make forecasts that guarantee regret scaling at a rate of $O(T^{2/3})$ (crucially, a dimension independent exponent), under the assumption that downstream agents smoothly best respond. Our results stand in contrast to rates that derive from agents who best respond to calibrated forecasts, which have an exponential dependence on the dimension of the prediction space.","sentences":["We study the problem of making predictions so that downstream agents who best respond to them will be guaranteed diminishing swap regret, no matter what their utility functions are.","It has been known since Foster and Vohra (1997) that agents who best-respond to calibrated forecasts have no swap regret.","Unfortunately, the best known algorithms for guaranteeing calibrated forecasts in sequential adversarial environments do so at rates that degrade exponentially with the dimension of the prediction space.","In this work, we show that by making predictions that are not calibrated, but are unbiased subject to a carefully selected collection of events, we can guarantee arbitrary downstream agents diminishing swap regret at rates that substantially improve over the rates that result from calibrated forecasts -- while maintaining the appealing property that our forecasts give guarantees for any downstream agent, without our forecasting algorithm needing to know their utility function.   ","We give separate results in the ``low'' (1 or 2) dimensional setting and the ``high'' ($> 2$) dimensional setting.","In the low dimensional setting, we show how to make predictions such that all agents who best respond to our predictions have diminishing swap regret -- in 1 dimension, at the optimal $O(\\sqrt{T})$ rate.","In the high dimensional setting we show how to make forecasts that guarantee regret scaling at a rate of $O(T^{2/3})$ (crucially, a dimension independent exponent), under the assumption that downstream agents smoothly best respond.","Our results stand in contrast to rates that derive from agents who best respond to calibrated forecasts, which have an exponential dependence on the dimension of the prediction space."],"url":"http://arxiv.org/abs/2402.08753v1","category":"cs.GT"}
{"created":"2024-02-13 19:36:23","title":"Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence","abstract":"Quality assessment, including inspecting the images for artifacts, is a critical step during MRI data acquisition to ensure data quality and downstream analysis or interpretation success. This study demonstrates a deep learning model to detect rigid motion in T1-weighted brain images. We leveraged a 2D CNN for three-class classification and tested it on publicly available retrospective and prospective datasets. Grad-CAM heatmaps enabled the identification of failure modes and provided an interpretation of the model's results. The model achieved average precision and recall metrics of 85% and 80% on six motion-simulated retrospective datasets. Additionally, the model's classifications on the prospective dataset showed a strong inverse correlation (-0.84) compared to average edge strength, an image quality metric indicative of motion. This model is part of the ArtifactID tool, aimed at inline automatic detection of Gibbs ringing, wrap-around, and motion artifacts. This tool automates part of the time-consuming QA process and augments expertise on-site, particularly relevant in low-resource settings where local MR knowledge is scarce.","sentences":["Quality assessment, including inspecting the images for artifacts, is a critical step during MRI data acquisition to ensure data quality and downstream analysis or interpretation success.","This study demonstrates a deep learning model to detect rigid motion in T1-weighted brain images.","We leveraged a 2D CNN for three-class classification and tested it on publicly available retrospective and prospective datasets.","Grad-CAM heatmaps enabled the identification of failure modes and provided an interpretation of the model's results.","The model achieved average precision and recall metrics of 85% and 80% on six motion-simulated retrospective datasets.","Additionally, the model's classifications on the prospective dataset showed a strong inverse correlation (-0.84) compared to average edge strength, an image quality metric indicative of motion.","This model is part of the ArtifactID tool, aimed at inline automatic detection of Gibbs ringing, wrap-around, and motion artifacts.","This tool automates part of the time-consuming QA process and augments expertise on-site, particularly relevant in low-resource settings where local MR knowledge is scarce."],"url":"http://arxiv.org/abs/2402.08749v1","category":"cs.CV"}
{"created":"2024-02-13 19:27:34","title":"ADS: Approximate Densest Subgraph for Novel Image Discovery","abstract":"The volume of image repositories continues to grow. Despite the availability of content-based addressing, we still lack a lightweight tool that allows us to discover images of distinct characteristics from a large collection. In this paper, we propose a fast and training-free algorithm for novel image discovery. The key of our algorithm is formulating a collection of images as a perceptual distance-weighted graph, within which our task is to locate the K-densest subgraph that corresponds to a subset of the most unique images. While solving this problem is not just NP-hard but also requires a full computation of the potentially huge distance matrix, we propose to relax it into a K-sparse eigenvector problem that we can efficiently solve using stochastic gradient descent (SGD) without explicitly computing the distance matrix. We compare our algorithm against state-of-the-arts on both synthetic and real datasets, showing that it is considerably faster to run with a smaller memory footprint while able to mine novel images more accurately.","sentences":["The volume of image repositories continues to grow.","Despite the availability of content-based addressing, we still lack a lightweight tool that allows us to discover images of distinct characteristics from a large collection.","In this paper, we propose a fast and training-free algorithm for novel image discovery.","The key of our algorithm is formulating a collection of images as a perceptual distance-weighted graph, within which our task is to locate the K-densest subgraph that corresponds to a subset of the most unique images.","While solving this problem is not just NP-hard but also requires a full computation of the potentially huge distance matrix, we propose to relax it into a K-sparse eigenvector problem that we can efficiently solve using stochastic gradient descent (SGD) without explicitly computing the distance matrix.","We compare our algorithm against state-of-the-arts on both synthetic and real datasets, showing that it is considerably faster to run with a smaller memory footprint while able to mine novel images more accurately."],"url":"http://arxiv.org/abs/2402.08743v1","category":"cs.CV"}
{"created":"2024-02-13 19:27:06","title":"Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize Energy Management in Sports Facilities","abstract":"Anomaly detection in sport facilities has gained significant attention due to its potential to promote energy saving and optimizing operational efficiency. In this research article, we investigate the role of machine learning, particularly deep learning, in anomaly detection for sport facilities. We explore the challenges and perspectives of utilizing deep learning methods for this task, aiming to address the drawbacks and limitations of conventional approaches. Our proposed approach involves feature extraction from the data collected in sport facilities. We present a problem formulation using Deep Feedforward Neural Networks (DFNN) and introduce threshold estimation techniques to identify anomalies effectively. Furthermore, we propose methods to reduce false alarms, ensuring the reliability and accuracy of anomaly detection. To evaluate the effectiveness of our approach, we conduct experiments on aquatic center dataset at Qatar University. The results demonstrate the superiority of our deep learning-based method over conventional techniques, highlighting its potential in real-world applications. Typically, 94.33% accuracy and 92.92% F1-score have been achieved using the proposed scheme.","sentences":["Anomaly detection in sport facilities has gained significant attention due to its potential to promote energy saving and optimizing operational efficiency.","In this research article, we investigate the role of machine learning, particularly deep learning, in anomaly detection for sport facilities.","We explore the challenges and perspectives of utilizing deep learning methods for this task, aiming to address the drawbacks and limitations of conventional approaches.","Our proposed approach involves feature extraction from the data collected in sport facilities.","We present a problem formulation using Deep Feedforward Neural Networks (DFNN) and introduce threshold estimation techniques to identify anomalies effectively.","Furthermore, we propose methods to reduce false alarms, ensuring the reliability and accuracy of anomaly detection.","To evaluate the effectiveness of our approach, we conduct experiments on aquatic center dataset at Qatar University.","The results demonstrate the superiority of our deep learning-based method over conventional techniques, highlighting its potential in real-world applications.","Typically, 94.33% accuracy and 92.92% F1-score have been achieved using the proposed scheme."],"url":"http://arxiv.org/abs/2402.08742v1","category":"cs.CY"}
{"created":"2024-02-13 18:58:16","title":"PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models","abstract":"Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail.","sentences":["Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives.","Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference.","However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts.","In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts.","Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability.","Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories.","We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective.","We further develop an online algorithm with proximal updates to stably optimize the RDP objective.","In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training.","Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail."],"url":"http://arxiv.org/abs/2402.08714v1","category":"cs.LG"}
{"created":"2024-02-13 18:49:09","title":"Combination of measurements of the top quark mass from data collected by the ATLAS and CMS experiments at $\\sqrt{s}=7$ and 8 TeV","abstract":"A combination of fifteen top quark mass measurements performed by the ATLAS and CMS experiments at the LHC is presented. The data sets used correspond to an integrated luminosity of up to 5 and 20$^{-1}$ of proton-proton collisions at center-of-mass energies of 7 and 8 TeV, respectively. The combination includes measurements in top quark pair events that exploit both the semileptonic and hadronic decays of the top quark, and a measurement using events enriched in single top quark production via the electroweak $t$-channel. The combination accounts for the correlations between measurements and achieves an improvement in the total uncertainty of 31% relative to the most precise input measurement. The result is $m_\\mathrm{t}$ = 172.52 $\\pm$ 0.14 (stat) $\\pm$ 0.30 (syst) GeV, with a total uncertainty of 0.33 GeV.","sentences":["A combination of fifteen top quark mass measurements performed by the ATLAS and CMS experiments at the LHC is presented.","The data sets used correspond to an integrated luminosity of up to 5 and 20$^{-1}$ of proton-proton collisions at center-of-mass energies of 7 and 8 TeV, respectively.","The combination includes measurements in top quark pair events that exploit both the semileptonic and hadronic decays of the top quark, and a measurement using events enriched in single top quark production via the electroweak $t$-channel.","The combination accounts for the correlations between measurements and achieves an improvement in the total uncertainty of 31% relative to the most precise input measurement.","The result is $m_\\mathrm{t}$ = 172.52 $\\pm$ 0.14 (stat) $\\pm$ 0.30 (syst) GeV, with a total uncertainty of 0.33 GeV."],"url":"http://arxiv.org/abs/2402.08713v1","category":"hep-ex"}
{"created":"2024-02-13 17:41:43","title":"Data Analytics for Intermodal Freight Transportation Applications","abstract":"With the growth of intermodal freight transportation, it is important that transportation planners and decision makers are knowledgeable about freight flow data to make informed decisions. This is particularly true with Intelligent Transportation Systems (ITS) offering new capabilities to intermodal freight transportation. Specifically, ITS enables access to multiple different data sources, but they have different formats, resolution, and time scales. Thus, knowledge of data science is essential to be successful in future ITS-enabled intermodal freight transportation system. This chapter discusses the commonly used descriptive and predictive data analytic techniques in intermodal freight transportation applications. These techniques cover the entire spectrum of univariate, bivariate, and multivariate analyses. In addition to illustrating how to apply these techniques through relatively simple examples, this chapter will also show how to apply them using the statistical software R. Additional exercises are provided for those who wish to apply the described techniques to more complex problems.","sentences":["With the growth of intermodal freight transportation, it is important that transportation planners and decision makers are knowledgeable about freight flow data to make informed decisions.","This is particularly true with Intelligent Transportation Systems (ITS) offering new capabilities to intermodal freight transportation.","Specifically, ITS enables access to multiple different data sources, but they have different formats, resolution, and time scales.","Thus, knowledge of data science is essential to be successful in future ITS-enabled intermodal freight transportation system.","This chapter discusses the commonly used descriptive and predictive data analytic techniques in intermodal freight transportation applications.","These techniques cover the entire spectrum of univariate, bivariate, and multivariate analyses.","In addition to illustrating how to apply these techniques through relatively simple examples, this chapter will also show how to apply them using the statistical software R. Additional exercises are provided for those who wish to apply the described techniques to more complex problems."],"url":"http://arxiv.org/abs/2402.08707v1","category":"stat.ME"}
{"created":"2024-02-13 16:56:31","title":"A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation","abstract":"Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level observations across different fields. We discuss parallel challenges and approaches between the two applications and highlight future directions for AI-driven de novo drug design as a whole. An organized repository of all covered sources is available at https://github.com/gersteinlab/GenAI4Drug.","sentences":["Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use.","Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction.","Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter.","In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation.","Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models.","We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level observations across different fields.","We discuss parallel challenges and approaches between the two applications and highlight future directions for AI-driven de novo drug design as a whole.","An organized repository of all covered sources is available at https://github.com/gersteinlab/GenAI4Drug."],"url":"http://arxiv.org/abs/2402.08703v1","category":"q-bio.BM"}
{"created":"2024-02-13 16:38:01","title":"PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment","abstract":"Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent prompt and its associated feedback; we use a learned heuristic function that predicts prompt performance to efficiently sample from these candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across eight representative multi-step tasks (an average 27.7% and 28.2% improvement to current best methods on GPT-3.5 and GPT-4, respectively). We further show that the score function for tasks can be modified to better align with individual preferences. We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks. Datasets and Codes are available at https://github.com/yongchao98/PROMST. Project Page is available at https://yongchao98.github.io/MIT-REALM-PROMST.","sentences":["Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task.","LLMs have been successfully used to help find and improve prompt candidates for single-step tasks.","However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution.","While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement.","Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent prompt and its associated feedback; we use a learned heuristic function that predicts prompt performance to efficiently sample from these candidates.","This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across eight representative multi-step tasks (an average 27.7% and 28.2% improvement to current best methods on GPT-3.5 and GPT-4, respectively).","We further show that the score function for tasks can be modified to better align with individual preferences.","We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks.","Datasets and Codes are available at https://github.com/yongchao98/PROMST.","Project Page is available at https://yongchao98.github.io/MIT-REALM-PROMST."],"url":"http://arxiv.org/abs/2402.08702v1","category":"cs.CL"}
{"created":"2024-02-13 12:22:46","title":"Optimal control of collective electrotaxis in epithelial monolayers","abstract":"Epithelial monolayers are some of the best-studied models for collective cell migration due to their abundance in multicellular systems and their tractability. Experimentally, the collective migration of epithelial monolayers can be robustly steered e.g. using electric fields, via a process termed electrotaxis. Theoretically, however, the question of how to design an electric field to achieve a desired spatiotemporal movement pattern is underexplored. In this work, we construct and calibrate an ordinary differential equation model to predict the average velocity of the centre of mass of a cellular monolayer in response to stimulation with an electric field. We use this model, in conjunction with optimal control theory, to derive physically realistic optimal electric field designs to achieve a variety of aims, including maximising the total distance travelled by the monolayer, maximising the monolayer velocity, and keeping the monolayer velocity constant during stimulation. Together, this work is the first to present a unified framework for optimal control of collective monolayer electrotaxis and provides a blueprint to optimally steer collective migration using other external cues.","sentences":["Epithelial monolayers are some of the best-studied models for collective cell migration due to their abundance in multicellular systems and their tractability.","Experimentally, the collective migration of epithelial monolayers can be robustly steered e.g. using electric fields, via a process termed electrotaxis.","Theoretically, however, the question of how to design an electric field to achieve a desired spatiotemporal movement pattern is underexplored.","In this work, we construct and calibrate an ordinary differential equation model to predict the average velocity of the centre of mass of a cellular monolayer in response to stimulation with an electric field.","We use this model, in conjunction with optimal control theory, to derive physically realistic optimal electric field designs to achieve a variety of aims, including maximising the total distance travelled by the monolayer, maximising the monolayer velocity, and keeping the monolayer velocity constant during stimulation.","Together, this work is the first to present a unified framework for optimal control of collective monolayer electrotaxis and provides a blueprint to optimally steer collective migration using other external cues."],"url":"http://arxiv.org/abs/2402.08700v1","category":"q-bio.QM"}
{"created":"2024-02-14 18:59:16","title":"From Architectures to Applications: A Review of Neural Quantum States","abstract":"Due to the exponential growth of the Hilbert space dimension with system size, the simulation of quantum many-body systems has remained a persistent challenge until today. Here, we review a relatively new class of variational states for the simulation of such systems, namely neural quantum states (NQS), which overcome the exponential scaling by compressing the state in terms of the network parameters rather than storing all exponentially many coefficients needed for an exact parameterization of the state. We introduce the commonly used NQS architectures and their various applications for the simulation of ground and excited states, finite temperature and open system states as well as NQS approaches to simulate the dynamics of quantum states. Furthermore, we discuss NQS in the context of quantum state tomography.","sentences":["Due to the exponential growth of the Hilbert space dimension with system size, the simulation of quantum many-body systems has remained a persistent challenge until today.","Here, we review a relatively new class of variational states for the simulation of such systems, namely neural quantum states (NQS), which overcome the exponential scaling by compressing the state in terms of the network parameters rather than storing all exponentially many coefficients needed for an exact parameterization of the state.","We introduce the commonly used NQS architectures and their various applications for the simulation of ground and excited states, finite temperature and open system states as well as NQS approaches to simulate the dynamics of quantum states.","Furthermore, we discuss NQS in the context of quantum state tomography."],"url":"http://arxiv.org/abs/2402.09402v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-14 18:46:28","title":"Non-utopian optical properties computed of a tomographically reconstructed real photonic band gap crystal","abstract":"State-of-the-art computational methods combined with common idealized structural models provide an incomplete understanding of experiments on real nanostructures, since manufacturing introduces unavoidable deviations from the design. We propose to close this knowledge gap by using the real structure of a manufactured crystal as input in computations to obtain a realistic comparison with observations on the same nanostructure. We demonstrate this approach on the structure of a real silicon inverse woodpile photonic bandgap crystal, obtained by previous synchrotron X-ray imaging. A 2D part of the dataset is selected and processed into a computational mesh suitable for a Discontinuous Galerkin Finite Element Method (DGFEM) to compute optical transmission spectra that are compared to those of a utopian crystal, i.e., a hypothetical model crystal with the same filling fraction where all pores are identical and circular. The nanopore shapes in the real crystal differ in a complex way from utopian pores, leading to a complex transmission spectrum with significant frequency speckle in and beyond the gap. The utopian model provides only a limited understanding of the spectrum: while it accurately predicts low frequency finite-size fringes and the lower band edge, the upper band edge is off, it completely misses the presence of speckle, the domination of speckle above the gap, and possible Anderson localized states in the gap. Moreover, unlike experiments where only external probes are available, numerical methods allow to study all fields everywhere. While the pore shapes hardly affect the fields at low frequency, major differences occur at high frequency such as localized fields deep inside the real crystal. In summary, using only external measurements and utopian models may give an erroneous picture of the fields and the LDOS inside a real crystal, which is remedied by our new approach.","sentences":["State-of-the-art computational methods combined with common idealized structural models provide an incomplete understanding of experiments on real nanostructures, since manufacturing introduces unavoidable deviations from the design.","We propose to close this knowledge gap by using the real structure of a manufactured crystal as input in computations to obtain a realistic comparison with observations on the same nanostructure.","We demonstrate this approach on the structure of a real silicon inverse woodpile photonic bandgap crystal, obtained by previous synchrotron X-ray imaging.","A 2D part of the dataset is selected and processed into a computational mesh suitable for a Discontinuous Galerkin Finite Element Method (DGFEM) to compute optical transmission spectra that are compared to those of a utopian crystal, i.e., a hypothetical model crystal with the same filling fraction where all pores are identical and circular.","The nanopore shapes in the real crystal differ in a complex way from utopian pores, leading to a complex transmission spectrum with significant frequency speckle in and beyond the gap.","The utopian model provides only a limited understanding of the spectrum: while it accurately predicts low frequency finite-size fringes and the lower band edge, the upper band edge is off, it completely misses the presence of speckle, the domination of speckle above the gap, and possible Anderson localized states in the gap.","Moreover, unlike experiments where only external probes are available, numerical methods allow to study all fields everywhere.","While the pore shapes hardly affect the fields at low frequency, major differences occur at high frequency such as localized fields deep inside the real crystal.","In summary, using only external measurements and utopian models may give an erroneous picture of the fields and the LDOS inside a real crystal, which is remedied by our new approach."],"url":"http://arxiv.org/abs/2402.09395v1","category":"physics.optics"}
{"created":"2024-02-14 18:26:58","title":"Safe Distributed Control of Multi-Robot Systems with Communication Delays","abstract":"Safe operation of multi-robot systems is critical, especially in communication-degraded environments such as underwater for seabed mapping, underground caves for navigation, and in extraterrestrial missions for assembly and construction. We address safety of networked autonomous systems where the information exchanged between robots incurs communication delays. We formalize a notion of distributed control barrier function (CBF) for multi-robot systems, a safety certificate amenable to a distributed implementation, which provides formal ground to using graph neural networks to learn safe distributed controllers. Further, we observe that learning a distributed controller ignoring delays can severely degrade safety. Our main contribution is a predictor-based framework to train a safe distributed controller under communication delays, where the current state of nearby robots is predicted from received data and age-of-information. Numerical experiments on multi-robot collision avoidance show that our predictor-based approach can significantly improve the safety of a learned distributed controller under communication delays","sentences":["Safe operation of multi-robot systems is critical, especially in communication-degraded environments such as underwater for seabed mapping, underground caves for navigation, and in extraterrestrial missions for assembly and construction.","We address safety of networked autonomous systems where the information exchanged between robots incurs communication delays.","We formalize a notion of distributed control barrier function (CBF) for multi-robot systems, a safety certificate amenable to a distributed implementation, which provides formal ground to using graph neural networks to learn safe distributed controllers.","Further, we observe that learning a distributed controller ignoring delays can severely degrade safety.","Our main contribution is a predictor-based framework to train a safe distributed controller under communication delays, where the current state of nearby robots is predicted from received data and age-of-information.","Numerical experiments on multi-robot collision avoidance show that our predictor-based approach can significantly improve the safety of a learned distributed controller under communication delays"],"url":"http://arxiv.org/abs/2402.09382v1","category":"cs.RO"}
{"created":"2024-02-14 18:26:50","title":"On the thermodynamic limit of two-times measurement entropy production","abstract":"We provide a justification, via the thermodynamic limit, of the modular formula for entropy production in two-times measurement proposed in [Benoist, Bruneau, Jak\\v{s}i\\'c, Panati and Pillet: arxiv:2310.10582]. We consider the cases of open quantum systems in which all thermal reservoirs are either (discrete) quantum spin systems or free Fermi gases.","sentences":["We provide a justification, via the thermodynamic limit, of the modular formula for entropy production in two-times measurement proposed in [Benoist, Bruneau, Jak\\v{s}i\\'c, Panati and Pillet: arxiv:2310.10582].","We consider the cases of open quantum systems in which all thermal reservoirs are either (discrete) quantum spin systems or free Fermi gases."],"url":"http://arxiv.org/abs/2402.09380v1","category":"math-ph"}
{"created":"2024-02-14 18:25:13","title":"Fixed-sparsity matrix approximation from matrix-vector products","abstract":"We study the problem of approximating a matrix $\\mathbf{A}$ with a matrix that has a fixed sparsity pattern (e.g., diagonal, banded, etc.), when $\\mathbf{A}$ is accessed only by matrix-vector products. We describe a simple randomized algorithm that returns an approximation with the given sparsity pattern with Frobenius-norm error at most $(1+\\varepsilon)$ times the best possible error. When each row of the desired sparsity pattern has at most $s$ nonzero entries, this algorithm requires $O(s/\\varepsilon)$ non-adaptive matrix-vector products with $\\mathbf{A}$. We proceed to prove a matching lower-bound. Specifically, we show that for any $s\\geq 1$, there are matrices $\\mathbf{A}$ such that, for any sparsity pattern with $\\Theta(s)$ nonzeros per row and column, any algorithm which obtains a $(1+\\varepsilon)$ accurate approximation of the given sparsity from matrix-vector products requires at least $\\Omega(s/\\varepsilon)$ matrix-vector products. Our bounds therefore resolve the matrix-vector product query complexity of the problem up to constant factors, even for the well-studied case of diagonal approximation, for which no previous lower bounds were known.","sentences":["We study the problem of approximating a matrix $\\mathbf{A}$ with a matrix that has a fixed sparsity pattern (e.g., diagonal, banded, etc.), when $\\mathbf{A}$ is accessed only by matrix-vector products.","We describe a simple randomized algorithm that returns an approximation with the given sparsity pattern with Frobenius-norm error at most $(1+\\varepsilon)$ times the best possible error.","When each row of the desired sparsity pattern has at most $s$ nonzero entries, this algorithm requires $O(s/\\varepsilon)$ non-adaptive matrix-vector products with $\\mathbf{A}$. We proceed to prove a matching lower-bound.","Specifically, we show that for any $s\\geq 1$, there are matrices $\\mathbf{A}$ such that, for any sparsity pattern with $\\Theta(s)$ nonzeros per row and column, any algorithm which obtains a $(1+\\varepsilon)$ accurate approximation of the given sparsity from matrix-vector products requires at least $\\Omega(s/\\varepsilon)$ matrix-vector products.","Our bounds therefore resolve the matrix-vector product query complexity of the problem up to constant factors, even for the well-studied case of diagonal approximation, for which no previous lower bounds were known."],"url":"http://arxiv.org/abs/2402.09379v1","category":"cs.DS"}
{"created":"2024-02-14 18:13:07","title":"A 350-MHz Green Bank Telescope Survey of Unassociated Fermi LAT Sources: Discovery and Timing of Ten Millisecond Pulsars","abstract":"We have searched for radio pulsations towards 49 Fermi Large Area Telescope (LAT) 1FGL Catalog $\\gamma$-ray sources using the Green Bank Telescope at 350 MHz. We detected 18 millisecond pulsars (MSPs) in blind searches of the data; 10 of these were discoveries unique to our survey. Sixteen are binaries, with eight having short orbital periods $P_B < 1$ day. No radio pulsations from young pulsars were detected, although three targets are coincident with apparently radio-quiet $\\gamma$-ray pulsars discovered in LAT data. Here, we give an overview of the survey and present radio and $\\gamma$-ray timing results for the 10 MSPs discovered. These include the only isolated MSP discovered in our survey and six short-$P_B$ binary MSPs. Of these, three have very low-mass companions ($M_c$ $\\ll$ 0.1M$_{\\odot}$) and hence belong to the class of black widow pulsars. Two have more massive, non-degenerate companions with extensive radio eclipses and orbitally modulated X-ray emission consistent with the redback class. Significant $\\gamma$-ray pulsations have been detected from nine of the discoveries. This survey and similar efforts suggest that the majority of Galactic $\\gamma$-ray sources at high Galactic latitudes are either MSPs or relatively nearby non-recycled pulsars, with the latter having on average a much smaller radio/$\\gamma$-ray beaming ratio as compared to MSPs. It also confirms that past surveys suffered from an observational bias against finding short-$P_B$ MSP systems.","sentences":["We have searched for radio pulsations towards 49 Fermi Large Area Telescope (LAT) 1FGL Catalog $\\gamma$-ray sources using the Green Bank Telescope at 350 MHz.","We detected 18 millisecond pulsars (MSPs) in blind searches of the data; 10 of these were discoveries unique to our survey.","Sixteen are binaries, with eight having short orbital periods $P_B < 1$ day.","No radio pulsations from young pulsars were detected, although three targets are coincident with apparently radio-quiet $\\gamma$-ray pulsars discovered in LAT data.","Here, we give an overview of the survey and present radio and $\\gamma$-ray timing results for the 10 MSPs discovered.","These include the only isolated MSP discovered in our survey and six short-$P_B$ binary MSPs.","Of these, three have very low-mass companions ($M_c$ $\\ll$ 0.1M$_{\\odot}$) and hence belong to the class of black widow pulsars.","Two have more massive, non-degenerate companions with extensive radio eclipses and orbitally modulated X-ray emission consistent with the redback class.","Significant $\\gamma$-ray pulsations have been detected from nine of the discoveries.","This survey and similar efforts suggest that the majority of Galactic $\\gamma$-ray sources at high Galactic latitudes are either MSPs or relatively nearby non-recycled pulsars, with the latter having on average a much smaller radio/$\\gamma$-ray beaming ratio as compared to MSPs.","It also confirms that past surveys suffered from an observational bias against finding short-$P_B$ MSP systems."],"url":"http://arxiv.org/abs/2402.09366v1","category":"astro-ph.HE"}
{"created":"2024-02-14 18:09:27","title":"Long-Distance Nuclear Matrix Elements for Neutrinoless Double-Beta Decay from Lattice QCD","abstract":"Neutrinoless double-beta ($0\\nu\\beta\\beta$) decay is a heretofore unobserved process which, if observed, would imply that neutrinos are Majorana particles. Interpretations of the stringent experimental constraints on $0\\nu\\beta\\beta$-decay half-lives require calculations of nuclear matrix elements. This work presents the first lattice quantum-chromodynamics (LQCD) calculation of the matrix element for $0\\nu\\beta\\beta$ decay in a multi-nucleon system, specifically the $nn \\rightarrow pp ee$ transition, mediated by a light left-handed Majorana neutrino propagating over nuclear-scale distances. This calculation is performed with quark masses corresponding to a pion mass of $m_\\pi = 806$ MeV at a single lattice spacing and volume. The statistically cleaner $\\Sigma^- \\rightarrow \\Sigma^+ ee$ transition is also computed in order to investigate various systematic uncertainties. The prospects for matching the results of LQCD calculations onto a nuclear effective field theory to determine a leading-order low-energy constant relevant for $0\\nu\\beta\\beta$ decay with a light Majorana neutrino are investigated. This work, therefore, sets the stage for future calculations at physical values of the quark masses that, combined with effective field theory and nuclear many-body studies, will provide controlled theoretical inputs to experimental searches of $0\\nu\\beta\\beta$ decay.","sentences":["Neutrinoless double-beta ($0\\nu\\beta\\beta$) decay is a heretofore unobserved process which, if observed, would imply that neutrinos are Majorana particles.","Interpretations of the stringent experimental constraints on $0\\nu\\beta\\beta$-decay half-lives require calculations of nuclear matrix elements.","This work presents the first lattice quantum-chromodynamics (LQCD) calculation of the matrix element for $0\\nu\\beta\\beta$ decay in a multi-nucleon system, specifically the $nn \\rightarrow pp ee$ transition, mediated by a light left-handed Majorana neutrino propagating over nuclear-scale distances.","This calculation is performed with quark masses corresponding to a pion mass of $m_\\pi = 806$ MeV at a single lattice spacing and volume.","The statistically cleaner $\\Sigma^- \\rightarrow \\Sigma^+ ee$ transition is also computed in order to investigate various systematic uncertainties.","The prospects for matching the results of LQCD calculations onto a nuclear effective field theory to determine a leading-order low-energy constant relevant for $0\\nu\\beta\\beta$ decay with a light Majorana neutrino are investigated.","This work, therefore, sets the stage for future calculations at physical values of the quark masses that, combined with effective field theory and nuclear many-body studies, will provide controlled theoretical inputs to experimental searches of $0\\nu\\beta\\beta$ decay."],"url":"http://arxiv.org/abs/2402.09362v1","category":"hep-lat"}
{"created":"2024-02-14 17:59:38","title":"Investigation of Ga interstitial and vacancy diffusion in $\u03b2$-Ga$_2$O$_3$ via split defects: a direct approach via master diffusion equations","abstract":"The low symmetry of monoclinic $\\beta$-Ga$_2$O$_3$ leads to elaborate intrinsic defects, such as Ga vacancies split amongst multiple lattice sites. These defects contribute to fast, anisotropic Ga diffusion, yet their complexity makes it challenging to understand dominant diffusion mechanisms. Here, we predict the 3D diffusivity tensors for Ga interstitials (Ga${_i^{3+}}$) and vacancies (V${_{Ga}^{3-}}$) via first principles and direct solution of the master diffusion equations. We first explore the maximum extent of configurationally complex ''$N$-split'' Ga interstitials and vacancies. With dominant low-energy defects identified, we enumerate all possible elementary hops connecting defect configurations to each other, including interstitialcy hops. Hopping barriers are obtained from nudged elastic band simulations. Finally, the comprehensive sets of (i) defect configurations and their energies and (ii) the hopping barriers that connect them are used to construct the master diffusion equations for both Ga${_i^{3+}}$ and V${_{Ga}^{3-}}$. The solution to these equations yields the Onsager transport coefficients, i.e. the components of the 3D diffusivity tensors $D_{{Ga}_i}$ and $D_{V_{Ga}}$ for Ga${_i^{3+}}$ and V${_{Ga}^{3-}}$, respectively. It further reveals the active diffusion paths along all crystallographic directions. We find that both Ga${_i^{3+}}$ and V${_{{Ga}}^{3-}}$ diffusion are fastest along the $c$-axis, due to 3-split defects that bridge neighboring unit cells along the $c$-axis and divert diffusing species around high-energy bottlenecks. Although isolated Ga${_i^{3+}}$ diffuse faster than isolated V${_{Ga}^{3-}}$, self-diffusion of Ga is predominantly mediated by V$_{Ga}^{3-}$ due to the higher V$_{Ga}^{3-}$ defect concentration under most thermodynamic environments.","sentences":["The low symmetry of monoclinic $\\beta$-Ga$_2$O$_3$ leads to elaborate intrinsic defects, such as Ga vacancies split amongst multiple lattice sites.","These defects contribute to fast, anisotropic Ga diffusion, yet their complexity makes it challenging to understand dominant diffusion mechanisms.","Here, we predict the 3D diffusivity tensors for Ga interstitials (Ga${_i^{3+}}$) and vacancies (V${_{Ga}^{3-}}$) via first principles and direct solution of the master diffusion equations.","We first explore the maximum extent of configurationally complex ''$N$-split'' Ga interstitials and vacancies.","With dominant low-energy defects identified, we enumerate all possible elementary hops connecting defect configurations to each other, including interstitialcy hops.","Hopping barriers are obtained from nudged elastic band simulations.","Finally, the comprehensive sets of (i) defect configurations and their energies and (ii) the hopping barriers that connect them are used to construct the master diffusion equations for both Ga${_i^{3+}}$ and V${_{Ga}^{3-}}$.","The solution to these equations yields the Onsager transport coefficients, i.e. the components of the 3D diffusivity tensors $D_{{Ga}_i}$ and $D_{V_{Ga}}$ for Ga${_i^{3+}}$ and V${_{Ga}^{3-}}$, respectively.","It further reveals the active diffusion paths along all crystallographic directions.","We find that both Ga${_i^{3+}}$ and V${_{{Ga}}^{3-}}$ diffusion are fastest along the $c$-axis, due to 3-split defects that bridge neighboring unit cells along the $c$-axis and divert diffusing species around high-energy bottlenecks.","Although isolated Ga${_i^{3+}}$ diffuse faster than isolated V${_{Ga}^{3-}}$, self-diffusion of Ga is predominantly mediated by V$_{Ga}^{3-}$ due to the higher V$_{Ga}^{3-}$ defect concentration under most thermodynamic environments."],"url":"http://arxiv.org/abs/2402.09354v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-14 17:53:21","title":"On the system size dependence of the diffusion coefficients in MD simulations: A simple correction formula for pure dense fluids","abstract":"A practical correction formula relating the self-diffusion coefficient of dense liquids from molecular dynamics simulations with periodic boundary conditions to the self-diffusion coefficient in the thermodynamic limit is discussed. This formula applies to pure dense fluids and has a very simple form $D=D_0(1-\\gamma N^{-1/3})$, where $D_0$ is the self-diffusion coefficient in the thermodynamic limit and $N$ is the number of particles in the simulation. The numerical factor $\\gamma$ depends on the geometry of the simulation cell. Remarkably, $\\gamma\\simeq 1.0$ for the most popular cubic geometry. The success of this formula is supported by results from MD simulations, including very recent simulations with a ``magic'' simulation geometry.","sentences":["A practical correction formula relating the self-diffusion coefficient of dense liquids from molecular dynamics simulations with periodic boundary conditions to the self-diffusion coefficient in the thermodynamic limit is discussed.","This formula applies to pure dense fluids and has a very simple form $D=D_0(1-\\gamma N^{-1/3})$, where $D_0$ is the self-diffusion coefficient in the thermodynamic limit and $N$ is the number of particles in the simulation.","The numerical factor $\\gamma$ depends on the geometry of the simulation cell.","Remarkably, $\\gamma\\simeq 1.0$ for the most popular cubic geometry.","The success of this formula is supported by results from MD simulations, including very recent simulations with a ``magic'' simulation geometry."],"url":"http://arxiv.org/abs/2402.09348v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-14 17:42:51","title":"Robust quasi-isometric embeddings inapproximable by Anosov representations","abstract":"Let $\\mathbb{K}=\\mathbb{R}$ or $\\mathbb{C}$. For all but finitely many $m\\in \\mathbb{N}$, we exhibit the first examples of non-locally rigid, robust quasi-isometric embeddings of hyperbolic groups in $\\mathsf{SL}_m(\\mathbb{K})$ which are not limits of Anosov representations.","sentences":["Let $\\mathbb{K}=\\mathbb{R}$ or $\\mathbb{C}$. For all but finitely many $m\\in \\mathbb{N}$, we exhibit the first examples of non-locally rigid, robust quasi-isometric embeddings of hyperbolic groups in $\\mathsf{SL}_m(\\mathbb{K})$ which are not limits of Anosov representations."],"url":"http://arxiv.org/abs/2402.09339v1","category":"math.GR"}
{"created":"2024-02-14 17:32:57","title":"A Modern Approach to Electoral Delimitation using the Quadtree Data Structure","abstract":"The boundaries of electoral constituencies for assembly and parliamentary seats are drafted using a process referred to as delimitation, which ensures fair and equal representation of all citizens. The current delimitation exercise suffers from a number of drawbacks viz. inefficiency, gerrymandering and an uneven seat-to-population ratio, owing to existing legal and constitutional dictates. The existing methods allocate seats to every state but remain silent about their actual shape and location within the state. The main purpose of this research is to study and analyse the performance of existing delimitation algorithms and further propose a potential solution, along with its merits, that involves using a computational model based on the quadtree data structure to automate the districting process by optimizing objective population criteria. The paper presents an approach to electoral delimitation using the quadtree data structure, which is used to partition a two-dimensional geographical space by recursively subdividing it into four quadrants or regions on the basis of population as a parameter value associated with the node. The quadtree makes use of a quadrant schema of the geographical space for representing constituencies, which not only keeps count of the allocated constituencies but also holds their location-specific information. The performance of the proposed algorithm is analysed and evaluated against existing techniques and proves to be an efficient solution in terms of algorithmic complexity and boundary visualisation to the process of political districting.","sentences":["The boundaries of electoral constituencies for assembly and parliamentary seats are drafted using a process referred to as delimitation, which ensures fair and equal representation of all citizens.","The current delimitation exercise suffers from a number of drawbacks viz.","inefficiency, gerrymandering and an uneven seat-to-population ratio, owing to existing legal and constitutional dictates.","The existing methods allocate seats to every state but remain silent about their actual shape and location within the state.","The main purpose of this research is to study and analyse the performance of existing delimitation algorithms and further propose a potential solution, along with its merits, that involves using a computational model based on the quadtree data structure to automate the districting process by optimizing objective population criteria.","The paper presents an approach to electoral delimitation using the quadtree data structure, which is used to partition a two-dimensional geographical space by recursively subdividing it into four quadrants or regions on the basis of population as a parameter value associated with the node.","The quadtree makes use of a quadrant schema of the geographical space for representing constituencies, which not only keeps count of the allocated constituencies but also holds their location-specific information.","The performance of the proposed algorithm is analysed and evaluated against existing techniques and proves to be an efficient solution in terms of algorithmic complexity and boundary visualisation to the process of political districting."],"url":"http://arxiv.org/abs/2402.09336v1","category":"cs.DS"}
{"created":"2024-02-14 17:32:30","title":"Efficient Unitary T-designs from Random Sums","abstract":"Unitary $T$-designs play an important role in quantum information, with diverse applications in quantum algorithms, benchmarking, tomography, and communication. Until now, the most efficient construction of unitary $T$-designs for $n$-qudit systems has been via random local quantum circuits, which have been shown to converge to approximate $T$-designs in the diamond norm using $O(T^{5+o(1)} n^2)$ quantum gates. In this work, we provide a new construction of $T$-designs via random matrix theory using $\\tilde{O}(T^2 n^2)$ quantum gates. Our construction leverages two key ideas. First, in the spirit of central limit theorems, we approximate the Gaussian Unitary Ensemble (GUE) by an i.i.d. sum of random Hermitian matrices. Second, we show that the product of just two exponentiated GUE matrices is already approximately Haar random. Thus, multiplying two exponentiated sums over rather simple random matrices yields a unitary $T$-design, via Hamiltonian simulation. A central feature of our proof is a new connection between the polynomial method in quantum query complexity and the large-dimension ($N$) expansion in random matrix theory. In particular, we show that the polynomial method provides exponentially improved bounds on the high moments of certain random matrix ensembles, without requiring intricate Weingarten calculations. In doing so, we define and solve a new type of moment problem on the unit circle, asking whether a finite number of equally weighted points, corresponding to eigenvalues of unitary matrices, can reproduce a given set of moments.","sentences":["Unitary $T$-designs play an important role in quantum information, with diverse applications in quantum algorithms, benchmarking, tomography, and communication.","Until now, the most efficient construction of unitary $T$-designs for $n$-qudit systems has been via random local quantum circuits, which have been shown to converge to approximate $T$-designs in the diamond norm using $O(T^{5+o(1)}","n^2)$ quantum gates.","In this work, we provide a new construction of $T$-designs via random matrix theory using $\\tilde{O}(T^2 n^2)$ quantum gates.","Our construction leverages two key ideas.","First, in the spirit of central limit theorems, we approximate the Gaussian Unitary Ensemble (GUE) by an i.i.d. sum of random Hermitian matrices.","Second, we show that the product of just two exponentiated GUE matrices is already approximately Haar random.","Thus, multiplying two exponentiated sums over rather simple random matrices yields a unitary $T$-design, via Hamiltonian simulation.","A central feature of our proof is a new connection between the polynomial method in quantum query complexity and the large-dimension ($N$) expansion in random matrix theory.","In particular, we show that the polynomial method provides exponentially improved bounds on the high moments of certain random matrix ensembles, without requiring intricate Weingarten calculations.","In doing so, we define and solve a new type of moment problem on the unit circle, asking whether a finite number of equally weighted points, corresponding to eigenvalues of unitary matrices, can reproduce a given set of moments."],"url":"http://arxiv.org/abs/2402.09335v1","category":"quant-ph"}
{"created":"2024-02-14 17:22:03","title":"3D-based RNA function prediction tools in rnaglib","abstract":"Understanding the connection between complex structural features of RNA and biological function is a fundamental challenge in evolutionary studies and in RNA design. However, building datasets of RNA 3D structures and making appropriate modeling choices remains time-consuming and lacks standardization. In this chapter, we describe the use of rnaglib, to train supervised and unsupervised machine learning-based function prediction models on datasets of RNA 3D structures.","sentences":["Understanding the connection between complex structural features of RNA and biological function is a fundamental challenge in evolutionary studies and in RNA design.","However, building datasets of RNA 3D structures and making appropriate modeling choices remains time-consuming and lacks standardization.","In this chapter, we describe the use of rnaglib, to train supervised and unsupervised machine learning-based function prediction models on datasets of RNA 3D structures."],"url":"http://arxiv.org/abs/2402.09330v1","category":"q-bio.BM"}
{"created":"2024-02-14 17:15:10","title":"A new mass and radius determination of the ultra-short period planet K2-106b and the fluffy planet K2-106c","abstract":"Ultra-short period planets have orbital periods of less than one day. Since their masses and radii can be determined to a higher precision than long-period planets, they are the preferred targets to determine the density of planets which constrains their composition. The K2-106 system is particularly interesting because it contains two planets of nearly identical masses. One is a high density USP, the other is a low-density planet that has an orbital period of 13 days. Combining the Gaia DR3 results with new ESPRESSO data allows us to determine the masses and radii of the two planets more precisely than before. We find that the USP K2-106b has a density consistent with an Earth-like composition, and K2-106c is a low-density planet that presumably has an extended atmosphere. We measure a radius of Rp=1.676-0.037+0.037 REarth, a mass of Mp=7.80-0.70+0.71 MEarth and a density of rho=9.09-0.98+0.98 gcm-3 for K2-106b. For K2-106c, we derive Rp=2.84-0.08+0.10 REarth, Mp=7.3-2.4+2.5 MEarth, and a density of rho= 1.72-0.58+0.66 gcm-3. We finally discuss the possible structures of the two planets with respect to other low-mass planets.","sentences":["Ultra-short period planets have orbital periods of less than one day.","Since their masses and radii can be determined to a higher precision than long-period planets, they are the preferred targets to determine the density of planets which constrains their composition.","The K2-106 system is particularly interesting because it contains two planets of nearly identical masses.","One is a high density USP, the other is a low-density planet that has an orbital period of 13 days.","Combining the Gaia DR3 results with new ESPRESSO data allows us to determine the masses and radii of the two planets more precisely than before.","We find that the USP K2-106b has a density consistent with an Earth-like composition, and K2-106c is a low-density planet that presumably has an extended atmosphere.","We measure a radius of Rp=1.676-0.037+0.037 REarth, a mass of Mp=7.80-0.70+0.71 MEarth and a density of rho=9.09-0.98+0.98 gcm-3 for K2-106b.","For K2-106c, we derive Rp=2.84-0.08+0.10 REarth, Mp=7.3-2.4+2.5 MEarth, and a density of rho= 1.72-0.58+0.66 gcm-3.","We finally discuss the possible structures of the two planets with respect to other low-mass planets."],"url":"http://arxiv.org/abs/2402.09322v1","category":"astro-ph.EP"}
{"created":"2024-02-14 16:58:21","title":"Quantized Thouless pumps protected by interactions in dimerized Rydberg tweezer arrays","abstract":"We study Thouless pumps, i.e., adiabatic topological transport, in an interacting spin chain described by the dimerized XXZ Hamiltonian. In the noninteracting case, quantized Thouless pumps can only occur when a topological singularity is encircled adiabatically. In contrast, here we show that, in the presence of interactions, such topological transport can even persist for exotic paths in which the system gets arbitrarily close to the singularity. We illustrate the robustness of these exotic Thouless pumps through the behavior of the noninteracting singularity, which for sufficiently strong interactions splits into two singularities separated by a spontaneous antiferromagnetic insulator. We perform a numerical benchmark of these phenomena by means of tensor network simulations of ground-state physics and real-time adiabatic dynamics. Finally, we propose an experimental protocol with Floquet-driven Rydberg tweezer arrays.","sentences":["We study Thouless pumps, i.e., adiabatic topological transport, in an interacting spin chain described by the dimerized XXZ Hamiltonian.","In the noninteracting case, quantized Thouless pumps can only occur when a topological singularity is encircled adiabatically.","In contrast, here we show that, in the presence of interactions, such topological transport can even persist for exotic paths in which the system gets arbitrarily close to the singularity.","We illustrate the robustness of these exotic Thouless pumps through the behavior of the noninteracting singularity, which for sufficiently strong interactions splits into two singularities separated by a spontaneous antiferromagnetic insulator.","We perform a numerical benchmark of these phenomena by means of tensor network simulations of ground-state physics and real-time adiabatic dynamics.","Finally, we propose an experimental protocol with Floquet-driven Rydberg tweezer arrays."],"url":"http://arxiv.org/abs/2402.09311v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-14 16:50:48","title":"Optimal design of equilibrium solutions of the Vlasov-Poisson system by an external electric field","abstract":"A new optimization framework to design steady equilibrium solutions of the Vlasov-Poisson system by means of external electric fields is presented. This optimization framework requires the minimization of an ensemble functional with Tikhonov regularization of the control field under the differential constraint of a nonlinear elliptic equation that models equilibrium solutions of the Vlasov-Poisson system. Existence of optimal control fields and their characterization as solutions to first-order optimality conditions are discussed. Numerical approximations and optimization schemes are developed to validate the proposed framework.","sentences":["A new optimization framework to design steady equilibrium solutions of the Vlasov-Poisson system by means of external electric fields is presented.","This optimization framework requires the minimization of an ensemble functional with Tikhonov regularization of the control field under the differential constraint of a nonlinear elliptic equation that models equilibrium solutions of the Vlasov-Poisson system.","Existence of optimal control fields and their characterization as solutions to first-order optimality conditions are discussed.","Numerical approximations and optimization schemes are developed to validate the proposed framework."],"url":"http://arxiv.org/abs/2402.09306v1","category":"math.OC"}
{"created":"2024-02-14 16:45:52","title":"Exploring semi-inclusive two-nucleon emission in neutrino scattering: a factorized approximation approach","abstract":"The semi-inclusive cross section of two-nucleon emission induced by neutrinos and antineutrinos is computed employing the relativistic mean field model of nuclear matter and the dynamics of meson exchange currents. Within this model we explore a factorization approximation based on the product of an integrated two-hole spectral function and a two-nucleon cross section averaged over hole pairs. We demonstrate that the integrated spectral function of the uncorrelated Fermi gas can be analytically computed, and we derive a simple fully relativistic formula for this function, showcasing its dependency solely on both missing momentum and missing energy. A prescription for the average momenta of the two holes in the factorized two-nucleon cross section is provided, assuming that these momenta are perpendicular to the missing momentum in the center-of-mass system. The validity of the factorized approach is assessed by comparing it with the unfactorized calculation. Our investigation includes the study of the semi-inclusive cross section integrated over the energy of one of the emitted nucleons and the cross section integrated over the emission angles of the two nucleons and the outgoing muon kinematics. A comparison is made with the pure phase-space model and other models from the literature. The results of this analysis offer valuable insights into the influence of the semi-inclusive hadronic tensor on the cross section, providing a deeper understanding of the underlying nuclear processes.","sentences":["The semi-inclusive cross section of two-nucleon emission induced by neutrinos and antineutrinos is computed employing the relativistic mean field model of nuclear matter and the dynamics of meson exchange currents.","Within this model we explore a factorization approximation based on the product of an integrated two-hole spectral function and a two-nucleon cross section averaged over hole pairs.","We demonstrate that the integrated spectral function of the uncorrelated Fermi gas can be analytically computed, and we derive a simple fully relativistic formula for this function, showcasing its dependency solely on both missing momentum and missing energy.","A prescription for the average momenta of the two holes in the factorized two-nucleon cross section is provided, assuming that these momenta are perpendicular to the missing momentum in the center-of-mass system.","The validity of the factorized approach is assessed by comparing it with the unfactorized calculation.","Our investigation includes the study of the semi-inclusive cross section integrated over the energy of one of the emitted nucleons and the cross section integrated over the emission angles of the two nucleons and the outgoing muon kinematics.","A comparison is made with the pure phase-space model and other models from the literature.","The results of this analysis offer valuable insights into the influence of the semi-inclusive hadronic tensor on the cross section, providing a deeper understanding of the underlying nuclear processes."],"url":"http://arxiv.org/abs/2402.09301v1","category":"hep-ph"}
{"created":"2024-02-14 16:37:09","title":"Reconstructing a state-independent cost function in a mean-field game model","abstract":"In this short note, we consider an inverse problem to a mean-field games system where we are interested in reconstructing the state-independent running cost function from observed value-function data. We provide an elementary proof of a uniqueness result for the inverse problem using the standard multilinearization technique. One of the main features of our work is that we insist that the population distribution be a probability measure, a requirement that is not enforced in some of the existing literature on theoretical inverse mean-field games.","sentences":["In this short note, we consider an inverse problem to a mean-field games system where we are interested in reconstructing the state-independent running cost function from observed value-function data.","We provide an elementary proof of a uniqueness result for the inverse problem using the standard multilinearization technique.","One of the main features of our work is that we insist that the population distribution be a probability measure, a requirement that is not enforced in some of the existing literature on theoretical inverse mean-field games."],"url":"http://arxiv.org/abs/2402.09297v1","category":"math.AP"}
{"created":"2024-02-14 16:36:29","title":"Mean eigenvector self-overlap in the real and complex elliptic Ginibre ensembles at strong and weak non-Hermiticity","abstract":"We study the mean diagonal overlap of left and right eigenvectors associated with complex eigenvalues in $N\\times N$ non-Hermitian random Gaussian matrices. In well known works by Chalker and Mehlig the expectation of this (self-)overlap was computed for the complex Ginibre ensemble as $N\\to \\infty$. In the present work, we consider the same quantity in the real and complex elliptic Ginibre ensembles characterized by correlations between off-diagonal entries controlled by a parameter $\\tau\\in[0,1]$, with $\\tau=1$ corresponding to the Hermitian limit. We derive exact expressions for the mean diagonal overlap in both ensembles at any finite $N$, for any eigenvalue off the real axis. We further investigate several scaling regimes as $N\\rightarrow \\infty$, both in the limit of strong non-Hermiticity keeping a fixed $\\tau\\in[0,1)$ and in the weak non-Hermiticity limit, with $\\tau$ approaching unity in such a way that $N(1-\\tau)$ remains finite.","sentences":["We study the mean diagonal overlap of left and right eigenvectors associated with complex eigenvalues in $N\\times N$ non-Hermitian random Gaussian matrices.","In well known works by Chalker and Mehlig the expectation of this (self-)overlap was computed for the complex Ginibre ensemble as $N\\to \\infty$. In the present work, we consider the same quantity in the real and complex elliptic Ginibre ensembles characterized by correlations between off-diagonal entries controlled by a parameter $\\tau\\in[0,1]$, with $\\tau=1$ corresponding to the Hermitian limit.","We derive exact expressions for the mean diagonal overlap in both ensembles at any finite $N$, for any eigenvalue off the real axis.","We further investigate several scaling regimes as $N\\rightarrow \\infty$, both in the limit of strong non-Hermiticity keeping a fixed $\\tau\\in[0,1)$ and in the weak non-Hermiticity limit, with $\\tau$ approaching unity in such a way that $N(1-\\tau)$ remains finite."],"url":"http://arxiv.org/abs/2402.09296v1","category":"math-ph"}
{"created":"2024-02-14 16:31:58","title":"Analysis of an Adaptive Safeguarded Newton-Anderson Algorithm with Applications to Fluid Problems","abstract":"The purpose of this paper is to develop a practical strategy to accelerate Newton's method in the vicinity of singular points. We do this by developing an adaptive safeguarding scheme, which we call gamma-safeguarding, that one can apply to Anderson accelerated Newton's method when solving problems near singular points. The key features of adaptive gamma-safeguarding are that it converges locally for singular problems, and it can detect nonsingular problems, in which case the Newton-Anderson iterates are scaled towards a standard Newton step. This leads to faster local convergence compared to both Newton's method and Newton-Anderson without safeguarding, at no additional computational cost. We demonstrate three strategies one can use when implementing Newton-Anderson and gamma-safeguarded Newton-Anderson to solve parameter-dependent problems near singular points. For our benchmark problems, we take two parameter-dependent incompressible flow systems: flow in a channel and Rayleigh-Benard convection.","sentences":["The purpose of this paper is to develop a practical strategy to accelerate Newton's method in the vicinity of singular points.","We do this by developing an adaptive safeguarding scheme, which we call gamma-safeguarding, that one can apply to Anderson accelerated Newton's method when solving problems near singular points.","The key features of adaptive gamma-safeguarding are that it converges locally for singular problems, and it can detect nonsingular problems, in which case the Newton-Anderson iterates are scaled towards a standard Newton step.","This leads to faster local convergence compared to both Newton's method and Newton-Anderson without safeguarding, at no additional computational cost.","We demonstrate three strategies one can use when implementing Newton-Anderson and gamma-safeguarded Newton-Anderson to solve parameter-dependent problems near singular points.","For our benchmark problems, we take two parameter-dependent incompressible flow systems: flow in a channel and Rayleigh-Benard convection."],"url":"http://arxiv.org/abs/2402.09295v1","category":"math.NA"}
{"created":"2024-02-14 16:26:10","title":"New proof of the Gaussian integral using the residue theorem with links to the Riemann Zeta function","abstract":"In this paper the Gaussian integral is proven using contour integration on $\\frac{1}{e^{x^2}+1}$ and linking it using a limit to said Gaussian integral. The limit is alsorelated to the Riemann Zeta function using a few manipulations. This new and original proof comes as an addition to the already many pre-existing proofs of the Gaussian integral.","sentences":["In this paper the Gaussian integral is proven using contour integration on $\\frac{1}{e^{x^2}+1}$ and linking it using a limit to said Gaussian integral.","The limit is alsorelated to the Riemann Zeta function using a few manipulations.","This new and original proof comes as an addition to the already many pre-existing proofs of the Gaussian integral."],"url":"http://arxiv.org/abs/2402.09292v1","category":"math.CV"}
{"created":"2024-02-14 16:10:31","title":"Proton-proton and proton-cluster femtoscopy at the HADES experiment","abstract":"This work explores femtoscopic correlations in proton-proton and proton-cluster systems at the HADES experiment, GSI. Through high-precision correlation functions, it reveals the impact of strong interactions between protons and light nuclei (deuteron, triton, Helium-3). For proton-proton interactions, measurements in two centralities and five $k_T$ bins were analyzed. Extracted radii were compared with different Equations of State in UrQMD, providing valuable insights into collision dynamics. This research enhances our understanding of particle interactions and contributes to refining theoretical models in nuclear and particle physics.This work explores femtoscopic correlations in proton-proton and proton-cluster systems at the HADES experiment, GSI. Through high-precision correlation functions, it reveals the impact of strong interactions between protons and light nuclei (deuteron, triton, Helium-3). For proton-proton interactions, measurements in two centralities and five $k_T$ bins were analyzed. Extracted radii were compared with different Equations of State in UrQMD, providing valuable insights into collision dynamics. This research enhances our understanding of particle interactions and contributes to refining theoretical models in nuclear and particle physics.","sentences":["This work explores femtoscopic correlations in proton-proton and proton-cluster systems at the HADES experiment, GSI.","Through high-precision correlation functions, it reveals the impact of strong interactions between protons and light nuclei (deuteron, triton, Helium-3).","For proton-proton interactions, measurements in two centralities and five $k_T$ bins were analyzed.","Extracted radii were compared with different Equations of State in UrQMD, providing valuable insights into collision dynamics.","This research enhances our understanding of particle interactions and contributes to refining theoretical models in nuclear and particle physics.","This work explores femtoscopic correlations in proton-proton and proton-cluster systems at the HADES experiment, GSI.","Through high-precision correlation functions, it reveals the impact of strong interactions between protons and light nuclei (deuteron, triton, Helium-3).","For proton-proton interactions, measurements in two centralities and five $k_T$ bins were analyzed.","Extracted radii were compared with different Equations of State in UrQMD, providing valuable insights into collision dynamics.","This research enhances our understanding of particle interactions and contributes to refining theoretical models in nuclear and particle physics."],"url":"http://arxiv.org/abs/2402.09280v1","category":"nucl-ex"}
{"created":"2024-02-14 16:09:22","title":"Disentangling the origin of chemical differences using GHOST","abstract":"Aims. We explore different scenarios to explain the chemical difference found in the remarkable giant-giant binary system HD 138202 + CD-30 12303. For the first time, we suggest how to distinguish these scenarios by taking advantage of the extensive convective envelopes of giant stars. Methods. We carried out a high-precision determination of stellar parameters and abundances by applying a full line-by-line differential analysis on GHOST high-resolution spectra. Results. We found a significant chemical difference between the two stars (0.08 dex), which is largely unexpected considering the insensitivity of giant stars to planetary ingestion and diffusion effects. We tested the possibility of engulfment events by using several different combinations of stellar mass, ingested mass, metallicity of the engulfed object and different convective envelopes. However, the planetary ingestion scenario does not seem to explain the observed differences. For the first time, we distinguished the source of chemical differences using a giant-giant binary system. By ruling out other possible scenarios such as planet formation and evolutionary effects between the two stars, we suggest that primordial inhomogeneities might explain the observed differences. This remarkable result implies that the metallicity differences that were observed in at least some main-sequence binary systems might be related to primordial inhomogeneities rather than engulfment events. We also discuss the important implications of finding primordial inhomogeneities, which affect chemical tagging and other fields such as planet formation. We strongly encourage the use of giant-giant pairs. They are a relevant complement to main-sequence pairs for determining the origin of the observed chemical differences in multiple systems. [abridged]","sentences":["Aims.","We explore different scenarios to explain the chemical difference found in the remarkable giant-giant binary system HD 138202 + CD-30 12303.","For the first time, we suggest how to distinguish these scenarios by taking advantage of the extensive convective envelopes of giant stars.","Methods.","We carried out a high-precision determination of stellar parameters and abundances by applying a full line-by-line differential analysis on GHOST high-resolution spectra. Results.","We found a significant chemical difference between the two stars (0.08 dex), which is largely unexpected considering the insensitivity of giant stars to planetary ingestion and diffusion effects.","We tested the possibility of engulfment events by using several different combinations of stellar mass, ingested mass, metallicity of the engulfed object and different convective envelopes.","However, the planetary ingestion scenario does not seem to explain the observed differences.","For the first time, we distinguished the source of chemical differences using a giant-giant binary system.","By ruling out other possible scenarios such as planet formation and evolutionary effects between the two stars, we suggest that primordial inhomogeneities might explain the observed differences.","This remarkable result implies that the metallicity differences that were observed in at least some main-sequence binary systems might be related to primordial inhomogeneities rather than engulfment events.","We also discuss the important implications of finding primordial inhomogeneities, which affect chemical tagging and other fields such as planet formation.","We strongly encourage the use of giant-giant pairs.","They are a relevant complement to main-sequence pairs for determining the origin of the observed chemical differences in multiple systems.","[abridged]"],"url":"http://arxiv.org/abs/2402.09278v1","category":"astro-ph.SR"}
{"created":"2024-02-14 16:08:33","title":"A Modular Deep Learning-based Approach for Diffuse Optical Tomography Reconstruction","abstract":"Medical imaging is nowadays a pillar in diagnostics and therapeutic follow-up. Current research tries to integrate established - but ionizing - tomographic techniques with technologies offering reduced radiation exposure. Diffuse Optical Tomography (DOT) uses non-ionizing light in the Near-Infrared (NIR) window to reconstruct optical coefficients in living beings, providing functional indications about the composition of the investigated organ/tissue. Due to predominant light scattering at NIR wavelengths, DOT reconstruction is, however, a severely ill-conditioned inverse problem. Conventional reconstruction approaches show severe weaknesses when dealing also with mildly complex cases and/or are computationally very intensive. In this work we explore deep learning techniques for DOT inversion. Namely, we propose a fully data-driven approach based on a modularity concept: first data and originating signal are separately processed via autoencoders, then the corresponding low-dimensional latent spaces are connected via a bridging network which acts at the same time as a learned regularizer.","sentences":["Medical imaging is nowadays a pillar in diagnostics and therapeutic follow-up.","Current research tries to integrate established - but ionizing - tomographic techniques with technologies offering reduced radiation exposure.","Diffuse Optical Tomography (DOT) uses non-ionizing light in the Near-Infrared (NIR) window to reconstruct optical coefficients in living beings, providing functional indications about the composition of the investigated organ/tissue.","Due to predominant light scattering at NIR wavelengths, DOT reconstruction is, however, a severely ill-conditioned inverse problem.","Conventional reconstruction approaches show severe weaknesses when dealing also with mildly complex cases and/or are computationally very intensive.","In this work we explore deep learning techniques for DOT inversion.","Namely, we propose a fully data-driven approach based on a modularity concept: first data and originating signal are separately processed via autoencoders, then the corresponding low-dimensional latent spaces are connected via a bridging network which acts at the same time as a learned regularizer."],"url":"http://arxiv.org/abs/2402.09277v1","category":"math.NA"}
{"created":"2024-02-14 15:56:42","title":"Fast Window-Based Event Denoising with Spatiotemporal Correlation Enhancement","abstract":"Previous deep learning-based event denoising methods mostly suffer from poor interpretability and difficulty in real-time processing due to their complex architecture designs. In this paper, we propose window-based event denoising, which simultaneously deals with a stack of events while existing element-based denoising focuses on one event each time. Besides, we give the theoretical analysis based on probability distributions in both temporal and spatial domains to improve interpretability. In temporal domain, we use timestamp deviations between processing events and central event to judge the temporal correlation and filter out temporal-irrelevant events. In spatial domain, we choose maximum a posteriori (MAP) to discriminate real-world event and noise, and use the learned convolutional sparse coding to optimize the objective function. Based on the theoretical analysis, we build Temporal Window (TW) module and Soft Spatial Feature Embedding (SSFE) module to process temporal and spatial information separately, and construct a novel multi-scale window-based event denoising network, named MSDNet. The high denoising accuracy and fast running speed of our MSDNet enables us to achieve real-time denoising in complex scenes. Extensive experimental results verify the effectiveness and robustness of our MSDNet. Our algorithm can remove event noise effectively and efficiently and improve the performance of downstream tasks.","sentences":["Previous deep learning-based event denoising methods mostly suffer from poor interpretability and difficulty in real-time processing due to their complex architecture designs.","In this paper, we propose window-based event denoising, which simultaneously deals with a stack of events while existing element-based denoising focuses on one event each time.","Besides, we give the theoretical analysis based on probability distributions in both temporal and spatial domains to improve interpretability.","In temporal domain, we use timestamp deviations between processing events and central event to judge the temporal correlation and filter out temporal-irrelevant events.","In spatial domain, we choose maximum a posteriori (MAP) to discriminate real-world event and noise, and use the learned convolutional sparse coding to optimize the objective function.","Based on the theoretical analysis, we build Temporal Window (TW) module and Soft Spatial Feature Embedding (SSFE) module to process temporal and spatial information separately, and construct a novel multi-scale window-based event denoising network, named MSDNet.","The high denoising accuracy and fast running speed of our MSDNet enables us to achieve real-time denoising in complex scenes.","Extensive experimental results verify the effectiveness and robustness of our MSDNet.","Our algorithm can remove event noise effectively and efficiently and improve the performance of downstream tasks."],"url":"http://arxiv.org/abs/2402.09270v1","category":"cs.CV"}
{"created":"2024-02-14 15:39:56","title":"Max-Min Fair Energy-Efficient Beam Design for Quantized ISAC LEO Satellite Systems: A Rate-Splitting Approach","abstract":"Low earth orbit (LEO) satellite systems with sensing functionality is envisioned to facilitate global-coverage service and emerging applications in 6G. Currently, two fundamental challenges, namely, inter-beam interference among users and power limitation at the LEO satellites, limit the full potential of the joint design of sensing and communication. To effectively control the interference, rate-splitting multiple access (RSMA) scheme is employed as the interference management strategy in the system design. On the other hand, to address the limited power supply at the LEO satellites, we consider low-resolution quantization digital-to-analog converters (DACs) at the transmitter to reduce power consumption, which grows exponentially with the number of quantization bits. Additionally, optimizing the total energy efficiency (EE) of the system is a common practice to save the power. However, this metric lacks fairness among users. To ensure this fairness and further enhance EE, we investigate the max-min fairness EE of the RSMA-assisted integrated sensing and communications (ISAC)-LEO satellite system. In this system, the satellite transmits a quantized dual-functional signal serving downlink users while detecting a target. Specifically, we optimize the precoders for maximizing the minimal EE among all users, considering the power consumption of each radio frequency (RF) chain under communication and sensing constraints. To tackle this optimization problem, we proposed an iterative algorithm based on successive convex approximation (SCA) and Dinkelbach's method. Numerical results illustrate that the proposed design outperforms the strategies that aim to maximize the total EE of the system and conventional space-division multiple access (SDMA) in terms of max-min fairness EE and the communication-sensing trade-off.","sentences":["Low earth orbit (LEO) satellite systems with sensing functionality is envisioned to facilitate global-coverage service and emerging applications in 6G. Currently, two fundamental challenges, namely, inter-beam interference among users and power limitation at the LEO satellites, limit the full potential of the joint design of sensing and communication.","To effectively control the interference, rate-splitting multiple access (RSMA) scheme is employed as the interference management strategy in the system design.","On the other hand, to address the limited power supply at the LEO satellites, we consider low-resolution quantization digital-to-analog converters (DACs) at the transmitter to reduce power consumption, which grows exponentially with the number of quantization bits.","Additionally, optimizing the total energy efficiency (EE) of the system is a common practice to save the power.","However, this metric lacks fairness among users.","To ensure this fairness and further enhance EE, we investigate the max-min fairness EE of the RSMA-assisted integrated sensing and communications (ISAC)-LEO satellite system.","In this system, the satellite transmits a quantized dual-functional signal serving downlink users while detecting a target.","Specifically, we optimize the precoders for maximizing the minimal EE among all users, considering the power consumption of each radio frequency (RF) chain under communication and sensing constraints.","To tackle this optimization problem, we proposed an iterative algorithm based on successive convex approximation (SCA) and Dinkelbach's method.","Numerical results illustrate that the proposed design outperforms the strategies that aim to maximize the total EE of the system and conventional space-division multiple access (SDMA) in terms of max-min fairness EE and the communication-sensing trade-off."],"url":"http://arxiv.org/abs/2402.09253v1","category":"eess.SP"}
{"created":"2024-02-14 15:35:53","title":"Momentum Approximation in Asynchronous Private Federated Learning","abstract":"Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum approximation can achieve $1.15 \\textrm{--}4\\times$ speed up in convergence compared to existing asynchronous FL optimizers with momentum.","sentences":["Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients.","Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL.","However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance.","It is still unclear how to effective combinie these two techniques together to achieve a win-win.","In this paper, we find that asynchrony introduces implicit bias to momentum updates.","In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates.","Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost.","We empirically demonstrate that on benchmark FL datasets, momentum approximation can achieve $1.15 \\textrm{--}4\\times$ speed up in convergence compared to existing asynchronous FL optimizers with momentum."],"url":"http://arxiv.org/abs/2402.09247v1","category":"cs.LG"}
{"created":"2024-02-14 15:23:40","title":"Some Characterizations of Weakly Uniformly Perfect Sets","abstract":"In this paper, the concept of weakly uniform perfectness is considered. As an analogue of the theory of uniform perfectness, we obtain the relationships between weakly uniform perfectness and Bergman kernel, Poincar\\'e metric and Hausdorff content. In particular, for a bounded domain $\\Omega \\subset \\mathbb{C}$, we show that the uniform perfectness of $\\partial \\Omega$ is equivalent to $K_{\\Omega}(z) \\gtrsim \\delta(z)^{-2}$, where $K_{\\Omega}(z)$ is the Bergman kernel of $\\Omega$ and $\\delta(z)$ denotes the boundary distance.","sentences":["In this paper, the concept of weakly uniform perfectness is considered.","As an analogue of the theory of uniform perfectness, we obtain the relationships between weakly uniform perfectness and Bergman kernel, Poincar\\'e metric and Hausdorff content.","In particular, for a bounded domain $\\Omega \\subset \\mathbb{C}$, we show that the uniform perfectness of $\\partial \\Omega$ is equivalent to $K_{\\Omega}(z)","\\gtrsim \\delta(z)^{-2}$, where $K_{\\Omega}(z)$ is the Bergman kernel of $\\Omega$ and $\\delta(z)$ denotes the boundary distance."],"url":"http://arxiv.org/abs/2402.09235v1","category":"math.CV"}
{"created":"2024-02-14 15:22:59","title":"Multi-Hierarchical Surrogate Learning for Structural Dynamics of Automotive Crashworthiness Using Graph Convolutional Neural Networks","abstract":"Crash simulations play an essential role in improving vehicle safety, design optimization, and injury risk estimation. Unfortunately, numerical solutions of such problems using state-of-the-art high-fidelity models require significant computational effort. Conventional data-driven surrogate modeling approaches create low-dimensional embeddings for evolving the dynamics in order to circumvent this computational effort. Most approaches directly operate on high-resolution data obtained from numerical discretization, which is both costly and complicated for mapping the flow of information over large spatial distances. Furthermore, working with a fixed resolution prevents the adaptation of surrogate models to environments with variable computing capacities, different visualization resolutions, and different accuracy requirements. We thus propose a multi-hierarchical framework for structurally creating a series of surrogate models for a kart frame, which is a good proxy for industrial-relevant crash simulations, at different levels of resolution. For multiscale phenomena, macroscale features are captured on a coarse surrogate, whereas microscale effects are resolved by finer ones. The learned behavior of the individual surrogates is passed from coarse to finer levels through transfer learning. In detail, we perform a mesh simplification on the kart model to obtain multi-resolution representations of it. We then train a graph-convolutional neural network-based surrogate that learns parameter-dependent low-dimensional latent dynamics on the coarsest representation. Subsequently, another, similarly structured surrogate is trained on the residual of the first surrogate using a finer resolution. This step can be repeated multiple times. By doing so, we construct multiple surrogates for the same system with varying hardware requirements and increasing accuracy.","sentences":["Crash simulations play an essential role in improving vehicle safety, design optimization, and injury risk estimation.","Unfortunately, numerical solutions of such problems using state-of-the-art high-fidelity models require significant computational effort.","Conventional data-driven surrogate modeling approaches create low-dimensional embeddings for evolving the dynamics in order to circumvent this computational effort.","Most approaches directly operate on high-resolution data obtained from numerical discretization, which is both costly and complicated for mapping the flow of information over large spatial distances.","Furthermore, working with a fixed resolution prevents the adaptation of surrogate models to environments with variable computing capacities, different visualization resolutions, and different accuracy requirements.","We thus propose a multi-hierarchical framework for structurally creating a series of surrogate models for a kart frame, which is a good proxy for industrial-relevant crash simulations, at different levels of resolution.","For multiscale phenomena, macroscale features are captured on a coarse surrogate, whereas microscale effects are resolved by finer ones.","The learned behavior of the individual surrogates is passed from coarse to finer levels through transfer learning.","In detail, we perform a mesh simplification on the kart model to obtain multi-resolution representations of it.","We then train a graph-convolutional neural network-based surrogate that learns parameter-dependent low-dimensional latent dynamics on the coarsest representation.","Subsequently, another, similarly structured surrogate is trained on the residual of the first surrogate using a finer resolution.","This step can be repeated multiple times.","By doing so, we construct multiple surrogates for the same system with varying hardware requirements and increasing accuracy."],"url":"http://arxiv.org/abs/2402.09234v1","category":"cs.LG"}
{"created":"2024-02-14 15:21:17","title":"Investigating Premature Convergence in Co-optimization of Morphology and Control in Evolved Virtual Soft Robots","abstract":"Evolving virtual creatures is a field with a rich history and recently it has been getting more attention, especially in the soft robotics domain. The compliance of soft materials endows soft robots with complex behavior, but it also makes their design process unintuitive and in need of automated design. Despite the great interest, evolved virtual soft robots lack the complexity, and co-optimization of morphology and control remains a challenging problem. Prior work identifies and investigates a major issue with the co-optimization process -- fragile co-adaptation of brain and body resulting in premature convergence of morphology. In this work, we expand the investigation of this phenomenon by comparing learnable controllers with proprioceptive observations and fixed controllers without any observations, whereas in the latter case, we only have the optimization of the morphology. Our experiments in two morphology spaces and two environments that vary in complexity show, concrete examples of the existence of high-performing regions in the morphology space that are not able to be discovered during the co-optimization of the morphology and control, yet exist and are easily findable when optimizing morphologies alone. Thus this work clearly demonstrates and characterizes the challenges of optimizing morphology during co-optimization. Based on these results, we propose a new body-centric framework to think about the co-optimization problem which helps us understand the issue from a search perspective. We hope the insights we share with this work attract more attention to the problem and help us to enable efficient brain-body co-optimization.","sentences":["Evolving virtual creatures is a field with a rich history and recently it has been getting more attention, especially in the soft robotics domain.","The compliance of soft materials endows soft robots with complex behavior, but it also makes their design process unintuitive and in need of automated design.","Despite the great interest, evolved virtual soft robots lack the complexity, and co-optimization of morphology and control remains a challenging problem.","Prior work identifies and investigates a major issue with the co-optimization process -- fragile co-adaptation of brain and body resulting in premature convergence of morphology.","In this work, we expand the investigation of this phenomenon by comparing learnable controllers with proprioceptive observations and fixed controllers without any observations, whereas in the latter case, we only have the optimization of the morphology.","Our experiments in two morphology spaces and two environments that vary in complexity show, concrete examples of the existence of high-performing regions in the morphology space that are not able to be discovered during the co-optimization of the morphology and control, yet exist and are easily findable when optimizing morphologies alone.","Thus this work clearly demonstrates and characterizes the challenges of optimizing morphology during co-optimization.","Based on these results, we propose a new body-centric framework to think about the co-optimization problem which helps us understand the issue from a search perspective.","We hope the insights we share with this work attract more attention to the problem and help us to enable efficient brain-body co-optimization."],"url":"http://arxiv.org/abs/2402.09231v1","category":"cs.RO"}
{"created":"2024-02-14 15:07:02","title":"Prescribing singularities of weak solutions of a nonlinear elliptic system in the plane","abstract":"Inspired by Frehse's [1] 1973 work, we show that his elliptic system $\\Delta u = F(u, \\nabla u)$ in the plane has bounded weak solutions $u$ with arbitrarily prescribed singular sets.","sentences":["Inspired by Frehse's [1] 1973 work, we show that his elliptic system $\\Delta u = F(u, \\nabla u)$ in the plane has bounded weak solutions $u$ with arbitrarily prescribed singular sets."],"url":"http://arxiv.org/abs/2402.09224v1","category":"math.AP"}
{"created":"2024-02-14 15:01:21","title":"Integrating ytopt and libEnsemble to Autotune OpenMC","abstract":"ytopt is a Python machine-learning-based autotuning software package developed within the ECP PROTEAS-TUNE project. The ytopt software adopts an asynchronous search framework that consists of sampling a small number of input parameter configurations and progressively fitting a surrogate model over the input-output space until exhausting the user-defined maximum number of evaluations or the wall-clock time. libEnsemble is a Python toolkit for coordinating workflows of asynchronous and dynamic ensembles of calculations across massively parallel resources developed within the ECP PETSc/TAO project. libEnsemble helps users take advantage of massively parallel resources to solve design, decision, and inference problems and expands the class of problems that can benefit from increased parallelism. In this paper we present our methodology and framework to integrate ytopt and libEnsemble to take advantage of massively parallel resources to accelerate the autotuning process. Specifically, we focus on using the proposed framework to autotune the ECP ExaSMR application OpenMC, an open source Monte Carlo particle transport code. OpenMC has seven tunable parameters some of which have large ranges such as the number of particles in-flight, which is in the range of 100,000 to 8 million, with its default setting of 1 million. Setting the proper combination of these parameter values to achieve the best performance is extremely time-consuming. Therefore, we apply the proposed framework to autotune the MPI/OpenMP offload version of OpenMC based on a user-defined metric such as the figure of merit (FoM) (particles/s) or energy efficiency energy-delay product (EDF) on the OLCF Frontier TDS system Crusher. The experimental results show that we achieve improvement up to 29.49% in FoM and up to 30.44% in EDP.","sentences":["ytopt is a Python machine-learning-based autotuning software package developed within the ECP PROTEAS-TUNE project.","The ytopt software adopts an asynchronous search framework that consists of sampling a small number of input parameter configurations and progressively fitting a surrogate model over the input-output space until exhausting the user-defined maximum number of evaluations or the wall-clock time.","libEnsemble is a Python toolkit for coordinating workflows of asynchronous and dynamic ensembles of calculations across massively parallel resources developed within the ECP PETSc/TAO project.","libEnsemble helps users take advantage of massively parallel resources to solve design, decision, and inference problems and expands the class of problems that can benefit from increased parallelism.","In this paper we present our methodology and framework to integrate ytopt and libEnsemble to take advantage of massively parallel resources to accelerate the autotuning process.","Specifically, we focus on using the proposed framework to autotune the ECP ExaSMR application OpenMC, an open source Monte Carlo particle transport code.","OpenMC has seven tunable parameters some of which have large ranges such as the number of particles in-flight, which is in the range of 100,000 to 8 million, with its default setting of 1 million.","Setting the proper combination of these parameter values to achieve the best performance is extremely time-consuming.","Therefore, we apply the proposed framework to autotune the MPI/OpenMP offload version of OpenMC based on a user-defined metric such as the figure of merit (FoM) (particles/s) or energy efficiency energy-delay product (EDF) on the OLCF Frontier TDS system Crusher.","The experimental results show that we achieve improvement up to 29.49% in FoM and up to 30.44% in EDP."],"url":"http://arxiv.org/abs/2402.09222v1","category":"cs.PF"}
{"created":"2024-02-14 14:54:36","title":"Inferentialist Resource Semantics","abstract":"In systems modelling, a system typically comprises located resources relative to which processes execute. One important use of logic in informatics is in modelling such systems for the purpose of reasoning (perhaps automated) about their behaviour and properties. To this end, one requires an interpretation of logical formulae in terms of the resources and states of the system; such an interpretation is called a resource semantics of the logic. This paper shows how inferentialism -- the view that meaning is given in terms of inferential behaviour -- enables a versatile and expressive framework for resource semantics. Specifically, how inferentialism seamlessly incorporates the assertion-based approach of the logic of Bunched Implications, foundational in program verification (e.g., as the basis of Separation Logic), and the renowned number-of-uses reading of Linear Logic. This integration enables reasoning about shared and separated resources in intuitive and familiar ways, as well as about the composition and interfacing of system components.","sentences":["In systems modelling, a system typically comprises located resources relative to which processes execute.","One important use of logic in informatics is in modelling such systems for the purpose of reasoning (perhaps automated) about their behaviour and properties.","To this end, one requires an interpretation of logical formulae in terms of the resources and states of the system; such an interpretation is called a resource semantics of the logic.","This paper shows how inferentialism -- the view that meaning is given in terms of inferential behaviour -- enables a versatile and expressive framework for resource semantics.","Specifically, how inferentialism seamlessly incorporates the assertion-based approach of the logic of Bunched Implications, foundational in program verification (e.g., as the basis of Separation Logic), and the renowned number-of-uses reading of Linear Logic.","This integration enables reasoning about shared and separated resources in intuitive and familiar ways, as well as about the composition and interfacing of system components."],"url":"http://arxiv.org/abs/2402.09217v1","category":"cs.LO"}
{"created":"2024-02-14 14:39:54","title":"Liquid-liquid phase separation of proteins is modulated by amino acids in vitro and in vivo","abstract":"Liquid liquid phase separation (LLPS) of proteins is an intracellular process that is widely used by cells for many purposes. In living cells (in vivo), LLPS occurs in complex and crowded environments. Amino acids (AAs) are vital components of such environments, occupying a significant fraction of the cellular volume. In this work, we studied the effects of proline and other proteinogenic AAs on the LLPS of proteins, both in test tubes (in vitro) and in cells (in vivo). The effects of proline on the protein-protein interaction (PPI) and LLPS of both bovine serum albumin (BSA, a folded protein) and the low-complexity domain of fused in sarcoma (FUS267, an intrinsically disordered protein) is first established in vitro. Then, the effects of proline and other proteinogenic AAs on the formation of stress granules (SGs) by LLPS in U2OS and HeLa cells are studied. We find that the presence of AAs renders the net interaction between proteins more repulsive (i.e. stabilizes protein solution), thus suppressing protein phase separation in vitro and in vivo. We also show that the formation of SGs is suppressed by AAs using both immunofluorescence and live-cell microscopy. Our study reveals an underappreciated role of cellular AAs in modulating intracellular phase separation. It may find biomedical applications, especially in the treatment of protein aggregation diseases.","sentences":["Liquid liquid phase separation (LLPS) of proteins is an intracellular process that is widely used by cells for many purposes.","In living cells (in vivo), LLPS occurs in complex and crowded environments.","Amino acids (AAs) are vital components of such environments, occupying a significant fraction of the cellular volume.","In this work, we studied the effects of proline and other proteinogenic AAs on the LLPS of proteins, both in test tubes (in vitro) and in cells (in vivo).","The effects of proline on the protein-protein interaction (PPI) and LLPS of both bovine serum albumin (BSA, a folded protein) and the low-complexity domain of fused in sarcoma (FUS267, an intrinsically disordered protein) is first established in vitro.","Then, the effects of proline and other proteinogenic AAs on the formation of stress granules (SGs) by LLPS in U2OS and HeLa cells are studied.","We find that the presence of AAs renders the net interaction between proteins more repulsive (i.e. stabilizes protein solution), thus suppressing protein phase separation in vitro and in vivo.","We also show that the formation of SGs is suppressed by AAs using both immunofluorescence and live-cell microscopy.","Our study reveals an underappreciated role of cellular AAs in modulating intracellular phase separation.","It may find biomedical applications, especially in the treatment of protein aggregation diseases."],"url":"http://arxiv.org/abs/2402.09206v1","category":"physics.bio-ph"}
{"created":"2024-02-14 14:08:06","title":"Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through Sparse Gaussian Process","abstract":"Nowadays, sensor suits have been equipped with redundant LiDARs and IMUs to mitigate the risks associated with sensor failure. It is challenging for the previous discrete-time and IMU-driven kinematic systems to incorporate multiple asynchronized sensors, which are susceptible to abnormal IMU data. To address these limitations, we introduce a multi-LiDAR multi-IMU state estimator by taking advantage of Gaussian Process (GP) that predicts a non-parametric continuous-time trajectory to capture sensors' spatial-temporal movement with limited control states. Since the kinematic model driven by three types of linear time-invariant stochastic differential equations are independent of external sensor measurements, our proposed approach is capable of handling different sensor configurations and resilient to sensor failures. Moreover, we replace the conventional $\\mathrm{SE}(3)$ state representation with the combination of $\\mathrm{SO}(3)$ and vector space, which enables GP-based LiDAR-inertial system to fulfill the real-time requirement. Extensive experiments on the public datasets demonstrate the versatility and resilience of our proposed multi-LiDAR multi-IMU state estimator. To contribute to the community, we will make our source code publicly available.","sentences":["Nowadays, sensor suits have been equipped with redundant LiDARs and IMUs to mitigate the risks associated with sensor failure.","It is challenging for the previous discrete-time and IMU-driven kinematic systems to incorporate multiple asynchronized sensors, which are susceptible to abnormal IMU data.","To address these limitations, we introduce a multi-LiDAR multi-IMU state estimator by taking advantage of Gaussian Process (GP) that predicts a non-parametric continuous-time trajectory to capture sensors' spatial-temporal movement with limited control states.","Since the kinematic model driven by three types of linear time-invariant stochastic differential equations are independent of external sensor measurements, our proposed approach is capable of handling different sensor configurations and resilient to sensor failures.","Moreover, we replace the conventional $\\mathrm{SE}(3)$ state representation with the combination of $\\mathrm{SO}(3)$ and vector space, which enables GP-based LiDAR-inertial system to fulfill the real-time requirement.","Extensive experiments on the public datasets demonstrate the versatility and resilience of our proposed multi-LiDAR multi-IMU state estimator.","To contribute to the community, we will make our source code publicly available."],"url":"http://arxiv.org/abs/2402.09189v1","category":"cs.RO"}
{"created":"2024-02-14 13:57:25","title":"Flattability of Priority Vector Addition Systems","abstract":"Vector addition systems (VAS), also known as Petri nets, are a popular model of concurrent systems. Many problems from many areas reduce to the reachability problem for VAS, which consists of deciding whether a target configuration of a VAS is reachable from a given initial configuration. One of the main approaches to solve the problem on practical instances is called flattening, intuitively removing nested loops. This technique is known to terminate for semilinear VAS. In this paper, we prove that also for VAS with nested zero tests, called Priority VAS, flattening does in fact terminate for all semilinear reachability relations. Furthermore, we prove that Priority VAS admit semilinear inductive invariants. Both of these results are obtained by defining a well-quasi-order on runs of Priority VAS which has good pumping properties.","sentences":["Vector addition systems (VAS), also known as Petri nets, are a popular model of concurrent systems.","Many problems from many areas reduce to the reachability problem for VAS, which consists of deciding whether a target configuration of a VAS is reachable from a given initial configuration.","One of the main approaches to solve the problem on practical instances is called flattening, intuitively removing nested loops.","This technique is known to terminate for semilinear VAS.","In this paper, we prove that also for VAS with nested zero tests, called Priority VAS, flattening does in fact terminate for all semilinear reachability relations.","Furthermore, we prove that Priority VAS admit semilinear inductive invariants.","Both of these results are obtained by defining a well-quasi-order on runs of Priority VAS which has good pumping properties."],"url":"http://arxiv.org/abs/2402.09185v1","category":"cs.FL"}
{"created":"2024-02-14 13:54:34","title":"Design Space of Visual Feedforward And Corrective Feedback in XR-Based Motion Guidance Systems","abstract":"Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements -- referred to as motion guidance. In motion guidance, the \"feedforward\" provides instructional cues of the motions that are to be performed, whereas the \"feedback\" provides cues which help correct mistakes and minimize errors. Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored. Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them. We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions. We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR.","sentences":["Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements -- referred to as motion guidance.","In motion guidance, the \"feedforward\" provides instructional cues of the motions that are to be performed, whereas the \"feedback\" provides cues which help correct mistakes and minimize errors.","Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored.","Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them.","We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions.","We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR."],"url":"http://arxiv.org/abs/2402.09182v1","category":"cs.HC"}
{"created":"2024-02-14 13:44:20","title":"Stochastic Comparisons of Random Extremes from non-identical Random Variables","abstract":"We propose some new results on the comparison of the minimum or maximum order statistic from a random number of non-identical random variables. Under the non-identical set-up, with certain conditions, we prove that random minimum (maximum) of one system dominates the other in hazard rate (reversed hazard rate) order. Further, we prove variation diminishing property (Karlin [8]) for all possible restrictions to derive the new results.","sentences":["We propose some new results on the comparison of the minimum or maximum order statistic from a random number of non-identical random variables.","Under the non-identical set-up, with certain conditions, we prove that random minimum (maximum) of one system dominates the other in hazard rate (reversed hazard rate) order.","Further, we prove variation diminishing property (Karlin [8]) for all possible restrictions to derive the new results."],"url":"http://arxiv.org/abs/2402.09174v1","category":"math.ST"}
{"created":"2024-02-14 13:43:58","title":"Davydov's soliton in an external alternating magnetic field","abstract":"The influence of an external oscillating in time magnetic field on the dynamics of the Davydov's soliton is investigated. It is shown that it essentially depends not only on the amplitude and frequency of the magnetic field, but also on the field orientation with respect to the molecular chain axis. The soliton velocity and phase are calculated. They are oscillating in time functions with the frequency of the main harmonic, given by the external field frequency, and higher multiple harmonics. It is concluded that such complex effects of external time-depending magnetic fields on the dynamics of solitons modify the charge transport in low-dimensional molecular systems, which can affect functioning of the devices based on such systems. These results suggest also the physical mechanism of therapeutic effects of oscillating magnetic fields, based on the field influence on the dynamics of solitons which provide charge transport through biological macro-molecules in the redox processes.","sentences":["The influence of an external oscillating in time magnetic field on the dynamics of the Davydov's soliton is investigated.","It is shown that it essentially depends not only on the amplitude and frequency of the magnetic field, but also on the field orientation with respect to the molecular chain axis.","The soliton velocity and phase are calculated.","They are oscillating in time functions with the frequency of the main harmonic, given by the external field frequency, and higher multiple harmonics.","It is concluded that such complex effects of external time-depending magnetic fields on the dynamics of solitons modify the charge transport in low-dimensional molecular systems, which can affect functioning of the devices based on such systems.","These results suggest also the physical mechanism of therapeutic effects of oscillating magnetic fields, based on the field influence on the dynamics of solitons which provide charge transport through biological macro-molecules in the redox processes."],"url":"http://arxiv.org/abs/2402.09172v1","category":"cond-mat.soft"}
{"created":"2024-02-14 13:40:48","title":"Enhancing energy storage crossing quantum phase transitions in an integrable spin quantum battery","abstract":"We investigate the performance of a one dimensional dimerized XY chain as a quantum battery. Such integrable model shows a rich quantum phase diagram which emerges through a mapping of the spins into auxiliary fermionic degrees of freedom. We consider a charging protocol relying on the double quench of an internal parameter, notably the strength of the dimerization. Within this picture we observe a substantial enhancement of the energy stored per spin as a consequence of driving the system across certain quantum phase transitions.","sentences":["We investigate the performance of a one dimensional dimerized XY chain as a quantum battery.","Such integrable model shows a rich quantum phase diagram which emerges through a mapping of the spins into auxiliary fermionic degrees of freedom.","We consider a charging protocol relying on the double quench of an internal parameter, notably the strength of the dimerization.","Within this picture we observe a substantial enhancement of the energy stored per spin as a consequence of driving the system across certain quantum phase transitions."],"url":"http://arxiv.org/abs/2402.09169v1","category":"quant-ph"}
{"created":"2024-02-14 13:27:31","title":"Cosmic radiation drives quasi-periodic changes in the diversity of siliceous marine microplankton","abstract":"Radiolarians are significant contributors to the oceanic primary productivity and the global silica cycle in the last 500 Myr. Their diversity throughout the Phanerozoic shows periodic fluctuations. We identify a possible abiotic candidate for driving these patterns which seems to potentially influence radiolarian diversity changes during this period at a significance level of $\\sim 2.2 \\sigma$. Our finding suggests a significant correlation between the origination of new radiolaria species and maximum excursions of the Solar system from the Galactic plane, where the magnetic shielding of cosmic rays is expected to be weaker. We connect the particularly strong radiolaria blooming during the Middle Triassic to the so-called Mesozoic dipole-low of the geomagnetic field, which was in its deepest state when radiolarias were blooming. According to the scenario, high-energy cosmic rays presumably implied particular damage to the DNA during the maximum excursions which may trigger large chromosomal abnormalities leading to the appearance of a large number of new genera and species during these periods.","sentences":["Radiolarians are significant contributors to the oceanic primary productivity and the global silica cycle in the last 500 Myr.","Their diversity throughout the Phanerozoic shows periodic fluctuations.","We identify a possible abiotic candidate for driving these patterns which seems to potentially influence radiolarian diversity changes during this period at a significance level of $\\sim 2.2 \\sigma$.","Our finding suggests a significant correlation between the origination of new radiolaria species and maximum excursions of the Solar system from the Galactic plane, where the magnetic shielding of cosmic rays is expected to be weaker.","We connect the particularly strong radiolaria blooming during the Middle Triassic to the so-called Mesozoic dipole-low of the geomagnetic field, which was in its deepest state when radiolarias were blooming.","According to the scenario, high-energy cosmic rays presumably implied particular damage to the DNA during the maximum excursions which may trigger large chromosomal abnormalities leading to the appearance of a large number of new genera and species during these periods."],"url":"http://arxiv.org/abs/2402.09163v1","category":"q-bio.PE"}
{"created":"2024-02-14 13:14:00","title":"Joint and Robust Beamforming Framework for Integrated Sensing and Communication Systems","abstract":"Integrated sensing and communication (ISAC) is widely recognized as a fundamental enabler for future wireless communications. In this paper, we present a joint communication and radar beamforming framework for maximizing a sum spectral efficiency (SE) while guaranteeing desired radar performance with imperfect channel state information (CSI) in multi-user and multi-target ISAC systems. To this end, we adopt either a radar transmit beam mean square error (MSE) or receive signal-to-clutter-plus-noise ratio (SCNR) as a radar performance constraint of a sum SE maximization problem. To resolve inherent challenges such as non-convexity and imperfect CSI, we reformulate the problems and identify first-order optimality conditions for the joint radar and communication beamformer. Turning the condition to a nonlinear eigenvalue problem with eigenvector dependency (NEPv), we develop an alternating method which finds the joint beamformer through power iteration and a Lagrangian multiplier through binary search. The proposed framework encompasses both the radar metrics and is robust to channel estimation error with low complexity. Simulations validate the proposed methods. In particular, we observe that the MSE and SCNR constraints exhibit complementary performance depending on the operating environment, which manifests the importance of the proposed comprehensive and robust optimization framework.","sentences":["Integrated sensing and communication (ISAC) is widely recognized as a fundamental enabler for future wireless communications.","In this paper, we present a joint communication and radar beamforming framework for maximizing a sum spectral efficiency (SE) while guaranteeing desired radar performance with imperfect channel state information (CSI) in multi-user and multi-target ISAC systems.","To this end, we adopt either a radar transmit beam mean square error (MSE) or receive signal-to-clutter-plus-noise ratio (SCNR) as a radar performance constraint of a sum SE maximization problem.","To resolve inherent challenges such as non-convexity and imperfect CSI, we reformulate the problems and identify first-order optimality conditions for the joint radar and communication beamformer.","Turning the condition to a nonlinear eigenvalue problem with eigenvector dependency (NEPv), we develop an alternating method which finds the joint beamformer through power iteration and a Lagrangian multiplier through binary search.","The proposed framework encompasses both the radar metrics and is robust to channel estimation error with low complexity.","Simulations validate the proposed methods.","In particular, we observe that the MSE and SCNR constraints exhibit complementary performance depending on the operating environment, which manifests the importance of the proposed comprehensive and robust optimization framework."],"url":"http://arxiv.org/abs/2402.09155v1","category":"eess.SP"}
{"created":"2024-02-14 12:59:43","title":"Better Decremental and Fully Dynamic Sensitivity Oracles for Subgraph Connectivity","abstract":"We study the \\emph{sensitivity oracles problem for subgraph connectivity} in the \\emph{decremental} and \\emph{fully dynamic} settings. In the fully dynamic setting, we preprocess an $n$-vertices $m$-edges undirected graph $G$ with $n_{\\rm off}$ deactivated vertices initially and the others are activated. Then we receive a single update $D\\subseteq V(G)$ of size $|D| = d \\leq d_{\\star}$, representing vertices whose states will be switched. Finally, we get a sequence of queries, each of which asks the connectivity of two given vertices $u$ and $v$ in the activated subgraph. The decremental setting is a special case when there is no deactivated vertex initially, and it is also known as the \\emph{vertex-failure connectivity oracles} problem.   We present a better deterministic vertex-failure connectivity oracle with $\\widehat{O}(d_{\\star}m)$ preprocessing time, $\\widetilde{O}(m)$ space, $\\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which improves the update time of the previous almost-optimal oracle [Long-Saranurak, FOCS 2022] from $\\widehat{O}(d^{2})$ to $\\widetilde{O}(d^{2})$.   We also present a better deterministic fully dynamic sensitivity oracle for subgraph connectivity with $\\widehat{O}(\\min\\{m(n_{\\rm off} + d_{\\star}),n^{\\omega}\\})$ preprocessing time, $\\widetilde{O}(\\min\\{m(n_{\\rm off} + d_{\\star}),n^{2}\\})$ space, $\\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which significantly improves the update time of the state of the art [Hu-Kosinas-Polak, 2023] from $\\widetilde{O}(d^{4})$ to $\\widetilde{O}(d^{2})$. Furthermore, our solution is even almost-optimal assuming popular fine-grained complexity conjectures.","sentences":["We study the \\emph{sensitivity oracles problem for subgraph connectivity} in the \\emph{decremental} and \\emph{fully dynamic} settings.","In the fully dynamic setting, we preprocess an $n$-vertices $m$-edges undirected graph $G$ with $n_{\\rm off}$ deactivated vertices initially and the others are activated.","Then we receive a single update $D\\subseteq V(G)$ of size $|D| = d \\leq d_{\\star}$, representing vertices whose states will be switched.","Finally, we get a sequence of queries, each of which asks the connectivity of two given vertices $u$ and $v$ in the activated subgraph.","The decremental setting is a special case when there is no deactivated vertex initially, and it is also known as the \\emph{vertex-failure connectivity oracles} problem.   ","We present a better deterministic vertex-failure connectivity oracle with $\\widehat{O}(d_{\\star}m)$ preprocessing time, $\\widetilde{O}(m)$ space, $\\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which improves the update time of the previous almost-optimal oracle [Long-Saranurak, FOCS 2022] from $\\widehat{O}(d^{2})$ to $\\widetilde{O}(d^{2})$.   We also present a better deterministic fully dynamic sensitivity oracle for subgraph connectivity with $\\widehat{O}(\\min\\{m(n_{\\rm off} + d_{\\star}),n^{\\omega}\\})$ preprocessing time, $\\widetilde{O}(\\min\\{m(n_{\\rm off} + d_{\\star}),n^{2}\\})$ space, $\\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which significantly improves the update time of the state of the art [Hu-Kosinas-Polak, 2023] from $\\widetilde{O}(d^{4})$ to $\\widetilde{O}(d^{2})$. Furthermore, our solution is even almost-optimal assuming popular fine-grained complexity conjectures."],"url":"http://arxiv.org/abs/2402.09150v1","category":"cs.DS"}
{"created":"2024-02-14 12:57:26","title":"BiasEye: A Bias-Aware Real-time Interactive Material Screening System for Impartial Candidate Assessment","abstract":"In the process of evaluating competencies for job or student recruitment through material screening, decision-makers can be influenced by inherent cognitive biases, such as the screening order or anchoring information, leading to inconsistent outcomes. To tackle this challenge, we conducted interviews with seven experts to understand their challenges and needs for support in the screening process. Building on their insights, we introduce BiasEye, a bias-aware real-time interactive material screening visualization system. BiasEye enhances awareness of cognitive biases by improving information accessibility and transparency. It also aids users in identifying and mitigating biases through a machine learning (ML) approach that models individual screening preferences. Findings from a mixed-design user study with 20 participants demonstrate that, compared to a baseline system lacking our bias-aware features, BiasEye increases participants' bias awareness and boosts their confidence in making final decisions. At last, we discuss the potential of ML and visualization in mitigating biases during human decision-making tasks.","sentences":["In the process of evaluating competencies for job or student recruitment through material screening, decision-makers can be influenced by inherent cognitive biases, such as the screening order or anchoring information, leading to inconsistent outcomes.","To tackle this challenge, we conducted interviews with seven experts to understand their challenges and needs for support in the screening process.","Building on their insights, we introduce BiasEye, a bias-aware real-time interactive material screening visualization system.","BiasEye enhances awareness of cognitive biases by improving information accessibility and transparency.","It also aids users in identifying and mitigating biases through a machine learning (ML) approach that models individual screening preferences.","Findings from a mixed-design user study with 20 participants demonstrate that, compared to a baseline system lacking our bias-aware features, BiasEye increases participants' bias awareness and boosts their confidence in making final decisions.","At last, we discuss the potential of ML and visualization in mitigating biases during human decision-making tasks."],"url":"http://arxiv.org/abs/2402.09148v1","category":"cs.HC"}
{"created":"2024-02-14 12:55:28","title":"ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks","abstract":"In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between layers. By inserting residual blocks between quanvolutional layers, we ensure enhanced gradient access throughout the network, leading to improved training performance. Moreover, we provide empirical evidence on the strategic placement of these residual blocks within QuNNs. Through extensive experimentation, we identify an efficient configuration of residual blocks, which enables gradients across all the layers in the network that eventually results in efficient training. Our findings suggest that the precise location of residual blocks plays a crucial role in maximizing the performance gains in QuNNs. Our results mark a substantial step forward in the evolution of quantum deep learning, offering new avenues for both theoretical development and practical quantum computing applications.","sentences":["In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them.","Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability.","Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs.","However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers.","To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between layers.","By inserting residual blocks between quanvolutional layers, we ensure enhanced gradient access throughout the network, leading to improved training performance.","Moreover, we provide empirical evidence on the strategic placement of these residual blocks within QuNNs.","Through extensive experimentation, we identify an efficient configuration of residual blocks, which enables gradients across all the layers in the network that eventually results in efficient training.","Our findings suggest that the precise location of residual blocks plays a crucial role in maximizing the performance gains in QuNNs.","Our results mark a substantial step forward in the evolution of quantum deep learning, offering new avenues for both theoretical development and practical quantum computing applications."],"url":"http://arxiv.org/abs/2402.09146v1","category":"cs.LG"}
{"created":"2024-02-14 12:48:17","title":"When Representations Align: Universality in Representation Learning Dynamics","abstract":"Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits phenomena similar to the \"rich\" and \"lazy\" regime. While many network behaviors depend quantitatively on architecture, our findings point to certain behaviors that are widely conserved once models are sufficiently flexible.","sentences":["Deep neural networks come in many sizes and architectures.","The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations.","Yet, recent results have shown that different architectures learn representations with striking qualitative similarities.","Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions.","This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization.","We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits phenomena similar to the \"rich\" and \"lazy\" regime.","While many network behaviors depend quantitatively on architecture, our findings point to certain behaviors that are widely conserved once models are sufficiently flexible."],"url":"http://arxiv.org/abs/2402.09142v1","category":"cs.LG"}
{"created":"2024-02-14 12:39:48","title":"The Photochemistry of Rydberg Excited Cyclobutanone: Photoinduced Processes and Ground State Dynamics","abstract":"Owing to ring-strain, cyclic ketones exhibit complex excited-state dynamics with multiple competing photochemical channels active on the ultrafast timescale. While the excited-state dynamics of cyclobutanone after $\\pi^{\\ast}\\leftarrow n$ excitation into the lowest-energy excited singlet state (S$_1$) has been extensively studied, the dynamics following 3$s\\leftarrow n$ excitation into the higher-lying singlet Rydberg (S$_2$) state are less well understood. Herein, we couple quantum and excited-state trajectory surface-hopping molecular dynamics simulations to study the relaxation of cyclobutanone following 3s$\\leftarrow n$ excitation and to predict the ultrafast electron diffraction scattering signal that we anticipate to arise from the relaxation dynamics that we observe. Our simulations indicate that relaxation from the initially-populated singlet Rydberg state occurs on the hundreds-of-femtosecond to picosecond timescale consistent with the symmetry-forbidden nature of the state-to-state transition involved. Once cyclobutanone has relaxed non-radiatively to the electronic ground state (S$_0$), the vibrationally hot molecules have sufficient energy to form multiple fragmentory products on the electronic ground-state surface including C$_2$H$_4$ + CH$_2$CO (C2; 20%), and C$_3$H$_6$ + CO (C3; 2.5%). We discuss the limitations of our simulations, how these may influence the outcome of the excited-state dynamics we observe, and -- ultimately -- the predictive power of the simulated experimental observable.","sentences":["Owing to ring-strain, cyclic ketones exhibit complex excited-state dynamics with multiple competing photochemical channels active on the ultrafast timescale.","While the excited-state dynamics of cyclobutanone after $\\pi^{\\ast}\\leftarrow n$ excitation into the lowest-energy excited singlet state (S$_1$) has been extensively studied, the dynamics following 3$s\\leftarrow n$ excitation into the higher-lying singlet Rydberg (S$_2$) state are less well understood.","Herein, we couple quantum and excited-state trajectory surface-hopping molecular dynamics simulations to study the relaxation of cyclobutanone following 3s$\\leftarrow n$ excitation and to predict the ultrafast electron diffraction scattering signal that we anticipate to arise from the relaxation dynamics that we observe.","Our simulations indicate that relaxation from the initially-populated singlet Rydberg state occurs on the hundreds-of-femtosecond to picosecond timescale consistent with the symmetry-forbidden nature of the state-to-state transition involved.","Once cyclobutanone has relaxed non-radiatively to the electronic ground state (S$_0$), the vibrationally hot molecules have sufficient energy to form multiple fragmentory products on the electronic ground-state surface including C$_2$H$_4$ + CH$_2$CO (C2; 20%), and C$_3$H$_6$ + CO (C3; 2.5%).","We discuss the limitations of our simulations, how these may influence the outcome of the excited-state dynamics we observe, and -- ultimately -- the predictive power of the simulated experimental observable."],"url":"http://arxiv.org/abs/2402.09140v1","category":"physics.chem-ph"}
{"created":"2024-02-14 12:38:22","title":"Unifying Graded Linear Logic and Differential Operators","abstract":"Linear Logic refines Intuitionnistic Logic by taking into account the resources used during the proof and program computation. In the past decades, it has been extended to various frameworks. The most famous are indexed linear logics which can describe the resource management or the complexity analysis of a program. From an other perspective, Differential Linear Logic is an extension which allows the linearization of proofs. In this article, we merge these two directions by first defining a differential version of Graded linear logic: this is made by indexing exponential connectives with a monoid of differential operators. We prove that it is equivalent to a graded version of previously defined extension of finitary differential linear logic. We give a denotational model of our logic, based on distribution theory and linear partial differential operators with constant coefficients.","sentences":["Linear Logic refines Intuitionnistic Logic by taking into account the resources used during the proof and program computation.","In the past decades, it has been extended to various frameworks.","The most famous are indexed linear logics which can describe the resource management or the complexity analysis of a program.","From an other perspective, Differential Linear Logic is an extension which allows the linearization of proofs.","In this article, we merge these two directions by first defining a differential version of Graded linear logic: this is made by indexing exponential connectives with a monoid of differential operators.","We prove that it is equivalent to a graded version of previously defined extension of finitary differential linear logic.","We give a denotational model of our logic, based on distribution theory and linear partial differential operators with constant coefficients."],"url":"http://arxiv.org/abs/2402.09138v1","category":"cs.LO"}
{"created":"2024-02-14 12:33:43","title":"Balancing the Norwegian regulated power market anno 2016 to 2022","abstract":"The balancing market for power is designed to account for the difference between predicted supply/demand of electricity and the realised supply/demand. However, increased electrification of society changes the consumption patterns, and increased production from renewable sources leads to larger un-predicted fluctuations in production, both effects potentially leading to increased balancing. We analyse public market data for the balancing market (manual Frequency Restoration Reserve) for Norway from 2016 to 2022 to investigate and document these effects. The data is newer than for similar analyses and the eight years of data is more than double the time span previously covered. The main findings are: a) The balancing volumes are dominated by hours of zero regulation but for non-zero hours, the balancing volumes are increasing during the eight-year period. b) The balancing prices are primarily correlated with day-ahead prices and secondary with balancing volumes. The latter correlation is found to be increasingly non-linear with time. c) The balancing volumes and the price difference between balancing price and day-ahead price are strongly correlated with the previous hour. d) The increasing share of wind power has not impacted the frequency of balancing, which has remained stable during the 8 years studied. However, the volumes and share of balancing power compared to overall production have increased, suggesting that the hours which are inherently difficult to predict remain the same. e) Market data alone cannot predict balancing volumes. If attempting, the auto-correlation becomes the main source of information.","sentences":["The balancing market for power is designed to account for the difference between predicted supply/demand of electricity and the realised supply/demand.","However, increased electrification of society changes the consumption patterns, and increased production from renewable sources leads to larger un-predicted fluctuations in production, both effects potentially leading to increased balancing.","We analyse public market data for the balancing market (manual Frequency Restoration Reserve) for Norway from 2016 to 2022 to investigate and document these effects.","The data is newer than for similar analyses and the eight years of data is more than double the time span previously covered.","The main findings are: a)","The balancing volumes are dominated by hours of zero regulation but for non-zero hours, the balancing volumes are increasing during the eight-year period.","b)","The balancing prices are primarily correlated with day-ahead prices and secondary with balancing volumes.","The latter correlation is found to be increasingly non-linear with time.","c)","The balancing volumes and the price difference between balancing price and day-ahead price are strongly correlated with the previous hour.","d)","The increasing share of wind power has not impacted the frequency of balancing, which has remained stable during the 8 years studied.","However, the volumes and share of balancing power compared to overall production have increased, suggesting that the hours which are inherently difficult to predict remain the same.","e) Market data alone cannot predict balancing volumes.","If attempting, the auto-correlation becomes the main source of information."],"url":"http://arxiv.org/abs/2402.09134v1","category":"eess.SY"}
{"created":"2024-02-14 12:15:33","title":"Joint Communication and Sensing for 6G -- A Cross-Layer Perspective","abstract":"As 6G emerges, cellular systems are envisioned to integrate sensing with communication capabilities, leading to multi-faceted communication and sensing (JCAS). This paper presents a comprehensive cross-layer overview of the Hexa-X-II project's endeavors in JCAS, aligning 6G use cases with service requirements and pinpointing distinct scenarios that bridge communication and sensing. This work relates to these scenarios through the lens of the cross-layer physical and networking domains, covering models, deployments, resource allocation, storage challenges, computational constraints, interfaces, and innovative functions.","sentences":["As 6G emerges, cellular systems are envisioned to integrate sensing with communication capabilities, leading to multi-faceted communication and sensing (JCAS).","This paper presents a comprehensive cross-layer overview of the Hexa-X-II project's endeavors in JCAS, aligning 6G use cases with service requirements and pinpointing distinct scenarios that bridge communication and sensing.","This work relates to these scenarios through the lens of the cross-layer physical and networking domains, covering models, deployments, resource allocation, storage challenges, computational constraints, interfaces, and innovative functions."],"url":"http://arxiv.org/abs/2402.09120v1","category":"eess.SP"}
{"created":"2024-02-14 11:49:30","title":"Theory of biexciton-polaritons in transition metal dichalcogenide monolayers","abstract":"We theoretically investigate a nonlinear optical response of a planar microcavity with an embedded transition metal dicalcogenide monolayer of a when an energy of a biexcitonic transition is brought in resonance with an energy of a cavity mode. We demonstrate that the emission spectrum of this system strongly depends on an external pump. For small and moderate pumps we reveal the presence of a doublet in the emission with the corresponding Rabi splitting scaling as a square root of the number of the excitations in the system. Further increase of the pump leads to the reshaping of the spectrum, which demonstrates the pattern typical for a Mollow triplet. An intermediate pumping regime shows a broad irregular spectrum reminiscent of a chaotic dynamics of the system.","sentences":["We theoretically investigate a nonlinear optical response of a planar microcavity with an embedded transition metal dicalcogenide monolayer of a when an energy of a biexcitonic transition is brought in resonance with an energy of a cavity mode.","We demonstrate that the emission spectrum of this system strongly depends on an external pump.","For small and moderate pumps we reveal the presence of a doublet in the emission with the corresponding Rabi splitting scaling as a square root of the number of the excitations in the system.","Further increase of the pump leads to the reshaping of the spectrum, which demonstrates the pattern typical for a Mollow triplet.","An intermediate pumping regime shows a broad irregular spectrum reminiscent of a chaotic dynamics of the system."],"url":"http://arxiv.org/abs/2402.09110v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 11:46:35","title":"Web 3.0 and Quantum Security: Long-Distance Free-Space QSDC for Global Web 3.0 Networks","abstract":"With the advent of Web 3.0, the swift advancement of technology confronts an imminent threat from quantum computing. Security protocols safeguarding the integrity of Web 2.0 and Web 3.0 are growing more susceptible to both quantum attacks and sophisticated classical threats. The article introduces long-distance free-space quantum secure direct communication (LF QSDC) as a method to safeguard against security breaches in both quantum and classical contexts. Differing from techniques like quantum key distribution (QKD), LF QSDC surpasses constraints by facilitating encrypted data transmission sans key exchanges, thus diminishing the inherent weaknesses of key-based systems. The distinctiveness of this attribute, coupled with its quantum mechanics base, protects against quantum computer assaults and advanced non-quantum dangers, harmonizing seamlessly with the untrustworthy tenets of the Web 3.0 age. The focus of our study is the incorporation of LF QSDC into network infrastructures, highlighting its efficacy for extended-range communication via memory DL04 protocol, quantum-aware low-density parity check (LDPC), and pointing, acquisition, and tracking (PAT) technologies. Utilizing this method not only bolsters the security of worldwide Web 3.0 networks but also guarantees their endurance in a time when quantum and sophisticated classical threats exist simultaneously. Consequently, LF QSDC stands out as a robust security solution, well-suited for Web 3.0 systems amidst the constantly evolving digital environment.","sentences":["With the advent of Web 3.0, the swift advancement of technology confronts an imminent threat from quantum computing.","Security protocols safeguarding the integrity of Web 2.0 and Web 3.0 are growing more susceptible to both quantum attacks and sophisticated classical threats.","The article introduces long-distance free-space quantum secure direct communication (LF QSDC) as a method to safeguard against security breaches in both quantum and classical contexts.","Differing from techniques like quantum key distribution (QKD), LF QSDC surpasses constraints by facilitating encrypted data transmission sans key exchanges, thus diminishing the inherent weaknesses of key-based systems.","The distinctiveness of this attribute, coupled with its quantum mechanics base, protects against quantum computer assaults and advanced non-quantum dangers, harmonizing seamlessly with the untrustworthy tenets of the Web 3.0 age.","The focus of our study is the incorporation of LF QSDC into network infrastructures, highlighting its efficacy for extended-range communication via memory DL04 protocol, quantum-aware low-density parity check (LDPC), and pointing, acquisition, and tracking (PAT) technologies.","Utilizing this method not only bolsters the security of worldwide Web 3.0 networks but also guarantees their endurance in a time when quantum and sophisticated classical threats exist simultaneously.","Consequently, LF QSDC stands out as a robust security solution, well-suited for Web 3.0 systems amidst the constantly evolving digital environment."],"url":"http://arxiv.org/abs/2402.09108v1","category":"quant-ph"}
{"created":"2024-02-14 11:42:15","title":"Headset: Human emotion awareness under partial occlusions multimodal dataset","abstract":"The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications. Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration. In this work, we present a new multimodal database to help advance the development of immersive technologies. Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs). The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously. Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction. The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video. HEADSET and its all associated raw data and license agreement will be publicly available for research purposes.","sentences":["The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications.","Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration.","In this work, we present a new multimodal database to help advance the development of immersive technologies.","Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs).","The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras.","In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously.","Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction.","The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video.","HEADSET and its all associated raw data and license agreement will be publicly available for research purposes."],"url":"http://arxiv.org/abs/2402.09107v1","category":"cs.CV"}
{"created":"2024-02-14 11:26:23","title":"Localization engineering by resonant driving in dissipative polariton arrays","abstract":"Arrays of microcavity polaritons are very versatile systems that allow for broad possibilities for the engineering of multi-orbital lattice geometries using different state preparation schemes. One of these schemes, spatially modulated resonant driving, can be used to selectively localize the polariton field on a particular region of the lattice. Both the frequency and the spatial amplitude distribution (module and phase) of the driven laser field are important and serve as a knob to control the extend of the spatial localization. Here, we analyse both the linear and nonlinear regimes using the lattice Green function formalism that is particularly suitable for the case of polariton arrays described in a tight-binding approximation. We identify the conditions for maximum localization on arbitrary lattice's geometries and discuss some experimentally relevant cases. We find that the polariton-polariton interaction leads to a frequency shift of the optimal localization condition that could be used to further control it.","sentences":["Arrays of microcavity polaritons are very versatile systems that allow for broad possibilities for the engineering of multi-orbital lattice geometries using different state preparation schemes.","One of these schemes, spatially modulated resonant driving, can be used to selectively localize the polariton field on a particular region of the lattice.","Both the frequency and the spatial amplitude distribution (module and phase) of the driven laser field are important and serve as a knob to control the extend of the spatial localization.","Here, we analyse both the linear and nonlinear regimes using the lattice Green function formalism that is particularly suitable for the case of polariton arrays described in a tight-binding approximation.","We identify the conditions for maximum localization on arbitrary lattice's geometries and discuss some experimentally relevant cases.","We find that the polariton-polariton interaction leads to a frequency shift of the optimal localization condition that could be used to further control it."],"url":"http://arxiv.org/abs/2402.09104v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 11:16:50","title":"FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning","abstract":"In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy. The non-independent and identically distributed (non-i.i.d.) nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL. Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation. Traditional FL approaches may suffer from these challenges. Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework. As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity. This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student models and addressing device constraints. FedSiKD outperforms state-of-the-art algorithms by achieving higher accuracy, exceeding by 25\\% and 18\\% for highly skewed data at $\\alpha = {0.1,0.5}$ on the HAR and MNIST datasets, respectively. Its faster convergence is illustrated by a 17\\% and 20\\% increase in accuracy within the first five rounds on the HAR and MNIST datasets, respectively, highlighting its early-stage learning proficiency. Code is publicly available and hosted on GitHub (https://github.com/SimuEnv/FedSiKD)","sentences":["In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy.","The non-independent and identically distributed (non-i.i.d.)","nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL.","Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation.","Traditional FL approaches may suffer from these challenges.","Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework.","As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity.","This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student models and addressing device constraints.","FedSiKD outperforms state-of-the-art algorithms by achieving higher accuracy, exceeding by 25\\% and 18\\% for highly skewed data at $\\alpha = {0.1,0.5}$ on the HAR and MNIST datasets, respectively.","Its faster convergence is illustrated by a 17\\% and 20\\% increase in accuracy within the first five rounds on the HAR and MNIST datasets, respectively, highlighting its early-stage learning proficiency.","Code is publicly available and hosted on GitHub (https://github.com/SimuEnv/FedSiKD)"],"url":"http://arxiv.org/abs/2402.09095v1","category":"cs.LG"}
{"created":"2024-02-14 11:13:33","title":"Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks","abstract":"Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life. Recently, their importance and practical usability have further been reinforced with the advent of deep learning. One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model. Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview. The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions. To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys. Our comprehensive compilation also references these surveys; however, its main goal is to provide the most comprehensive overview and systematization of previously published activation functions with links to their original sources. The secondary aim is to update the current understanding of this family of functions.","sentences":["Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life.","Recently, their importance and practical usability have further been reinforced with the advent of deep learning.","One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model.","Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview.","The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions.","To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys.","Our comprehensive compilation also references these surveys; however, its main goal is to provide the most comprehensive overview and systematization of previously published activation functions with links to their original sources.","The secondary aim is to update the current understanding of this family of functions."],"url":"http://arxiv.org/abs/2402.09092v1","category":"cs.LG"}
{"created":"2024-02-14 11:11:41","title":"Software in the natural world: A computational approach to emergence in complex multi-level systems","abstract":"Understanding the functional architecture of complex systems is crucial to illuminate their inner workings and enable effective methods for their prediction and control. Recent advances have introduced tools to characterise emergent macroscopic levels; however, while these approaches are successful in identifying when emergence takes place, they are limited in the extent they can determine how it does. Here we address this limitation by developing a computational approach to emergence, which characterises macroscopic processes in terms of their computational capabilities. Concretely, we articulate a view on emergence based on how software works, which is rooted on a mathematical formalism that articulates how macroscopic processes can express self-contained informational, interventional, and computational properties. This framework establishes a hierarchy of nested self-contained processes that determines what computations take place at what level, which in turn delineates the functional architecture of a complex system. This approach is illustrated on paradigmatic models from the statistical physics and computational neuroscience literature, which are shown to exhibit macroscopic processes that are akin to software in human-engineered systems. Overall, this framework enables a deeper understanding of the multi-level structure of complex systems, revealing specific ways in which they can be efficiently simulated, predicted, and controlled.","sentences":["Understanding the functional architecture of complex systems is crucial to illuminate their inner workings and enable effective methods for their prediction and control.","Recent advances have introduced tools to characterise emergent macroscopic levels; however, while these approaches are successful in identifying when emergence takes place, they are limited in the extent they can determine how it does.","Here we address this limitation by developing a computational approach to emergence, which characterises macroscopic processes in terms of their computational capabilities.","Concretely, we articulate a view on emergence based on how software works, which is rooted on a mathematical formalism that articulates how macroscopic processes can express self-contained informational, interventional, and computational properties.","This framework establishes a hierarchy of nested self-contained processes that determines what computations take place at what level, which in turn delineates the functional architecture of a complex system.","This approach is illustrated on paradigmatic models from the statistical physics and computational neuroscience literature, which are shown to exhibit macroscopic processes that are akin to software in human-engineered systems.","Overall, this framework enables a deeper understanding of the multi-level structure of complex systems, revealing specific ways in which they can be efficiently simulated, predicted, and controlled."],"url":"http://arxiv.org/abs/2402.09090v1","category":"nlin.AO"}
{"created":"2024-02-14 10:52:39","title":"Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?","abstract":"The ever-evolving landscape of attacks, coupled with the growing complexity of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error detectors (ED) a difficult task: they must accurately detect attacks, and they should promptly perform detections. Although improving and comparing the detection capability is the focus of most research works, the timeliness of the detection is less considered and often insufficiently evaluated or discussed. In this paper, we argue the relevance of measuring the temporal latency of attacks and errors, and we propose an evaluation approach for detectors to ensure a pragmatic trade-off between correct and in-time detection. Briefly, the approach relates the false positive rate with the temporal latency of attacks and errors, and this ultimately leads to guidelines for configuring a detector. We apply our approach by evaluating different ED and ID solutions in two industrial cases: i) an embedded railway on-board system that optimizes public mobility, and ii) an edge device for the Industrial Internet of Things. Our results show that considering latency in addition to traditional metrics like the false positive rate, precision, and coverage gives an additional fundamental perspective on the actual performance of the detector and should be considered when assessing and configuring anomaly detectors.","sentences":["The ever-evolving landscape of attacks, coupled with the growing complexity of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error detectors (ED) a difficult task: they must accurately detect attacks, and they should promptly perform detections.","Although improving and comparing the detection capability is the focus of most research works, the timeliness of the detection is less considered and often insufficiently evaluated or discussed.","In this paper, we argue the relevance of measuring the temporal latency of attacks and errors, and we propose an evaluation approach for detectors to ensure a pragmatic trade-off between correct and in-time detection.","Briefly, the approach relates the false positive rate with the temporal latency of attacks and errors, and this ultimately leads to guidelines for configuring a detector.","We apply our approach by evaluating different ED and ID solutions in two industrial cases: i) an embedded railway on-board system that optimizes public mobility, and ii) an edge device for the Industrial Internet of Things.","Our results show that considering latency in addition to traditional metrics like the false positive rate, precision, and coverage gives an additional fundamental perspective on the actual performance of the detector and should be considered when assessing and configuring anomaly detectors."],"url":"http://arxiv.org/abs/2402.09082v1","category":"cs.CR"}
{"created":"2024-02-14 10:35:26","title":"Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards","abstract":"The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes steady-state errors but also results in smoother variations in system states.","sentences":["The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance.","Issues of steady-state error often manifest when quadratic reward functions are employed.","Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes.","In response to this challenge, this study proposes an approach that introduces an integral term.","By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors.","Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes steady-state errors but also results in smoother variations in system states."],"url":"http://arxiv.org/abs/2402.09075v1","category":"eess.SY"}
{"created":"2024-02-14 10:34:06","title":"Stable-to-unstable transition in quantum friction","abstract":"We investigate the frictional force arising from quantum fluctuations when two dissipative metallic plates are set in a shear motion. While early studies showed that the electromagnetic fields in the quantum friction setup reach nonequilibrium steady states, yielding a time-independent force, other works have demonstrated the failure to attain steady states, leading to instability and time-varying friction under sufficiently low-loss conditions. Here, we develop a fully quantum-mechanical theory without perturbative approximations and unveil the transition from stable to unstable regimes of the quantum friction setup. Due to the relative motion of the plates, their electromagnetic response may be active in some conditions, resulting in optical gain. We prove that the standard fluctuation-dissipation leads to inconsistent results when applied to our system, and, in particular, it predicts a vanishing frictional force. Using a modified fluctuation-dissipation relation tailored for gain media, we calculate the frictional force in terms of the system Green's function, thereby recovering early works on quantum friction. Remarkably, we also find that the frictional force diverges to infinity as the relative velocity of the plates approaches a threshold. This threshold is determined by the damping strength and the distance between the metal surfaces. Beyond this critical velocity, the system exhibits instability, akin to the behaviour of a laser cavity, where no steady state exists. In such a scenario, the frictional force escalates exponentially. Our findings pave the way for experimental exploration of the frictional force in proximity to this critical regime.","sentences":["We investigate the frictional force arising from quantum fluctuations when two dissipative metallic plates are set in a shear motion.","While early studies showed that the electromagnetic fields in the quantum friction setup reach nonequilibrium steady states, yielding a time-independent force, other works have demonstrated the failure to attain steady states, leading to instability and time-varying friction under sufficiently low-loss conditions.","Here, we develop a fully quantum-mechanical theory without perturbative approximations and unveil the transition from stable to unstable regimes of the quantum friction setup.","Due to the relative motion of the plates, their electromagnetic response may be active in some conditions, resulting in optical gain.","We prove that the standard fluctuation-dissipation leads to inconsistent results when applied to our system, and, in particular, it predicts a vanishing frictional force.","Using a modified fluctuation-dissipation relation tailored for gain media, we calculate the frictional force in terms of the system Green's function, thereby recovering early works on quantum friction.","Remarkably, we also find that the frictional force diverges to infinity as the relative velocity of the plates approaches a threshold.","This threshold is determined by the damping strength and the distance between the metal surfaces.","Beyond this critical velocity, the system exhibits instability, akin to the behaviour of a laser cavity, where no steady state exists.","In such a scenario, the frictional force escalates exponentially.","Our findings pave the way for experimental exploration of the frictional force in proximity to this critical regime."],"url":"http://arxiv.org/abs/2402.09074v1","category":"quant-ph"}
{"created":"2024-02-14 10:30:52","title":"High-precision and low-noise dielectric tensor tomography using a micro-electromechanical system mirror","abstract":"Dielectric tensor tomography is an imaging technique for mapping three-dimensional distributions of dielectric properties in transparent materials. This work introduces an enhanced illumination strategy employing a micro-electromechanical system mirror to achieve high precision and reduced noise in imaging. This illumination approach allows for precise manipulation of light, significantly improving the accuracy of angle control and minimizing diffraction noise compared to traditional beam steering approaches. Our experiments have successfully reconstructed the dielectric properties of liquid crystal droplets, which are known for their anisotropic structures, while demonstrating a notable reduction in background noise of the imag-es. Additionally, the technique has been applied to more complex samples, revealing its capability to achieve a high signal-to-noise ratio. This development represents a significant step forward in the field of birefringence imaging, offering a powerful tool for detailed study of materials with anisotropic properties.","sentences":["Dielectric tensor tomography is an imaging technique for mapping three-dimensional distributions of dielectric properties in transparent materials.","This work introduces an enhanced illumination strategy employing a micro-electromechanical system mirror to achieve high precision and reduced noise in imaging.","This illumination approach allows for precise manipulation of light, significantly improving the accuracy of angle control and minimizing diffraction noise compared to traditional beam steering approaches.","Our experiments have successfully reconstructed the dielectric properties of liquid crystal droplets, which are known for their anisotropic structures, while demonstrating a notable reduction in background noise of the imag-es.","Additionally, the technique has been applied to more complex samples, revealing its capability to achieve a high signal-to-noise ratio.","This development represents a significant step forward in the field of birefringence imaging, offering a powerful tool for detailed study of materials with anisotropic properties."],"url":"http://arxiv.org/abs/2402.09070v1","category":"physics.optics"}
{"created":"2024-02-14 10:28:41","title":"Control of multi-modal scattering in a microwave frequency comb","abstract":"Control over the coupling between multiple modes of a frequency comb is an important step toward measurement-based quantum computation with a continuous-variable system. We demonstrate the creation of square-ladder correlation graphs in a microwave comb with 95 modes. The graphs are engineered through precise control of the relative phase of three pumps applied to a Josephson parametric oscillator. Experimental measurement of the mode scattering matrix is in good agreement with theoretical predictions based on a linearized equation of motion of the parametric oscillator. The digital methods used to create and measure the correlations are easily scaled to more modes and more pumps, with the potential to tailor a specific correlation graph topology.","sentences":["Control over the coupling between multiple modes of a frequency comb is an important step toward measurement-based quantum computation with a continuous-variable system.","We demonstrate the creation of square-ladder correlation graphs in a microwave comb with 95 modes.","The graphs are engineered through precise control of the relative phase of three pumps applied to a Josephson parametric oscillator.","Experimental measurement of the mode scattering matrix is in good agreement with theoretical predictions based on a linearized equation of motion of the parametric oscillator.","The digital methods used to create and measure the correlations are easily scaled to more modes and more pumps, with the potential to tailor a specific correlation graph topology."],"url":"http://arxiv.org/abs/2402.09068v1","category":"quant-ph"}
{"created":"2024-02-14 10:16:56","title":"High resolution optical spectra of the dormant LBV star P Cyg","abstract":"High resolution optical spectra (R = 60 000) of the LBV star P Cyg beyond outburst were obtained on the 6-meter BTA telescope in the wavelength range 477-780 nm. We perform a detailed identification of different types lines (photospheric absorptions, permitted and forbidden emissions, components of lines with P Cyg type profiles), and studied the variability of their profiles and radial velocities. The average radial velocity from positions of forbidden emissions ([NII] 5754.64, [FeII] 5261.62, [FeII] 7155.14 and [NiII] 7377.83 \\r{A}) is accepted as the system velocity Vsys=$-34\\pm1.4$ km/s. About a dozen photospheric absorptions of CNO triad ions and SiIII are found, their stable position, Vr(abs)=$-73.8$ km/s, shifted relative to at $-40$ km/s, indicates that these absorbtions are formed in the pseudophotosphere region. The high-excitation emissions ([OI] 5577, 6300, 6363 \\r{A}, [OIII] 4959 and 5007 \\r{A}, as well as HeII 4686 \\r{A}) are absent in the spectra. The radial velocity Vr(DIBs)=$-11.8$ km/s according to the position of numerous DIBs is consistent with the position of the interstellar components of the D-lines NaI and KI forming in the galactic Perseus arm. A color excess E(B-V)=0.34+/-0.03 mag and interstellar absorption Av=1.09 mag were determined by measurements of equivalent widths of nine DIBs.","sentences":["High resolution optical spectra (R = 60 000) of the LBV star P Cyg beyond outburst were obtained on the 6-meter BTA telescope in the wavelength range 477-780 nm.","We perform a detailed identification of different types lines (photospheric absorptions, permitted and forbidden emissions, components of lines with P Cyg type profiles), and studied the variability of their profiles and radial velocities.","The average radial velocity from positions of forbidden emissions ([NII] 5754.64, [FeII] 5261.62, [FeII] 7155.14 and [NiII] 7377.83 \\r{A}) is accepted as the system velocity Vsys=$-34\\pm1.4$ km/s. About a dozen photospheric absorptions of CNO triad ions and SiIII are found, their stable position, Vr(abs)=$-73.8$ km/s, shifted relative to at $-40$ km/s, indicates that these absorbtions are formed in the pseudophotosphere region.","The high-excitation emissions ([OI] 5577, 6300, 6363 \\r{A}, [OIII] 4959 and 5007 \\r{A}, as well as HeII 4686 \\r{A}) are absent in the spectra.","The radial velocity Vr(DIBs)=$-11.8$ km/s according to the position of numerous DIBs is consistent with the position of the interstellar components of the D-lines NaI and KI forming in the galactic Perseus arm.","A color excess E(B-V)=0.34+/-0.03 mag and interstellar absorption Av=1.09 mag were determined by measurements of equivalent widths of nine DIBs."],"url":"http://arxiv.org/abs/2402.09061v1","category":"astro-ph.SR"}
{"created":"2024-02-14 09:59:00","title":"The weak (1,1) boundedness of Fourier integral operators with complex phases","abstract":"Let $T$ be a Fourier integral operator of order $-(n-1)/2$ associated with a canonical relation locally parametrised by a real-phase function. A fundamental result due to Seeger, Sogge, and Stein proved in the 90's, gives the boundedness of $T$ from the Hardy space $H^1$ into $L^1.$ Additionally, it was shown by T. Tao the weak (1,1) type of $T$. In this work, we establish the weak (1,1) boundedness of a Fourier integral operator $T$ of order $-(n-1)/2$ when it has associated a canonical relation parametrised by a complex phase function.","sentences":["Let $T$ be a Fourier integral operator of order $-(n-1)/2$ associated with a canonical relation locally parametrised by a real-phase function.","A fundamental result due to Seeger, Sogge, and Stein proved in the 90's, gives the boundedness of $T$ from the Hardy space $H^1$ into $L^1.$ Additionally, it was shown by T. Tao the weak (1,1) type of $T$. In this work, we establish the weak (1,1) boundedness of a Fourier integral operator $T$ of order $-(n-1)/2$ when it has associated a canonical relation parametrised by a complex phase function."],"url":"http://arxiv.org/abs/2402.09054v1","category":"math.AP"}
{"created":"2024-02-14 09:46:48","title":"Quantum Theory of Phonon Induced Anomalous Hall Effect in 2D Massive Dirac metals","abstract":"The phonon induced anomalous Hall or thermal Hall effects have been observed in various systems in recent experiments. However, the theoretical studies on this subject are very scarce and incomplete. In this work, we present a systematic quantum field theory study on the phonon induced anomalous Hall effect, including both the side jump and skew scattering contributions, in a 2D massive Dirac metal, which is considered as the minimum anomalous Hall system. We reveal significant difference from the anomalous Hall effect induced by the widely studied Gaussian disorder which is known to be insensitive to temperature. While the anomalous Hall effect induced by phonon approaches that by Gaussian disorder at high temperature, it behaves very differently at low temperature. Our work provides a microscopic and quantitative description of the crossover from the low to high temperature regime of the phonon induced anomalous Hall conductivity, which may be observed in 2D Dirac metals with breaking time reversal symmetry.","sentences":["The phonon induced anomalous Hall or thermal Hall effects have been observed in various systems in recent experiments.","However, the theoretical studies on this subject are very scarce and incomplete.","In this work, we present a systematic quantum field theory study on the phonon induced anomalous Hall effect, including both the side jump and skew scattering contributions, in a 2D massive Dirac metal, which is considered as the minimum anomalous Hall system.","We reveal significant difference from the anomalous Hall effect induced by the widely studied Gaussian disorder which is known to be insensitive to temperature.","While the anomalous Hall effect induced by phonon approaches that by Gaussian disorder at high temperature, it behaves very differently at low temperature.","Our work provides a microscopic and quantitative description of the crossover from the low to high temperature regime of the phonon induced anomalous Hall conductivity, which may be observed in 2D Dirac metals with breaking time reversal symmetry."],"url":"http://arxiv.org/abs/2402.09049v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 09:44:34","title":"Sensing in Bi-Static ISAC Systems with Clock Asynchronism: A Signal Processing Perspective","abstract":"Integrated Sensing and Communications (ISAC) has been identified as a pillar usage scenario for the impending 6G era. Bi-static sensing, a major type of sensing in \\ac{isac}, is promising to expedite ISAC in the near future, as it requires minimal changes to the existing network infrastructure. However, a critical challenge for bi-static sensing is clock asynchronism due to the use of different clocks at far separated transmitter and receiver. This causes the received signal to be affected by time-varying random phase offsets, severely degrading, or even failing, direct sensing. Considerable research attention has been directed toward addressing the clock asynchronism issue in bi-static sensing. In this white paper, we endeavor to fill the gap by providing an overview of the issue and existing techniques developed in an ISAC background. Based on the review and comparison, we also draw insights into the future research directions and open problems, aiming to nurture the maturation of bi-static sensing in ISAC.","sentences":["Integrated Sensing and Communications (ISAC) has been identified as a pillar usage scenario for the impending 6G era.","Bi-static sensing, a major type of sensing in \\ac{isac}, is promising to expedite ISAC in the near future, as it requires minimal changes to the existing network infrastructure.","However, a critical challenge for bi-static sensing is clock asynchronism due to the use of different clocks at far separated transmitter and receiver.","This causes the received signal to be affected by time-varying random phase offsets, severely degrading, or even failing, direct sensing.","Considerable research attention has been directed toward addressing the clock asynchronism issue in bi-static sensing.","In this white paper, we endeavor to fill the gap by providing an overview of the issue and existing techniques developed in an ISAC background.","Based on the review and comparison, we also draw insights into the future research directions and open problems, aiming to nurture the maturation of bi-static sensing in ISAC."],"url":"http://arxiv.org/abs/2402.09048v1","category":"eess.SP"}
{"created":"2024-02-14 09:38:09","title":"Under manipulations, are some AI models harder to audit?","abstract":"Auditors need robust methods to assess the compliance of web platforms with the law. However, since they hardly ever have access to the algorithm, implementation, or training data used by a platform, the problem is harder than a simple metric estimation. Within the recent framework of manipulation-proof auditing, we study in this paper the feasibility of robust audits in realistic settings, in which models exhibit large capacities. We first prove a constraining result: if a web platform uses models that may fit any data, no audit strategy -- whether active or not -- can outperform random sampling when estimating properties such as demographic parity. To better understand the conditions under which state-of-the-art auditing techniques may remain competitive, we then relate the manipulability of audits to the capacity of the targeted models, using the Rademacher complexity. We empirically validate these results on popular models of increasing capacities, thus confirming experimentally that large-capacity models, which are commonly used in practice, are particularly hard to audit robustly. These results refine the limits of the auditing problem, and open up enticing questions on the connection between model capacity and the ability of platforms to manipulate audit attempts.","sentences":["Auditors need robust methods to assess the compliance of web platforms with the law.","However, since they hardly ever have access to the algorithm, implementation, or training data used by a platform, the problem is harder than a simple metric estimation.","Within the recent framework of manipulation-proof auditing, we study in this paper the feasibility of robust audits in realistic settings, in which models exhibit large capacities.","We first prove a constraining result: if a web platform uses models that may fit any data, no audit strategy -- whether active or not -- can outperform random sampling when estimating properties such as demographic parity.","To better understand the conditions under which state-of-the-art auditing techniques may remain competitive, we then relate the manipulability of audits to the capacity of the targeted models, using the Rademacher complexity.","We empirically validate these results on popular models of increasing capacities, thus confirming experimentally that large-capacity models, which are commonly used in practice, are particularly hard to audit robustly.","These results refine the limits of the auditing problem, and open up enticing questions on the connection between model capacity and the ability of platforms to manipulate audit attempts."],"url":"http://arxiv.org/abs/2402.09043v1","category":"cs.LG"}
{"created":"2024-02-14 09:16:46","title":"Cross-Temporal Forecast Reconciliation at Digital Platforms with Machine Learning","abstract":"Platform businesses operate on a digital core and their decision making requires high-dimensional accurate forecast streams at different levels of cross-sectional (e.g., geographical regions) and temporal aggregation (e.g., minutes to days). It also necessitates coherent forecasts across all levels of the hierarchy to ensure aligned decision making across different planning units such as pricing, product, controlling and strategy. Given that platform data streams feature complex characteristics and interdependencies, we introduce a non-linear hierarchical forecast reconciliation method that produces cross-temporal reconciled forecasts in a direct and automated way through the use of popular machine learning methods. The method is sufficiently fast to allow forecast-based high-frequency decision making that platforms require. We empirically test our framework on a unique, large-scale streaming dataset from a leading on-demand delivery platform in Europe.","sentences":["Platform businesses operate on a digital core and their decision making requires high-dimensional accurate forecast streams at different levels of cross-sectional (e.g., geographical regions) and temporal aggregation (e.g., minutes to days).","It also necessitates coherent forecasts across all levels of the hierarchy to ensure aligned decision making across different planning units such as pricing, product, controlling and strategy.","Given that platform data streams feature complex characteristics and interdependencies, we introduce a non-linear hierarchical forecast reconciliation method that produces cross-temporal reconciled forecasts in a direct and automated way through the use of popular machine learning methods.","The method is sufficiently fast to allow forecast-based high-frequency decision making that platforms require.","We empirically test our framework on a unique, large-scale streaming dataset from a leading on-demand delivery platform in Europe."],"url":"http://arxiv.org/abs/2402.09033v1","category":"econ.EM"}
{"created":"2024-02-14 09:07:25","title":"Random Matrix Theory Approach to Quantum Fisher Information in Quantum Many-Body Systems","abstract":"We theoretically investigate parameter quantum estimation in quantum chaotic systems. Our analysis is based on an effective description of non-integrable quantum systems in terms of a random matrix Hamiltonian. Based on this approach we derive an analytical expression for the time evolution of the quantum Fisher information. We test our random matrix theory prediction with the exact diagonalization of a non-integrable spin system, focusing on the estimation of a local magnetic field by measurements of the many-body state. Our numerical calculations agree with the effective random matrix theory approach and show that the information on the local Hamiltonian parameter is distributed throughout the quantum system during the quantum thermalization process. Our analysis shows a first stage in which the initial information spread is quadratic in time which quickly passes into linear increase with slope determine by the decay rate of the measured spin observable. When the information is fully spread among all degrees of freedom a second quadratic time scale determines the long time behaviour of the quantum Fisher information.","sentences":["We theoretically investigate parameter quantum estimation in quantum chaotic systems.","Our analysis is based on an effective description of non-integrable quantum systems in terms of a random matrix Hamiltonian.","Based on this approach we derive an analytical expression for the time evolution of the quantum Fisher information.","We test our random matrix theory prediction with the exact diagonalization of a non-integrable spin system, focusing on the estimation of a local magnetic field by measurements of the many-body state.","Our numerical calculations agree with the effective random matrix theory approach and show that the information on the local Hamiltonian parameter is distributed throughout the quantum system during the quantum thermalization process.","Our analysis shows a first stage in which the initial information spread is quadratic in time which quickly passes into linear increase with slope determine by the decay rate of the measured spin observable.","When the information is fully spread among all degrees of freedom a second quadratic time scale determines the long time behaviour of the quantum Fisher information."],"url":"http://arxiv.org/abs/2402.09029v1","category":"quant-ph"}
{"created":"2024-02-14 09:01:54","title":"Doubly heavy dibaryons as seen by string theory","abstract":"We propose the stringy description of the system consisting of two heavy and four light quarks in the case of two light flavors of equal mass. As an application, we consider the three low-lying Born-Oppenheimer potentials as a function of the heavy quark separation. Our analysis shows that the ground state potential is described in terms of both hadro-quarkonia and hadronic molecules. A connected string configuration makes the dominant contribution to the potential of an excited state at small separations, and for separations larger than $0.1\\,\\text{fm}$, it exhibits the diquark-diquark-diquark structure $[Qq][Qq][qq]$. For better understanding the quark organization inside the system, we introduce several critical separations related to the processes of string reconnection, breaking and junction annihilation. We also discuss the simplest string configurations including the five-string junctions and their implications for the system, in particular the emergence of composite quark objects different from diquarks and the process of junction fusion.","sentences":["We propose the stringy description of the system consisting of two heavy and four light quarks in the case of two light flavors of equal mass.","As an application, we consider the three low-lying Born-Oppenheimer potentials as a function of the heavy quark separation.","Our analysis shows that the ground state potential is described in terms of both hadro-quarkonia and hadronic molecules.","A connected string configuration makes the dominant contribution to the potential of an excited state at small separations, and for separations larger than $0.1\\,\\text{fm}$, it exhibits the diquark-diquark-diquark structure $","[Qq][Qq][qq]$. For better understanding the quark organization inside the system, we introduce several critical separations related to the processes of string reconnection, breaking and junction annihilation.","We also discuss the simplest string configurations including the five-string junctions and their implications for the system, in particular the emergence of composite quark objects different from diquarks and the process of junction fusion."],"url":"http://arxiv.org/abs/2402.09026v1","category":"hep-ph"}
{"created":"2024-02-14 09:01:13","title":"SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks","abstract":"Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without compromising the linguistic capabilities of these models, making it a promising technique for optimizing the efficiency of LLMs. The code is available at: https://github.com/leapingjagg-dev/SLEB","sentences":["Large language models (LLMs) have proven to be highly effective across various natural language processing tasks.","However, their large number of parameters poses significant challenges for practical deployment.","Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network.","Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup.","In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks.","We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks.","This choice allows us to effectively enhance the processing speed of LLMs.","Our experimental results demonstrate that SLEB successfully accelerates LLM inference without compromising the linguistic capabilities of these models, making it a promising technique for optimizing the efficiency of LLMs.","The code is available at: https://github.com/leapingjagg-dev/SLEB"],"url":"http://arxiv.org/abs/2402.09025v1","category":"cs.CL"}
{"created":"2024-02-14 08:52:06","title":"Unified Opinion Dynamic Modeling as Concurrent Set Relations in Rewriting Logic","abstract":"Social media platforms have played a key role in weaponizing the polarization of social, political, and democratic processes. This is, mainly, because they are a medium for opinion formation. Opinion dynamic models are a tool for understanding the role of specific social factors on the acceptance/rejection of opinions because they can be used to analyze certain assumptions on human behaviors. This work presents a framework that uses concurrent set relations as the formal basis to specify, simulate, and analyze social interaction systems with dynamic opinion models. Standard models for social learning are obtained as particular instances of the proposed framework. It has been implemented in the Maude system as a fully executable rewrite theory that can be used to better understand how opinions of a system of agents can be shaped. This paper also reports an initial exploration in Maude on the use of reachability analysis, probabilistic simulation, and statistical model checking of important properties related to opinion dynamic models.","sentences":["Social media platforms have played a key role in weaponizing the polarization of social, political, and democratic processes.","This is, mainly, because they are a medium for opinion formation.","Opinion dynamic models are a tool for understanding the role of specific social factors on the acceptance/rejection of opinions because they can be used to analyze certain assumptions on human behaviors.","This work presents a framework that uses concurrent set relations as the formal basis to specify, simulate, and analyze social interaction systems with dynamic opinion models.","Standard models for social learning are obtained as particular instances of the proposed framework.","It has been implemented in the Maude system as a fully executable rewrite theory that can be used to better understand how opinions of a system of agents can be shaped.","This paper also reports an initial exploration in Maude on the use of reachability analysis, probabilistic simulation, and statistical model checking of important properties related to opinion dynamic models."],"url":"http://arxiv.org/abs/2402.09021v1","category":"cs.LO"}
{"created":"2024-02-14 08:43:30","title":"Asgard/NOTT: L-band nulling interferometry at the VLTI. II. Warm optical design and injection system","abstract":"Asgard/NOTT (previously Hi-5) is a European Research Council (ERC)-funded project hosted at KU Leuven and a new visitor instrument for the Very Large Telescope Interferometer (VLTI). Its primary goal is to image the snow line region around young stars using nulling interferometry in the L-band (3.5 to 4.0)$\\mu$m, where the contrast between exoplanets and their host stars is advantageous. The breakthrough is the use of a photonic beam combiner, which only recently allowed the required theoretical raw contrast of $10^{-3}$ in this spectral range. Nulling interferometry observations of exoplanets also require a high degree of balancing between the four pupils of the VLTI in terms of intensity, phase, and polarization. The injection into the beam combiner and the requirements of nulling interferometry are driving the design of the warm optics and the injection system. The optical design up to the beam combiner is presented. It offers a technical solution to efficiently couple the light from the VLTI into the beam combiner. During the coupling, the objective is to limit throughput losses to 5% of the best expected efficiency for the injection. To achieve this, a list of different loss sources is considered with their respective impact on the injection efficiency. Solutions are also proposed to meet the requirements on beam balancing for intensity, phase, and polarization. The different properties of the design are listed, including the optics used, their alignment and tolerances, and their impact on the instrumental performances in terms of throughput and null depth. The performance evaluation gives an expected throughput loss of less than <6.4% of the best efficiency for the injection and a null depth of $\\sim2.10^{-3}$, mainly from optical path delay errors outside the scope of this work.","sentences":["Asgard/NOTT (previously Hi-5) is a European Research Council (ERC)-funded project hosted at KU Leuven and a new visitor instrument for the Very Large Telescope Interferometer (VLTI).","Its primary goal is to image the snow line region around young stars using nulling interferometry in the L-band (3.5 to 4.0)$\\mu$m, where the contrast between exoplanets and their host stars is advantageous.","The breakthrough is the use of a photonic beam combiner, which only recently allowed the required theoretical raw contrast of $10^{-3}$ in this spectral range.","Nulling interferometry observations of exoplanets also require a high degree of balancing between the four pupils of the VLTI in terms of intensity, phase, and polarization.","The injection into the beam combiner and the requirements of nulling interferometry are driving the design of the warm optics and the injection system.","The optical design up to the beam combiner is presented.","It offers a technical solution to efficiently couple the light from the VLTI into the beam combiner.","During the coupling, the objective is to limit throughput losses to 5% of the best expected efficiency for the injection.","To achieve this, a list of different loss sources is considered with their respective impact on the injection efficiency.","Solutions are also proposed to meet the requirements on beam balancing for intensity, phase, and polarization.","The different properties of the design are listed, including the optics used, their alignment and tolerances, and their impact on the instrumental performances in terms of throughput and null depth.","The performance evaluation gives an expected throughput loss of less than <6.4% of the best efficiency for the injection and a null depth of $\\sim2.10^{-3}$, mainly from optical path delay errors outside the scope of this work."],"url":"http://arxiv.org/abs/2402.09013v1","category":"astro-ph.IM"}
{"created":"2024-02-14 08:22:58","title":"Multi-Query Focused Disaster Summarization via Instruction-Based Prompting","abstract":"Automatic summarization of mass-emergency events plays a critical role in disaster management. The second edition of CrisisFACTS aims to advance disaster summarization based on multi-stream fact-finding with a focus on web sources such as Twitter, Reddit, Facebook, and Webnews. Here, participants are asked to develop systems that can extract key facts from several disaster-related events, which ultimately serve as a summary. This paper describes our method to tackle this challenging task. We follow previous work and propose to use a combination of retrieval, reranking, and an embarrassingly simple instruction-following summarization. The two-stage retrieval pipeline relies on BM25 and MonoT5, while the summarizer module is based on the open-source Large Language Model (LLM) LLaMA-13b. For summarization, we explore a Question Answering (QA)-motivated prompting approach and find the evidence useful for extracting query-relevant facts. The automatic metrics and human evaluation show strong results but also highlight the gap between open-source and proprietary systems.","sentences":["Automatic summarization of mass-emergency events plays a critical role in disaster management.","The second edition of CrisisFACTS aims to advance disaster summarization based on multi-stream fact-finding with a focus on web sources such as Twitter, Reddit, Facebook, and Webnews.","Here, participants are asked to develop systems that can extract key facts from several disaster-related events, which ultimately serve as a summary.","This paper describes our method to tackle this challenging task.","We follow previous work and propose to use a combination of retrieval, reranking, and an embarrassingly simple instruction-following summarization.","The two-stage retrieval pipeline relies on BM25 and MonoT5, while the summarizer module is based on the open-source Large Language Model (LLM) LLaMA-13b.","For summarization, we explore a Question Answering (QA)-motivated prompting approach and find the evidence useful for extracting query-relevant facts.","The automatic metrics and human evaluation show strong results but also highlight the gap between open-source and proprietary systems."],"url":"http://arxiv.org/abs/2402.09008v1","category":"cs.CL"}
{"created":"2024-02-14 08:01:42","title":"Chiral Interaction Induced Near-Perfect Photon Blockade","abstract":"Based on the scattering matrix method, we theoretically demonstrate that the chiral interaction can induce the almost perfect photon blockade (PB) in the waveguide-cavity quantum electrodynamics (QED) system. The mechanism relies on the multi-photon-paths interference within the waveguide, which is clearly shown by the analytical parameter regime for $g^{(2)}(0)\\approx0$. When $N$ cavities are introduced into the system, there are $N$ optimal parameter points accordingly for the almost perfect PB, and the required chirality decreases exponentially with increasing $N$. Under the conditions of resonant driving and specific chirality, the output light only relies on the parity of $N$ ($N\\ge2$), where the coherent state and single-photon state correspond to the case of system including the odd and even number of cavities, respectively. Our work offers an alternative route for achieving almost perfect PB effects by employing the chirality of system, with potential application in the on-chip single-photon source with integrability.","sentences":["Based on the scattering matrix method, we theoretically demonstrate that the chiral interaction can induce the almost perfect photon blockade (PB) in the waveguide-cavity quantum electrodynamics (QED) system.","The mechanism relies on the multi-photon-paths interference within the waveguide, which is clearly shown by the analytical parameter regime for $g^{(2)}(0)\\approx0$. When $N$ cavities are introduced into the system, there are $N$ optimal parameter points accordingly for the almost perfect PB, and the required chirality decreases exponentially with increasing $N$. Under the conditions of resonant driving and specific chirality, the output light only relies on the parity of $N$ ($N\\ge2$), where the coherent state and single-photon state correspond to the case of system including the odd and even number of cavities, respectively.","Our work offers an alternative route for achieving almost perfect PB effects by employing the chirality of system, with potential application in the on-chip single-photon source with integrability."],"url":"http://arxiv.org/abs/2402.09000v1","category":"quant-ph"}
{"created":"2024-02-14 07:34:22","title":"Variance Reduction and Low Sample Complexity in Stochastic Optimization via Proximal Point Method","abstract":"This paper proposes a stochastic proximal point method to solve a stochastic convex composite optimization problem. High probability results in stochastic optimization typically hinge on restrictive assumptions on the stochastic gradient noise, for example, sub-Gaussian distributions. Assuming only weak conditions such as bounded variance of the stochastic gradient, this paper establishes a low sample complexity to obtain a high probability guarantee on the convergence of the proposed method. Additionally, a notable aspect of this work is the development of a subroutine to solve the proximal subproblem, which also serves as a novel technique for variance reduction.","sentences":["This paper proposes a stochastic proximal point method to solve a stochastic convex composite optimization problem.","High probability results in stochastic optimization typically hinge on restrictive assumptions on the stochastic gradient noise, for example, sub-Gaussian distributions.","Assuming only weak conditions such as bounded variance of the stochastic gradient, this paper establishes a low sample complexity to obtain a high probability guarantee on the convergence of the proposed method.","Additionally, a notable aspect of this work is the development of a subroutine to solve the proximal subproblem, which also serves as a novel technique for variance reduction."],"url":"http://arxiv.org/abs/2402.08992v1","category":"math.OC"}
{"created":"2024-02-14 06:57:21","title":"Detecting Adversarial Spectrum Attacks via Distance to Decision Boundary Statistics","abstract":"Machine learning has been adopted for efficient cooperative spectrum sensing. However, it incurs an additional security risk due to attacks leveraging adversarial machine learning to create malicious spectrum sensing values to deceive the fusion center, called adversarial spectrum attacks. In this paper, we propose an efficient framework for detecting adversarial spectrum attacks. Our design leverages the concept of the distance to the decision boundary (DDB) observed at the fusion center and compares the training and testing DDB distributions to identify adversarial spectrum attacks. We create a computationally efficient way to compute the DDB for machine learning based spectrum sensing systems. Experimental results based on realistic spectrum data show that our method, under typical settings, achieves a high detection rate of up to 99\\% and maintains a low false alarm rate of less than 1\\%. In addition, our method to compute the DDB based on spectrum data achieves 54\\%--64\\% improvements in computational efficiency over existing distance calculation methods. The proposed DDB-based detection framework offers a practical and efficient solution for identifying malicious sensing values created by adversarial spectrum attacks.","sentences":["Machine learning has been adopted for efficient cooperative spectrum sensing.","However, it incurs an additional security risk due to attacks leveraging adversarial machine learning to create malicious spectrum sensing values to deceive the fusion center, called adversarial spectrum attacks.","In this paper, we propose an efficient framework for detecting adversarial spectrum attacks.","Our design leverages the concept of the distance to the decision boundary (DDB) observed at the fusion center and compares the training and testing DDB distributions to identify adversarial spectrum attacks.","We create a computationally efficient way to compute the DDB for machine learning based spectrum sensing systems.","Experimental results based on realistic spectrum data show that our method, under typical settings, achieves a high detection rate of up to 99\\% and maintains a low false alarm rate of less than 1\\%.","In addition, our method to compute the DDB based on spectrum data achieves 54\\%--64\\% improvements in computational efficiency over existing distance calculation methods.","The proposed DDB-based detection framework offers a practical and efficient solution for identifying malicious sensing values created by adversarial spectrum attacks."],"url":"http://arxiv.org/abs/2402.08986v1","category":"cs.CR"}
{"created":"2024-02-14 06:55:50","title":"Quantum Algorithm Exploration using Application-Oriented Performance Benchmarks","abstract":"The QED-C suite of Application-Oriented Benchmarks provides the ability to gauge performance characteristics of quantum computers as applied to real-world applications. Its benchmark programs sweep over a range of problem sizes and inputs, capturing key performance metrics related to the quality of results, total time of execution, and quantum gate resources consumed. In this manuscript, we investigate challenges in broadening the relevance of this benchmarking methodology to applications of greater complexity. First, we introduce a method for improving landscape coverage by varying algorithm parameters systematically, exemplifying this functionality in a new scalable HHL linear equation solver benchmark. Second, we add a VQE implementation of a Hydrogen Lattice simulation to the QED-C suite, and introduce a methodology for analyzing the result quality and run-time cost trade-off. We observe a decrease in accuracy with increased number of qubits, but only a mild increase in the execution time. Third, unique characteristics of a supervised machine-learning classification application are explored as a benchmark to gauge the extensibility of the framework to new classes of application. Applying this to a binary classification problem revealed the increase in training time required for larger anzatz circuits, and the significant classical overhead. Fourth, we add methods to include optimization and error mitigation in the benchmarking workflow which allows us to: identify a favourable trade off between approximate gate synthesis and gate noise; observe the benefits of measurement error mitigation and a form of deterministic error mitigation algorithm; and to contrast the improvement with the resulting time overhead. Looking ahead, we discuss how the benchmark framework can be instrumental in facilitating the exploration of algorithmic options and their impact on performance.","sentences":["The QED-C suite of Application-Oriented Benchmarks provides the ability to gauge performance characteristics of quantum computers as applied to real-world applications.","Its benchmark programs sweep over a range of problem sizes and inputs, capturing key performance metrics related to the quality of results, total time of execution, and quantum gate resources consumed.","In this manuscript, we investigate challenges in broadening the relevance of this benchmarking methodology to applications of greater complexity.","First, we introduce a method for improving landscape coverage by varying algorithm parameters systematically, exemplifying this functionality in a new scalable HHL linear equation solver benchmark.","Second, we add a VQE implementation of a Hydrogen Lattice simulation to the QED-C suite, and introduce a methodology for analyzing the result quality and run-time cost trade-off.","We observe a decrease in accuracy with increased number of qubits, but only a mild increase in the execution time.","Third, unique characteristics of a supervised machine-learning classification application are explored as a benchmark to gauge the extensibility of the framework to new classes of application.","Applying this to a binary classification problem revealed the increase in training time required for larger anzatz circuits, and the significant classical overhead.","Fourth, we add methods to include optimization and error mitigation in the benchmarking workflow which allows us to: identify a favourable trade off between approximate gate synthesis and gate noise; observe the benefits of measurement error mitigation and a form of deterministic error mitigation algorithm; and to contrast the improvement with the resulting time overhead.","Looking ahead, we discuss how the benchmark framework can be instrumental in facilitating the exploration of algorithmic options and their impact on performance."],"url":"http://arxiv.org/abs/2402.08985v1","category":"quant-ph"}
{"created":"2024-02-14 06:55:09","title":"Interface logistic problems: large diffusion and singular perturbation results","abstract":"In this work we consider an interface logistic problem where two populations live in two different regions, separated by a membrane or interface where it happens an interchange of flux. Thus, the two populations only interact or are coupled through such a membrane where we impose the so-called Kedem-Katchalsky boundary conditions. For this particular scenario we analyze the existence and uniqueness of positive solutions depending on the parameters involve in the system, obtaining interesting results where one can see for the first time the effect of the membrane under such boundary conditions. To do so, we first ascertain the asymptotic behaviour of several linear and nonlinear problems for which we include a diffusion coefficient and analyse the behaviour of the solutions when such a diffusion parameter goes to zero or infinity. Despite their own interest, since these asymptotic results have never been studied before, they will be crucial in analyzing the existence and uniqueness for the main interface logistic problems under analysis. Finally, we apply such an asymptotic analysis to characterize the existence of solutions in terms of the growth rate of the populations, when both populations possess the same growth rate and, also, when they depend on different parameters.","sentences":["In this work we consider an interface logistic problem where two populations live in two different regions, separated by a membrane or interface where it happens an interchange of flux.","Thus, the two populations only interact or are coupled through such a membrane where we impose the so-called Kedem-Katchalsky boundary conditions.","For this particular scenario we analyze the existence and uniqueness of positive solutions depending on the parameters involve in the system, obtaining interesting results where one can see for the first time the effect of the membrane under such boundary conditions.","To do so, we first ascertain the asymptotic behaviour of several linear and nonlinear problems for which we include a diffusion coefficient and analyse the behaviour of the solutions when such a diffusion parameter goes to zero or infinity.","Despite their own interest, since these asymptotic results have never been studied before, they will be crucial in analyzing the existence and uniqueness for the main interface logistic problems under analysis.","Finally, we apply such an asymptotic analysis to characterize the existence of solutions in terms of the growth rate of the populations, when both populations possess the same growth rate and, also, when they depend on different parameters."],"url":"http://arxiv.org/abs/2402.08984v1","category":"math.AP"}
{"created":"2024-02-14 06:51:33","title":"On the hardness of conversion from entangled proof into separable one","abstract":"A quantum channel whose image approximates the set of separable states is called a disentangler, which plays a prominent role in the investigation of variants of the computational model called Quantum Merlin Arthur games, and has potential applications in classical and quantum algorithms for the separability testing and NP-complete problems. So far, two types of a disentangler, constructed based on $\\epsilon$-nets and the quantum de Finetti theorem, have been known; however, both of them require an exponentially large input system. Moreover, in 2008, John Watrous conjectured that any disentangler requires an exponentially large input system, called the disentangler conjecture. In this paper, we show that both of the two known disentanglers can be regarded as examples of a strong disentangler, which is a disentangler approximately breaking entanglement between one output system and the composite system of another output system and the arbitrarily large environment. Note that the strong disentangler is essentially an approximately entanglement-breaking channel while the original disentangler is an approximately entanglement-annihilating channel, and the set of strong disentanglers is a subset of disentanglers. As a main result, we show that the disentangler conjecture is true for this subset, the set of strong disentanglers, for a wide range of approximation parameters without any computational hardness assumptions.","sentences":["A quantum channel whose image approximates the set of separable states is called a disentangler, which plays a prominent role in the investigation of variants of the computational model called Quantum Merlin Arthur games, and has potential applications in classical and quantum algorithms for the separability testing and NP-complete problems.","So far, two types of a disentangler, constructed based on $\\epsilon$-nets and the quantum de Finetti theorem, have been known; however, both of them require an exponentially large input system.","Moreover, in 2008, John Watrous conjectured that any disentangler requires an exponentially large input system, called the disentangler conjecture.","In this paper, we show that both of the two known disentanglers can be regarded as examples of a strong disentangler, which is a disentangler approximately breaking entanglement between one output system and the composite system of another output system and the arbitrarily large environment.","Note that the strong disentangler is essentially an approximately entanglement-breaking channel while the original disentangler is an approximately entanglement-annihilating channel, and the set of strong disentanglers is a subset of disentanglers.","As a main result, we show that the disentangler conjecture is true for this subset, the set of strong disentanglers, for a wide range of approximation parameters without any computational hardness assumptions."],"url":"http://arxiv.org/abs/2402.08981v1","category":"quant-ph"}
{"created":"2024-02-14 06:50:16","title":"OmniBOR: A System for Automatic, Verifiable Artifact Resolution across Software Supply Chains","abstract":"Software supply chain attacks, which exploit the build process or artifacts used in the process of building a software product, are increasingly of concern. To combat these attacks, one must be able to check that every artifact that a software product depends on does not contain vulnerabilities. In this paper, we introduce OmniBOR, (Universal Bill of Receipts) a minimalistic scheme for build tools to create an artifact dependency graph which can be used to track every software artifact incorporated into a built software product. We present the architecture of OmniBOR, the underlying data representations, and two implementations that produce OmniBOR data and embed an OmniBOR Identifier into built software, including a compiler-based approach and one based on tracing the build process. We demonstrate the efficacy of this approach on benchmarks including a Linux distribution for applications such as Common Vulnerabilities and Exposures (CVE) detection and software bill of materials (SBOM) computation.","sentences":["Software supply chain attacks, which exploit the build process or artifacts used in the process of building a software product, are increasingly of concern.","To combat these attacks, one must be able to check that every artifact that a software product depends on does not contain vulnerabilities.","In this paper, we introduce OmniBOR, (Universal Bill of Receipts) a minimalistic scheme for build tools to create an artifact dependency graph which can be used to track every software artifact incorporated into a built software product.","We present the architecture of OmniBOR, the underlying data representations, and two implementations that produce OmniBOR data and embed an OmniBOR Identifier into built software, including a compiler-based approach and one based on tracing the build process.","We demonstrate the efficacy of this approach on benchmarks including a Linux distribution for applications such as Common Vulnerabilities and Exposures (CVE) detection and software bill of materials (SBOM) computation."],"url":"http://arxiv.org/abs/2402.08980v1","category":"cs.SE"}
{"created":"2024-02-14 06:43:02","title":"Confidence-aware Fine-tuning of Sequential Recommendation Systems via Conformal Prediction","abstract":"In Sequential Recommendation Systems, Cross-Entropy (CE) loss is commonly used but fails to harness item confidence scores during training. Recognizing the critical role of confidence in aligning training objectives with evaluation metrics, we propose CPFT, a versatile framework that enhances recommendation confidence by integrating Conformal Prediction (CP)-based losses with CE loss during fine-tuning. CPFT dynamically generates a set of items with a high probability of containing the ground truth, enriching the training process by incorporating validation data without compromising its role in model selection. This innovative approach, coupled with CP-based losses, sharpens the focus on refining recommendation sets, thereby elevating the confidence in potential item predictions. By fine-tuning item confidence through CP-based losses, CPFT significantly enhances model performance, leading to more precise and trustworthy recommendations that increase user trust and satisfaction. Our extensive evaluation across five diverse datasets and four distinct sequential models confirms CPFT's substantial impact on improving recommendation quality through strategic confidence optimization. Access to the framework's code will be provided following the acceptance of the paper.","sentences":["In Sequential Recommendation Systems, Cross-Entropy (CE) loss is commonly used but fails to harness item confidence scores during training.","Recognizing the critical role of confidence in aligning training objectives with evaluation metrics, we propose CPFT, a versatile framework that enhances recommendation confidence by integrating Conformal Prediction (CP)-based losses with CE loss during fine-tuning.","CPFT dynamically generates a set of items with a high probability of containing the ground truth, enriching the training process by incorporating validation data without compromising its role in model selection.","This innovative approach, coupled with CP-based losses, sharpens the focus on refining recommendation sets, thereby elevating the confidence in potential item predictions.","By fine-tuning item confidence through CP-based losses, CPFT significantly enhances model performance, leading to more precise and trustworthy recommendations that increase user trust and satisfaction.","Our extensive evaluation across five diverse datasets and four distinct sequential models confirms CPFT's substantial impact on improving recommendation quality through strategic confidence optimization.","Access to the framework's code will be provided following the acceptance of the paper."],"url":"http://arxiv.org/abs/2402.08976v1","category":"cs.IR"}
{"created":"2024-02-14 06:29:37","title":"A remark on \"A non-singular dynamical system without maximal ergodic inequality\" by E. H. El Abdalaoui\"","abstract":"In this note we would like to correct a comment made by E.H. El Abdaloui about my work [arXiv:1312:5270].","sentences":["In this note we would like to correct a comment made by E.H. El Abdaloui about my work [arXiv:1312:5270]."],"url":"http://arxiv.org/abs/2402.08970v1","category":"math.DS"}
{"created":"2024-02-14 06:29:32","title":"Hamiltonian input model and spectroscopy on quantum computers","abstract":"We present a novel input model for general second-quantized Hamiltonians of relativistic or non-relativistic many-fermion systems. This input model incorporates the fermionic anticommutation relations, particle number variations, and respects the symmetries of the Hamiltonian. Based on our input model, we propose a hybrid framework for spectral calculations on future quantum hardwares. We provide explicit circuit designs and the associated gate cost and circuit depth. We demonstrate our framework by solving the low-lying spectra of ${^{42}}Ca$ and ${^{46}}Ca$. Our input model provides new pathways to solving the spectra and time evolutions of the relativistic and nonrelativistic many-fermion systems.","sentences":["We present a novel input model for general second-quantized Hamiltonians of relativistic or non-relativistic many-fermion systems.","This input model incorporates the fermionic anticommutation relations, particle number variations, and respects the symmetries of the Hamiltonian.","Based on our input model, we propose a hybrid framework for spectral calculations on future quantum hardwares.","We provide explicit circuit designs and the associated gate cost and circuit depth.","We demonstrate our framework by solving the low-lying spectra of ${^{42}}Ca$ and ${^{46}}Ca$.","Our input model provides new pathways to solving the spectra and time evolutions of the relativistic and nonrelativistic many-fermion systems."],"url":"http://arxiv.org/abs/2402.08969v1","category":"quant-ph"}
{"created":"2024-02-14 06:20:48","title":"Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays","abstract":"Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist of pairs of X-ray images, along with question-answer sets and radiologist's reports that describe the changes in lung abnormalities and diseases over time. Our experimental results show that the PLURAL model outperforms state-of-the-art methods not only in diff-VQA for longitudinal X-rays but also in conventional VQA for a single X-ray image. Through extensive experiments, we demonstrate the effectiveness of the proposed VLM architecture and pretraining method in improving the model's performance.","sentences":["Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images.","This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice.","However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM).","Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task.","The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data.","The longitudinal data consist of pairs of X-ray images, along with question-answer sets and radiologist's reports that describe the changes in lung abnormalities and diseases over time.","Our experimental results show that the PLURAL model outperforms state-of-the-art methods not only in diff-VQA for longitudinal X-rays but also in conventional VQA for a single X-ray image.","Through extensive experiments, we demonstrate the effectiveness of the proposed VLM architecture and pretraining method in improving the model's performance."],"url":"http://arxiv.org/abs/2402.08966v1","category":"cs.CV"}
{"created":"2024-02-14 06:10:44","title":"Predicting User Experience on Laptops from Hardware Specifications","abstract":"Estimating the overall user experience (UX) on a device is a common challenge faced by manufacturers. Today, device makers primarily rely on microbenchmark scores, such as Geekbench, that stress test specific hardware components, such as CPU or RAM, but do not satisfactorily capture consumer workloads. System designers often rely on domain-specific heuristics and extensive testing of prototypes to reach a desired UX goal, and yet there is often a mismatch between the manufacturers' performance claims and the consumers' experience.   We present our initial results on predicting real-life experience on laptops from their hardware specifications. We target web applications that run on Chromebooks (ChromeOS laptops) for a simple and fair aggregation of experience across applications and workloads. On 54 laptops, we track 9 UX metrics on common end-user workloads: web browsing, video playback and audio/video calls. We focus on a subset of high-level metrics exposed by the Chrome browser, that are part of the Web Vitals initiative for judging the UX on web applications.   With a dataset of 100K UX data points, we train gradient boosted regression trees that predict the metric values from device specifications. Across our 9 metrics, we note a mean $R^2$ score (goodness-of-fit on our dataset) of 97.8% and a mean MAAPE (percentage error in prediction on unseen data) of 10.1%.","sentences":["Estimating the overall user experience (UX) on a device is a common challenge faced by manufacturers.","Today, device makers primarily rely on microbenchmark scores, such as Geekbench, that stress test specific hardware components, such as CPU or RAM, but do not satisfactorily capture consumer workloads.","System designers often rely on domain-specific heuristics and extensive testing of prototypes to reach a desired UX goal, and yet there is often a mismatch between the manufacturers' performance claims and the consumers' experience.   ","We present our initial results on predicting real-life experience on laptops from their hardware specifications.","We target web applications that run on Chromebooks (ChromeOS laptops) for a simple and fair aggregation of experience across applications and workloads.","On 54 laptops, we track 9 UX metrics on common end-user workloads: web browsing, video playback and audio/video calls.","We focus on a subset of high-level metrics exposed by the Chrome browser, that are part of the Web Vitals initiative for judging the UX on web applications.   ","With a dataset of 100K UX data points, we train gradient boosted regression trees that predict the metric values from device specifications.","Across our 9 metrics, we note a mean $R^2$ score (goodness-of-fit on our dataset) of 97.8% and a mean MAAPE (percentage error in prediction on unseen data) of 10.1%."],"url":"http://arxiv.org/abs/2402.08964v1","category":"cs.LG"}
{"created":"2024-02-14 05:56:51","title":"Seagull: Privacy preserving network verification system","abstract":"The current routing protocol used in the internet backbone is based on manual configuration, making it susceptible to errors. To mitigate these configuration-related issues, it becomes imperative to validate the accuracy and convergence of the algorithm, ensuring a seamless operation devoid of problems. However, the process of network verification faces challenges related to privacy and scalability. This paper addresses these challenges by introducing a novel approach: leveraging privacy-preserving computation, specifically multiparty computation (MPC), to verify the correctness of configurations in the internet backbone, governed by the BGP protocol. Not only does our proposed solution effectively address scalability concerns, but it also establishes a robust privacy framework. Through rigorous analysis, we demonstrate that our approach maintains privacy by not disclosing any information beyond the query result, thus providing a comprehensive and secure solution to the intricacies associated with routing protocol verification in large-scale networks.","sentences":["The current routing protocol used in the internet backbone is based on manual configuration, making it susceptible to errors.","To mitigate these configuration-related issues, it becomes imperative to validate the accuracy and convergence of the algorithm, ensuring a seamless operation devoid of problems.","However, the process of network verification faces challenges related to privacy and scalability.","This paper addresses these challenges by introducing a novel approach: leveraging privacy-preserving computation, specifically multiparty computation (MPC), to verify the correctness of configurations in the internet backbone, governed by the BGP protocol.","Not only does our proposed solution effectively address scalability concerns, but it also establishes a robust privacy framework.","Through rigorous analysis, we demonstrate that our approach maintains privacy by not disclosing any information beyond the query result, thus providing a comprehensive and secure solution to the intricacies associated with routing protocol verification in large-scale networks."],"url":"http://arxiv.org/abs/2402.08956v1","category":"cs.CR"}
{"created":"2024-02-14 05:45:11","title":"A two-stage solution to quantum process tomography: error analysis and optimal design","abstract":"Quantum process tomography is a critical task for characterizing the dynamics of quantum systems and achieving precise quantum control. In this paper, we propose a two-stage solution for both trace-preserving and non-trace-preserving quantum process tomography. Utilizing a tensor structure, our algorithm exhibits a computational complexity of $O(MLd^2)$ where $d$ is the dimension of the quantum system and $ M $, $ L $ represent the numbers of different input states and measurement operators, respectively. We establish an analytical error upper bound and then design the optimal input states and the optimal measurement operators, which are both based on minimizing the error upper bound and maximizing the robustness characterized by the condition number. Numerical examples and testing on IBM quantum devices are presented to demonstrate the performance and efficiency of our algorithm.","sentences":["Quantum process tomography is a critical task for characterizing the dynamics of quantum systems and achieving precise quantum control.","In this paper, we propose a two-stage solution for both trace-preserving and non-trace-preserving quantum process tomography.","Utilizing a tensor structure, our algorithm exhibits a computational complexity of $O(MLd^2)$ where $d$ is the dimension of the quantum system and $ M $, $ L $ represent the numbers of different input states and measurement operators, respectively.","We establish an analytical error upper bound and then design the optimal input states and the optimal measurement operators, which are both based on minimizing the error upper bound and maximizing the robustness characterized by the condition number.","Numerical examples and testing on IBM quantum devices are presented to demonstrate the performance and efficiency of our algorithm."],"url":"http://arxiv.org/abs/2402.08952v1","category":"quant-ph"}
{"created":"2024-02-14 05:35:03","title":"Unraveling the emergence of quantum state designs in systems with symmetry","abstract":"Quantum state designs, by enabling an efficient sampling of random quantum states, play a quintessential role in devising and benchmarking various quantum protocols with broad applications ranging from circuit designs to black hole physics. Symmetries, on the other hand, are expected to reduce the randomness of a state. Despite being ubiquitous, the effects of symmetries on the quantum state designs remain an outstanding question. The recently introduced projected ensemble framework generates efficient approximate state t-designs by hinging on projective measurements and many-body quantum chaos. In this work, we examine the emergence of state designs from the random generator states exhibiting symmetries. Leveraging on translation symmetry, we analytically establish a sufficient condition for the measurement basis leading to the state t-designs. Then, by making use of a trace distance measure, we numerically investigate the convergence to the designs. Subsequently, we inspect the violation of the sufficient condition to identify bases that fail to converge. We further demonstrate the emergence of state designs in a physical system by studying dynamics of a chaotic tilted field Ising chain with periodic boundary conditions. We find faster convergence of the trace distance in the initial time, however, it saturates to a finite value deviating from random matrix prediction at late times, in contrast to the case with open boundary condition. To delineate the general applicability of our results, we extend our analysis to other symmetries. We expect our findings to pave the way for further exploration of deep thermalization and equilibration of closed and open quantum many-body systems.","sentences":["Quantum state designs, by enabling an efficient sampling of random quantum states, play a quintessential role in devising and benchmarking various quantum protocols with broad applications ranging from circuit designs to black hole physics.","Symmetries, on the other hand, are expected to reduce the randomness of a state.","Despite being ubiquitous, the effects of symmetries on the quantum state designs remain an outstanding question.","The recently introduced projected ensemble framework generates efficient approximate state t-designs by hinging on projective measurements and many-body quantum chaos.","In this work, we examine the emergence of state designs from the random generator states exhibiting symmetries.","Leveraging on translation symmetry, we analytically establish a sufficient condition for the measurement basis leading to the state t-designs.","Then, by making use of a trace distance measure, we numerically investigate the convergence to the designs.","Subsequently, we inspect the violation of the sufficient condition to identify bases that fail to converge.","We further demonstrate the emergence of state designs in a physical system by studying dynamics of a chaotic tilted field Ising chain with periodic boundary conditions.","We find faster convergence of the trace distance in the initial time, however, it saturates to a finite value deviating from random matrix prediction at late times, in contrast to the case with open boundary condition.","To delineate the general applicability of our results, we extend our analysis to other symmetries.","We expect our findings to pave the way for further exploration of deep thermalization and equilibration of closed and open quantum many-body systems."],"url":"http://arxiv.org/abs/2402.08949v1","category":"quant-ph"}
{"created":"2024-02-14 05:35:03","title":"An Evaluative Comparison of Performance Portability across GPU Programming Models","abstract":"Ensuring high productivity in scientific software development necessitates developing and maintaining a single codebase that can run efficiently on a range of accelerator-based supercomputing platforms. While prior work has investigated the performance portability of a few selected proxy applications or programming models, this paper provides a comprehensive study of a range of proxy applications implemented in the major programming models suitable for GPU-based platforms. We present and analyze performance results across NVIDIA and AMD GPU hardware currently deployed in leadership-class computing facilities using a representative range of scientific codes and several programming models -- CUDA, HIP, Kokkos, RAJA, OpenMP, OpenACC, and SYCL. Based on the specific characteristics of applications tested, we include recommendations to developers on how to choose the right programming model for their code. We find that Kokkos and RAJA in particular offer the most promise empirically as performance portable programming models. These results provide a comprehensive evaluation of the extent to which each programming model for heterogeneous systems provides true performance portability in real-world usage.","sentences":["Ensuring high productivity in scientific software development necessitates developing and maintaining a single codebase that can run efficiently on a range of accelerator-based supercomputing platforms.","While prior work has investigated the performance portability of a few selected proxy applications or programming models, this paper provides a comprehensive study of a range of proxy applications implemented in the major programming models suitable for GPU-based platforms.","We present and analyze performance results across NVIDIA and AMD GPU hardware currently deployed in leadership-class computing facilities using a representative range of scientific codes and several programming models -- CUDA, HIP, Kokkos, RAJA, OpenMP, OpenACC, and SYCL.","Based on the specific characteristics of applications tested, we include recommendations to developers on how to choose the right programming model for their code.","We find that Kokkos and RAJA in particular offer the most promise empirically as performance portable programming models.","These results provide a comprehensive evaluation of the extent to which each programming model for heterogeneous systems provides true performance portability in real-world usage."],"url":"http://arxiv.org/abs/2402.08950v1","category":"cs.DC"}
{"created":"2024-02-14 04:44:05","title":"AINeedsPlanner: AWorkbook to Support Effective Collaboration Between AI Experts and Clients","abstract":"Clients often partner with AI experts to develop AI applications tailored to their needs. In these partnerships, careful planning and clear communication are critical, as inaccurate or incomplete specifications can result in misaligned model characteristics, expensive reworks, and potential friction between collaborators. Unfortunately, given the complexity of requirements ranging from functionality, data, and governance, effective guidelines for collaborative specification of requirements in client-AI expert collaborations are missing. In this work, we introduce AINeedsPlanner, a workbook that AI experts and clients can use to facilitate effective interchange and clear specifications. The workbook is based on (1) an interview of 10 completed AI application project teams, which identifies and characterizes steps in AI application planning and (2) a study with 12 AI experts, which defines a taxonomy of AI experts' information needs and dimensions that affect the information needs. Finally, we demonstrate the workbook's utility with two case studies in real-world settings.","sentences":["Clients often partner with AI experts to develop AI applications tailored to their needs.","In these partnerships, careful planning and clear communication are critical, as inaccurate or incomplete specifications can result in misaligned model characteristics, expensive reworks, and potential friction between collaborators.","Unfortunately, given the complexity of requirements ranging from functionality, data, and governance, effective guidelines for collaborative specification of requirements in client-AI expert collaborations are missing.","In this work, we introduce AINeedsPlanner, a workbook that AI experts and clients can use to facilitate effective interchange and clear specifications.","The workbook is based on (1) an interview of 10 completed AI application project teams, which identifies and characterizes steps in AI application planning and (2) a study with 12 AI experts, which defines a taxonomy of AI experts' information needs and dimensions that affect the information needs.","Finally, we demonstrate the workbook's utility with two case studies in real-world settings."],"url":"http://arxiv.org/abs/2402.08938v1","category":"cs.HC"}
{"created":"2024-02-14 04:34:48","title":"Predictive Temporal Attention on Event-based Video Stream for Energy-efficient Situation Awareness","abstract":"The Dynamic Vision Sensor (DVS) is an innovative technology that efficiently captures and encodes visual information in an event-driven manner. By combining it with event-driven neuromorphic processing, the sparsity in DVS camera output can result in high energy efficiency. However, similar to many embedded systems, the off-chip communication between the camera and processor presents a bottleneck in terms of power consumption. Inspired by the predictive coding model and expectation suppression phenomenon found in human brain, we propose a temporal attention mechanism to throttle the camera output and pay attention to it only when the visual events cannot be well predicted. The predictive attention not only reduces power consumption in the sensor-processor interface but also effectively decreases the computational workload by filtering out noisy events. We demonstrate that the predictive attention can reduce 46.7% of data communication between the camera and the processor and reduce 43.8% computation activities in the processor.","sentences":["The Dynamic Vision Sensor (DVS) is an innovative technology that efficiently captures and encodes visual information in an event-driven manner.","By combining it with event-driven neuromorphic processing, the sparsity in DVS camera output can result in high energy efficiency.","However, similar to many embedded systems, the off-chip communication between the camera and processor presents a bottleneck in terms of power consumption.","Inspired by the predictive coding model and expectation suppression phenomenon found in human brain, we propose a temporal attention mechanism to throttle the camera output and pay attention to it only when the visual events cannot be well predicted.","The predictive attention not only reduces power consumption in the sensor-processor interface but also effectively decreases the computational workload by filtering out noisy events.","We demonstrate that the predictive attention can reduce 46.7% of data communication between the camera and the processor and reduce 43.8% computation activities in the processor."],"url":"http://arxiv.org/abs/2402.08936v1","category":"cs.CV"}
{"created":"2024-02-14 04:03:20","title":"Noise estimation in an entanglement distillation protocol","abstract":"Estimating noise processes is an essential step for practical quantum information processing. Standard estimation tools require consuming valuable quantum resources. Here we ask the question of whether the noise affecting entangled states can be learned solely from the measurement statistics obtained during a distillation protocol. As a first step, we consider states of the Werner form and find that the Werner parameter can be estimated efficiently from the measurement statistics of an idealized distillation protocol. Our proposed estimation method can find application in scenarios where distillation is an unavoidable step.","sentences":["Estimating noise processes is an essential step for practical quantum information processing.","Standard estimation tools require consuming valuable quantum resources.","Here we ask the question of whether the noise affecting entangled states can be learned solely from the measurement statistics obtained during a distillation protocol.","As a first step, we consider states of the Werner form and find that the Werner parameter can be estimated efficiently from the measurement statistics of an idealized distillation protocol.","Our proposed estimation method can find application in scenarios where distillation is an unavoidable step."],"url":"http://arxiv.org/abs/2402.08928v1","category":"quant-ph"}
{"created":"2024-02-14 03:46:13","title":"Study of local conserved quantities in the one-dimensional Hubbard model","abstract":"In this thesis, I study the local charges $\\{Q_k\\}$ in the one-dimensional Hubbard model, which is an integrable system used for theoretical studies of non-perturbative effects in strongly correlated electron systems. I obtained their explicit expressions in a closed form. An expression of the $k$-local charge $Q_k$ for $k > 5$, where $k$ denotes the support, had been unknown in previous studies. I find that $Q_k$ is a linear combination of a particular kind of products of the hopping terms. I introduce a diagrammatic notation to represent these products efficiently, which enables the prediction of the general formula of the local charges. These linear combinations have non-trivial coefficients for $k > 5$, and some of the coefficients were proved to be identical to the generalized Catalan numbers, which also appear in the local charges of the Heisenberg chain. I derive the recursion equation for these coefficients. I also prove that the obtained local charges are exhaustive. Thus, any local charge is written as a linear combination of $\\{Q_k\\}$. Although the completeness of the local charges is a natural conjecture in the transfer-matrix formulation, its proof has not been presented for almost all cases. Lastly, I emphasize that this is the first study demonstrating general explicit expressions of the local charges for an electron system that lacks the boost operator.","sentences":["In this thesis, I study the local charges $\\{Q_k\\}$ in the one-dimensional Hubbard model, which is an integrable system used for theoretical studies of non-perturbative effects in strongly correlated electron systems.","I obtained their explicit expressions in a closed form.","An expression of the $k$-local charge $Q_k$ for $k > 5$, where $k$ denotes the support, had been unknown in previous studies.","I find that $Q_k$ is a linear combination of a particular kind of products of the hopping terms.","I introduce a diagrammatic notation to represent these products efficiently, which enables the prediction of the general formula of the local charges.","These linear combinations have non-trivial coefficients for $k > 5$, and some of the coefficients were proved to be identical to the generalized Catalan numbers, which also appear in the local charges of the Heisenberg chain.","I derive the recursion equation for these coefficients.","I also prove that the obtained local charges are exhaustive.","Thus, any local charge is written as a linear combination of $\\{Q_k\\}$. Although the completeness of the local charges is a natural conjecture in the transfer-matrix formulation, its proof has not been presented for almost all cases.","Lastly, I emphasize that this is the first study demonstrating general explicit expressions of the local charges for an electron system that lacks the boost operator."],"url":"http://arxiv.org/abs/2402.08924v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-14 03:34:02","title":"Quantifying and Characterizing Clones of Self-Admitted Technical Debt in Build Systems","abstract":"Self-Admitted Technical Debt (SATD) annotates development decisions that intentionally exchange long-term software artifact quality for short-term goals. Recent work explores the existence of SATD clones (duplicate or near duplicate SATD comments) in source code. Cloning of SATD in build systems (e.g., CMake and Maven) may propagate suboptimal design choices, threatening qualities of the build system that stakeholders rely upon (e.g., maintainability, reliability, repeatability). Hence, we conduct a large-scale study on 50,608 SATD comments extracted from Autotools, CMake, Maven, and Ant build systems to investigate the prevalence of SATD clones and to characterize their incidences. We observe that: (i) prior work suggests that 41-65% of SATD comments in source code are clones, but in our studied build system context, the rates range from 62% to 95%, suggesting that SATD clones are a more prevalent phenomenon in build systems than in source code; (ii) statements surrounding SATD clones are highly similar, with 76% of occurrences having similarity scores greater than 0.8; (iii) a quarter of SATD clones are introduced by the author of the original SATD statements; and (iv) among the most commonly cloned SATD comments, external factors (e.g., platform and tool configuration) are the most frequent locations, limitations in tools and libraries are the most frequent causes, and developers often copy SATD comments that describe issues to be fixed later. Our work presents the first step toward systematically understanding SATD clones in build systems and opens up avenues for future work, such as distinguishing different SATD clone behavior, as well as designing an automated recommendation system for repaying SATD effectively based on resolved clones.","sentences":["Self-Admitted Technical Debt (SATD) annotates development decisions that intentionally exchange long-term software artifact quality for short-term goals.","Recent work explores the existence of SATD clones (duplicate or near duplicate SATD comments) in source code.","Cloning of SATD in build systems (e.g., CMake and Maven) may propagate suboptimal design choices, threatening qualities of the build system that stakeholders rely upon (e.g., maintainability, reliability, repeatability).","Hence, we conduct a large-scale study on 50,608 SATD comments extracted from Autotools, CMake, Maven, and Ant build systems to investigate the prevalence of SATD clones and to characterize their incidences.","We observe that: (i) prior work suggests that 41-65% of SATD comments in source code are clones, but in our studied build system context, the rates range from 62% to 95%, suggesting that SATD clones are a more prevalent phenomenon in build systems than in source code; (ii) statements surrounding SATD clones are highly similar, with 76% of occurrences having similarity scores greater than 0.8; (iii) a quarter of SATD clones are introduced by the author of the original SATD statements; and (iv) among the most commonly cloned SATD comments, external factors (e.g., platform and tool configuration) are the most frequent locations, limitations in tools and libraries are the most frequent causes, and developers often copy SATD comments that describe issues to be fixed later.","Our work presents the first step toward systematically understanding SATD clones in build systems and opens up avenues for future work, such as distinguishing different SATD clone behavior, as well as designing an automated recommendation system for repaying SATD effectively based on resolved clones."],"url":"http://arxiv.org/abs/2402.08920v1","category":"cs.SE"}
{"created":"2024-02-14 03:31:17","title":"Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding","abstract":"Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of \"conceptual similarity\" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate \"explanations\" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this definition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks. Beyond just providing a number, our method also offers interpretability by pointing to the specific level of granularity of the description where the source data are differentiated.","sentences":["Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning.","In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls.","Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying.","We seek to define and compute a notion of \"conceptual similarity\" among images that captures high-level relations even among images that do not share repeated elements or visually similar components.","The idea is to use a base multi-modal model to generate \"explanations\" (captions) of visual data at increasing levels of complexity.","Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished.","We operationalize this definition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks.","Beyond just providing a number, our method also offers interpretability by pointing to the specific level of granularity of the description where the source data are differentiated."],"url":"http://arxiv.org/abs/2402.08919v1","category":"cs.CV"}
{"created":"2024-02-14 03:08:46","title":"Lightweight Deep Learning Based Channel Estimation for Extremely Large-Scale Massive MIMO Systems","abstract":"Extremely large-scale massive multiple-input multiple-output (XL-MIMO) systems introduce the much higher channel dimensionality and incur the additional near-field propagation effect, aggravating the computation load and the difficulty to acquire the prior knowledge for channel estimation. In this article, an XL-MIMO channel network (XLCNet) is developed to estimate the high-dimensional channel, which is a universal solution for both the near-field users and far-field users with different channel statistics. Furthermore, a compressed XLCNet (C-XLCNet) is designed via weight pruning and quantization to accelerate the model inference as well as to facilitate the model storage and transmission. Simulation results show the performance superiority and universality of XLCNet. Compared to XLCNet, C-XLCNet incurs the limited performance loss while reducing the computational complexity and model size by about $10 \\times$ and $36 \\times$, respectively.","sentences":["Extremely large-scale massive multiple-input multiple-output (XL-MIMO) systems introduce the much higher channel dimensionality and incur the additional near-field propagation effect, aggravating the computation load and the difficulty to acquire the prior knowledge for channel estimation.","In this article, an XL-MIMO channel network (XLCNet) is developed to estimate the high-dimensional channel, which is a universal solution for both the near-field users and far-field users with different channel statistics.","Furthermore, a compressed XLCNet (C-XLCNet) is designed via weight pruning and quantization to accelerate the model inference as well as to facilitate the model storage and transmission.","Simulation results show the performance superiority and universality of XLCNet.","Compared to XLCNet, C-XLCNet incurs the limited performance loss while reducing the computational complexity and model size by about $10 \\times$ and $36 \\times$, respectively."],"url":"http://arxiv.org/abs/2402.08916v1","category":"eess.SP"}
{"created":"2024-02-14 03:04:41","title":"Sharp decay estimates and asymptotic stability for incompressible MHD equations without viscosity or magnetic diffusion","abstract":"Whether the global existence and uniqueness of strong solutions of $n$-dimensional incompressible magnetohydrodynamic (MHD for short) equations with only kinematic viscosity or magnetic diffusion holds true or not remains an outstanding open problem. In recent years, more attention has been paid to the case when the magnetic field close to an equilibrium state (the background magnetic field for short). Specifically, when the background magnetic field satisfies the Diophantine condition (see (1.2) for details), Chen, Zhang and Zhou [Sci. China Math. 41 (2022), pp.1-10] first studied the perturbation system and established the decay estimates and stability of its solutions in 3D periodic domain $\\mathbb{T}^3$, which was then improved to $H^{(3r+5+(3\\alpha+2\\beta))}(\\mathbb{T}^2)$ for 2D periodic domain $\\mathbb{T}^2$ by Zhai [J. Differ. Equ. 374 (2023), pp.267-278]. In this paper, we seek to find the optimal decay estimates and improve the space where the global stability is taking place. Through deeply exploring and fully utilizing the structure of perturbation system, we discover a new dissipative mechanism, which enables us to establish the decay estimates in Sobolev space with much lower regularity. Based on the above discovery, we greatly reduce the initial regularity requirement of aforementioned two works from $H^{(4r+7)}(\\mathbb{T}^3)$ and $H^{(3r+5+(3\\alpha+2\\beta))}(\\mathbb{T}^2)$ to $H^{(3r+3)^+}(\\mathbb{T}^n)$ for $r>n-1$, $\\alpha>0$ and $\\beta>0$ when $n=3$ and $n=2$ respectively. Additionally, we first present the linear stability result via the method of spectral analysis in this paper. From which, the decay estimates obtained for the nonlinear system can be seen as sharp in the sense that they are in line with those for the linearized system.","sentences":["Whether the global existence and uniqueness of strong solutions of $n$-dimensional incompressible magnetohydrodynamic (MHD for short) equations with only kinematic viscosity or magnetic diffusion holds true or not remains an outstanding open problem.","In recent years, more attention has been paid to the case when the magnetic field close to an equilibrium state (the background magnetic field for short).","Specifically, when the background magnetic field satisfies the Diophantine condition (see (1.2) for details), Chen, Zhang and Zhou [Sci. China Math. 41","(2022)",", pp.1-10] first studied the perturbation system and established the decay estimates and stability of its solutions in 3D periodic domain $\\mathbb{T}^3$, which was then improved to $H^{(3r+5+(3\\alpha+2\\beta))}(\\mathbb{T}^2)$ for 2D periodic domain $\\mathbb{T}^2$ by Zhai","[J. Differ.","Equ. 374 (2023), pp.267-278].","In this paper, we seek to find the optimal decay estimates and improve the space where the global stability is taking place.","Through deeply exploring and fully utilizing the structure of perturbation system, we discover a new dissipative mechanism, which enables us to establish the decay estimates in Sobolev space with much lower regularity.","Based on the above discovery, we greatly reduce the initial regularity requirement of aforementioned two works from $H^{(4r+7)}(\\mathbb{T}^3)$ and $H^{(3r+5+(3\\alpha+2\\beta))}(\\mathbb{T}^2)$ to $H^{(3r+3)^+}(\\mathbb{T}^n)$ for $r>n-1$, $\\alpha>0$ and $\\beta>0$ when $n=3$ and $n=2$ respectively.","Additionally, we first present the linear stability result via the method of spectral analysis in this paper.","From which, the decay estimates obtained for the nonlinear system can be seen as sharp in the sense that they are in line with those for the linearized system."],"url":"http://arxiv.org/abs/2402.08913v1","category":"math.AP"}
{"created":"2024-02-14 02:56:40","title":"A merger-driven scenario for clumpy galaxy formation in the epoch of reionization: Physical properties of clumps in the FirstLight simulation","abstract":"Recent JWST observations with superb angular resolution have revealed the existence of clumpy galaxies at high redshift through the detection of rest-frame optical emission lines. We use the FirstLight simulation to study the properties of (sub-)galactic clumps that are bright in [OIII] 5007$\\mathrm{\\mathring{A}}$ line with flux greater than $\\sim 10^{-18} \\, {\\rm erg\\, s^{-1}\\, cm^{-2}}$, to be detected by JWST. For 62 simulated galaxies that have stellar masses of $(0.5-6) \\times 10^{10} \\, M_\\odot$ at $z=5$, we find clumps in 1828 snapshots in the redshift range $z = 9.5-5.5$. The clumps are identified by the surface density of star formation rate. About one-tenth of the snapshots show the existence of clumpy systems with two or more components. Most of the clumps are formed by mergers and can be characterized by their ages; central clumps dominated by stellar populations older than 50 Myr, and off-centered clumps dominated by younger stellar populations with specific star formation rates of $\\sim 50 \\, {\\rm Gyr^{-1}}$. The latter type of young clumps is formed from gas debris in the tidal tails of major mergers with baryonic mass ratios of $1 \\leq q < 4$. The merger-induced clumps are short-lived, and merge within a dynamical time of several tens million years. The number density of the clumpy systems is estimated to be $\\sim 10^{-5}\\, {\\rm cMpc^{-3}}$, which is large enough to be detected in recent JWST surveys.","sentences":["Recent JWST observations with superb angular resolution have revealed the existence of clumpy galaxies at high redshift through the detection of rest-frame optical emission lines.","We use the FirstLight simulation to study the properties of (sub-)galactic clumps that are bright in [OIII] 5007$\\mathrm{\\mathring{A}}$ line with flux greater than $\\sim 10^{-18} \\, {\\rm erg\\, s^{-1}\\, cm^{-2}}$, to be detected by JWST.","For 62 simulated galaxies that have stellar masses of $(0.5-6) \\times 10^{10} \\, M_\\odot$ at $z=5$, we find clumps in 1828 snapshots in the redshift range $z = 9.5-5.5$. The clumps are identified by the surface density of star formation rate.","About one-tenth of the snapshots show the existence of clumpy systems with two or more components.","Most of the clumps are formed by mergers and can be characterized by their ages; central clumps dominated by stellar populations older than 50 Myr, and off-centered clumps dominated by younger stellar populations with specific star formation rates of $\\sim 50 \\, {\\rm Gyr^{-1}}$.","The latter type of young clumps is formed from gas debris in the tidal tails of major mergers with baryonic mass ratios of $1 \\leq q <","4$.","The merger-induced clumps are short-lived, and merge within a dynamical time of several tens million years.","The number density of the clumpy systems is estimated to be $\\sim 10^{-5}\\, {\\rm cMpc^{-3}}$, which is large enough to be detected in recent JWST surveys."],"url":"http://arxiv.org/abs/2402.08911v1","category":"astro-ph.GA"}
{"created":"2024-02-14 02:53:51","title":"Learning-based Bone Quality Classification Method for Spinal Metastasis","abstract":"Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by $+12.54\\%$, $+7.23\\%$ and $+29.06\\%$ for blastic, mixed and lytic lesions, respectively, meanwhile $+12.33\\%$, $+23.21\\%$ and $+34.25\\%$ at vertebrae level.","sentences":["Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries.","Early detection of spinal metastasis is critical for accurate staging and optimal treatment.","The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists.","In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images.","We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance.","MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks.","Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions.","In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process.","The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset.","At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by $+12.54\\%$, $+7.23\\%$ and $+29.06\\%$ for blastic, mixed and lytic lesions, respectively, meanwhile $+12.33\\%$, $+23.21\\%$ and $+34.25\\%$ at vertebrae level."],"url":"http://arxiv.org/abs/2402.08910v1","category":"cs.CV"}
{"created":"2024-02-14 02:41:36","title":"Time preference, wealth and utility inequality: A microeconomic interaction and dynamic macroeconomic model connection approach","abstract":"Based on interactions between individuals and others and references to social norms, this study reveals the impact of heterogeneity in time preference on wealth distribution and inequality. We present a novel approach that connects the interactions between microeconomic agents that generate heterogeneity to the dynamic equations for capital and consumption in macroeconomic models. Using this approach, we estimate the impact of changes in the discount rate due to microeconomic interactions on capital, consumption and utility and the degree of inequality. The results show that intercomparisons with others regarding consumption significantly affect capital, i.e. wealth inequality. Furthermore, the impact on utility is never small and social norms can reduce this impact. Our supporting evidence shows that the quantitative results of inequality calculations correspond to survey data from cohort and cross-cultural studies. This study's micro-macro connection approach can be deployed to connect microeconomic interactions, such as exchange, interest and debt, redistribution, mutual aid and time preference, to dynamic macroeconomic models.","sentences":["Based on interactions between individuals and others and references to social norms, this study reveals the impact of heterogeneity in time preference on wealth distribution and inequality.","We present a novel approach that connects the interactions between microeconomic agents that generate heterogeneity to the dynamic equations for capital and consumption in macroeconomic models.","Using this approach, we estimate the impact of changes in the discount rate due to microeconomic interactions on capital, consumption and utility and the degree of inequality.","The results show that intercomparisons with others regarding consumption significantly affect capital, i.e. wealth inequality.","Furthermore, the impact on utility is never small and social norms can reduce this impact.","Our supporting evidence shows that the quantitative results of inequality calculations correspond to survey data from cohort and cross-cultural studies.","This study's micro-macro connection approach can be deployed to connect microeconomic interactions, such as exchange, interest and debt, redistribution, mutual aid and time preference, to dynamic macroeconomic models."],"url":"http://arxiv.org/abs/2402.08905v1","category":"econ.GN"}
{"created":"2024-02-14 02:17:37","title":"Auto-Encoding Bayesian Inverse Games","abstract":"When multiple agents interact in a common environment, each agent's actions impact others' future decisions, and noncooperative dynamic games naturally capture this coupling. In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players. Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations. Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior. To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters. To render inference tractable, we employ a variational autoencoder (VAE) with an embedded differentiable game solver. This structured VAE can be trained from an unlabeled dataset of observed interactions, naturally handles continuous, multi-modal distributions, and supports efficient sampling from the inferred posteriors without computing game solutions at runtime. Extensive evaluations in simulated driving scenarios demonstrate that the proposed approach successfully learns the prior and posterior objective distributions, provides more accurate objective estimates than MLE baselines, and facilitates safer and more efficient game-theoretic motion planning.","sentences":["When multiple agents interact in a common environment, each agent's actions impact others' future decisions, and noncooperative dynamic games naturally capture this coupling.","In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players.","Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations.","Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior.","To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters.","To render inference tractable, we employ a variational autoencoder (VAE) with an embedded differentiable game solver.","This structured VAE can be trained from an unlabeled dataset of observed interactions, naturally handles continuous, multi-modal distributions, and supports efficient sampling from the inferred posteriors without computing game solutions at runtime.","Extensive evaluations in simulated driving scenarios demonstrate that the proposed approach successfully learns the prior and posterior objective distributions, provides more accurate objective estimates than MLE baselines, and facilitates safer and more efficient game-theoretic motion planning."],"url":"http://arxiv.org/abs/2402.08902v1","category":"cs.RO"}
{"created":"2024-02-14 02:13:23","title":"The photodissociation dynamics and ultrafast electron diffraction image of cyclobutanone from the surface hopping dynamics simulation","abstract":"The comprehension of nonadiabatic dynamics in polyatomic systems relies heavily on the simultaneous advancements in theoretical and experimental domains. The gas-phase electron diffraction (GUED) technique has attracted widespread attention as a promising tool for observing the photochemical and photophysical features at all-atomic level with high temporal and spatial resolutions. In this work, the GUED spectra were predicted to perform a double-blind test of accuracy in excited-state simulation for cyclobutanone based on the trajectory surface hopping method, with respect to the benchmark data obtained by upcoming MeV-GUED experiments at the Stanfold Linear Accelerator Laboratory. The results show that the ultrafast nonadiabatic dynamics occurs in the photoinduced dynamics, and two C2 and C3 channels play dominant roles in the nonadiabatic reactions of cyclobutanone. The simulated UED signal can be directly interpreted by atomic movements, providing a unique view to monitor the time-dependent evolution of the molecular structure in the femtosecond dynamics.","sentences":["The comprehension of nonadiabatic dynamics in polyatomic systems relies heavily on the simultaneous advancements in theoretical and experimental domains.","The gas-phase electron diffraction (GUED) technique has attracted widespread attention as a promising tool for observing the photochemical and photophysical features at all-atomic level with high temporal and spatial resolutions.","In this work, the GUED spectra were predicted to perform a double-blind test of accuracy in excited-state simulation for cyclobutanone based on the trajectory surface hopping method, with respect to the benchmark data obtained by upcoming MeV-GUED experiments at the Stanfold Linear Accelerator Laboratory.","The results show that the ultrafast nonadiabatic dynamics occurs in the photoinduced dynamics, and two C2 and C3 channels play dominant roles in the nonadiabatic reactions of cyclobutanone.","The simulated UED signal can be directly interpreted by atomic movements, providing a unique view to monitor the time-dependent evolution of the molecular structure in the femtosecond dynamics."],"url":"http://arxiv.org/abs/2402.08900v1","category":"physics.chem-ph"}
{"created":"2024-02-14 02:12:56","title":"Free Space Optical Frequency Comparison Over Rapidly Moving Links","abstract":"The comparison of optical reference frequency signals over free-space optical links is limited by the relative motion between local and remote sites. Extreme Doppler shifts experienced in rapidly moving optical links, such as for ground-to-space, prevent the narrow-band detection required to compare or transfer optical frequencies at the highest levels of stability. We demonstrate a system capable of optical frequency comparison in the presence of extreme Doppler shifts, using an electro-optic phase modulator with an actuation bandwidth of 10 GHz, sufficient to enable ground-to-space frequency comparison. This system was demonstrated over a retro-reflected drone link, with a maximum line-of-sight velocity of 15 m/s and Doppler shift of 20 MHz. The best fractional frequency stability obtained was 2E-17 at an integration time of 2 s. These results suggest that optical frequency comparison between rapidly moving targets, as in ground-to-space applications, is possible with further system development.","sentences":["The comparison of optical reference frequency signals over free-space optical links is limited by the relative motion between local and remote sites.","Extreme Doppler shifts experienced in rapidly moving optical links, such as for ground-to-space, prevent the narrow-band detection required to compare or transfer optical frequencies at the highest levels of stability.","We demonstrate a system capable of optical frequency comparison in the presence of extreme Doppler shifts, using an electro-optic phase modulator with an actuation bandwidth of 10 GHz, sufficient to enable ground-to-space frequency comparison.","This system was demonstrated over a retro-reflected drone link, with a maximum line-of-sight velocity of 15 m/s and Doppler shift of 20 MHz.","The best fractional frequency stability obtained was 2E-17 at an integration time of 2 s. These results suggest that optical frequency comparison between rapidly moving targets, as in ground-to-space applications, is possible with further system development."],"url":"http://arxiv.org/abs/2402.08899v1","category":"physics.ins-det"}
{"created":"2024-02-14 02:07:04","title":"RB5 Low-Cost Explorer: Implementing Autonomous Long-Term Exploration on Low-Cost Robotic Hardware","abstract":"This systems paper presents the implementation and design of RB5, a wheeled robot for autonomous long-term exploration with fewer and cheaper sensors. Requiring just an RGB-D camera and low-power computing hardware, the system consists of an experimental platform with rocker-bogie suspension. It operates in unknown and GPS-denied environments and on indoor and outdoor terrains. The exploration consists of a methodology that extends frontier- and sampling-based exploration with a path-following vector field and a state-of-the-art SLAM algorithm. The methodology allows the robot to explore its surroundings at lower update frequencies, enabling the use of lower-performing and lower-cost hardware while still retaining good autonomous performance. The approach further consists of a methodology to interact with a remotely located human operator based on an inexpensive long-range and low-power communication technology from the internet-of-things domain (i.e., LoRa) and a customized communication protocol. The results and the feasibility analysis show the possible applications and limitations of the approach.","sentences":["This systems paper presents the implementation and design of RB5, a wheeled robot for autonomous long-term exploration with fewer and cheaper sensors.","Requiring just an RGB-D camera and low-power computing hardware, the system consists of an experimental platform with rocker-bogie suspension.","It operates in unknown and GPS-denied environments and on indoor and outdoor terrains.","The exploration consists of a methodology that extends frontier- and sampling-based exploration with a path-following vector field and a state-of-the-art SLAM algorithm.","The methodology allows the robot to explore its surroundings at lower update frequencies, enabling the use of lower-performing and lower-cost hardware while still retaining good autonomous performance.","The approach further consists of a methodology to interact with a remotely located human operator based on an inexpensive long-range and low-power communication technology from the internet-of-things domain (i.e., LoRa) and a customized communication protocol.","The results and the feasibility analysis show the possible applications and limitations of the approach."],"url":"http://arxiv.org/abs/2402.08897v1","category":"cs.RO"}
{"created":"2024-02-14 02:04:27","title":"High-resolution spectroscopy of proximity superconductivity in finite-size quantized surface states","abstract":"Adding superconducting (SC) electron pairing via the proximity effect to pristinely non-superconducting materials can lead to a variety of interesting physical phenomena. Particular interest has recently focused on inducing SC into two-dimensional surface states (SSs), potentially also combined with non-trivial topology. We study the mechanism of proximity-induced SC into the Shockley-type SSs of the noble metals Ag(111) and Cu(111) grown on the elemental SC Nb(110) using scanning tunneling spectroscopy. The tunneling spectra exhibit an intriguing multitude of sharp states at low energies. Their appearance can be explained by Andreev bound states (ABS) formed by the weakly proximitized SSs subject to lateral finite-size confinement. We study systematically how the proximity gap in the bulk states of both Ag(111) and Cu(111) persists up to island thicknesses of several times the bulk coherence length of Nb. We find that even for thick islands, the SSs acquire a gap, with the gap size for Cu being consistently larger than for Ag. Based on this, we argue that the SC in the SS is not provided through direct overlap of the SS wavefunction with the SC host but can be understood to be mediated by step edges inducing electronic coupling to the bulk. Our work provides important input for the microscopic understanding of induced superconductivity in heterostructures and its spectral manifestation. Moreover, it lays the foundation for more complex SC heterostructures based on noble metals.","sentences":["Adding superconducting (SC) electron pairing via the proximity effect to pristinely non-superconducting materials can lead to a variety of interesting physical phenomena.","Particular interest has recently focused on inducing SC into two-dimensional surface states (SSs), potentially also combined with non-trivial topology.","We study the mechanism of proximity-induced SC into the Shockley-type SSs of the noble metals Ag(111) and Cu(111) grown on the elemental SC Nb(110) using scanning tunneling spectroscopy.","The tunneling spectra exhibit an intriguing multitude of sharp states at low energies.","Their appearance can be explained by Andreev bound states (ABS) formed by the weakly proximitized SSs subject to lateral finite-size confinement.","We study systematically how the proximity gap in the bulk states of both Ag(111) and Cu(111) persists up to island thicknesses of several times the bulk coherence length of Nb.","We find that even for thick islands, the SSs acquire a gap, with the gap size for Cu being consistently larger than for Ag.","Based on this, we argue that the SC in the SS is not provided through direct overlap of the SS wavefunction with the SC host but can be understood to be mediated by step edges inducing electronic coupling to the bulk.","Our work provides important input for the microscopic understanding of induced superconductivity in heterostructures and its spectral manifestation.","Moreover, it lays the foundation for more complex SC heterostructures based on noble metals."],"url":"http://arxiv.org/abs/2402.08895v1","category":"cond-mat.supr-con"}
{"created":"2024-02-14 01:56:24","title":"Towards a dynamically reconfigurable pixelated reflective display: Focused ion beam for phase-change metapixel structures","abstract":"The switching and optical properties of phase-change thin films are actively investigated for future smart optical devices. The possibility of having more than one stable state, the large optical contrast between phases, and the fast and reversible switching are some attractive properties driving the research interest. Optical devices based on phase change alloys are considered the frontier contenders for tunable photonics. The combination of vivid structural color formation, with partial amorphization/crystallization of phase change alloys, and the associated optical tunability could be integrated into an energy-efficient reflective display device with high pixel density. This work demonstrates a contrast formation due to relative height differences from isolated pixelated structures. A reflective heterostructure device consisting of a low-loss Sb2Se3 alloy on a gold substrate was produced. With a focused ion beam, a pixelated metasurface structure was produced. Moreover, the ability to create local height differences using an ion beam was employed to create a structural color combination mimicking traditional LED like RGB pixels. We believe our approach in creating metapixels on phase change thin film surfaces could open up research interest in phase change alloys and moving away from semi/static plasmonic systems into truly dynamic display devices.","sentences":["The switching and optical properties of phase-change thin films are actively investigated for future smart optical devices.","The possibility of having more than one stable state, the large optical contrast between phases, and the fast and reversible switching are some attractive properties driving the research interest.","Optical devices based on phase change alloys are considered the frontier contenders for tunable photonics.","The combination of vivid structural color formation, with partial amorphization/crystallization of phase change alloys, and the associated optical tunability could be integrated into an energy-efficient reflective display device with high pixel density.","This work demonstrates a contrast formation due to relative height differences from isolated pixelated structures.","A reflective heterostructure device consisting of a low-loss Sb2Se3 alloy on a gold substrate was produced.","With a focused ion beam, a pixelated metasurface structure was produced.","Moreover, the ability to create local height differences using an ion beam was employed to create a structural color combination mimicking traditional LED like RGB pixels.","We believe our approach in creating metapixels on phase change thin film surfaces could open up research interest in phase change alloys and moving away from semi/static plasmonic systems into truly dynamic display devices."],"url":"http://arxiv.org/abs/2402.08891v1","category":"physics.optics"}
{"created":"2024-02-14 01:18:48","title":"Three-dimensional, multi-wavelength beam formation with integrated metasurface optics for Sr laser cooling","abstract":"We demonstrate the formation of a complex, multi-wavelength, three-dimensional laser beam configuration with integrated metasurface optics. Our experiments support the development of a compact Sr optical-lattice clock, which leverages magneto-optical trapping on atomic transitions at 461 nm and 689 nm without bulk free-space optics. We integrate six, mm-scale metasurface optics on a fused-silica substrate and illuminate them with light from optical fibers. The metasurface optics provide full control of beam pointing, divergence, and polarization to create the laser configuration for a magneto-optical trap. We report the efficiency and integration of the three-dimensional visible laser beam configuration, demonstrating the suitability of metasurface optics for atomic laser cooling.","sentences":["We demonstrate the formation of a complex, multi-wavelength, three-dimensional laser beam configuration with integrated metasurface optics.","Our experiments support the development of a compact Sr optical-lattice clock, which leverages magneto-optical trapping on atomic transitions at 461 nm and 689 nm without bulk free-space optics.","We integrate six, mm-scale metasurface optics on a fused-silica substrate and illuminate them with light from optical fibers.","The metasurface optics provide full control of beam pointing, divergence, and polarization to create the laser configuration for a magneto-optical trap.","We report the efficiency and integration of the three-dimensional visible laser beam configuration, demonstrating the suitability of metasurface optics for atomic laser cooling."],"url":"http://arxiv.org/abs/2402.08885v1","category":"physics.optics"}
{"created":"2024-02-14 01:18:16","title":"Machine Learning, Density Functional Theory, and Experiments to Understand the Photocatalytic Reduction of CO$_2$ by CuPt/TiO$_2$","abstract":"The photoconversion of CO$_2$ to hydrocarbons is a sustainable route to its transformation into value-added compounds and, thereby, crucial to mitigating the energy and climate crises. CuPt nanoparticles on TiO$_2$ surfaces have been reported to show promising photoconversion efficiency. For further progress, a mechanistic understanding of the catalytic properties of these CuPt/TiO$_2$ systems is vital. Here, we employ \\textit{ab-initio} calculations, machine learning, and photocatalysis experiments to explore their configurational space and examine their reactivity and find that the interface plays a key role in stabilizing *CO$_2$, *CO, and other CH-containing intermediates, facilitating higher activity and selectivity for methane. A bias-corrected machine-learning interatomic potential trained on density functional theory data enables efficient exploration of the potential energy surfaces of numerous CO$_2$@CuPt/TiO$_2$ configurations via basin-hopping Monte Carlo simulations, greatly accelerating the study of these photocatalyst systems. Our simulations show that CO$_2$ preferentially adsorbs at the interface, with C atom bonded to a Pt site and one O atom occupying an O-vacancy site. The interface also promotes the formation of *CH and *CH$_2$ intermediates. For confirmation, we synthesize CuPt/TiO$_2$ samples with a variety of compositions and analyze their morphologies and compositions using scanning electron microscopy and energy-dispersive X-ray spectroscopy, and measure their photocatalytic activity. Our computational and experimental findings qualitatively agree and highlight the importance of interface design for selective conversion of CO$_2$ to hydrocarbons.","sentences":["The photoconversion of CO$_2$ to hydrocarbons is a sustainable route to its transformation into value-added compounds and, thereby, crucial to mitigating the energy and climate crises.","CuPt nanoparticles on TiO$_2$ surfaces have been reported to show promising photoconversion efficiency.","For further progress, a mechanistic understanding of the catalytic properties of these CuPt/TiO$_2$ systems is vital.","Here, we employ \\textit{ab-initio} calculations, machine learning, and photocatalysis experiments to explore their configurational space and examine their reactivity and find that the interface plays a key role in stabilizing *CO$_2$, *CO, and other CH-containing intermediates, facilitating higher activity and selectivity for methane.","A bias-corrected machine-learning interatomic potential trained on density functional theory data enables efficient exploration of the potential energy surfaces of numerous CO$_2$@CuPt/TiO$_2$ configurations via basin-hopping Monte Carlo simulations, greatly accelerating the study of these photocatalyst systems.","Our simulations show that CO$_2$ preferentially adsorbs at the interface, with C atom bonded to a Pt site and one O atom occupying an O-vacancy site.","The interface also promotes the formation of *CH and *CH$_2$ intermediates.","For confirmation, we synthesize CuPt/TiO$_2$ samples with a variety of compositions and analyze their morphologies and compositions using scanning electron microscopy and energy-dispersive X-ray spectroscopy, and measure their photocatalytic activity.","Our computational and experimental findings qualitatively agree and highlight the importance of interface design for selective conversion of CO$_2$ to hydrocarbons."],"url":"http://arxiv.org/abs/2402.08884v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-14 00:53:40","title":"Distributed Secret Securing in Discrete-Event Systems","abstract":"In this paper, we study a security problem of protecting secrets in distributed systems. Specifically, we employ discrete-event systems to describe the structure and behaviour of distributed systems, in which global secret information is separated into pieces and stored in local component agents. The goal is to prevent such secrets from being exposed to intruders by imposing appropriate protection measures. This problem is formulated as to ensure that at least one piece of every distributed global secret is secured by a required number of protections, while the overall cost to apply protections is minimum. We first characterize the solvability of this security problem by providing a necessary and sufficient condition, and then develop an algorithm to compute a solution based on the supervisory control theory of discrete-event systems. Finally, we illustrate the effectiveness of our solution with an example system comprising distributed databases.","sentences":["In this paper, we study a security problem of protecting secrets in distributed systems.","Specifically, we employ discrete-event systems to describe the structure and behaviour of distributed systems, in which global secret information is separated into pieces and stored in local component agents.","The goal is to prevent such secrets from being exposed to intruders by imposing appropriate protection measures.","This problem is formulated as to ensure that at least one piece of every distributed global secret is secured by a required number of protections, while the overall cost to apply protections is minimum.","We first characterize the solvability of this security problem by providing a necessary and sufficient condition, and then develop an algorithm to compute a solution based on the supervisory control theory of discrete-event systems.","Finally, we illustrate the effectiveness of our solution with an example system comprising distributed databases."],"url":"http://arxiv.org/abs/2402.08878v1","category":"eess.SY"}
{"created":"2024-02-14 00:47:32","title":"Computational Considerations for the Linear Model of Coregionalization","abstract":"In the last two decades, the linear model of coregionalization (LMC) has been widely used to model multivariate spatial processes. From a computational standpoint, the LMC is a substantially easier model to work with than other multidimensional alternatives. Up to now, this fact has been largely overlooked in the literature. Starting from an analogy with matrix normal models, we propose a reformulation of the LMC likelihood that highlights the linear, rather than cubic, computational complexity as a function of the dimension of the response vector. Further, we describe in detail how those simplifications can be included in a Gaussian hierarchical model. In addition, we demonstrate in two examples how the disentangled version of the likelihood we derive can be exploited to improve Markov chain Monte Carlo (MCMC) based computations when conducting Bayesian inference. The first is an interwoven approach that combines samples from centered and whitened parametrizations of the latent LMC distributed random fields. The second is a sparsity-inducing method that introduces structural zeros in the coregionalization matrix in an attempt to reduce the number of parameters in a principled way. It also provides a new way to investigate the strength of the correlation among the components of the outcome vector. Both approaches come at virtually no additional cost and are shown to significantly improve MCMC performance and predictive performance respectively. We apply our methodology to a dataset comprised of air pollutant measurements in the state of California.","sentences":["In the last two decades, the linear model of coregionalization (LMC) has been widely used to model multivariate spatial processes.","From a computational standpoint, the LMC is a substantially easier model to work with than other multidimensional alternatives.","Up to now, this fact has been largely overlooked in the literature.","Starting from an analogy with matrix normal models, we propose a reformulation of the LMC likelihood that highlights the linear, rather than cubic, computational complexity as a function of the dimension of the response vector.","Further, we describe in detail how those simplifications can be included in a Gaussian hierarchical model.","In addition, we demonstrate in two examples how the disentangled version of the likelihood we derive can be exploited to improve Markov chain Monte Carlo (MCMC) based computations when conducting Bayesian inference.","The first is an interwoven approach that combines samples from centered and whitened parametrizations of the latent LMC distributed random fields.","The second is a sparsity-inducing method that introduces structural zeros in the coregionalization matrix in an attempt to reduce the number of parameters in a principled way.","It also provides a new way to investigate the strength of the correlation among the components of the outcome vector.","Both approaches come at virtually no additional cost and are shown to significantly improve MCMC performance and predictive performance respectively.","We apply our methodology to a dataset comprised of air pollutant measurements in the state of California."],"url":"http://arxiv.org/abs/2402.08877v1","category":"stat.ME"}
{"created":"2024-02-14 00:39:09","title":"Balancing Method for Non-monotone Missing Data","abstract":"Covariate balancing methods have been widely applied to single or monotone missing patterns and have certain advantages over likelihood-based methods and inverse probability weighting approaches based on standard logistic regression. In this paper, we consider non-monotone missing data under the complete-case missing variable condition (CCMV), which is a case of missing not at random (MNAR). Using relationships between each missing pattern and the complete-case subsample, a weighted estimator can be used for estimation, where the weight is a sum of ratios of conditional probability of observing a particular missing pattern versus that of observing the complete-case pattern, given the variables observed in the corresponding missing pattern. Plug-in estimators of the propensity ratios, however, can be unbounded and lead to unstable estimation. Using further relations between propensity ratios and balancing of moments across missing patterns, we employ tailored loss functions each encouraging empirical balance across patterns to estimate propensity ratios flexibly using functional basis expansion. We propose two penalizations to separately control propensity ratio model complexity and covariate imbalance. We study the asymptotic properties of the proposed estimators and show that they are consistent under mild smoothness assumptions. Asymptotic normality and efficiency are also developed. Numerical simulation results show that the proposed method achieves smaller mean squared errors than other methods.","sentences":["Covariate balancing methods have been widely applied to single or monotone missing patterns and have certain advantages over likelihood-based methods and inverse probability weighting approaches based on standard logistic regression.","In this paper, we consider non-monotone missing data under the complete-case missing variable condition (CCMV), which is a case of missing not at random (MNAR).","Using relationships between each missing pattern and the complete-case subsample, a weighted estimator can be used for estimation, where the weight is a sum of ratios of conditional probability of observing a particular missing pattern versus that of observing the complete-case pattern, given the variables observed in the corresponding missing pattern.","Plug-in estimators of the propensity ratios, however, can be unbounded and lead to unstable estimation.","Using further relations between propensity ratios and balancing of moments across missing patterns, we employ tailored loss functions each encouraging empirical balance across patterns to estimate propensity ratios flexibly using functional basis expansion.","We propose two penalizations to separately control propensity ratio model complexity and covariate imbalance.","We study the asymptotic properties of the proposed estimators and show that they are consistent under mild smoothness assumptions.","Asymptotic normality and efficiency are also developed.","Numerical simulation results show that the proposed method achieves smaller mean squared errors than other methods."],"url":"http://arxiv.org/abs/2402.08873v1","category":"stat.ME"}
{"created":"2024-02-14 00:36:22","title":"Slow-Wave Hybrid Magnonics","abstract":"Cavity magnonics is an emerging research area focusing on the coupling between magnons and photons. Despite its great potential for coherent information processing, it has been long restricted by the narrow interaction bandwidth. In this work, we theoretically propose and experimentally demonstrate a novel approach to achieve broadband photon-magnon coupling by adopting slow waves on engineered microwave waveguides. To the best of our knowledge, this is the first time that slow wave is combined with hybrid magnonics. Its unique properties promise great potentials for both fundamental research and practical applications, for instance, by deepening our understanding of the light-matter interaction in the slow wave regime and providing high-efficiency spin wave transducers. The device concept can be extended to other systems such as optomagnonics and magnomechanics, opening up new directions for hybrid magnonics.","sentences":["Cavity magnonics is an emerging research area focusing on the coupling between magnons and photons.","Despite its great potential for coherent information processing, it has been long restricted by the narrow interaction bandwidth.","In this work, we theoretically propose and experimentally demonstrate a novel approach to achieve broadband photon-magnon coupling by adopting slow waves on engineered microwave waveguides.","To the best of our knowledge, this is the first time that slow wave is combined with hybrid magnonics.","Its unique properties promise great potentials for both fundamental research and practical applications, for instance, by deepening our understanding of the light-matter interaction in the slow wave regime and providing high-efficiency spin wave transducers.","The device concept can be extended to other systems such as optomagnonics and magnomechanics, opening up new directions for hybrid magnonics."],"url":"http://arxiv.org/abs/2402.08872v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 00:34:47","title":"Controllability and feedback stabilizability in a nonuniform framework","abstract":"We propose a new controllability property for linear time varying control systems in finite dimension: the nonuniform complete controllability, which is halfway between the classical Kalman's properties of complete controllability and uniform complete controllability. This new concept is described in terms of two gramian inequalities, which have a strong relation; as we prove in our first result; with the property of nonuniform bounded growth for the corresponding plant, also called uncontrolled part. On the other hand, the second result proves that if a control system is nonuniformly completely controllable and its plant has the property of nonuniform bounded growth, then there exist a linear feedback control leading to a nonuniformly exponentially stable closed--loop system.","sentences":["We propose a new controllability property for linear time varying control systems in finite dimension: the nonuniform complete controllability, which is halfway between the classical Kalman's properties of complete controllability and uniform complete controllability.","This new concept is described in terms of two gramian inequalities, which have a strong relation; as we prove in our first result; with the property of nonuniform bounded growth for the corresponding plant, also called uncontrolled part.","On the other hand, the second result proves that if a control system is nonuniformly completely controllable and its plant has the property of nonuniform bounded growth, then there exist a linear feedback control leading to a nonuniformly exponentially stable closed--loop system."],"url":"http://arxiv.org/abs/2402.08870v1","category":"math.OC"}
{"created":"2024-02-14 00:16:50","title":"Multiscale graph neural networks with adaptive mesh refinement for accelerating mesh-based simulations","abstract":"Mesh-based Graph Neural Networks (GNNs) have recently shown capabilities to simulate complex multiphysics problems with accelerated performance times. However, mesh-based GNNs require a large number of message-passing (MP) steps and suffer from over-smoothing for problems involving very fine mesh. In this work, we develop a multiscale mesh-based GNN framework mimicking a conventional iterative multigrid solver, coupled with adaptive mesh refinement (AMR), to mitigate challenges with conventional mesh-based GNNs. We use the framework to accelerate phase field (PF) fracture problems involving coupled partial differential equations with a near-singular operator due to near-zero modulus inside the crack. We define the initial graph representation using all mesh resolution levels. We perform a series of downsampling steps using Transformer MP GNNs to reach the coarsest graph followed by upsampling steps to reach the original graph. We use skip connectors from the generated embedding during coarsening to prevent over-smoothing. We use Transfer Learning (TL) to significantly reduce the size of training datasets needed to simulate different crack configurations and loading conditions. The trained framework showed accelerated simulation times, while maintaining high accuracy for all cases compared to physics-based PF fracture model. Finally, this work provides a new approach to accelerate a variety of mesh-based engineering multiphysics problems","sentences":["Mesh-based Graph Neural Networks (GNNs) have recently shown capabilities to simulate complex multiphysics problems with accelerated performance times.","However, mesh-based GNNs require a large number of message-passing (MP) steps and suffer from over-smoothing for problems involving very fine mesh.","In this work, we develop a multiscale mesh-based GNN framework mimicking a conventional iterative multigrid solver, coupled with adaptive mesh refinement (AMR), to mitigate challenges with conventional mesh-based GNNs.","We use the framework to accelerate phase field (PF) fracture problems involving coupled partial differential equations with a near-singular operator due to near-zero modulus inside the crack.","We define the initial graph representation using all mesh resolution levels.","We perform a series of downsampling steps using Transformer MP GNNs to reach the coarsest graph followed by upsampling steps to reach the original graph.","We use skip connectors from the generated embedding during coarsening to prevent over-smoothing.","We use Transfer Learning (TL) to significantly reduce the size of training datasets needed to simulate different crack configurations and loading conditions.","The trained framework showed accelerated simulation times, while maintaining high accuracy for all cases compared to physics-based PF fracture model.","Finally, this work provides a new approach to accelerate a variety of mesh-based engineering multiphysics problems"],"url":"http://arxiv.org/abs/2402.08863v1","category":"cs.CE"}
{"created":"2024-02-14 00:08:45","title":"On generalized Beauville decompositions","abstract":"Motivated by the Beauville decomposition of an abelian scheme and the \"Perverse = Chern\" phenomenon for a compactified Jacobian fibration, we study in this paper splittings of the perverse filtration for compactified Jacobian fibrations.   On the one hand, we prove for the Beauville-Mukai system associated with an irreducible curve class on a K3 surface the existence of a Fourier-stable multiplicative splitting of the perverse filtration, which extends the Beauville decomposition for the nonsingular fibers. Our approach is to construct a Lefschetz decomposition associated with a Fourier-conjugate $\\mathfrak{sl}_2$-triple, which relies heavily on recent work concerning the interaction between derived equivalences and LLV algebras for hyper-K\\\"ahler varieties. Motivic lifting and connections to the Beauville-Voisin conjectures are also discussed.   On the other hand, we construct for any $g\\geq 2$ a compactified Jacobian fibration of genus g curves such that each curve is integral with at worst simple nodes and the (multiplicative) perverse filtration does not admit a multiplicative splitting. This shows that in general an extension of the Beauville decomposition cannot exist for compactified Jacobian fibrations even when the simplest singular point appears.","sentences":["Motivated by the Beauville decomposition of an abelian scheme and the \"Perverse = Chern\" phenomenon for a compactified Jacobian fibration, we study in this paper splittings of the perverse filtration for compactified Jacobian fibrations.   ","On the one hand, we prove for the Beauville-Mukai system associated with an irreducible curve class on a K3 surface the existence of a Fourier-stable multiplicative splitting of the perverse filtration, which extends the Beauville decomposition for the nonsingular fibers.","Our approach is to construct a Lefschetz decomposition associated with a Fourier-conjugate $\\mathfrak{sl}_2$-triple, which relies heavily on recent work concerning the interaction between derived equivalences and LLV algebras for hyper-K\\\"ahler varieties.","Motivic lifting and connections to the Beauville-Voisin conjectures are also discussed.   ","On the other hand, we construct for any $g\\geq 2$ a compactified Jacobian fibration of genus g curves such that each curve is integral with at worst simple nodes and the (multiplicative) perverse filtration does not admit a multiplicative splitting.","This shows that in general an extension of the Beauville decomposition cannot exist for compactified Jacobian fibrations even when the simplest singular point appears."],"url":"http://arxiv.org/abs/2402.08861v1","category":"math.AG"}
{"created":"2024-02-14 18:37:40","title":"Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning","abstract":"The tokamak offers a promising path to fusion energy, but plasma disruptions pose a major economic risk, motivating considerable advances in disruption avoidance. This work develops a reinforcement learning approach to this problem by training a policy to safely ramp-down the plasma current while avoiding limits on a number of quantities correlated with disruptions. The policy training environment is a hybrid physics and machine learning model trained on simulations of the SPARC primary reference discharge (PRD) ramp-down, an upcoming burning plasma scenario which we use as a testbed. To address physics uncertainty and model inaccuracies, the simulation environment is massively parallelized on GPU with randomized physics parameters during policy training. The trained policy is then successfully transferred to a higher fidelity simulator where it successfully ramps down the plasma while avoiding user-specified disruptive limits. We also address the crucial issue of safety criticality by demonstrating that a constraint-conditioned policy can be used as a trajectory design assistant to design a library of feed-forward trajectories to handle different physics conditions and user settings. As a library of trajectories is more interpretable and verifiable offline, we argue such an approach is a promising path for leveraging the capabilities of reinforcement learning in the safety-critical context of burning plasma tokamaks. Finally, we demonstrate how the training environment can be a useful platform for other feed-forward optimization approaches by using an evolutionary algorithm to perform optimization of feed-forward trajectories that are robust to physics uncertainty","sentences":["The tokamak offers a promising path to fusion energy, but plasma disruptions pose a major economic risk, motivating considerable advances in disruption avoidance.","This work develops a reinforcement learning approach to this problem by training a policy to safely ramp-down the plasma current while avoiding limits on a number of quantities correlated with disruptions.","The policy training environment is a hybrid physics and machine learning model trained on simulations of the SPARC primary reference discharge (PRD) ramp-down, an upcoming burning plasma scenario which we use as a testbed.","To address physics uncertainty and model inaccuracies, the simulation environment is massively parallelized on GPU with randomized physics parameters during policy training.","The trained policy is then successfully transferred to a higher fidelity simulator where it successfully ramps down the plasma while avoiding user-specified disruptive limits.","We also address the crucial issue of safety criticality by demonstrating that a constraint-conditioned policy can be used as a trajectory design assistant to design a library of feed-forward trajectories to handle different physics conditions and user settings.","As a library of trajectories is more interpretable and verifiable offline, we argue such an approach is a promising path for leveraging the capabilities of reinforcement learning in the safety-critical context of burning plasma tokamaks.","Finally, we demonstrate how the training environment can be a useful platform for other feed-forward optimization approaches by using an evolutionary algorithm to perform optimization of feed-forward trajectories that are robust to physics uncertainty"],"url":"http://arxiv.org/abs/2402.09387v1","category":"physics.plasm-ph"}
{"created":"2024-02-14 18:21:17","title":"Varentropy Estimation via Nearest Neighbor Graphs","abstract":"The Varentropy is a measure of the variability of the information content of random vector and it is invariant under affine transformations. We introduce the statistical estimate of varentropy of random vector based on the nearest neighbor graphs (distances). The asymptotic unbiasedness and L2-consistency of the estimates are established.","sentences":["The Varentropy is a measure of the variability of the information content of random vector and it is invariant under affine transformations.","We introduce the statistical estimate of varentropy of random vector based on the nearest neighbor graphs (distances).","The asymptotic unbiasedness and L2-consistency of the estimates are established."],"url":"http://arxiv.org/abs/2402.09374v1","category":"math.ST"}
{"created":"2024-02-14 17:53:48","title":"Monoclinic LaSb$_2$ Superconducting Thin Films","abstract":"Rare-earth diantimondes exhibit coupling between structural and electronic orders which are tunable under pressure and temperature. Here we present the discovery of a new polymorph of LaSb$_2$ stabilized in thin films synthesized using molecular beam epitaxy. Using diffraction, electron microscopy, and first principles calculations we identify a YbSb$_2$-type monoclinic lattice as a yet-uncharacterized stacking configuration. The material hosts superconductivity with a $T_\\mathrm{c}$ = 2 K, which is enhanced relative to the bulk ambient phase, and a long superconducting coherence length of 140 nm. This result highlights the potential thin film growth has in stabilizing novel stacking configurations in quasi-two dimensional compounds with competing layered structures.","sentences":["Rare-earth diantimondes exhibit coupling between structural and electronic orders which are tunable under pressure and temperature.","Here we present the discovery of a new polymorph of LaSb$_2$ stabilized in thin films synthesized using molecular beam epitaxy.","Using diffraction, electron microscopy, and first principles calculations we identify a YbSb$_2$-type monoclinic lattice as a yet-uncharacterized stacking configuration.","The material hosts superconductivity with a $T_\\mathrm{c}$ = 2 K, which is enhanced relative to the bulk ambient phase, and a long superconducting coherence length of 140 nm.","This result highlights the potential thin film growth has in stabilizing novel stacking configurations in quasi-two dimensional compounds with competing layered structures."],"url":"http://arxiv.org/abs/2402.09349v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-14 17:45:11","title":"Floor and fractional part statistics and arithmetic functions","abstract":"We apply statistical properties of the floor and fractional part functions to give several results describing the M\\\"obius function $\\mu(.)$ and the summatory M\\\"obius function $M(.)$. These results include the equalities $$ \\sum_{n}\\frac{\\mu(n)}{n^{2}}\\sum_{m<n}\\mu(m)=\\sum_{n}\\mu(n)\\sum_{m>n}\\frac{\\mu(m)}{m^{2}} $$ and $$ \\int_{1}^{N}\\frac{M^{2}(u)}{u^{2}}du=\\sum_{n\\leq N}\\mu(n)\\int_{n}^{N}\\frac{M(u)}{u^{2}}du, $$ the latter of which implies that, under the Riemann hypothesis and simplicity of the nontrivial zeta zeros $\\rho=1/2+i\\gamma$, $$ \\int_{1}^{N}\\frac{M^{2}(u)}{u^{2}}du\\sim\\sum_{\\rho}\\frac{N^{2i\\gamma}}{2i\\gamma}\\left(\\frac{1}{\\rho\\zeta'(\\rho)}\\right)^{2}+\\sum_{\\rho\\neq \\rho'}\\frac{N^{i(\\gamma+\\gamma')}}{i(\\gamma+\\gamma')}\\frac{1}{\\rho \\rho' \\zeta'(\\rho)\\zeta'(\\rho')} $$ as $N\\rightarrow \\infty$, where $\\zeta(.)$ denotes the zeta function. The integral on the left-hand side diverges at least logarithmically with $N$, hence this result demonstrates constructive interference properties of the ordinates $\\gamma$.","sentences":["We apply statistical properties of the floor and fractional part functions to give several results describing the M\\\"obius function $\\mu(.)$ and the summatory M\\\"obius function $M(.)$. These results include the equalities $$ \\sum_{n}\\frac{\\mu(n)}{n^{2}}\\sum_{m<n}\\mu(m)=\\sum_{n}\\mu(n)\\sum_{m>n}\\frac{\\mu(m)}{m^{2}} $$ and $$ \\int_{1}^{N}\\frac{M^{2}(u)}{u^{2}}du=\\sum_{n\\leq N}\\mu(n)\\int_{n}^{N}\\frac{M(u)}{u^{2}}du, $$ the latter of which implies that, under the Riemann hypothesis and simplicity of the nontrivial zeta zeros $\\rho=1/2+i\\gamma$, $$ \\int_{1}^{N}\\frac{M^{2}(u)}{u^{2}}du\\sim\\sum_{\\rho}\\frac{N^{2i\\gamma}}{2i\\gamma}\\left(\\frac{1}{\\rho\\zeta'(\\rho)}\\right)^{2}+\\sum_{\\rho\\neq \\rho'}\\frac{N^{i(\\gamma+\\gamma')}}{i(\\gamma+\\gamma')}\\frac{1}{\\rho \\rho' \\zeta'(\\rho)\\zeta'(\\rho')} $$ as $N\\rightarrow \\infty$, where $\\zeta(.)$ denotes the zeta function.","The integral on the left-hand side diverges at least logarithmically with $N$, hence this result demonstrates constructive interference properties of the ordinates $\\gamma$."],"url":"http://arxiv.org/abs/2402.09343v1","category":"math.NT"}
{"created":"2024-02-14 17:17:05","title":"Stability and Multigroup Fairness in Ranking with Uncertain Predictions","abstract":"Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but -- as we show -- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result is that UA rankings also achieve multigroup fairness through successful composition with multiaccurate or multicalibrated predictors. Our work demonstrates that UA rankings naturally interpolate between group and individual level fairness guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used.","sentences":["Rankings are ubiquitous across many applications, from search engines to hiring committees.","In practice, many rankings are derived from the output of predictors.","However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings.","Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings.","We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups.","Not only is stability an important requirement for its own sake, but -- as we show -- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012).","While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al.","(2021) are stable.","Our main result is that UA rankings also achieve multigroup fairness through successful composition with multiaccurate or multicalibrated predictors.","Our work demonstrates that UA rankings naturally interpolate between group and individual level fairness guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used."],"url":"http://arxiv.org/abs/2402.09326v1","category":"cs.LG"}
{"created":"2024-02-14 17:06:45","title":"Bolometric detection of Josephson radiation","abstract":"A Josephson junction (JJ) has been under intensive study ever since 1960's. Yet even in the present era of building quantum information processing devices based on many JJs, open questions regarding a single junction remain unsolved, such as quantum phase transitions, coupling of the JJ to an environment and improving coherence of a superconducting qubit. Here we design and build an engineered on-chip reservoir that acts as an efficient bolometer for detecting the Josephson radiation under non-equilibrium (biased) conditions. The bolometer converts ac Josephson current at microwave frequencies, up to about $100\\,$GHz, into a measurable dc temperature rise. The present experiment demonstrates an efficient, wide-band, thermal detection scheme of microwave photons and provides a sensitive detector of Josephson dynamics beyond the standard conductance measurements. Using a circuit model, we capture both the current-voltage characteristics and the measured power quantitatively.","sentences":["A Josephson junction (JJ) has been under intensive study ever since 1960's.","Yet even in the present era of building quantum information processing devices based on many JJs, open questions regarding a single junction remain unsolved, such as quantum phase transitions, coupling of the JJ to an environment and improving coherence of a superconducting qubit.","Here we design and build an engineered on-chip reservoir that acts as an efficient bolometer for detecting the Josephson radiation under non-equilibrium (biased) conditions.","The bolometer converts ac Josephson current at microwave frequencies, up to about $100\\,$GHz, into a measurable dc temperature rise.","The present experiment demonstrates an efficient, wide-band, thermal detection scheme of microwave photons and provides a sensitive detector of Josephson dynamics beyond the standard conductance measurements.","Using a circuit model, we capture both the current-voltage characteristics and the measured power quantitatively."],"url":"http://arxiv.org/abs/2402.09314v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 16:51:54","title":"Wave-particle correlations in multiphoton resonances of coherent light-matter interaction","abstract":"We discuss the conditional measurement of field amplitudes by a nonclassical photon sequence in the Jaynes-Cummings (JC) model under multiphoton operation. We do so by employing a correlator of immediate experimental relevance to reveal a distinct nonclassical evolution in the spirit of [G. T. Foster et al., Phys. Rev. Lett. 85 3149 (2000)]. The correlator relies on the complementary nature of the pictures obtained from different unravelings of a JC source master equation. We demonstrate that direct photodetection entails a conditioned separation of timescales, a quantum beat and a semiclassical oscillation, produced by the coherent light-matter interaction in its strong-coupling limit. We single the quantum beat out in the analytical expression for the waiting-time distribution, pertaining to the particle nature of the scattered light, and find a negative spectrum of quadrature amplitude squeezing, characteristic of its wave nature. Finally, we jointly detect the dual aspects through the wave-particle correlator, showing an asymmetric regression of fluctuations to the steady state which depends on the quadrature amplitude being measured.","sentences":["We discuss the conditional measurement of field amplitudes by a nonclassical photon sequence in the Jaynes-Cummings (JC) model under multiphoton operation.","We do so by employing a correlator of immediate experimental relevance to reveal a distinct nonclassical evolution in the spirit of [G. T. Foster et al., Phys.","Rev. Lett. 85 3149 (2000)].","The correlator relies on the complementary nature of the pictures obtained from different unravelings of a JC source master equation.","We demonstrate that direct photodetection entails a conditioned separation of timescales, a quantum beat and a semiclassical oscillation, produced by the coherent light-matter interaction in its strong-coupling limit.","We single the quantum beat out in the analytical expression for the waiting-time distribution, pertaining to the particle nature of the scattered light, and find a negative spectrum of quadrature amplitude squeezing, characteristic of its wave nature.","Finally, we jointly detect the dual aspects through the wave-particle correlator, showing an asymmetric regression of fluctuations to the steady state which depends on the quadrature amplitude being measured."],"url":"http://arxiv.org/abs/2402.09308v1","category":"quant-ph"}
{"created":"2024-02-14 16:21:47","title":"EcoVal: An Efficient Data Valuation Framework for Machine Learning","abstract":"Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \\textit{production function}, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models.","sentences":["Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives.","The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value.","In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner.","Instead of directly working with individual data sample, we determine the value of a cluster of similar data points.","This value is further propagated amongst all the member cluster points.","We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data.","This is enabled by formulating the performance of a model as a \\textit{production function}, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market.","We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance.","We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data.","This work addresses one of the core challenges of efficient data valuation at scale in machine learning models."],"url":"http://arxiv.org/abs/2402.09288v1","category":"cs.LG"}
{"created":"2024-02-14 16:18:56","title":"Smart Cities and Villages: Concept Review and Implementation Perspectives in Developing Cities","abstract":"The \"Smart City\" (SC) concept has been around for decades with deployment scenarios revealed in major cities of developed countries. However, while SC has enhanced the living conditions of city dwellers in the developed world, the concept is still either missing or poorly deployed in the developing world. This paper presents a review of the SC concept from the perspective of its application to cities in developing nations, the opportunities it avails, and challenges related to its applicability to these cities. Building upon a systematic review of literature, this paper shows that there are neither canonical definitions, models or frameworks of references for the SC concept. This paper also aims to bridge the gap between the \"smart city\" and \"smart village\" concepts, with the expectation of providing a holistic approach to solving common issues in cities around the world. Drawing inspiration from other authors, we propose a conceptual model for a SC initiative in Africa and demonstrate the need to prioritize research and capacity development. We also discuss the potential opportunities for such SC implementations in sub-Saharan Africa. As a case study, we consider the city of Lubumbashi in the Democratic Republic of Congo and discuss ways of making it a smart city by building around successful smart city initiatives. It is our belief that for Lubumbashi, as with any other city in Sub-Saharan Africa, the first step to developing a smart city is to build knowledge and create an intellectual capital.","sentences":["The \"Smart City\" (SC) concept has been around for decades with deployment scenarios revealed in major cities of developed countries.","However, while SC has enhanced the living conditions of city dwellers in the developed world, the concept is still either missing or poorly deployed in the developing world.","This paper presents a review of the SC concept from the perspective of its application to cities in developing nations, the opportunities it avails, and challenges related to its applicability to these cities.","Building upon a systematic review of literature, this paper shows that there are neither canonical definitions, models or frameworks of references for the SC concept.","This paper also aims to bridge the gap between the \"smart city\" and \"smart village\" concepts, with the expectation of providing a holistic approach to solving common issues in cities around the world.","Drawing inspiration from other authors, we propose a conceptual model for a SC initiative in Africa and demonstrate the need to prioritize research and capacity development.","We also discuss the potential opportunities for such SC implementations in sub-Saharan Africa.","As a case study, we consider the city of Lubumbashi in the Democratic Republic of Congo and discuss ways of making it a smart city by building around successful smart city initiatives.","It is our belief that for Lubumbashi, as with any other city in Sub-Saharan Africa, the first step to developing a smart city is to build knowledge and create an intellectual capital."],"url":"http://arxiv.org/abs/2402.09284v1","category":"cs.DC"}
{"created":"2024-02-14 14:35:57","title":"Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration","abstract":"Although deep neural networks yield high classification accuracy given sufficient training data, their predictions are typically overconfident or under-confident, i.e., the prediction confidences cannot truly reflect the accuracy. Post-hoc calibration tackles this problem by calibrating the prediction confidences without re-training the classification model. However, current approaches assume congruence between test and validation data distributions, limiting their applicability to out-of-distribution scenarios. To this end, we propose a novel meta-set-based cascaded temperature regression method for post-hoc calibration. Our method tailors fine-grained scaling functions to distinct test sets by simulating various domain shifts through data augmentation on the validation set. We partition each meta-set into subgroups based on predicted category and confidence level, capturing diverse uncertainties. A regression network is then trained to derive category-specific and confidence-level-specific scaling, achieving calibration across meta-sets. Extensive experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate the effectiveness of the proposed method.","sentences":["Although deep neural networks yield high classification accuracy given sufficient training data, their predictions are typically overconfident or under-confident, i.e., the prediction confidences cannot truly reflect the accuracy.","Post-hoc calibration tackles this problem by calibrating the prediction confidences without re-training the classification model.","However, current approaches assume congruence between test and validation data distributions, limiting their applicability to out-of-distribution scenarios.","To this end, we propose a novel meta-set-based cascaded temperature regression method for post-hoc calibration.","Our method tailors fine-grained scaling functions to distinct test sets by simulating various domain shifts through data augmentation on the validation set.","We partition each meta-set into subgroups based on predicted category and confidence level, capturing diverse uncertainties.","A regression network is then trained to derive category-specific and confidence-level-specific scaling, achieving calibration across meta-sets.","Extensive experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2402.09204v1","category":"cs.CV"}
{"created":"2024-02-14 14:18:14","title":"The Boosted Difference of Convex Functions Algorithm for Value-at-Risk Constrained Portfolio Optimization","abstract":"A highly relevant problem of modern finance is the design of Value-at-Risk (VaR) optimal portfolios. Due to contemporary financial regulations, banks and other financial institutions are tied to use the risk measure to control their credit, market and operational risks. For a portfolio with a discrete return distribution and finitely many scenarios, a Difference of Convex (DC) functions representation of the VaR can be derived. Wozabal (2012) showed that this yields a solution to a VaR constrained Markowitz style portfolio selection problem using the Difference of Convex Functions Algorithm (DCA). A recent algorithmic extension is the so-called Boosted Difference of Convex Functions Algorithm (BDCA) which accelerates the convergence due to an additional line search step. It has been shown that the BDCA converges linearly for solving non-smooth quadratic problems with linear inequality constraints. In this paper, we prove that the linear rate of convergence is also guaranteed for a piecewise linear objective function with linear equality and inequality constraints using the Kurdyka-{\\L}ojasiewicz property. An extended case study under consideration of best practices for comparing optimization algorithms demonstrates the superiority of the BDCA over the DCA for real-world financial market data. We are able to show that the results of the BDCA are significantly closer to the efficient frontier compared to the DCA. Due to the open availability of all data sets and code, this paper further provides a practical guide for transparent and easily reproducible comparisons of VaR constrained portfolio selection problems in Python.","sentences":["A highly relevant problem of modern finance is the design of Value-at-Risk (VaR) optimal portfolios.","Due to contemporary financial regulations, banks and other financial institutions are tied to use the risk measure to control their credit, market and operational risks.","For a portfolio with a discrete return distribution and finitely many scenarios, a Difference of Convex (DC) functions representation of the VaR can be derived.","Wozabal (2012) showed that this yields a solution to a VaR constrained Markowitz style portfolio selection problem using the Difference of Convex Functions Algorithm (DCA).","A recent algorithmic extension is the so-called Boosted Difference of Convex Functions Algorithm (BDCA) which accelerates the convergence due to an additional line search step.","It has been shown that the BDCA converges linearly for solving non-smooth quadratic problems with linear inequality constraints.","In this paper, we prove that the linear rate of convergence is also guaranteed for a piecewise linear objective function with linear equality and inequality constraints using the Kurdyka-{\\L}ojasiewicz property.","An extended case study under consideration of best practices for comparing optimization algorithms demonstrates the superiority of the BDCA over the DCA for real-world financial market data.","We are able to show that the results of the BDCA are significantly closer to the efficient frontier compared to the DCA.","Due to the open availability of all data sets and code, this paper further provides a practical guide for transparent and easily reproducible comparisons of VaR constrained portfolio selection problems in Python."],"url":"http://arxiv.org/abs/2402.09194v1","category":"math.OC"}
{"created":"2024-02-14 13:39:20","title":"Stabilizing Agreement is Impossible in Delayed Message Passing Models","abstract":"Most distributed computing research has focused on terminating problems like consensus and similar agreement problems. Non-terminating problems have been studied exhaustively in the context of self-stabilizing distributed algorithms, however, which may start from arbitrary initial states and can tolerate arbitrary transient faults. Somehow in-between is the stabilizing consensus problem, where the processes start from a well-defined initial state but do not need to decide irrevocably and need to agree on a common value only eventually. Charron-Bost and Moran studied stabilizing consensus in synchronous dynamic networks controlled by a message adversary. They introduced the simple and elegant class of min-max algorithms, which allow to solve stabilizing consensus under every message adversary that (i) allows at least one process to reach all other processes infinitely often, and (ii) does so within a bounded (but unknown) number of rounds. Moreover, the authors proved that (i) is a necessary condition. The question whether (i) is also sufficient, i.e., whether (ii) is also necessary, was left open. We answer this question by proving that stabilizing consensus is impossible if (ii) is dropped, i.e., even if some process reaches all other processes infinitely often but only within finite time. We accomplish this by introducing a novel class of arbitrarily delayed message adversaries, which also allows us to establish a connection between terminating task solvability under some message adversary to stabilizing task solvability under the corresponding arbitrarily delayed message adversary. Finally, we outline how to extend this relation to terminating task solvability in asynchronous message passing with guaranteed broadcasts, which highlights the asynchronous characteristics induced by arbitrary delays.","sentences":["Most distributed computing research has focused on terminating problems like consensus and similar agreement problems.","Non-terminating problems have been studied exhaustively in the context of self-stabilizing distributed algorithms, however, which may start from arbitrary initial states and can tolerate arbitrary transient faults.","Somehow in-between is the stabilizing consensus problem, where the processes start from a well-defined initial state but do not need to decide irrevocably and need to agree on a common value only eventually.","Charron-Bost and Moran studied stabilizing consensus in synchronous dynamic networks controlled by a message adversary.","They introduced the simple and elegant class of min-max algorithms, which allow to solve stabilizing consensus under every message adversary that (i) allows at least one process to reach all other processes infinitely often, and (ii) does so within a bounded (but unknown) number of rounds.","Moreover, the authors proved that (i) is a necessary condition.","The question whether (i) is also sufficient, i.e., whether (ii) is also necessary, was left open.","We answer this question by proving that stabilizing consensus is impossible if (ii) is dropped, i.e., even if some process reaches all other processes infinitely often but only within finite time.","We accomplish this by introducing a novel class of arbitrarily delayed message adversaries, which also allows us to establish a connection between terminating task solvability under some message adversary to stabilizing task solvability under the corresponding arbitrarily delayed message adversary.","Finally, we outline how to extend this relation to terminating task solvability in asynchronous message passing with guaranteed broadcasts, which highlights the asynchronous characteristics induced by arbitrary delays."],"url":"http://arxiv.org/abs/2402.09168v1","category":"cs.DC"}
{"created":"2024-02-14 13:32:23","title":"Deinterleaving of Discrete Renewal Process Mixtures with Application to Electronic Support Measures","abstract":"In this paper, we propose a new deinterleaving method for mixtures of discrete renewal Markov chains. This method relies on the maximization of a penalized likelihood score. It exploits all available information about both the sequence of the different symbols and their arrival times. A theoretical analysis is carried out to prove that minimizing this score allows to recover the true partition of symbols in the large sample limit, under mild conditions on the component processes. This theoretical analysis is then validated by experiments on synthetic data. Finally, the method is applied to deinterleave pulse trains received from different emitters in a RESM (Radar Electronic Support Measurements) context and we show that the proposed method competes favorably with state-of-the-art methods on simulated warfare datasets.","sentences":["In this paper, we propose a new deinterleaving method for mixtures of discrete renewal Markov chains.","This method relies on the maximization of a penalized likelihood score.","It exploits all available information about both the sequence of the different symbols and their arrival times.","A theoretical analysis is carried out to prove that minimizing this score allows to recover the true partition of symbols in the large sample limit, under mild conditions on the component processes.","This theoretical analysis is then validated by experiments on synthetic data.","Finally, the method is applied to deinterleave pulse trains received from different emitters in a RESM (Radar Electronic Support Measurements) context and we show that the proposed method competes favorably with state-of-the-art methods on simulated warfare datasets."],"url":"http://arxiv.org/abs/2402.09166v1","category":"cs.LG"}
{"created":"2024-02-14 13:20:24","title":"Wireless Crowd Detection for Smart Overtourism Mitigation","abstract":"Overtourism occurs when the number of tourists exceeds the carrying capacity of a destination, leading to negative impacts on the environment, culture, and quality of life for residents. By monitoring overtourism, destination managers can identify areas of concern and implement measures to mitigate the negative impacts of tourism while promoting smarter tourism practices. This can help ensure that tourism benefits both visitors and residents while preserving the natural and cultural resources that make these destinations so appealing.   This chapter describes a low-cost approach to monitoring overtourism based on mobile devices' wireless activity. A flexible architecture was designed for a smart tourism toolkit to be used by Small and Medium-sized Enterprises (SMEs) in crowding management solutions, to build better tourism services, improve efficiency and sustainability, and reduce the overwhelming feeling of pressure in critical hotspots.   The crowding sensors count the number of surrounding mobile devices, by detecting trace elements of wireless technologies, mitigating the effect of MAC address randomization. They run detection programs for several technologies, and fingerprinting analysis results are only stored locally in an anonymized database, without infringing privacy rights. After that edge computing, sensors communicate the crowding information to a cloud server, by using a variety of uplink techniques to mitigate local connectivity limitations, something that has been often disregarded in alternative approaches.   Field validation of sensors has been performed on Iscte's campus. Preliminary results show that these sensors can be deployed in multiple scenarios and provide a diversity of spatio-temporal crowding data that can scaffold tourism overcrowding management strategies.","sentences":["Overtourism occurs when the number of tourists exceeds the carrying capacity of a destination, leading to negative impacts on the environment, culture, and quality of life for residents.","By monitoring overtourism, destination managers can identify areas of concern and implement measures to mitigate the negative impacts of tourism while promoting smarter tourism practices.","This can help ensure that tourism benefits both visitors and residents while preserving the natural and cultural resources that make these destinations so appealing.   ","This chapter describes a low-cost approach to monitoring overtourism based on mobile devices' wireless activity.","A flexible architecture was designed for a smart tourism toolkit to be used by Small and Medium-sized Enterprises (SMEs) in crowding management solutions, to build better tourism services, improve efficiency and sustainability, and reduce the overwhelming feeling of pressure in critical hotspots.   ","The crowding sensors count the number of surrounding mobile devices, by detecting trace elements of wireless technologies, mitigating the effect of MAC address randomization.","They run detection programs for several technologies, and fingerprinting analysis results are only stored locally in an anonymized database, without infringing privacy rights.","After that edge computing, sensors communicate the crowding information to a cloud server, by using a variety of uplink techniques to mitigate local connectivity limitations, something that has been often disregarded in alternative approaches.   ","Field validation of sensors has been performed on Iscte's campus.","Preliminary results show that these sensors can be deployed in multiple scenarios and provide a diversity of spatio-temporal crowding data that can scaffold tourism overcrowding management strategies."],"url":"http://arxiv.org/abs/2402.09158v1","category":"cs.CY"}
{"created":"2024-02-14 13:13:26","title":"Attacking Large Language Models with Projected Gradient Descent","abstract":"Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.","sentences":["Current LLM alignment methods are readily broken through specifically crafted adversarial prompts.","While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls.","This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training.","To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt.","Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy.","Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results."],"url":"http://arxiv.org/abs/2402.09154v1","category":"cs.LG"}
{"created":"2024-02-14 13:08:26","title":"Improved Regret for Bandit Convex Optimization with Delayed Feedback","abstract":"We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm. In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism. Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\\sqrt{dT})$ for convex functions. Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consider the case with strongly convex functions, and prove that the proposed algorithm can enjoy a better regret bound of $O(T^{2/3}\\log^{1/3}T+d\\log T)$. Finally, we show that in a special case with unconstrained action sets, it can be simply extended to achieve a regret bound of $O(\\sqrt{T\\log T}+d\\log T)$ for strongly convex and smooth functions.","sentences":["We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay.","Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm.","In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism.","Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\\sqrt{dT})$ for convex functions.","Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consider the case with strongly convex functions, and prove that the proposed algorithm can enjoy a better regret bound of $O(T^{2/3}\\log^{1/3}T+d\\log T)$. Finally, we show that in a special case with unconstrained action sets, it can be simply extended to achieve a regret bound of $O(\\sqrt{T\\log T}+d\\log T)$ for strongly convex and smooth functions."],"url":"http://arxiv.org/abs/2402.09152v1","category":"cs.LG"}
{"created":"2024-02-14 12:22:50","title":"Finding Densest Subgraphs with Edge-Color Constraints","abstract":"We consider a variant of the densest subgraph problem in networks with single or multiple edge attributes. For example, in a social network, the edge attributes may describe the type of relationship between users, such as friends, family, or acquaintances, or different types of communication. For conceptual simplicity, we view the attributes as edge colors. The new problem we address is to find a diverse densest subgraph that fulfills given requirements on the numbers of edges of specific colors. When searching for a dense social network community, our problem will enforce the requirement that the community is diverse according to criteria specified by the edge attributes. We show that the decision versions for finding exactly, at most, and at least $\\textbf{h}$ colored edges densest subgraph, where $\\textbf{h}$ is a vector of color requirements, are NP-complete, for already two colors. For the problem of finding a densest subgraph with at least $\\textbf{h}$ colored edges, we provide a linear-time constant-factor approximation algorithm when the input graph is sparse. On the way, we introduce the related at least $h$ (non-colored) edges densest subgraph problem, show its hardness, and also provide a linear-time constant-factor approximation. In our experiments, we demonstrate the efficacy and efficiency of our new algorithms.","sentences":["We consider a variant of the densest subgraph problem in networks with single or multiple edge attributes.","For example, in a social network, the edge attributes may describe the type of relationship between users, such as friends, family, or acquaintances, or different types of communication.","For conceptual simplicity, we view the attributes as edge colors.","The new problem we address is to find a diverse densest subgraph that fulfills given requirements on the numbers of edges of specific colors.","When searching for a dense social network community, our problem will enforce the requirement that the community is diverse according to criteria specified by the edge attributes.","We show that the decision versions for finding exactly, at most, and at least $\\textbf{h}$ colored edges densest subgraph, where $\\textbf{h}$ is a vector of color requirements, are NP-complete, for already two colors.","For the problem of finding a densest subgraph with at least $\\textbf{h}$ colored edges, we provide a linear-time constant-factor approximation algorithm when the input graph is sparse.","On the way, we introduce the related at least $h$ (non-colored) edges densest subgraph problem, show its hardness, and also provide a linear-time constant-factor approximation.","In our experiments, we demonstrate the efficacy and efficiency of our new algorithms."],"url":"http://arxiv.org/abs/2402.09124v1","category":"cs.SI"}
{"created":"2024-02-14 11:10:21","title":"Differential Sensitivity of the KM3NeT/ARCA detector to a diffuse neutrino flux and to point-like source emission: exploring the case of the Starburst Galaxies","abstract":"KM3NeT/ARCA is a Cherenkov neutrino telescope under construction in the Mediterranean sea, optimised for the detection of astrophysical neutrinos with energies above $\\sim$1~TeV. In this work, using Monte Carlo simulations including all-flavour neutrinos, the integrated and differential sensitivities for KM3NeT/ARCA are presented considering the case of a diffuse neutrino flux as well as extended and point-like neutrino sources. This analysis is applied to Starburst Galaxies demonstrating that the detector has the capability of tracing TeV neutrinos from these sources. Remarkably, after eight years, a hard power-law spectrum from the nearby Small Magellanic Cloud can be constrained. The sensitivity and discovery potential for NGC 1068 is also evaluated showing that KM3NeT/ARCA will discriminate between different astrophysical components of the measured neutrino flux after 3 years of data taking.","sentences":["KM3NeT/ARCA is a Cherenkov neutrino telescope under construction in the Mediterranean sea, optimised for the detection of astrophysical neutrinos with energies above $\\sim$1~TeV.","In this work, using Monte Carlo simulations including all-flavour neutrinos, the integrated and differential sensitivities for KM3NeT/ARCA are presented considering the case of a diffuse neutrino flux as well as extended and point-like neutrino sources.","This analysis is applied to Starburst Galaxies demonstrating that the detector has the capability of tracing TeV neutrinos from these sources.","Remarkably, after eight years, a hard power-law spectrum from the nearby Small Magellanic Cloud can be constrained.","The sensitivity and discovery potential for NGC 1068 is also evaluated showing that KM3NeT/ARCA will discriminate between different astrophysical components of the measured neutrino flux after 3 years of data taking."],"url":"http://arxiv.org/abs/2402.09088v1","category":"astro-ph.HE"}
{"created":"2024-02-14 11:07:36","title":"Impact of Non-Informative Censoring on Propensity Score Based Estimation of Marginal Hazard Ratios","abstract":"In medical and epidemiological studies, one of the most common settings is studying the effect of a treatment on a time-to-event outcome, where the time-to-event might be censored before end of study. A common parameter of interest in such a setting is the marginal hazard ratio (MHR). When a study is based on observational data, propensity score (PS) based methods are often used, in an attempt to make the treatment groups comparable despite having a non-randomized treatment. Previous studies have shown censoring to be a factor that induces bias when using PS based estimators. In this paper we study the magnitude of the bias under different rates of non-informative censoring when estimating MHR using PS weighting or PS matching. A bias correction involving the probability of event is suggested and compared to conventional PS based methods.","sentences":["In medical and epidemiological studies, one of the most common settings is studying the effect of a treatment on a time-to-event outcome, where the time-to-event might be censored before end of study.","A common parameter of interest in such a setting is the marginal hazard ratio (MHR).","When a study is based on observational data, propensity score (PS) based methods are often used, in an attempt to make the treatment groups comparable despite having a non-randomized treatment.","Previous studies have shown censoring to be a factor that induces bias when using PS based estimators.","In this paper we study the magnitude of the bias under different rates of non-informative censoring when estimating MHR using PS weighting or PS matching.","A bias correction involving the probability of event is suggested and compared to conventional PS based methods."],"url":"http://arxiv.org/abs/2402.09086v1","category":"stat.ME"}
{"created":"2024-02-14 10:48:00","title":"Low-Rank Extragradient Methods for Scalable Semidefinite Optimization","abstract":"We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains. We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition. We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in worst-case. Our approach is supported by numerical experiments conducted with a dataset of Max-Cut instances.","sentences":["We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains.","We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition.","We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in worst-case.","Our approach is supported by numerical experiments conducted with a dataset of Max-Cut instances."],"url":"http://arxiv.org/abs/2402.09081v1","category":"math.OC"}
{"created":"2024-02-14 10:40:09","title":"DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform","abstract":"In this paper, we propose a graph neural network, DisGNet, for learning the graph distance matrix to address the forward kinematics problem of the Gough-Stewart platform. DisGNet employs the k-FWL algorithm for message-passing, providing high expressiveness with a small parameter count, making it suitable for practical deployment. Additionally, we introduce the GPU-friendly Newton-Raphson method, an efficient parallelized optimization method executed on the GPU to refine DisGNet's output poses, achieving ultra-high-precision pose. This novel two-stage approach delivers ultra-high precision output while meeting real-time requirements. Our results indicate that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at 79.8\\% and 98.2\\%, respectively. As executed on a GPU, our two-stage method can ensure the requirement for real-time computation. Codes are released at https://github.com/FLAMEZZ5201/DisGNet.","sentences":["In this paper, we propose a graph neural network, DisGNet, for learning the graph distance matrix to address the forward kinematics problem of the Gough-Stewart platform.","DisGNet employs the k-FWL algorithm for message-passing, providing high expressiveness with a small parameter count, making it suitable for practical deployment.","Additionally, we introduce the GPU-friendly Newton-Raphson method, an efficient parallelized optimization method executed on the GPU to refine DisGNet's output poses, achieving ultra-high-precision pose.","This novel two-stage approach delivers ultra-high precision output while meeting real-time requirements.","Our results indicate that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at 79.8\\% and 98.2\\%, respectively.","As executed on a GPU, our two-stage method can ensure the requirement for real-time computation.","Codes are released at https://github.com/FLAMEZZ5201/DisGNet."],"url":"http://arxiv.org/abs/2402.09077v1","category":"cs.RO"}
{"created":"2024-02-14 09:36:07","title":"Closure properties and heavy tails: random vectors in the presence of dependence","abstract":"This paper is organized in three parts closely related to closure properties of heavy-tailed distributions and heavy-tailed random vectors. In the first part we consider two random variables X and Y with distributions F and G respectively. We assume that these random variables satisfy one type of a weak dependence structure. Under some mild conditions, we examine whether their product convolution distribution H belongs in the same distribution class of the distribution F. Namely we establish the closure property with respect to the product convolution, under this specific weak dependence structure, in the classes ERV, C, D, M, OS, OL, PD and K. Further in the second part we introduce a new distribution class, which satisfies some closure properties such as convolution, product convolution and mixture. Although the multivariate regular variation is well-established distribution, it does not happen in other heavy tailed random vectors. Therefore in the third part we introduced the class of dominatedly varying vectors and positively decreasing random vectors and we study the closure property of the scalar product under independent and dependent cases. Furthermore we study the closure property of the first class under convolution and mixture, and we study the distribution of stopped sums where the summands are random vectors which belongs to this class. Some of these results holds and for positively decreasing random vectors.","sentences":["This paper is organized in three parts closely related to closure properties of heavy-tailed distributions and heavy-tailed random vectors.","In the first part we consider two random variables X and Y with distributions F and G respectively.","We assume that these random variables satisfy one type of a weak dependence structure.","Under some mild conditions, we examine whether their product convolution distribution H belongs in the same distribution class of the distribution F. Namely we establish the closure property with respect to the product convolution, under this specific weak dependence structure, in the classes ERV, C, D, M, OS, OL, PD and K. Further in the second part we introduce a new distribution class, which satisfies some closure properties such as convolution, product convolution and mixture.","Although the multivariate regular variation is well-established distribution, it does not happen in other heavy tailed random vectors.","Therefore in the third part we introduced the class of dominatedly varying vectors and positively decreasing random vectors and we study the closure property of the scalar product under independent and dependent cases.","Furthermore we study the closure property of the first class under convolution and mixture, and we study the distribution of stopped sums where the summands are random vectors which belongs to this class.","Some of these results holds and for positively decreasing random vectors."],"url":"http://arxiv.org/abs/2402.09041v1","category":"math.PR"}
{"created":"2024-02-14 09:35:20","title":"Morse index stability for Yang-Mills connections","abstract":"We prove stability results of the Morse index plus nullity of Yang-Mills connections in dimension 4 under weak convergence. Precisely we establish that the sum of the Morse indices and the nullity of a bounded sequence of Yang-Mills connections is asymptotically bounded above by the sum of the Morse index and the nullity of the weak limit and the bubbles while the Morse indices are asymptotically bounded below by the sum of the Morse index of the weak limit and the bubbles.","sentences":["We prove stability results of the Morse index plus nullity of Yang-Mills connections in dimension 4 under weak convergence.","Precisely we establish that the sum of the Morse indices and the nullity of a bounded sequence of Yang-Mills connections is asymptotically bounded above by the sum of the Morse index and the nullity of the weak limit and the bubbles while the Morse indices are asymptotically bounded below by the sum of the Morse index of the weak limit and the bubbles."],"url":"http://arxiv.org/abs/2402.09039v1","category":"math.DG"}
{"created":"2024-02-14 08:51:53","title":"Bayesian reliability acceptance sampling plan with optional warranty under hybrid censoring","abstract":"This work considers design of Bayesian reliability acceptance sampling plan (RASP) under hybrid censored life test for the products sold under optional warranty. The consumer and manufacturer agree on a common lifetime distribution of the product. However, they differ in the assessment of the prior distributions because of the adversarial nature of the consumer and manufacturer. The consumer takes decision based on his/her utility and prior belief without warranty offer by the manufacturer. If the decision is rejection, manufacturer provides warranty offer to the consumer. If the consumer rejects the lot with a warranty, the manufacturer conducts life test under hybrid censoring scheme (HCS) and provide lifetime information to the consumer. The consumer updates his/her belief based on lifetime information provided by the manufacturer. The consumer then takes decision of acceptance or rejection of lot based on updated belief. Task of the manufacturer is to determine the optimal life testing plan.","sentences":["This work considers design of Bayesian reliability acceptance sampling plan (RASP) under hybrid censored life test for the products sold under optional warranty.","The consumer and manufacturer agree on a common lifetime distribution of the product.","However, they differ in the assessment of the prior distributions because of the adversarial nature of the consumer and manufacturer.","The consumer takes decision based on his/her utility and prior belief without warranty offer by the manufacturer.","If the decision is rejection, manufacturer provides warranty offer to the consumer.","If the consumer rejects the lot with a warranty, the manufacturer conducts life test under hybrid censoring scheme (HCS) and provide lifetime information to the consumer.","The consumer updates his/her belief based on lifetime information provided by the manufacturer.","The consumer then takes decision of acceptance or rejection of lot based on updated belief.","Task of the manufacturer is to determine the optimal life testing plan."],"url":"http://arxiv.org/abs/2402.09020v1","category":"stat.AP"}
{"created":"2024-02-14 08:36:52","title":"Improved Deterministic Distributed Maximum Weight Independent Set Approximation in Sparse Graphs","abstract":"We design new deterministic CONGEST approximation algorithms for \\emph{maximum weight independent set (MWIS)} in \\emph{sparse graphs}. As our main results, we obtain new $\\Delta(1+\\epsilon)$-approximation algorithms as well as algorithms whose approximation ratio depend strictly on $\\alpha$, in graphs with maximum degree $\\Delta$ and arboricity $\\alpha$. For (deterministic) $\\Delta(1+\\epsilon)$-approximation, the current state-of-the-art is due to a recent breakthrough by Faour et al.\\ [SODA 2023] that showed an $O(\\log^{2} (\\Delta W)\\cdot \\log (1/\\epsilon)+\\log ^{*}n)$-round algorithm, where $W$ is the largest node-weight (this bound translates to $O(\\log^{2} n\\cdot\\log (1/\\epsilon))$ under the common assumption that $W=\\text{poly}(n)$). As for $\\alpha$-dependent approximations, a deterministic CONGEST $(8(1+\\epsilon)\\cdot\\alpha)$-approximation algorithm with runtime $O(\\log^{3} n\\cdot\\log (1/\\epsilon))$ can be derived by combining the aforementioned algorithm of Faour et al.\\ with a method presented by Kawarabayashi et al.\\ [DISC 2020].","sentences":["We design new deterministic CONGEST approximation algorithms for \\emph{maximum weight independent set (MWIS)} in \\emph{sparse graphs}.","As our main results, we obtain new $\\Delta(1+\\epsilon)$-approximation algorithms as well as algorithms whose approximation ratio depend strictly on $\\alpha$, in graphs with maximum degree $\\Delta$ and arboricity $\\alpha$. For (deterministic) $\\Delta(1+\\epsilon)$-approximation, the current state-of-the-art is due to a recent breakthrough by Faour et al.\\","[SODA 2023] that showed an $O(\\log^{2} (\\Delta W)\\cdot \\log (1/\\epsilon)+\\log ^{*}n)$-round algorithm, where $W$ is the largest node-weight (this bound translates to $O(\\log^{2} n\\cdot\\log (1/\\epsilon))$ under the common assumption that $W=\\text{poly}(n)$).","As for $\\alpha$-dependent approximations, a deterministic CONGEST $(8(1+\\epsilon)\\cdot\\alpha)$-approximation algorithm with runtime $O(\\log^{3} n\\cdot\\log (1/\\epsilon))$ can be derived by combining the aforementioned algorithm of Faour et al.\\ with a method presented by Kawarabayashi et al.\\","[DISC 2020]."],"url":"http://arxiv.org/abs/2402.09011v1","category":"cs.DS"}
{"created":"2024-02-14 08:17:21","title":"Gradient Alignment with Prototype Feature for Fully Test-time Adaptation","abstract":"In context of Test-time Adaptation(TTA), we propose a regularizer, dubbed Gradient Alignment with Prototype feature (GAP), which alleviates the inappropriate guidance from entropy minimization loss from misclassified pseudo label. We developed a gradient alignment loss to precisely manage the adaptation process, ensuring that changes made for some data don't negatively impact the model's performance on other data. We introduce a prototype feature of a class as a proxy measure of the negative impact. To make GAP regularizer feasible under the TTA constraints, where model can only access test data without labels, we tailored its formula in two ways: approximating prototype features with weight vectors of the classifier, calculating gradient without back-propagation. We demonstrate GAP significantly improves TTA methods across various datasets, which proves its versatility and effectiveness.","sentences":["In context of Test-time Adaptation(TTA), we propose a regularizer, dubbed Gradient Alignment with Prototype feature (GAP), which alleviates the inappropriate guidance from entropy minimization loss from misclassified pseudo label.","We developed a gradient alignment loss to precisely manage the adaptation process, ensuring that changes made for some data don't negatively impact the model's performance on other data.","We introduce a prototype feature of a class as a proxy measure of the negative impact.","To make GAP regularizer feasible under the TTA constraints, where model can only access test data without labels, we tailored its formula in two ways: approximating prototype features with weight vectors of the classifier, calculating gradient without back-propagation.","We demonstrate GAP significantly improves TTA methods across various datasets, which proves its versatility and effectiveness."],"url":"http://arxiv.org/abs/2402.09004v1","category":"cs.CV"}
{"created":"2024-02-14 08:15:44","title":"High-level moving excursions for spatiotemporal Gaussian random fields with long range dependence","abstract":"This paper analyzes the limit distribution of spatiotemporal sojourn measures of Gaussian subordinated random fields. The family of Gaussian spatiotemporal random fields studied displays long--range dependence. The approach presented is based on Hermite expansion, and reduction theorems. Two new ingredients are incorporated to this analysis. Specifically, on the one hand, the limiting distribution is computed under increasing domain asymptotics in space and/or time, and, on the other hand, the involved non--linear transformation in the Gaussian subordination is time dependent. In particular, a central limit theorem is obtained for first Minkowski functional providing the spatiotemporal volume of excursion sets of spatiotemporal Gaussian random fields under moving levels. These results are also applied to the case where subordination to a spherical Gaussian random field, defined by restriction to the sphere, is performed.","sentences":["This paper analyzes the limit distribution of spatiotemporal sojourn measures of Gaussian subordinated random fields.","The family of Gaussian spatiotemporal random fields studied displays long--range dependence.","The approach presented is based on Hermite expansion, and reduction theorems.","Two new ingredients are incorporated to this analysis.","Specifically, on the one hand, the limiting distribution is computed under increasing domain asymptotics in space and/or time, and, on the other hand, the involved non--linear transformation in the Gaussian subordination is time dependent.","In particular, a central limit theorem is obtained for first Minkowski functional providing the spatiotemporal volume of excursion sets of spatiotemporal Gaussian random fields under moving levels.","These results are also applied to the case where subordination to a spherical Gaussian random field, defined by restriction to the sphere, is performed."],"url":"http://arxiv.org/abs/2402.09003v1","category":"math.PR"}
{"created":"2024-02-14 07:27:30","title":"Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption","abstract":"This study tackles the challenges of adversarial corruption in model-based reinforcement learning (RL), where the transition dynamics can be corrupted by an adversary. Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation. However, these techniques cannot be directly applied to model-based RL. In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model. Our work encompasses both online and offline settings. In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of $\\tilde{\\mathcal{O}}(\\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes. We also prove a lower bound to show that the additive dependence on $C$ is optimal. We extend our weighting technique to the offline setting, and propose an algorithm named corruption-robust pessimistic MLE (CR-PMLE). Under a uniform coverage condition, CR-PMLE exhibits suboptimality worsened by $\\mathcal{O}(C/n)$, nearly matching the lower bound. To the best of our knowledge, this is the first work on corruption-robust model-based RL algorithms with provable guarantees.","sentences":["This study tackles the challenges of adversarial corruption in model-based reinforcement learning (RL), where the transition dynamics can be corrupted by an adversary.","Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation.","However, these techniques cannot be directly applied to model-based RL.","In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model.","Our work encompasses both online and offline settings.","In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE.","We prove that CR-OMLE achieves a regret of $\\tilde{\\mathcal{O}}(\\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes.","We also prove a lower bound to show that the additive dependence on $C$ is optimal.","We extend our weighting technique to the offline setting, and propose an algorithm named corruption-robust pessimistic MLE (CR-PMLE).","Under a uniform coverage condition, CR-PMLE exhibits suboptimality worsened by $\\mathcal{O}(C/n)$, nearly matching the lower bound.","To the best of our knowledge, this is the first work on corruption-robust model-based RL algorithms with provable guarantees."],"url":"http://arxiv.org/abs/2402.08991v1","category":"stat.ML"}
{"created":"2024-02-14 06:37:04","title":"Convergence rate and exponential stability of backward Euler method for neutral stochastic delay differential equations under generalized monotonicity conditions","abstract":"This work focuses on the numerical approximations of neutral stochastic delay differential equations with their drift and diffusion coefficients growing super-linearly with respect to both delay variables and state variables. Under generalized monotonicity conditions, we prove that the backward Euler method not only converges strongly in the mean square sense with order $1/2$, but also inherit the mean square exponential stability of the original equations. As a byproduct, we obtain the same results on convergence rate and exponential stability of the backward Euler method for stochastic delay differential equations with generalized monotonicity conditions. These theoretical results are finally supported by several numerical experiments.","sentences":["This work focuses on the numerical approximations of neutral stochastic delay differential equations with their drift and diffusion coefficients growing super-linearly with respect to both delay variables and state variables.","Under generalized monotonicity conditions, we prove that the backward Euler method not only converges strongly in the mean square sense with order $1/2$, but also inherit the mean square exponential stability of the original equations.","As a byproduct, we obtain the same results on convergence rate and exponential stability of the backward Euler method for stochastic delay differential equations with generalized monotonicity conditions.","These theoretical results are finally supported by several numerical experiments."],"url":"http://arxiv.org/abs/2402.08973v1","category":"math.NA"}
{"created":"2024-02-14 06:33:22","title":"Structured Language Generation Model for Robust Structure Prediction","abstract":"We propose Structured Language Generation Model (SLGM), a mixture of new loss function and inference method for better generalization of structured outputs. Previous studies on structure prediction (e.g. NER, RE) make use of explicit dataset information, which would boost performance, yet it might pose challenges to robust generalization in real-world situations. Instead, our model gives generalized format information about data indirectly. With format information, we could reduce sequence-to-sequence problem into classification problem via loss calibration and formatted decoding. Our experimental results showed SLGM successfully maintain performance without dataset information, and showed much less format errors. We also showed our model can work like adapters on individual dataset, with no additional training.","sentences":["We propose Structured Language Generation Model (SLGM), a mixture of new loss function and inference method for better generalization of structured outputs.","Previous studies on structure prediction (e.g. NER, RE) make use of explicit dataset information, which would boost performance, yet it might pose challenges to robust generalization in real-world situations.","Instead, our model gives generalized format information about data indirectly.","With format information, we could reduce sequence-to-sequence problem into classification problem via loss calibration and formatted decoding.","Our experimental results showed SLGM successfully maintain performance without dataset information, and showed much less format errors.","We also showed our model can work like adapters on individual dataset, with no additional training."],"url":"http://arxiv.org/abs/2402.08971v1","category":"cs.CL"}
{"created":"2024-02-14 05:47:42","title":"Entropy Jump and Entropic Central Limit Theorem for Independent Sum","abstract":"It is a manuscript for results about entropic central limit theorem for independent sum under finite Poincar\\'e constant conditions.","sentences":["It is a manuscript for results about entropic central limit theorem for independent sum under finite Poincar\\'e constant conditions."],"url":"http://arxiv.org/abs/2402.08953v1","category":"math.PR"}
{"created":"2024-02-14 05:22:53","title":"Measuring Sharpness in Grokking","abstract":"Neural networks sometimes exhibit grokking, a phenomenon where perfect or near-perfect performance is achieved on a validation set well after the same performance has been obtained on the corresponding training set. In this workshop paper, we introduce a robust technique for measuring grokking, based on fitting an appropriate functional form. We then use this to investigate the sharpness of transitions in training and validation accuracy under two settings. The first setting is the theoretical framework developed by Levi et al. (2023) where closed form expressions are readily accessible. The second setting is a two-layer MLP trained to predict the parity of bits, with grokking induced by the concealment strategy of Miller et al. (2023). We find that trends between relative grokking gap and grokking sharpness are similar in both settings when using absolute and relative measures of sharpness. Reflecting on this, we make progress toward explaining some trends and identify the need for further study to untangle the various mechanisms which influence the sharpness of grokking.","sentences":["Neural networks sometimes exhibit grokking, a phenomenon where perfect or near-perfect performance is achieved on a validation set well after the same performance has been obtained on the corresponding training set.","In this workshop paper, we introduce a robust technique for measuring grokking, based on fitting an appropriate functional form.","We then use this to investigate the sharpness of transitions in training and validation accuracy under two settings.","The first setting is the theoretical framework developed by Levi et al.","(2023) where closed form expressions are readily accessible.","The second setting is a two-layer MLP trained to predict the parity of bits, with grokking induced by the concealment strategy of Miller et al.","(2023).","We find that trends between relative grokking gap and grokking sharpness are similar in both settings when using absolute and relative measures of sharpness.","Reflecting on this, we make progress toward explaining some trends and identify the need for further study to untangle the various mechanisms which influence the sharpness of grokking."],"url":"http://arxiv.org/abs/2402.08946v1","category":"cs.LG"}
{"created":"2024-02-14 05:12:27","title":"The Racah algebra of rank 2: Properties, symmetries and representation","abstract":"This paper is threefold. First, we provide a new \"universal\" definition for the Racah algebra of rank-2 as an extension of the rank-1 Racah algebra where the generators are indexed by subsets and any three disjoint indexing sets define a subalgebra isomorphic to the rank-1 case. With this definition, we explore some of the properties of the algebra including verifying that these natural assumptions are equivalent to other defining relations in the literature. Second, we look at the symmetries of the generators of the rank-2 Racah algebra. Those symmetries allows us to partially make abstraction of the choice of the generators and write relations and properties in a different format. Last, we provide a novel representation of the Racah algebra. This new representation requires only one generator to be diagonal and is based on an expansion of the split basis representation from the rank-1 Racah algebra.","sentences":["This paper is threefold.","First, we provide a new \"universal\" definition for the Racah algebra of rank-2 as an extension of the rank-1 Racah algebra where the generators are indexed by subsets and any three disjoint indexing sets define a subalgebra isomorphic to the rank-1 case.","With this definition, we explore some of the properties of the algebra including verifying that these natural assumptions are equivalent to other defining relations in the literature.","Second, we look at the symmetries of the generators of the rank-2 Racah algebra.","Those symmetries allows us to partially make abstraction of the choice of the generators and write relations and properties in a different format.","Last, we provide a novel representation of the Racah algebra.","This new representation requires only one generator to be diagonal and is based on an expansion of the split basis representation from the rank-1 Racah algebra."],"url":"http://arxiv.org/abs/2402.08944v1","category":"math-ph"}
{"created":"2024-02-14 05:04:26","title":"Structure and magnetic properties of a La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\u03b4}$ single crystal","abstract":"We have successfully grown large and good-quality single crystals of the La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\\delta}$ compound using the floating-zone method with laser diodes. We investigated the crystal quality, crystallography, chemical composition, magnetic properties and the oxidation state of Cr in the grown single crystals by employing a combination of techniques, including X-ray Laue and powder diffraction, scanning electron microscopy, magnetization measurements, X-ray photoelectron spectroscopy and light absorption. The La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\\delta}$ single crystal exhibits a single-phase composition, crystallizing in a trigonal structure with the space group $R\\bar{3}c$ at room temperature. The chemical composition was determined as La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\\delta}$, indicating a significant chromium deficiency. Upon warming, we observed five distinctive characteristic temperatures, namely $T_1 =$ 21.50(1) K, $T_2 =$ 34.98(1) K, $T_3 =$ 117.94(1) K, $T_4 =$ 155.01(1) K, and $T_{\\textrm{N}} =$ 271.80(1) K, revealing five distinct magnetic anomalies. Our magnetization study allows us to explore the nature of these anomalies. Remarkably, the oxidation state of chromium in the single-crystal La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\\delta}$, characterized by a band gap of 1.630(8) eV, is exclusively attributed to Cr$^{3+}$ ions, making a departure from the findings of previous studies on polycrystalline materials.","sentences":["We have successfully grown large and good-quality single crystals of the La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\\delta}$ compound using the floating-zone method with laser diodes.","We investigated the crystal quality, crystallography, chemical composition, magnetic properties and the oxidation state of Cr in the grown single crystals by employing a combination of techniques, including X-ray Laue and powder diffraction, scanning electron microscopy, magnetization measurements, X-ray photoelectron spectroscopy and light absorption.","The La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\\delta}$ single crystal exhibits a single-phase composition, crystallizing in a trigonal structure with the space group $R\\bar{3}c$ at room temperature.","The chemical composition was determined as La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\\delta}$, indicating a significant chromium deficiency.","Upon warming, we observed five distinctive characteristic temperatures, namely $T_1 =$ 21.50(1) K, $T_2 =$ 34.98(1) K, $T_3 =$ 117.94(1) K, $T_4 =$ 155.01(1) K, and $T_{\\textrm{N}} =$ 271.80(1) K, revealing five distinct magnetic anomalies.","Our magnetization study allows us to explore the nature of these anomalies.","Remarkably, the oxidation state of chromium in the single-crystal La$_{0.75}$Sr$_{0.25}$Cr$_{0.90}$O$_{3-\\delta}$, characterized by a band gap of 1.630(8) eV, is exclusively attributed to Cr$^{3+}$ ions, making a departure from the findings of previous studies on polycrystalline materials."],"url":"http://arxiv.org/abs/2402.08940v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-14 04:05:51","title":"Subwavelength Photorefractive Grating in a Thin-Film Lithium Niobate Microcavity","abstract":"Subwavelength gratings play a fundamental and pivotal role in numerous science and applications for wave manipulation, exhibiting distinctive features such as filtering, phase manipulation, and anti-reflection. However, conventional fabrication methods for ultrasmall periodic structures are constrained by the fundamental optical diffraction limit, making it challenging to produce subwavelength gratings for optics. Here, we demonstrate a novel technique to build a reconfigurable subwavelength photorefractive grating (SPG) in a thin-film lithium niobate on the platform of an optical microcavity. Such SPGs are optically induced through the photorefractive effect and the subwavelength features originate from the spatial phase modulations of the pump's standing wave. The resulting SPGs lead to the mode splitting of two counter-propagating modes inside the microcavity, exhibiting an Electromagnetically Induced Transparency (EIT)-like transmission spectrum. Moreover, the unique subwavelength characteristic of SPGs enables first-order quasi-phase-matching for backward second-harmonic generation, a long-standing problem in nonlinear optics. Also, free-space-to-chip vertical nonlinear frequency conversion can be achieved in a similar manner. These results provide a flexible approach towards fabricating subwavelength gratings, which holds significant potential in various applications such as nonlinear frequency conversion, optical communication, sensing, and quantum technologies.","sentences":["Subwavelength gratings play a fundamental and pivotal role in numerous science and applications for wave manipulation, exhibiting distinctive features such as filtering, phase manipulation, and anti-reflection.","However, conventional fabrication methods for ultrasmall periodic structures are constrained by the fundamental optical diffraction limit, making it challenging to produce subwavelength gratings for optics.","Here, we demonstrate a novel technique to build a reconfigurable subwavelength photorefractive grating (SPG) in a thin-film lithium niobate on the platform of an optical microcavity.","Such SPGs are optically induced through the photorefractive effect and the subwavelength features originate from the spatial phase modulations of the pump's standing wave.","The resulting SPGs lead to the mode splitting of two counter-propagating modes inside the microcavity, exhibiting an Electromagnetically Induced Transparency (EIT)-like transmission spectrum.","Moreover, the unique subwavelength characteristic of SPGs enables first-order quasi-phase-matching for backward second-harmonic generation, a long-standing problem in nonlinear optics.","Also, free-space-to-chip vertical nonlinear frequency conversion can be achieved in a similar manner.","These results provide a flexible approach towards fabricating subwavelength gratings, which holds significant potential in various applications such as nonlinear frequency conversion, optical communication, sensing, and quantum technologies."],"url":"http://arxiv.org/abs/2402.08930v1","category":"physics.optics"}
{"created":"2024-02-14 04:03:38","title":"Second Order Methods for Bandit Optimization and Control","abstract":"Bandit convex optimization (BCO) is a general framework for online decision making under uncertainty. While tight regret bounds for general convex losses have been established, existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data.   In this paper, we propose a simple and practical BCO algorithm inspired by the online Newton step algorithm. We show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that we call $\\kappa$-convex. This class contains a wide range of practically relevant loss functions including linear, quadratic, and generalized linear models. In addition to optimal regret, this method is the most efficient known algorithm for several well-studied applications including bandit logistic regression.   Furthermore, we investigate the adaptation of our second-order bandit algorithm to online convex optimization with memory. We show that for loss functions with a certain affine structure, the extended algorithm attains optimal regret. This leads to an algorithm with optimal regret for bandit LQR/LQG problems under a fully adversarial noise model, thereby resolving an open question posed in \\citep{gradu2020non} and \\citep{sun2023optimal}.   Finally, we show that the more general problem of BCO with (non-affine) memory is harder. We derive a $\\tilde{\\Omega}(T^{2/3})$ regret lower bound, even under the assumption of smooth and quadratic losses.","sentences":["Bandit convex optimization (BCO) is a general framework for online decision making under uncertainty.","While tight regret bounds for general convex losses have been established, existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data.   ","In this paper, we propose a simple and practical BCO algorithm inspired by the online Newton step algorithm.","We show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that we call $\\kappa$-convex.","This class contains a wide range of practically relevant loss functions including linear, quadratic, and generalized linear models.","In addition to optimal regret, this method is the most efficient known algorithm for several well-studied applications including bandit logistic regression.   ","Furthermore, we investigate the adaptation of our second-order bandit algorithm to online convex optimization with memory.","We show that for loss functions with a certain affine structure, the extended algorithm attains optimal regret.","This leads to an algorithm with optimal regret for bandit LQR/LQG problems under a fully adversarial noise model, thereby resolving an open question posed in \\citep{gradu2020non} and \\citep{sun2023optimal}.   ","Finally, we show that the more general problem of BCO with (non-affine) memory is harder.","We derive a $\\tilde{\\Omega}(T^{2/3})$ regret lower bound, even under the assumption of smooth and quadratic losses."],"url":"http://arxiv.org/abs/2402.08929v1","category":"cs.LG"}
{"created":"2024-02-14 03:43:05","title":"The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes","abstract":"Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models.   In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches.   We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analysis of memorization, mislabeled data detection, and tracing behavior in language models. Our code will be made available at https://github.com/ruoxi-jia-group/Forward-INF.","sentences":["Large-scale black-box models have become ubiquitous across numerous applications.","Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness.","Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets.","These approaches face obvious computational challenges when scaled up to large datasets and models.   ","In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data.","Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples.","Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis.","Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point.","This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches.   ","We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analysis of memorization, mislabeled data detection, and tracing behavior in language models.","Our code will be made available at https://github.com/ruoxi-jia-group/Forward-INF."],"url":"http://arxiv.org/abs/2402.08922v1","category":"cs.LG"}
{"created":"2024-02-14 03:15:28","title":"An Interference-aware Approach for Co-located Container Orchestration with Novel Metric","abstract":"Container orchestration technologies are widely employed in cloud computing, facilitating the co-location of online and offline services on the same infrastructure. Online services demand rapid responsiveness and high availability, whereas offline services require extensive computational resources. However, this mixed deployment can lead to resource contention, adversely affecting the performance of online services, yet the metrics used by existing methods cannot accurately reflect the extent of interference.   In this paper, we introduce scheduling latency as a novel metric for quantifying interference and compare it with existing metrics. Empirical evidence demonstrates that scheduling latency more accurately reflects the performance degradation of online services. We also utilize various machine learning techniques to predict potential interference on specific hosts for online services, providing reference information for subsequent scheduling decisions. Simultaneously, we propose a method for quantifying node interference based on scheduling latency. To enhance resource utilization, we train a model for online services that predicts CPU and MEM (memory) resource allocation based on workload type and QPS. Finally, we present a scheduling algorithm based on predictive modeling, aiming to reduce interference in online services while balancing node resource utilization. Through experiments and comparisons with three other baseline methods, we demonstrate the effectiveness of our approach. Compared with three baselines, our approach can reduce the average response time, 90th percentile response time, and 99th percentile response time of online services by 29.4%, 31.4%, and 14.5%, respectively.","sentences":["Container orchestration technologies are widely employed in cloud computing, facilitating the co-location of online and offline services on the same infrastructure.","Online services demand rapid responsiveness and high availability, whereas offline services require extensive computational resources.","However, this mixed deployment can lead to resource contention, adversely affecting the performance of online services, yet the metrics used by existing methods cannot accurately reflect the extent of interference.   ","In this paper, we introduce scheduling latency as a novel metric for quantifying interference and compare it with existing metrics.","Empirical evidence demonstrates that scheduling latency more accurately reflects the performance degradation of online services.","We also utilize various machine learning techniques to predict potential interference on specific hosts for online services, providing reference information for subsequent scheduling decisions.","Simultaneously, we propose a method for quantifying node interference based on scheduling latency.","To enhance resource utilization, we train a model for online services that predicts CPU and MEM (memory) resource allocation based on workload type and QPS.","Finally, we present a scheduling algorithm based on predictive modeling, aiming to reduce interference in online services while balancing node resource utilization.","Through experiments and comparisons with three other baseline methods, we demonstrate the effectiveness of our approach.","Compared with three baselines, our approach can reduce the average response time, 90th percentile response time, and 99th percentile response time of online services by 29.4%, 31.4%, and 14.5%, respectively."],"url":"http://arxiv.org/abs/2402.08917v1","category":"cs.DC"}
{"created":"2024-02-14 03:02:49","title":"Supercloseness of the DDG method for a singularly perturbed convection diffusion problem on Shishkin mesh","abstract":"This paper investigates the supercloseness of a singularly perturbed convection diffusion problem using the direct discontinuous Galerkin (DDG) method on a Shishkin mesh. The main technical difficulties lie in controlling the diffusion term inside the layer, the convection term outside the layer, and the inter element jump term caused by the discontinuity of the numerical solution. The main idea is to design a new composite interpolation, in which a global projection is used outside the layer to satisfy the interface conditions determined by the selection of numerical flux, thereby eliminating or controlling the troublesome terms on the unit interface; and inside the layer, Gau{\\ss} Lobatto projection is used to improve the convergence order of the diffusion term. On the basis of that, by selecting appropriate parameters in the numerical flux, we obtain the supercloseness result of almost $k+1$ order under an energy norm. Numerical experiments support our main theoretical conclusion.","sentences":["This paper investigates the supercloseness of a singularly perturbed convection diffusion problem using the direct discontinuous Galerkin (DDG) method on a Shishkin mesh.","The main technical difficulties lie in controlling the diffusion term inside the layer, the convection term outside the layer, and the inter element jump term caused by the discontinuity of the numerical solution.","The main idea is to design a new composite interpolation, in which a global projection is used outside the layer to satisfy the interface conditions determined by the selection of numerical flux, thereby eliminating or controlling the troublesome terms on the unit interface; and inside the layer, Gau{\\ss} Lobatto projection is used to improve the convergence order of the diffusion term.","On the basis of that, by selecting appropriate parameters in the numerical flux, we obtain the supercloseness result of almost $k+1$ order under an energy norm.","Numerical experiments support our main theoretical conclusion."],"url":"http://arxiv.org/abs/2402.08912v1","category":"math.NA"}
{"created":"2024-02-14 02:51:01","title":"Teamwork Makes TEE Work: Open and Resilient Remote Attestation on Decentralized Trust","abstract":"Remote Attestation (RA) enables the integrity and authenticity of applications in Trusted Execution Environment (TEE) to be verified. Existing TEE RA designs employ a centralized trust model where they rely on a single provisioned secret key and a centralized verifier to establish trust for remote parties. This model is however brittle and can be untrusted under advanced attacks nowadays. Besides, most designs only provide fixed functionalities once deployed, making them hard to adapt to different needs on availability, Quality of Service (QoS), etc.   Therefore, we propose JANUS, an open and resilient TEE RA scheme. To decentralize trust, we, on one hand, introduce Physically Unclonable Function (PUF) as an intrinsic root of trust (RoT) in TEE to provide additional measurements and cryptographic enhancements. On the other hand, we use blockchain and smart contract to realize decentralized verification and result audit. Furthermore, we design an automated turnout mechanism that allows JANUS to remain resilient and offer flexible RA services under various situations. We provide a UC-based security proof and demonstrate the scalability and generality of JANUS by implementing an open-sourced prototype.","sentences":["Remote Attestation (RA) enables the integrity and authenticity of applications in Trusted Execution Environment (TEE) to be verified.","Existing TEE RA designs employ a centralized trust model where they rely on a single provisioned secret key and a centralized verifier to establish trust for remote parties.","This model is however brittle and can be untrusted under advanced attacks nowadays.","Besides, most designs only provide fixed functionalities once deployed, making them hard to adapt to different needs on availability, Quality of Service (QoS), etc.   ","Therefore, we propose JANUS, an open and resilient TEE RA scheme.","To decentralize trust, we, on one hand, introduce Physically Unclonable Function (PUF) as an intrinsic root of trust (RoT) in TEE to provide additional measurements and cryptographic enhancements.","On the other hand, we use blockchain and smart contract to realize decentralized verification and result audit.","Furthermore, we design an automated turnout mechanism that allows JANUS to remain resilient and offer flexible RA services under various situations.","We provide a UC-based security proof and demonstrate the scalability and generality of JANUS by implementing an open-sourced prototype."],"url":"http://arxiv.org/abs/2402.08908v1","category":"cs.CR"}
{"created":"2024-02-14 02:00:28","title":"Inconsistency of evaluation metrics in link prediction","abstract":"Link prediction is a paradigmatic and challenging problem in network science, which predicts missing links, future links and temporal links based on known topology. Along with the increasing number of link prediction algorithms, a critical yet previously ignored risk is that the evaluation metrics for algorithm performance are usually chosen at will. This paper implements extensive experiments on hundreds of real networks and 25 well-known algorithms, revealing statistically significant inconsistency of evaluation metrics, namely different metrics probably produce remarkably different rankings of algorithms. Therefore, we conclude that any single metric cannot comprehensively or credibly evaluate algorithm performance. Further analysis suggests the usage of at least two metrics: one is the area under the receiver operating characteristic curve (AUC), and the other is one of the following three candidates metrics, say the area under the precision-recall curve (AUPR), the area under the precision curve (AUC-Precision), and the normalized discounted cumulative gain (NDCG). In addition, as we have proved the essential equivalence of threshold-dependent metrics, if in a link prediction task, some specific thresholds are meaningful, we can consider any one threshold-dependent metric with those thresholds. This work completes a missing part in the landscape of link prediction, and provides a starting point toward a well-accepted criterion or standard to select proper evaluation metrics for link prediction.","sentences":["Link prediction is a paradigmatic and challenging problem in network science, which predicts missing links, future links and temporal links based on known topology.","Along with the increasing number of link prediction algorithms, a critical yet previously ignored risk is that the evaluation metrics for algorithm performance are usually chosen at will.","This paper implements extensive experiments on hundreds of real networks and 25 well-known algorithms, revealing statistically significant inconsistency of evaluation metrics, namely different metrics probably produce remarkably different rankings of algorithms.","Therefore, we conclude that any single metric cannot comprehensively or credibly evaluate algorithm performance.","Further analysis suggests the usage of at least two metrics: one is the area under the receiver operating characteristic curve (AUC), and the other is one of the following three candidates metrics, say the area under the precision-recall curve (AUPR), the area under the precision curve (AUC-Precision), and the normalized discounted cumulative gain (NDCG).","In addition, as we have proved the essential equivalence of threshold-dependent metrics, if in a link prediction task, some specific thresholds are meaningful, we can consider any one threshold-dependent metric with those thresholds.","This work completes a missing part in the landscape of link prediction, and provides a starting point toward a well-accepted criterion or standard to select proper evaluation metrics for link prediction."],"url":"http://arxiv.org/abs/2402.08893v1","category":"cs.SI"}
{"created":"2024-02-14 00:56:09","title":"Inference for an Algorithmic Fairness-Accuracy Frontier","abstract":"Decision-making processes increasingly rely on the use of algorithms. Yet, algorithms' predictive ability frequently exhibit systematic variation across subgroups of the population. While both fairness and accuracy are desirable properties of an algorithm, they often come at the cost of one another. What should a fairness-minded policymaker do then, when confronted with finite data? In this paper, we provide a consistent estimator for a theoretical fairness-accuracy frontier put forward by Liang, Lu and Mu (2023) and propose inference methods to test hypotheses that have received much attention in the fairness literature, such as (i) whether fully excluding a covariate from use in training the algorithm is optimal and (ii) whether there are less discriminatory alternatives to an existing algorithm. We also provide an estimator for the distance between a given algorithm and the fairest point on the frontier, and characterize its asymptotic distribution. We leverage the fact that the fairness-accuracy frontier is part of the boundary of a convex set that can be fully represented by its support function. We show that the estimated support function converges to a tight Gaussian process as the sample size increases, and then express policy-relevant hypotheses as restrictions on the support function to construct valid test statistics.","sentences":["Decision-making processes increasingly rely on the use of algorithms.","Yet, algorithms' predictive ability frequently exhibit systematic variation across subgroups of the population.","While both fairness and accuracy are desirable properties of an algorithm, they often come at the cost of one another.","What should a fairness-minded policymaker do then, when confronted with finite data?","In this paper, we provide a consistent estimator for a theoretical fairness-accuracy frontier put forward by Liang, Lu and Mu (2023) and propose inference methods to test hypotheses that have received much attention in the fairness literature, such as (i) whether fully excluding a covariate from use in training the algorithm is optimal and (ii) whether there are less discriminatory alternatives to an existing algorithm.","We also provide an estimator for the distance between a given algorithm and the fairest point on the frontier, and characterize its asymptotic distribution.","We leverage the fact that the fairness-accuracy frontier is part of the boundary of a convex set that can be fully represented by its support function.","We show that the estimated support function converges to a tight Gaussian process as the sample size increases, and then express policy-relevant hypotheses as restrictions on the support function to construct valid test statistics."],"url":"http://arxiv.org/abs/2402.08879v1","category":"econ.EM"}
{"created":"2024-02-14 00:41:10","title":"TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition","abstract":"The increasing variety and quantity of tagged multimedia content on platforms such as TikTok provides an opportunity to advance computer vision modeling. We have curated a distinctive dataset of 283,582 unique video clips categorized under 386 hashtags relating to modern human actions. We release this dataset as a valuable resource for building domain-specific foundation models for human movement modeling tasks such as action recognition. To validate this dataset, which we name TikTokActions, we perform two sets of experiments. First, we pretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on TikTokActions subset, and then fine-tune and evaluate on popular datasets such as UCF101 and the HMDB51. We find that the performance of the model pre-trained using our Tik-Tok dataset is comparable to models trained on larger action recognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, our investigation into the relationship between pre-training dataset size and fine-tuning performance reveals that beyond a certain threshold, the incremental benefit of larger training sets diminishes. This work introduces a useful TikTok video dataset that is available for public use and provides insights into the marginal benefit of increasing pre-training dataset sizes for video-based foundation models.","sentences":["The increasing variety and quantity of tagged multimedia content on platforms such as TikTok provides an opportunity to advance computer vision modeling.","We have curated a distinctive dataset of 283,582 unique video clips categorized under 386 hashtags relating to modern human actions.","We release this dataset as a valuable resource for building domain-specific foundation models for human movement modeling tasks such as action recognition.","To validate this dataset, which we name TikTokActions, we perform two sets of experiments.","First, we pretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on TikTokActions subset, and then fine-tune and evaluate on popular datasets such as UCF101 and the HMDB51.","We find that the performance of the model pre-trained using our Tik-Tok dataset is comparable to models trained on larger action recognition datasets (95.3% on UCF101 and 53.24% on HMDB51).","Furthermore, our investigation into the relationship between pre-training dataset size and fine-tuning performance reveals that beyond a certain threshold, the incremental benefit of larger training sets diminishes.","This work introduces a useful TikTok video dataset that is available for public use and provides insights into the marginal benefit of increasing pre-training dataset sizes for video-based foundation models."],"url":"http://arxiv.org/abs/2402.08875v1","category":"cs.CV"}
{"created":"2024-02-13 23:46:17","title":"Physics-informed learning fully quantifies uncertainty in seismic structure and source estimate","abstract":"The source estimation of earthquakes and other fault activities in seismogenic subduction zones is essential for hazard assessment. Accurate and reliable earthquake source estimation (ESE) requires full uncertainty quantification (UQ), targeting both the ESE and subsurface seismic velocity structure models incorporated in the ESE, which is technically challenging. Here, we address this problem by adopting a physics-informed deep learning approach that enables such full UQ-based ESE by introducing neural network ensembles trained based on both earthquake observation data and the physical equation of wavefront propagation. The proposed full UQ-based approach enabled the investigation of rupture characteristics of an earthquake in southwest Japan from a statistical viewpoint, demonstrating innovations toward stronger scientific objectivity in the studies of seismogenic zones empowered by physics-informed learning.","sentences":["The source estimation of earthquakes and other fault activities in seismogenic subduction zones is essential for hazard assessment.","Accurate and reliable earthquake source estimation (ESE) requires full uncertainty quantification (UQ), targeting both the ESE and subsurface seismic velocity structure models incorporated in the ESE, which is technically challenging.","Here, we address this problem by adopting a physics-informed deep learning approach that enables such full UQ-based ESE by introducing neural network ensembles trained based on both earthquake observation data and the physical equation of wavefront propagation.","The proposed full UQ-based approach enabled the investigation of rupture characteristics of an earthquake in southwest Japan from a statistical viewpoint, demonstrating innovations toward stronger scientific objectivity in the studies of seismogenic zones empowered by physics-informed learning."],"url":"http://arxiv.org/abs/2402.08854v1","category":"physics.geo-ph"}
{"created":"2024-02-13 23:25:01","title":"Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation","abstract":"We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution.Finally, we combine FANS and gradient-based optimization to extract the subset with the largest PNS.We demonstrate that FANS outperforms existing feature attribution methods on six benchmarks.","sentences":["We investigate the problem of explainability in machine learning.","To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.","However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.","In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.","Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution.","Finally, we combine FANS and gradient-based optimization to extract the subset with the largest PNS.We demonstrate that FANS outperforms existing feature attribution methods on six benchmarks."],"url":"http://arxiv.org/abs/2402.08845v1","category":"cs.LG"}
{"created":"2024-02-13 23:23:37","title":"Variable stars in galactic globular Clusters I. The population of RR Lyrae stars","abstract":"We present a comprehensive catalog of 2824 RR Lyrae stars (RRLs) residing in 115 Galactic globular clusters (GCs). Our catalog includes 1594 fundamental-mode (RRab), 824 first-overtone (RRc), and 28 double-mode (RRd) RRLs, as well as 378 RRLs of an unknown pulsation mode. We cross-matched 481349 RRLs reported in the third data release (DR3) of the ESA mission Gaia and the literature to 170 known GCs. Membership probabilities were computed as the products of a position and shape-dependent prior and a likelihood was computed using parallaxes, proper motions, and, where available, radial velocities from Gaia. Membership likelihoods of RRLs were computed by comparing cluster average parameters based on known member stars and the cross-matched RRLs. We determined empirical RRL instability strip (IS) boundaries based on our catalog and detected three new cluster RRLs inside this region via their excess Gaia G-band photometric uncertainties. We find that 77% of RRLs in GCs are included in the Gaia DR3 Specific Object Study, and 82% were classified as RRLs by the Gaia DR3 classifier, with the majority of the missing sources being located at the crowded GC centers. Surprisingly, we find that 25% of cluster member stars located within the empirical IS are not RRLs and appear to be non-variable. Additionally, we find that 80% of RRab, 84% of RRc, and 100% of the RRd stars are located within theoretical IS boundaries predicted using MESA models with Z = 0.0003, M = 0.7 (M_\\odot), and Y = 0.290. Unexpectedly, a higher Y = 0.357 is required to fully match the location of RRc stars, and lower Y = 0.220 is needed to match the location of RRab stars. Lastly, our catalog does not exhibit an Oosterhoff dichotomy, with at least 22 GCs located inside the Oosterhoff \"gap,\" which is close to the mode of the distribution of mean RRL periods in GCs.","sentences":["We present a comprehensive catalog of 2824 RR Lyrae stars (RRLs) residing in 115 Galactic globular clusters (GCs).","Our catalog includes 1594 fundamental-mode (RRab), 824 first-overtone (RRc), and 28 double-mode (RRd) RRLs, as well as 378 RRLs of an unknown pulsation mode.","We cross-matched 481349 RRLs reported in the third data release (DR3) of the ESA mission Gaia and the literature to 170 known GCs.","Membership probabilities were computed as the products of a position and shape-dependent prior and a likelihood was computed using parallaxes, proper motions, and, where available, radial velocities from Gaia.","Membership likelihoods of RRLs were computed by comparing cluster average parameters based on known member stars and the cross-matched RRLs.","We determined empirical RRL instability strip (IS) boundaries based on our catalog and detected three new cluster RRLs inside this region via their excess Gaia G-band photometric uncertainties.","We find that 77% of RRLs in GCs are included in the Gaia DR3 Specific Object Study, and 82% were classified as RRLs by the Gaia DR3 classifier, with the majority of the missing sources being located at the crowded GC centers.","Surprisingly, we find that 25% of cluster member stars located within the empirical IS are not RRLs and appear to be non-variable.","Additionally, we find that 80% of RRab, 84% of RRc, and 100% of the RRd stars are located within theoretical IS boundaries predicted using MESA models with Z = 0.0003, M = 0.7 (M_\\odot), and Y = 0.290.","Unexpectedly, a higher Y = 0.357 is required to fully match the location of RRc stars, and lower Y = 0.220 is needed to match the location of RRab stars.","Lastly, our catalog does not exhibit an Oosterhoff dichotomy, with at least 22 GCs located inside the Oosterhoff \"gap,\" which is close to the mode of the distribution of mean RRL periods in GCs."],"url":"http://arxiv.org/abs/2402.08843v1","category":"astro-ph.SR"}
{"created":"2024-02-13 23:22:48","title":"Synchronization Games","abstract":"We propose a new mean-field game model with two states to study synchronization phenomena, and we provide a comprehensive characterization of stationary and dynamic equilibria along with their stability properties. The game undergoes a phase transition with increasing interaction strength. In the subcritical regime, the uniform distribution, representing incoherence, is the unique and stable stationary equilibrium. Above the critical interaction threshold, the uniform equilibrium becomes unstable and there is a multiplicity of stationary equilibria that are self-organizing. Under a discounted cost, dynamic equilibria spiral around the uniform distribution before converging to the self-organizing equilibria. With an ergodic cost, however, unexpected periodic equilibria around the uniform distribution emerge, which also minimize the associated mean-field control problem.","sentences":["We propose a new mean-field game model with two states to study synchronization phenomena, and we provide a comprehensive characterization of stationary and dynamic equilibria along with their stability properties.","The game undergoes a phase transition with increasing interaction strength.","In the subcritical regime, the uniform distribution, representing incoherence, is the unique and stable stationary equilibrium.","Above the critical interaction threshold, the uniform equilibrium becomes unstable and there is a multiplicity of stationary equilibria that are self-organizing.","Under a discounted cost, dynamic equilibria spiral around the uniform distribution before converging to the self-organizing equilibria.","With an ergodic cost, however, unexpected periodic equilibria around the uniform distribution emerge, which also minimize the associated mean-field control problem."],"url":"http://arxiv.org/abs/2402.08842v1","category":"math.OC"}
{"created":"2024-02-13 23:15:13","title":"Approximate Sequential Optimization for Informative Path Planning","abstract":"We consider the problem of finding an informative path through a graph, given initial and terminal nodes and a given maximum path length. We assume that a linear noise corrupted measurement is taken at each node of an underlying unknown vector that we wish to estimate. The informativeness is measured by the reduction in uncertainty in our estimate, evaluated using several metrics. We present a convex relaxation for this informative path planning problem, which we can readily solve to obtain a bound on the possible performance. We develop an approximate sequential method where the path is constructed segment by segment through dynamic programming. This involves solving an orienteering problem, with the node reward acting as a surrogate for informativeness, taking the first step, and then repeating the process. The method scales to very large problem instances and achieves performance not too far from the bound produced by the convex relaxation. We also demonstrate our method's ability to handle adaptive objectives, multimodal sensing, and multi-agent variations of the informative path planning problem.","sentences":["We consider the problem of finding an informative path through a graph, given initial and terminal nodes and a given maximum path length.","We assume that a linear noise corrupted measurement is taken at each node of an underlying unknown vector that we wish to estimate.","The informativeness is measured by the reduction in uncertainty in our estimate, evaluated using several metrics.","We present a convex relaxation for this informative path planning problem, which we can readily solve to obtain a bound on the possible performance.","We develop an approximate sequential method where the path is constructed segment by segment through dynamic programming.","This involves solving an orienteering problem, with the node reward acting as a surrogate for informativeness, taking the first step, and then repeating the process.","The method scales to very large problem instances and achieves performance not too far from the bound produced by the convex relaxation.","We also demonstrate our method's ability to handle adaptive objectives, multimodal sensing, and multi-agent variations of the informative path planning problem."],"url":"http://arxiv.org/abs/2402.08841v1","category":"cs.RO"}
{"created":"2024-02-13 22:47:22","title":"Learning to Generate Context-Sensitive Backchannel Smiles for Embodied AI Agents with Applications in Mental Health Dialogues","abstract":"Addressing the critical shortage of mental health resources for effective screening, diagnosis, and treatment remains a significant challenge. This scarcity underscores the need for innovative solutions, particularly in enhancing the accessibility and efficacy of therapeutic support. Embodied agents with advanced interactive capabilities emerge as a promising and cost-effective supplement to traditional caregiving methods. Crucial to these agents' effectiveness is their ability to simulate non-verbal behaviors, like backchannels, that are pivotal in establishing rapport and understanding in therapeutic contexts but remain under-explored. To improve the rapport-building capabilities of embodied agents we annotated backchannel smiles in videos of intimate face-to-face conversations over topics such as mental health, illness, and relationships. We hypothesized that both speaker and listener behaviors affect the duration and intensity of backchannel smiles. Using cues from speech prosody and language along with the demographics of the speaker and listener, we found them to contain significant predictors of the intensity of backchannel smiles. Based on our findings, we introduce backchannel smile production in embodied agents as a generation problem. Our attention-based generative model suggests that listener information offers performance improvements over the baseline speaker-centric generation approach. Conditioned generation using the significant predictors of smile intensity provides statistically significant improvements in empirical measures of generation quality. Our user study by transferring generated smiles to an embodied agent suggests that agent with backchannel smiles is perceived to be more human-like and is an attractive alternative for non-personal conversations over agent without backchannel smiles.","sentences":["Addressing the critical shortage of mental health resources for effective screening, diagnosis, and treatment remains a significant challenge.","This scarcity underscores the need for innovative solutions, particularly in enhancing the accessibility and efficacy of therapeutic support.","Embodied agents with advanced interactive capabilities emerge as a promising and cost-effective supplement to traditional caregiving methods.","Crucial to these agents' effectiveness is their ability to simulate non-verbal behaviors, like backchannels, that are pivotal in establishing rapport and understanding in therapeutic contexts but remain under-explored.","To improve the rapport-building capabilities of embodied agents we annotated backchannel smiles in videos of intimate face-to-face conversations over topics such as mental health, illness, and relationships.","We hypothesized that both speaker and listener behaviors affect the duration and intensity of backchannel smiles.","Using cues from speech prosody and language along with the demographics of the speaker and listener, we found them to contain significant predictors of the intensity of backchannel smiles.","Based on our findings, we introduce backchannel smile production in embodied agents as a generation problem.","Our attention-based generative model suggests that listener information offers performance improvements over the baseline speaker-centric generation approach.","Conditioned generation using the significant predictors of smile intensity provides statistically significant improvements in empirical measures of generation quality.","Our user study by transferring generated smiles to an embodied agent suggests that agent with backchannel smiles is perceived to be more human-like and is an attractive alternative for non-personal conversations over agent without backchannel smiles."],"url":"http://arxiv.org/abs/2402.08837v1","category":"cs.CL"}
{"created":"2024-02-13 22:42:58","title":"Fast Pulsars, Neutron Stars, and Astrophysical Strange Quark Matter Objects","abstract":"This book chapter explores key aspects of neutron stars, pulsar glitches, tidal deformability, fast pulsars, the equation of state, and strange quark matter stars. Challenges in directly measuring neutron star radius have led to reliance on spectroscopic and timing techniques, with uncertainties addressed through careful source selection and theoretical modeling. Pulsar glitches reveal insights into the equation of state through angular momentum transfer within the neutron star. Tidal deformability is crucial in gravitational-wave astronomy, exemplified by the GW170817 event. Fast pulsars, instrumental in astrophysical testing, are classified into ordinary pulsars, millisecond pulsars, and magnetars. The EOS is vital for understanding neutron star internal structure, explored through various models. The chapter delves into the theoretical framework for rotating neutron stars, addressing uniform and differential rotation scenarios and their impacts on mass and radius. Additionally, the intriguing concept of quark stars and strange dwarfs is investigated. The various topics discussed in this book chapter contribute to a broader understanding of dense matter physics, astrophysical phenomena, and the potential for transformative discoveries through advanced observational techniques and technologies like gravitational wave detectors, radio telescopes, and X-ray telescopes.","sentences":["This book chapter explores key aspects of neutron stars, pulsar glitches, tidal deformability, fast pulsars, the equation of state, and strange quark matter stars.","Challenges in directly measuring neutron star radius have led to reliance on spectroscopic and timing techniques, with uncertainties addressed through careful source selection and theoretical modeling.","Pulsar glitches reveal insights into the equation of state through angular momentum transfer within the neutron star.","Tidal deformability is crucial in gravitational-wave astronomy, exemplified by the GW170817 event.","Fast pulsars, instrumental in astrophysical testing, are classified into ordinary pulsars, millisecond pulsars, and magnetars.","The EOS is vital for understanding neutron star internal structure, explored through various models.","The chapter delves into the theoretical framework for rotating neutron stars, addressing uniform and differential rotation scenarios and their impacts on mass and radius.","Additionally, the intriguing concept of quark stars and strange dwarfs is investigated.","The various topics discussed in this book chapter contribute to a broader understanding of dense matter physics, astrophysical phenomena, and the potential for transformative discoveries through advanced observational techniques and technologies like gravitational wave detectors, radio telescopes, and X-ray telescopes."],"url":"http://arxiv.org/abs/2402.08835v1","category":"astro-ph.HE"}
{"created":"2024-02-13 22:22:51","title":"Sequence graphs realizations and ambiguity in language models","abstract":"Several popular language models represent local contexts in an input text as bags of words. Such representations are naturally encoded by a sequence graph whose vertices are the distinct words occurring in x, with edges representing the (ordered) co-occurrence of two words within a sliding window of size w. However, this compressed representation is not generally bijective, and may introduce some degree of ambiguity. Some sequence graphs may admit several realizations as a sequence, while others may not admit any realization. In this paper, we study the realizability and ambiguity of sequence graphs from a combinatorial and computational point of view. We consider the existence and enumeration of realizations of a sequence graph under multiple settings: window size w, presence/absence of graph orientation, and presence/absence of weights (multiplicities). When w = 2, we provide polynomial time algorithms for realizability and enumeration in all cases except the undirected/weighted setting, where we show the #P-hardness of enumeration. For a window of size at least 3, we prove hardness of all variants, even when w is considered as a constant, with the notable exception of the undirected/unweighted case for which we propose an XP algorithms for both (realizability and enumeration) problems, tight due to a corresponding W[1]-hardness result. We conclude with an integer program formulation to solve the realizability problem, and with dynamic programming to solve the enumeration problem. This work leaves open the membership to NP for both problems, a non-trivial question due to the existence of minimum realizations having exponential size on the instance encoding.","sentences":["Several popular language models represent local contexts in an input text as bags of words.","Such representations are naturally encoded by a sequence graph whose vertices are the distinct words occurring in x, with edges representing the (ordered) co-occurrence of two words within a sliding window of size w.","However, this compressed representation is not generally bijective, and may introduce some degree of ambiguity.","Some sequence graphs may admit several realizations as a sequence, while others may not admit any realization.","In this paper, we study the realizability and ambiguity of sequence graphs from a combinatorial and computational point of view.","We consider the existence and enumeration of realizations of a sequence graph under multiple settings: window size w, presence/absence of graph orientation, and presence/absence of weights (multiplicities).","When w = 2, we provide polynomial time algorithms for realizability and enumeration in all cases except the undirected/weighted setting, where we show the #P-hardness of enumeration.","For a window of size at least 3, we prove hardness of all variants, even when w is considered as a constant, with the notable exception of the undirected/unweighted case for which we propose an XP algorithms for both (realizability and enumeration) problems, tight due to a corresponding W[1]-hardness result.","We conclude with an integer program formulation to solve the realizability problem, and with dynamic programming to solve the enumeration problem.","This work leaves open the membership to NP for both problems, a non-trivial question due to the existence of minimum realizations having exponential size on the instance encoding."],"url":"http://arxiv.org/abs/2402.08830v1","category":"cs.DS"}
{"created":"2024-02-13 22:16:36","title":"Fusing Individualized Treatment Rules Using Secondary Outcomes","abstract":"An individualized treatment rule (ITR) is a decision rule that recommends treatments for patients based on their individual feature variables. In many practices, the ideal ITR for the primary outcome is also expected to cause minimal harm to other secondary outcomes. Therefore, our objective is to learn an ITR that not only maximizes the value function for the primary outcome, but also approximates the optimal rule for the secondary outcomes as closely as possible. To achieve this goal, we introduce a fusion penalty to encourage the ITRs based on different outcomes to yield similar recommendations. Two algorithms are proposed to estimate the ITR using surrogate loss functions. We prove that the agreement rate between the estimated ITR of the primary outcome and the optimal ITRs of the secondary outcomes converges to the true agreement rate faster than if the secondary outcomes are not taken into consideration. Furthermore, we derive the non-asymptotic properties of the value function and misclassification rate for the proposed method. Finally, simulation studies and a real data example are used to demonstrate the finite-sample performance of the proposed method.","sentences":["An individualized treatment rule (ITR) is a decision rule that recommends treatments for patients based on their individual feature variables.","In many practices, the ideal ITR for the primary outcome is also expected to cause minimal harm to other secondary outcomes.","Therefore, our objective is to learn an ITR that not only maximizes the value function for the primary outcome, but also approximates the optimal rule for the secondary outcomes as closely as possible.","To achieve this goal, we introduce a fusion penalty to encourage the ITRs based on different outcomes to yield similar recommendations.","Two algorithms are proposed to estimate the ITR using surrogate loss functions.","We prove that the agreement rate between the estimated ITR of the primary outcome and the optimal ITRs of the secondary outcomes converges to the true agreement rate faster than if the secondary outcomes are not taken into consideration.","Furthermore, we derive the non-asymptotic properties of the value function and misclassification rate for the proposed method.","Finally, simulation studies and a real data example are used to demonstrate the finite-sample performance of the proposed method."],"url":"http://arxiv.org/abs/2402.08828v1","category":"stat.ME"}
{"created":"2024-02-13 22:10:57","title":"Equilibria of Data Marketplaces with Privacy-Aware Sellers under Endogenous Privacy Costs","abstract":"We study a two-sided online data ecosystem comprised of an online platform, users on the platform, and downstream learners or data buyers. The learners can buy user data on the platform (to run a statistic or machine learning task). Potential users decide whether to join by looking at the trade-off between i) their benefit from joining the platform and interacting with other users and ii) the privacy costs they incur from sharing their data.   First, we introduce a novel modeling element for two-sided data platforms: the privacy costs of the users are endogenous and depend on how much of their data is purchased by the downstream learners. Then, we characterize marketplace equilibria in certain simple settings. In particular, we provide a full characterization in two variants of our model that correspond to different utility functions for the users: i) when each user gets a constant benefit for participating in the platform and ii) when each user's benefit is linearly increasing in the number of other users that participate. In both variants, equilibria in our setting are significantly different from equilibria when privacy costs are exogenous and fixed, highlighting the importance of taking endogeneity in the privacy costs into account. Finally, we provide simulations and semi-synthetic experiments to extend our results to more general assumptions. We experiment with different distributions of users' privacy costs and different functional forms of the users' utilities for joining the platform.","sentences":["We study a two-sided online data ecosystem comprised of an online platform, users on the platform, and downstream learners or data buyers.","The learners can buy user data on the platform (to run a statistic or machine learning task).","Potential users decide whether to join by looking at the trade-off between i) their benefit from joining the platform and interacting with other users and ii) the privacy costs they incur from sharing their data.   ","First, we introduce a novel modeling element for two-sided data platforms: the privacy costs of the users are endogenous and depend on how much of their data is purchased by the downstream learners.","Then, we characterize marketplace equilibria in certain simple settings.","In particular, we provide a full characterization in two variants of our model that correspond to different utility functions for the users: i) when each user gets a constant benefit for participating in the platform and ii) when each user's benefit is linearly increasing in the number of other users that participate.","In both variants, equilibria in our setting are significantly different from equilibria when privacy costs are exogenous and fixed, highlighting the importance of taking endogeneity in the privacy costs into account.","Finally, we provide simulations and semi-synthetic experiments to extend our results to more general assumptions.","We experiment with different distributions of users' privacy costs and different functional forms of the users' utilities for joining the platform."],"url":"http://arxiv.org/abs/2402.08826v1","category":"cs.GT"}
{"created":"2024-02-13 22:05:22","title":"Extended symmetry analysis of (1+2)-dimensional fine Kolmogorov backward equation","abstract":"Within the class of (1+2)-dimensional ultraparabolic linear equations, we distinguish a fine Kolmogorov backward equation with a quadratic diffusivity. Modulo the point equivalence, it is a unique equation within the class whose essential Lie invariance algebra is five-dimensional and nonsolvable. Using the direct method, we compute the point symmetry pseudogroup of this equation and analyze its structure. In particular, we single out its essential subgroup and classify its discrete elements. We exhaustively classify all subalgebras of the corresponding essential Lie invariance algebra up to inner automorphisms and up to the action of the essential point-symmetry group. This allowed us to classify Lie reductions and Lie invariant solutions of the equation under consideration. We also discuss the generation of its solutions using point and linear generalized symmetries and carry out its peculiar generalized reductions. As a result, we construct wide families of its solutions parameterized by an arbitrary finite number of arbitrary solutions of the (1+1)-dimensional linear heat equation or one or two arbitrary solutions of (1+1)-dimensional linear heat equations with inverse square potentials.","sentences":["Within the class of (1+2)-dimensional ultraparabolic linear equations, we distinguish a fine Kolmogorov backward equation with a quadratic diffusivity.","Modulo the point equivalence, it is a unique equation within the class whose essential Lie invariance algebra is five-dimensional and nonsolvable.","Using the direct method, we compute the point symmetry pseudogroup of this equation and analyze its structure.","In particular, we single out its essential subgroup and classify its discrete elements.","We exhaustively classify all subalgebras of the corresponding essential Lie invariance algebra up to inner automorphisms and up to the action of the essential point-symmetry group.","This allowed us to classify Lie reductions and Lie invariant solutions of the equation under consideration.","We also discuss the generation of its solutions using point and linear generalized symmetries and carry out its peculiar generalized reductions.","As a result, we construct wide families of its solutions parameterized by an arbitrary finite number of arbitrary solutions of the (1+1)-dimensional linear heat equation or one or two arbitrary solutions of (1+1)-dimensional linear heat equations with inverse square potentials."],"url":"http://arxiv.org/abs/2402.08822v1","category":"math-ph"}
{"created":"2024-02-13 22:04:42","title":"Problem-Parameter-Free Decentralized Nonconvex Stochastic Optimization","abstract":"Existing decentralized algorithms usually require knowledge of problem parameters for updating local iterates. For example, the hyperparameters (such as learning rate) usually require the knowledge of Lipschitz constant of the global gradient or topological information of the communication networks, which are usually not accessible in practice. In this paper, we propose D-NASA, the first algorithm for decentralized nonconvex stochastic optimization that requires no prior knowledge of any problem parameters. We show that D-NASA has the optimal rate of convergence for nonconvex objectives under very mild conditions and enjoys the linear-speedup effect, i.e. the computation becomes faster as the number of nodes in the system increases. Extensive numerical experiments are conducted to support our findings.","sentences":["Existing decentralized algorithms usually require knowledge of problem parameters for updating local iterates.","For example, the hyperparameters (such as learning rate) usually require the knowledge of Lipschitz constant of the global gradient or topological information of the communication networks, which are usually not accessible in practice.","In this paper, we propose D-NASA, the first algorithm for decentralized nonconvex stochastic optimization that requires no prior knowledge of any problem parameters.","We show that D-NASA has the optimal rate of convergence for nonconvex objectives under very mild conditions and enjoys the linear-speedup effect, i.e. the computation becomes faster as the number of nodes in the system increases.","Extensive numerical experiments are conducted to support our findings."],"url":"http://arxiv.org/abs/2402.08821v1","category":"math.OC"}
{"created":"2024-02-13 21:56:04","title":"Infinite-horizon optimal scheduling for feedback control","abstract":"Emerging cyber-physical systems impel the development of communication protocols to efficiently utilize resources. This paper investigates the optimal co-design of control and scheduling in networked control systems. The objective is to co-design the control law and the scheduling mechanism that jointly optimize the tradeoff between regulation performance and communication resource consumption in the long run. The concept of the value of information (VoI) is employed to evaluate the importance of data being transmitted. The optimal solution includes a certainty equivalent control law and a stationary scheduling policy based on the VoI function. The closed-loop system under the designed scheduling policy is shown to be stochastically stable. By analyzing the property of the VoI function, we show that the optimal scheduling policy is symmetric and is a monotone function when the system matrix is diagonal. Moreover, by the diagonal system matrix assumption, the optimal scheduling policy is shown to be of threshold type. Then we provide a simplified yet equivalent form of the threshold-based optimal scheduling policy. The threshold value searching region is also given. Finally, the numerical simulation illustrates the theoretical result of the VoI-based scheduling.","sentences":["Emerging cyber-physical systems impel the development of communication protocols to efficiently utilize resources.","This paper investigates the optimal co-design of control and scheduling in networked control systems.","The objective is to co-design the control law and the scheduling mechanism that jointly optimize the tradeoff between regulation performance and communication resource consumption in the long run.","The concept of the value of information (VoI) is employed to evaluate the importance of data being transmitted.","The optimal solution includes a certainty equivalent control law and a stationary scheduling policy based on the VoI function.","The closed-loop system under the designed scheduling policy is shown to be stochastically stable.","By analyzing the property of the VoI function, we show that the optimal scheduling policy is symmetric and is a monotone function when the system matrix is diagonal.","Moreover, by the diagonal system matrix assumption, the optimal scheduling policy is shown to be of threshold type.","Then we provide a simplified yet equivalent form of the threshold-based optimal scheduling policy.","The threshold value searching region is also given.","Finally, the numerical simulation illustrates the theoretical result of the VoI-based scheduling."],"url":"http://arxiv.org/abs/2402.08819v1","category":"eess.SY"}
{"created":"2024-02-13 21:36:30","title":"Model approximation in MDPs with unbounded per-step cost","abstract":"We consider the problem of designing a control policy for an infinite-horizon discounted cost Markov decision process $\\mathcal{M}$ when we only have access to an approximate model $\\hat{\\mathcal{M}}$. How well does an optimal policy $\\hat{\\pi}^{\\star}$ of the approximate model perform when used in the original model $\\mathcal{M}$? We answer this question by bounding a weighted norm of the difference between the value function of $\\hat{\\pi}^\\star $ when used in $\\mathcal{M}$ and the optimal value function of $\\mathcal{M}$. We then extend our results and obtain potentially tighter upper bounds by considering affine transformations of the per-step cost. We further provide upper bounds that explicitly depend on the weighted distance between cost functions and weighted distance between transition kernels of the original and approximate models. We present examples to illustrate our results.","sentences":["We consider the problem of designing a control policy for an infinite-horizon discounted cost Markov decision process $\\mathcal{M}$ when we only have access to an approximate model $\\hat{\\mathcal{M}}$. How well does an optimal policy $\\hat{\\pi}^{\\star}$ of the approximate model perform when used in the original model $\\mathcal{M}$?","We answer this question by bounding a weighted norm of the difference between the value function of $\\hat{\\pi}^\\star $ when used in $\\mathcal{M}$ and the optimal value function of $\\mathcal{M}$. We then extend our results and obtain potentially tighter upper bounds by considering affine transformations of the per-step cost.","We further provide upper bounds that explicitly depend on the weighted distance between cost functions and weighted distance between transition kernels of the original and approximate models.","We present examples to illustrate our results."],"url":"http://arxiv.org/abs/2402.08813v1","category":"math.OC"}
{"created":"2024-02-13 21:30:44","title":"Deep and shallow data science for multi-scale optical neuroscience","abstract":"Optical imaging of the brain has expanded dramatically in the past two decades. New optics, indicators, and experimental paradigms are now enabling in-vivo imaging from the synaptic to the cortex-wide scales. To match the resulting flood of data across scales, computational methods are continuously being developed to meet the need of extracting biologically relevant information. In this pursuit, challenges arise in some domains (e.g., SNR and resolution limits in micron-scale data) that require specialized algorithms. These algorithms can, for example, make use of state-of-the-art machine learning to maximally learn the details of a given scale to optimize the processing pipeline. In contrast, other methods, however, such as graph signal processing, seek to abstract away from some of the details that are scale-specific to provide solutions to specific sub-problems common across scales of neuroimaging. Here we discuss limitations and tradeoffs in algorithmic design with the goal of identifying how data quality and variability can hamper algorithm use and dissemination.","sentences":["Optical imaging of the brain has expanded dramatically in the past two decades.","New optics, indicators, and experimental paradigms are now enabling in-vivo imaging from the synaptic to the cortex-wide scales.","To match the resulting flood of data across scales, computational methods are continuously being developed to meet the need of extracting biologically relevant information.","In this pursuit, challenges arise in some domains (e.g., SNR and resolution limits in micron-scale data) that require specialized algorithms.","These algorithms can, for example, make use of state-of-the-art machine learning to maximally learn the details of a given scale to optimize the processing pipeline.","In contrast, other methods, however, such as graph signal processing, seek to abstract away from some of the details that are scale-specific to provide solutions to specific sub-problems common across scales of neuroimaging.","Here we discuss limitations and tradeoffs in algorithmic design with the goal of identifying how data quality and variability can hamper algorithm use and dissemination."],"url":"http://arxiv.org/abs/2402.08811v1","category":"eess.IV"}
{"created":"2024-02-13 21:22:50","title":"Deriving the Gibbons-Maldacena-Nunez no-go theorem from the Raychaudhuri equation","abstract":"In this article, we point out that to solve the null Raychaudhuri equation for higher dimensional spacetime with accelerating FRW solution in external directions and static compact internal directions, it is necessary to violate the Strong Energy condition in higher dimensions. This constraint is well-known in obtaining accelerating cosmological solutions in string compactification, first described by Gibbons-Maldacena-Nunez. In deriving this constraint, we do not make any assumptions regarding the matter content.","sentences":["In this article, we point out that to solve the null Raychaudhuri equation for higher dimensional spacetime with accelerating FRW solution in external directions and static compact internal directions, it is necessary to violate the Strong Energy condition in higher dimensions.","This constraint is well-known in obtaining accelerating cosmological solutions in string compactification, first described by Gibbons-Maldacena-Nunez.","In deriving this constraint, we do not make any assumptions regarding the matter content."],"url":"http://arxiv.org/abs/2402.08805v1","category":"hep-th"}
{"created":"2024-02-13 21:21:32","title":"When Should you Offer an Upgrade: Online Upgrading Mechanisms for Resource Allocation","abstract":"In this work, we study an upgrading scheme for online resource allocation problems. We work in a sequential setting, where at each round a request for a resource arrives and the decision-maker has to decide whether to accept it (and thus, offer the resource) or reject it. The resources are ordered in terms of their value. If the decision-maker decides to accept the request, they can offer an upgrade-for-a-fee to the next more valuable resource. This fee is dynamically decided based on the currently available resources. After the upgrade-for-a-fee option is presented to the requester, they can either accept it, get upgraded, and pay the additional fee, or reject it and maintain their originally allocated resource.   We take the perspective of the decision-maker and wish to design upgrading mechanisms in a way that simultaneously maximizes revenue and minimizes underutilization of resources. Both of these desiderata are encapsulated in a notion of regret that we define, and according to which we measure our algorithms' performance. We present a fast algorithm that achieves O(log T) regret. Finally, we implemented our algorithm utilizing data akin to those observed in the hospitality industry and estimated our upgrading mechanism would increase the annual revenue by over 17%.","sentences":["In this work, we study an upgrading scheme for online resource allocation problems.","We work in a sequential setting, where at each round a request for a resource arrives and the decision-maker has to decide whether to accept it (and thus, offer the resource) or reject it.","The resources are ordered in terms of their value.","If the decision-maker decides to accept the request, they can offer an upgrade-for-a-fee to the next more valuable resource.","This fee is dynamically decided based on the currently available resources.","After the upgrade-for-a-fee option is presented to the requester, they can either accept it, get upgraded, and pay the additional fee, or reject it and maintain their originally allocated resource.   ","We take the perspective of the decision-maker and wish to design upgrading mechanisms in a way that simultaneously maximizes revenue and minimizes underutilization of resources.","Both of these desiderata are encapsulated in a notion of regret that we define, and according to which we measure our algorithms' performance.","We present a fast algorithm that achieves O(log T) regret.","Finally, we implemented our algorithm utilizing data akin to those observed in the hospitality industry and estimated our upgrading mechanism would increase the annual revenue by over 17%."],"url":"http://arxiv.org/abs/2402.08804v1","category":"math.OC"}
{"created":"2024-02-13 21:03:25","title":"Interpretation of local false discovery rates under the zero assumption","abstract":"In large-scale studies with parallel signal-plus-noise observations, the local false discovery rate is a summary statistic that is often presumed to be equal to the posterior probability that the signal is null. We prefer to call the latter quantity the local null-signal rate to emphasize our view that a null signal and a false discovery are not identical events. The local null-signal rate is commonly estimated through empirical Bayes procedures that build on the `zero density assumption', which attributes the density of observations near zero entirely to null signals. In this paper, we argue that this strategy does not furnish estimates of the local null-signal rate, but instead of a quantity we call the complementary local activity rate (clar). Although it is likely to be small, an inactive signal is not necessarily zero. The local activity rate addresses two shortcomings of the local null-signal rate. First, it is a weakly continuous functional of the signal distribution, and second, it takes on sensible values when the signal is sparse but not exactly zero. Our findings clarify the interpretation of local false-discovery rates estimated under the zero density assumption.","sentences":["In large-scale studies with parallel signal-plus-noise observations, the local false discovery rate is a summary statistic that is often presumed to be equal to the posterior probability that the signal is null.","We prefer to call the latter quantity the local null-signal rate to emphasize our view that a null signal and a false discovery are not identical events.","The local null-signal rate is commonly estimated through empirical Bayes procedures that build on the `zero density assumption', which attributes the density of observations near zero entirely to null signals.","In this paper, we argue that this strategy does not furnish estimates of the local null-signal rate, but instead of a quantity we call the complementary local activity rate (clar).","Although it is likely to be small, an inactive signal is not necessarily zero.","The local activity rate addresses two shortcomings of the local null-signal rate.","First, it is a weakly continuous functional of the signal distribution, and second, it takes on sensible values when the signal is sparse but not exactly zero.","Our findings clarify the interpretation of local false-discovery rates estimated under the zero density assumption."],"url":"http://arxiv.org/abs/2402.08792v1","category":"math.ST"}
{"created":"2024-02-13 20:46:37","title":"Preconditioners for the Stochastic Training of Implicit Neural Representations","abstract":"Implicit neural representations have emerged as a powerful technique for encoding complex continuous multidimensional signals as neural networks, enabling a wide range of applications in computer vision, robotics, and geometry. While Adam is commonly used for training due to its stochastic proficiency, it entails lengthy training durations. To address this, we explore alternative optimization techniques for accelerated training without sacrificing accuracy. Traditional second-order optimizers like L-BFGS are suboptimal in stochastic settings, making them unsuitable for large-scale data sets. Instead, we propose stochastic training using curvature-aware diagonal preconditioners, showcasing their effectiveness across various signal modalities such as images, shape reconstruction, and Neural Radiance Fields (NeRF).","sentences":["Implicit neural representations have emerged as a powerful technique for encoding complex continuous multidimensional signals as neural networks, enabling a wide range of applications in computer vision, robotics, and geometry.","While Adam is commonly used for training due to its stochastic proficiency, it entails lengthy training durations.","To address this, we explore alternative optimization techniques for accelerated training without sacrificing accuracy.","Traditional second-order optimizers like L-BFGS are suboptimal in stochastic settings, making them unsuitable for large-scale data sets.","Instead, we propose stochastic training using curvature-aware diagonal preconditioners, showcasing their effectiveness across various signal modalities such as images, shape reconstruction, and Neural Radiance Fields (NeRF)."],"url":"http://arxiv.org/abs/2402.08784v1","category":"cs.CV"}
{"created":"2024-02-13 20:17:24","title":"Almost Tight Bounds for Online Hypergraph Matching","abstract":"In the online hypergraph matching problem, hyperedges of size $k$ over a common ground set arrive online in adversarial order. The goal is to obtain a maximum matching (disjoint set of hyperedges). A na\\\"ive greedy algorithm for this problem achieves a competitive ratio of $\\frac{1}{k}$. We show that no (randomized) online algorithm has competitive ratio better than $\\frac{2+o(1)}{k}$. If edges are allowed to be assigned fractionally, we give a deterministic online algorithm with competitive ratio $\\frac{1-o(1)}{\\ln(k)}$ and show that no online algorithm can have competitive ratio strictly better than $\\frac{1+o(1)}{\\ln(k)}$. Lastly, we give a $\\frac{1-o(1)}{\\ln(k)}$ competitive algorithm for the fractional edge-weighted version of the problem under a free disposal assumption.","sentences":["In the online hypergraph matching problem, hyperedges of size $k$ over a common ground set arrive online in adversarial order.","The goal is to obtain a maximum matching (disjoint set of hyperedges).","A na\\\"ive greedy algorithm for this problem achieves a competitive ratio of $\\frac{1}{k}$. We show that no (randomized) online algorithm has competitive ratio better than $\\frac{2+o(1)}{k}$. If edges are allowed to be assigned fractionally, we give a deterministic online algorithm with competitive ratio $\\frac{1-o(1)}{\\ln(k)}$ and show that no online algorithm can have competitive ratio strictly better than $\\frac{1+o(1)}{\\ln(k)}$. Lastly, we give a $\\frac{1-o(1)}{\\ln(k)}$ competitive algorithm for the fractional edge-weighted version of the problem under a free disposal assumption."],"url":"http://arxiv.org/abs/2402.08775v1","category":"cs.DS"}
{"created":"2024-02-13 20:16:31","title":"LDTrack: Dynamic People Tracking by Service Robots using Diffusion Models","abstract":"Tracking of dynamic people in cluttered and crowded human-centered environments is a challenging robotics problem due to the presence of intraclass variations including occlusions, pose deformations, and lighting variations. This paper introduces a novel deep learning architecture, using conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for tracking multiple dynamic people under intraclass variations. By uniquely utilizing conditional latent diffusion models to capture temporal person embeddings, our architecture can adapt to appearance changes of people over time. We incorporated a latent feature encoder network which enables the diffusion process to operate within a high-dimensional latent space to allow for the extraction and spatial-temporal refinement of such rich features as person appearance, motion, location, identity, and contextual information. Extensive experiments demonstrate the effectiveness of LDTrack over other state-of-the-art tracking methods in cluttered and crowded human-centered environments under intraclass variations. Namely, the results show our method outperforms existing deep learning robotic people tracking methods in both tracking accuracy and tracking precision with statistical significance.","sentences":["Tracking of dynamic people in cluttered and crowded human-centered environments is a challenging robotics problem due to the presence of intraclass variations including occlusions, pose deformations, and lighting variations.","This paper introduces a novel deep learning architecture, using conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for tracking multiple dynamic people under intraclass variations.","By uniquely utilizing conditional latent diffusion models to capture temporal person embeddings, our architecture can adapt to appearance changes of people over time.","We incorporated a latent feature encoder network which enables the diffusion process to operate within a high-dimensional latent space to allow for the extraction and spatial-temporal refinement of such rich features as person appearance, motion, location, identity, and contextual information.","Extensive experiments demonstrate the effectiveness of LDTrack over other state-of-the-art tracking methods in cluttered and crowded human-centered environments under intraclass variations.","Namely, the results show our method outperforms existing deep learning robotic people tracking methods in both tracking accuracy and tracking precision with statistical significance."],"url":"http://arxiv.org/abs/2402.08774v1","category":"cs.CV"}
{"created":"2024-02-13 20:04:58","title":"Unitarily equivalent bilateral weighted shifts with operator weights","abstract":"We study unitarily equivalent bilateral weighted shifts with operator weights. We establish a general characterization of unitary equivalence of such shifts under the assumption that the weights are quasi-invertible. We prove that under certain assumptions unitary equivalence of bilateral weighted shifts with operator weights defined on $ \\mathbb{C}^{2} $ can always be given by a unitary operator with at most two non-zero diagonals. We provide examples of unitarily equivalent shifts with weights defined on $ \\mathbb{C}^{k} $ such that every unitary operator, which intertwines them has at least $ k $ non-zero diagonals.","sentences":["We study unitarily equivalent bilateral weighted shifts with operator weights.","We establish a general characterization of unitary equivalence of such shifts under the assumption that the weights are quasi-invertible.","We prove that under certain assumptions unitary equivalence of bilateral weighted shifts with operator weights defined on $ \\mathbb{C}^{2} $ can always be given by a unitary operator with at most two non-zero diagonals.","We provide examples of unitarily equivalent shifts with weights defined on $ \\mathbb{C}^{k} $ such that every unitary operator, which intertwines them has at least $ k $ non-zero diagonals."],"url":"http://arxiv.org/abs/2402.08770v1","category":"math.FA"}
{"created":"2024-02-13 20:04:39","title":"FLASH: Federated Learning Across Simultaneous Heterogeneities","abstract":"The key premise of federated learning (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data. An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency. An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa. In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client's data quality, data distribution, and latency. FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified manner. To do so, FLASH models the learning dynamics through contextual multi-armed bandits (CMAB) and dynamically selects the most promising clients. Through extensive experiments, we demonstrate that FLASH achieves substantial and consistent improvements over state-of-the-art baselines -- as much as 10% in absolute accuracy -- thanks to its unified approach. Importantly, FLASH also outperforms federated aggregation methods that are designed to handle highly heterogeneous settings and even enjoys a performance boost when integrated with them.","sentences":["The key premise of federated learning (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data.","An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency.","An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa.","In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client's data quality, data distribution, and latency.","FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified manner.","To do so, FLASH models the learning dynamics through contextual multi-armed bandits (CMAB) and dynamically selects the most promising clients.","Through extensive experiments, we demonstrate that FLASH achieves substantial and consistent improvements over state-of-the-art baselines -- as much as 10% in absolute accuracy -- thanks to its unified approach.","Importantly, FLASH also outperforms federated aggregation methods that are designed to handle highly heterogeneous settings and even enjoys a performance boost when integrated with them."],"url":"http://arxiv.org/abs/2402.08769v1","category":"cs.LG"}
{"created":"2024-02-13 19:54:29","title":"Linear-quadratic optimal control for abstract differential-algebraic equations","abstract":"In this paper, we extend classical approach to linear quadratic (LQ) optimal control via Popov operators to abstract linear differential-algebraic equations in Hilbert spaces. To ensure existence of solutions, we assume that the underlying differential-algebraic equation has index one in the pseudo-resolvent sense. This leads to the existence of a degenerate semigroup that can be used to define a Popov operator for our system. It is shown that under a suitable coercivity assumption for the Popov operator the optimal costs can be described by a bounded Riccati operator and that the optimal control input is of feedback form. Furthermore, we characterize exponential stability of abstract differential-algebraic equations which is required to solve the infinite horizon LQ problem.","sentences":["In this paper, we extend classical approach to linear quadratic (LQ) optimal control via Popov operators to abstract linear differential-algebraic equations in Hilbert spaces.","To ensure existence of solutions, we assume that the underlying differential-algebraic equation has index one in the pseudo-resolvent sense.","This leads to the existence of a degenerate semigroup that can be used to define a Popov operator for our system.","It is shown that under a suitable coercivity assumption for the Popov operator the optimal costs can be described by a bounded Riccati operator and that the optimal control input is of feedback form.","Furthermore, we characterize exponential stability of abstract differential-algebraic equations which is required to solve the infinite horizon LQ problem."],"url":"http://arxiv.org/abs/2402.08762v1","category":"math.OC"}
{"created":"2024-02-13 19:43:22","title":"On-the-Fly Syntax Highlighting: Generalisation and Speed-ups","abstract":"On-the-fly syntax highlighting is the task of rapidly associating visual secondary notation values with each character of a language derivation. Research in this domain is driven by the prevalence of online software development tools, which frequently display source code on screen and heavily rely on syntax highlighting mechanisms. In this context, three contrasting demands confront resolvers in this space: speed, accuracy, and development costs. Speed constraints are essential to ensure tool usability, manifesting as responsiveness for end users accessing online source code and minimising system overhead. Simultaneously, achieving precise highlighting is critical for enhancing code comprehensibility. Nevertheless, obtaining accurate results necessitates the capacity to perform grammatical analysis on the code under consideration, even in cases of varying grammatical correctness. Furthermore, addressing the development costs of such resolvers is imperative, given the multitude of programming language versions. The current state-of-the-art approach in this field leverages the original lexer and parser of programming languages to create syntax highlighting oracles, subsequently used for training base Recurrent Neural Network models. As the question of the generalisation of such a solution persists, this paper addresses this aspect by extending the original work to three additional mainstream programming languages and conducting a comprehensive review of the outcomes. Moreover, the original limitations in evaluation performance and training costs are mitigated through the introduction of a novel Convolutional based Neural Network model. This study examines the performance gains of running models on GPUs, finding that the new CNN implementation is much faster than previous methods while maintaining high accuracy.","sentences":["On-the-fly syntax highlighting is the task of rapidly associating visual secondary notation values with each character of a language derivation.","Research in this domain is driven by the prevalence of online software development tools, which frequently display source code on screen and heavily rely on syntax highlighting mechanisms.","In this context, three contrasting demands confront resolvers in this space: speed, accuracy, and development costs.","Speed constraints are essential to ensure tool usability, manifesting as responsiveness for end users accessing online source code and minimising system overhead.","Simultaneously, achieving precise highlighting is critical for enhancing code comprehensibility.","Nevertheless, obtaining accurate results necessitates the capacity to perform grammatical analysis on the code under consideration, even in cases of varying grammatical correctness.","Furthermore, addressing the development costs of such resolvers is imperative, given the multitude of programming language versions.","The current state-of-the-art approach in this field leverages the original lexer and parser of programming languages to create syntax highlighting oracles, subsequently used for training base Recurrent Neural Network models.","As the question of the generalisation of such a solution persists, this paper addresses this aspect by extending the original work to three additional mainstream programming languages and conducting a comprehensive review of the outcomes.","Moreover, the original limitations in evaluation performance and training costs are mitigated through the introduction of a novel Convolutional based Neural Network model.","This study examines the performance gains of running models on GPUs, finding that the new CNN implementation is much faster than previous methods while maintaining high accuracy."],"url":"http://arxiv.org/abs/2402.08754v1","category":"cs.SE"}
{"created":"2024-02-13 19:38:01","title":"Nearest Neighbor Representations of Neural Circuits","abstract":"Neural networks successfully capture the computational power of the human brain for many tasks. Similarly inspired by the brain architecture, Nearest Neighbor (NN) representations is a novel approach of computation. We establish a firmer correspondence between NN representations and neural networks. Although it was known how to represent a single neuron using NN representations, there were no results even for small depth neural networks. Specifically, for depth-2 threshold circuits, we provide explicit constructions for their NN representation with an explicit bound on the number of bits to represent it. Example functions include NN representations of convex polytopes (AND of threshold gates), IP2, OR of threshold gates, and linear or exact decision lists.","sentences":["Neural networks successfully capture the computational power of the human brain for many tasks.","Similarly inspired by the brain architecture, Nearest Neighbor (NN) representations is a novel approach of computation.","We establish a firmer correspondence between NN representations and neural networks.","Although it was known how to represent a single neuron using NN representations, there were no results even for small depth neural networks.","Specifically, for depth-2 threshold circuits, we provide explicit constructions for their NN representation with an explicit bound on the number of bits to represent it.","Example functions include NN representations of convex polytopes (AND of threshold gates), IP2, OR of threshold gates, and linear or exact decision lists."],"url":"http://arxiv.org/abs/2402.08751v1","category":"cs.CC"}
{"created":"2024-02-13 19:32:46","title":"Rationality of Learning Algorithms in Repeated Normal-Form Games","abstract":"Many learning algorithms are known to converge to an equilibrium for specific classes of games if the same learning algorithm is adopted by all agents. However, when the agents are self-interested, a natural question is whether agents have a strong incentive to adopt an alternative learning algorithm that yields them greater individual utility. We capture such incentives as an algorithm's rationality ratio, which is the ratio of the highest payoff an agent can obtain by deviating from a learning algorithm to its payoff from following it. We define a learning algorithm to be $c$-rational if its rationality ratio is at most $c$ irrespective of the game. We first establish that popular learning algorithms such as fictitious play and regret matching are not $c$-rational for any constant $c\\geq 1$. We then propose and analyze two algorithms that are provably $1$-rational under mild assumptions, and have the same properties as (a generalized version of) fictitious play and regret matching, respectively, if all agents follow them. Finally, we show that if an assumption of perfect monitoring is not satisfied, there are games for which $c$-rational algorithms do not exist, and illustrate our results with numerical case studies.","sentences":["Many learning algorithms are known to converge to an equilibrium for specific classes of games if the same learning algorithm is adopted by all agents.","However, when the agents are self-interested, a natural question is whether agents have a strong incentive to adopt an alternative learning algorithm that yields them greater individual utility.","We capture such incentives as an algorithm's rationality ratio, which is the ratio of the highest payoff an agent can obtain by deviating from a learning algorithm to its payoff from following it.","We define a learning algorithm to be $c$-rational if its rationality ratio is at most $c$ irrespective of the game.","We first establish that popular learning algorithms such as fictitious play and regret matching are not $c$-rational for any constant $c\\geq 1$.","We then propose and analyze two algorithms that are provably $1$-rational under mild assumptions, and have the same properties as (a generalized version of) fictitious play and regret matching, respectively, if all agents follow them.","Finally, we show that if an assumption of perfect monitoring is not satisfied, there are games for which $c$-rational algorithms do not exist, and illustrate our results with numerical case studies."],"url":"http://arxiv.org/abs/2402.08747v1","category":"cs.GT"}
{"created":"2024-02-13 19:31:30","title":"CaseCohortCoxSurvival: an R Package for Case-Cohort Inference for Relative Hazard and Pure Risk under the Cox Model","abstract":"The case-cohort design allows analysis of multiple endpoints and only requires covariates to be measured for cases and non-cases in a random subcohort from the cohort. Stratification of subcohort sampling and weight calibration increase efficiency of estimates of log-relative hazards and covariate-specific pure risk, but they may require specifically adapted variance estimators. Some recent articles in epidemiology and medical journals used an inappropriate \"robust\" variance estimator. In addition, stratification, weight calibration and analysis of pure risk seem underutilized in case-cohort studies, possibly because practitioners are put off by the varied technical methodologic literature and lack of convenient software. We recently proposed a unified approach to variance estimation for Cox model log-relative hazards and pure risks, and we implemented it in an R package, CaseCohortCoxSurvival, available on CRAN, that allows appropriate and convenient analysis of case-cohort data, with and without stratification, weight calibration, or missing at random phase-two data. Here we illustrate how easy it is to use CaseCohortCoxSurvival to analyze data from the Prostate, Lung, Colorectal and Ovarian Cancer Screening Trial to estimate pure covariate-specific risk of prostate cancer incidence with various case-cohort design and analysis options. These analyses also indicate situations where the simple \"robust\" variance is too large.","sentences":["The case-cohort design allows analysis of multiple endpoints and only requires covariates to be measured for cases and non-cases in a random subcohort from the cohort.","Stratification of subcohort sampling and weight calibration increase efficiency of estimates of log-relative hazards and covariate-specific pure risk, but they may require specifically adapted variance estimators.","Some recent articles in epidemiology and medical journals used an inappropriate \"robust\" variance estimator.","In addition, stratification, weight calibration and analysis of pure risk seem underutilized in case-cohort studies, possibly because practitioners are put off by the varied technical methodologic literature and lack of convenient software.","We recently proposed a unified approach to variance estimation for Cox model log-relative hazards and pure risks, and we implemented it in an R package, CaseCohortCoxSurvival, available on CRAN, that allows appropriate and convenient analysis of case-cohort data, with and without stratification, weight calibration, or missing at random phase-two data.","Here we illustrate how easy it is to use CaseCohortCoxSurvival to analyze data from the Prostate, Lung, Colorectal and Ovarian Cancer Screening Trial to estimate pure covariate-specific risk of prostate cancer incidence with various case-cohort design and analysis options.","These analyses also indicate situations where the simple \"robust\" variance is too large."],"url":"http://arxiv.org/abs/2402.08744v1","category":"stat.AP"}
{"created":"2024-02-13 19:22:00","title":"SEASONS: Signal and Energy Aware Sensing on iNtermittent Systems","abstract":"Both energy-aware, batteryless intermittent systems and signal-aware adaptive sampling algorithms (ASA) aim to maximize sensor data accuracy under energy constraints in edge devices. Intuitively, combining both into a signal- & energy-aware solution would yield even better accuracy. Unfortunately, ASAs and intermittent systems rely on conflicting energy availability assumptions. So, a straightforward combination cannot achieve their combined benefits. Therefore, we propose SEASONS, the first framework for signal- and energy-aware intermittent systems. SEASONS buffers signal data in time, monitoring queue dynamics to ensure the data is reported within a user-specified latency constraint. SEASONS improves sensor data accuracy by 31% without increasing energy.","sentences":["Both energy-aware, batteryless intermittent systems and signal-aware adaptive sampling algorithms (ASA) aim to maximize sensor data accuracy under energy constraints in edge devices.","Intuitively, combining both into a signal- & energy-aware solution would yield even better accuracy.","Unfortunately, ASAs and intermittent systems rely on conflicting energy availability assumptions.","So, a straightforward combination cannot achieve their combined benefits.","Therefore, we propose SEASONS, the first framework for signal- and energy-aware intermittent systems.","SEASONS buffers signal data in time, monitoring queue dynamics to ensure the data is reported within a user-specified latency constraint.","SEASONS improves sensor data accuracy by 31% without increasing energy."],"url":"http://arxiv.org/abs/2402.08739v1","category":"eess.SY"}
{"created":"2024-02-13 19:17:34","title":"Memory in a sequence of weak and short duration measurements of non-commuting observables","abstract":"Sequential projective measurements of non-commuting observables destroy information about previous measurement outcomes, but weak measurements conducted over short durations do not cause as much disturbance to the system, and as a result, memory of previous outcomes may not be entirely eliminated. We use Quantum State Diffusion to unravel a Lindblad equation, generating numerical, stochastic trajectories for a spin 1/2 system and a system of two entangled spin 1/2 subsystems, each undergoing a sequence of measurements of S_{z} and S_{x} spin observables. The autocorrelation function of completed S_{z} measurement outcomes for a range of lag times is then found. In a second set of trajectories, the simulation is terminated if the system reaches the vicinity of a chosen set of eigenstates of S_{z} and S_{x}: the `terminating eigenstates', during the respective measurement intervals of the two observables. From the generated trajectories, the `probability of termination' that the system will arrive at one of the terminating eigenstates after a certain number of measurements have been performed, can be calculated, as well as the mean first passage, which is the number of measurements performed, on average, before the simulation ends. Results can then be compared with projective measurement Born rule statistics. We demonstrate that with short and weak measurements the autocorrelation function of S_{z} can be non-zero for a range of lag times, while it vanishes for strong, longer duration measurements in agreement with the Born rule. The probability of termination and the mean first passage increase under weaker and shorter measurements, revealing that the system is able to avoid particular eigenstates of S_{z} and S_{x} for extended periods of time. A bias can develop in favour of a return to the same eigenstate visited in the previous measurement(s) of a given observable: a memory effect.","sentences":["Sequential projective measurements of non-commuting observables destroy information about previous measurement outcomes, but weak measurements conducted over short durations do not cause as much disturbance to the system, and as a result, memory of previous outcomes may not be entirely eliminated.","We use Quantum State Diffusion to unravel a Lindblad equation, generating numerical, stochastic trajectories for a spin 1/2 system and a system of two entangled spin 1/2 subsystems, each undergoing a sequence of measurements of S_{z} and S_{x} spin observables.","The autocorrelation function of completed S_{z} measurement outcomes for a range of lag times is then found.","In a second set of trajectories, the simulation is terminated if the system reaches the vicinity of a chosen set of eigenstates of S_{z} and S_{x}: the `terminating eigenstates', during the respective measurement intervals of the two observables.","From the generated trajectories, the `probability of termination' that the system will arrive at one of the terminating eigenstates after a certain number of measurements have been performed, can be calculated, as well as the mean first passage, which is the number of measurements performed, on average, before the simulation ends.","Results can then be compared with projective measurement Born rule statistics.","We demonstrate that with short and weak measurements the autocorrelation function of S_{z} can be non-zero for a range of lag times, while it vanishes for strong, longer duration measurements in agreement with the Born rule.","The probability of termination and the mean first passage increase under weaker and shorter measurements, revealing that the system is able to avoid particular eigenstates of S_{z} and S_{x} for extended periods of time.","A bias can develop in favour of a return to the same eigenstate visited in the previous measurement(s) of a given observable: a memory effect."],"url":"http://arxiv.org/abs/2402.08737v1","category":"quant-ph"}
{"created":"2024-02-13 19:04:17","title":"Magneto-optical Hall response in generic Weyl semimetals","abstract":"Weyl semimetals are predicted to host signature magneto-optical properties sourced by their peculiar Landau level structure, including the chiral level. Analytical studies are often leaving out the Hall component of the conductivity due to its complicated nature, and even though the chiral anomaly requires Weyl nodes to come in charge-conjugate pairs, toy-models hosting only one node are considered almost exclusively; numerical studies including several Weyl nodes are on the other hand often limited to high-field quantum limits or DC studies. Here, I present a twofold purpose study, where I a) analytically derive a closed-form expression also for the Hall conductivity of a generic Weyl semimetal using linear response theory, and b) apply this general framework to evaluate the transverse conductivity components for Weyl systems with two nodes. I study how various model parameters, including the tilt, momentum separation, and energy location of the nodes, as well as the chemical potential affect the magneto-optical conductivity, and complement these studies with deriving an analytical expression for the DC Hall conductivity, which is also evaluated in various systems. Including a chiral pair of nodes result two important differences compared to earlier studies; the contribution from the chiral level is equal in size but opposite at the two nodes, making the net contribution to disappear; the energy scales at which intraband transitions occur is smeared out and approaches that of interband transitions, strengthening the hypothesis that intraband transitions mask signature optical features in materials. This general formalism can be applied for a large family of generic Weyl semimetals, and comprise an important piece towards unravelling the source of the mismatch between theoretical predictions and experimental observations in candidate materials.","sentences":["Weyl semimetals are predicted to host signature magneto-optical properties sourced by their peculiar Landau level structure, including the chiral level.","Analytical studies are often leaving out the Hall component of the conductivity due to its complicated nature, and even though the chiral anomaly requires Weyl nodes to come in charge-conjugate pairs, toy-models hosting only one node are considered almost exclusively; numerical studies including several Weyl nodes are on the other hand often limited to high-field quantum limits or DC studies.","Here, I present a twofold purpose study, where I a) analytically derive a closed-form expression also for the Hall conductivity of a generic Weyl semimetal using linear response theory, and b) apply this general framework to evaluate the transverse conductivity components for Weyl systems with two nodes.","I study how various model parameters, including the tilt, momentum separation, and energy location of the nodes, as well as the chemical potential affect the magneto-optical conductivity, and complement these studies with deriving an analytical expression for the DC Hall conductivity, which is also evaluated in various systems.","Including a chiral pair of nodes result two important differences compared to earlier studies; the contribution from the chiral level is equal in size but opposite at the two nodes, making the net contribution to disappear; the energy scales at which intraband transitions occur is smeared out and approaches that of interband transitions, strengthening the hypothesis that intraband transitions mask signature optical features in materials.","This general formalism can be applied for a large family of generic Weyl semimetals, and comprise an important piece towards unravelling the source of the mismatch between theoretical predictions and experimental observations in candidate materials."],"url":"http://arxiv.org/abs/2402.08735v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-13 19:03:30","title":"Passing Stars as an Important Driver of Paleoclimate and the Solar System's Orbital Evolution","abstract":"Reconstructions of the paleoclimate indicate that ancient climatic fluctuations on Earth are often correlated with variations in its orbital elements. However, the chaos inherent in the solar system's orbital evolution prevents numerical simulations from confidently predicting Earth's past orbital evolution beyond 50-100 Myrs. Gravitational interactions among the Sun's planets and asteroids are believed to set this limiting time horizon, but most prior works approximate the solar system as an isolated system and neglect our surrounding Galaxy. Here we present simulations that include the Sun's nearby stellar population, and we find that close-passing field stars alter our entire planetary system's orbital evolution via their gravitational perturbations on the giant planets. This shortens the timespan over which Earth's orbital evolution can be definitively known by a further ~10%. In particular, in simulations that include an exceptionally close passage of the Sun-like star HD 7977 2.8 Myrs ago, new sequences of Earth's orbital evolution become possible in epochs before ~50 Myrs ago, which includes the Paleocene-Eocene Thermal Maximum. Thus, simulations predicting Earth's past orbital evolution before ~50 Myrs ago must consider the additional uncertainty from passing stars, which can open new regimes of past orbital evolution not seen in previous modeling efforts.","sentences":["Reconstructions of the paleoclimate indicate that ancient climatic fluctuations on Earth are often correlated with variations in its orbital elements.","However, the chaos inherent in the solar system's orbital evolution prevents numerical simulations from confidently predicting Earth's past orbital evolution beyond 50-100 Myrs.","Gravitational interactions among the Sun's planets and asteroids are believed to set this limiting time horizon, but most prior works approximate the solar system as an isolated system and neglect our surrounding Galaxy.","Here we present simulations that include the Sun's nearby stellar population, and we find that close-passing field stars alter our entire planetary system's orbital evolution via their gravitational perturbations on the giant planets.","This shortens the timespan over which Earth's orbital evolution can be definitively known by a further ~10%.","In particular, in simulations that include an exceptionally close passage of the Sun-like star HD 7977 2.8 Myrs ago, new sequences of Earth's orbital evolution become possible in epochs before ~50 Myrs ago, which includes the Paleocene-Eocene Thermal Maximum.","Thus, simulations predicting Earth's past orbital evolution before ~50 Myrs ago must consider the additional uncertainty from passing stars, which can open new regimes of past orbital evolution not seen in previous modeling efforts."],"url":"http://arxiv.org/abs/2402.08734v1","category":"astro-ph.EP"}
{"created":"2024-02-13 19:01:45","title":"Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs","abstract":"Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or \"hallucinated\" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p(Y|X)$ and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques.","sentences":["Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or \"hallucinated\" answers or taking unsafe actions.","But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits.","We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats.","Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p(Y|X)$ and detect incorrect responses with high probability.","We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques."],"url":"http://arxiv.org/abs/2402.08733v1","category":"cs.LG"}
{"created":"2024-02-13 19:01:12","title":"Radio-loud fraction of z>6 quasars","abstract":"Quasars at redshifts $z>6$ are an excellent probe of the formation and evolution of supermassive black holes in the early Universe. The population of radio-luminous quasars is of particular interest, as such quasars could potentially be used to study the neutral intergalactic medium during cosmic reionisation via H$\\,$I 21$\\,$cm absorption studies. However, the lack of deep radio observations of $z>6$ quasars leaves the population poorly constrained, and suitable candidates for an H$\\,$I 21$\\,$cm absorption study have yet to be found. In this work, we present Jansky Very Large Array (VLA) 1$-$2 GHz radio continuum observations of 138 quasars at redshifts $6.0 \\leq z<7.6$. We detect the radio continuum emission of the $z=6.1$ quasar J1034-1425, with a 1.6 GHz flux density of $170\\pm 36\\,\\mu$Jy. This quasar is radio-quiet with radio-loudness, $R \\equiv f_{5\\text{~GHz}}/f_{\\nu,\\text{4400 A}} = 2.4\\pm0.5$. In addition, we detect 7 other quasars at z>6, which have previously been characterised in the literature at these frequencies. Using the full sample, we estimate the radio-loud fraction to be $3.8^{+6.2}_{-2.4}\\%$, where the uncertainties are 95% confidence intervals. This is lower than recent estimates of the radio-loud fraction in the literature, but is still marginally consistent with no redshift evolution of the radio-loud fraction. We explore the undetected quasar population by stacking their continuum images at their optical positions and obtain a median stacked flux density of $13.8\\pm 3.9~\\mu$Jy and luminosity of $\\log{L_{5~\\mathrm{GHz}}/(\\mathrm{W~Hz}^{-1})}=24.2\\pm0.1$.","sentences":["Quasars at redshifts $z>6$ are an excellent probe of the formation and evolution of supermassive black holes in the early Universe.","The population of radio-luminous quasars is of particular interest, as such quasars could potentially be used to study the neutral intergalactic medium during cosmic reionisation via H$\\,$I 21$\\,$cm absorption studies.","However, the lack of deep radio observations of $z>6$ quasars leaves the population poorly constrained, and suitable candidates for an H$\\,$I 21$\\,$cm absorption study have yet to be found.","In this work, we present Jansky Very Large Array (VLA) 1$-$2 GHz radio continuum observations of 138 quasars at redshifts $6.0 \\leq z<7.6$.","We detect the radio continuum emission of the $z=6.1$ quasar J1034-1425, with a 1.6 GHz flux density of $170\\pm 36\\,\\mu$Jy.","This quasar is radio-quiet with radio-loudness, $R \\equiv f_{5\\text{~GHz}}/f_{\\nu,\\text{4400 A}} = 2.4\\pm0.5$.","In addition, we detect 7 other quasars at z>6, which have previously been characterised in the literature at these frequencies.","Using the full sample, we estimate the radio-loud fraction to be $3.8^{+6.2}_{-2.4}\\%$, where the uncertainties are 95% confidence intervals.","This is lower than recent estimates of the radio-loud fraction in the literature, but is still marginally consistent with no redshift evolution of the radio-loud fraction.","We explore the undetected quasar population by stacking their continuum images at their optical positions and obtain a median stacked flux density of $13.8\\pm 3.9~\\mu$Jy and luminosity of $\\log{L_{5~\\mathrm{GHz}}/(\\mathrm{W~Hz}^{-1})}=24.2\\pm0.1$."],"url":"http://arxiv.org/abs/2402.08732v1","category":"astro-ph.GA"}
{"created":"2024-02-13 18:21:59","title":"Stability analysis of a dark energy model in Rastall gravity","abstract":"We study a cosmological model in Rastall's theory of gravity in the framework of flat FLRW metric. We formulate the value of the Hubble parameter which contains two model parameters $ \\alpha $ and $ j $. Employing the Markov Chain Monte Carlo (MCMC) sampling technique, we constrain the magnitude of the model parameters with their uncertainties. Moreover, we reconstruct the effective equation-of-state (EoS) parameter, which converges around the quintessence region. We perform a dynamical system analysis using the linearization technique to validate the results independently. Also, we discuss various physical properties of the model which shows the transition to acceleration and the violation of the Strong energy condition (SEC) in late times evolution. Finally, we conclude that our model mimics the behavior of a dark matter fluid during the past epoch and transitions into a quintessence dark energy model in later times.","sentences":["We study a cosmological model in Rastall's theory of gravity in the framework of flat FLRW metric.","We formulate the value of the Hubble parameter which contains two model parameters $ \\alpha $ and $ j $.","Employing the Markov Chain Monte Carlo (MCMC) sampling technique, we constrain the magnitude of the model parameters with their uncertainties.","Moreover, we reconstruct the effective equation-of-state (EoS) parameter, which converges around the quintessence region.","We perform a dynamical system analysis using the linearization technique to validate the results independently.","Also, we discuss various physical properties of the model which shows the transition to acceleration and the violation of the Strong energy condition (SEC) in late times evolution.","Finally, we conclude that our model mimics the behavior of a dark matter fluid during the past epoch and transitions into a quintessence dark energy model in later times."],"url":"http://arxiv.org/abs/2402.08709v1","category":"gr-qc"}
{"created":"2024-02-13 17:29:13","title":"Planetary Defense Use of the SPHEREx Solar System Object Catalog","abstract":"The upcoming NASA SPHEREx (Spectro-Photometer for the History of the Universe, Epoch of Reionization, and Ices Explorer) all-sky 0.7 to 5.0 um spectral survey, to be conducted from 2025 to 2027, provides a unique space-based opportunity to detect, spectrally categorize, and catalog hundreds of thousands of solar system objects at WISE/NEOWISE sensitivities. This paper discusses the unique near-infrared capabilities of SPHEREx, its potential applications in Planetary Defense, (PD), and the implications for risk mitigation associated with Potentially Hazardous Objects (PHOs). By leveraging SPHEREx data, scientists and decision-makers can enhance our ability to track and characterize PHOs, ultimately contributing to the protection of our planet.","sentences":["The upcoming NASA SPHEREx (Spectro-Photometer for the History of the Universe, Epoch of Reionization, and Ices Explorer) all-sky 0.7 to 5.0 um spectral survey, to be conducted from 2025 to 2027, provides a unique space-based opportunity to detect, spectrally categorize, and catalog hundreds of thousands of solar system objects at WISE/NEOWISE sensitivities.","This paper discusses the unique near-infrared capabilities of SPHEREx, its potential applications in Planetary Defense, (PD), and the implications for risk mitigation associated with Potentially Hazardous Objects (PHOs).","By leveraging SPHEREx data, scientists and decision-makers can enhance our ability to track and characterize PHOs, ultimately contributing to the protection of our planet."],"url":"http://arxiv.org/abs/2402.08705v1","category":"astro-ph.IM"}
{"created":"2024-02-14 17:59:34","title":"DoRA: Weight-Decomposed Low-Rank Adaptation","abstract":"Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding.","sentences":["Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs.","However, there still often exists an accuracy gap between these methods and full fine-tuning (FT).","In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA.","Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA).","DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters.","By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead.","DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding."],"url":"http://arxiv.org/abs/2402.09353v1","category":"cs.CL"}
{"created":"2024-02-14 15:41:07","title":"TDViT: Temporal Dilated Video Transformer for Dense Video Tasks","abstract":"Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video. However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame. Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations. To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully designed temporal dilated transformer blocks (TDTB). TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy. Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics. Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation. Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.","sentences":["Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video.","However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame.","Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations.","To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully designed temporal dilated transformer blocks (TDTB).","TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy.","Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics.","Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation.","Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method.","The code is available at https://github.com/guanxiongsun/vfe.pytorch."],"url":"http://arxiv.org/abs/2402.09257v1","category":"cs.CV"}
{"created":"2024-02-14 15:32:07","title":"Efficient One-stage Video Object Detection by Exploiting Temporal Consistency","abstract":"Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data. However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors. Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs. In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD. Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames. Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames. We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset. Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.","sentences":["Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data.","However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors.","Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs.","In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD.","Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames.","Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames.","We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset.","Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method.","The code is available at https://github.com/guanxiongsun/vfe.pytorch."],"url":"http://arxiv.org/abs/2402.09241v1","category":"cs.CV"}
{"created":"2024-02-14 11:14:52","title":"Unity is Strength: Enhancing Precision in Reentrancy Vulnerability Detection of Smart Contract Analysis Tools","abstract":"Reentrancy is one of the most notorious vulnerabilities in smart contracts, resulting in significant digital asset losses. However, many previous works indicate that current Reentrancy detection tools suffer from high false positive rates. Even worse, recent years have witnessed the emergence of new Reentrancy attack patterns fueled by intricate and diverse vulnerability exploit mechanisms. Unfortunately, current tools face a significant limitation in their capacity to adapt and detect these evolving Reentrancy patterns. Consequently, ensuring precise and highly extensible Reentrancy vulnerability detection remains critical challenges for existing tools. To address this issue, we propose a tool named ReEP, designed to reduce the false positives for Reentrancy vulnerability detection. Additionally, ReEP can integrate multiple tools, expanding its capacity for vulnerability detection. It evaluates results from existing tools to verify vulnerability likelihood and reduce false positives. ReEP also offers excellent extensibility, enabling the integration of different detection tools to enhance precision and cover different vulnerability attack patterns. We perform ReEP to eight existing state-of-the-art Reentrancy detection tools. The average precision of these eight tools increased from the original 0.5% to 73% without sacrificing recall. Furthermore, ReEP exhibits robust extensibility. By integrating multiple tools, the precision further improved to a maximum of 83.6%. These results demonstrate that ReEP effectively unites the strengths of existing works, enhances the precision of Reentrancy vulnerability detection tools.","sentences":["Reentrancy is one of the most notorious vulnerabilities in smart contracts, resulting in significant digital asset losses.","However, many previous works indicate that current Reentrancy detection tools suffer from high false positive rates.","Even worse, recent years have witnessed the emergence of new Reentrancy attack patterns fueled by intricate and diverse vulnerability exploit mechanisms.","Unfortunately, current tools face a significant limitation in their capacity to adapt and detect these evolving Reentrancy patterns.","Consequently, ensuring precise and highly extensible Reentrancy vulnerability detection remains critical challenges for existing tools.","To address this issue, we propose a tool named ReEP, designed to reduce the false positives for Reentrancy vulnerability detection.","Additionally, ReEP can integrate multiple tools, expanding its capacity for vulnerability detection.","It evaluates results from existing tools to verify vulnerability likelihood and reduce false positives.","ReEP also offers excellent extensibility, enabling the integration of different detection tools to enhance precision and cover different vulnerability attack patterns.","We perform ReEP to eight existing state-of-the-art Reentrancy detection tools.","The average precision of these eight tools increased from the original 0.5% to 73% without sacrificing recall.","Furthermore, ReEP exhibits robust extensibility.","By integrating multiple tools, the precision further improved to a maximum of 83.6%.","These results demonstrate that ReEP effectively unites the strengths of existing works, enhances the precision of Reentrancy vulnerability detection tools."],"url":"http://arxiv.org/abs/2402.09094v1","category":"cs.CR"}
{"created":"2024-02-14 09:16:06","title":"Ecient spatial designs for targeted regions or level set detection","abstract":"Acquiring information on spatial phenomena can be costly and time-consuming. In this context, to obtain reliable global knowledge, the choice of measurement location is a crucial issue. Space-lling designs are often used to control variability uniformly across the whole space. However, in a monitoring context, it is more relevant to focus on crucial regions, especially when dealing with sensitive areas such as the environment, climate or public health. It is therefore important to choose a relevant optimality criterion to build models adapted to the purpose of the experiment. In this article, we propose two new optimality criteria: the rst aims to focus on areas where the response exceeds a given threshold, while the second is suitable for estimating sets of levels. We introduce several algorithms for constructing optimal designs. We also focus on cost-eective algorithms that produce non-optimal but ecient designs. For both sequential and non-sequential contexts, we compare our designs with existing ones through extensive simulation studies.","sentences":["Acquiring information on spatial phenomena can be costly and time-consuming.","In this context, to obtain reliable global knowledge, the choice of measurement location is a crucial issue.","Space-lling designs are often used to control variability uniformly across the whole space.","However, in a monitoring context, it is more relevant to focus on crucial regions, especially when dealing with sensitive areas such as the environment, climate or public health.","It is therefore important to choose a relevant optimality criterion to build models adapted to the purpose of the experiment.","In this article, we propose two new optimality criteria: the rst aims to focus on areas where the response exceeds a given threshold, while the second is suitable for estimating sets of levels.","We introduce several algorithms for constructing optimal designs.","We also focus on cost-eective algorithms that produce non-optimal but ecient designs.","For both sequential and non-sequential contexts, we compare our designs with existing ones through extensive simulation studies."],"url":"http://arxiv.org/abs/2402.09032v1","category":"stat.AP"}
{"created":"2024-02-14 09:01:55","title":"Using Fricke modular polynomials to compute isogenies","abstract":"Let $\\mathcal{E}$ be an elliptic curve over a field $\\mathbf{K}$ and $\\ell$ a prime. There exists an elliptic curve $\\mathcal{E}^*$ related to $\\mathcal{E}$ by an isogeny of degree $\\ell$ only if $\\Phi_\\ell^t(X, j(\\mathcal{E})) = 0$, where $\\Phi_\\ell^t(X, Y)$ is the traditional modular polynomial. Moreover, $\\Phi_\\ell^t$ gives the coefficients of $\\mathcal{E}^*$, together with parameters needed to build the isogeny explicitly. Since $\\Phi_\\ell^t$ has very large coefficients, many families with smaller coefficients can be used instead, as described by Elkies, Atkin and others. In this work, we concentrate on the computation of the family of modular polynomials introduced by Fricke and more recently used by Charlap, Coley and Robbins. In some cases, the resulting polynomials are small, which justifies the interest of this study. We review and adapt the known algorithms to perform the computations of these polynomials. After describing the use of series computations, we investigate fast algorithms using floating point numbers based on fast numerical evaluation of Eisenstein series. We also explain how to use isogeny volcanoes as an alternative. The last part is concerned with finding explicit formulas for computing the coefficients of $\\mathcal{E}^*$. To this we add tables of numerical examples.","sentences":["Let $\\mathcal{E}$ be an elliptic curve over a field $\\mathbf{K}$ and $\\ell$ a prime.","There exists an elliptic curve $\\mathcal{E}^*$ related to $\\mathcal{E}$ by an isogeny of degree $\\ell$ only if $\\Phi_\\ell^t(X, j(\\mathcal{E}))","= 0$, where $\\Phi_\\ell^t(X, Y)$ is the traditional modular polynomial.","Moreover, $\\Phi_\\ell^t$ gives the coefficients of $\\mathcal{E}^*$, together with parameters needed to build the isogeny explicitly.","Since $\\Phi_\\ell^t$ has very large coefficients, many families with smaller coefficients can be used instead, as described by Elkies, Atkin and others.","In this work, we concentrate on the computation of the family of modular polynomials introduced by Fricke and more recently used by Charlap, Coley and Robbins.","In some cases, the resulting polynomials are small, which justifies the interest of this study.","We review and adapt the known algorithms to perform the computations of these polynomials.","After describing the use of series computations, we investigate fast algorithms using floating point numbers based on fast numerical evaluation of Eisenstein series.","We also explain how to use isogeny volcanoes as an alternative.","The last part is concerned with finding explicit formulas for computing the coefficients of $\\mathcal{E}^*$. To this we add tables of numerical examples."],"url":"http://arxiv.org/abs/2402.09027v1","category":"math.NT"}
{"created":"2024-02-14 00:40:51","title":"Tree-Based Hard Attention with Self-Motivation for Large Language Models","abstract":"While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps. In fact, selectively comprehending the hierarchical structure of large-scale text is pivotal to understanding its substance. Aligning LLMs more closely with the classification or regression values of specific task through prompting also remains challenging. To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for Large Language Models (TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs to process hierarchically structured text inputs. By leveraging prompting, it enables a frozen LLM to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representation of their relationship. Moreover, TEAROOM comprises a self-motivation strategy for another LLM equipped with a trainable adapter and a linear layer. The selected symbolic outcomes are integrated into another prompt, along with the predictive value of the task. We iteratively feed output values back into the prompt, enabling the trainable LLM to progressively approximate the golden truth. TEAROOM outperforms existing state-of-the-art methods in experimental evaluations across three benchmark datasets, showing its effectiveness in estimating task-specific properties. Through comprehensive experiments and analysis, we have validated the ability of TEAROOM to gradually approach the underlying golden truth through multiple inferences.","sentences":["While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures.","Extracting the task-desired property from their natural language responses typically necessitates additional processing steps.","In fact, selectively comprehending the hierarchical structure of large-scale text is pivotal to understanding its substance.","Aligning LLMs more closely with the classification or regression values of specific task through prompting also remains challenging.","To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for Large Language Models (TEAROOM).","TEAROOM incorporates a tree-based hard attention mechanism for LLMs to process hierarchically structured text inputs.","By leveraging prompting, it enables a frozen LLM to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representation of their relationship.","Moreover, TEAROOM comprises a self-motivation strategy for another LLM equipped with a trainable adapter and a linear layer.","The selected symbolic outcomes are integrated into another prompt, along with the predictive value of the task.","We iteratively feed output values back into the prompt, enabling the trainable LLM to progressively approximate the golden truth.","TEAROOM outperforms existing state-of-the-art methods in experimental evaluations across three benchmark datasets, showing its effectiveness in estimating task-specific properties.","Through comprehensive experiments and analysis, we have validated the ability of TEAROOM to gradually approach the underlying golden truth through multiple inferences."],"url":"http://arxiv.org/abs/2402.08874v1","category":"cs.CL"}
{"created":"2024-02-14 00:29:18","title":"Distributed Optimization with Consensus Constraint for Multi-Robot Semantic Octree Mapping","abstract":"This work develops a distributed optimization algorithm for multi-robot 3-D semantic mapping using streaming range and visual observations and single-hop communication. Our approach relies on gradient-based optimization of the observation log-likelihood of each robot subject to a map consensus constraint to build a common multi-class map of the environment. This formulation leads to closed-form updates which resemble Bayes rule with one-hop prior averaging. To reduce the amount of information exchanged among the robots, we utilize an octree data structure that compresses the multi-class map distribution using adaptive-resolution.","sentences":["This work develops a distributed optimization algorithm for multi-robot 3-D semantic mapping using streaming range and visual observations and single-hop communication.","Our approach relies on gradient-based optimization of the observation log-likelihood of each robot subject to a map consensus constraint to build a common multi-class map of the environment.","This formulation leads to closed-form updates which resemble Bayes rule with one-hop prior averaging.","To reduce the amount of information exchanged among the robots, we utilize an octree data structure that compresses the multi-class map distribution using adaptive-resolution."],"url":"http://arxiv.org/abs/2402.08867v1","category":"cs.RO"}
{"created":"2024-02-13 23:56:04","title":"Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With Spheres","abstract":"Generating safe motion plans in real-time is necessary for the wide-scale deployment of robots in unstructured and human-centric environments. These motion plans must be safe to ensure humans are not harmed and nearby objects are not damaged. However, they must also be generated in real-time to ensure the robot can quickly adapt to changes in the environment. Many trajectory optimization methods introduce heuristics that trade-off safety and real-time performance, which can lead to potentially unsafe plans. This paper addresses this challenge by proposing Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With Spheres (SPARROWS). SPARROWS is a receding-horizon trajectory planner that utilizes the combination of a novel reachable set representation and an exact signed distance function to generate provably-safe motion plans. At runtime, SPARROWS uses parameterized trajectories to compute reachable sets composed entirely of spheres that overapproximate the swept volume of the robot's motion. SPARROWS then performs trajectory optimization to select a safe trajectory that is guaranteed to be collision-free. We demonstrate that SPARROWS' novel reachable set is significantly less conservative than previous approaches. We also demonstrate that SPARROWS outperforms a variety of state-of-the-art methods in solving challenging motion planning tasks in cluttered environments. Code, data, and video demonstrations can be found at \\url{https://roahmlab.github.io/sparrows/}.","sentences":["Generating safe motion plans in real-time is necessary for the wide-scale deployment of robots in unstructured and human-centric environments.","These motion plans must be safe to ensure humans are not harmed and nearby objects are not damaged.","However, they must also be generated in real-time to ensure the robot can quickly adapt to changes in the environment.","Many trajectory optimization methods introduce heuristics that trade-off safety and real-time performance, which can lead to potentially unsafe plans.","This paper addresses this challenge by proposing Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With Spheres (SPARROWS).","SPARROWS is a receding-horizon trajectory planner that utilizes the combination of a novel reachable set representation and an exact signed distance function to generate provably-safe motion plans.","At runtime, SPARROWS uses parameterized trajectories to compute reachable sets composed entirely of spheres that overapproximate the swept volume of the robot's motion.","SPARROWS then performs trajectory optimization to select a safe trajectory that is guaranteed to be collision-free.","We demonstrate that SPARROWS' novel reachable set is significantly less conservative than previous approaches.","We also demonstrate that SPARROWS outperforms a variety of state-of-the-art methods in solving challenging motion planning tasks in cluttered environments.","Code, data, and video demonstrations can be found at \\url{https://roahmlab.github.io/sparrows/}."],"url":"http://arxiv.org/abs/2402.08857v1","category":"cs.RO"}
{"created":"2024-02-13 23:40:33","title":"Coexistence and interplay of two ferroelectric mechanisms in Zn1-xMgxO","abstract":"Ferroelectric materials promise exceptional attributes including low power dissipation, fast operational speeds, enhanced endurance, and superior retention to revolutionize information technology. However, the practical application of ferroelectric-semiconductor memory devices has been significantly challenged by the incompatibility of traditional perovskite oxide ferroelectrics with metal-oxide-semiconductor technology. Recent discoveries of ferroelectricity in binary oxides such as Zn1-xMgxO and Hf1-xZrxO have been a focal point of research in ferroelectric information technology. This work investigates the ferroelectric properties of Zn1-xMgxO utilizing automated band excitation piezoresponse force microscopy. Our findings reveal the coexistence of two ferroelectric subsystems within Zn1-xMgxO. We propose a \"fringing-ridge mechanism\" of polarization switching that is characterized by initial lateral expansion of nucleation without significant propagation in depth, contradicting the conventional domain growth process observed in ferroelectrics. This unique polarization dynamics in Zn1-xMgxO suggests a new understanding of ferroelectric behavior, contributing to both the fundamental science of ferroelectrics and their application in information technology.","sentences":["Ferroelectric materials promise exceptional attributes including low power dissipation, fast operational speeds, enhanced endurance, and superior retention to revolutionize information technology.","However, the practical application of ferroelectric-semiconductor memory devices has been significantly challenged by the incompatibility of traditional perovskite oxide ferroelectrics with metal-oxide-semiconductor technology.","Recent discoveries of ferroelectricity in binary oxides such as Zn1-xMgxO and Hf1-xZrxO have been a focal point of research in ferroelectric information technology.","This work investigates the ferroelectric properties of Zn1-xMgxO utilizing automated band excitation piezoresponse force microscopy.","Our findings reveal the coexistence of two ferroelectric subsystems within Zn1-xMgxO.","We propose a \"fringing-ridge mechanism\" of polarization switching that is characterized by initial lateral expansion of nucleation without significant propagation in depth, contradicting the conventional domain growth process observed in ferroelectrics.","This unique polarization dynamics in Zn1-xMgxO suggests a new understanding of ferroelectric behavior, contributing to both the fundamental science of ferroelectrics and their application in information technology."],"url":"http://arxiv.org/abs/2402.08852v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-13 23:24:07","title":"Adaptive tempered reversible jump algorithm for Bayesian curve fitting","abstract":"Bayesian curve fitting plays an important role in inverse problems, and is often addressed using the Reversible Jump Markov Chain Monte Carlo (RJMCMC) algorithm. However, this algorithm can be computationally inefficient without appropriately tuned proposals. As a remedy, we present an adaptive RJMCMC algorithm for the curve fitting problems by extending the Adaptive Metropolis sampler from a fixed-dimensional to a trans-dimensional case. In this presented algorithm, both the size and orientation of the proposal function can be automatically adjusted in the sampling process. Specifically, the curve fitting setting allows for the approximation of the posterior covariance of the a priori unknown function on a representative grid of points. This approximation facilitates the definition of efficient proposals. In addition, we introduce an auxiliary-tempered version of this algorithm via non-reversible parallel tempering. To evaluate the algorithms, we conduct numerical tests involving a series of controlled experiments. The results demonstrate that the adaptive algorithms exhibit significantly higher efficiency compared to the conventional ones. Even in cases where the posterior distribution is highly complex, leading to ineffective convergence in the auxiliary-tempered conventional RJMCMC, the proposed auxiliary-tempered adaptive RJMCMC performs satisfactorily. Furthermore, we present a realistic inverse example to test the algorithms. The successful application of the adaptive algorithm distinguishes it again from the conventional one that fails to converge effectively even after millions of iterations.","sentences":["Bayesian curve fitting plays an important role in inverse problems, and is often addressed using the Reversible Jump Markov Chain Monte Carlo (RJMCMC) algorithm.","However, this algorithm can be computationally inefficient without appropriately tuned proposals.","As a remedy, we present an adaptive RJMCMC algorithm for the curve fitting problems by extending the Adaptive Metropolis sampler from a fixed-dimensional to a trans-dimensional case.","In this presented algorithm, both the size and orientation of the proposal function can be automatically adjusted in the sampling process.","Specifically, the curve fitting setting allows for the approximation of the posterior covariance of the a priori unknown function on a representative grid of points.","This approximation facilitates the definition of efficient proposals.","In addition, we introduce an auxiliary-tempered version of this algorithm via non-reversible parallel tempering.","To evaluate the algorithms, we conduct numerical tests involving a series of controlled experiments.","The results demonstrate that the adaptive algorithms exhibit significantly higher efficiency compared to the conventional ones.","Even in cases where the posterior distribution is highly complex, leading to ineffective convergence in the auxiliary-tempered conventional RJMCMC, the proposed auxiliary-tempered adaptive RJMCMC performs satisfactorily.","Furthermore, we present a realistic inverse example to test the algorithms.","The successful application of the adaptive algorithm distinguishes it again from the conventional one that fails to converge effectively even after millions of iterations."],"url":"http://arxiv.org/abs/2402.08844v1","category":"stat.CO"}
{"created":"2024-02-13 21:54:15","title":"Corridor Geometry in Gradient-Based Optimization","abstract":"We characterize regions of a loss surface as corridors when the continuous curves of steepest descent -- the solutions of the gradient flow -- become straight lines. We show that corridors provide insights into gradient-based optimization, since corridors are exactly the regions where gradient descent and the gradient flow follow the same trajectory, while the loss decreases linearly. As a result, inside corridors there are no implicit regularization effects or training instabilities that have been shown to occur due to the drift between gradient descent and the gradient flow. Using the loss linear decrease on corridors, we devise a learning rate adaptation scheme for gradient descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation coincides with a special case of Polyak step-size, discovered in the context of convex optimization. The Polyak step-size has been shown recently to have also good convergence properties for neural networks; we further confirm this here with results on CIFAR-10 and ImageNet.","sentences":["We characterize regions of a loss surface as corridors when the continuous curves of steepest descent -- the solutions of the gradient flow -- become straight lines.","We show that corridors provide insights into gradient-based optimization, since corridors are exactly the regions where gradient descent and the gradient flow follow the same trajectory, while the loss decreases linearly.","As a result, inside corridors there are no implicit regularization effects or training instabilities that have been shown to occur due to the drift between gradient descent and the gradient flow.","Using the loss linear decrease on corridors, we devise a learning rate adaptation scheme for gradient descent; we call this scheme Corridor Learning Rate (CLR).","The CLR formulation coincides with a special case of Polyak step-size, discovered in the context of convex optimization.","The Polyak step-size has been shown recently to have also good convergence properties for neural networks; we further confirm this here with results on CIFAR-10 and ImageNet."],"url":"http://arxiv.org/abs/2402.08818v1","category":"stat.ML"}
{"created":"2024-02-13 20:54:24","title":"Syllable based DNN-HMM Cantonese Speech to Text System","abstract":"This paper reports our work on building up a Cantonese Speech-to-Text (STT) system with a syllable based acoustic model. This is a part of an effort in building a STT system to aid dyslexic students who have cognitive deficiency in writing skills but have no problem expressing their ideas through speech. For Cantonese speech recognition, the basic unit of acoustic models can either be the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC) syllables where finals are further split into nucleus and coda to reflect the intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system is trained using the stochastic gradient descent optimization model with the aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model (DNN-HMM) with and without I-vector based speaker adaptive training technique. The input features of the same Gaussian Mixture Model with speaker adaptive training (GMM-SAT) to DNN are used in all cases. Experiments show that the ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the best performance with the word error rate (WER) of 9.66% and the real time factor (RTF) of 1.38812.","sentences":["This paper reports our work on building up a Cantonese Speech-to-Text (STT) system with a syllable based acoustic model.","This is a part of an effort in building a STT system to aid dyslexic students who have cognitive deficiency in writing skills but have no problem expressing their ideas through speech.","For Cantonese speech recognition, the basic unit of acoustic models can either be the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC) syllables where finals are further split into nucleus and coda to reflect the intra-syllable variations in Cantonese.","By using the Kaldi toolkit, our system is trained using the stochastic gradient descent optimization model with the aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model (DNN-HMM) with and without I-vector based speaker adaptive training technique.","The input features of the same Gaussian Mixture Model with speaker adaptive training (GMM-SAT) to DNN are used in all cases.","Experiments show that the ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the best performance with the word error rate (WER) of 9.66% and the real time factor (RTF) of 1.38812."],"url":"http://arxiv.org/abs/2402.08788v1","category":"cs.CL"}
{"created":"2024-02-13 19:00:04","title":"Anomaly inflow, dualities, and quantum simulation of abelian lattice gauge theories induced by measurements","abstract":"A previous work demonstrated that quantum simulation of abelian lattice gauge theories (Wegner models including the toric code in a limit) in general dimensions can be achieved by local adaptive measurements on symmetry-protected topological (SPT) states with higher-form generalized global symmetries. The entanglement structure of the resource SPT state reflects the geometric structure of the gauge theory. In this work, we explicitly demonstrate the anomaly inflow mechanism between the deconfining phase of the simulated gauge theory on the boundary and the SPT state in the bulk, by showing that the anomalous gauge variation of the boundary state obtained by bulk measurement matches that of the bulk theory. Moreover, we construct the resource state and the measurement pattern for the measurement-based quantum simulation of a lattice gauge theory with a matter field (Fradkin-Shenker model), where a simple scheme to protect gauge invariance of the simulated state against errors is proposed. We further consider taking an overlap between the wave function of the resource state for lattice gauge theories and that of a parameterized product state, and we derive precise dualities between partition functions with insertion of defects corresponding to gauging higher-form global symmetries, as well as measurement-induced phases where states induced by a partial overlap possess different (symmetry-protected) topological orders. Measurement-assisted operators to dualize quantum Hamiltonians of lattice gauge theories and their non-invertibility are also presented.","sentences":["A previous work demonstrated that quantum simulation of abelian lattice gauge theories (Wegner models including the toric code in a limit) in general dimensions can be achieved by local adaptive measurements on symmetry-protected topological (SPT) states with higher-form generalized global symmetries.","The entanglement structure of the resource SPT state reflects the geometric structure of the gauge theory.","In this work, we explicitly demonstrate the anomaly inflow mechanism between the deconfining phase of the simulated gauge theory on the boundary and the SPT state in the bulk, by showing that the anomalous gauge variation of the boundary state obtained by bulk measurement matches that of the bulk theory.","Moreover, we construct the resource state and the measurement pattern for the measurement-based quantum simulation of a lattice gauge theory with a matter field (Fradkin-Shenker model), where a simple scheme to protect gauge invariance of the simulated state against errors is proposed.","We further consider taking an overlap between the wave function of the resource state for lattice gauge theories and that of a parameterized product state, and we derive precise dualities between partition functions with insertion of defects corresponding to gauging higher-form global symmetries, as well as measurement-induced phases where states induced by a partial overlap possess different (symmetry-protected) topological orders.","Measurement-assisted operators to dualize quantum Hamiltonians of lattice gauge theories and their non-invertibility are also presented."],"url":"http://arxiv.org/abs/2402.08720v1","category":"cond-mat.str-el"}
{"created":"2024-02-13 18:37:53","title":"BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation","abstract":"Continual Test Time Adaptation (CTTA) is required to adapt efficiently to continuous unseen domains while retaining previously learned knowledge. However, despite the progress of CTTA, forgetting-adaptation trade-offs and efficiency are still unexplored. Moreover, current CTTA scenarios assume only the disjoint situation, even though real-world domains are seamlessly changed. To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts (MoDE) that contains two core components: i) Domain-Adaptive Routing, which aids in selectively capturing the domain-adaptive knowledge with multiple domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency between each domain and expert. We validate our method outperforms multiple CTTA scenarios including disjoint and gradual domain shits, while only requiring ~98% fewer trainable parameters. We also provide analyses of our method, including the construction of experts, the effect of domain-adaptive experts, and visualizations.","sentences":["Continual Test Time Adaptation (CTTA) is required to adapt efficiently to continuous unseen domains while retaining previously learned knowledge.","However, despite the progress of CTTA, forgetting-adaptation trade-offs and efficiency are still unexplored.","Moreover, current CTTA scenarios assume only the disjoint situation, even though real-world domains are seamlessly changed.","To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet efficient framework for CTTA.","We propose Mixture-of-Domain Low-rank Experts (MoDE) that contains two core components: i) Domain-Adaptive Routing, which aids in selectively capturing the domain-adaptive knowledge with multiple domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency between each domain and expert.","We validate our method outperforms multiple CTTA scenarios including disjoint and gradual domain shits, while only requiring ~98% fewer trainable parameters.","We also provide analyses of our method, including the construction of experts, the effect of domain-adaptive experts, and visualizations."],"url":"http://arxiv.org/abs/2402.08712v1","category":"cs.LG"}
{"created":"2024-02-12 20:14:46","title":"Game of Trojans: Adaptive Adversaries Against Output-based Trojaned-Model Detectors","abstract":"We propose and analyze an adaptive adversary that can retrain a Trojaned DNN and is also aware of SOTA output-based Trojaned model detectors. We show that such an adversary can ensure (1) high accuracy on both trigger-embedded and clean samples and (2) bypass detection. Our approach is based on an observation that the high dimensionality of the DNN parameters provides sufficient degrees of freedom to simultaneously achieve these objectives. We also enable SOTA detectors to be adaptive by allowing retraining to recalibrate their parameters, thus modeling a co-evolution of parameters of a Trojaned model and detectors. We then show that this co-evolution can be modeled as an iterative game, and prove that the resulting (optimal) solution of this interactive game leads to the adversary successfully achieving the above objectives. In addition, we provide a greedy algorithm for the adversary to select a minimum number of input samples for embedding triggers. We show that for cross-entropy or log-likelihood loss functions used by the DNNs, the greedy algorithm provides provable guarantees on the needed number of trigger-embedded input samples. Extensive experiments on four diverse datasets -- MNIST, CIFAR-10, CIFAR-100, and SpeechCommand -- reveal that the adversary effectively evades four SOTA output-based Trojaned model detectors: MNTD, NeuralCleanse, STRIP, and TABOR.","sentences":["We propose and analyze an adaptive adversary that can retrain a Trojaned DNN and is also aware of SOTA output-based Trojaned model detectors.","We show that such an adversary can ensure (1) high accuracy on both trigger-embedded and clean samples and (2) bypass detection.","Our approach is based on an observation that the high dimensionality of the DNN parameters provides sufficient degrees of freedom to simultaneously achieve these objectives.","We also enable SOTA detectors to be adaptive by allowing retraining to recalibrate their parameters, thus modeling a co-evolution of parameters of a Trojaned model and detectors.","We then show that this co-evolution can be modeled as an iterative game, and prove that the resulting (optimal) solution of this interactive game leads to the adversary successfully achieving the above objectives.","In addition, we provide a greedy algorithm for the adversary to select a minimum number of input samples for embedding triggers.","We show that for cross-entropy or log-likelihood loss functions used by the DNNs, the greedy algorithm provides provable guarantees on the needed number of trigger-embedded input samples.","Extensive experiments on four diverse datasets -- MNIST, CIFAR-10, CIFAR-100, and SpeechCommand -- reveal that the adversary effectively evades four SOTA output-based Trojaned model detectors: MNTD, NeuralCleanse, STRIP, and TABOR."],"url":"http://arxiv.org/abs/2402.08695v1","category":"cs.CR"}
{"created":"2024-02-12 18:58:01","title":"A holographic mobile-based application for practicing pronunciation of basic English vocabulary for Spanish speaking children","abstract":"This paper describes a holographic mobile-based application designed to help Spanish-speaking children to practice the pronunciation of basic English vocabulary words. The mastery of vocabulary is a fundamental step when learning a language but is often perceived as boring. Producing the correct pronunciation is frequently regarded as the most difficult and complex skill for new learners of English. In order to address these problems this research takes advantage of the power of multi-channel stimuli (sound, image and interaction) in a mobilebased hologram application in order to motivate students and improve their experience of practicing. We adapted the prize-winning HolograFX game and developed a new mobile application to help practice English pronunciation. A 3D holographic robot that acts as a virtual teacher interacts via voice with the children. To test the tool we carried out an experiment with 70 Spanish pre-school children divided into three classes, the control group using traditional methods such as images in books and on the blackboard, and two experimental groups using our drills and practice software. One experimental group used the mobile application without the holographic game and the other experimental group used the application with the holographic game. We performed pre-test and post-test performance assessments, a satisfaction survey and emotion analysis. The results are very promising. They show that the use of the holographic mobile-based application had a significant impact on the children's motivation. It also improved their performance compared to traditional methods used in the classroom.","sentences":["This paper describes a holographic mobile-based application designed to help Spanish-speaking children to practice the pronunciation of basic English vocabulary words.","The mastery of vocabulary is a fundamental step when learning a language but is often perceived as boring.","Producing the correct pronunciation is frequently regarded as the most difficult and complex skill for new learners of English.","In order to address these problems this research takes advantage of the power of multi-channel stimuli (sound, image and interaction) in a mobilebased hologram application in order to motivate students and improve their experience of practicing.","We adapted the prize-winning HolograFX game and developed a new mobile application to help practice English pronunciation.","A 3D holographic robot that acts as a virtual teacher interacts via voice with the children.","To test the tool we carried out an experiment with 70 Spanish pre-school children divided into three classes, the control group using traditional methods such as images in books and on the blackboard, and two experimental groups using our drills and practice software.","One experimental group used the mobile application without the holographic game and the other experimental group used the application with the holographic game.","We performed pre-test and post-test performance assessments, a satisfaction survey and emotion analysis.","The results are very promising.","They show that the use of the holographic mobile-based application had a significant impact on the children's motivation.","It also improved their performance compared to traditional methods used in the classroom."],"url":"http://arxiv.org/abs/2402.07897v1","category":"cs.HC"}
{"created":"2024-02-12 18:56:13","title":"The TESS-Keck Survey XXI: 13 New Planets and Homogeneous Properties for 21 Subgiant Systems","abstract":"We present a dedicated transit and radial velocity survey of planets orbiting subgiant stars observed by the TESS Mission. Using $\\sim$$16$ nights on Keck/HIRES, we confirm and characterize $12$ new transiting planets -- $\\rm TOI-329\\,b$, $\\rm HD\\,39688\\,b$ ($\\rm TOI-480$), $\\rm TOI-603\\,b$, $\\rm TOI-1199\\,b$, $\\rm TOI-1294\\,b$, $\\rm TOI-1439\\,b$, $\\rm TOI-1605\\,b$, $\\rm TOI-1828\\,b$, $\\rm HD\\,148193\\,b$ ($\\rm TOI-1836$), $\\rm TOI-1885\\,b$, $\\rm HD\\,83342\\,b$ ($\\rm TOI-1898$), $\\rm TOI-2019\\,b$ -- and provide updated properties for 9 previously confirmed TESS subgiant systems ($\\rm TOI-197$, $\\rm TOI-954$, $\\rm TOI-1181$, $\\rm TOI-1296$, $\\rm TOI-1298$, $\\rm TOI-1601$, $\\rm TOI-1736$, $\\rm TOI-1842$, $\\rm TOI-2145$). We also report the discovery of an outer, non-transiting planet, $\\rm TOI-1294\\,c$ ($P=160.1\\pm2.5$ days, $M_{\\mathrm{p}}=148.3^{+18.2}_{-16.4} \\,M_{\\oplus}$), and three additional stars with long-term RV trends. We find that at least $19\\pm8\\%$ of subgiants in our sample of $21$ stars have outer companions, comparable to main-sequence stars. We perform a homogeneous analysis of the stars and planets in the sample, with median uncertainties of $3\\%$, $8\\%$ and $15\\%$ for planet radii, masses and ages, doubling the number of known planets orbiting subgiant stars with bulk densities measured to better than $10\\%$. We observe a dearth of giant planets around evolved stars with short orbital periods, consistent with tidal dissipation theories that predict the rapid inspiral of planets as their host stars leave the main sequence. We note the possible evidence for two distinct classes of hot Jupiter populations, indicating multiple formation channels to explain the observed distributions around evolved stars. Finally, continued RV monitoring of planets in this sample will provide a more comprehensive understanding of demographics for evolved planetary systems.","sentences":["We present a dedicated transit and radial velocity survey of planets orbiting subgiant stars observed by the TESS Mission.","Using $\\sim$$16$ nights on Keck/HIRES, we confirm and characterize $12$ new transiting planets -- $\\rm TOI-329\\,b$, $\\rm HD\\,39688\\,b$ ($\\rm TOI-480$), $\\rm TOI-603\\,b$, $\\rm TOI-1199\\,b$, $\\rm TOI-1294\\,b$, $\\rm TOI-1439\\,b$, $\\rm TOI-1605\\,b$, $\\rm TOI-1828\\,b$, $\\rm HD\\,148193\\,b$ ($\\rm TOI-1836$), $\\rm TOI-1885\\,b$, $\\rm HD\\,83342\\,b$ ($\\rm TOI-1898$), $\\rm TOI-2019\\,b$ -- and provide updated properties for 9 previously confirmed TESS subgiant systems ($\\rm TOI-197$, $\\rm TOI-954$, $\\rm TOI-1181$, $\\rm TOI-1296$, $\\rm TOI-1298$, $\\rm TOI-1601$, $\\rm TOI-1736$, $\\rm TOI-1842$, $\\rm TOI-2145$).","We also report the discovery of an outer, non-transiting planet, $\\rm TOI-1294\\,c$ ($P=160.1\\pm2.5$ days, $M_{\\mathrm{p}}=148.3^{+18.2}_{-16.4} \\,M_{\\oplus}$), and three additional stars with long-term RV trends.","We find that at least $19\\pm8\\%$ of subgiants in our sample of $21$ stars have outer companions, comparable to main-sequence stars.","We perform a homogeneous analysis of the stars and planets in the sample, with median uncertainties of $3\\%$, $8\\%$ and $15\\%$ for planet radii, masses and ages, doubling the number of known planets orbiting subgiant stars with bulk densities measured to better than $10\\%$. We observe a dearth of giant planets around evolved stars with short orbital periods, consistent with tidal dissipation theories that predict the rapid inspiral of planets as their host stars leave the main sequence.","We note the possible evidence for two distinct classes of hot Jupiter populations, indicating multiple formation channels to explain the observed distributions around evolved stars.","Finally, continued RV monitoring of planets in this sample will provide a more comprehensive understanding of demographics for evolved planetary systems."],"url":"http://arxiv.org/abs/2402.07893v1","category":"astro-ph.EP"}
{"created":"2024-02-12 18:41:55","title":"WildfireGPT: Tailored Large Language Model for Wildfire Analysis","abstract":"The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML). However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change. For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including researchers, engineers, urban planners, emergency managers, and infrastructure operators.","sentences":["The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML).","However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change.","For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic.","To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks.","We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate.","This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including researchers, engineers, urban planners, emergency managers, and infrastructure operators."],"url":"http://arxiv.org/abs/2402.07877v1","category":"cs.AI"}
{"created":"2024-02-12 18:41:34","title":"Policy Improvement using Language Feedback Models","abstract":"We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.","sentences":["We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following.","To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions.","First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld).","Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens.","Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation.","Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning."],"url":"http://arxiv.org/abs/2402.07876v1","category":"cs.LG"}
{"created":"2024-02-12 18:23:11","title":"Virtual Channel Purification","abstract":"Quantum error mitigation is a key approach for extracting target state properties on state-of-the-art noisy machines and early fault-tolerant devices. Using the ideas from flag fault tolerance and virtual state purification, we develop the virtual channel purification (VCP) protocol, which consumes similar qubit and gate resources as virtual state purification but offers up to exponentially stronger error suppression with increased system size and more noisy operation copies. Furthermore, VCP removes most of the assumptions required in virtual state purification. Essentially, VCP is the first quantum error mitigation protocol that does not require specific knowledge about the noise models, the target quantum state, and the target problem while still offering rigorous performance guarantees for practical noise regimes. Further connections are made between VCP and quantum error correction to produce one of the first protocols that combine quantum error correction and quantum error mitigation beyond concatenation. We can remove all noise in the channel while paying only the same sampling cost as low-order purification, reaching beyond the standard bias-variance trade-off in quantum error mitigation. Our protocol can also be adapted to key tasks in quantum networks like channel capacity activation and entanglement distribution.","sentences":["Quantum error mitigation is a key approach for extracting target state properties on state-of-the-art noisy machines and early fault-tolerant devices.","Using the ideas from flag fault tolerance and virtual state purification, we develop the virtual channel purification (VCP) protocol, which consumes similar qubit and gate resources as virtual state purification but offers up to exponentially stronger error suppression with increased system size and more noisy operation copies.","Furthermore, VCP removes most of the assumptions required in virtual state purification.","Essentially, VCP is the first quantum error mitigation protocol that does not require specific knowledge about the noise models, the target quantum state, and the target problem while still offering rigorous performance guarantees for practical noise regimes.","Further connections are made between VCP and quantum error correction to produce one of the first protocols that combine quantum error correction and quantum error mitigation beyond concatenation.","We can remove all noise in the channel while paying only the same sampling cost as low-order purification, reaching beyond the standard bias-variance trade-off in quantum error mitigation.","Our protocol can also be adapted to key tasks in quantum networks like channel capacity activation and entanglement distribution."],"url":"http://arxiv.org/abs/2402.07866v1","category":"quant-ph"}
{"created":"2024-02-12 17:22:42","title":"Injecting Wiktionary to improve token-level contextual representations using contrastive learning","abstract":"While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist.","sentences":["While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019).","Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b).","In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary.","We also test how dimensionality reduction impacts the resulting contextual word embeddings.","We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set).","We achieve new SoTA result on the original WiC test set.","We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements.","We also observe improvements, although modest, for the semantic frame induction task.","Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist."],"url":"http://arxiv.org/abs/2402.07817v1","category":"cs.CL"}
{"created":"2024-02-12 17:06:01","title":"Flux qubit-based detector of microwave photons","abstract":"A theory of detection of microwave photons with a flux qubit-based detector is presented. We consider semiclassical approximation with the electromagnetic field being in a coherent state. Flux qubit is considered as a multilevel quantum system (qudit). By solving the Lindblad equation, we describe the time evolution of occupations of the qudit's levels for readout and reset stages of detection. When considering the reset stage, the time evolution is described by multiple avoided-level crossings, thus providing a multilevel Landau-Zener-Stuckelberg-Majorana (LZSM) problem. In addition to numerical calculations, we present an approximate analytical solution for the description of the reset stage dynamics based on the adiabatic-impulse approximation and rate equation approach. Our theory may be useful for the theoretical description of driven-dissipative dynamics of qudits, including applications such as single-photon detection.","sentences":["A theory of detection of microwave photons with a flux qubit-based detector is presented.","We consider semiclassical approximation with the electromagnetic field being in a coherent state.","Flux qubit is considered as a multilevel quantum system (qudit).","By solving the Lindblad equation, we describe the time evolution of occupations of the qudit's levels for readout and reset stages of detection.","When considering the reset stage, the time evolution is described by multiple avoided-level crossings, thus providing a multilevel Landau-Zener-Stuckelberg-Majorana (LZSM) problem.","In addition to numerical calculations, we present an approximate analytical solution for the description of the reset stage dynamics based on the adiabatic-impulse approximation and rate equation approach.","Our theory may be useful for the theoretical description of driven-dissipative dynamics of qudits, including applications such as single-photon detection."],"url":"http://arxiv.org/abs/2402.07801v1","category":"quant-ph"}
{"created":"2024-02-12 16:50:35","title":"\"Layer-by-layer\" Unsupervised Clustering of Statistically Relevant Fluctuations in Noisy Time-series Data of Complex Dynamical Systems","abstract":"Complex systems are typically characterized by intricate internal dynamics that are often hard to elucidate. Ideally, this requires methods that allow to detect and classify in unsupervised way the microscopic dynamical events occurring in the system. However, decoupling statistically relevant fluctuations from the internal noise remains most often non-trivial. Here we describe \"Onion Clustering\": a simple, iterative unsupervised clustering method that efficiently detects and classifies statistically relevant fluctuations in noisy time-series data. We demonstrate its efficiency by analyzing simulation and experimental trajectories of various systems with complex internal dynamics, ranging from the atomic- to the microscopic-scale, in- and out-of-equilibrium. The method is based on an iterative detect-classify-archive approach. In similar way as peeling the external (evident) layer of an onion reveals the internal hidden ones, the method performs a first detection and classification of the most populated dynamical environment in the system and of its characteristic noise. The signal of such dynamical cluster is then removed from the time-series data and the remaining part, cleared-out from its noise, is analyzed again. At every iteration, the detection of hidden dynamical sub-domains is facilitated by an increasing (and adaptive) relevance-to-noise ratio. The process iterates until no new dynamical domains can be uncovered, revealing, as an output, the number of clusters that can be effectively distinguished/classified in statistically robust way as a function of the time-resolution of the analysis. Onion Clustering is general and benefits from clear-cut physical interpretability. We expect that it will help analyzing a variety of complex dynamical systems and time-series data.","sentences":["Complex systems are typically characterized by intricate internal dynamics that are often hard to elucidate.","Ideally, this requires methods that allow to detect and classify in unsupervised way the microscopic dynamical events occurring in the system.","However, decoupling statistically relevant fluctuations from the internal noise remains most often non-trivial.","Here we describe \"Onion Clustering\": a simple, iterative unsupervised clustering method that efficiently detects and classifies statistically relevant fluctuations in noisy time-series data.","We demonstrate its efficiency by analyzing simulation and experimental trajectories of various systems with complex internal dynamics, ranging from the atomic- to the microscopic-scale, in- and out-of-equilibrium.","The method is based on an iterative detect-classify-archive approach.","In similar way as peeling the external (evident) layer of an onion reveals the internal hidden ones, the method performs a first detection and classification of the most populated dynamical environment in the system and of its characteristic noise.","The signal of such dynamical cluster is then removed from the time-series data and the remaining part, cleared-out from its noise, is analyzed again.","At every iteration, the detection of hidden dynamical sub-domains is facilitated by an increasing (and adaptive) relevance-to-noise ratio.","The process iterates until no new dynamical domains can be uncovered, revealing, as an output, the number of clusters that can be effectively distinguished/classified in statistically robust way as a function of the time-resolution of the analysis.","Onion Clustering is general and benefits from clear-cut physical interpretability.","We expect that it will help analyzing a variety of complex dynamical systems and time-series data."],"url":"http://arxiv.org/abs/2402.07786v3","category":"physics.data-an"}
{"created":"2024-02-12 15:57:31","title":"Task-conditioned adaptation of visual features in multi-task policy learning","abstract":"Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.","sentences":["Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules.","An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task.","Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning.","We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks.","We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations.","To this end, we propose a new optimization-based estimator.","We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy.","In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations."],"url":"http://arxiv.org/abs/2402.07739v1","category":"cs.CV"}
{"created":"2024-02-12 15:52:27","title":"Universal link predictor by In-context Learning","abstract":"Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph. Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training. Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches. Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs. Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models. UniLP is designed to autonomously identify connectivity patterns across diverse graphs, ready for immediate application to any unseen graph dataset without targeted training. We address the challenge of conflicting connectivity patterns-arising from the unique distributions of different graphs-through the implementation of In-context Learning (ICL). This approach allows UniLP to dynamically adjust to various target graphs based on contextual demonstrations, thereby avoiding negative transfer. Through rigorous experimentation, we demonstrate UniLP's effectiveness in adapting to new, unseen graphs at test time, showcasing its ability to perform comparably or even outperform parametric models that have been finetuned for specific datasets. Our findings highlight UniLP's potential to set a new standard in link prediction, combining the strengths of heuristic and parametric methods in a single, versatile framework.","sentences":["Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph.","Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training.","Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches.","Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs.","Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph.","In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models.","UniLP is designed to autonomously identify connectivity patterns across diverse graphs, ready for immediate application to any unseen graph dataset without targeted training.","We address the challenge of conflicting connectivity patterns-arising from the unique distributions of different graphs-through the implementation of In-context Learning (ICL).","This approach allows UniLP to dynamically adjust to various target graphs based on contextual demonstrations, thereby avoiding negative transfer.","Through rigorous experimentation, we demonstrate UniLP's effectiveness in adapting to new, unseen graphs at test time, showcasing its ability to perform comparably or even outperform parametric models that have been finetuned for specific datasets.","Our findings highlight UniLP's potential to set a new standard in link prediction, combining the strengths of heuristic and parametric methods in a single, versatile framework."],"url":"http://arxiv.org/abs/2402.07738v1","category":"cs.LG"}
{"created":"2024-02-12 15:39:21","title":"Distributed Observer Design over Directed Switching Topologies","abstract":"The distributed observer design problem holds significant importance in cases in which the output information of a system is decentralized across different subsystems. Each subsystem has a local observer and access to one part of the measurement outputs and information exchanged through communication networks. This paper focuses on the design of distributed observer with jointly connected directed switching networks. The problem presents challenges due to passive switching modes and the open-loop unboundedness that results from local observability. To overcome these challenges, we develop a network transformation mapping method whereby each local observer can classify itself into an independent subgraph based on independent judgment. Next, an observable decomposition and reorganization method is developed for the digraph case to ensure that each subgraph possesses independent dynamic properties. Asymptotic omniscience is then proven using a developed recursive proof method. This paper includes many previous results as special cases, because most are only suitable for undirected switching topologies or fast-switching cases. An adaptive coupling gain design is proposed to simplify the calculation and verification of conditions that guarantee asymptotic omniscience. Finally, simulation results with the power system show the validity of the developed theory.","sentences":["The distributed observer design problem holds significant importance in cases in which the output information of a system is decentralized across different subsystems.","Each subsystem has a local observer and access to one part of the measurement outputs and information exchanged through communication networks.","This paper focuses on the design of distributed observer with jointly connected directed switching networks.","The problem presents challenges due to passive switching modes and the open-loop unboundedness that results from local observability.","To overcome these challenges, we develop a network transformation mapping method whereby each local observer can classify itself into an independent subgraph based on independent judgment.","Next, an observable decomposition and reorganization method is developed for the digraph case to ensure that each subgraph possesses independent dynamic properties.","Asymptotic omniscience is then proven using a developed recursive proof method.","This paper includes many previous results as special cases, because most are only suitable for undirected switching topologies or fast-switching cases.","An adaptive coupling gain design is proposed to simplify the calculation and verification of conditions that guarantee asymptotic omniscience.","Finally, simulation results with the power system show the validity of the developed theory."],"url":"http://arxiv.org/abs/2402.07727v1","category":"math.DS"}
{"created":"2024-02-12 15:34:56","title":"LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation","abstract":"Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.","sentences":["Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources.","But it still faces challenges of resource consumption when scaling up to larger models.","Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem.","However, these efforts only analyzed parameter features to evaluate their importance.","Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model.","To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output.","We retain LoRA for important layers and the LoRA of the other layers share the same parameters.","Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop."],"url":"http://arxiv.org/abs/2402.07721v1","category":"cs.LG"}
{"created":"2024-02-12 15:26:37","title":"Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks","abstract":"Denial of service attacks pose a threat in constant growth. This is mainly due to their tendency to gain in sophistication, ease of implementation, obfuscation and the recent improvements in occultation of fingerprints. On the other hand, progress towards self-organizing networks, and the different techniques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, facilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network. In order to contribute to their development, in this paper, the use of artificial immune systems to mitigate denial of service attacks is proposed. The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment. These components are capable of identifying threats and reacting according to the behavior of the biological defense mechanisms in human beings. It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory. For their assessment, experiments with public domain datasets (KDD'99, CAIDA'07 and CAIDA'08) and simulations on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the tool DDoSIM were performed.","sentences":["Denial of service attacks pose a threat in constant growth.","This is mainly due to their tendency to gain in sophistication, ease of implementation, obfuscation and the recent improvements in occultation of fingerprints.","On the other hand, progress towards self-organizing networks, and the different techniques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, facilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network.","In order to contribute to their development, in this paper, the use of artificial immune systems to mitigate denial of service attacks is proposed.","The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment.","These components are capable of identifying threats and reacting according to the behavior of the biological defense mechanisms in human beings.","It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory.","For their assessment, experiments with public domain datasets (KDD'99, CAIDA'07 and CAIDA'08) and simulations on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the tool DDoSIM were performed."],"url":"http://arxiv.org/abs/2402.07714v1","category":"cs.CR"}
{"created":"2024-02-12 15:26:01","title":"Model Collapse Demystified: The Case of Regression","abstract":"In the era of large language models like ChatGPT, the phenomenon of \"model collapse\" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.","sentences":["In the era of large language models like ChatGPT, the phenomenon of \"model collapse\" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses.","In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses.","Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates.","We also propose a simple strategy based on adaptive regularization to mitigate model collapse.","Our theoretical results are validated with experiments."],"url":"http://arxiv.org/abs/2402.07712v1","category":"cs.LG"}
{"created":"2024-02-12 14:40:43","title":"AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer","abstract":"Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at https://github.com/sanjay-810/AYDIV2","sentences":["Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems.","Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras.","Besides, discrepancies in the two data representations further complicate fusion methods.","We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies.","AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion.","AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods.","Our code is publicly available at https://github.com/sanjay-810/AYDIV2"],"url":"http://arxiv.org/abs/2402.07680v1","category":"cs.CV"}
{"created":"2024-02-12 14:10:38","title":"A hybrid memetic-ANS optimization algorithm for the home health care and home care routing and re","abstract":"This paper addresses a realistic home health care and home care (HHC\\&HC) problem which has become increasingly complex in the face of demographic aging and post-COVID-19 disruptions. The HHC\\&HC sector, as the essential component of modern health care systems, faces unique challenges in efficiently scheduling and routing caregivers to meet the rising demand for home-based care services. Traditional approaches often fall short in addressing the dynamic nature of care requests, especially in accommodating new, same-day service requests without compromising scheduled visits. To tackle these issues, We define the problem as an HHC\\&HC routing and rescheduling problem with rejection of new customers (HHC\\&HCRRP-RNC), focusing on rescheduling for a single HHC\\&HC caregiver in response to new customer requests within a single period. This problem is a variant of both the single-machine reschedule problem and the orienteering problem with mandatory visits (OPMV), where certain nodes must be visited while others are optional. A mixed integer linear programming (MILP) model is developed to cater to two groups of customers: pre-scheduled existing customers and same-day service new customers. The model emphasized maintaining minimal disruptions to the original schedule for existing customers as a constraint, highlighting the balance between adhering to scheduled visits and accommodating new customers. A hybrid memetic-Adaptive Neighborhood Search (ANS) optimization algorithm is proposed to tackle the model. This approach aims to minimize operational costs and opportunity costs while enhancing service quality and patient satisfaction. Through computational experiments, our proposed algorithm demonstrates notable performance, offering significant improvements in both efficiency and robustness within the problem domain.","sentences":["This paper addresses a realistic home health care and home care (HHC\\&HC) problem which has become increasingly complex in the face of demographic aging and post-COVID-19 disruptions.","The HHC\\&HC sector, as the essential component of modern health care systems, faces unique challenges in efficiently scheduling and routing caregivers to meet the rising demand for home-based care services.","Traditional approaches often fall short in addressing the dynamic nature of care requests, especially in accommodating new, same-day service requests without compromising scheduled visits.","To tackle these issues, We define the problem as an HHC\\&HC routing and rescheduling problem with rejection of new customers (HHC\\&HCRRP-RNC), focusing on rescheduling for a single HHC\\&HC caregiver in response to new customer requests within a single period.","This problem is a variant of both the single-machine reschedule problem and the orienteering problem with mandatory visits (OPMV), where certain nodes must be visited while others are optional.","A mixed integer linear programming (MILP) model is developed to cater to two groups of customers: pre-scheduled existing customers and same-day service new customers.","The model emphasized maintaining minimal disruptions to the original schedule for existing customers as a constraint, highlighting the balance between adhering to scheduled visits and accommodating new customers.","A hybrid memetic-Adaptive Neighborhood Search (ANS) optimization algorithm is proposed to tackle the model.","This approach aims to minimize operational costs and opportunity costs while enhancing service quality and patient satisfaction.","Through computational experiments, our proposed algorithm demonstrates notable performance, offering significant improvements in both efficiency and robustness within the problem domain."],"url":"http://arxiv.org/abs/2402.07662v1","category":"math.OC"}
{"created":"2024-02-12 13:50:07","title":"Well-posedness for the NLS hierarchy","abstract":"We prove well-posedness for higher-order equations in the so-called NLS hierarchy (also known as part of the AKNS hierarchy) in almost critical Fourier-Lebesgue spaces and in modulation spaces. We show the $j$th equation in the hierarchy is locally well-posed for initial data in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{j-1}{r'}$ and $1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s = \\frac{j-1}{2}$ and $2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness results in Fourier-Lebesgue spaces shows optimality. Using the conserved quantities derived in Koch-Tataru (2018) we argue that the hierarchy equations are globally well-posed for data in $H^s(\\mathbb{R})$ for $s \\ge \\frac{j-1}{2}$.   Our arguments are based on the Fourier restriction norm method in Bourgain spaces adapted to our data spaces and bi- & trilinear refinements of Strichartz estimates.","sentences":["We prove well-posedness for higher-order equations in the so-called NLS hierarchy (also known as part of the AKNS hierarchy) in almost critical Fourier-Lebesgue spaces and in modulation spaces.","We show the $j$th equation in the hierarchy is locally well-posed for initial data in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{j-1}{r'}$ and $1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s = \\frac{j-1}{2}$ and $2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness results in Fourier-Lebesgue spaces shows optimality.","Using the conserved quantities derived in Koch-Tataru (2018)","we argue that the hierarchy equations are globally well-posed for data in $H^s(\\mathbb{R})$ for $s \\ge \\frac{j-1}{2}$.   ","Our arguments are based on the Fourier restriction norm method in Bourgain spaces adapted to our data spaces and bi- & trilinear refinements of Strichartz estimates."],"url":"http://arxiv.org/abs/2402.07652v1","category":"math.AP"}
{"created":"2024-02-12 13:46:30","title":"Spin orbit resonance cascade via core shell model. Application to Mercury and Ganymede","abstract":"We discuss a model describing the spin orbit resonance cascade. We assume that the primary has a two-layer (core-shell) structure: it is composed by a thin solid crust and an inner and heavier solid core that are interacting due to the presence of a fluid interface. We assume two sources of dissipation: a viscous one, depending on the relative angular velocity between core and crust and a tidal one, smaller than the first, due to the viscoelastic structure of the core. We show how these two sources of dissipation are needful for the capture in spin-orbit resonance. The crust and the core fall in resonance with different time scales if the viscous coupling between them is big enough. Finally, the tidal dissipation of the viscoelastic core, decreasing the eccentricity, brings the system out of the resonance in a third very long time scale. This mechanism of entry and exit from resonance ends in the $1:1$ stable state.","sentences":["We discuss a model describing the spin orbit resonance cascade.","We assume that the primary has a two-layer (core-shell) structure: it is composed by a thin solid crust and an inner and heavier solid core that are interacting due to the presence of a fluid interface.","We assume two sources of dissipation: a viscous one, depending on the relative angular velocity between core and crust and a tidal one, smaller than the first, due to the viscoelastic structure of the core.","We show how these two sources of dissipation are needful for the capture in spin-orbit resonance.","The crust and the core fall in resonance with different time scales if the viscous coupling between them is big enough.","Finally, the tidal dissipation of the viscoelastic core, decreasing the eccentricity, brings the system out of the resonance in a third very long time scale.","This mechanism of entry and exit from resonance ends in the $1:1$ stable state."],"url":"http://arxiv.org/abs/2402.07650v1","category":"math-ph"}
{"created":"2024-02-12 13:42:11","title":"GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants","abstract":"We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.","sentences":["We tackle the challenge of building real-world multimodal assistants for complex real-world tasks.","We describe the practicalities and challenges of developing and deploying","GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge.","Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency.","OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner.","For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns.","For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency.","Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge.","These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures."],"url":"http://arxiv.org/abs/2402.07647v1","category":"cs.IR"}
{"created":"2024-02-12 13:15:08","title":"Higher-order Connection Laplacians for Directed Simplicial Complexes","abstract":"Higher-order networks encode the many-body interactions existing in complex systems, such as the brain, protein complexes, and social interactions. Simplicial complexes are higher-order networks that allow a comprehensive investigation of the interplay between topology and dynamics. However, simplicial complexes have the limitation that they only capture undirected higher-order interactions while in real-world scenarios, often there is a need to introduce the direction of simplices, extending the popular notion of direction of edges. On graphs and networks the Magnetic Laplacian, a special case of Connection Laplacian, is becoming a popular operator to treat edge directionality. Here we tackle the challenge of treating directional simplicial complexes by formulating Higher-order Connection Laplacians taking into account the configurations induced by the simplices' directions. Specifically, we define all the Connection Laplacians of directed simplicial complexes of dimension two and we discuss the induced higher-order diffusion dynamics by considering instructive synthetic examples of simplicial complexes. The proposed higher-order diffusion processes can be adopted in real scenarios when we want to consider higher-order diffusion displaying non-trivial frustration effects due to conflicting directionalities of the incident simplices.","sentences":["Higher-order networks encode the many-body interactions existing in complex systems, such as the brain, protein complexes, and social interactions.","Simplicial complexes are higher-order networks that allow a comprehensive investigation of the interplay between topology and dynamics.","However, simplicial complexes have the limitation that they only capture undirected higher-order interactions while in real-world scenarios, often there is a need to introduce the direction of simplices, extending the popular notion of direction of edges.","On graphs and networks the Magnetic Laplacian, a special case of Connection Laplacian, is becoming a popular operator to treat edge directionality.","Here we tackle the challenge of treating directional simplicial complexes by formulating Higher-order Connection Laplacians taking into account the configurations induced by the simplices' directions.","Specifically, we define all the Connection Laplacians of directed simplicial complexes of dimension two","and we discuss the induced higher-order diffusion dynamics by considering instructive synthetic examples of simplicial complexes.","The proposed higher-order diffusion processes can be adopted in real scenarios when we want to consider higher-order diffusion displaying non-trivial frustration effects due to conflicting directionalities of the incident simplices."],"url":"http://arxiv.org/abs/2402.07631v1","category":"cs.SI"}
{"created":"2024-02-12 13:11:40","title":"On implicit and explicit representations for 1D distributed port-Hamiltonian systems","abstract":"First, two examples of 1D distributed port-Hamiltonian systems with dissipation, given in explicit (descriptor) form, are considered: the Dzekster model for the seepage of underground water and a nanorod model with non-local viscous damping. Implicit representations in Stokes-Lagrange subspaces are formulated. These formulations lead to modified Hamiltonian functions with spatial differential operators. The associated power balance equations are derived, together with the new boundary ports. Second, the port-Hamiltonian formulations for the Timoshenko and the Euler-Bernoulli beams are recalled, the latter being a flow-constrained version of the former. Implicit representations of these models in Stokes-Lagrange subspaces and corresponding power balance equations are derived. Bijective transformations are proposed between the explicit and implicit representations. It is proven these transformations commute with the flow-constraint projection operator.","sentences":["First, two examples of 1D distributed port-Hamiltonian systems with dissipation, given in explicit (descriptor) form, are considered: the Dzekster model for the seepage of underground water and a nanorod model with non-local viscous damping.","Implicit representations in Stokes-Lagrange subspaces are formulated.","These formulations lead to modified Hamiltonian functions with spatial differential operators.","The associated power balance equations are derived, together with the new boundary ports.","Second, the port-Hamiltonian formulations for the Timoshenko and the Euler-Bernoulli beams are recalled, the latter being a flow-constrained version of the former.","Implicit representations of these models in Stokes-Lagrange subspaces and corresponding power balance equations are derived.","Bijective transformations are proposed between the explicit and implicit representations.","It is proven these transformations commute with the flow-constraint projection operator."],"url":"http://arxiv.org/abs/2402.07628v1","category":"math.DS"}
{"created":"2024-02-12 12:59:40","title":"Fluctuation-dissipation relation in cosmic microwave background","abstract":"We study the fluctuation-dissipation relation for sound waves in the cosmic microwave background (CMB), employing effective field theory (EFT) for fluctuating hydrodynamics. Treating sound waves as the linear response to thermal radiation, we establish the fluctuation-dissipation relation within a cosmological framework. While dissipation is elucidated in established linear cosmological perturbation theory, the standard Boltzmann theory overlooks the associated noise, possibly contributing to inconsistencies in Lambda Cold Dark Matter ($\\Lambda$CDM) cosmology. This paper employs EFT for fluctuating hydrodynamics in cosmological perturbation theory, deriving sound wave noise. Notably, the long-time limit of the noise spectrum is independent of viscosity details, resembling a Brownian motion bounded in a harmonic potential. The net energy transfer between the sound wave system and the radiation environment reaches a balance within Hubble time, suggesting the thermal equilibrium of the sound waves themselves. The induced density power spectrum is characterized as white noise dependent on the inverse of the entropy density, which is negligibly small on the CMB scale. The energy density of the entire sound wave system scales as $a^{-4}$, akin to radiation. While the numerical factor is not determined in the present calculation, the back reaction of the sound wave system to the background radiation may not be negligible, serving as a potential source for various fitting issues in $\\Lambda$CDM cosmology.","sentences":["We study the fluctuation-dissipation relation for sound waves in the cosmic microwave background (CMB), employing effective field theory (EFT) for fluctuating hydrodynamics.","Treating sound waves as the linear response to thermal radiation, we establish the fluctuation-dissipation relation within a cosmological framework.","While dissipation is elucidated in established linear cosmological perturbation theory, the standard Boltzmann theory overlooks the associated noise, possibly contributing to inconsistencies in Lambda Cold Dark Matter ($\\Lambda$CDM) cosmology.","This paper employs EFT for fluctuating hydrodynamics in cosmological perturbation theory, deriving sound wave noise.","Notably, the long-time limit of the noise spectrum is independent of viscosity details, resembling a Brownian motion bounded in a harmonic potential.","The net energy transfer between the sound wave system and the radiation environment reaches a balance within Hubble time, suggesting the thermal equilibrium of the sound waves themselves.","The induced density power spectrum is characterized as white noise dependent on the inverse of the entropy density, which is negligibly small on the CMB scale.","The energy density of the entire sound wave system scales as $a^{-4}$, akin to radiation.","While the numerical factor is not determined in the present calculation, the back reaction of the sound wave system to the background radiation may not be negligible, serving as a potential source for various fitting issues in $\\Lambda$CDM cosmology."],"url":"http://arxiv.org/abs/2402.07623v1","category":"hep-th"}
{"created":"2024-02-12 12:02:13","title":"Resistive switching acceleration induced by thermal confinement","abstract":"Enhancing the switching speed of oxide-based memristive devices at a low voltage level is crucial for their use as non-volatile memory and their integration into emerging computing paradigms such as neuromorphic computing. Efforts to accelerate the switching speed often result in an energy tradeoff, leading to an increase of the minimum working voltage. In our study, we present an innovative solution: the introduction of a low thermal conductivity layer placed within the active electrode, which impedes the dissipation of heat generated during the switching process. The result is a notable acceleration in the switching speed of the memristive model system SrTiO$_{3}$ by a remarkable factor of 10$^{3}$, while preserving the integrity of the switching layer and the interfaces with the electrodes, rendering it adaptable to various filamentary memristive systems. The incorporation of HfO$_{2}$ or TaO$_{x}$ as heat-blocking layers not only streamlines the fabrication process, but also ensures compatibility with complementary metal-oxide-semiconductor technology.","sentences":["Enhancing the switching speed of oxide-based memristive devices at a low voltage level is crucial for their use as non-volatile memory and their integration into emerging computing paradigms such as neuromorphic computing.","Efforts to accelerate the switching speed often result in an energy tradeoff, leading to an increase of the minimum working voltage.","In our study, we present an innovative solution: the introduction of a low thermal conductivity layer placed within the active electrode, which impedes the dissipation of heat generated during the switching process.","The result is a notable acceleration in the switching speed of the memristive model system SrTiO$_{3}$ by a remarkable factor of 10$^{3}$, while preserving the integrity of the switching layer and the interfaces with the electrodes, rendering it adaptable to various filamentary memristive systems.","The incorporation of HfO$_{2}$ or TaO$_{x}$ as heat-blocking layers not only streamlines the fabrication process, but also ensures compatibility with complementary metal-oxide-semiconductor technology."],"url":"http://arxiv.org/abs/2402.07603v1","category":"physics.app-ph"}
{"created":"2024-02-12 11:58:41","title":"Interactive singing melody extraction based on active adaptation","abstract":"Extraction of predominant pitch from polyphonic audio is one of the fundamental tasks in the field of music information retrieval and computational musicology. To accomplish this task using machine learning, a large amount of labeled audio data is required to train the model. However, a classical model pre-trained on data from one domain (source), e.g., songs of a particular singer or genre, may not perform comparatively well in extracting melody from other domains (target). The performance of such models can be boosted by adapting the model using very little annotated data from the target domain. In this work, we propose an efficient interactive melody adaptation method. Our method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability. The annotations are used by the model to adapt itself to the target domain using meta-learning. Our method also provides a novel meta-learning approach that handles class imbalance, i.e., a few representative samples from a few classes are available for adaptation in the target domain. Experimental results show that the proposed method outperforms other adaptive melody extraction baselines. The proposed method is model-agnostic and hence can be applied to other non-adaptive melody extraction models to boost their performance. Also, we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio files of about 6.86 hours of duration intended for singing melody extraction tasks.","sentences":["Extraction of predominant pitch from polyphonic audio is one of the fundamental tasks in the field of music information retrieval and computational musicology.","To accomplish this task using machine learning, a large amount of labeled audio data is required to train the model.","However, a classical model pre-trained on data from one domain (source), e.g., songs of a particular singer or genre, may not perform comparatively well in extracting melody from other domains (target).","The performance of such models can be boosted by adapting the model using very little annotated data from the target domain.","In this work, we propose an efficient interactive melody adaptation method.","Our method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability.","The annotations are used by the model to adapt itself to the target domain using meta-learning.","Our method also provides a novel meta-learning approach that handles class imbalance, i.e., a few representative samples from a few classes are available for adaptation in the target domain.","Experimental results show that the proposed method outperforms other adaptive melody extraction baselines.","The proposed method is model-agnostic and hence can be applied to other non-adaptive melody extraction models to boost their performance.","Also, we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio files of about 6.86 hours of duration intended for singing melody extraction tasks."],"url":"http://arxiv.org/abs/2402.07599v1","category":"eess.AS"}
{"created":"2024-02-12 11:52:21","title":"Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription","abstract":"State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations. Despite their efficacy, these approaches imply challenges related to scalability and limitations. This paper presents the Sheet Music Transformer, the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies. Our model employs a Transformer-based image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images. Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively. The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription.","sentences":["State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations.","Despite their efficacy, these approaches imply challenges related to scalability and limitations.","This paper presents the Sheet Music Transformer, the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies.","Our model employs a Transformer-based image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images.","Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively.","The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription."],"url":"http://arxiv.org/abs/2402.07596v1","category":"cs.CV"}
{"created":"2024-02-12 11:35:25","title":"Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning","abstract":"In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of fairness. In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous clustering of models over time. The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance fairness in machine learning.","sentences":["In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes.","However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard.","Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable.","Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness.","One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of fairness.","In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous clustering of models over time.","The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance fairness in machine learning."],"url":"http://arxiv.org/abs/2402.07586v1","category":"cs.LG"}
{"created":"2024-02-12 11:12:27","title":"Love numbers and Love symmetries for $p$-form and gravitational perturbations of higher-dimensional spherically symmetric black holes","abstract":"The static Love numbers of four-dimensional asymptotically flat, isolated, general-relativistic black holes are known to be identically vanishing. The Love symmetry proposal suggests that such vanishings are addressed by selection rules following from the emergence of an enhanced $\\text{SL}(2,\\mathbb{R})$ (``Love'') symmetry in the near-zone region; more specifically, it is the fact that the black hole perturbations belong to a highest-weight representation of this near-zone $\\text{SL}(2,\\mathbb{R})$ symmetry, rather than the existence of the Love symmetry itself, that outputs the vanishings of the corresponding Love numbers. In higher spacetime dimensions, some towers of magic zeroes with regards to the black hole response problem have also been reported for scalar, electromagnetic and gravitational perturbations of the Schwarzschild-Tangherlini black hole. Here, we extend these results by supplementing with $p$-form perturbations of the Schwarzschild-Tangherlini black hole. We furthermore analytically extract the static Love numbers and the leading order dissipation numbers associated with spin-$0$ scalar and spin-$2$ tensor-type tidal perturbations of the higher-dimensional Reissner-Nordstr\\\"om black hole. We find that Love symmetries exist and that the vanishings of the static Love numbers are captured by representation theory arguments even for these higher spin perturbations of the higher-dimensional spherically symmetric black holes of General Relativity. Interestingly, these near-zone $\\text{SL}(2,\\mathbb{R})$ structures acquire extensions to Witt algebras. Our setup allows to also study the $p$-form response problem of a static spherically symmetric black hole in a generic theory of gravity. We perform explicit computations for some black holes in the presence of string-theoretic corrections and investigate under what geometric conditions Love symmetries emerge in the near-zone.","sentences":["The static Love numbers of four-dimensional asymptotically flat, isolated, general-relativistic black holes are known to be identically vanishing.","The Love symmetry proposal suggests that such vanishings are addressed by selection rules following from the emergence of an enhanced $\\text{SL}(2,\\mathbb{R})$ (``Love'') symmetry in the near-zone region; more specifically, it is the fact that the black hole perturbations belong to a highest-weight representation of this near-zone $\\text{SL}(2,\\mathbb{R})$ symmetry, rather than the existence of the Love symmetry itself, that outputs the vanishings of the corresponding Love numbers.","In higher spacetime dimensions, some towers of magic zeroes with regards to the black hole response problem have also been reported for scalar, electromagnetic and gravitational perturbations of the Schwarzschild-Tangherlini black hole.","Here, we extend these results by supplementing with $p$-form perturbations of the Schwarzschild-Tangherlini black hole.","We furthermore analytically extract the static Love numbers and the leading order dissipation numbers associated with spin-$0$ scalar and spin-$2$ tensor-type tidal perturbations of the higher-dimensional Reissner-Nordstr\\\"om black hole.","We find that Love symmetries exist and that the vanishings of the static Love numbers are captured by representation theory arguments even for these higher spin perturbations of the higher-dimensional spherically symmetric black holes of General Relativity.","Interestingly, these near-zone $\\text{SL}(2,\\mathbb{R})$ structures acquire extensions to Witt algebras.","Our setup allows to also study the $p$-form response problem of a static spherically symmetric black hole in a generic theory of gravity.","We perform explicit computations for some black holes in the presence of string-theoretic corrections and investigate under what geometric conditions Love symmetries emerge in the near-zone."],"url":"http://arxiv.org/abs/2402.07574v1","category":"hep-th"}
{"created":"2024-02-12 10:39:17","title":"Digital Twins Below the Surface: Enhancing Underwater Teleoperation","abstract":"Subsea exploration, inspection, and intervention operations heavily rely on remotely operated vehicles (ROVs). However, the inherent complexity of the underwater environment presents significant challenges to the operators of these vehicles. This paper delves into the challenges associated with navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced situational awareness and heightened teleoperator workload. To address these challenges, we introduce an underwater Digital Twin (DT) system designed to enhance underwater teleoperation, enable autonomous navigation, support system monitoring, and facilitate system testing through simulation. Our approach involves a dynamic representation of the underwater robot and its environment using desktop virtual reality, as well as the integration of mapping, localization, path planning and simulation capabilities within the DT system. Our research demonstrates the system's adaptability, versatility and feasibility, highlighting significant challenges and, in turn, improving the teleoperators' situational awareness and reducing their workload.","sentences":["Subsea exploration, inspection, and intervention operations heavily rely on remotely operated vehicles (ROVs).","However, the inherent complexity of the underwater environment presents significant challenges to the operators of these vehicles.","This paper delves into the challenges associated with navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced situational awareness and heightened teleoperator workload.","To address these challenges, we introduce an underwater Digital Twin (DT) system designed to enhance underwater teleoperation, enable autonomous navigation, support system monitoring, and facilitate system testing through simulation.","Our approach involves a dynamic representation of the underwater robot and its environment using desktop virtual reality, as well as the integration of mapping, localization, path planning and simulation capabilities within the DT system.","Our research demonstrates the system's adaptability, versatility and feasibility, highlighting significant challenges and, in turn, improving the teleoperators' situational awareness and reducing their workload."],"url":"http://arxiv.org/abs/2402.07556v1","category":"cs.RO"}
{"created":"2024-02-12 10:37:54","title":"Thermodynamically consistent modelling of viscoelastic solids under finite strain","abstract":"The present article is concerned with modelling the viscoelastic behavior of Polydimethylsiloxane (PDMS) in large-strain regime. Starting from the basic principles of thermodynamics, an incremental variational formulation is derived. Within this model, the free energy density and dissipation function determine elastic and viscous properties of the solid. The main contribution of this paper is the estimation of the parameters in the proposed phenomenological model from measurements conducted on PDMS samples. This non-linear material model simplifies to a Prony-series representation in frequency domain in case of small deformations. The coefficients of this Prony-series are detected from dynamical temperature mechanical analysis measurements. Time-temperature superposition allows to combine measurements at different temperatures, such that a sufficiently large frequency range is available for subsequent fitting of Prony-parameters. A set of material parameters is thus provided. The incremental variational formulation directly lends itself to finite element discretization, where an efficient and stable choice of elements is proposed for radially symmetric problems. This formulation allows to verify the proposed model against experimental data gained in ball-drop experiments.","sentences":["The present article is concerned with modelling the viscoelastic behavior of Polydimethylsiloxane (PDMS) in large-strain regime.","Starting from the basic principles of thermodynamics, an incremental variational formulation is derived.","Within this model, the free energy density and dissipation function determine elastic and viscous properties of the solid.","The main contribution of this paper is the estimation of the parameters in the proposed phenomenological model from measurements conducted on PDMS samples.","This non-linear material model simplifies to a Prony-series representation in frequency domain in case of small deformations.","The coefficients of this Prony-series are detected from dynamical temperature mechanical analysis measurements.","Time-temperature superposition allows to combine measurements at different temperatures, such that a sufficiently large frequency range is available for subsequent fitting of Prony-parameters.","A set of material parameters is thus provided.","The incremental variational formulation directly lends itself to finite element discretization, where an efficient and stable choice of elements is proposed for radially symmetric problems.","This formulation allows to verify the proposed model against experimental data gained in ball-drop experiments."],"url":"http://arxiv.org/abs/2402.07555v1","category":"physics.app-ph"}
{"created":"2024-02-12 10:00:10","title":"Semantic Data for Humanities and Social Sciences (SDHSS): an Ecosystem of CIDOC CRM Extensions for Research Data Production and Reuse","abstract":"Given the challenge of giant knowledge graphs created by major eco-nomic actors, which could virtually replace research in the Humani-ties and Social Sciences (HSS) in responding to public concerns, thequestion arises of how to increase the value of research data throughtheir publication and networking, applying the FAIR principles. Bothan epistemological and a semantic analysis show that the most rel-evant part of research data is factual information, understood as arepresentation of the objects observed by the scientific disciplines,their properties and their relationships.This rich universe of information will be made understandable andtherefore reusable through the application of foundational ontologiesand a methodology based on the distinction between different levelsof abstraction, allowing the collective development of one or moreshared and reusable domain ontologies. This vision is being carriedout around the CIDOC CRM, as core ontology, and Semantic Datafor Humanities and Social Sciences (SDHSS), as a high-level exten-sion of it, as well as an ecosystem of sub-domain extensions that canbe easily managed through the ontome.net application. This willresult in an interoperability that is semantically richer than the sim-ple alignment of ontologies and less costly in terms of resources, andabove all adapted to the scientific and humanistic project of the HSS.","sentences":["Given the challenge of giant knowledge graphs created by major eco-nomic actors, which could virtually replace research in the Humani-ties and Social Sciences (HSS) in responding to public concerns, thequestion arises of how to increase the value of research data throughtheir publication and networking, applying the FAIR principles.","Bothan epistemological and a semantic analysis show that the most rel-evant part of research data is factual information, understood as arepresentation of the objects observed by the scientific disciplines,their properties and their relationships.","This rich universe of information will be made understandable andtherefore reusable through the application of foundational ontologiesand a methodology based on the distinction between different levelsof abstraction, allowing the collective development of one or moreshared and reusable domain ontologies.","This vision is being carriedout around the CIDOC CRM, as core ontology, and Semantic Datafor Humanities and Social Sciences (SDHSS), as a high-level exten-sion of it, as well as an ecosystem of sub-domain extensions that canbe easily managed through the ontome.net application.","This willresult in an interoperability that is semantically richer than the sim-ple alignment of ontologies and less costly in terms of resources, andabove all adapted to the scientific and humanistic project of the HSS."],"url":"http://arxiv.org/abs/2402.07531v1","category":"cs.IT"}
{"created":"2024-02-12 09:41:00","title":"MAFIA: Multi-Adapter Fused Inclusive LanguAge Models","abstract":"Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a pretrained language model across multiple dimensions. Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach.","sentences":["Pretrained Language Models (PLMs) are widely used in NLP for various tasks.","Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases.","However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion.","Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task.","In this work, we aim to modularly debias a pretrained language model across multiple dimensions.","Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA).","We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way.","We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously.","An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach."],"url":"http://arxiv.org/abs/2402.07519v1","category":"cs.CL"}
{"created":"2024-02-12 09:31:43","title":"Global subelliptic estimates for geometric Kramers-Fokker-Planck operators on closed manifolds","abstract":"In this article we reconsider the proof of subelliptic estimates for Geometric Kramers-Fokker-Planck operators, a class which includes Bismut's hypoelliptic Laplacian, when the base manifold is closed (no boundary). The method is significantly different from the ones proposed by Bismut-Lebeau in [BiLe] and Lebeau in [Leb1] and [Leb2]. As a new result we are able to prove maximal subelliptic estimates with some control of the constants in the two asymptotic regimes of high (b $\\rightarrow$ 0) and low (b $\\rightarrow$ +$\\infty$) friction. After a dyadic partition in the momentum variable, the analysis is essentially local in the position variable, contrary to the microlocal reduction techniques of the previous works. In particular this method will be easier to adapt on manifolds with boundaries. A byproduct of our analysis is the introduction of a very convenient double exponent Sobolev scale associated with globally defined differential operators. Applications of this convenient parameter dependent functional analysis to accurate spectral problems, in particular for Bismut's hypoelliptic Laplacian with all its specific geometry, is deferred to subsequent works.","sentences":["In this article we reconsider the proof of subelliptic estimates for Geometric Kramers-Fokker-Planck operators, a class which includes Bismut's hypoelliptic Laplacian, when the base manifold is closed (no boundary).","The method is significantly different from the ones proposed by Bismut-Lebeau in [BiLe] and Lebeau in [Leb1] and [Leb2].","As a new result we are able to prove maximal subelliptic estimates with some control of the constants in the two asymptotic regimes of high (b $\\rightarrow$ 0) and low (b $\\rightarrow$ +$\\infty$) friction.","After a dyadic partition in the momentum variable, the analysis is essentially local in the position variable, contrary to the microlocal reduction techniques of the previous works.","In particular this method will be easier to adapt on manifolds with boundaries.","A byproduct of our analysis is the introduction of a very convenient double exponent Sobolev scale associated with globally defined differential operators.","Applications of this convenient parameter dependent functional analysis to accurate spectral problems, in particular for Bismut's hypoelliptic Laplacian with all its specific geometry, is deferred to subsequent works."],"url":"http://arxiv.org/abs/2402.07511v1","category":"math.AP"}
{"created":"2024-02-12 08:47:53","title":"An elementary approach to mixing and dissipation enhancement by transport noise","abstract":"We investigate the mixing properties of solutions to the stochastic transport equation $d u= \\circ d W \\cdot\\nabla u$, where the driving noise $W(t,x)$ is white in time, colored and divergence-free in space. Furthermore, we prove the dissipation enhancement in the presence of a small viscous term. Applying our results, we also derive the mixing properties for a regularized stochastic 2D Euler equation.","sentences":["We investigate the mixing properties of solutions to the stochastic transport equation $d u= \\circ d","W \\cdot\\nabla u$, where the driving noise $W(t,x)$ is white in time, colored and divergence-free in space.","Furthermore, we prove the dissipation enhancement in the presence of a small viscous term.","Applying our results, we also derive the mixing properties for a regularized stochastic 2D Euler equation."],"url":"http://arxiv.org/abs/2402.07484v1","category":"math.PR"}
{"created":"2024-02-14 18:59:27","title":"Auditing Private Prediction","abstract":"Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of analgorithm, while empirical auditing establishes a practical lower bound. Auditing techniques exist forDP training algorithms. However machine learning can also be made private at inference. We propose thefirst framework for auditing private prediction where we instantiate adversaries with varying poisoningand query capabilities. This enables us to study the privacy leakage of four private prediction algorithms:PATE [Papernot et al., 2016], CaPC [Choquette-Choo et al., 2020], PromptPATE [Duan et al., 2023],and Private-kNN [Zhu et al., 2020]. To conduct our audit, we introduce novel techniques to empiricallyevaluate privacy leakage in terms of Renyi DP. Our experiments show that (i) the privacy analysis ofprivate prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacyleakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than thosewith full control.","sentences":["Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of analgorithm, while empirical auditing establishes a practical lower bound.","Auditing techniques exist forDP training algorithms.","However machine learning can also be made private at inference.","We propose thefirst framework for auditing private prediction where we instantiate adversaries with varying poisoningand query capabilities.","This enables us to study the privacy leakage of four private prediction algorithms:PATE","[Papernot et al., 2016], CaPC [Choquette-Choo et al., 2020], PromptPATE","[Duan et al., 2023],and Private-kNN [Zhu et al., 2020].","To conduct our audit, we introduce novel techniques to empiricallyevaluate privacy leakage in terms of Renyi DP.","Our experiments show that (i) the privacy analysis ofprivate prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacyleakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than thosewith full control."],"url":"http://arxiv.org/abs/2402.09403v1","category":"cs.CR"}
{"created":"2024-02-14 17:18:15","title":"YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture Detection","abstract":"Wrist trauma and even fractures occur frequently in daily life, particularly among children who account for a significant proportion of fracture cases. Before performing surgery, surgeons often request patients to undergo X-ray imaging first and prepare for it based on the analysis of the radiologist. With the development of neural networks, You Only Look Once (YOLO) series models have been widely used in fracture detection as computer-assisted diagnosis (CAD). In 2023, Ultralytics presented the latest version of the YOLO models, which has been employed for detecting fractures across various parts of the body. Attention mechanism is one of the hottest methods to improve the model performance. This research work proposes YOLOv8-AM, which incorporates the attention mechanism into the original YOLOv8 architecture. Specifically, we respectively employ four attention modules, Convolutional Block Attention Module (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention (ECA), and Shuffle Attention (SA), to design the improved models and train them on GRAZPEDWRI-DX dataset. Experimental results demonstrate that the mean Average Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock + CBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the state-of-the-art (SOTA) performance. Conversely, YOLOv8-AM model incorporating GAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement. Therefore, we combine ResBlock and GAM, introducing ResGAM to design another new YOLOv8-AM model, whose mAP 50 value is increased to 65.0%.","sentences":["Wrist trauma and even fractures occur frequently in daily life, particularly among children who account for a significant proportion of fracture cases.","Before performing surgery, surgeons often request patients to undergo X-ray imaging first and prepare for it based on the analysis of the radiologist.","With the development of neural networks, You Only Look Once (YOLO) series models have been widely used in fracture detection as computer-assisted diagnosis (CAD).","In 2023, Ultralytics presented the latest version of the YOLO models, which has been employed for detecting fractures across various parts of the body.","Attention mechanism is one of the hottest methods to improve the model performance.","This research work proposes YOLOv8-AM, which incorporates the attention mechanism into the original YOLOv8 architecture.","Specifically, we respectively employ four attention modules, Convolutional Block Attention Module (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention (ECA), and Shuffle Attention (SA), to design the improved models and train them on GRAZPEDWRI-DX dataset.","Experimental results demonstrate that the mean Average Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock + CBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the state-of-the-art (SOTA) performance.","Conversely, YOLOv8-AM model incorporating GAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement.","Therefore, we combine ResBlock and GAM, introducing ResGAM to design another new YOLOv8-AM model, whose mAP 50 value is increased to 65.0%."],"url":"http://arxiv.org/abs/2402.09329v1","category":"cs.CV"}
{"created":"2024-02-14 17:16:39","title":"PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames in Autonomous Driving Environments","abstract":"Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames. However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution. Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored. To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels. The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation. With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes. Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs. Our approach implementation and the pre-trained models are available at https://github.com/biter0088/pc-nerf.","sentences":["Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames.","However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution.","Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored.","To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF).","Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels.","The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation.","With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes.","Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs.","Our approach implementation and the pre-trained models are available at https://github.com/biter0088/pc-nerf."],"url":"http://arxiv.org/abs/2402.09325v1","category":"cs.CV"}
{"created":"2024-02-14 17:03:04","title":"Mixture to Mixture: Leveraging Close-talk Mixtures as Weak-supervision for Speech Separation","abstract":"We propose mixture to mixture (M2M) training, a weakly-supervised neural speech separation algorithm that leverages close-talk mixtures as a weak supervision for training discriminative models to separate far-field mixtures. Our idea is that, for a target speaker, its close-talk mixture has a much higher signal-to-noise ratio (SNR) of the target speaker than any far-field mixtures, and hence could be utilized to design a weak supervision for separation. To realize this, at each training step we feed a far-field mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, and, for each of considered close-talk and far-field microphones, we linearly filter the DNN estimates and optimize a loss so that the filtered estimates of all the speakers can sum up to the mixture captured by each of the considered microphones. Evaluation results on a 2-speaker separation task in simulated reverberant conditions show that M2M can effectively leverage close-talk mixtures as a weak supervision for separating far-field mixtures.","sentences":["We propose mixture to mixture (M2M) training, a weakly-supervised neural speech separation algorithm that leverages close-talk mixtures as a weak supervision for training discriminative models to separate far-field mixtures.","Our idea is that, for a target speaker, its close-talk mixture has a much higher signal-to-noise ratio (SNR) of the target speaker than any far-field mixtures, and hence could be utilized to design a weak supervision for separation.","To realize this, at each training step we feed a far-field mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, and, for each of considered close-talk and far-field microphones, we linearly filter the DNN estimates and optimize a loss so that the filtered estimates of all the speakers can sum up to the mixture captured by each of the considered microphones.","Evaluation results on a 2-speaker separation task in simulated reverberant conditions show that M2M can effectively leverage close-talk mixtures as a weak supervision for separating far-field mixtures."],"url":"http://arxiv.org/abs/2402.09313v1","category":"eess.AS"}
{"created":"2024-02-14 15:54:55","title":"Transformers, parallel computation, and logarithmic depth","abstract":"We show that a constant number of self-attention layers can efficiently simulate, and be simulated by, a constant number of communication rounds of Massively Parallel Computation. As a consequence, we show that logarithmic depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub-quadratic transformer approximations. We thus establish parallelism as a key distinguishing property of transformers.","sentences":["We show that a constant number of self-attention layers can efficiently simulate, and be simulated by, a constant number of communication rounds of Massively Parallel Computation.","As a consequence, we show that logarithmic depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub-quadratic transformer approximations.","We thus establish parallelism as a key distinguishing property of transformers."],"url":"http://arxiv.org/abs/2402.09268v1","category":"cs.LG"}
{"created":"2024-02-14 15:27:53","title":"Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives","abstract":"Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss. During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.","sentences":["Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks.","Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss.","During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance.","In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling.","We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples.","Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance."],"url":"http://arxiv.org/abs/2402.09239v1","category":"cs.LG"}
{"created":"2024-02-14 15:16:35","title":"Invariant conformal Killing forms on almost abelian Lie groups","abstract":"We describe completely conformal Killing or conformal Killing-Yano (CKY) $p$-forms on almost abelian metric Lie algebras. In particular we prove that if a $n$-dimensional almost abelian metric Lie algebra admits a non-parallel CKY $p$-form, then $p=1$ or $p=n-1$. In other words, any CKY $p$-form on a metric almost abelian Lie algebra is parallel for $2\\leq p\\leq n-2$. Moreover, we characterize almost abelian Lie algebras admitting non-parallel CKY $p$-forms, and we classify all Lie algebras with this property up to dimension $5$, distinguishing also those cases where the associated simply connected Lie group admits lattices.","sentences":["We describe completely conformal Killing or conformal Killing-Yano (CKY)","$p$-forms on almost abelian metric Lie algebras.","In particular we prove that if a $n$-dimensional almost abelian metric Lie algebra admits a non-parallel CKY $p$-form, then $p=1$ or $p=n-1$. In other words, any CKY $p$-form on a metric almost abelian Lie algebra is parallel for $2\\leq p\\leq","n-2$. Moreover, we characterize almost abelian Lie algebras admitting non-parallel CKY $p$-forms, and we classify all Lie algebras with this property up to dimension $5$, distinguishing also those cases where the associated simply connected Lie group admits lattices."],"url":"http://arxiv.org/abs/2402.09229v1","category":"math.DG"}
{"created":"2024-02-14 15:10:37","title":"Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks","abstract":"This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin. For both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set. For square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin. Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points.","sentences":["This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin.","For both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set.","For square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin.","Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points."],"url":"http://arxiv.org/abs/2402.09226v1","category":"cs.LG"}
{"created":"2024-02-14 15:06:44","title":"UV-complete 4-derivative scalar field theory","abstract":"A scalar field theory with 4-derivative kinetic terms and 4-derivative cubic and quartic couplings is presented as a proxy for quantum quadratic gravity (QQG). The scalar theory is renormalizable and asymptotically free and the remaining key issue is unitarity, or more precisely positivity, just as it is in QQG. We have extended calculations for the optical theorem and for a differential cross section, both in the high energy limit, to show how positivity constrains the theory. The results also show how it is that differential cross sections can have good high energy behavior. Finally we use the scalar theory to extend the Stuckelberg theory of a massive U(1) gauge boson to a renormalizable theory of a self-interacting gauge boson.","sentences":["A scalar field theory with 4-derivative kinetic terms and 4-derivative cubic and quartic couplings is presented as a proxy for quantum quadratic gravity (QQG).","The scalar theory is renormalizable and asymptotically free and the remaining key issue is unitarity, or more precisely positivity, just as it is in QQG.","We have extended calculations for the optical theorem and for a differential cross section, both in the high energy limit, to show how positivity constrains the theory.","The results also show how it is that differential cross sections can have good high energy behavior.","Finally we use the scalar theory to extend the Stuckelberg theory of a massive U(1) gauge boson to a renormalizable theory of a self-interacting gauge boson."],"url":"http://arxiv.org/abs/2402.09223v1","category":"hep-th"}
{"created":"2024-02-14 13:17:56","title":"On the Thomas-Fermi model: Gabor J. Kalman's contribution and numerical approximations","abstract":"In this article, we would like to pay tribute to Gabor Kalman, outlining his contribution to a model widely used in dense plasma physics: the high-temperature Thomas-Fermi model. The approach of Ruoxian Ying and Kalman relies on the separation of the bound and free electrons, a physically reasonable definition of the bound electrons, a description of the source density in the Poisson equation through the electron-ion and ion-ion pair correlation functions and a determination of the degree of ionization from the minimization of the total free energy. We also report on different approximations of the function $\\Phi$, which is a cornerstone of the original Thomas-Femi model.","sentences":["In this article, we would like to pay tribute to Gabor Kalman, outlining his contribution to a model widely used in dense plasma physics: the high-temperature Thomas-Fermi model.","The approach of Ruoxian Ying and Kalman relies on the separation of the bound and free electrons, a physically reasonable definition of the bound electrons, a description of the source density in the Poisson equation through the electron-ion and ion-ion pair correlation functions and a determination of the degree of ionization from the minimization of the total free energy.","We also report on different approximations of the function $\\Phi$, which is a cornerstone of the original Thomas-Femi model."],"url":"http://arxiv.org/abs/2402.09157v1","category":"physics.plasm-ph"}
{"created":"2024-02-14 12:26:10","title":"1D stochastic pressure equation with log-correlated Gaussian coefficients","abstract":"We study unique solvability for one dimensional stochastic pressure equation with diffusion coefficient given by the Wick exponential of log-correlated Gaussian fields. We prove well-posedness for Dirichlet, Neumann and periodic boundary data, and the initial value problem, covering the cases of both the Wick renormalization of the diffusion and of point-wise multiplication. We provide explicit representations for the solutions in both cases, characterized by the $S$-transform and the Gaussian multiplicative chaos measure.","sentences":["We study unique solvability for one dimensional stochastic pressure equation with diffusion coefficient given by the Wick exponential of log-correlated Gaussian fields.","We prove well-posedness for Dirichlet, Neumann and periodic boundary data, and the initial value problem, covering the cases of both the Wick renormalization of the diffusion and of point-wise multiplication.","We provide explicit representations for the solutions in both cases, characterized by the $S$-transform and the Gaussian multiplicative chaos measure."],"url":"http://arxiv.org/abs/2402.09127v1","category":"math.PR"}
{"created":"2024-02-14 11:56:03","title":"Non-Volatile Analog Control and Reconfiguration of a Vortex Nano-Oscillator Frequency","abstract":"Magnetic tunnel junctions are nanoscale devices which have recently attracted interested in the context of frequency multiplexed spintronic neural networks, due to their interesting dynamical properties, which are defined during the fabrication process, and depend on the material parameters and geometry. This paper proposes an approach to extending the functionality of a standard magnetic tunnel junction (MTJ) by introducing an additional ferromagnet/antiferromagnet (FM/AFM) storage layer (SL) vertically integrated with the standard Vortex MTJ stack into the nanopillar. The magnetostatic field created by this storage layer acts on the free layer and can be used to change its static and dynamic properties. To tune the magnitude and direction of this magnetostatic field, magnetic reconfiguration is carried out through a thermally assisted switching mechanism using a voltage pulse that heats the AFM layer in the SL above the Neel temperature in the presence of an external field. It is experimentally shown that using an MTJ based on a 600 nm diameter nanopillar with a vortex in the free layer, reconfiguration of the SL allows to continuously change the core precession frequency in the 15 MHz range. The reconfigurable analogue storage layer locally affects both the static and dynamic properties of the MTJ free layer, demonstrating vertical 3D integration of additional functionalities into a single MTJ nanopillar.","sentences":["Magnetic tunnel junctions are nanoscale devices which have recently attracted interested in the context of frequency multiplexed spintronic neural networks, due to their interesting dynamical properties, which are defined during the fabrication process, and depend on the material parameters and geometry.","This paper proposes an approach to extending the functionality of a standard magnetic tunnel junction (MTJ) by introducing an additional ferromagnet/antiferromagnet (FM/AFM) storage layer (SL) vertically integrated with the standard Vortex MTJ stack into the nanopillar.","The magnetostatic field created by this storage layer acts on the free layer and can be used to change its static and dynamic properties.","To tune the magnitude and direction of this magnetostatic field, magnetic reconfiguration is carried out through a thermally assisted switching mechanism using a voltage pulse that heats the AFM layer in the SL above the Neel temperature in the presence of an external field.","It is experimentally shown that using an MTJ based on a 600 nm diameter nanopillar with a vortex in the free layer, reconfiguration of the SL allows to continuously change the core precession frequency in the 15 MHz range.","The reconfigurable analogue storage layer locally affects both the static and dynamic properties of the MTJ free layer, demonstrating vertical 3D integration of additional functionalities into a single MTJ nanopillar."],"url":"http://arxiv.org/abs/2402.09114v1","category":"physics.app-ph"}
{"created":"2024-02-14 11:11:33","title":"Vapor equilibrium models of accreting rocky planets demonstrate direct core growth by pebble accretion","abstract":"The gaseous envelope of an accreting rocky planet becomes hot enough to sublimate silicates and other refractory minerals.   % aims heading (mandatory) For this work, we studied the effect of the resulting envelope enrichment with a heavy vapor species on the composition and temperature of the envelope. For simplification, we used the gas-phase molecule SiO to represent the sublimation of silicate material. We solved the equilibrium structure equations in 1D for planets in the mass range of $0.1$ to $3\\,M_\\oplus$. The convective stability criterion was extended to take the stabilizing effect of the condensation of SiO clouds into account. We assumed that the envelope is both in hydrostatic equilibrium and in vapor equilibrium with the underlying magma ocean. This means that pebbles do not undergo sublimation in the envelope and therefore survive until they plunge into the magma ocean. We find that the emergence of an inner radiative region, where SiO condensation suppresses convection, increases the pressure and temperature in the inner envelope compared to pure H$_2$/He envelopes once $M_\\mathrm{pl} \\gtrsim 0.3\\,M_\\oplus$. For $M_\\mathrm{pl}>0.75\\,M_\\oplus$, the temperature and pressure close to the surface reach the supercritical point of SiO. The amount of SiO stored in the envelope is lower than the total planet mass for low mass planets. However, for $M_\\mathrm{pl}>2.0\\,M_\\oplus$, all accreted pebble material must contribute to maintain the vapor equilibrium in the envelope. Therefore, the non-vapor mass of the planet ceases to increase beyond this threshold. Overall, our vapor equilibrium model of the planetary envelope allows for direct core growth by pebble accretion up to much higher masses than previously thought.","sentences":["The gaseous envelope of an accreting rocky planet becomes hot enough to sublimate silicates and other refractory minerals.   ","% aims heading (mandatory) For this work, we studied the effect of the resulting envelope enrichment with a heavy vapor species on the composition and temperature of the envelope.","For simplification, we used the gas-phase molecule SiO to represent the sublimation of silicate material.","We solved the equilibrium structure equations in 1D for planets in the mass range of $0.1$ to $3\\,M_\\oplus$. The convective stability criterion was extended to take the stabilizing effect of the condensation of SiO clouds into account.","We assumed that the envelope is both in hydrostatic equilibrium and in vapor equilibrium with the underlying magma ocean.","This means that pebbles do not undergo sublimation in the envelope and therefore survive until they plunge into the magma ocean.","We find that the emergence of an inner radiative region, where SiO condensation suppresses convection, increases the pressure and temperature in the inner envelope compared to pure H$_2$/He envelopes once $M_\\mathrm{pl} \\gtrsim 0.3\\,M_\\oplus$. For $M_\\mathrm{pl}>0.75\\,M_\\oplus$, the temperature and pressure close to the surface reach the supercritical point of SiO. The amount of SiO stored in the envelope is lower than the total planet mass for low mass planets.","However, for $M_\\mathrm{pl}>2.0\\,M_\\oplus$, all accreted pebble material must contribute to maintain the vapor equilibrium in the envelope.","Therefore, the non-vapor mass of the planet ceases to increase beyond this threshold.","Overall, our vapor equilibrium model of the planetary envelope allows for direct core growth by pebble accretion up to much higher masses than previously thought."],"url":"http://arxiv.org/abs/2402.09089v1","category":"astro-ph.EP"}
{"created":"2024-02-14 10:28:43","title":"Using quantum annealing to design lattice proteins","abstract":"Quantum annealing has shown promise for finding solutions to difficult optimization problems, including protein folding. Recently, we used the D-Wave Advantage quantum annealer to explore the folding problem in a coarse-grained lattice model, the HP model, in which amino acids are classified into two broad groups: hydrophobic (H) and polar (P). Using a set of 22 HP sequences with up to 64 amino acids, we demonstrated the fast and consistent identification of the correct HP model ground states using the D-Wave hybrid quantum-classical solver. An equally relevant biophysical challenge, called the protein design problem, is the inverse of the above, where the task is to predict protein sequences that fold to a given structure. Here, we approach the design problem by a two-step procedure, implemented and executed on a D-Wave machine. In the first step, we perform a pure sequence-space search by varying the type of amino acid at each sequence position, and seek sequences which minimize the HP-model energy of the target structure. After mapping this task onto an Ising spin glass representation, we employ a hybrid quantum-classical solver to deliver energy-optimal sequences for structures with 30-64 amino acids, with a 100% success rate. In the second step, we filter the optimized sequences from the first step according to their ability to fold to the intended structure. In addition, we try solving the sequence optimization problem using only the QPU, which confines us to sizes $\\le$20, due to exponentially decreasing success rates. To shed light on the pure QPU results, we investigate the effects of control errors caused by an imperfect implementation of the intended Hamiltonian on the QPU, by numerically analyzing the Schr\\\"odinger equation. We find that the simulated success rates in the presence of control noise semi-quantitatively reproduce the modest pure QPU results for larger chains.","sentences":["Quantum annealing has shown promise for finding solutions to difficult optimization problems, including protein folding.","Recently, we used the D-Wave Advantage quantum annealer to explore the folding problem in a coarse-grained lattice model, the HP model, in which amino acids are classified into two broad groups: hydrophobic (H) and polar (P).","Using a set of 22 HP sequences with up to 64 amino acids, we demonstrated the fast and consistent identification of the correct HP model ground states using the D-Wave hybrid quantum-classical solver.","An equally relevant biophysical challenge, called the protein design problem, is the inverse of the above, where the task is to predict protein sequences that fold to a given structure.","Here, we approach the design problem by a two-step procedure, implemented and executed on a D-Wave machine.","In the first step, we perform a pure sequence-space search by varying the type of amino acid at each sequence position, and seek sequences which minimize the HP-model energy of the target structure.","After mapping this task onto an Ising spin glass representation, we employ a hybrid quantum-classical solver to deliver energy-optimal sequences for structures with 30-64 amino acids, with a 100% success rate.","In the second step, we filter the optimized sequences from the first step according to their ability to fold to the intended structure.","In addition, we try solving the sequence optimization problem using only the QPU, which confines us to sizes $\\le$20, due to exponentially decreasing success rates.","To shed light on the pure QPU results, we investigate the effects of control errors caused by an imperfect implementation of the intended Hamiltonian on the QPU, by numerically analyzing the Schr\\\"odinger equation.","We find that the simulated success rates in the presence of control noise semi-quantitatively reproduce the modest pure QPU results for larger chains."],"url":"http://arxiv.org/abs/2402.09069v1","category":"quant-ph"}
{"created":"2024-02-14 10:18:00","title":"Blind Deep-Learning-Based Image Watermarking Robust Against Geometric Transformations","abstract":"Digital watermarking enables protection against copyright infringement of images. Although existing methods embed watermarks imperceptibly and demonstrate robustness against attacks, they typically lack resilience against geometric transformations. Therefore, this paper proposes a new watermarking method that is robust against geometric attacks. The proposed method is based on the existing HiDDeN architecture that uses deep learning for watermark encoding and decoding. We add new noise layers to this architecture, namely for a differentiable JPEG estimation, rotation, rescaling, translation, shearing and mirroring. We demonstrate that our method outperforms the state of the art when it comes to geometric robustness. In conclusion, the proposed method can be used to protect images when viewed on consumers' devices.","sentences":["Digital watermarking enables protection against copyright infringement of images.","Although existing methods embed watermarks imperceptibly and demonstrate robustness against attacks, they typically lack resilience against geometric transformations.","Therefore, this paper proposes a new watermarking method that is robust against geometric attacks.","The proposed method is based on the existing HiDDeN architecture that uses deep learning for watermark encoding and decoding.","We add new noise layers to this architecture, namely for a differentiable JPEG estimation, rotation, rescaling, translation, shearing and mirroring.","We demonstrate that our method outperforms the state of the art when it comes to geometric robustness.","In conclusion, the proposed method can be used to protect images when viewed on consumers' devices."],"url":"http://arxiv.org/abs/2402.09062v1","category":"cs.MM"}
{"created":"2024-02-14 09:46:53","title":"End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training","abstract":"End-to-end (E2E) training, optimizing the entire model through error backpropagation, fundamentally supports the advancements of deep learning. Despite its high performance, E2E training faces the problems of memory consumption, parallel computing, and discrepancy with the functionalities of the actual brain. Various alternative methods have been proposed to overcome these difficulties; however, no one can yet match the performance of E2E training, thereby falling short in practicality. Furthermore, there is no deep understanding regarding differences in the trained model properties beyond the performance gap. In this paper, we reconsider why E2E training demonstrates a superior performance through a comparison with layer-wise training, a non-E2E method that locally sets errors. On the basis of the observation that E2E training has an advantage in propagating input information, we analyze the information plane dynamics of intermediate representations based on the Hilbert-Schmidt independence criterion (HSIC). The results of our normalized HSIC value analysis reveal the E2E training ability to exhibit different information dynamics across layers, in addition to efficient information propagation. Furthermore, we show that this layer-role differentiation leads to the final representation following the information bottleneck principle. It suggests the need to consider the cooperative interactions between layers, not just the final layer when analyzing the information bottleneck of deep learning.","sentences":["End-to-end (E2E) training, optimizing the entire model through error backpropagation, fundamentally supports the advancements of deep learning.","Despite its high performance, E2E training faces the problems of memory consumption, parallel computing, and discrepancy with the functionalities of the actual brain.","Various alternative methods have been proposed to overcome these difficulties; however, no one can yet match the performance of E2E training, thereby falling short in practicality.","Furthermore, there is no deep understanding regarding differences in the trained model properties beyond the performance gap.","In this paper, we reconsider why E2E training demonstrates a superior performance through a comparison with layer-wise training, a non-E2E method that locally sets errors.","On the basis of the observation that E2E training has an advantage in propagating input information, we analyze the information plane dynamics of intermediate representations based on the Hilbert-Schmidt independence criterion (HSIC).","The results of our normalized HSIC value analysis reveal the E2E training ability to exhibit different information dynamics across layers, in addition to efficient information propagation.","Furthermore, we show that this layer-role differentiation leads to the final representation following the information bottleneck principle.","It suggests the need to consider the cooperative interactions between layers, not just the final layer when analyzing the information bottleneck of deep learning."],"url":"http://arxiv.org/abs/2402.09050v1","category":"cs.LG"}
{"created":"2024-02-14 09:00:04","title":"From Rewrite Rules to Axioms in the $\u03bb$$\u03a0$-Calculus Modulo Theory","abstract":"The $\\lambda$$\\Pi$-calculus modulo theory is an extension of simply typed $\\lambda$-calculus with dependent types and user-defined rewrite rules. We show that it is possible to replace the rewrite rules of a theory of the $\\lambda$$\\Pi$-calculus modulo theory by equational axioms, when this theory features the notions of proposition and proof, while maintaining the same expressiveness. To do so, we introduce in the target theory a heterogeneous equality, and we build a translation that replaces each use of the conversion rule by the insertion of a transport. At the end, the theory with rewrite rules is a conservative extension of the theory with axioms.","sentences":["The $\\lambda$$\\Pi$-calculus modulo theory is an extension of simply typed $\\lambda$-calculus with dependent types and user-defined rewrite rules.","We show that it is possible to replace the rewrite rules of a theory of the $\\lambda$$\\Pi$-calculus modulo theory by equational axioms, when this theory features the notions of proposition and proof, while maintaining the same expressiveness.","To do so, we introduce in the target theory a heterogeneous equality, and we build a translation that replaces each use of the conversion rule by the insertion of a transport.","At the end, the theory with rewrite rules is a conservative extension of the theory with axioms."],"url":"http://arxiv.org/abs/2402.09024v1","category":"cs.LO"}
{"created":"2024-02-14 08:42:08","title":"Holomorphic geometric structures on Oeljeklaus-Toma manifolds","abstract":"We prove that any holomorphic geometric structure of algebraic type on an Oeljeklaus- Toma manifold is locally homogeneous. For locally conformal K\\\"ahler Oeljeklaus-Toma manifolds we prove that all holomorphic geometric structures, and also all holomorphic Cartan geometries, on them are locally homogeneous.","sentences":["We prove that any holomorphic geometric structure of algebraic type on an Oeljeklaus- Toma manifold is locally homogeneous.","For locally conformal K\\\"ahler Oeljeklaus-Toma manifolds we prove that all holomorphic geometric structures, and also all holomorphic Cartan geometries, on them are locally homogeneous."],"url":"http://arxiv.org/abs/2402.09012v1","category":"math.DG"}
{"created":"2024-02-14 06:15:04","title":"Dynamical 4-D Gauss-Bonnet action from matter-graviton interaction at one-loop","abstract":"The occurrence of singularities at the centers of black holes suggests that general relativity (GR), although a highly successful model of gravity and cosmology, is inapplicable. This is due to the breakdown of the equivalence principle. Gauss-Bonnet (GB) action is a simplest extension of GR as it possess second-order equations of motion and is devoid of ghosts. However, in 4-D, the GB action is topological. Recently, Glavan and Lin proposed a mathematical framework that transforms the 4-D GB gravity theory into a non-topological one. However, it has been argued that without a canonical way to choose 4-D from the higher-dimensional space, such a GB gravity is not well-defined in 4-D. The current work takes a step in addressing this issue by demonstrating that the rescaling of the GB coupling $\\alpha \\rightarrow \\alpha/(D - 4)$ arises from the self-energy correction of gravitons in 4-D via \\emph{the dimensional regularization}. To keep things transparent, we focus on the linearized theory of gravity coupled with matter fields. By computing the one-loop self-energy correction of gravitons induced by the matter fields, we explicitly provide the origin of the prescription provided by Glavan and Lin. Our work naturally opens a new window to considering 4-D Einstein Gauss-Bonnet gravity as the most straightforward modification to GR.","sentences":["The occurrence of singularities at the centers of black holes suggests that general relativity (GR), although a highly successful model of gravity and cosmology, is inapplicable.","This is due to the breakdown of the equivalence principle.","Gauss-Bonnet (GB) action is a simplest extension of GR as it possess second-order equations of motion and is devoid of ghosts.","However, in 4-D, the GB action is topological.","Recently, Glavan and Lin proposed a mathematical framework that transforms the 4-D GB gravity theory into a non-topological one.","However, it has been argued that without a canonical way to choose 4-D from the higher-dimensional space, such a GB gravity is not well-defined in 4-D. The current work takes a step in addressing this issue by demonstrating that the rescaling of the GB coupling $\\alpha \\rightarrow \\alpha/(D - 4)$ arises from the self-energy correction of gravitons in 4-D via \\emph{the dimensional regularization}.","To keep things transparent, we focus on the linearized theory of gravity coupled with matter fields.","By computing the one-loop self-energy correction of gravitons induced by the matter fields, we explicitly provide the origin of the prescription provided by Glavan and Lin.","Our work naturally opens a new window to considering 4-D Einstein Gauss-Bonnet gravity as the most straightforward modification to GR."],"url":"http://arxiv.org/abs/2402.08965v1","category":"hep-th"}
{"created":"2024-02-14 06:01:19","title":"Quasi-Akaike information criterion of structural equation modeling with latent variables for diffusion processes","abstract":"We consider a model selection problem for structural equation modeling (SEM) with latent variables for diffusion processes based on high-frequency data. First, we propose the quasi-Akaike information criterion of the SEM and study the asymptotic properties. Next, we consider the situation where the set of competing models includes some misspecified parametric models. It is shown that the probability of choosing the misspecified models converges to zero. Furthermore, examples and simulation results are given.","sentences":["We consider a model selection problem for structural equation modeling (SEM) with latent variables for diffusion processes based on high-frequency data.","First, we propose the quasi-Akaike information criterion of the SEM and study the asymptotic properties.","Next, we consider the situation where the set of competing models includes some misspecified parametric models.","It is shown that the probability of choosing the misspecified models converges to zero.","Furthermore, examples and simulation results are given."],"url":"http://arxiv.org/abs/2402.08959v1","category":"math.ST"}
{"created":"2024-02-14 05:34:24","title":"Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input","abstract":"In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We propose a basis-free generalization of the merged-staircase property in Abbe et al. (2022) and establish a necessary condition for the SGD-learnability. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.","sentences":["In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace.","We propose a basis-free generalization of the merged-staircase property in Abbe et al. (2022) and establish a necessary condition for the SGD-learnability.","In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero."],"url":"http://arxiv.org/abs/2402.08948v1","category":"cs.LG"}
{"created":"2024-02-14 04:28:21","title":"Search for ultralight vector dark matter with a Torsion Pendulum Dual Oscillator","abstract":"Ultralight bosons with masses in the range from $\\sim 10^{-22}$ eV to $\\sim 1$ eV, are well-motivated, wave-like dark matter candidates. Particles on the lower-mass end are less explored in experiments due to their vanishingly small mass and weak coupling to the Standard Model. We propose to search for U(1)$_{B\\!-\\!L}$ gauge boson dark matter using a Torsion Pendulum Dual Oscillator (TorPeDO), a sensor originally designed to detect Newtonian gravitational field fluctuations with an enhanced differential torque sensitivity in a frequency band of $\\sim 10^{-2}$-$10$ Hz. We describe the experiment setup of a modified TorPeDO sensor, equipped with four 5-kg end test masses (two made from beryllium and two from aluminium). We present the estimated sensitivity to an ultralight dark matter field coupled to baryon minus lepton ($B\\!-\\!L$) number, in a mass range of $\\sim 10^{-17}$-$10^{-13}$ eV. The projected constraints on the coupling constant $g_{B\\!-\\!L} (\\hbar c)^{-1/2}$ can reach $\\sim 10^{-29}$ for a boson mass of $\\sim 10^{-15}$ eV in the most sensitive frequency band.","sentences":["Ultralight bosons with masses in the range from $\\sim 10^{-22}$ eV to $\\sim 1$ eV, are well-motivated, wave-like dark matter candidates.","Particles on the lower-mass end are less explored in experiments due to their vanishingly small mass and weak coupling to the Standard Model.","We propose to search for U(1)$_{B\\!-\\!L}$ gauge boson dark matter using a Torsion Pendulum Dual Oscillator (TorPeDO), a sensor originally designed to detect Newtonian gravitational field fluctuations with an enhanced differential torque sensitivity in a frequency band of $\\sim 10^{-2}$-$10$ Hz.","We describe the experiment setup of a modified TorPeDO sensor, equipped with four 5-kg end test masses (two made from beryllium and two from aluminium).","We present the estimated sensitivity to an ultralight dark matter field coupled to baryon minus lepton ($B\\!-\\!L$) number, in a mass range of $\\sim 10^{-17}$-$10^{-13}$ eV. The projected constraints on the coupling constant $g_{B\\!-\\!L} (\\hbar c)^{-1/2}$ can reach $\\sim 10^{-29}$ for a boson mass of $\\sim 10^{-15}$ eV in the most sensitive frequency band."],"url":"http://arxiv.org/abs/2402.08935v1","category":"hep-ph"}
{"created":"2024-02-14 04:23:05","title":"Extreme Video Compression with Pre-trained Diffusion Models","abstract":"Diffusion models have achieved remarkable success in generating high quality image and video data. More recently, they have also been used for image compression with high perceptual quality. In this paper, we present a novel approach to extreme video compression leveraging the predictive power of diffusion-based generative models at the decoder. The conditional diffusion model takes several neural compressed frames and generates subsequent frames. When the reconstruction quality drops below the desired level, new frames are encoded to restart prediction. The entire video is sequentially encoded to achieve a visually pleasing reconstruction, considering perceptual quality metrics such as the learned perceptual image patch similarity (LPIPS) and the Frechet video distance (FVD), at bit rates as low as 0.02 bits per pixel (bpp). Experimental results demonstrate the effectiveness of the proposed scheme compared to standard codecs such as H.264 and H.265 in the low bpp regime. The results showcase the potential of exploiting the temporal relations in video data using generative models. Code is available at: https://github.com/ElesionKyrie/Extreme-Video-Compression-With-Prediction-Using-Pre-trainded-Diffusion-Models-","sentences":["Diffusion models have achieved remarkable success in generating high quality image and video data.","More recently, they have also been used for image compression with high perceptual quality.","In this paper, we present a novel approach to extreme video compression leveraging the predictive power of diffusion-based generative models at the decoder.","The conditional diffusion model takes several neural compressed frames and generates subsequent frames.","When the reconstruction quality drops below the desired level, new frames are encoded to restart prediction.","The entire video is sequentially encoded to achieve a visually pleasing reconstruction, considering perceptual quality metrics such as the learned perceptual image patch similarity (LPIPS) and the Frechet video distance (FVD), at bit rates as low as 0.02 bits per pixel (bpp).","Experimental results demonstrate the effectiveness of the proposed scheme compared to standard codecs such as H.264 and H.265 in the low bpp regime.","The results showcase the potential of exploiting the temporal relations in video data using generative models.","Code is available at: https://github.com/ElesionKyrie/Extreme-Video-Compression-With-Prediction-Using-Pre-trainded-Diffusion-Models-"],"url":"http://arxiv.org/abs/2402.08934v1","category":"eess.IV"}
{"created":"2024-02-14 02:32:18","title":"Sound Field Reconstruction Using a Compact Acoustics-informed Neural Network","abstract":"Sound field reconstruction (SFR) augments the information of a sound field captured by a microphone array. Conventional SFR methods using basis function decomposition are straightforward and computationally efficient, but may require more microphones than needed to measure the sound field. Recent studies show that pure data-driven and learning-based methods are promising in some SFR tasks, but they are usually computationally heavy and may fail to reconstruct a physically valid sound field. This paper proposes a compact acoustics-informed neural network (AINN) method for SFR, whereby the Helmholtz equation is exploited to regularize the neural network. As opposed to pure data-driven approaches that solely rely on measured sound pressures, the integration of the Helmholtz equation improves robustness of the neural network against variations during the measurement processes and prompts the generation of physically valid reconstructions. The AINN is designed to be compact, and is able to predict not only the sound pressures but also sound pressure gradients within a spatial region of interest based on measured sound pressures along the boundary. Numerical experiments with acoustic transfer functions measured in different environments demonstrate the superiority of the AINN method over the traditional cylinder harmonic decomposition and the singular value decomposition methods.","sentences":["Sound field reconstruction (SFR) augments the information of a sound field captured by a microphone array.","Conventional SFR methods using basis function decomposition are straightforward and computationally efficient, but may require more microphones than needed to measure the sound field.","Recent studies show that pure data-driven and learning-based methods are promising in some SFR tasks, but they are usually computationally heavy and may fail to reconstruct a physically valid sound field.","This paper proposes a compact acoustics-informed neural network (AINN) method for SFR, whereby the Helmholtz equation is exploited to regularize the neural network.","As opposed to pure data-driven approaches that solely rely on measured sound pressures, the integration of the Helmholtz equation improves robustness of the neural network against variations during the measurement processes and prompts the generation of physically valid reconstructions.","The AINN is designed to be compact, and is able to predict not only the sound pressures but also sound pressure gradients within a spatial region of interest based on measured sound pressures along the boundary.","Numerical experiments with acoustic transfer functions measured in different environments demonstrate the superiority of the AINN method over the traditional cylinder harmonic decomposition and the singular value decomposition methods."],"url":"http://arxiv.org/abs/2402.08904v1","category":"eess.AS"}
{"created":"2024-02-14 01:13:55","title":"Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation","abstract":"Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community. In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted. The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance. In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP). We first train an unsupervised convolutional neural network (UnFlow) to generate optical flow estimation. Then we render the output of optical flow net to a fully convolutional SegNet model. The main contribution of our work is (1) Fine-tuning the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural networks with Encoder-Decoder architecture to segment objects. We developed the codes with TensorFlow, and executed the training and evaluation processes on an AWS EC2 instance.","sentences":["Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community.","In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted.","The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance.","In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP).","We first train an unsupervised convolutional neural network (UnFlow) to generate optical flow estimation.","Then we render the output of optical flow net to a fully convolutional SegNet model.","The main contribution of our work is (1) Fine-tuning the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural networks with Encoder-Decoder architecture to segment objects.","We developed the codes with TensorFlow, and executed the training and evaluation processes on an AWS EC2 instance."],"url":"http://arxiv.org/abs/2402.08882v1","category":"cs.CV"}
{"created":"2024-02-14 01:09:19","title":"Solving the Einstein Constraints Numerically on Compact Three-Manifolds Using Hyperbolic Relaxation","abstract":"The effectiveness of the hyperbolic relaxation method for solving the Einstein constraint equations numerically is studied here on a variety of compact orientable three-manifolds. Convergent numerical solutions are found using this method on manifolds admitting negative Ricci scalar curvature metrics, i.e. those from the $H^3$ and the $H^2\\times S^1$ geometrization classes. The method fails to produce solutions, however, on all the manifolds examined here admitting non-negative Ricci scalar curvatures, i.e. those from the $S^3$, $S^2\\times S^1$, and the $E^3$ classes. This study also finds that the accuracy of the convergent solutions produced by hyperbolic relaxation can be increased significantly by performing fairly low-cost standard elliptic solves using the hyperbolic relaxation solutions as initial guesses.","sentences":["The effectiveness of the hyperbolic relaxation method for solving the Einstein constraint equations numerically is studied here on a variety of compact orientable three-manifolds.","Convergent numerical solutions are found using this method on manifolds admitting negative Ricci scalar curvature metrics, i.e. those from the $H^3$ and the $H^2\\times S^1$ geometrization classes.","The method fails to produce solutions, however, on all the manifolds examined here admitting non-negative Ricci scalar curvatures, i.e. those from the $S^3$, $S^2\\times S^1$, and the $E^3$ classes.","This study also finds that the accuracy of the convergent solutions produced by hyperbolic relaxation can be increased significantly by performing fairly low-cost standard elliptic solves using the hyperbolic relaxation solutions as initial guesses."],"url":"http://arxiv.org/abs/2402.08880v1","category":"gr-qc"}
{"created":"2024-02-14 00:18:10","title":"DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep Learning","abstract":"Polar codes, developed on the foundation of Arikan's polarization kernel, represent a breakthrough in coding theory and have emerged as the state-of-the-art error-correction-code in short-to-medium block length regimes. Importantly, recent research has indicated that the reliability of polar codes can be further enhanced by substituting Arikan's kernel with a larger one, leading to a faster polarization. However, for short-to-medium block length regimes, the development of polar codes that effectively employ large kernel sizes has not yet been realized. In this paper, we explore a novel, non-linear generalization of polar codes with an expanded kernel size, which we call DeepPolar codes. Our results show that DeepPolar codes effectively utilize the benefits of larger kernel size, resulting in enhanced reliability compared to both the existing neural codes and conventional polar codes.","sentences":["Polar codes, developed on the foundation of Arikan's polarization kernel, represent a breakthrough in coding theory and have emerged as the state-of-the-art error-correction-code in short-to-medium block length regimes.","Importantly, recent research has indicated that the reliability of polar codes can be further enhanced by substituting Arikan's kernel with a larger one, leading to a faster polarization.","However, for short-to-medium block length regimes, the development of polar codes that effectively employ large kernel sizes has not yet been realized.","In this paper, we explore a novel, non-linear generalization of polar codes with an expanded kernel size, which we call DeepPolar codes.","Our results show that DeepPolar codes effectively utilize the benefits of larger kernel size, resulting in enhanced reliability compared to both the existing neural codes and conventional polar codes."],"url":"http://arxiv.org/abs/2402.08864v1","category":"cs.IT"}
{"created":"2024-02-14 00:13:39","title":"Saliency-aware End-to-end Learned Variable-Bitrate 360-degree Image Compression","abstract":"Effective compression of 360$^\\circ$ images, also referred to as omnidirectional images (ODIs), is of high interest for various virtual reality (VR) and related applications. 2D image compression methods ignore the equator-biased nature of ODIs and fail to address oversampling near the poles, leading to inefficient compression when applied to ODI. We present a new learned saliency-aware 360$^\\circ$ image compression architecture that prioritizes bit allocation to more significant regions, considering the unique properties of ODIs. By assigning fewer bits to less important regions, significant data size reduction can be achieved while maintaining high visual quality in the significant regions. To the best of our knowledge, this is the first study that proposes an end-to-end variable-rate model to compress 360$^\\circ$ images leveraging saliency information. The results show significant bit-rate savings over the state-of-the-art learned and traditional ODI compression methods at similar perceptual visual quality.","sentences":["Effective compression of 360$^\\circ$ images, also referred to as omnidirectional images (ODIs), is of high interest for various virtual reality (VR) and related applications.","2D image compression methods ignore the equator-biased nature of ODIs and fail to address oversampling near the poles, leading to inefficient compression when applied to ODI.","We present a new learned saliency-aware 360$^\\circ$ image compression architecture that prioritizes bit allocation to more significant regions, considering the unique properties of ODIs.","By assigning fewer bits to less important regions, significant data size reduction can be achieved while maintaining high visual quality in the significant regions.","To the best of our knowledge, this is the first study that proposes an end-to-end variable-rate model to compress 360$^\\circ$ images leveraging saliency information.","The results show significant bit-rate savings over the state-of-the-art learned and traditional ODI compression methods at similar perceptual visual quality."],"url":"http://arxiv.org/abs/2402.08862v1","category":"eess.IV"}
{"created":"2024-02-13 23:53:47","title":"Approximation of relation functions and attention mechanisms","abstract":"Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. This work studies the approximation properties of inner products of neural networks. It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces. Finally, these approximation results are applied to analyzing the attention mechanism underlying Transformers, showing that any retrieval mechanism defined by an abstract preorder can be approximated by attention through its inner product relations. This result uses the Debreu representation theorem in economics to represent preference relations in terms of utility functions.","sentences":["Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs.","This work studies the approximation properties of inner products of neural networks.","It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions.","In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator.","In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation.","In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces.","Finally, these approximation results are applied to analyzing the attention mechanism underlying Transformers, showing that any retrieval mechanism defined by an abstract preorder can be approximated by attention through its inner product relations.","This result uses the Debreu representation theorem in economics to represent preference relations in terms of utility functions."],"url":"http://arxiv.org/abs/2402.08856v1","category":"cs.LG"}
{"created":"2024-02-13 22:45:51","title":"The Euler non-mixing made easy","abstract":"The non-transitivity without extra constraints in the 3D Euler equation is almost evident and can be derived, e.g. from Morse theory.","sentences":["The non-transitivity without extra constraints in the 3D Euler equation is almost evident and can be derived, e.g. from Morse theory."],"url":"http://arxiv.org/abs/2402.08836v1","category":"math.DS"}
{"created":"2024-02-13 22:39:17","title":"Machine Learning Potential Powered Insights into the Mechanical Stability of Amorphous Li-Si Alloys","abstract":"Understanding the mechanical properties of solid-state materials at the atomic scale is crucial for developing novel materials. For example, amorphous LiSi alloys are attractive anode materials for solid-state Li-ion batteries but face mechanical instabilities due to significant volume variations with changing Li content. A fundamental grasp of the mechanical behavior in such systems is essential to address their poor mechanical integrity. Experimental methods offer insufficient information to elaborate on dynamic mechanical degradation mechanisms at the atomic scale, and computationally demanding first-principles methods, like DFT, struggle to access the system sizes needed for modeling mechanical phenomena. Machine learning potentials (MLPs) can overcome the computational constraints of traditional DFT-based simulations, enabling large-scale, accurate, and efficient simulations. Here, we provide a concise tutorial on developing and applying MLPs to investigate mechanical properties in materials systems, ranging from bulk to nanoparticles, using Li-Si alloys as an example. Trained on a comprehensive dataset (~45,000 DFT structures) with the aenet package accelerated by PyTorch, a robust MLP is constructed to reproduce results consistent with previous experimental observations. We demonstrate applying the MLP to realistic structures to visualize the deformation mechanism and determine the origin of mechanical instabilities caused by fracturing. This work aims to establish MLP-based simulations as a tool to understand the atomic-scale mechanical behavior in different materials systems.","sentences":["Understanding the mechanical properties of solid-state materials at the atomic scale is crucial for developing novel materials.","For example, amorphous LiSi alloys are attractive anode materials for solid-state Li-ion batteries but face mechanical instabilities due to significant volume variations with changing Li content.","A fundamental grasp of the mechanical behavior in such systems is essential to address their poor mechanical integrity.","Experimental methods offer insufficient information to elaborate on dynamic mechanical degradation mechanisms at the atomic scale, and computationally demanding first-principles methods, like DFT, struggle to access the system sizes needed for modeling mechanical phenomena.","Machine learning potentials (MLPs) can overcome the computational constraints of traditional DFT-based simulations, enabling large-scale, accurate, and efficient simulations.","Here, we provide a concise tutorial on developing and applying MLPs to investigate mechanical properties in materials systems, ranging from bulk to nanoparticles, using Li-Si alloys as an example.","Trained on a comprehensive dataset (~45,000 DFT structures) with the aenet package accelerated by PyTorch, a robust MLP is constructed to reproduce results consistent with previous experimental observations.","We demonstrate applying the MLP to realistic structures to visualize the deformation mechanism and determine the origin of mechanical instabilities caused by fracturing.","This work aims to establish MLP-based simulations as a tool to understand the atomic-scale mechanical behavior in different materials systems."],"url":"http://arxiv.org/abs/2402.08834v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-13 22:34:54","title":"Symbolic powers: Simis and weighted monomial ideals","abstract":"The aim of this work is to compare symbolic and ordinary powers of monomial ideals using commutative algebra and combinatorics. Monomial ideals whose symbolic and ordinary powers coincide are called Simis ideals. Weighted monomial ideals are defined by assigning linear weights to monomials. We examine Simis and normally torsion-free ideals, relate some of the properties of monomial ideals and weighted monomial ideals, and present an structure theorem for edge ideals of $d$-uniform clutters whose ideal of covers is Simis in degree $d$. One of our main results is a combinatorial classification of when the dual of the edge ideal of a weighted oriented graph is Simis in degree $2$.","sentences":["The aim of this work is to compare symbolic and ordinary powers of monomial ideals using commutative algebra and combinatorics.","Monomial ideals whose symbolic and ordinary powers coincide are called Simis ideals.","Weighted monomial ideals are defined by assigning linear weights to monomials.","We examine Simis and normally torsion-free ideals, relate some of the properties of monomial ideals and weighted monomial ideals, and present an structure theorem for edge ideals of $d$-uniform clutters whose ideal of covers is Simis in degree $d$. One of our main results is a combinatorial classification of when the dual of the edge ideal of a weighted oriented graph is Simis in degree $2$."],"url":"http://arxiv.org/abs/2402.08833v1","category":"math.AC"}
{"created":"2024-02-13 22:07:57","title":"Disambiguated Node Classification with Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, {\\method}, which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions. {\\method} identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner. {\\method} promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of {\\method} and highlight its potential to improve GNN performance in underrepresented graph regions.","sentences":["Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains.","Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions.","These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity.","In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem.","We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions.","To disambiguate node embeddings, we propose a novel method, {\\method}, which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions.","{\\method} identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner.","{\\method} promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem.","Empirical results validate the efficiency of {\\method} and highlight its potential to improve GNN performance in underrepresented graph regions."],"url":"http://arxiv.org/abs/2402.08824v1","category":"cs.LG"}
{"created":"2024-02-13 21:46:45","title":"Interacting Scalar fields: Dark Matter--Early Dark Energy","abstract":"The main aim of this work is to explore the possibility that cold dark matter (CDM) and early dark energy (EDE) can be described by canonical scalar fields that are coupled at the level of its conservation equations. The formalism covers dynamical aspects at the background and linear perturbation levels for an arbitrary coupling function, followed by an example of it. We emphasize the impact of this model on the Matter Power Spectrum and the Cosmic Microwave Background (CMB) spectra, with or without direct interaction. Our findings indicate that the presence of a scalar field can partially counteract the known effects of the other, opening the possibility to avoid some undesired aspects, such as the increase in $\\Omega_{m}$ that usually is needed in the case of a purely EDE scalar field scenario, in order to fit the CMB spectra. This opens up the possibility to analyzing whether the interaction can help to ameliorate the cosmological tensions.","sentences":["The main aim of this work is to explore the possibility that cold dark matter (CDM) and early dark energy (EDE) can be described by canonical scalar fields that are coupled at the level of its conservation equations.","The formalism covers dynamical aspects at the background and linear perturbation levels for an arbitrary coupling function, followed by an example of it.","We emphasize the impact of this model on the Matter Power Spectrum and the Cosmic Microwave Background (CMB) spectra, with or without direct interaction.","Our findings indicate that the presence of a scalar field can partially counteract the known effects of the other, opening the possibility to avoid some undesired aspects, such as the increase in $\\Omega_{m}$ that usually is needed in the case of a purely EDE scalar field scenario, in order to fit the CMB spectra.","This opens up the possibility to analyzing whether the interaction can help to ameliorate the cosmological tensions."],"url":"http://arxiv.org/abs/2402.08815v1","category":"astro-ph.CO"}
{"created":"2024-02-13 21:26:38","title":"Depth Separation in Norm-Bounded Infinite-Width Neural Networks","abstract":"We study depth separation in infinite-width neural networks, where complexity is controlled by the overall squared $\\ell_2$-norm of the weights (sum of squares of all weights in the network). Whereas previous depth separation results focused on separation in terms of width, such results do not give insight into whether depth determines if it is possible to learn a network that generalizes well even when the network width is unbounded. Here, we study separation in terms of the sample complexity required for learnability. Specifically, we show that there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 ReLU networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 ReLU networks (with any value for the norm). We also show that a similar statement in the reverse direction is not possible: any function learnable with polynomial sample complexity by a norm-controlled depth-2 ReLU network with infinite width is also learnable with polynomial sample complexity by a norm-controlled depth-3 ReLU network.","sentences":["We study depth separation in infinite-width neural networks, where complexity is controlled by the overall squared $\\ell_2$-norm of the weights (sum of squares of all weights in the network).","Whereas previous depth separation results focused on separation in terms of width, such results do not give insight into whether depth determines if it is possible to learn a network that generalizes well even when the network width is unbounded.","Here, we study separation in terms of the sample complexity required for learnability.","Specifically, we show that there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 ReLU networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 ReLU networks (with any value for the norm).","We also show that a similar statement in the reverse direction is not possible: any function learnable with polynomial sample complexity by a norm-controlled depth-2 ReLU network with infinite width is also learnable with polynomial sample complexity by a norm-controlled depth-3 ReLU network."],"url":"http://arxiv.org/abs/2402.08808v1","category":"cs.LG"}
{"created":"2024-02-13 21:13:14","title":"Dimers and M-Curves","abstract":"In this paper we develop a general approach to dimer models analogous to Krichever's scheme in the theory of integrable systems. We start with a Riemann surface and the simplest generic meromorphic functions on it and demonstrate how to obtain integrable dimer models. These are dimer models on doubly periodic bipartite graphs with quasi-periodic positive weights. Dimer models with periodic weights and Harnack curves are recovered as a special case. This generalization from Harnack curves to general M-curves leads to transparent algebro-geometric structures. In particular explicit formulas for the Ronkin function and surface tension as integrals of meromorphic differentials on M-curves are obtained. Furthermore we describe the variational principle for the height function in the quasi-periodic case. Based on Schottky uniformizations of Riemann surfaces we present concrete computational results including computing the weights and sampling dimer configurations with them. The computational results are in complete agreement with the theoretical predictions.","sentences":["In this paper we develop a general approach to dimer models analogous to Krichever's scheme in the theory of integrable systems.","We start with a Riemann surface and the simplest generic meromorphic functions on it and demonstrate how to obtain integrable dimer models.","These are dimer models on doubly periodic bipartite graphs with quasi-periodic positive weights.","Dimer models with periodic weights and Harnack curves are recovered as a special case.","This generalization from Harnack curves to general M-curves leads to transparent algebro-geometric structures.","In particular explicit formulas for the Ronkin function and surface tension as integrals of meromorphic differentials on M-curves are obtained.","Furthermore we describe the variational principle for the height function in the quasi-periodic case.","Based on Schottky uniformizations of Riemann surfaces we present concrete computational results including computing the weights and sampling dimer configurations with them.","The computational results are in complete agreement with the theoretical predictions."],"url":"http://arxiv.org/abs/2402.08798v1","category":"math-ph"}
{"created":"2024-02-13 21:03:36","title":"BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation","abstract":"The accurate segmentation of medical images is critical for various healthcare applications. Convolutional neural networks (CNNs), especially Fully Convolutional Networks (FCNs) like U-Net, have shown remarkable success in medical image segmentation tasks. However, they have limitations in capturing global context and long-range relations, especially for objects with significant variations in shape, scale, and texture. While transformers have achieved state-of-the-art results in natural language processing and image recognition, they face challenges in medical image segmentation due to image locality and translational invariance issues. To address these challenges, this paper proposes an innovative U-shaped network called BEFUnet, which enhances the fusion of body and edge information for precise medical image segmentation. The BEFUnet comprises three main modules, including a novel Local Cross-Attention Feature (LCAF) fusion module, a novel Double-Level Fusion (DLF) module, and dual-branch encoder. The dual-branch encoder consists of an edge encoder and a body encoder. The edge encoder employs PDC blocks for effective edge information extraction, while the body encoder uses the Swin Transformer to capture semantic information with global attention. The LCAF module efficiently fuses edge and body features by selectively performing local cross-attention on features that are spatially close between the two modalities. This local approach significantly reduces computational complexity compared to global cross-attention while ensuring accurate feature matching. BEFUnet demonstrates superior performance over existing methods across various evaluation metrics on medical image segmentation datasets.","sentences":["The accurate segmentation of medical images is critical for various healthcare applications.","Convolutional neural networks (CNNs), especially Fully Convolutional Networks (FCNs) like U-Net, have shown remarkable success in medical image segmentation tasks.","However, they have limitations in capturing global context and long-range relations, especially for objects with significant variations in shape, scale, and texture.","While transformers have achieved state-of-the-art results in natural language processing and image recognition, they face challenges in medical image segmentation due to image locality and translational invariance issues.","To address these challenges, this paper proposes an innovative U-shaped network called BEFUnet, which enhances the fusion of body and edge information for precise medical image segmentation.","The BEFUnet comprises three main modules, including a novel Local Cross-Attention Feature (LCAF) fusion module, a novel Double-Level Fusion (DLF) module, and dual-branch encoder.","The dual-branch encoder consists of an edge encoder and a body encoder.","The edge encoder employs PDC blocks for effective edge information extraction, while the body encoder uses the Swin Transformer to capture semantic information with global attention.","The LCAF module efficiently fuses edge and body features by selectively performing local cross-attention on features that are spatially close between the two modalities.","This local approach significantly reduces computational complexity compared to global cross-attention while ensuring accurate feature matching.","BEFUnet demonstrates superior performance over existing methods across various evaluation metrics on medical image segmentation datasets."],"url":"http://arxiv.org/abs/2402.08793v1","category":"cs.CV"}
{"created":"2024-02-13 20:18:18","title":"An assessment of frozen natural orbitals and band gaps using equation of motion coupled cluster theory: a case study on polyacene and trans-polyacetylene","abstract":"Frozen natural orbitals (FNOs) are used to augment IP/EA-EOM-CCSD calculations targeting the band gap of trans-polyacetylene and polyacene. We show the resulting electron affinities (EAs), ionization potentials (IPs), and extrapolated band gaps incur errors that are largely tunable to a desired accuracy, yet require many orders of magnitude fewer core-hours as compared to the corresponding full calculation. The relationship between various FNO truncation schemes and (cc-pV$n$Z) basis set is also examined.","sentences":["Frozen natural orbitals (FNOs) are used to augment IP/EA-EOM-CCSD calculations targeting the band gap of trans-polyacetylene and polyacene.","We show the resulting electron affinities (EAs), ionization potentials (IPs), and extrapolated band gaps incur errors that are largely tunable to a desired accuracy, yet require many orders of magnitude fewer core-hours as compared to the corresponding full calculation.","The relationship between various FNO truncation schemes and (cc-pV$n$Z) basis set is also examined."],"url":"http://arxiv.org/abs/2402.08776v1","category":"physics.chem-ph"}
{"created":"2024-02-13 19:57:24","title":"Enhancing Robustness of Indoor Robotic Navigation with Free-Space Segmentation Models Against Adversarial Attacks","abstract":"Endeavors in indoor robotic navigation rely on the accuracy of segmentation models to identify free space in RGB images. However, deep learning models are vulnerable to adversarial attacks, posing a significant challenge to their real-world deployment. In this study, we identify vulnerabilities within the hidden layers of neural networks and introduce a practical approach to reinforce traditional adversarial training. Our method incorporates a novel distance loss function, minimizing the gap between hidden layers in clean and adversarial images. Experiments demonstrate satisfactory performance in improving the model's robustness against adversarial perturbations.","sentences":["Endeavors in indoor robotic navigation rely on the accuracy of segmentation models to identify free space in RGB images.","However, deep learning models are vulnerable to adversarial attacks, posing a significant challenge to their real-world deployment.","In this study, we identify vulnerabilities within the hidden layers of neural networks and introduce a practical approach to reinforce traditional adversarial training.","Our method incorporates a novel distance loss function, minimizing the gap between hidden layers in clean and adversarial images.","Experiments demonstrate satisfactory performance in improving the model's robustness against adversarial perturbations."],"url":"http://arxiv.org/abs/2402.08763v1","category":"cs.CV"}
{"created":"2024-02-13 19:54:03","title":"Entanglement of vortices in the Ginzburg--Landau equations for superconductors","abstract":"In 1988, Nelson proposed that neighboring vortex lines in high-temperature superconductors may become entangled with each other. In this article we construct solutions to the Ginzburg--Landau equations which indeed have this property, as they exhibit entangled vortex lines of arbitrary topological complexity.","sentences":["In 1988, Nelson proposed that neighboring vortex lines in high-temperature superconductors may become entangled with each other.","In this article we construct solutions to the Ginzburg--Landau equations which indeed have this property, as they exhibit entangled vortex lines of arbitrary topological complexity."],"url":"http://arxiv.org/abs/2402.08760v1","category":"math-ph"}
{"created":"2024-02-13 19:52:41","title":"Spectral instability of peakons for the $b$-family of Novikov equations","abstract":"{{In this paper}}, we are concerned with a one-parameter family of peakon equations with cubic nonlinearity parametrized by a parameter usually denoted by the letter $b$. This family is called the ``$b$-Novikov'' since it reduces to the integrable Novikov equation in the case $b=3$. By extending the corresponding linearized operator defined on functions in $H^1(\\mathbb{R})$ to one defined on weaker functions on $L^2(\\mathbb{R})$, we prove spectral and linear instability on $\\LT$ of peakons in the $b$-Novikov equations for any $b$. We also consider the stability on $\\HT$ and show that the peakons are spectrally or linearly stable only in the case $b=3$.","sentences":["{{In this paper}}, we are concerned with a one-parameter family of peakon equations with cubic nonlinearity parametrized by a parameter usually denoted by the letter $b$. This family is called the ``$b$-Novikov'' since it reduces to the integrable Novikov equation in the case $b=3$. By extending the corresponding linearized operator defined on functions in $H^1(\\mathbb{R})$ to one defined on weaker functions on $L^2(\\mathbb{R})$, we prove spectral and linear instability on $\\LT$ of peakons in the $b$-Novikov equations for any $b$. We also consider the stability on $\\HT$ and show that the peakons are spectrally or linearly stable only in the case $b=3$."],"url":"http://arxiv.org/abs/2402.08759v1","category":"math.AP"}
{"created":"2024-02-13 19:50:51","title":"Nonlinear, non-signaling Schr\u00f6dinger equation","abstract":"A nonlinear extension of Schr\\\"odinger's wave equation is proposed that ensures non-signaling by keeping linear the evolution of \\textit{coordinate-diagonal} elements of the density matrix. The equation contains a negative kinetic energy term that turns spreading of wave packets into its opposite: collapsing, as some effective mass $M$ grows beyond a universal critical mass, estimated to be about $\\mu = 2\\cdot10^{-23}~$kg; then linear quantum kinetic energy gets negligible, which marks the quantum-classical border. Interference of large molecules is suggested for an experimental check of the proposed framework.","sentences":["A nonlinear extension of Schr\\\"odinger's wave equation is proposed that ensures non-signaling by keeping linear the evolution of \\textit{coordinate-diagonal} elements of the density matrix.","The equation contains a negative kinetic energy term that turns spreading of wave packets into its opposite: collapsing, as some effective mass $M$ grows beyond a universal critical mass, estimated to be about $\\mu = 2\\cdot10^{-23}~$kg; then linear quantum kinetic energy gets negligible, which marks the quantum-classical border.","Interference of large molecules is suggested for an experimental check of the proposed framework."],"url":"http://arxiv.org/abs/2402.08757v1","category":"quant-ph"}
{"created":"2024-02-13 19:33:41","title":"Nearest Neighbor Representations of Neurons","abstract":"The Nearest Neighbor (NN) Representation is an emerging computational model that is inspired by the brain. We study the complexity of representing a neuron (threshold function) using the NN representations. It is known that two anchors (the points to which NN is computed) are sufficient for a NN representation of a threshold function, however, the resolution (the maximum number of bits required for the entries of an anchor) is $O(n\\log{n})$. In this work, the trade-off between the number of anchors and the resolution of a NN representation of threshold functions is investigated. We prove that the well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which require 2 or 3 anchors and resolution of $O(n)$, can be represented by polynomially large number of anchors in $n$ and $O(\\log{n})$ resolution. We conjecture that for all threshold functions, there are NN representations with polynomially large size and logarithmic resolution in $n$.","sentences":["The Nearest Neighbor (NN) Representation is an emerging computational model that is inspired by the brain.","We study the complexity of representing a neuron (threshold function) using the NN representations.","It is known that two anchors (the points to which NN is computed) are sufficient for a NN representation of a threshold function, however, the resolution (the maximum number of bits required for the entries of an anchor) is $O(n\\log{n})$.","In this work, the trade-off between the number of anchors and the resolution of a NN representation of threshold functions is investigated.","We prove that the well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which require 2 or 3 anchors and resolution of $O(n)$, can be represented by polynomially large number of anchors in $n$ and $O(\\log{n})$ resolution.","We conjecture that for all threshold functions, there are NN representations with polynomially large size and logarithmic resolution in $n$."],"url":"http://arxiv.org/abs/2402.08748v1","category":"cs.CC"}
{"created":"2024-02-13 19:00:08","title":"Image deconvolution and PSF reconstruction with STARRED: a wavelet-based two-channel method optimized for light curve extraction","abstract":"We present STARRED, a Point Spread Function (PSF) reconstruction, two-channel deconvolution, and light curve extraction method designed for high-precision photometric measurements in imaging time series. An improved resolution of the data is targeted rather than an infinite one, thereby minimizing deconvolution artifacts. In addition, STARRED performs a joint deconvolution of all available data, accounting for epoch-to-epoch variations of the PSF and decomposing the resulting deconvolved image into a point source and an extended source channel. The output is a deep sharp frame combining all data, and the photometry of all point sources in the field of view as a function of time. Of note, STARRED also provides exquisite PSF models for each data frame. We showcase three applications of STARRED in the context of the imminent LSST survey and of JWST imaging: i) the extraction of supernovae light curves and the scene representation of their host galaxy, ii) the extraction of lensed quasar light curves for time-delay cosmography, and iii) the measurement of the spectral energy distribution of globular clusters in the \"Sparkler\", a galaxy at redshift z=1.378 strongly lensed by the galaxy cluster SMACS J0723.3-7327. STARRED is implemented in JAX, leveraging automatic differentiation and GPU acceleration. This enables rapid processing of large time-domain datasets, positioning the method as a powerful tool for extracting light curves from the multitude of lensed or unlensed variable and transient objects in the Rubin-LSST data, even when blended with intervening objects.","sentences":["We present STARRED, a Point Spread Function (PSF) reconstruction, two-channel deconvolution, and light curve extraction method designed for high-precision photometric measurements in imaging time series.","An improved resolution of the data is targeted rather than an infinite one, thereby minimizing deconvolution artifacts.","In addition, STARRED performs a joint deconvolution of all available data, accounting for epoch-to-epoch variations of the PSF and decomposing the resulting deconvolved image into a point source and an extended source channel.","The output is a deep sharp frame combining all data, and the photometry of all point sources in the field of view as a function of time.","Of note, STARRED also provides exquisite PSF models for each data frame.","We showcase three applications of STARRED in the context of the imminent LSST survey and of JWST imaging: i) the extraction of supernovae light curves and the scene representation of their host galaxy, ii) the extraction of lensed quasar light curves for time-delay cosmography, and iii) the measurement of the spectral energy distribution of globular clusters in the \"Sparkler\", a galaxy at redshift z=1.378 strongly lensed by the galaxy cluster SMACS J0723.3-7327.","STARRED is implemented in JAX, leveraging automatic differentiation and GPU acceleration.","This enables rapid processing of large time-domain datasets, positioning the method as a powerful tool for extracting light curves from the multitude of lensed or unlensed variable and transient objects in the Rubin-LSST data, even when blended with intervening objects."],"url":"http://arxiv.org/abs/2402.08725v1","category":"astro-ph.IM"}
{"created":"2024-02-13 19:00:08","title":"Trained quantum neural networks are Gaussian processes","abstract":"We study quantum neural networks made by parametric one-qubit gates and fixed two-qubit gates in the limit of infinite width, where the generated function is the expectation value of the sum of single-qubit observables over all the qubits. First, we prove that the probability distribution of the function generated by the untrained network with randomly initialized parameters converges in distribution to a Gaussian process whenever each measured qubit is correlated only with few other measured qubits. Then, we analytically characterize the training of the network via gradient descent with square loss on supervised learning problems. We prove that, as long as the network is not affected by barren plateaus, the trained network can perfectly fit the training set and that the probability distribution of the function generated after training still converges in distribution to a Gaussian process. Finally, we consider the statistical noise of the measurement at the output of the network and prove that a polynomial number of measurements is sufficient for all the previous results to hold and that the network can always be trained in polynomial time.","sentences":["We study quantum neural networks made by parametric one-qubit gates and fixed two-qubit gates in the limit of infinite width, where the generated function is the expectation value of the sum of single-qubit observables over all the qubits.","First, we prove that the probability distribution of the function generated by the untrained network with randomly initialized parameters converges in distribution to a Gaussian process whenever each measured qubit is correlated only with few other measured qubits.","Then, we analytically characterize the training of the network via gradient descent with square loss on supervised learning problems.","We prove that, as long as the network is not affected by barren plateaus, the trained network can perfectly fit the training set and that the probability distribution of the function generated after training still converges in distribution to a Gaussian process.","Finally, we consider the statistical noise of the measurement at the output of the network and prove that a polynomial number of measurements is sufficient for all the previous results to hold and that the network can always be trained in polynomial time."],"url":"http://arxiv.org/abs/2402.08726v1","category":"quant-ph"}
{"created":"2024-02-13 19:00:00","title":"Axion dark matter from inflation-driven quantum phase transition","abstract":"We propose a new mechanism to produce axion dark matter from inflationary fluctuations. Quantum fluctuations during inflation are strengthened by a coupling of the axion kinetic term to the inflaton, which we parametrize as an effective curvature $\\kappa$ in the axion equation of motion. A nonvanishing curvature breaks the scale invariance of the axion power spectrum, driving a quantum phase transition with $\\kappa$ as the order parameter. The axion power spectrum is proportional to the inverse comoving horizon to the power of $\\kappa$. For positive $\\kappa$ the spectrum gets a red tilt, leading to an exponential enhancement of the axion abundance as the comoving horizon shrinks during inflation. This enhancement allows sufficient axion production to comprise the entire dark matter relic abundance despite the ultralight mass. Our mechanism predicts a significantly different parameter space from the usual misalignment mechanism. It allows for axion-like particle dark matter with a much lower decay constant and thus a larger coupling to Standard Model particles. Much of the parameter space can be probed by future experiments including haloscopes, nuclear clocks, and CMB-S4. We can also generate heavier QCD axion dark matter than the misalignment mechanism.","sentences":["We propose a new mechanism to produce axion dark matter from inflationary fluctuations.","Quantum fluctuations during inflation are strengthened by a coupling of the axion kinetic term to the inflaton, which we parametrize as an effective curvature $\\kappa$ in the axion equation of motion.","A nonvanishing curvature breaks the scale invariance of the axion power spectrum, driving a quantum phase transition with $\\kappa$ as the order parameter.","The axion power spectrum is proportional to the inverse comoving horizon to the power of $\\kappa$. For positive $\\kappa$ the spectrum gets a red tilt, leading to an exponential enhancement of the axion abundance as the comoving horizon shrinks during inflation.","This enhancement allows sufficient axion production to comprise the entire dark matter","relic abundance despite the ultralight mass.","Our mechanism predicts a significantly different parameter space from the usual misalignment mechanism.","It allows for axion-like particle dark matter with a much lower decay constant and thus a larger coupling to Standard Model particles.","Much of the parameter space can be probed by future experiments including haloscopes, nuclear clocks, and CMB-S4.","We can also generate heavier QCD axion dark matter than the misalignment mechanism."],"url":"http://arxiv.org/abs/2402.08716v1","category":"hep-ph"}
{"created":"2024-02-13 18:31:55","title":"Correction to \"Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations\"","abstract":"A method for analyzing non-asymptotic guarantees of numerical discretizations of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and Zygalakis in ``Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations\". They analyze the UBU integrator which is strong order two and only requires one gradient evaluation per step, resulting in desirable non-asymptotic guarantees, in particular $\\mathcal{O}(d^{1/4}\\epsilon^{-1/2})$ steps to reach a distance of $\\epsilon > 0$ in Wasserstein-2 distance away from the target distribution. However, there is a mistake in the local error estimates in Sanz-Serna and Zygalakis (2021), in particular, a stronger assumption is needed to achieve these complexity estimates. This note reconciles the theory with the dimension dependence observed in practice in many applications of interest.","sentences":["A method for analyzing non-asymptotic guarantees of numerical discretizations of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and Zygalakis in ``Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations\".","They analyze the UBU integrator which is strong order two and only requires one gradient evaluation per step, resulting in desirable non-asymptotic guarantees, in particular $\\mathcal{O}(d^{1/4}\\epsilon^{-1/2})$ steps to reach a distance of $\\epsilon > 0$ in Wasserstein-2 distance away from the target distribution.","However, there is a mistake in the local error estimates in Sanz-Serna and Zygalakis (2021), in particular, a stronger assumption is needed to achieve these complexity estimates.","This note reconciles the theory with the dimension dependence observed in practice in many applications of interest."],"url":"http://arxiv.org/abs/2402.08711v1","category":"stat.ML"}
{"created":"2024-02-13 18:23:31","title":"Applications of multiplicative functions to arithmetic statistics and Diophantine equations","abstract":"We prove matching upper and lower bounds for the average of the 6-torsion of class groups of quadratic fields.   Furthermore, we count the number of integer solutions on an affine quartic surface.   These, and other, applications follow from a result that we establish in this paper regarding the average of multiplicative functions over a general integer sequence.","sentences":["We prove matching upper and lower bounds for the average of the 6-torsion of class groups of quadratic fields.   ","Furthermore, we count the number of integer solutions on an affine quartic surface.   ","These, and other, applications follow from a result that we establish in this paper regarding the average of multiplicative functions over a general integer sequence."],"url":"http://arxiv.org/abs/2402.08710v1","category":"math.NT"}
{"created":"2024-02-13 17:53:44","title":"Zero Shot Molecular Generation via Similarity Kernels","abstract":"Generative modelling aims to accelerate the discovery of novel chemicals by directly proposing structures with desirable properties. Recently, score-based, or diffusion, generative models have significantly outperformed previous approaches. Key to their success is the close relationship between the score and physical force, allowing the use of powerful equivariant neural networks. However, the behaviour of the learnt score is not yet well understood. Here, we analyse the score by training an energy-based diffusion model for molecular generation. We find that during the generation the score resembles a restorative potential initially and a quantum-mechanical force at the end. In between the two endpoints, it exhibits special properties that enable the building of large molecules. Using insights from the trained model, we present Similarity-based Molecular Generation (SiMGen), a new method for zero shot molecular generation. SiMGen combines a time-dependent similarity kernel with descriptors from a pretrained machine learning force field to generate molecules without any further training. Our approach allows full control over the molecular shape through point cloud priors and supports conditional generation. We also release an interactive web tool that allows users to generate structures with SiMGen online (https://zndraw.icp.uni-stuttgart.de).","sentences":["Generative modelling aims to accelerate the discovery of novel chemicals by directly proposing structures with desirable properties.","Recently, score-based, or diffusion, generative models have significantly outperformed previous approaches.","Key to their success is the close relationship between the score and physical force, allowing the use of powerful equivariant neural networks.","However, the behaviour of the learnt score is not yet well understood.","Here, we analyse the score by training an energy-based diffusion model for molecular generation.","We find that during the generation the score resembles a restorative potential initially and a quantum-mechanical force at the end.","In between the two endpoints, it exhibits special properties that enable the building of large molecules.","Using insights from the trained model, we present Similarity-based Molecular Generation (SiMGen), a new method for zero shot molecular generation.","SiMGen combines a time-dependent similarity kernel with descriptors from a pretrained machine learning force field to generate molecules without any further training.","Our approach allows full control over the molecular shape through point cloud priors and supports conditional generation.","We also release an interactive web tool that allows users to generate structures with SiMGen online (https://zndraw.icp.uni-stuttgart.de)."],"url":"http://arxiv.org/abs/2402.08708v1","category":"physics.chem-ph"}
{"created":"2024-02-14 18:20:44","title":"Loss Shaping Constraints for Long-Term Time Series Forecasting","abstract":"Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap. We propose a practical Primal-Dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting benchmarks, while shaping the distribution of errors across the predicted window.","sentences":["Several applications in time series forecasting require predicting multiple steps ahead.","Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window.","We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks.","That is, optimising performance on average can lead to undesirably large errors at specific time-steps.","In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step.","We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap.","We propose a practical Primal-Dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting benchmarks, while shaping the distribution of errors across the predicted window."],"url":"http://arxiv.org/abs/2402.09373v1","category":"cs.LG"}
{"created":"2024-02-14 17:18:03","title":"Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production","abstract":"National Statistical Organizations (NSOs) increasingly draw on Machine Learning (ML) to improve the timeliness and cost-effectiveness of their products. When introducing ML solutions, NSOs must ensure that high standards with respect to robustness, reproducibility, and accuracy are upheld as codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA; Yung et al. 2022). At the same time, a growing body of research focuses on fairness as a pre-condition of a safe deployment of ML to prevent disparate social impacts in practice. However, fairness has not yet been explicitly discussed as a quality aspect in the context of the application of ML at NSOs. We employ Yung et al. (2022)'s QF4SA quality framework and present a mapping of its quality dimensions to algorithmic fairness. We thereby extend the QF4SA framework in several ways: we argue for fairness as its own quality dimension, we investigate the interaction of fairness with other dimensions, and we explicitly address data, both on its own and its interaction with applied methodology. In parallel with empirical illustrations, we show how our mapping can contribute to methodology in the domains of official statistics, algorithmic fairness, and trustworthy machine learning.","sentences":["National Statistical Organizations (NSOs) increasingly draw on Machine Learning (ML) to improve the timeliness and cost-effectiveness of their products.","When introducing ML solutions, NSOs must ensure that high standards with respect to robustness, reproducibility, and accuracy are upheld as codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA; Yung et al. 2022).","At the same time, a growing body of research focuses on fairness as a pre-condition of a safe deployment of ML to prevent disparate social impacts in practice.","However, fairness has not yet been explicitly discussed as a quality aspect in the context of the application of ML at NSOs.","We employ Yung et al.","(2022)'s QF4SA quality framework and present a mapping of its quality dimensions to algorithmic fairness.","We thereby extend the QF4SA framework in several ways: we argue for fairness as its own quality dimension, we investigate the interaction of fairness with other dimensions, and we explicitly address data, both on its own and its interaction with applied methodology.","In parallel with empirical illustrations, we show how our mapping can contribute to methodology in the domains of official statistics, algorithmic fairness, and trustworthy machine learning."],"url":"http://arxiv.org/abs/2402.09328v1","category":"stat.ML"}
{"created":"2024-02-14 17:10:01","title":"Few-Shot Object Detection with Sparse Context Transformers","abstract":"Few-shot detection is a major task in pattern recognition which seeks to localize objects using models trained with few labeled data. One of the mainstream few-shot methods is transfer learning which consists in pretraining a detection model in a source domain prior to its fine-tuning in a target domain. However, it is challenging for fine-tuned models to effectively identify new classes in the target domain, particularly when the underlying labeled training data are scarce. In this paper, we devise a novel sparse context transformer (SCT) that effectively leverages object knowledge in the source domain, and automatically learns a sparse context from only few training images in the target domain. As a result, it combines different relevant clues in order to enhance the discrimination power of the learned detectors and reduce class confusion. We evaluate the proposed method on two challenging few-shot object detection benchmarks, and empirical results show that the proposed method obtains competitive performance compared to the related state-of-the-art.","sentences":["Few-shot detection is a major task in pattern recognition which seeks to localize objects using models trained with few labeled data.","One of the mainstream few-shot methods is transfer learning which consists in pretraining a detection model in a source domain prior to its fine-tuning in a target domain.","However, it is challenging for fine-tuned models to effectively identify new classes in the target domain, particularly when the underlying labeled training data are scarce.","In this paper, we devise a novel sparse context transformer (SCT) that effectively leverages object knowledge in the source domain, and automatically learns a sparse context from only few training images in the target domain.","As a result, it combines different relevant clues in order to enhance the discrimination power of the learned detectors and reduce class confusion.","We evaluate the proposed method on two challenging few-shot object detection benchmarks, and empirical results show that the proposed method obtains competitive performance compared to the related state-of-the-art."],"url":"http://arxiv.org/abs/2402.09315v1","category":"cs.CV"}
{"created":"2024-02-14 13:47:35","title":"Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization","abstract":"The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs development guidelines. We conduct extensive experiments on 4 prominent LLMs and 5 benchmark text classification datasets. The results show that our instruction backdoor attacks achieve the desired attack performance without compromising utility. Additionally, we propose an instruction-ignoring defense mechanism and demonstrate its partial effectiveness in mitigating such attacks. Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.","sentences":["The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs.","These solutions facilitate tailored LLM creation via natural language prompts without coding.","However, the trustworthiness of third-party custom versions of LLMs remains an essential concern.","In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs).","Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers.","Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness.","We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs development guidelines.","We conduct extensive experiments on 4 prominent LLMs and 5 benchmark text classification datasets.","The results show that our instruction backdoor attacks achieve the desired attack performance without compromising utility.","Additionally, we propose an instruction-ignoring defense mechanism and demonstrate its partial effectiveness in mitigating such attacks.","Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs."],"url":"http://arxiv.org/abs/2402.09179v1","category":"cs.CR"}
{"created":"2024-02-14 13:44:16","title":"Nearly Optimal Regret for Decentralized Online Convex Optimization","abstract":"We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\\rho^{-1/2}\\sqrt{T})$ and ${O}(n^{3/2}\\rho^{-1}\\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\\Omega(n\\sqrt{T})$ for convex functions and $\\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\\tilde{O}(n\\rho^{-1/4}\\sqrt{T})$ and $\\tilde{O}(n\\rho^{-1/2}\\log T)$. The primary technique is to design an online accelerated gossip strategy that enjoys a faster average consensus among local learners. Furthermore, by carefully exploiting the spectral properties of a specific network topology, we enhance the lower bounds for convex and strongly convex functions to $\\Omega(n\\rho^{-1/4}\\sqrt{T})$ and $\\Omega(n\\rho^{-1/2})$, respectively. These lower bounds suggest that our algorithms are nearly optimal in terms of $T$, $n$, and $\\rho$.","sentences":["We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications.","Previous studies have established $O(n^{5/4}\\rho^{-1/2}\\sqrt{T})$ and ${O}(n^{3/2}\\rho^{-1}\\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon.","However, there exist large gaps from the existing lower bounds, i.e., $\\Omega(n\\sqrt{T})$ for convex functions and $\\Omega(n)$ for strongly convex functions.","To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\\tilde{O}(n\\rho^{-1/4}\\sqrt{T})$ and $\\tilde{O}(n\\rho^{-1/2}\\log T)$.","The primary technique is to design an online accelerated gossip strategy that enjoys a faster average consensus among local learners.","Furthermore, by carefully exploiting the spectral properties of a specific network topology, we enhance the lower bounds for convex and strongly convex functions to $\\Omega(n\\rho^{-1/4}\\sqrt{T})$ and $\\Omega(n\\rho^{-1/2})$, respectively.","These lower bounds suggest that our algorithms are nearly optimal in terms of $T$, $n$, and $\\rho$."],"url":"http://arxiv.org/abs/2402.09173v1","category":"cs.LG"}
{"created":"2024-02-14 13:36:20","title":"Evolving Restricted Boltzmann Machine-Kohonen Network for Online Clustering","abstract":"A novel online clustering algorithm is presented where an Evolving Restricted Boltzmann Machine (ERBM) is embedded with a Kohonen Network called ERBM-KNet. The proposed ERBM-KNet efficiently handles streaming data in a single-pass mode using the ERBM, employing a bias-variance strategy for neuron growing and pruning, as well as online clustering based on a cluster update strategy for cluster prediction and cluster center update using KNet. Initially, ERBM evolves its architecture while processing unlabeled image data, effectively disentangling the data distribution in the latent space. Subsequently, the KNet utilizes the feature extracted from ERBM to predict the number of clusters and updates the cluster centers. By overcoming the common challenges associated with clustering algorithms, such as prior initialization of the number of clusters and subpar clustering accuracy, the proposed ERBM-KNet offers significant improvements. Extensive experimental evaluations on four benchmarks and one industry dataset demonstrate the superiority of ERBM-KNet compared to state-of-the-art approaches.","sentences":["A novel online clustering algorithm is presented where an Evolving Restricted Boltzmann Machine (ERBM) is embedded with a Kohonen Network called ERBM-KNet.","The proposed ERBM-KNet efficiently handles streaming data in a single-pass mode using the ERBM, employing a bias-variance strategy for neuron growing and pruning, as well as online clustering based on a cluster update strategy for cluster prediction and cluster center update using KNet.","Initially, ERBM evolves its architecture while processing unlabeled image data, effectively disentangling the data distribution in the latent space.","Subsequently, the KNet utilizes the feature extracted from ERBM to predict the number of clusters and updates the cluster centers.","By overcoming the common challenges associated with clustering algorithms, such as prior initialization of the number of clusters and subpar clustering accuracy, the proposed ERBM-KNet offers significant improvements.","Extensive experimental evaluations on four benchmarks and one industry dataset demonstrate the superiority of ERBM-KNet compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.09167v1","category":"cs.LG"}
{"created":"2024-02-14 12:18:23","title":"Mixed-Output Gaussian Process Latent Variable Models","abstract":"This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. Our key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. Our framework allows the use of a range of priors for the weights of each observation. This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temperatures, a simulated data set for identifying flow configuration through a pipe, and a data set for determining the type of rock from its reflectance.","sentences":["This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables.","Our key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations.","Our framework allows the use of a range of priors for the weights of each observation.","This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification.","Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample.","To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temperatures, a simulated data set for identifying flow configuration through a pipe, and a data set for determining the type of rock from its reflectance."],"url":"http://arxiv.org/abs/2402.09122v1","category":"stat.ML"}
{"created":"2024-02-14 11:55:50","title":"Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space","abstract":"Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning. Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm. Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution. The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space. We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour of any RL algorithm, and also allows us to compare the exploratory behaviours of different RL algorithms.","sentences":["Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning.","Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm.","Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution.","The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space.","We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour of any RL algorithm, and also allows us to compare the exploratory behaviours of different RL algorithms."],"url":"http://arxiv.org/abs/2402.09113v1","category":"cs.LG"}
{"created":"2024-02-14 11:26:30","title":"Scheduling for On-Board Federated Learning with Satellite Clusters","abstract":"Mega-constellations of small satellites have evolved into a source of massive amount of valuable data. To manage this data efficiently, on-board federated learning (FL) enables satellites to train a machine learning (ML) model collaboratively without having to share the raw data. This paper introduces a scheme for scheduling on-board FL for constellations connected with intra-orbit inter-satellite links. The proposed scheme utilizes the predictable visibility pattern between satellites and ground station (GS), both at the individual satellite level and cumulatively within the entire orbit, to mitigate intermittent connectivity and best use of available time. To this end, two distinct schedulers are employed: one for coordinating the FL procedures among orbits, and the other for controlling those within each orbit. These two schedulers cooperatively determine the appropriate time to perform global updates in GS and then allocate suitable duration to satellites within each orbit for local training, proportional to usable time until next global update. This scheme leads to improved test accuracy within a shorter time.","sentences":["Mega-constellations of small satellites have evolved into a source of massive amount of valuable data.","To manage this data efficiently, on-board federated learning (FL) enables satellites to train a machine learning (ML) model collaboratively without having to share the raw data.","This paper introduces a scheme for scheduling on-board FL for constellations connected with intra-orbit inter-satellite links.","The proposed scheme utilizes the predictable visibility pattern between satellites and ground station (GS), both at the individual satellite level and cumulatively within the entire orbit, to mitigate intermittent connectivity and best use of available time.","To this end, two distinct schedulers are employed: one for coordinating the FL procedures among orbits, and the other for controlling those within each orbit.","These two schedulers cooperatively determine the appropriate time to perform global updates in GS and then allocate suitable duration to satellites within each orbit for local training, proportional to usable time until next global update.","This scheme leads to improved test accuracy within a shorter time."],"url":"http://arxiv.org/abs/2402.09105v1","category":"cs.DC"}
{"created":"2024-02-14 10:08:24","title":"Distributed Sensing Along Fibres for Smart Clothing","abstract":"Textile sensors transform our everyday clothing into a means to track movement and bio-signals in a completely unobtrusive way. One major hindrance to the adoption of \"smart\" clothing is the difficulty encountered with connections and space when scaling up the number of sensors. There is a lack of research addressing a key limitation in wearable electronics: connections between rigid and textile elements are often unreliable and they require interfacing sensors in a way incompatible with textile mass production methods. We introduce a prototype garment, compact readout circuit, and algorithm to measure localized strain along multiple regions of a fibre. We employ a helical auxetic yarn sensor with tunable sensitivity along its length to selectively respond to strain signals. We demonstrate distributed sensing in clothing, monitoring arm joint angles from a single continuous fibre. Compared to optical motion capture, we achieve around 5{\\deg} error in reconstructing shoulder, elbow, and wrist joint angles.","sentences":["Textile sensors transform our everyday clothing into a means to track movement and bio-signals in a completely unobtrusive way.","One major hindrance to the adoption of \"smart\" clothing is the difficulty encountered with connections and space when scaling up the number of sensors.","There is a lack of research addressing a key limitation in wearable electronics: connections between rigid and textile elements are often unreliable and they require interfacing sensors in a way incompatible with textile mass production methods.","We introduce a prototype garment, compact readout circuit, and algorithm to measure localized strain along multiple regions of a fibre.","We employ a helical auxetic yarn sensor with tunable sensitivity along its length to selectively respond to strain signals.","We demonstrate distributed sensing in clothing, monitoring arm joint angles from a single continuous fibre.","Compared to optical motion capture, we achieve around 5{\\deg} error in reconstructing shoulder, elbow, and wrist joint angles."],"url":"http://arxiv.org/abs/2402.09057v1","category":"eess.SP"}
{"created":"2024-02-14 07:52:00","title":"Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic Shortest Path","abstract":"We study the Stochastic Shortest Path (SSP) problem with a linear mixture transition kernel, where an agent repeatedly interacts with a stochastic environment and seeks to reach certain goal state while minimizing the cumulative cost. Existing works often assume a strictly positive lower bound of the cost function or an upper bound of the expected length for the optimal policy. In this paper, we propose a new algorithm to eliminate these restrictive assumptions. Our algorithm is based on extended value iteration with a fine-grained variance-aware confidence set, where the variance is estimated recursively from high-order moments. Our algorithm achieves an $\\tilde{\\mathcal O}(dB_*\\sqrt{K})$ regret bound, where $d$ is the dimension of the feature mapping in the linear transition kernel, $B_*$ is the upper bound of the total cumulative cost for the optimal policy, and $K$ is the number of episodes. Our regret upper bound matches the $\\Omega(dB_*\\sqrt{K})$ lower bound of linear mixture SSPs in Min et al. (2022), which suggests that our algorithm is nearly minimax optimal.","sentences":["We study the Stochastic Shortest Path (SSP) problem with a linear mixture transition kernel, where an agent repeatedly interacts with a stochastic environment and seeks to reach certain goal state while minimizing the cumulative cost.","Existing works often assume a strictly positive lower bound of the cost function or an upper bound of the expected length for the optimal policy.","In this paper, we propose a new algorithm to eliminate these restrictive assumptions.","Our algorithm is based on extended value iteration with a fine-grained variance-aware confidence set, where the variance is estimated recursively from high-order moments.","Our algorithm achieves an $\\tilde{\\mathcal O}(dB_*\\sqrt{K})$ regret bound, where $d$ is the dimension of the feature mapping in the linear transition kernel, $B_*$ is the upper bound of the total cumulative cost for the optimal policy, and $K$ is the number of episodes.","Our regret upper bound matches the $\\Omega(dB_*\\sqrt{K})$ lower bound of linear mixture SSPs in Min et al. (2022), which suggests that our algorithm is nearly minimax optimal."],"url":"http://arxiv.org/abs/2402.08998v1","category":"cs.LG"}
{"created":"2024-02-14 07:48:40","title":"Multi-Task Learning of Active Fault-Tolerant Controller for Leg Failures in Quadruped robots","abstract":"Electric quadruped robots used in outdoor exploration are susceptible to leg-related electrical or mechanical failures. Unexpected joint power loss and joint locking can immediately pose a falling threat. Typically, controllers lack the capability to actively sense the condition of their own joints and take proactive actions. Maintaining the original motion patterns could lead to disastrous consequences, as the controller may produce irrational output within a short period of time, further creating the risk of serious physical injuries. This paper presents a hierarchical fault-tolerant control scheme employing a multi-task training architecture capable of actively perceiving and overcoming two types of leg joint faults. The architecture simultaneously trains three joint task policies for health, power loss, and locking scenarios in parallel, introducing a symmetric reflection initialization technique to ensure rapid and stable gait skill transformations. Experiments demonstrate that the control scheme is robust in unexpected scenarios where a single leg experiences concurrent joint faults in two joints. Furthermore, the policy retains the robot's planar mobility, enabling rough velocity tracking. Finally, zero-shot Sim2Real transfer is achieved on the real-world SOLO8 robot, countering both electrical and mechanical failures.","sentences":["Electric quadruped robots used in outdoor exploration are susceptible to leg-related electrical or mechanical failures.","Unexpected joint power loss and joint locking can immediately pose a falling threat.","Typically, controllers lack the capability to actively sense the condition of their own joints and take proactive actions.","Maintaining the original motion patterns could lead to disastrous consequences, as the controller may produce irrational output within a short period of time, further creating the risk of serious physical injuries.","This paper presents a hierarchical fault-tolerant control scheme employing a multi-task training architecture capable of actively perceiving and overcoming two types of leg joint faults.","The architecture simultaneously trains three joint task policies for health, power loss, and locking scenarios in parallel, introducing a symmetric reflection initialization technique to ensure rapid and stable gait skill transformations.","Experiments demonstrate that the control scheme is robust in unexpected scenarios where a single leg experiences concurrent joint faults in two joints.","Furthermore, the policy retains the robot's planar mobility, enabling rough velocity tracking.","Finally, zero-shot Sim2Real transfer is achieved on the real-world SOLO8 robot, countering both electrical and mechanical failures."],"url":"http://arxiv.org/abs/2402.08996v1","category":"cs.RO"}
{"created":"2024-02-14 05:08:47","title":"Evaluating DTW Measures via a Synthesis Framework for Time-Series Data","abstract":"Time-series data originate from various applications that describe specific observations or quantities of interest over time. Their analysis often involves the comparison across different time-series data sequences, which in turn requires the alignment of these sequences. Dynamic Time Warping (DTW) is the standard approach to achieve an optimal alignment between two temporal signals. Different variations of DTW have been proposed to address various needs for signal alignment or classifications. However, a comprehensive evaluation of their performance in these time-series data processing tasks is lacking. Most DTW measures perform well on certain types of time-series data without a clear explanation of the reason. To address that, we propose a synthesis framework to model the variation between two time-series data sequences for comparison. Our synthesis framework can produce a realistic initial signal and deform it with controllable variations that mimic real-world scenarios. With this synthesis framework, we produce a large number of time-series sequence pairs with different but known variations, which are used to assess the performance of a number of well-known DTW measures for the tasks of alignment and classification. We report their performance on different variations and suggest the proper DTW measure to use based on the type of variations between two time-series sequences. This is the first time such a guideline is presented for selecting a proper DTW measure. To validate our conclusion, we apply our findings to real-world applications, i.e., the detection of the formation top for the oil and gas industry and the pattern search in streamlines for flow visualization.","sentences":["Time-series data originate from various applications that describe specific observations or quantities of interest over time.","Their analysis often involves the comparison across different time-series data sequences, which in turn requires the alignment of these sequences.","Dynamic Time Warping (DTW) is the standard approach to achieve an optimal alignment between two temporal signals.","Different variations of DTW have been proposed to address various needs for signal alignment or classifications.","However, a comprehensive evaluation of their performance in these time-series data processing tasks is lacking.","Most DTW measures perform well on certain types of time-series data without a clear explanation of the reason.","To address that, we propose a synthesis framework to model the variation between two time-series data sequences for comparison.","Our synthesis framework can produce a realistic initial signal and deform it with controllable variations that mimic real-world scenarios.","With this synthesis framework, we produce a large number of time-series sequence pairs with different but known variations, which are used to assess the performance of a number of well-known DTW measures for the tasks of alignment and classification.","We report their performance on different variations and suggest the proper DTW measure to use based on the type of variations between two time-series sequences.","This is the first time such a guideline is presented for selecting a proper DTW measure.","To validate our conclusion, we apply our findings to real-world applications, i.e., the detection of the formation top for the oil and gas industry and the pattern search in streamlines for flow visualization."],"url":"http://arxiv.org/abs/2402.08943v1","category":"cs.LG"}
{"created":"2024-02-14 03:45:26","title":"IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture","abstract":"This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two main innovations: a data-driven strategy for optimal IMU placement and a transformer-based model architecture for time series analysis. Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the transformer architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of transformers, provides significant improvements to the field of IMU-based pose estimation.","sentences":["This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional RNNs.","We introduce two main innovations: a data-driven strategy for optimal IMU placement and a transformer-based model architecture for time series analysis.","Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the transformer architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs.","The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of transformers, provides significant improvements to the field of IMU-based pose estimation."],"url":"http://arxiv.org/abs/2402.08923v1","category":"cs.LG"}
{"created":"2024-02-14 01:56:31","title":"Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation","abstract":"Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset, WISS achieves distinct improvements with regard to two different backbones. For the second dataset, WISS achieves dice coefficients of $91.7\\%$ and $83.7\\%$ for mid-sagittal slices and 3D CT volumes, respectively, saving a lot of labeling costs and only sacrificing a little segmentation performance.","sentences":["Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases.","However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate.","In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs.","WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner.","This self-training method alternates between training and refining labels in the training set.","Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations.","We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset.","On the first dataset, WISS achieves distinct improvements with regard to two different backbones.","For the second dataset, WISS achieves dice coefficients of $91.7\\%$ and $83.7\\%$ for mid-sagittal slices and 3D CT volumes, respectively, saving a lot of labeling costs and only sacrificing a little segmentation performance."],"url":"http://arxiv.org/abs/2402.08892v1","category":"cs.CV"}
{"created":"2024-02-14 01:41:50","title":"Predicting the Emergence of Solar Active Regions Using Machine Learning","abstract":"To create early warning capabilities for upcoming Space Weather disturbances, we have selected a dataset of 61 emerging active regions, which allows us to identify characteristic features in the evolution of acoustic power density to predict continuum intensity emergence. For our study, we have utilized Doppler shift and continuum intensity observations from the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO). The local tracking of 30.66 x 30.66-degree patches in the vicinity of active regions allowed us to trace the evolution of active regions starting from the pre-emergence state. We have developed a machine learning model to capture the acoustic power flux density variations associated with upcoming magnetic flux emergence. The trained Long Short-Term Memory (LSTM) model is able to predict 5 hours ahead whether, in a given area of the solar surface, continuum intensity values will decrease. The performed study allows us to investigate the potential of the machine learning approach to predict the emergence of active regions using acoustic power maps as input.","sentences":["To create early warning capabilities for upcoming Space Weather disturbances, we have selected a dataset of 61 emerging active regions, which allows us to identify characteristic features in the evolution of acoustic power density to predict continuum intensity emergence.","For our study, we have utilized Doppler shift and continuum intensity observations from the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO).","The local tracking of 30.66 x 30.66-degree patches in the vicinity of active regions allowed us to trace the evolution of active regions starting from the pre-emergence state.","We have developed a machine learning model to capture the acoustic power flux density variations associated with upcoming magnetic flux emergence.","The trained Long Short-Term Memory (LSTM) model is able to predict 5 hours ahead whether, in a given area of the solar surface, continuum intensity values will decrease.","The performed study allows us to investigate the potential of the machine learning approach to predict the emergence of active regions using acoustic power maps as input."],"url":"http://arxiv.org/abs/2402.08890v1","category":"astro-ph.SR"}
{"created":"2024-02-14 00:35:10","title":"Position Paper: Challenges and Opportunities in Topological Deep Learning","abstract":"Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.","sentences":["Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models.","This paper posits that TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings.","To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations.","For each problem, it outlines potential solutions and future research opportunities.","At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field."],"url":"http://arxiv.org/abs/2402.08871v1","category":"cs.LG"}
{"created":"2024-02-13 23:26:11","title":"Space-Time Bridge-Diffusion","abstract":"In this study, we introduce a novel method for generating new synthetic samples that are independent and identically distributed (i.i.d.) from high-dimensional real-valued probability distributions, as defined implicitly by a set of Ground Truth (GT) samples. Central to our method is the integration of space-time mixing strategies that extend across temporal and spatial dimensions. Our methodology is underpinned by three interrelated stochastic processes designed to enable optimal transport from an easily tractable initial probability distribution to the target distribution represented by the GT samples: (a) linear processes incorporating space-time mixing that yield Gaussian conditional probability densities, (b) their bridge-diffusion analogs that are conditioned to the initial and final state vectors, and (c) nonlinear stochastic processes refined through score-matching techniques. The crux of our training regime involves fine-tuning the nonlinear model, and potentially the linear models - to align closely with the GT data. We validate the efficacy of our space-time diffusion approach with numerical experiments, laying the groundwork for more extensive future theory and experiments to fully authenticate the method, particularly providing a more efficient (possibly simulation-free) inference.","sentences":["In this study, we introduce a novel method for generating new synthetic samples that are independent and identically distributed (i.i.d.) from high-dimensional real-valued probability distributions, as defined implicitly by a set of Ground Truth (GT) samples.","Central to our method is the integration of space-time mixing strategies that extend across temporal and spatial dimensions.","Our methodology is underpinned by three interrelated stochastic processes designed to enable optimal transport from an easily tractable initial probability distribution to the target distribution represented by the GT samples: (a) linear processes incorporating space-time mixing that yield Gaussian conditional probability densities, (b) their bridge-diffusion analogs that are conditioned to the initial and final state vectors, and (c) nonlinear stochastic processes refined through score-matching techniques.","The crux of our training regime involves fine-tuning the nonlinear model, and potentially the linear models - to align closely with the GT data.","We validate the efficacy of our space-time diffusion approach with numerical experiments, laying the groundwork for more extensive future theory and experiments to fully authenticate the method, particularly providing a more efficient (possibly simulation-free) inference."],"url":"http://arxiv.org/abs/2402.08847v1","category":"stat.ML"}
{"created":"2024-02-13 22:07:29","title":"RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning","abstract":"We propose RanDumb to examine the efficacy of continual representation learning. RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top. We present a surprising and consistent finding: RanDumb significantly outperforms the continually learned representations using deep networks across numerous continual learning benchmarks, demonstrating the poor performance of representation learning in these scenarios. RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time. It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance. We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor. Our investigation is both surprising and alarming as it questions our understanding of how to effectively design and train models that require efficient continual representation learning, and necessitates a principled reinvestigation of the widely explored problem formulation itself. Our code is available at https://github.com/drimpossible/RanDumb.","sentences":["We propose RanDumb to examine the efficacy of continual representation learning.","RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top.","We present a surprising and consistent finding:","RanDumb significantly outperforms the continually learned representations using deep networks across numerous continual learning benchmarks, demonstrating the poor performance of representation learning in these scenarios.","RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time.","It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance.","We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor.","Our investigation is both surprising and alarming as it questions our understanding of how to effectively design and train models that require efficient continual representation learning, and necessitates a principled reinvestigation of the widely explored problem formulation itself.","Our code is available at https://github.com/drimpossible/RanDumb."],"url":"http://arxiv.org/abs/2402.08823v1","category":"cs.CV"}
{"created":"2024-02-13 21:19:00","title":"Multi-Label Zero-Shot Product Attribute-Value Extraction","abstract":"E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation. However, attribute value information is typically not available for new products. To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model. Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for graph nodes. Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes. This enables HyperPAVE to identify new attribute values without the need for labeled training data. We conduct extensive experiments with ablation studies on different categories of the MAVE dataset. The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based large language models for attribute value extraction in the zero-shot setting.","sentences":["E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation.","However, attribute value information is typically not available for new products.","To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model.","Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles.","In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting).","We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs.","In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for graph nodes.","Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes.","This enables HyperPAVE to identify new attribute values without the need for labeled training data.","We conduct extensive experiments with ablation studies on different categories of the MAVE dataset.","The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based large language models for attribute value extraction in the zero-shot setting."],"url":"http://arxiv.org/abs/2402.08802v1","category":"cs.IR"}
{"created":"2024-02-13 21:13:29","title":"Projection-Free Online Convex Optimization with Time-Varying Constraints","abstract":"We consider the setting of online convex optimization with adversarial time-varying constraints in which actions must be feasible w.r.t. a fixed constraint set, and are also required on average to approximately satisfy additional time-varying constraints. Motivated by scenarios in which the fixed feasible set (hard constraint) is difficult to project on, we consider projection-free algorithms that access this set only through a linear optimization oracle (LOO). We present an algorithm that, on a sequence of length $T$ and using overall $T$ calls to the LOO, guarantees $\\tilde{O}(T^{3/4})$ regret w.r.t. the losses and $O(T^{7/8})$ constraints violation (ignoring all quantities except for $T$) . In particular, these bounds hold w.r.t. any interval of the sequence. We also present a more efficient algorithm that requires only first-order oracle access to the soft constraints and achieves similar bounds w.r.t. the entire sequence. We extend the latter to the setting of bandit feedback and obtain similar bounds (as a function of $T$) in expectation.","sentences":["We consider the setting of online convex optimization with adversarial time-varying constraints in which actions must be feasible w.r.t.","a fixed constraint set, and are also required on average to approximately satisfy additional time-varying constraints.","Motivated by scenarios in which the fixed feasible set (hard constraint) is difficult to project on, we consider projection-free algorithms that access this set only through a linear optimization oracle (LOO).","We present an algorithm that, on a sequence of length $T$ and using overall $T$ calls to the LOO, guarantees $\\tilde{O}(T^{3/4})$ regret w.r.t.","the losses and $O(T^{7/8})$ constraints violation (ignoring all quantities except for $T$) .","In particular, these bounds hold w.r.t.","any interval of the sequence.","We also present a more efficient algorithm that requires only first-order oracle access to the soft constraints and achieves similar bounds w.r.t.","the entire sequence.","We extend the latter to the setting of bandit feedback and obtain similar bounds (as a function of $T$) in expectation."],"url":"http://arxiv.org/abs/2402.08799v1","category":"cs.LG"}
{"created":"2024-02-13 20:58:36","title":"Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model","abstract":"Recent advancements in generative models have established state-of-the-art benchmarks in generating molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called K-DReAM. We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while ensuring validity and synthesizability. K-DReAM outperforms state-of-the-art generative models on both unconditional and targeted generation tasks.","sentences":["Recent advancements in generative models have established state-of-the-art benchmarks in generating molecules and novel drug candidates.","Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized.","In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called K-DReAM.","We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity and incorporate this contextual information into a generative framework to guide a diffusion-based model.","The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while ensuring validity and synthesizability.","K-DReAM outperforms state-of-the-art generative models on both unconditional and targeted generation tasks."],"url":"http://arxiv.org/abs/2402.08790v1","category":"cs.LG"}
{"created":"2024-02-13 20:51:58","title":"Rethinking Machine Unlearning for Large Language Models","abstract":"We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.","sentences":["We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning.","This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information.","We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining.","We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications.","In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment.","We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning.","Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction."],"url":"http://arxiv.org/abs/2402.08787v1","category":"cs.LG"}
{"created":"2024-02-13 19:51:49","title":"Bayesian Strategic Classification","abstract":"In strategic classification, agents modify their features, at a cost, to ideally obtain a positive classification from the learner's classifier. The typical response of the learner is to carefully modify their classifier to be robust to such strategic behavior. When reasoning about agent manipulations, most papers that study strategic classification rely on the following strong assumption: agents fully know the exact parameters of the deployed classifier by the learner. This often is an unrealistic assumption when using complex or proprietary machine learning techniques in real-world prediction tasks.   We initiate the study of partial information release by the learner in strategic classification. We move away from the traditional assumption that agents have full knowledge of the classifier. Instead, we consider agents that have a common distributional prior on which classifier the learner is using. The learner in our model can reveal truthful, yet not necessarily complete, information about the deployed classifier to the agents. The learner's goal is to release just enough information about the classifier to maximize accuracy. We show how such partial information release can, counter-intuitively, benefit the learner's accuracy, despite increasing agents' abilities to manipulate.   We show that while it is intractable to compute the best response of an agent in the general case, there exist oracle-efficient algorithms that can solve the best response of the agents when the learner's hypothesis class is the class of linear classifiers, or when the agents' cost function satisfies a natural notion of submodularity as we define. We then turn our attention to the learner's optimization problem and provide both positive and negative results on the algorithmic problem of how much information the learner should release about the classifier to maximize their expected accuracy.","sentences":["In strategic classification, agents modify their features, at a cost, to ideally obtain a positive classification from the learner's classifier.","The typical response of the learner is to carefully modify their classifier to be robust to such strategic behavior.","When reasoning about agent manipulations, most papers that study strategic classification rely on the following strong assumption: agents fully know the exact parameters of the deployed classifier by the learner.","This often is an unrealistic assumption when using complex or proprietary machine learning techniques in real-world prediction tasks.   ","We initiate the study of partial information release by the learner in strategic classification.","We move away from the traditional assumption that agents have full knowledge of the classifier.","Instead, we consider agents that have a common distributional prior on which classifier the learner is using.","The learner in our model can reveal truthful, yet not necessarily complete, information about the deployed classifier to the agents.","The learner's goal is to release just enough information about the classifier to maximize accuracy.","We show how such partial information release can, counter-intuitively, benefit the learner's accuracy, despite increasing agents' abilities to manipulate.   ","We show that while it is intractable to compute the best response of an agent in the general case, there exist oracle-efficient algorithms that can solve the best response of the agents when the learner's hypothesis class is the class of linear classifiers, or when the agents' cost function satisfies a natural notion of submodularity as we define.","We then turn our attention to the learner's optimization problem and provide both positive and negative results on the algorithmic problem of how much information the learner should release about the classifier to maximize their expected accuracy."],"url":"http://arxiv.org/abs/2402.08758v1","category":"cs.LG"}
{"created":"2024-02-14 16:44:08","title":"Manipulating a beam of barium fluoride molecules using an electrostatic hexapole","abstract":"An electrostatic hexapole lens is used to manipulate the transverse properties of a beam of barium fluoride molecules from a cryogenic buffer gas source. The spatial distribution of the beam is measured by recording state-selective laser-induced fluorescence on an emccd camera, providing insight into the intensity and transverse position spread of the molecular beam. Although the high mass and unfavorable Stark shift of barium fluoride pose a considerable challenge, the number of molecules in the low-field seeking component of the N=1 state that pass a 4 mm diameter aperture 712 mm behind the source is increased by a factor of 12. Furthermore, it is demonstrated that the molecular beam can be displaced by up to +/-5 mm by moving the hexapole lens. Our measurements agree well with numerical trajectory simulations. We discuss how electrostatic lenses may be used to increase the sensitivity of beam experiments such as the search for the electric dipole moment of the electron.","sentences":["An electrostatic hexapole lens is used to manipulate the transverse properties of a beam of barium fluoride molecules from a cryogenic buffer gas source.","The spatial distribution of the beam is measured by recording state-selective laser-induced fluorescence on an emccd camera, providing insight into the intensity and transverse position spread of the molecular beam.","Although the high mass and unfavorable Stark shift of barium fluoride pose a considerable challenge, the number of molecules in the low-field seeking component of the N=1 state that pass a 4 mm diameter aperture 712 mm behind the source is increased by a factor of 12.","Furthermore, it is demonstrated that the molecular beam can be displaced by up to +/-5 mm by moving the hexapole lens.","Our measurements agree well with numerical trajectory simulations.","We discuss how electrostatic lenses may be used to increase the sensitivity of beam experiments such as the search for the electric dipole moment of the electron."],"url":"http://arxiv.org/abs/2402.09300v1","category":"physics.atom-ph"}
{"created":"2024-02-14 16:10:45","title":"Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies","abstract":"The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations. This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks. Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data. The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings. The study also discusses the challenges encountered, such as LLM output variability and the tendency towards hallucinations, proposing future work directions to enhance prompt design and annotation selection. Our findings indicate a promising synergy between LLM insights and traditional NLP techniques, paving the way for more accessible and robust NLP applications.","sentences":["The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations.","This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks.","Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data.","The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings.","The study also discusses the challenges encountered, such as LLM output variability and the tendency towards hallucinations, proposing future work directions to enhance prompt design and annotation selection.","Our findings indicate a promising synergy between LLM insights and traditional NLP techniques, paving the way for more accessible and robust NLP applications."],"url":"http://arxiv.org/abs/2402.09282v1","category":"cs.CL"}
{"created":"2024-02-14 13:51:56","title":"Approximating maximum independent set on Rydberg atom arrays using local detunings","abstract":"Rydberg atom arrays are among the most promising quantum simulating platforms due to their scalability and long coherence time. From the perspective of combinatorial optimization, they are intrinsic solver for the maximum independent set problem because of the resemblance between the Rydberg Hamiltonian and the cost function of the maximum independent set problem. In this paper, we suggest a strategy to approximate maximum independent sets by adjusting local detunings on the Rydberg Hamiltonian according to each vertex's vertex support, which is a quantity that represents connectivity between vertices. By doing so, we explicitly reflect on the Rydberg Hamiltonian the potential probability that each vertex will be included in maximum independent sets. Our strategy reduces an error rate three times for the checkerboard graphs with defects when the adiabaticity is enough. Our strategy also decreases the error rate for random graphs of density 3.0, even when the adiabaticity is relatively insufficient. Moreover, we harness our strategy to raise the fidelity between the evolved quantum state and a 2D cat state on a square lattice, showing that our strategy helps to prepare a quantum many-body ground state.","sentences":["Rydberg atom arrays are among the most promising quantum simulating platforms due to their scalability and long coherence time.","From the perspective of combinatorial optimization, they are intrinsic solver for the maximum independent set problem because of the resemblance between the Rydberg Hamiltonian and the cost function of the maximum independent set problem.","In this paper, we suggest a strategy to approximate maximum independent sets by adjusting local detunings on the Rydberg Hamiltonian according to each vertex's vertex support, which is a quantity that represents connectivity between vertices.","By doing so, we explicitly reflect on the Rydberg Hamiltonian the potential probability that each vertex will be included in maximum independent sets.","Our strategy reduces an error rate three times for the checkerboard graphs with defects when the adiabaticity is enough.","Our strategy also decreases the error rate for random graphs of density 3.0, even when the adiabaticity is relatively insufficient.","Moreover, we harness our strategy to raise the fidelity between the evolved quantum state and a 2D cat state on a square lattice, showing that our strategy helps to prepare a quantum many-body ground state."],"url":"http://arxiv.org/abs/2402.09180v1","category":"quant-ph"}
{"created":"2024-02-14 08:44:26","title":"The Order Oracle: a New Concept in The Black Box Optimization Problems","abstract":"Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function. In this paper, we provide the new concept of the \"Order Oracle\" as a novel approach to solving such problems. The Order Oracle offers a unique perspective, using only access to the order between function values (possibly with some bounded noise), but without assuming access to their values. As theoretical results, we provide estimates of the convergence rates of the algorithms (obtained by integrating the Order Oracle into existing optimization \"tools\") in the non-convex, convex, and strongly convex settings. Our theoretical results demonstrate the effectiveness of the Order Oracle through numerical experiments. Finally, we show the possibility of accelerating the convergence of such algorithms.","sentences":["Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function.","In this paper, we provide the new concept of the \"Order Oracle\" as a novel approach to solving such problems.","The Order Oracle offers a unique perspective, using only access to the order between function values (possibly with some bounded noise), but without assuming access to their values.","As theoretical results, we provide estimates of the convergence rates of the algorithms (obtained by integrating the Order Oracle into existing optimization \"tools\") in the non-convex, convex, and strongly convex settings.","Our theoretical results demonstrate the effectiveness of the Order Oracle through numerical experiments.","Finally, we show the possibility of accelerating the convergence of such algorithms."],"url":"http://arxiv.org/abs/2402.09014v1","category":"math.OC"}
{"created":"2024-02-14 08:02:50","title":"A Comprehensive Review of Software and Hardware Energy Efficiency of Video Decoders","abstract":"Energy and compression efficiency are two essential parts of modern video decoder implementations that have to be considered. This work comprehensively studies the following six video coding formats regarding compression and decoding energy efficiency: AVC, VP9, HEVC, AV1, VVC, and AVM. We first evaluate the energy demand of reference and optimized software decoder implementations. Furthermore, we consider the influence of the usage of SIMD instructions on those decoder implementations. We find that AV1 is a sweet spot for optimized software decoder implementations with an additional energy demand of 16.55% and bitrate savings of -43.95% compared to VP9. We furthermore evaluate the hardware decoding energy demand of four video coding formats. Thereby, we show that AV1 has energy demand increases by 117.50% compared to VP9. For HEVC, we found a sweet spot in terms of energy demand with an increase of 6.06% with respect to VP9. Relative to their optimized software counterparts, hardware video decoders reduce the energy consumption to less than 9% compared to software decoders.","sentences":["Energy and compression efficiency are two essential parts of modern video decoder implementations that have to be considered.","This work comprehensively studies the following six video coding formats regarding compression and decoding energy efficiency: AVC, VP9, HEVC, AV1, VVC, and AVM.","We first evaluate the energy demand of reference and optimized software decoder implementations.","Furthermore, we consider the influence of the usage of SIMD instructions on those decoder implementations.","We find that AV1 is a sweet spot for optimized software decoder implementations with an additional energy demand of 16.55% and bitrate savings of -43.95% compared to VP9.","We furthermore evaluate the hardware decoding energy demand of four video coding formats.","Thereby, we show that AV1 has energy demand increases by 117.50% compared to VP9.","For HEVC, we found a sweet spot in terms of energy demand with an increase of 6.06% with respect to VP9.","Relative to their optimized software counterparts, hardware video decoders reduce the energy consumption to less than 9% compared to software decoders."],"url":"http://arxiv.org/abs/2402.09001v1","category":"eess.IV"}
{"created":"2024-02-14 03:58:18","title":"Conformal Finite Element Methods for Nonlinear Rosenau-Burgers-Biharmonic Models","abstract":"We present a novel and comparative analysis of finite element discretizations for a nonlinear Rosenau-Burgers model including a biharmonic term. We analyze both continuous and mixed finite element approaches, providing stability, existence, and uniqueness statements of the corresponding variational methods. We also obtain optimal error estimates of the semidiscrete scheme in corresponding B\\^ochner spaces. Finally, we construct a fully discrete scheme through a backward Euler discretization of the time derivative, and prove well-posedness statements for this fully discrete scheme. Our findings show that the mixed approach removes some theoretical impediments to analysis and is numerically easier to implement. We provide numerical simulations for the mixed formulation approach using $C^0$ Taylor-Hood finite elements on several domains. Our numerical results confirm that the algorithm has optimal convergence in accordance with the observed theoretical results.","sentences":["We present a novel and comparative analysis of finite element discretizations for a nonlinear Rosenau-Burgers model including a biharmonic term.","We analyze both continuous and mixed finite element approaches, providing stability, existence, and uniqueness statements of the corresponding variational methods.","We also obtain optimal error estimates of the semidiscrete scheme in corresponding B\\^ochner spaces.","Finally, we construct a fully discrete scheme through a backward Euler discretization of the time derivative, and prove well-posedness statements for this fully discrete scheme.","Our findings show that the mixed approach removes some theoretical impediments to analysis and is numerically easier to implement.","We provide numerical simulations for the mixed formulation approach using $C^0$ Taylor-Hood finite elements on several domains.","Our numerical results confirm that the algorithm has optimal convergence in accordance with the observed theoretical results."],"url":"http://arxiv.org/abs/2402.08926v1","category":"math.NA"}
{"created":"2024-02-14 02:52:43","title":"A locally mass-conservative enriched Petrov-Galerkin method without penalty for the Darcy flow in porous media","abstract":"In this work we present an enriched Petrov-Galerkin (EPG) method for the simulation of the Darcy flow in porous media. The new method enriches the approximation trial space of the conforming continuous Galerkin (CG) method with bubble functions and enriches the approximation test space of the CG method with piecewise constant functions, and it does not require any penalty term in the weak formulation. Moreover, we propose a framework for constructing the bubble functions and consider a decoupled algorithm for the EPG method based on this framework, which enables the process of solving pressure to be decoupled into two steps. The first step is to solve the pressure by the standard CG method, and the second step is a post-processing correction of the first step. Compared with the CG method, the proposed EPG method is locally mass-conservative, while keeping fewer degrees of freedom than the discontinuous Galerkin (DG) method. In addition, this method is more concise in the error analysis than the enriched Galerkin (EG) method. The coupled flow and transport in porous media is considered to illustrate the advantages of locally mass-conservative properties of the EPG method. We establish the optimal convergence of numerical solutions and present several numerical examples to illustrate the performance of the proposed method.","sentences":["In this work we present an enriched Petrov-Galerkin (EPG) method for the simulation of the Darcy flow in porous media.","The new method enriches the approximation trial space of the conforming continuous Galerkin (CG) method with bubble functions and enriches the approximation test space of the CG method with piecewise constant functions, and it does not require any penalty term in the weak formulation.","Moreover, we propose a framework for constructing the bubble functions and consider a decoupled algorithm for the EPG method based on this framework, which enables the process of solving pressure to be decoupled into two steps.","The first step is to solve the pressure by the standard CG method, and the second step is a post-processing correction of the first step.","Compared with the CG method, the proposed EPG method is locally mass-conservative, while keeping fewer degrees of freedom than the discontinuous Galerkin (DG) method.","In addition, this method is more concise in the error analysis than the enriched Galerkin (EG) method.","The coupled flow and transport in porous media is considered to illustrate the advantages of locally mass-conservative properties of the EPG method.","We establish the optimal convergence of numerical solutions and present several numerical examples to illustrate the performance of the proposed method."],"url":"http://arxiv.org/abs/2402.08909v1","category":"math.NA"}
{"created":"2024-02-14 01:10:01","title":"A note on the critical set of harmonic functions near the boundary","abstract":"Let $u$ be a harmonic function in a $C^1$ domain $D\\subset \\mathbb{R}^d$, which vanishes on an open subset of the boundary. In this note we study its critical set $\\{x \\in \\overline{D}: \\nabla u(x) = 0 \\}$. When $D$ is a $C^{1,\\alpha}$ domain for some $\\alpha \\in (0,1]$, we give an upper bound on the $(d-2)$-dimensional Hausdorff measure of the critical set by the frequency function. We also discuss possible ways to extend such estimate to all $C^1$-Dini domains, the optimal class of domains for which analogous estimates have been shown to hold for the singular set $\\{x \\in \\overline{D}: u(x) = 0 = |\\nabla u(x)| \\}$ (see [KZ1, KZ2]).","sentences":["Let $u$ be a harmonic function in a $C^1$ domain $D\\subset \\mathbb{R}^d$, which vanishes on an open subset of the boundary.","In this note we study its critical set $\\{x \\in \\overline{D}: \\nabla u(x)","= 0 \\}$. When $D$ is a $C^{1,\\alpha}$ domain for some $\\alpha \\in (0,1]$, we give an upper bound on the $(d-2)$-dimensional Hausdorff measure of the critical set by the frequency function.","We also discuss possible ways to extend such estimate to all $C^1$-Dini domains, the optimal class of domains for which analogous estimates have been shown to hold for the singular set $\\{x \\in \\overline{D}: u(x)","= 0","= |\\nabla u(x)| \\}$ (see [KZ1, KZ2])."],"url":"http://arxiv.org/abs/2402.08881v1","category":"math.AP"}
{"created":"2024-02-14 00:23:53","title":"An approximation algorithm for zero forcing","abstract":"We give an algorithm that finds a zero forcing set which approximates the optimal size by a factor of $\\text{pw}(G)+1$, where $\\text{pw}(G)$ is the pathwidth of $G$. Starting from a path decomposition, the algorithm runs in $O(nm)$ time, where $n$ and $m$ are the order and size of the graph, respectively. As a corollary, we obtain a new upper bound on the zero forcing number in terms of the fort number and the pathwidth. The algorithm is based on a correspondence between zero forcing sets and forcing arc sets. This correspondence leads to a new bound on the zero forcing number in terms of vertex cuts, and to new, short proofs for known bounds on the zero forcing number.","sentences":["We give an algorithm that finds a zero forcing set which approximates the optimal size by a factor of $\\text{pw}(G)+1$, where $\\text{pw}(G)$ is the pathwidth of $G$. Starting from a path decomposition, the algorithm runs in $O(nm)$ time, where $n$ and $m$ are the order and size of the graph, respectively.","As a corollary, we obtain a new upper bound on the zero forcing number in terms of the fort number and the pathwidth.","The algorithm is based on a correspondence between zero forcing sets and forcing arc sets.","This correspondence leads to a new bound on the zero forcing number in terms of vertex cuts, and to new, short proofs for known bounds on the zero forcing number."],"url":"http://arxiv.org/abs/2402.08866v1","category":"math.CO"}
{"created":"2024-02-13 23:37:53","title":"Cardinal-Utility Matching Markets: The Quest for Envy-Freeness, Pareto-Optimality, and Efficient Computability","abstract":"Unlike ordinal-utility matching markets, which are well-developed from the viewpoint of both theory and practice, recent insights from a computer science perspective have left cardinal-utility matching markets in a quandary. The celebrated pricing-based mechanism for one-sided cardinal-utility matching markets due to Hylland and Zeckhauser, which had long eluded efficient algorithms, was finally shown to be PPAD-complete.   This led us to ask the question: is there an alternative, polynomial time, mechanism for one-sided cardinal-utility matching markets which achieves the desirable properties of HZ, i.e.\\ (ex-ante) envy-freeness (EF) and Pareto-optimality (PO)? In this paper we show:   1. The problem of finding an EF+PO lottery in a one-sided cardinal-utility matching market is PPAD-complete.   2. A $(2 + \\epsilon)$-approximately envy-free and (exactly) Pareto-optimal lottery can be found in polynomial time using Nash bargaining.   We also present several results on two-sided cardinal-utility matching markets, including non-existence of EF+PO lotteries as well as existence of justified-envy-free and weak Pareto-optimal lotteries.","sentences":["Unlike ordinal-utility matching markets, which are well-developed from the viewpoint of both theory and practice, recent insights from a computer science perspective have left cardinal-utility matching markets in a quandary.","The celebrated pricing-based mechanism for one-sided cardinal-utility matching markets due to Hylland and Zeckhauser, which had long eluded efficient algorithms, was finally shown to be PPAD-complete.   ","This led us to ask the question: is there an alternative, polynomial time, mechanism for one-sided cardinal-utility matching markets which achieves the desirable properties of HZ, i.e.\\ (ex-ante) envy-freeness (EF) and Pareto-optimality (PO)?","In this paper we show:   1.","The problem of finding an EF+PO lottery in a one-sided cardinal-utility matching market is PPAD-complete.   ","2.","A $(2 + \\epsilon)$-approximately envy-free and (exactly) Pareto-optimal lottery can be found in polynomial time using Nash bargaining.   ","We also present several results on two-sided cardinal-utility matching markets, including non-existence of EF+PO lotteries as well as existence of justified-envy-free and weak Pareto-optimal lotteries."],"url":"http://arxiv.org/abs/2402.08851v1","category":"cs.GT"}
{"created":"2024-02-13 22:13:18","title":"The Mixed Integer Trust Region Problem","abstract":"In this paper we consider the problem of minimizing a general quadratic function over the mixed integer points in an ellipsoid. This problem is strongly NP-hard, NP-hard to approximate within a constant factor, and optimal solutions can be irrational. In our main result we show that an arbitrarily good solution can be found in polynomial time, if we fix the number of integer variables. This algorithm provides a natural extension to the mixed integer setting, of the polynomial solvability of the trust region problem proven by Ye, Karmarkar, Vavasis, and Zippel. Our result removes a key bottleneck in the design and analysis of model trust region methods for mixed integer nonlinear optimization problems. The techniques introduced to prove this result are of independent interest and can be used in other mixed integer programming problems involving quadratic functions. As an example we consider the problem of minimizing a general quadratic function over the mixed integer points in a polyhedron. For this problem, we show that a solution satisfying weak bounds with respect to optimality can be computed in polynomial time, provided that the number of integer variables is fixed. It is well-known that finding a solution satisfying stronger bounds cannot be done in polynomial time, unless P=NP.","sentences":["In this paper we consider the problem of minimizing a general quadratic function over the mixed integer points in an ellipsoid.","This problem is strongly NP-hard, NP-hard to approximate within a constant factor, and optimal solutions can be irrational.","In our main result we show that an arbitrarily good solution can be found in polynomial time, if we fix the number of integer variables.","This algorithm provides a natural extension to the mixed integer setting, of the polynomial solvability of the trust region problem proven by Ye, Karmarkar, Vavasis, and Zippel.","Our result removes a key bottleneck in the design and analysis of model trust region methods for mixed integer nonlinear optimization problems.","The techniques introduced to prove this result are of independent interest and can be used in other mixed integer programming problems involving quadratic functions.","As an example we consider the problem of minimizing a general quadratic function over the mixed integer points in a polyhedron.","For this problem, we show that a solution satisfying weak bounds with respect to optimality can be computed in polynomial time, provided that the number of integer variables is fixed.","It is well-known that finding a solution satisfying stronger bounds cannot be done in polynomial time, unless P=NP."],"url":"http://arxiv.org/abs/2402.08827v1","category":"math.OC"}
{"created":"2024-02-13 21:27:20","title":"Byzantine fault-tolerant distributed set intersection with redundancy","abstract":"In this report, we study the problem of Byzantine fault-tolerant distributed set intersection and the importance of redundancy in solving this problem. Specifically, consider a distributed system with $n$ agents, each of which has a local set. There are up to $f$ agents that are Byzantine faulty. The goal is to find the intersection of the sets of the non-faulty agents.   We derive the Byzantine set intersection problem from the Byzantine optimization problem. We present the definition of $2f$-redundancy, and identify the necessary and sufficient condition if the Byzantine set intersection problem can be solved if a certain redundancy property is satisfied, and then present an equivalent condition. We further extend our results to arbitrary communication graphs in a decentralized setting. Finally, we present solvability results for the Byzantine optimization problem, inspired by our findings on Byzantine set intersection. The results we provide are for synchronous and asynchronous systems both.","sentences":["In this report, we study the problem of Byzantine fault-tolerant distributed set intersection and the importance of redundancy in solving this problem.","Specifically, consider a distributed system with $n$ agents, each of which has a local set.","There are up to $f$ agents that are Byzantine faulty.","The goal is to find the intersection of the sets of the non-faulty agents.   ","We derive the Byzantine set intersection problem from the Byzantine optimization problem.","We present the definition of $2f$-redundancy, and identify the necessary and sufficient condition if the Byzantine set intersection problem can be solved if a certain redundancy property is satisfied, and then present an equivalent condition.","We further extend our results to arbitrary communication graphs in a decentralized setting.","Finally, we present solvability results for the Byzantine optimization problem, inspired by our findings on Byzantine set intersection.","The results we provide are for synchronous and asynchronous systems both."],"url":"http://arxiv.org/abs/2402.08809v1","category":"cs.DC"}
{"created":"2024-02-13 20:23:19","title":"Strategic Contract Negotiation in Financial Networks","abstract":"How can firms optimally negotiate bilateral contracts with each other in a financial network? Every firm seeks to maximize the utility it gains from its portfolio of contracts. We focus on mean-variance utilities, where each firm has its own beliefs about the expected returns of the contracts and the covariances between them (Markowitz, J. Finance 7(11), 1952). Instead of revealing these beliefs, a firm may adopt a different negotiating position, seeking better contract terms. We formulate a contract negotiation process by which such strategic behavior leads to a network of contracts. In our formulation, any subset of firms can be strategic. The negotiating positions of these firms can form Nash equilibria, where each firm's position is optimal given the others' positions.   We give a polynomial-time algorithm to find the Nash equilibria, if they exist, and certify their nonexistence otherwise. We explore the implications of such equilibria on several model networks. These illustrate that firms' utilities can be sensitive to their negotiating position. We then propose trade deadlines as a mechanism to reduce the need for strategic behavior. At the deadline, each firm can unilaterally cancel some or all of its contracts, for a penalty. In our model networks, we show that trade deadlines can reduce the loss of utility from being honest. We empirically verify our insights using data on international trade between 46 large economies.","sentences":["How can firms optimally negotiate bilateral contracts with each other in a financial network?","Every firm seeks to maximize the utility it gains from its portfolio of contracts.","We focus on mean-variance utilities, where each firm has its own beliefs about the expected returns of the contracts and the covariances between them (Markowitz, J. Finance 7(11), 1952).","Instead of revealing these beliefs, a firm may adopt a different negotiating position, seeking better contract terms.","We formulate a contract negotiation process by which such strategic behavior leads to a network of contracts.","In our formulation, any subset of firms can be strategic.","The negotiating positions of these firms can form Nash equilibria, where each firm's position is optimal given the others' positions.   ","We give a polynomial-time algorithm to find the Nash equilibria, if they exist, and certify their nonexistence otherwise.","We explore the implications of such equilibria on several model networks.","These illustrate that firms' utilities can be sensitive to their negotiating position.","We then propose trade deadlines as a mechanism to reduce the need for strategic behavior.","At the deadline, each firm can unilaterally cancel some or all of its contracts, for a penalty.","In our model networks, we show that trade deadlines can reduce the loss of utility from being honest.","We empirically verify our insights using data on international trade between 46 large economies."],"url":"http://arxiv.org/abs/2402.08779v1","category":"math.OC"}
{"created":"2024-02-13 20:08:14","title":"Concentration of the number of real roots of random polynomials","abstract":"Many statistics of roots of random polynomials have been studied in the literature, but not much is known on the concentration aspect. In this note we present a systematic study of this question, aiming towards nearly optimal bounds to some extent. Our method is elementary and works well for many models of random polynomials, with gaussian or non-gaussian coefficients.","sentences":["Many statistics of roots of random polynomials have been studied in the literature, but not much is known on the concentration aspect.","In this note we present a systematic study of this question, aiming towards nearly optimal bounds to some extent.","Our method is elementary and works well for many models of random polynomials, with gaussian or non-gaussian coefficients."],"url":"http://arxiv.org/abs/2402.08773v1","category":"math.PR"}
{"created":"2024-02-13 20:06:16","title":"Introducing RSESS: An Open Source Enumerative Sphere Shaping Implementation Coded in Rust","abstract":"In this work, we present an open-source implementation of the enumerative sphere shaping (ESS) algorithm used for probabilistic constellation shaping (PCS). PCS aims at closing the shaping gap caused by using uniformly distributed modulation symbols in channels for which information theory shows non-uniformly distributed signaling to be optimal. ESS is one such PCS algorithm that sets itself apart as it operates on a trellis representation of a subset of the possible symbol sequences. ESS leads to an empirical distribution of the symbols that closely approximates the optimal distribution for the additive white Gaussian noise (AWGN) channel. We provide an open-source implementation of this algorithm in the compiled language Rust, as well as Python bindings with which our Rust code can be called in a regular Python script. We also compare simulation results on the AWGN channel using our implementation with previous works on this topic.","sentences":["In this work, we present an open-source implementation of the enumerative sphere shaping (ESS) algorithm used for probabilistic constellation shaping (PCS).","PCS aims at closing the shaping gap caused by using uniformly distributed modulation symbols in channels for which information theory shows non-uniformly distributed signaling to be optimal.","ESS is one such PCS algorithm that sets itself apart as it operates on a trellis representation of a subset of the possible symbol sequences.","ESS leads to an empirical distribution of the symbols that closely approximates the optimal distribution for the additive white Gaussian noise (AWGN) channel.","We provide an open-source implementation of this algorithm in the compiled language Rust, as well as Python bindings with which our Rust code can be called in a regular Python script.","We also compare simulation results on the AWGN channel using our implementation with previous works on this topic."],"url":"http://arxiv.org/abs/2402.08771v1","category":"cs.IT"}
{"created":"2024-02-13 19:38:58","title":"Edge coloring lattice graphs","abstract":"We develop the theory of the edge coloring of infinite lattice graphs, proving a necessary and sufficient condition for a proper edge coloring of a patch of a lattice graph to induce a proper edge coloring of the entire lattice graph by translation. This condition forms the cornerstone of a method that finds nearly minimal or minimal edge colorings of infinite lattice graphs. In case a nearly minimal edge coloring is requested, the running time is $O(\\mu^2 D^4)$, where $\\mu$ is the number of edges in one cell (or `basis graph') of the lattice graph and $D$ is the maximum distance between two cells so that there is an edge from within one cell to the other. In case a minimal edge coloring is requested, we lack an upper bound on the running time, which we find need not pose a limitation in practice; we use the method to minimal edge color the meshes of all $k$-uniform tilings of the plane for $k\\leq 6$, while utilizing modest computational resources. We find that all these lattice graphs are Vizing class~I. Relating edge colorings to quantum circuits, our work finds direct application by offering minimal-depth quantum circuits in the areas of quantum simulation, quantum optimization, and quantum state verification.","sentences":["We develop the theory of the edge coloring of infinite lattice graphs, proving a necessary and sufficient condition for a proper edge coloring of a patch of a lattice graph to induce a proper edge coloring of the entire lattice graph by translation.","This condition forms the cornerstone of a method that finds nearly minimal or minimal edge colorings of infinite lattice graphs.","In case a nearly minimal edge coloring is requested, the running time is $O(\\mu^2 D^4)$, where $\\mu$ is the number of edges in one cell (or `basis graph') of the lattice graph and $D$ is the maximum distance between two cells so that there is an edge from within one cell to the other.","In case a minimal edge coloring is requested, we lack an upper bound on the running time, which we find need not pose a limitation in practice; we use the method to minimal edge color the meshes of all $k$-uniform tilings of the plane for $k\\leq 6$, while utilizing modest computational resources.","We find that all these lattice graphs are Vizing class~I. Relating edge colorings to quantum circuits, our work finds direct application by offering minimal-depth quantum circuits in the areas of quantum simulation, quantum optimization, and quantum state verification."],"url":"http://arxiv.org/abs/2402.08752v1","category":"quant-ph"}
