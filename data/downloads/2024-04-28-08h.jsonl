{"created":"2024-04-25 17:59:59","title":"The Black-Hole Masses of High-Redshift QSOs","abstract":"Observations of high-redshift quasars frequently promote suggestions of large black hole masses, whose presence so early in cosmic time is not easily explicable. I consider the parallel with ultraluminous X-ray sources (ULXs) -- now known to be stellar-mass black hole (and neutron star) binaries apparently radiating far above their Eddington luminosities $L_{\\rm Edd}$. The true luminosity in ULXs is actually only of order $L_{\\rm Edd}$, for {\\it stellar-mass} accretors, but has a very anisotropic (`beamed') component, plus a near-isotropic component of similar luminosity but much lower specific intensity. Observers viewing ULXs from within the beam but assuming spherical symmetry deduce a luminosity $\\gg L_{\\rm Edd}$. These features appear because the accretors are fed mass at highly super-Eddington rates, most of it expelled in high-speed ($v >0.2c$) outflows from the accretion disc.   I show that in similarly-beamed AGN, emission-line properties would be essentially the same as in unbeamed sources, but standard virial mass indicators unusable because velocity widths are dominated by the outflows, not bound motions about the black holes. In an ensemble of this kind the apparently most luminous systems are always the most distant, but have the lowest black hole masses. Interpreting observations of this ensemble without knowing that they are beamed leads instead to very high black hole mass estimates. The analogy with ULXs therefore suggests that high-redshift quasars might actually have central black hole masses which could have grown from stellar values within the lookback time. I consider how one might test these ideas observationally.","sentences":["Observations of high-redshift quasars frequently promote suggestions of large black hole masses, whose presence so early in cosmic time is not easily explicable.","I consider the parallel with ultraluminous X-ray sources (ULXs) -- now known to be stellar-mass black hole (and neutron star) binaries apparently radiating far above their Eddington luminosities $L_{\\rm Edd}$. The true luminosity in ULXs is actually only of order $L_{\\rm Edd}$, for {\\it stellar-mass} accretors, but has a very anisotropic (`beamed') component, plus a near-isotropic component of similar luminosity but much lower specific intensity.","Observers viewing ULXs from within the beam but assuming spherical symmetry deduce a luminosity $\\gg L_{\\rm Edd}$. These features appear because the accretors are fed mass at highly super-Eddington rates, most of it expelled in high-speed ($v >0.2c$) outflows from the accretion disc.   ","I show that in similarly-beamed AGN, emission-line properties would be essentially the same as in unbeamed sources, but standard virial mass indicators unusable because velocity widths are dominated by the outflows, not bound motions about the black holes.","In an ensemble of this kind the apparently most luminous systems are always the most distant, but have the lowest black hole masses.","Interpreting observations of this ensemble without knowing that they are beamed leads instead to very high black hole mass estimates.","The analogy with ULXs therefore suggests that high-redshift quasars might actually have central black hole masses which could have grown from stellar values within the lookback time.","I consider how one might test these ideas observationally."],"url":"http://arxiv.org/abs/2404.16832v1","category":"astro-ph.GA"}
{"created":"2024-04-25 17:59:46","title":"ResVR: Joint Rescaling and Viewport Rendering of Omnidirectional Images","abstract":"With the advent of virtual reality technology, omnidirectional image (ODI) rescaling techniques are increasingly embraced for reducing transmitted and stored file sizes while preserving high image quality. Despite this progress, current ODI rescaling methods predominantly focus on enhancing the quality of images in equirectangular projection (ERP) format, which overlooks the fact that the content viewed on head mounted displays (HMDs) is actually a rendered viewport instead of an ERP image. In this work, we emphasize that focusing solely on ERP quality results in inferior viewport visual experiences for users. Thus, we propose ResVR, which is the first comprehensive framework for the joint Rescaling and Viewport Rendering of ODIs. ResVR allows obtaining LR ERP images for transmission while rendering high-quality viewports for users to watch on HMDs. In our ResVR, a novel discrete pixel sampling strategy is developed to tackle the complex mapping between the viewport and ERP, enabling end-to-end training of ResVR pipeline. Furthermore, a spherical pixel shape representation technique is innovatively derived from spherical differentiation to significantly improve the visual quality of rendered viewports. Extensive experiments demonstrate that our ResVR outperforms existing methods in viewport rendering tasks across different fields of view, resolutions, and view directions while keeping a low transmission overhead.","sentences":["With the advent of virtual reality technology, omnidirectional image (ODI) rescaling techniques are increasingly embraced for reducing transmitted and stored file sizes while preserving high image quality.","Despite this progress, current ODI rescaling methods predominantly focus on enhancing the quality of images in equirectangular projection (ERP) format, which overlooks the fact that the content viewed on head mounted displays (HMDs) is actually a rendered viewport instead of an ERP image.","In this work, we emphasize that focusing solely on ERP quality results in inferior viewport visual experiences for users.","Thus, we propose ResVR, which is the first comprehensive framework for the joint Rescaling and Viewport Rendering of ODIs.","ResVR allows obtaining LR ERP images for transmission while rendering high-quality viewports for users to watch on HMDs.","In our ResVR, a novel discrete pixel sampling strategy is developed to tackle the complex mapping between the viewport and ERP, enabling end-to-end training of ResVR pipeline.","Furthermore, a spherical pixel shape representation technique is innovatively derived from spherical differentiation to significantly improve the visual quality of rendered viewports.","Extensive experiments demonstrate that our ResVR outperforms existing methods in viewport rendering tasks across different fields of view, resolutions, and view directions while keeping a low transmission overhead."],"url":"http://arxiv.org/abs/2404.16825v1","category":"cs.CV"}
{"created":"2024-04-25 17:58:16","title":"Ordered and disordered stealthy hyperuniform point patterns across spatial dimensions","abstract":"In previous work [Phys. Rev. X 5, 021020 (2015)], it was shown that stealthy hyperuniform systems can be regarded as hard spheres in Fourier-space in the sense that the the structure factor is exactly zero in a spherical region around the origin in analogy with the pair-correlation function of real-space hard spheres. In this work, we exploit this correspondence to confirm that the densest Fourier-space hard-sphere system is that of a Bravais lattice. This is in contrast to real-space hard-spheres, whose densest configuration is conjectured to be disordered. We also extend the virial series previously suggested for disordered stealthy hyperuniform systems to higher dimensions in order to predict spatial decorrelation as function of dimension. This prediction is then borne out by numerical simulations of disordered stealthy hyperuniform ground states in dimensions $d=2$-$8$.","sentences":["In previous work [Phys. Rev. X 5, 021020 (2015)], it was shown that stealthy hyperuniform systems can be regarded as hard spheres in Fourier-space in the sense that the the structure factor is exactly zero in a spherical region around the origin in analogy with the pair-correlation function of real-space hard spheres.","In this work, we exploit this correspondence to confirm that the densest Fourier-space hard-sphere system is that of a Bravais lattice.","This is in contrast to real-space hard-spheres, whose densest configuration is conjectured to be disordered.","We also extend the virial series previously suggested for disordered stealthy hyperuniform systems to higher dimensions in order to predict spatial decorrelation as function of dimension.","This prediction is then borne out by numerical simulations of disordered stealthy hyperuniform ground states in dimensions $d=2$-$8$."],"url":"http://arxiv.org/abs/2404.16819v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 17:54:54","title":"The Spectrum of $\\mathbb{Q}$-Isotropic Binary Quadratic Forms","abstract":"We give a complete list of the points in the spectrum $$\\mathcal{Z}=\\{\\inf_{(x,y)\\in\\Lambda,xy\\neq0}{\\left\\vert xy\\right\\vert},\\,\\text{$\\Lambda$ is a unimodular rational lattice of $\\mathbb{R}^2$}\\}$$ above $\\frac{1}{3}.$ We further show that the set of limit points of $\\mathcal{Z}$ with values larger than $\\frac{1}{3},$ is equal to the set $\\{\\frac{2m}{\\sqrt{9m^2-4}+3m},\\text{ where $m$ is a Markoff number}\\}$.","sentences":["We give a complete list of the points in the spectrum $$\\mathcal{Z}=\\{\\inf_{(x,y)\\in\\Lambda,xy\\neq0}{\\left\\vert xy\\right\\vert},\\,\\text{$\\Lambda$ is a unimodular rational lattice of $\\mathbb{R}^2$}\\}$$ above $\\frac{1}{3}.$ We further show that the set of limit points of $\\mathcal{Z}$ with values larger than $\\frac{1}{3},$ is equal to the set $\\{\\frac{2m}{\\sqrt{9m^2-4}+3m},\\text{ where $m$ is a Markoff number}\\}$."],"url":"http://arxiv.org/abs/2404.16810v1","category":"math.NT"}
{"created":"2024-04-25 17:53:26","title":"Enhancing nanocrystal superlattice self-assembly near a metastable liquid binodal","abstract":"Bottom-up assembly of nanocrystals (NCs) into ordered arrays, or superlattices (SLs), is a promising route to design materials with new functionalities, but the degree of control over assembly into functional structures remains challenging. Using electrostatics, rather than density, to tune the interactions between semiconductor NCs, we watch self-assembly proceeding through a metastable liquid phase. We systematically investigate the phase behavior as a function of quench conditions in situ and in real time using small angle X-ray scattering (SAXS). Through quantitative fitting to colloid, liquid, and SL models, we extract the time evolution of each phase and the system phase diagram, which we find to be consistent with short-range attractive interactions. Using the phase diagram's predictive power, we establish control of the self-assembly rate over three orders of magnitude, and identify one- and two-step self-assembly regimes, with only the latter implicating the metastable liquid as an intermediate. Importantly, the presence of the metastable liquid increases SL formation rates relative to the equivalent one-step pathway, and SL order counterintuitively increases with the rate, revealing a highly desirable and generalizable kinetic strategy to promote and enhance ordered assembly.","sentences":["Bottom-up assembly of nanocrystals (NCs) into ordered arrays, or superlattices (SLs), is a promising route to design materials with new functionalities, but the degree of control over assembly into functional structures remains challenging.","Using electrostatics, rather than density, to tune the interactions between semiconductor NCs, we watch self-assembly proceeding through a metastable liquid phase.","We systematically investigate the phase behavior as a function of quench conditions in situ and in real time using small angle X-ray scattering (SAXS).","Through quantitative fitting to colloid, liquid, and SL models, we extract the time evolution of each phase and the system phase diagram, which we find to be consistent with short-range attractive interactions.","Using the phase diagram's predictive power, we establish control of the self-assembly rate over three orders of magnitude, and identify one-","and two-step self-assembly regimes, with only the latter implicating the metastable liquid as an intermediate.","Importantly, the presence of the metastable liquid increases SL formation rates relative to the equivalent one-step pathway, and SL order counterintuitively increases with the rate, revealing a highly desirable and generalizable kinetic strategy to promote and enhance ordered assembly."],"url":"http://arxiv.org/abs/2404.16808v1","category":"cond-mat.soft"}
{"created":"2024-04-25 17:52:10","title":"Simple tunable phase-locked lasers for quantum technologies","abstract":"In a wide range of quantum technology applications, ranging from atomic clocks to the creation of ultracold or quantum degenerate samples for atom interferometry, optimal laser sources are critical. In particular, two phase-locked laser sources with a precise difference frequency are needed for efficient coherent population trapping (CPT) clocks, gray molasses laser cooling, or driving Raman transitions. Here we show how a simple cost-effective laser diode can selectively amplify only one sideband of a fiber-electrooptically-modulated seed laser to produce moderate-power phase-locked light with sub-Hz relative linewidth and tunable difference frequencies up to $\\approx 15\\,$GHz. The architecture is readily scalable to multiple phase-locked lasers and could conceivably be used for future on-chip compact phase-locked laser systems for quantum technologies.","sentences":["In a wide range of quantum technology applications, ranging from atomic clocks to the creation of ultracold or quantum degenerate samples for atom interferometry, optimal laser sources are critical.","In particular, two phase-locked laser sources with a precise difference frequency are needed for efficient coherent population trapping (CPT) clocks, gray molasses laser cooling, or driving Raman transitions.","Here we show how a simple cost-effective laser diode can selectively amplify only one sideband of a fiber-electrooptically-modulated seed laser to produce moderate-power phase-locked light with sub-Hz relative linewidth and tunable difference frequencies up to $\\approx 15\\,$GHz.","The architecture is readily scalable to multiple phase-locked lasers and could conceivably be used for future on-chip compact phase-locked laser systems for quantum technologies."],"url":"http://arxiv.org/abs/2404.16806v1","category":"physics.atom-ph"}
{"created":"2024-04-25 17:46:55","title":"The Directed Landscape is a Black Noise","abstract":"We show that the directed landscape is a black noise in the sense of Tsirelson and Vershik. As a corollary, we show that for any microscopic system in which the height profile converges in law to the directed landscape, the driving noise is asymptotically independent of the height profile. This decoupling result provides one answer to the question of what happens to the driving noise in the limit under the KPZ scaling, and illustrates a type of noise sensitivity for systems in the KPZ universality class. Such decoupling and sensitivity phenomena are not present in the intermediate-disorder or weak-asymmetry regime, thus illustrating a contrast from the weak KPZ scaling regime. Along the way, we prove a strong mixing property for the directed landscape on a bounded time interval under spatial shifts, with a mixing rate $\\alpha(N)\\leq Ce^{-dN^3}$ for some $C,d>0$.","sentences":["We show that the directed landscape is a black noise in the sense of Tsirelson and Vershik.","As a corollary, we show that for any microscopic system in which the height profile converges in law to the directed landscape, the driving noise is asymptotically independent of the height profile.","This decoupling result provides one answer to the question of what happens to the driving noise in the limit under the KPZ scaling, and illustrates a type of noise sensitivity for systems in the KPZ universality class.","Such decoupling and sensitivity phenomena are not present in the intermediate-disorder or weak-asymmetry regime, thus illustrating a contrast from the weak KPZ scaling regime.","Along the way, we prove a strong mixing property for the directed landscape on a bounded time interval under spatial shifts, with a mixing rate $\\alpha(N)\\leq Ce^{-dN^3}$ for some $C,d>0$."],"url":"http://arxiv.org/abs/2404.16801v1","category":"math.PR"}
{"created":"2024-04-25 17:39:52","title":"A Communication- and Memory-Aware Model for Load Balancing Tasks","abstract":"While load balancing in distributed-memory computing has been well-studied, we present an innovative approach to this problem: a unified, reduced-order model that combines three key components to describe \"work\" in a distributed system: computation, communication, and memory. Our model enables an optimizer to explore complex tradeoffs in task placement, such as increased parallelism at the expense of data replication, which increases memory usage. We propose a fully distributed, heuristic-based load balancing optimization algorithm, and demonstrate that it quickly finds close-to-optimal solutions. We formalize the complex optimization problem as a mixed-integer linear program, and compare it to our strategy. Finally, we show that when applied to an electromagnetics code, our approach obtains up to 2.3x speedups for the imbalanced execution.","sentences":["While load balancing in distributed-memory computing has been well-studied, we present an innovative approach to this problem: a unified, reduced-order model that combines three key components to describe \"work\" in a distributed system: computation, communication, and memory.","Our model enables an optimizer to explore complex tradeoffs in task placement, such as increased parallelism at the expense of data replication, which increases memory usage.","We propose a fully distributed, heuristic-based load balancing optimization algorithm, and demonstrate that it quickly finds close-to-optimal solutions.","We formalize the complex optimization problem as a mixed-integer linear program, and compare it to our strategy.","Finally, we show that when applied to an electromagnetics code, our approach obtains up to 2.3x speedups for the imbalanced execution."],"url":"http://arxiv.org/abs/2404.16793v1","category":"cs.DC"}
{"created":"2024-04-25 17:34:40","title":"Enhancing Quality of Experience in Telecommunication Networks: A Review of Frameworks and Machine Learning Algorithms","abstract":"The Internet service provider industry is currently experiencing intense competition as companies strive to provide top-notch services to their customers. Providers are introducing cutting-edge technologies to enhance service quality, understanding that their survival depends on the level of service they offer. However, evaluating service quality is a complex task. A crucial aspect of this evaluation lies in understanding user experience, which significantly impacts the success and reputation of a service or product. Ensuring a seamless and positive user experience is essential for attracting and retaining customers. To date, much effort has been devoted to developing tools for measuring Quality of Experience (QoE), which incorporate both subjective and objective criteria. These tools, available in closed and open-source formats, are accessible to organizations and contribute to improving user experience quality. This review article delves into recent research and initiatives aimed at creating frameworks for assessing user QoE. It also explores the integration of machine learning algorithms to enhance these tools for future advancements. Additionally, the article examines current challenges and envisions future directions in the development of these measurement tools.","sentences":["The Internet service provider industry is currently experiencing intense competition as companies strive to provide top-notch services to their customers.","Providers are introducing cutting-edge technologies to enhance service quality, understanding that their survival depends on the level of service they offer.","However, evaluating service quality is a complex task.","A crucial aspect of this evaluation lies in understanding user experience, which significantly impacts the success and reputation of a service or product.","Ensuring a seamless and positive user experience is essential for attracting and retaining customers.","To date, much effort has been devoted to developing tools for measuring Quality of Experience (QoE), which incorporate both subjective and objective criteria.","These tools, available in closed and open-source formats, are accessible to organizations and contribute to improving user experience quality.","This review article delves into recent research and initiatives aimed at creating frameworks for assessing user QoE. It also explores the integration of machine learning algorithms to enhance these tools for future advancements.","Additionally, the article examines current challenges and envisions future directions in the development of these measurement tools."],"url":"http://arxiv.org/abs/2404.16787v1","category":"cs.NI"}
{"created":"2024-04-25 17:19:18","title":"Threshold and frequency properties of a cold ytterbium laser","abstract":"We investigate properties of the lasing action observed on the 1S0--3P1 intercombination transition of ytterbium atoms that are laser-cooled and -trapped inside a high-finesse cavity. The dressing of the atomic states on the 1S0--1P1 transition by the magneto-optical trap (MOT) laser light allows the coupled atom-cavity system to lase, via a two-photon transition, on the same line on which it is pumped. The observation and basic description of this phenomenon was presented earlier by Gothe et al. [Phys. Rev. A 99, 013415 (2019)]. In the current work, we focus on a detailed analysis of the lasing threshold and frequency properties and perform a comparison to our theoretical models.","sentences":["We investigate properties of the lasing action observed on the 1S0--3P1 intercombination transition of ytterbium atoms that are laser-cooled and -trapped inside a high-finesse cavity.","The dressing of the atomic states on the 1S0--1P1 transition by the magneto-optical trap (MOT) laser light allows the coupled atom-cavity system to lase, via a two-photon transition, on the same line on which it is pumped.","The observation and basic description of this phenomenon was presented earlier by Gothe et al.","[Phys. Rev.","A 99, 013415 (2019)].","In the current work, we focus on a detailed analysis of the lasing threshold and frequency properties and perform a comparison to our theoretical models."],"url":"http://arxiv.org/abs/2404.16765v1","category":"quant-ph"}
{"created":"2024-04-25 17:19:13","title":"Dataset of Quotation Attribution in German News Articles","abstract":"Extracting who says what to whom is a crucial part in analyzing human communication in today's abundance of data such as online news articles. Yet, the lack of annotated data for this task in German news articles severely limits the quality and usability of possible systems. To remedy this, we present a new, freely available, creative-commons-licensed dataset for quotation attribution in German news articles based on WIKINEWS. The dataset provides curated, high-quality annotations across 1000 documents (250,000 tokens) in a fine-grained annotation schema enabling various downstream uses for the dataset. The annotations not only specify who said what but also how, in which context, to whom and define the type of quotation. We specify our annotation schema, describe the creation of the dataset and provide a quantitative analysis. Further, we describe suitable evaluation metrics, apply two existing systems for quotation attribution, discuss their results to evaluate the utility of our dataset and outline use cases of our dataset in downstream tasks.","sentences":["Extracting who says what to whom is a crucial part in analyzing human communication in today's abundance of data such as online news articles.","Yet, the lack of annotated data for this task in German news articles severely limits the quality and usability of possible systems.","To remedy this, we present a new, freely available, creative-commons-licensed dataset for quotation attribution in German news articles based on WIKINEWS.","The dataset provides curated, high-quality annotations across 1000 documents (250,000 tokens) in a fine-grained annotation schema enabling various downstream uses for the dataset.","The annotations not only specify who said what but also how, in which context, to whom and define the type of quotation.","We specify our annotation schema, describe the creation of the dataset and provide a quantitative analysis.","Further, we describe suitable evaluation metrics, apply two existing systems for quotation attribution, discuss their results to evaluate the utility of our dataset and outline use cases of our dataset in downstream tasks."],"url":"http://arxiv.org/abs/2404.16764v1","category":"cs.CL"}
{"created":"2024-04-25 17:17:32","title":"Analysis of Ethanol Blending Effects on Auto-Ignition and Heat Release in n-Heptane/Ethanol Non-Premixed Flames","abstract":"This study delves into the auto-ignition temperature of n-heptane and ethanol mixtures within a counterflow flame configuration under low strain rate, with a particular focus on the impact of ethanol blending on heat release rates. Employing the sensitivity analysis method inspired by Zurada's sensitivity approach for neural network, this study identifies the group of critical species influencing the heat release rate. Further analysis concentration change reveals the intricate interactions among these various radicals across different temperature zones. It is found that, in n-heptane dominant mixtures, inhibition of low-temperature chemistry (LTC) caused by additional ethanol, impacts heat release rate at high temperature zone through diffusion effect of specific radicals such as CH2O, C2H4, C3H6 and H2O2. For ethanol-dominant mixtures, an increase in heat release rate was observed with higher ethanol fraction. Further concentration change analysis elucidated it is primarily attributed to the decomposition of ethanol and its subsequent reactions. This research underscores the significance of incorporating both chemical kinetics and species diffusion effects when analyzing the counterflow configuration of complex fuel mixtures.","sentences":["This study delves into the auto-ignition temperature of n-heptane and ethanol mixtures within a counterflow flame configuration under low strain rate, with a particular focus on the impact of ethanol blending on heat release rates.","Employing the sensitivity analysis method inspired by Zurada's sensitivity approach for neural network, this study identifies the group of critical species influencing the heat release rate.","Further analysis concentration change reveals the intricate interactions among these various radicals across different temperature zones.","It is found that, in n-heptane dominant mixtures, inhibition of low-temperature chemistry (LTC) caused by additional ethanol, impacts heat release rate at high temperature zone through diffusion effect of specific radicals such as CH2O, C2H4, C3H6 and H2O2.","For ethanol-dominant mixtures, an increase in heat release rate was observed with higher ethanol fraction.","Further concentration change analysis elucidated it is primarily attributed to the decomposition of ethanol and its subsequent reactions.","This research underscores the significance of incorporating both chemical kinetics and species diffusion effects when analyzing the counterflow configuration of complex fuel mixtures."],"url":"http://arxiv.org/abs/2404.16762v1","category":"physics.chem-ph"}
{"created":"2024-04-25 17:15:17","title":"Compact almost automorphic dynamics of non-autonomous differential equations with exponential dichotomy and applications to biological models with delay","abstract":"In the present work, we prove that, if $A(\\cdot)$ is a compact almost automorphic matrix and the system $$x'(t) = A(t)x(t)\\, ,$$ possesses an exponential dichotomy with Green function $G(\\cdot, \\cdot)$, then its associated system $$y'(t) = B(t)y(t)\\, ,$$ where $B(\\cdot) \\in H(A)$ (the hull of $A(\\cdot)$) also possesses an exponential dichotomy. Moreover, the Green function $G(\\cdot, \\cdot)$ is compact Bi-almost automorphic in $\\mathbb{R}^2$, this implies that $G(\\cdot, \\cdot)$ is $\\Delta_2$ - like uniformly continuous, where $\\Delta_2$ is the principal diagonal of $\\mathbb{R}^2$, an important ingredient in the proof of invariance of the compact almost automorphic function space under convolution product with kernel $G(\\cdot, \\cdot)$. Finally, we study the existence of a positive compact almost automorphic solution of non-autonomous differential equations of biological interest having non-linear harvesting terms and mixed delays.","sentences":["In the present work, we prove that, if $A(\\cdot)$ is a compact almost automorphic matrix and the system $$x'(t)","= A(t)x(t)\\, ,$$ possesses an exponential dichotomy with Green function $G(\\cdot, \\cdot)$, then its associated system $$y'(t) = B(t)y(t)\\, ,$$ where $B(\\cdot) \\in H(A)$ (the hull of $A(\\cdot)$) also possesses an exponential dichotomy.","Moreover, the Green function $G(\\cdot, \\cdot)$ is compact Bi-almost automorphic in $\\mathbb{R}^2$, this implies that $G(\\cdot, \\cdot)$ is $\\Delta_2$ - like uniformly continuous, where $\\Delta_2$ is the principal diagonal of $\\mathbb{R}^2$, an important ingredient in the proof of invariance of the compact almost automorphic function space under convolution product with kernel $G(\\cdot,","\\cdot)$. Finally, we study the existence of a positive compact almost automorphic solution of non-autonomous differential equations of biological interest having non-linear harvesting terms and mixed delays."],"url":"http://arxiv.org/abs/2404.16758v1","category":"math.DS"}
{"created":"2024-04-25 17:12:32","title":"Second-order adiabatic expansions of heat and charge currents with nonequilibrium Green's functions","abstract":"Due to technological needs, nanoscale heat management, energy conversion and quantum thermodynamics have become key areas of research, putting heat pumps and nanomotors center stage. The treatment of these particular systems often requires the use of adiabatic expansions in terms of the frequency of the external driving or the velocity of some classical degree of freedom. However, due to the difficulty of getting the expressions, most works have only explored first-order terms. Despite this, adiabatic expansions have allowed the study of intriguing phenomena such as adiabatic quantum pumps and motors, or electronic friction. Here, we use nonequilibrium Green's functions, within a Schwinger-Keldysh approach, to develop second-order expressions for the energy, heat, and charge currents. We illustrate, through two simple models, how the obtained formulas produce physically consistent results, and allow for the thermodynamic study of unexplored phenomena, such as second-order monoparametric pumping.","sentences":["Due to technological needs, nanoscale heat management, energy conversion and quantum thermodynamics have become key areas of research, putting heat pumps and nanomotors center stage.","The treatment of these particular systems often requires the use of adiabatic expansions in terms of the frequency of the external driving or the velocity of some classical degree of freedom.","However, due to the difficulty of getting the expressions, most works have only explored first-order terms.","Despite this, adiabatic expansions have allowed the study of intriguing phenomena such as adiabatic quantum pumps and motors, or electronic friction.","Here, we use nonequilibrium Green's functions, within a Schwinger-Keldysh approach, to develop second-order expressions for the energy, heat, and charge currents.","We illustrate, through two simple models, how the obtained formulas produce physically consistent results, and allow for the thermodynamic study of unexplored phenomena, such as second-order monoparametric pumping."],"url":"http://arxiv.org/abs/2404.16757v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 17:05:55","title":"On the global dynamics of a forest model with monotone positive feedback and memory","abstract":"We continue to study (see arXiv:2401.08618, https://doi.org/10.48550/arXiv.2401.08618) a renewal equation $\\phi(t)=\\frak F\\phi_t$ proposed in [C. Barril et al., J. Math. Biology, https://doi.org/10.1007/s00285-024-02084-x] to model trees growth. This time we are considering the case when the per capita reproduction rate $\\beta(x)$ is a non-monotone (unimodal) function of tree's height $x$. Note that the height of some species of trees can impact negatively seed viability, in a kind of autogamy depression. Similarly to previous works, it is also assumed that the growth rate $g(x)$ of an individual of height $x$ is a strictly decreasing function. Here we analyse the connection between dynamics of the associated one-dimensional map $F(b)= {\\frak F}b,$ $b \\in {\\mathbb R}_+$, and the delayed (hence infinite-dimensional) model $\\phi(t)=\\frak F\\phi_t$. Our key observation is that this model is of monotone positive feedback type since $F$ is strictly increasing on ${\\mathbb R}_+$ independently on the monotonicity properties of $\\beta$.","sentences":["We continue to study (see arXiv:2401.08618, https://doi.org/10.48550/arXiv.2401.08618) a renewal equation $\\phi(t)=\\frak F\\phi_t$ proposed in [C. Barril et al., J. Math.","Biology, https://doi.org/10.1007/s00285-024-02084-x] to model trees growth.","This time we are considering the case when the per capita reproduction rate $\\beta(x)$ is a non-monotone (unimodal) function of tree's height $x$.","Note that the height of some species of trees can impact negatively seed viability, in a kind of autogamy depression.","Similarly to previous works, it is also assumed that the growth rate $g(x)$ of an individual of height $x$ is a strictly decreasing function.","Here we analyse the connection between dynamics of the associated one-dimensional map $F(b)= {\\frak F}b,$ $b \\in {\\mathbb R}_+$, and the delayed (hence infinite-dimensional) model $\\phi(t)=\\frak F\\phi_t$.","Our key observation is that this model is of monotone positive feedback type since $F$ is strictly increasing on ${\\mathbb R}_+$ independently on the monotonicity properties of $\\beta$."],"url":"http://arxiv.org/abs/2404.16749v1","category":"math.DS"}
{"created":"2024-04-25 17:04:42","title":"Characterizing Solar Center-to-Limb Radial-Velocity Variability with SDO","abstract":"Stellar photospheric inhomogeneities are a significant source of noise which currently precludes the discovery of Earth-mass planets orbiting Sun-like stars with the radial-velocity (RV) method. To complement several previous studies which have used ground- and spaced-based facilities to characterize the RV of the Sun, we here characterize the center-to-limb variability (CLV) of solar RVs arising from various solar-surface inhomogeneities observed by SDO/HMI and SDO/AIA. By using various SDO observables to classify pixels and calculate line-of-sight velocities as a function of pixel classification and limb angle, we show that each identified feature type, including the umbrae and penumbrae of sunspots, quiet-Sun magnetoconvective cells, magnetic network, and plage, exhibit distinct and complex CLV signatures, including a notable limb-angle dependence in the observed suppression of convective blueshift for magnetically active regions. We discuss the observed distributions of velocities by identified region type and limb angle, offer interpretations of the physical phenomena that shape these distributions, and emphasize the need to understand the RV signatures of these regions as astrophysical signals, rather than simple (un)correlated noise processes.","sentences":["Stellar photospheric inhomogeneities are a significant source of noise which currently precludes the discovery of Earth-mass planets orbiting Sun-like stars with the radial-velocity (RV) method.","To complement several previous studies which have used ground- and spaced-based facilities to characterize the RV of the Sun, we here characterize the center-to-limb variability (CLV) of solar RVs arising from various solar-surface inhomogeneities observed by SDO/HMI and SDO/AIA.","By using various SDO observables to classify pixels and calculate line-of-sight velocities as a function of pixel classification and limb angle, we show that each identified feature type, including the umbrae and penumbrae of sunspots, quiet-Sun magnetoconvective cells, magnetic network, and plage, exhibit distinct and complex CLV signatures, including a notable limb-angle dependence in the observed suppression of convective blueshift for magnetically active regions.","We discuss the observed distributions of velocities by identified region type and limb angle, offer interpretations of the physical phenomena that shape these distributions, and emphasize the need to understand the RV signatures of these regions as astrophysical signals, rather than simple (un)correlated noise processes."],"url":"http://arxiv.org/abs/2404.16747v1","category":"astro-ph.SR"}
{"created":"2024-04-25 17:00:08","title":"JITScanner: Just-in-Time Executable Page Check in the Linux Operating System","abstract":"Modern malware poses a severe threat to cybersecurity, continually evolving in sophistication. To combat this threat, researchers and security professionals continuously explore advanced techniques for malware detection and analysis. Dynamic analysis, a prevalent approach, offers advantages over static analysis by enabling observation of runtime behavior and detecting obfuscated or encrypted code used to evade detection. However, executing programs within a controlled environment can be resource-intensive, often necessitating compromises, such as limiting sandboxing to an initial period. In our article, we propose an alternative method for dynamic executable analysis: examining the presence of malicious signatures within executable virtual pages precisely when their current content, including any updates over time, is accessed for instruction fetching. Our solution, named JITScanner, is developed as a Linux-oriented package built upon a Loadable Kernel Module (LKM). It integrates a user-level component that communicates efficiently with the LKM using scalable multi-processor/core technology. JITScanner's effectiveness in detecting malware programs and its minimal intrusion in normal runtime scenarios have been extensively tested, with the experiment results detailed in this article. These experiments affirm the viability of our approach, showcasing JITScanner's capability to effectively identify malware while minimizing runtime overhead.","sentences":["Modern malware poses a severe threat to cybersecurity, continually evolving in sophistication.","To combat this threat, researchers and security professionals continuously explore advanced techniques for malware detection and analysis.","Dynamic analysis, a prevalent approach, offers advantages over static analysis by enabling observation of runtime behavior and detecting obfuscated or encrypted code used to evade detection.","However, executing programs within a controlled environment can be resource-intensive, often necessitating compromises, such as limiting sandboxing to an initial period.","In our article, we propose an alternative method for dynamic executable analysis: examining the presence of malicious signatures within executable virtual pages precisely when their current content, including any updates over time, is accessed for instruction fetching.","Our solution, named JITScanner, is developed as a Linux-oriented package built upon a Loadable Kernel Module (LKM).","It integrates a user-level component that communicates efficiently with the LKM using scalable multi-processor/core technology.","JITScanner's effectiveness in detecting malware programs and its minimal intrusion in normal runtime scenarios have been extensively tested, with the experiment results detailed in this article.","These experiments affirm the viability of our approach, showcasing JITScanner's capability to effectively identify malware while minimizing runtime overhead."],"url":"http://arxiv.org/abs/2404.16744v1","category":"cs.CR"}
{"created":"2024-04-25 16:50:59","title":"Bayesian Nonparametric Inference in McKean-Vlasov models","abstract":"We consider nonparametric statistical inference on a periodic interaction potential $W$ from noisy discrete space-time measurements of solutions $\\rho=\\rho_W$ of the nonlinear McKean-Vlasov equation, describing the probability density of the mean field limit of an interacting particle system. We show how Gaussian process priors assigned to $W$ give rise to posterior mean estimators that exhibit fast convergence rates for the implied estimated densities $\\bar \\rho$ towards $\\rho_W$. We further show that if the initial condition $\\phi$ is not too smooth and satisfies a standard deconvolvability condition, then one can consistently infer the potential $W$ itself at convergence rates $N^{-\\theta}$ for appropriate $\\theta>0$, where $N$ is the number of measurements. The exponent $\\theta$ can be taken to approach $1/2$ as the regularity of $W$ increases corresponding to `near-parametric' models.","sentences":["We consider nonparametric statistical inference on a periodic interaction potential $W$ from noisy discrete space-time measurements of solutions $\\rho=\\rho_W$ of the nonlinear McKean-Vlasov equation, describing the probability density of the mean field limit of an interacting particle system.","We show how Gaussian process priors assigned to $W$ give rise to posterior mean estimators that exhibit fast convergence rates for the implied estimated densities $\\bar \\rho$ towards $\\rho_W$. We further show that if the initial condition $\\phi$ is not too smooth and satisfies a standard deconvolvability condition, then one can consistently infer the potential $W$ itself at convergence rates $N^{-\\theta}$ for appropriate $\\theta>0$, where $N$ is the number of measurements.","The exponent $\\theta$ can be taken to approach $1/2$ as the regularity of $W$ increases corresponding to `near-parametric' models."],"url":"http://arxiv.org/abs/2404.16742v1","category":"math.ST"}
{"created":"2024-04-25 16:50:54","title":"Parameterized Complexity of Efficient Sortation","abstract":"A crucial challenge arising in the design of large-scale logistical networks is to optimize parcel sortation for routing. We study this problem under the recent graph-theoretic formalization of Van Dyk, Klause, Koenemann and Megow (IPCO 2024). The problem asks - given an input digraph D (the fulfillment network) together with a set of commodities represented as source-sink tuples - for a minimum-outdegree subgraph H of the transitive closure of D that contains a source-sink route for each of the commodities. Given the underlying motivation, we study two variants of the problem which differ in whether the routes for the commodities are assumed to be given, or can be chosen arbitrarily.   We perform a thorough parameterized analysis of the complexity of both problems. Our results concentrate on three fundamental parameterizations of the problem: (1) When attempting to parameterize by the target outdegree of H, we show that the problems are paraNP-hard even in highly restricted cases; (2) When parameterizing by the number of commodities, we utilize Ramsey-type arguments, kernelization and treewidth reduction techniques to obtain parameterized algorithms for both problems; (3) When parameterizing by the structure of D, we establish fixed-parameter tractability for both problems w.r.t. treewidth, maximum degree and the maximum routing length. We combine this with lower bounds which show that omitting any of the three parameters results in paraNP-hardness.","sentences":["A crucial challenge arising in the design of large-scale logistical networks is to optimize parcel sortation for routing.","We study this problem under the recent graph-theoretic formalization of Van Dyk, Klause, Koenemann and Megow (IPCO 2024).","The problem asks - given an input digraph D (the fulfillment network) together with a set of commodities represented as source-sink tuples - for a minimum-outdegree subgraph H of the transitive closure of D that contains a source-sink route for each of the commodities.","Given the underlying motivation, we study two variants of the problem which differ in whether the routes for the commodities are assumed to be given, or can be chosen arbitrarily.   ","We perform a thorough parameterized analysis of the complexity of both problems.","Our results concentrate on three fundamental parameterizations of the problem: (1) When attempting to parameterize by the target outdegree of H, we show that the problems are paraNP-hard even in highly restricted cases; (2) When parameterizing by the number of commodities, we utilize Ramsey-type arguments, kernelization and treewidth reduction techniques to obtain parameterized algorithms for both problems; (3) When parameterizing by the structure of D, we establish fixed-parameter tractability for both problems w.r.t",". treewidth, maximum degree and the maximum routing length.","We combine this with lower bounds which show that omitting any of the three parameters results in paraNP-hardness."],"url":"http://arxiv.org/abs/2404.16741v1","category":"cs.DS"}
{"created":"2024-04-25 16:43:25","title":"Uniform Substitution for Differential Refinement Logic","abstract":"This paper introduces a uniform substitution calculus for differential refinement logic dRL. The logic dRL extends the differential dynamic logic dL such that one can simultaneously reason about properties of and relations between hybrid systems. Refinements is useful e.g. for simplifying proofs by relating a concrete hybrid system to an abstract one from which the property can be proved more easily. Uniform substitution is the key to parsimonious prover microkernels. It enables the verbatim use of single axiom formulas instead of axiom schemata with soundness-critical side conditions scattered across the proof calculus. The uniform substitution rule can then be used to instantiate all axioms soundly. Access to differential variables in dRL enables more control over the notion of refinement, which is shown to be decidable on a fragment of hybrid programs.","sentences":["This paper introduces a uniform substitution calculus for differential refinement logic dRL.","The logic dRL extends the differential dynamic logic dL such that one can simultaneously reason about properties of and relations between hybrid systems.","Refinements is useful e.g. for simplifying proofs by relating a concrete hybrid system to an abstract one from which the property can be proved more easily.","Uniform substitution is the key to parsimonious prover microkernels.","It enables the verbatim use of single axiom formulas instead of axiom schemata with soundness-critical side conditions scattered across the proof calculus.","The uniform substitution rule can then be used to instantiate all axioms soundly.","Access to differential variables in dRL enables more control over the notion of refinement, which is shown to be decidable on a fragment of hybrid programs."],"url":"http://arxiv.org/abs/2404.16734v1","category":"cs.LO"}
{"created":"2024-04-25 16:41:57","title":"Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search","abstract":"In this paper, we establish the first explicit and non-asymptotic global convergence analysis of the BFGS method when deployed with an inexact line search scheme that satisfies the Armijo-Wolfe conditions. We show that BFGS achieves a global convergence rate of $(1-\\frac{1}{\\kappa})^k$ for $\\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\\kappa=\\frac{L}{\\mu}$ denotes the condition number. Furthermore, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate only determined by the line search parameters and independent of the condition number. These results hold for any initial point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, although the choice of $B_0$ affects the iteration count required to attain these rates. Specifically, we show that for $B_0 = LI$, the rate of $O((1-\\frac{1}{\\kappa})^k)$ appears from the first iteration, while for $B_0 = \\mu I$, it takes $d\\log \\kappa$ iterations. Conversely, the condition number-independent linear convergence rate for $B_0 = LI$ occurs after $O\\left(\\kappa\\left(d +\\frac{M \\sqrt{f(x_0)-f(x_*)}}{\\mu^{3/2}}\\right)\\right)$ iterations, whereas for $B_0 = \\mu I$, it holds after $O\\left(\\frac{M \\sqrt{f(x_0)-f(x_*)}}{\\mu^{3/2}}\\left(d\\log \\kappa + \\kappa\\right)\\right)$ iterations. Here, $d$ denotes the dimension of the problem, $M$ is the Lipschitz parameter of the Hessian, and $x_*$ denotes the optimal solution. We further leverage these global linear convergence results to characterize the overall iteration complexity of BFGS when deployed with the Armijo-Wolfe line search.","sentences":["In this paper, we establish the first explicit and non-asymptotic global convergence analysis of the BFGS method when deployed with an inexact line search scheme that satisfies the Armijo-Wolfe conditions.","We show that BFGS achieves a global convergence rate of $(1-\\frac{1}{\\kappa})^k$ for $\\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\\kappa=\\frac{L}{\\mu}$ denotes the condition number.","Furthermore, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate only determined by the line search parameters and independent of the condition number.","These results hold for any initial point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, although the choice of $B_0$ affects the iteration count required to attain these rates.","Specifically, we show that for $B_0 = LI$, the rate of $O((1-\\frac{1}{\\kappa})^k)$ appears from the first iteration, while for $B_0 = \\mu I$, it takes $d\\log \\kappa$ iterations.","Conversely, the condition number-independent linear convergence rate for $B_0 = LI$ occurs after $O\\left(\\kappa\\left(d +\\frac{M \\sqrt{f(x_0)-f(x_*)}}{\\mu^{3/2}}\\right)\\right)$ iterations, whereas for $B_0 = \\mu I$, it holds after $O\\left(\\frac{M \\sqrt{f(x_0)-f(x_*)}}{\\mu^{3/2}}\\left(d\\log \\kappa + \\kappa\\right)\\right)$ iterations.","Here, $d$ denotes the dimension of the problem, $M$ is the Lipschitz parameter of the Hessian, and $x_*$ denotes the optimal solution.","We further leverage these global linear convergence results to characterize the overall iteration complexity of BFGS when deployed with the Armijo-Wolfe line search."],"url":"http://arxiv.org/abs/2404.16731v1","category":"math.OC"}
{"created":"2024-04-25 16:41:12","title":"Finch: Sparse and Structured Array Programming with Control Flow","abstract":"From FORTRAN to NumPy, arrays have revolutionized how we express computation. However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids. Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Support for structured data is fragmented and incomplete. Existing frameworks limit the array structures and program control flow they support to better simplify the problem.   In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures. Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized. Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms. Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks. Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros. We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface.","sentences":["From FORTRAN to NumPy, arrays have revolutionized how we express computation.","However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids.","Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry.","Support for structured data is fragmented and incomplete.","Existing frameworks limit the array structures and program control flow they support to better simplify the problem.   ","In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures.","Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized.","Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms.","Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks.","Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros.","We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface."],"url":"http://arxiv.org/abs/2404.16730v1","category":"cs.MS"}
{"created":"2024-04-25 16:34:26","title":"Clique Is Hard on Average for Sherali-Adams with Bounded Coefficients","abstract":"We prove that Sherali-Adams with polynomially bounded coefficients requires proofs of size $n^{\\Omega(d)}$ to rule out the existence of an $n^{\\Theta(1)}$-clique in Erd\\H{o}s-R\\'{e}nyi random graphs whose maximum clique is of size $d\\leq 2\\log n$. This lower bound is tight up to the multiplicative constant in the exponent. We obtain this result by introducing a technique inspired by pseudo-calibration which may be of independent interest. The technique involves defining a measure on monomials that precisely captures the contribution of a monomial to a refutation. This measure intuitively captures progress and should have further applications in proof complexity.","sentences":["We prove that Sherali-Adams with polynomially bounded coefficients requires proofs of size $n^{\\Omega(d)}$ to rule out the existence of an $n^{\\Theta(1)}$-clique in Erd\\H{o}s-R\\'{e}nyi random graphs whose maximum clique is of size $d\\leq 2\\log n$. This lower bound is tight up to the multiplicative constant in the exponent.","We obtain this result by introducing a technique inspired by pseudo-calibration which may be of independent interest.","The technique involves defining a measure on monomials that precisely captures the contribution of a monomial to a refutation.","This measure intuitively captures progress and should have further applications in proof complexity."],"url":"http://arxiv.org/abs/2404.16722v1","category":"cs.CC"}
{"created":"2024-04-25 16:22:12","title":"Distributed MPC for PWA Systems Based on Switching ADMM","abstract":"This paper presents a novel approach for distributed model predictive control (MPC) for piecewise affine (PWA) systems. Existing approaches rely on solving mixed-integer optimization problems, requiring significant computation power or time. We propose a distributed MPC scheme that requires solving only convex optimization problems. The key contribution is a novel method, based on the alternating direction method of multipliers, for solving the non-convex optimal control problem that arises due to the PWA dynamics. We present a distributed MPC scheme, leveraging this method, that explicitly accounts for the coupling between subsystems by reaching agreement on the values of coupled states. Stability and recursive feasibility are shown under additional assumptions on the underlying system. Two numerical examples are provided, in which the proposed controller is shown to significantly improve the CPU time and closed-loop performance over existing state-of-the-art approaches.","sentences":["This paper presents a novel approach for distributed model predictive control (MPC) for piecewise affine (PWA) systems.","Existing approaches rely on solving mixed-integer optimization problems, requiring significant computation power or time.","We propose a distributed MPC scheme that requires solving only convex optimization problems.","The key contribution is a novel method, based on the alternating direction method of multipliers, for solving the non-convex optimal control problem that arises due to the PWA dynamics.","We present a distributed MPC scheme, leveraging this method, that explicitly accounts for the coupling between subsystems by reaching agreement on the values of coupled states.","Stability and recursive feasibility are shown under additional assumptions on the underlying system.","Two numerical examples are provided, in which the proposed controller is shown to significantly improve the CPU time and closed-loop performance over existing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.16712v1","category":"math.OC"}
{"created":"2024-04-25 16:17:23","title":"Understanding Reliability from a Regression Perspective","abstract":"Reliability is an important quantification of measurement precision based on a latent variable measurement model. Inspired by McDonald (2011), we present a regression framework of reliability, placing emphasis on whether latent or observed scores serve as the regression outcome. Our theory unifies two extant perspectives of reliability: (a) classical test theory (measurement decomposition), and (b) optimal prediction of latent scores (prediction decomposition). Importantly, reliability should be treated as a property of the observed score under a measurement decomposition, but a property of the latent score under a prediction decomposition. To facilitate the evaluation and interpretation of distinct reliability coefficients for complex measurement models, we introduce a Monte Carlo approach for approximate calculation of reliability. We illustrate the proposed computational procedure with an empirical data analysis, which concerns measuring susceptibility and severity of depressive symptoms using a two-dimensional item response theory model. We conclude with a discussion on computing reliability coefficients and outline future avenues of research.","sentences":["Reliability is an important quantification of measurement precision based on a latent variable measurement model.","Inspired by McDonald (2011), we present a regression framework of reliability, placing emphasis on whether latent or observed scores serve as the regression outcome.","Our theory unifies two extant perspectives of reliability: (a) classical test theory (measurement decomposition), and (b) optimal prediction of latent scores (prediction decomposition).","Importantly, reliability should be treated as a property of the observed score under a measurement decomposition, but a property of the latent score under a prediction decomposition.","To facilitate the evaluation and interpretation of distinct reliability coefficients for complex measurement models, we introduce a Monte Carlo approach for approximate calculation of reliability.","We illustrate the proposed computational procedure with an empirical data analysis, which concerns measuring susceptibility and severity of depressive symptoms using a two-dimensional item response theory model.","We conclude with a discussion on computing reliability coefficients and outline future avenues of research."],"url":"http://arxiv.org/abs/2404.16709v1","category":"stat.ME"}
{"created":"2024-04-25 16:07:55","title":"Fidelity and criticality in the nonreciprocal Aubry-Andr{\u00e9}-Harper model","abstract":"We study the critical behaviors of the ground and first excited states in the one-dimensional nonreciprocal Aubry-Andr{\\'e}-Harper model using both the self-normal and biorthogonal fidelity susceptibilities. We demonstrate that fidelity susceptibilities serve as a probe for the phase transition in the nonreciprocal AAH model. For ground states, characterized by real eigenenergies across the entire regime, both fidelity susceptibilities near the critical points scale as $N^{2}$, akin to the Hermitian AAH model. However, for the first-excited states, where $\\mathcal{PT}$ transitions occur, the fidelity susceptibilities exhibit distinct scaling laws, contingent upon whether the lattice consists of even or odd sites. For even lattices, the self-normal fidelity susceptibilities near the critical points continue to scale as $N^{2}$. For odd lattices, the biorthogonal fidelity susceptibilities diverge, while the self-normal fidelity susceptibilities exhibit linear behavior, indicating a novel scaling law.","sentences":["We study the critical behaviors of the ground and first excited states in the one-dimensional nonreciprocal Aubry-Andr{\\'e}-Harper model using both the self-normal and biorthogonal fidelity susceptibilities.","We demonstrate that fidelity susceptibilities serve as a probe for the phase transition in the nonreciprocal AAH model.","For ground states, characterized by real eigenenergies across the entire regime, both fidelity susceptibilities near the critical points scale as $N^{2}$, akin to the Hermitian AAH model.","However, for the first-excited states, where $\\mathcal{PT}$ transitions occur, the fidelity susceptibilities exhibit distinct scaling laws, contingent upon whether the lattice consists of even or odd sites.","For even lattices, the self-normal fidelity susceptibilities near the critical points continue to scale as $N^{2}$. For odd lattices, the biorthogonal fidelity susceptibilities diverge, while the self-normal fidelity susceptibilities exhibit linear behavior, indicating a novel scaling law."],"url":"http://arxiv.org/abs/2404.16704v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-25 16:02:44","title":"On the Streaming Complexity of Expander Decomposition","abstract":"In this paper we study the problem of finding $(\\epsilon, \\phi)$-expander decompositions of a graph in the streaming model, in particular for dynamic streams of edge insertions and deletions. The goal is to partition the vertex set so that every component induces a $\\phi$-expander, while the number of inter-cluster edges is only an $\\epsilon$ fraction of the total volume. It was recently shown that there exists a simple algorithm to construct a $(O(\\phi \\log n), \\phi)$-expander decomposition of an $n$-vertex graph using $\\widetilde{O}(n/\\phi^2)$ bits of space [Filtser, Kapralov, Makarov, ITCS'23]. This result calls for understanding the extent to which a dependence in space on the sparsity parameter $\\phi$ is inherent. We move towards answering this question on two fronts. We prove that a $(O(\\phi \\log n), \\phi)$-expander decomposition can be found using $\\widetilde{O}(n)$ space, for every $\\phi$. At the core of our result is the first streaming algorithm for computing boundary-linked expander decompositions, a recently introduced strengthening of the classical notion [Goranci et al., SODA'21]. The key advantage is that a classical sparsifier [Fung et al., STOC'11], with size independent of $\\phi$, preserves the cuts inside the clusters of a boundary-linked expander decomposition within a multiplicative error. Notable algorithmic applications use sequences of expander decompositions, in particular one often repeatedly computes a decomposition of the subgraph induced by the inter-cluster edges (e.g., the seminal work of Spielman and Teng on spectral sparsifiers [Spielman, Teng, SIAM Journal of Computing 40(4)], or the recent maximum flow breakthrough [Chen et al., FOCS'22], among others). We prove that any streaming algorithm that computes a sequence of $(O(\\phi \\log n), \\phi)$-expander decompositions requires ${\\widetilde{\\Omega}}(n/\\phi)$ bits of space, even in insertion only streams.","sentences":["In this paper we study the problem of finding $(\\epsilon, \\phi)$-expander decompositions of a graph in the streaming model, in particular for dynamic streams of edge insertions and deletions.","The goal is to partition the vertex set so that every component induces a $\\phi$-expander, while the number of inter-cluster edges is only an $\\epsilon$ fraction of the total volume.","It was recently shown that there exists a simple algorithm to construct a $(O(\\phi \\log n), \\phi)$-expander decomposition of an $n$-vertex graph using $\\widetilde{O}(n/\\phi^2)$ bits of space [Filtser, Kapralov, Makarov, ITCS'23].","This result calls for understanding the extent to which a dependence in space on the sparsity parameter $\\phi$ is inherent.","We move towards answering this question on two fronts.","We prove that a $(O(\\phi \\log n), \\phi)$-expander decomposition can be found using $\\widetilde{O}(n)$ space, for every $\\phi$. At the core of our result is the first streaming algorithm for computing boundary-linked expander decompositions, a recently introduced strengthening of the classical notion","[Goranci et al., SODA'21].","The key advantage is that a classical sparsifier","[Fung et al., STOC'11], with size independent of $\\phi$, preserves the cuts inside the clusters of a boundary-linked expander decomposition within a multiplicative error.","Notable algorithmic applications use sequences of expander decompositions, in particular one often repeatedly computes a decomposition of the subgraph induced by the inter-cluster edges (e.g., the seminal work of Spielman and Teng on spectral sparsifiers [Spielman, Teng, SIAM Journal of Computing 40(4)], or the recent maximum flow breakthrough [Chen et al., FOCS'22], among others).","We prove that any streaming algorithm that computes a sequence of $(O(\\phi \\log n), \\phi)$-expander decompositions requires ${\\widetilde{\\Omega}}(n/\\phi)$ bits of space, even in insertion only streams."],"url":"http://arxiv.org/abs/2404.16701v1","category":"cs.DS"}
{"created":"2024-04-25 16:01:14","title":"Dimensional Crossover of Microscopic Magnetic Metasurfaces for Magnetic Field Amplification","abstract":"Transformation optics applied to low frequency magnetic systems has been recently implemented to design magnetic field concentrators and cloaks with superior performance. Although this achievement has been amply demonstrated theoretically and experimentally in bulk 3D macrostructures, the performance of these devices at low dimensions remains an open question. In this work, we numerically investigate the non-monotonic evolution of the gain of a magnetic metamaterial field concentrator as the axial dimension is progressively shrunk. In particular, we show that in planar structures the role played by the diamagnetic components becomes negligible, whereas the paramagnetic elements increase their magnetic field channeling efficiency. This is further demonstrated experimentally by tracking the gain of superconductor-ferromagnet concentrators through the superconducting transition. Interestingly, for thicknesses where the diamagnetic petals play an important role for the concentration gain, they also help to reduce the stray field of the concentrator, thus limiting the perturbation of the external field (invisibility). Our findings establish a roadmap and set clear geometrical limits for designing low dimensional magnetic field concentrators.","sentences":["Transformation optics applied to low frequency magnetic systems has been recently implemented to design magnetic field concentrators and cloaks with superior performance.","Although this achievement has been amply demonstrated theoretically and experimentally in bulk 3D macrostructures, the performance of these devices at low dimensions remains an open question.","In this work, we numerically investigate the non-monotonic evolution of the gain of a magnetic metamaterial field concentrator as the axial dimension is progressively shrunk.","In particular, we show that in planar structures the role played by the diamagnetic components becomes negligible, whereas the paramagnetic elements increase their magnetic field channeling efficiency.","This is further demonstrated experimentally by tracking the gain of superconductor-ferromagnet concentrators through the superconducting transition.","Interestingly, for thicknesses where the diamagnetic petals play an important role for the concentration gain, they also help to reduce the stray field of the concentrator, thus limiting the perturbation of the external field (invisibility).","Our findings establish a roadmap and set clear geometrical limits for designing low dimensional magnetic field concentrators."],"url":"http://arxiv.org/abs/2404.16700v1","category":"physics.app-ph"}
{"created":"2024-04-25 15:42:10","title":"Reusing Deep Learning Models: Challenges and Directions in Software Engineering","abstract":"Deep neural networks (DNNs) achieve state-of-the-art performance in many areas, including computer vision, system configuration, and question-answering. However, DNNs are expensive to develop, both in intellectual effort (e.g., devising new architectures) and computational costs (e.g., training). Reusing DNNs is a promising direction to amortize costs within a company and across the computing industry. As with any new technology, however, there are many challenges in reusing DNNs. These challenges include both missing technical capabilities and missing engineering practices.   This vision paper describes challenges in current approaches to DNN re-use. We summarize studies of re-use failures across the spectrum of re-use techniques, including conceptual (e.g., reusing based on a research paper), adaptation (e.g., re-using by building on an existing implementation), and deployment (e.g., direct re-use on a new device). We outline possible advances that would improve each kind of re-use.","sentences":["Deep neural networks (DNNs) achieve state-of-the-art performance in many areas, including computer vision, system configuration, and question-answering.","However, DNNs are expensive to develop, both in intellectual effort (e.g., devising new architectures) and computational costs (e.g., training).","Reusing DNNs is a promising direction to amortize costs within a company and across the computing industry.","As with any new technology, however, there are many challenges in reusing DNNs.","These challenges include both missing technical capabilities and missing engineering practices.   ","This vision paper describes challenges in current approaches to DNN re-use.","We summarize studies of re-use failures across the spectrum of re-use techniques, including conceptual (e.g., reusing based on a research paper), adaptation (e.g., re-using by building on an existing implementation), and deployment (e.g., direct re-use on a new device).","We outline possible advances that would improve each kind of re-use."],"url":"http://arxiv.org/abs/2404.16688v1","category":"cs.SE"}
{"created":"2024-04-25 15:33:15","title":"Monolithic two-level Schwarz preconditioner for Biot's consolidation model in two space dimensions","abstract":"This paper addresses the construction and analysis of a class of domain decomposition methods for the iterative solution of the quasi-static Biot problem in three-field formulation. The considered discrete model arises from time discretization by the implicit Euler method and space discretization by a family of strongly mass-conserving methods exploiting $H^{div}$-conforming approximations of the solid displacement and fluid flux fields. For the resulting saddle-point problem, we construct monolithic overlapping domain decomposition (DD) methods whose analysis relies on a transformation into an equivalent symmetric positive definite system and on stable decompositions of the involved finite element spaces under proper problem-dependent norms. Numerical results on two-dimensional test problems are in accordance with the provided theoretical uniform convergence estimates for the two-level multiplicative Schwarz method.","sentences":["This paper addresses the construction and analysis of a class of domain decomposition methods for the iterative solution of the quasi-static Biot problem in three-field formulation.","The considered discrete model arises from time discretization by the implicit Euler method and space discretization by a family of strongly mass-conserving methods exploiting $H^{div}$-conforming approximations of the solid displacement and fluid flux fields.","For the resulting saddle-point problem, we construct monolithic overlapping domain decomposition (DD) methods whose analysis relies on a transformation into an equivalent symmetric positive definite system and on stable decompositions of the involved finite element spaces under proper problem-dependent norms.","Numerical results on two-dimensional test problems are in accordance with the provided theoretical uniform convergence estimates for the two-level multiplicative Schwarz method."],"url":"http://arxiv.org/abs/2404.16684v1","category":"math.NA"}
{"created":"2024-04-25 15:28:08","title":"The azimuthal distribution of ejecta mass from oblique impacts into sand","abstract":"We measure ejecta mass as a function of azimuthal and impact angle for 104 m/s oblique impacts into sand. We find that the ejecta mass distribution is strongly sensitive to azimuthal angle with up to 8 times more mass in ejecta on the downrange side compared to the uprange side. Crater radii, measured from the site of impact, are measured at different impact and azimuthal angles. Crater ejecta scaling laws are modified to depend on azimuthal and impact angle. We find that crater radii are sensitive to both impact and azimuthal angle but the ejecta mass as a function of both angles can be estimated from the cube of the crater radius without an additional angular dependent function. The ejecta distributions are relevant for processes that depend upon the integrated properties of intermediate velocity impacts occurring in the outer solar system and possibly during planetesimal formation.","sentences":["We measure ejecta mass as a function of azimuthal and impact angle for 104 m/s oblique impacts into sand.","We find that the ejecta mass distribution is strongly sensitive to azimuthal angle with up to 8 times more mass in ejecta on the downrange side compared to the uprange side.","Crater radii, measured from the site of impact, are measured at different impact and azimuthal angles.","Crater ejecta scaling laws are modified to depend on azimuthal and impact angle.","We find that crater radii are sensitive to both impact and azimuthal angle but the ejecta mass as a function of both angles can be estimated from the cube of the crater radius without an additional angular dependent function.","The ejecta distributions are relevant for processes that depend upon the integrated properties of intermediate velocity impacts occurring in the outer solar system and possibly during planetesimal formation."],"url":"http://arxiv.org/abs/2404.16677v1","category":"astro-ph.EP"}
{"created":"2024-04-25 15:20:47","title":"RUMOR: Reinforcement learning for Understanding a Model of the Real World for Navigation in Dynamic Environments","abstract":"Autonomous navigation in dynamic environments is a complex but essential task for autonomous robots, with recent deep reinforcement learning approaches showing promising results. However, the complexity of the real world makes it infeasible to train agents in every possible scenario configuration. Moreover, existing methods typically overlook factors such as robot kinodynamic constraints, or assume perfect knowledge of the environment. In this work, we present RUMOR, a novel planner for differential-drive robots that uses deep reinforcement learning to navigate in highly dynamic environments. Unlike other end-to-end DRL planners, it uses a descriptive robocentric velocity space model to extract the dynamic environment information, enhancing training effectiveness and scenario interpretation. Additionally, we propose an action space that inherently considers robot kinodynamics and train it in a simulator that reproduces the real world problematic aspects, reducing the gap between the reality and simulation. We extensively compare RUMOR with other state-of-the-art approaches, demonstrating a better performance, and provide a detailed analysis of the results. Finally, we validate RUMOR's performance in real-world settings by deploying it on a ground robot. Our experiments, conducted in crowded scenarios and unseen environments, confirm the algorithm's robustness and transferability.","sentences":["Autonomous navigation in dynamic environments is a complex but essential task for autonomous robots, with recent deep reinforcement learning approaches showing promising results.","However, the complexity of the real world makes it infeasible to train agents in every possible scenario configuration.","Moreover, existing methods typically overlook factors such as robot kinodynamic constraints, or assume perfect knowledge of the environment.","In this work, we present RUMOR, a novel planner for differential-drive robots that uses deep reinforcement learning to navigate in highly dynamic environments.","Unlike other end-to-end DRL planners, it uses a descriptive robocentric velocity space model to extract the dynamic environment information, enhancing training effectiveness and scenario interpretation.","Additionally, we propose an action space that inherently considers robot kinodynamics and train it in a simulator that reproduces the real world problematic aspects, reducing the gap between the reality and simulation.","We extensively compare RUMOR with other state-of-the-art approaches, demonstrating a better performance, and provide a detailed analysis of the results.","Finally, we validate RUMOR's performance in real-world settings by deploying it on a ground robot.","Our experiments, conducted in crowded scenarios and unseen environments, confirm the algorithm's robustness and transferability."],"url":"http://arxiv.org/abs/2404.16672v1","category":"cs.RO"}
{"created":"2024-04-25 15:15:26","title":"A candidate period of 4.605 day for FRB 20121102A and one possible implication of its origin","abstract":"A firm establishment of the presence or the lack of periodicity in repeating Fast Radio Bursts (FRBs) is crucial for determining their origins. Here we compile 1145 radio bursts of FRB 20121102A with fluence larger than 0.15 Jy ms from observations using the Five-hundredmeter Aperture Spherical radio Telescope, Arecibo Observatory, Green Bank Telescope, Effelsberg Telescope, MeerKAT Telescope, Lovell Telescope, Deep Space Network 70 m radio telescopes, Very Large Array, and the Westerbork Synthesis Radio Telescope spanning the time interval of MJD 57175-58776. A quasi-period of $157.1_{-4.8}^{+5.2}$ day and a candidate quasi-period of $4.605_{-0.010}^{+0.003}$ day are found through the phase-folding probability binomial analysis. The former is consistent with previous findings and the latter is new. The 4.605 day periodicity is more obvious in high-energy bursts with fluence larger than $10^{38}$ erg. The presence of these (candidate) quasi-periods, together with the corresponding width of burst accumulation in the phase space, are consistent with the bursts' originating from a binary degenerate star system with a close-by planet around the primary neutron star.","sentences":["A firm establishment of the presence or the lack of periodicity in repeating Fast Radio Bursts (FRBs) is crucial for determining their origins.","Here we compile 1145 radio bursts of FRB 20121102A with fluence larger than 0.15 Jy ms from observations using the Five-hundredmeter Aperture Spherical radio Telescope, Arecibo Observatory, Green Bank Telescope, Effelsberg Telescope, MeerKAT Telescope, Lovell Telescope, Deep Space Network 70 m radio telescopes, Very Large Array, and the Westerbork Synthesis Radio Telescope spanning the time interval of MJD 57175-58776.","A quasi-period of $157.1_{-4.8}^{+5.2}$ day and a candidate quasi-period of $4.605_{-0.010}^{+0.003}$ day are found through the phase-folding probability binomial analysis.","The former is consistent with previous findings and the latter is new.","The 4.605 day periodicity is more obvious in high-energy bursts with fluence larger than $10^{38}$ erg.","The presence of these (candidate) quasi-periods, together with the corresponding width of burst accumulation in the phase space, are consistent with the bursts' originating from a binary degenerate star system with a close-by planet around the primary neutron star."],"url":"http://arxiv.org/abs/2404.16669v1","category":"astro-ph.HE"}
{"created":"2024-04-25 15:11:50","title":"The First Estimation of the Ambipolar Diffusivity Coefficient from Multi-Scale Observations of the Class 0/I Protostar, HOPS-370","abstract":"Protostars are born in magnetized environments. As a consequence, the formation of protostellar disks can be suppressed by the magnetic field efficiently removing angular momentum of the infalling material. Non-ideal MHD effects are proposed to as one way to allow protostellar disks to form. Thus, it is important to understand their contributions in observations of protostellar systems. We derive an analytical equation to estimate the ambipolar diffusivity coefficient at the edge of the protostellar disk in the Class 0/I protostar, HOPS-370, for the first time, under the assumption that the disk radius is set by ambipolar diffusion. Using previous results of the protostellar mass, disk mass, disk radius, density and temperature profiles and magnetic field strength, we estimate the ambipolar diffusivity coefficient to be $1.7^{+1.5}_{-1.4}\\times10^{19}\\,\\mathrm{cm^{2}\\,s^{-1}}$. We quantify the contribution of ambipolar diffusion by estimating its dimensionless Els\\\"{a}sser number to be $\\sim1.7^{+1.0}_{-1.0}$, indicating its dynamical importance in this region. We compare to chemical calculations of the ambipolar diffusivity coefficient using the Non-Ideal magnetohydrodynamics Coefficients and Ionisation Library (NICIL), which is consistent with our results. In addition, we compare our derived ambipolar diffusivity coefficient to the diffusivity coefficients for Ohmic dissipation and the Hall effect, and find ambipolar diffusion is dominant in our density regime. These results demonstrate a new methodology to understand non-ideal MHD effects in observations of protostellar disks. More detailed modeling of the magnetic field, envelope and microphysics, along with a larger sample of protostellar systems is needed to further understand the contributions of non-ideal MHD.","sentences":["Protostars are born in magnetized environments.","As a consequence, the formation of protostellar disks can be suppressed by the magnetic field efficiently removing angular momentum of the infalling material.","Non-ideal MHD effects are proposed to as one way to allow protostellar disks to form.","Thus, it is important to understand their contributions in observations of protostellar systems.","We derive an analytical equation to estimate the ambipolar diffusivity coefficient at the edge of the protostellar disk in the Class 0/I protostar, HOPS-370, for the first time, under the assumption that the disk radius is set by ambipolar diffusion.","Using previous results of the protostellar mass, disk mass, disk radius, density and temperature profiles and magnetic field strength, we estimate the ambipolar diffusivity coefficient to be $1.7^{+1.5}_{-1.4}\\times10^{19}\\,\\mathrm{cm^{2}\\,s^{-1}}$. We quantify the contribution of ambipolar diffusion by estimating its dimensionless Els\\\"{a}sser number to be $\\sim1.7^{+1.0}_{-1.0}$, indicating its dynamical importance in this region.","We compare to chemical calculations of the ambipolar diffusivity coefficient using the Non-Ideal magnetohydrodynamics Coefficients and Ionisation Library (NICIL), which is consistent with our results.","In addition, we compare our derived ambipolar diffusivity coefficient to the diffusivity coefficients for Ohmic dissipation and the Hall effect, and find ambipolar diffusion is dominant in our density regime.","These results demonstrate a new methodology to understand non-ideal MHD effects in observations of protostellar disks.","More detailed modeling of the magnetic field, envelope and microphysics, along with a larger sample of protostellar systems is needed to further understand the contributions of non-ideal MHD."],"url":"http://arxiv.org/abs/2404.16668v1","category":"astro-ph.SR"}
{"created":"2024-04-25 15:00:34","title":"Computing Hamiltonian Paths with Partial Order Restrictions","abstract":"When solving the Hamiltonian path problem it seems natural to be given additional precedence constraints for the order in which the vertices are visited. For example one could decide whether a Hamiltonian path exists for a fixed starting point, or that some vertices are visited before another vertex. We consider the problem of finding a Hamiltonian path that observes all precedence constraints given in a partial order on the vertex set. We show that this problem is $\\mathsf{NP}$-complete even if restricted to complete bipartite graphs and posets of height 2. In contrast, for posets of width $k$ there is an $\\mathcal{O}(k^2 n^k)$ algorithm for arbitrary graphs with $n$ vertices. We show that it is unlikely that the running time of this algorithm can be improved significantly, i.e., there is no $f(k) n^{o(k)}$ time algorithm under the assumption of the Exponential Time Hypothesis. Furthermore, for the class of outerplanar graphs, we give an $\\mathcal{O}(n^2)$ algorithm for arbitrary posets.","sentences":["When solving the Hamiltonian path problem it seems natural to be given additional precedence constraints for the order in which the vertices are visited.","For example one could decide whether a Hamiltonian path exists for a fixed starting point, or that some vertices are visited before another vertex.","We consider the problem of finding a Hamiltonian path that observes all precedence constraints given in a partial order on the vertex set.","We show that this problem is $\\mathsf{NP}$-complete even if restricted to complete bipartite graphs and posets of height 2.","In contrast, for posets of width $k$ there is an $\\mathcal{O}(k^2 n^k)$ algorithm for arbitrary graphs with $n$ vertices.","We show that it is unlikely that the running time of this algorithm can be improved significantly, i.e., there is no $f(k) n^{o(k)}$ time algorithm under the assumption of the Exponential Time Hypothesis.","Furthermore, for the class of outerplanar graphs, we give an $\\mathcal{O}(n^2)$ algorithm for arbitrary posets."],"url":"http://arxiv.org/abs/2404.16662v1","category":"cs.DM"}
{"created":"2024-04-25 14:44:23","title":"Obstruction classes for moduli spaces of sheaves and Lagrangian fibrations","abstract":"We investigate obstruction classes of moduli spaces of sheaves on K3 surfaces. We extend previous results by Caldararu, explicitly determining the obstruction class and its order in the Brauer group. Our main theorem establishes a short exact sequence relating the Brauer group of the moduli space to that of the underlying K3 surface. This provides a criterion for when the moduli space is fine, generalising well-known results for K3 surfaces. Additionally, we explore applications to Ogg-Shafarevich theory for Beauville-Mukai systems. Furthermore, we investigate birational equivalences of Beauville-Mukai systems on elliptic K3 surfaces, presenting a complete characterisation of such equivalences.","sentences":["We investigate obstruction classes of moduli spaces of sheaves on K3 surfaces.","We extend previous results by Caldararu, explicitly determining the obstruction class and its order in the Brauer group.","Our main theorem establishes a short exact sequence relating the Brauer group of the moduli space to that of the underlying K3 surface.","This provides a criterion for when the moduli space is fine, generalising well-known results for K3 surfaces.","Additionally, we explore applications to Ogg-Shafarevich theory for Beauville-Mukai systems.","Furthermore, we investigate birational equivalences of Beauville-Mukai systems on elliptic K3 surfaces, presenting a complete characterisation of such equivalences."],"url":"http://arxiv.org/abs/2404.16652v1","category":"math.AG"}
{"created":"2024-04-25 14:42:12","title":"Evolutionary Large Language Models for Hardware Security: A Comparative Survey","abstract":"Automating hardware (HW) security vulnerability detection and mitigation during the design phase is imperative for two reasons: (i) It must be before chip fabrication, as post-fabrication fixes can be costly or even impractical; (ii) The size and complexity of modern HW raise concerns about unknown vulnerabilities compromising CIA triad. While Large Language Models (LLMs) can revolutionize both HW design and testing processes, within the semiconductor context, LLMs can be harnessed to automatically rectify security-relevant vulnerabilities inherent in HW designs. This study explores the seeds of LLM integration in register transfer level (RTL) designs, focusing on their capacity for autonomously resolving security-related vulnerabilities. The analysis involves comparing methodologies, assessing scalability, interpretability, and identifying future research directions. Potential areas for exploration include developing specialized LLM architectures for HW security tasks and enhancing model performance with domain-specific knowledge, leading to reliable automated security measurement and risk mitigation associated with HW vulnerabilities.","sentences":["Automating hardware (HW) security vulnerability detection and mitigation during the design phase is imperative for two reasons: (i) It must be before chip fabrication, as post-fabrication fixes can be costly or even impractical; (ii) The size and complexity of modern HW raise concerns about unknown vulnerabilities compromising CIA triad.","While Large Language Models (LLMs) can revolutionize both HW design and testing processes, within the semiconductor context, LLMs can be harnessed to automatically rectify security-relevant vulnerabilities inherent in HW designs.","This study explores the seeds of LLM integration in register transfer level (RTL) designs, focusing on their capacity for autonomously resolving security-related vulnerabilities.","The analysis involves comparing methodologies, assessing scalability, interpretability, and identifying future research directions.","Potential areas for exploration include developing specialized LLM architectures for HW security tasks and enhancing model performance with domain-specific knowledge, leading to reliable automated security measurement and risk mitigation associated with HW vulnerabilities."],"url":"http://arxiv.org/abs/2404.16651v1","category":"cs.CR"}
{"created":"2024-04-25 14:41:53","title":"Design optimization of advanced tow-steered composites with manufacturing constraints","abstract":"Tow steering technologies, such as Automated fiber placement, enable the fabrication of composite laminates with curvilinear fiber, tow, or tape paths. Designers may therefore tailor tow orientations locally according to the expected local stress state within a structure, such that strong and stiff orientations of the tow are (for example) optimized to provide maximal mechanical benefit. Tow path optimization can be an effective tool in automating this design process, yet has a tendency to create complex designs that may be challenging to manufacture. In the context of tow steering, these complexities can manifest in defects such as tow wrinkling, gaps, overlaps. In this work, we implement manufacturing constraints within the tow path optimization formulation to restrict the minimum tow turning radius and the maximum density of gaps between and overlaps of tows. This is achieved by bounding the local value of the curl and divergence of the vector field associated with the tow orientations. The resulting local constraints are effectively enforced in the optimization framework through the Augmented Lagrangian method. The resulting optimization methodology is demonstrated by designing 2D and 3D structures with optimized tow orientation paths that maximize stiffness (minimize compliance) considering various levels of manufacturing restrictions. The optimized tow paths are shown to be structurally efficient and to respect imposed manufacturing constraints. As expected, the more geometrical complexity that can be achieved by the feedstock tow and placement technology, the higher the stiffness of the resulting optimized design.","sentences":["Tow steering technologies, such as Automated fiber placement, enable the fabrication of composite laminates with curvilinear fiber, tow, or tape paths.","Designers may therefore tailor tow orientations locally according to the expected local stress state within a structure, such that strong and stiff orientations of the tow are (for example) optimized to provide maximal mechanical benefit.","Tow path optimization can be an effective tool in automating this design process, yet has a tendency to create complex designs that may be challenging to manufacture.","In the context of tow steering, these complexities can manifest in defects such as tow wrinkling, gaps, overlaps.","In this work, we implement manufacturing constraints within the tow path optimization formulation to restrict the minimum tow turning radius and the maximum density of gaps between and overlaps of tows.","This is achieved by bounding the local value of the curl and divergence of the vector field associated with the tow orientations.","The resulting local constraints are effectively enforced in the optimization framework through the Augmented Lagrangian method.","The resulting optimization methodology is demonstrated by designing 2D and 3D structures with optimized tow orientation paths that maximize stiffness (minimize compliance) considering various levels of manufacturing restrictions.","The optimized tow paths are shown to be structurally efficient and to respect imposed manufacturing constraints.","As expected, the more geometrical complexity that can be achieved by the feedstock tow and placement technology, the higher the stiffness of the resulting optimized design."],"url":"http://arxiv.org/abs/2404.16650v1","category":"cs.CE"}
{"created":"2024-04-25 14:41:01","title":"Kalman-based approaches for online estimation of bioreactor dynamics from fluorescent reporter measurements","abstract":"We address online estimation of microbial growth dynamics in bioreactors from measurements of a fluorescent reporter protein synthesized along with microbial growth. We consider an extended version of standard growth models that accounts for the dynamics of reporter synthesis. We develop state estimation from sampled, noisy measurements in the cases of known and unknown growth rate functions. Leveraging conservation laws and regularized estimation techniques, we reduce these nonlinear estimation problems to linear time-varying ones, and solve them via Kalman filtering. We establish convergence results in absence of noise and show performance on noisy data in simulation.","sentences":["We address online estimation of microbial growth dynamics in bioreactors from measurements of a fluorescent reporter protein synthesized along with microbial growth.","We consider an extended version of standard growth models that accounts for the dynamics of reporter synthesis.","We develop state estimation from sampled, noisy measurements in the cases of known and unknown growth rate functions.","Leveraging conservation laws and regularized estimation techniques, we reduce these nonlinear estimation problems to linear time-varying ones, and solve them via Kalman filtering.","We establish convergence results in absence of noise and show performance on noisy data in simulation."],"url":"http://arxiv.org/abs/2404.16649v1","category":"math.OC"}
{"created":"2024-04-25 14:35:16","title":"Improving TAS Adaptability with a Variable Temperature Threshold","abstract":"Thermal-Aware Scheduling (TAS) provides methods to manage the thermal dissipation of a computing chip during task execution. These methods aim to avoid issues such as accelerated aging of the device, premature failure and degraded chip performance. In this work, we implement a new TAS algorithm, VTF-TAS, which makes use of a variable temperature threshold to control task execution and thermal dissipation. To enable adequate execution of the tasks to reach their deadlines, this threshold is managed based on the theory of fluid scheduling. Using an evaluation methodology as described in POD-TAS, we evaluate VTF-TAS using a set of 4 benchmarks from the COMBS benchmark suite to examine its ability to minimize chip temperature throughout schedule execution. Through our evaluation, we demonstrate that this new algorithm is able to adaptively manage the temperature threshold such that the peak temperature during schedule execution is lower than POD-TAS, with no requirement for an expensive search procedure to obtain an optimal threshold for scheduling.","sentences":["Thermal-Aware Scheduling (TAS) provides methods to manage the thermal dissipation of a computing chip during task execution.","These methods aim to avoid issues such as accelerated aging of the device, premature failure and degraded chip performance.","In this work, we implement a new TAS algorithm, VTF-TAS, which makes use of a variable temperature threshold to control task execution and thermal dissipation.","To enable adequate execution of the tasks to reach their deadlines, this threshold is managed based on the theory of fluid scheduling.","Using an evaluation methodology as described in POD-TAS, we evaluate VTF-TAS using a set of 4 benchmarks from the COMBS benchmark suite to examine its ability to minimize chip temperature throughout schedule execution.","Through our evaluation, we demonstrate that this new algorithm is able to adaptively manage the temperature threshold such that the peak temperature during schedule execution is lower than POD-TAS, with no requirement for an expensive search procedure to obtain an optimal threshold for scheduling."],"url":"http://arxiv.org/abs/2404.16646v1","category":"eess.SY"}
{"created":"2024-04-25 14:34:10","title":"Explanations in Everyday Software Systems: Towards a Taxonomy for Explainability Needs","abstract":"Modern software systems are becoming increasingly complex and opaque. The integration of explanations within software has shown the potential to address this opacity and can make the system more understandable to end-users. As a result, explainability has gained much traction as a non-functional requirement of complex systems. Understanding what type of system requires what types of explanations is necessary to facilitate the inclusion of explainability in early software design processes. In order to specify explainability requirements, an explainability taxonomy that applies to a variety of different software types is needed. In this paper, we present the results of an online survey with 84 participants. We asked the participants to state their questions and confusions concerning their three most recently used software systems and elicited both explicit and implicit explainability needs from their statements. These needs were coded by three researchers. In total, we identified and classified 315 explainability needs from the survey answers. Drawing from a large pool of explainability needs and our coding procedure, we present two major contributions of this work: 1) a taxonomy for explainability needs in everyday software systems and 2) an overview of how the need for explanations differs between different types of software systems.","sentences":["Modern software systems are becoming increasingly complex and opaque.","The integration of explanations within software has shown the potential to address this opacity and can make the system more understandable to end-users.","As a result, explainability has gained much traction as a non-functional requirement of complex systems.","Understanding what type of system requires what types of explanations is necessary to facilitate the inclusion of explainability in early software design processes.","In order to specify explainability requirements, an explainability taxonomy that applies to a variety of different software types is needed.","In this paper, we present the results of an online survey with 84 participants.","We asked the participants to state their questions and confusions concerning their three most recently used software systems and elicited both explicit and implicit explainability needs from their statements.","These needs were coded by three researchers.","In total, we identified and classified 315 explainability needs from the survey answers.","Drawing from a large pool of explainability needs and our coding procedure, we present two major contributions of this work: 1) a taxonomy for explainability needs in everyday software systems and 2) an overview of how the need for explanations differs between different types of software systems."],"url":"http://arxiv.org/abs/2404.16644v1","category":"cs.SE"}
{"created":"2024-04-25 14:22:59","title":"Inverse scattering for repulsive potential and strong singular interactions","abstract":"In a previous work of 2014 on a quantum system governed by the repulsive Hamiltonian, the author proved uniqueness for short-range interactions described by a scattering operator consisting of regular and singular parts. In this paper, the singular part is assumed to have much stronger singularities and the same uniqueness theorem is proved. By applying the time-dependent method invented by Enss and Weder in 1995, the high-velocity limit for a wider class of the scattering operator with stronger singularities also uniquely determines uniquely the interactions of a multi-dimensional system.","sentences":["In a previous work of 2014 on a quantum system governed by the repulsive Hamiltonian, the author proved uniqueness for short-range interactions described by a scattering operator consisting of regular and singular parts.","In this paper, the singular part is assumed to have much stronger singularities and the same uniqueness theorem is proved.","By applying the time-dependent method invented by Enss and Weder in 1995, the high-velocity limit for a wider class of the scattering operator with stronger singularities also uniquely determines uniquely the interactions of a multi-dimensional system."],"url":"http://arxiv.org/abs/2404.16634v1","category":"math-ph"}
{"created":"2024-04-25 14:21:15","title":"Introducing Systems Thinking as a Framework for Teaching and Assessing Threat Modeling Competency","abstract":"Computing systems face diverse and substantial cybersecurity threats. To mitigate these cybersecurity threats, software engineers need to be competent in the skill of threat modeling. In industry and academia, there are many frameworks for teaching threat modeling, but our analysis of these frameworks suggests that (1) these approaches tend to be focused on component-level analysis rather than educating students to reason holistically about a system's cybersecurity, and (2) there is no rubric for assessing a student's threat modeling competency. To address these concerns, we propose using systems thinking in conjunction with popular and industry-standard threat modeling frameworks like STRIDE for teaching and assessing threat modeling competency. Prior studies suggest a holistic approach, like systems thinking, can help understand and mitigate cybersecurity threats. Thus, we developed and piloted two novel rubrics - one for assessing STRIDE threat modeling performance and the other for assessing systems thinking performance while conducting STRIDE.   To conduct this study, we piloted the two rubrics mentioned above to assess threat model artifacts of students enrolled in an upper-level software engineering course at Purdue University in Fall 2021, Spring 2023, and Fall 2023. Students who had both systems thinking and STRIDE instruction identified and attempted to mitigate component-level as well as systems-level threats. Students with only STRIDE instruction tended to focus on identifying and mitigating component-level threats and discounted system-level threats. We contribute to engineering education by: (1) describing a new rubric for assessing threat modeling based on systems thinking; (2) identifying trends and blindspots in students' threat modeling approach; and (3) envisioning the benefits of integrating systems thinking in threat modeling teaching and assessment.","sentences":["Computing systems face diverse and substantial cybersecurity threats.","To mitigate these cybersecurity threats, software engineers need to be competent in the skill of threat modeling.","In industry and academia, there are many frameworks for teaching threat modeling, but our analysis of these frameworks suggests that (1) these approaches tend to be focused on component-level analysis rather than educating students to reason holistically about a system's cybersecurity, and (2) there is no rubric for assessing a student's threat modeling competency.","To address these concerns, we propose using systems thinking in conjunction with popular and industry-standard threat modeling frameworks like STRIDE for teaching and assessing threat modeling competency.","Prior studies suggest a holistic approach, like systems thinking, can help understand and mitigate cybersecurity threats.","Thus, we developed and piloted two novel rubrics - one for assessing STRIDE threat modeling performance and the other for assessing systems thinking performance while conducting STRIDE.   ","To conduct this study, we piloted the two rubrics mentioned above to assess threat model artifacts of students enrolled in an upper-level software engineering course at Purdue University in Fall 2021, Spring 2023, and Fall 2023.","Students who had both systems thinking and STRIDE instruction identified and attempted to mitigate component-level as well as systems-level threats.","Students with only STRIDE instruction tended to focus on identifying and mitigating component-level threats and discounted system-level threats.","We contribute to engineering education by: (1) describing a new rubric for assessing threat modeling based on systems thinking; (2) identifying trends and blindspots in students' threat modeling approach; and (3) envisioning the benefits of integrating systems thinking in threat modeling teaching and assessment."],"url":"http://arxiv.org/abs/2404.16632v1","category":"cs.CR"}
{"created":"2024-04-25 14:16:36","title":"Implementing and Optimizing the Scaled Dot-Product Attention on Streaming Dataflow","abstract":"Transformer models serve as the backbone of many state-ofthe-art language models, and most use the scaled dot-product attention (SDPA) mechanism to capture relationships between tokens. However, the straightforward implementation of SDPA has quadratic compute and memory complexity with respect to the sequence length. On processor architectures such as GPUs and TPUs, there is a robust body of prior work. However, little work has been performed on non-processor architectures.In this work, we show how the architecture and execution model of Streaming Dataflow Accelerators can help tackle this challenge. We first define abstract hardware that adopts a streaming execution model, and we implement a cycle-accurate simulator of the abstract hardware using the Dataflow Abstract Machine simulation framework. Second, we implement the naive SDPA algorithm on this abstract hardware and show it requires linear (O(N)) intermediate memory. Third, we then modify the naive algorithm, taking inspiration from prior processor-oriented works, by reordering the multiplication and division operations. Finally, we map the modified algorithm to abstract hardware, and confirm that the implementation computes SDPA at full throughput while only using a constant amount (O(1)) of intermediate memory.","sentences":["Transformer models serve as the backbone of many state-ofthe-art language models, and most use the scaled dot-product attention (SDPA) mechanism to capture relationships between tokens.","However, the straightforward implementation of SDPA has quadratic compute and memory complexity with respect to the sequence length.","On processor architectures such as GPUs and TPUs, there is a robust body of prior work.","However, little work has been performed on non-processor architectures.","In this work, we show how the architecture and execution model of Streaming Dataflow Accelerators can help tackle this challenge.","We first define abstract hardware that adopts a streaming execution model, and we implement a cycle-accurate simulator of the abstract hardware using the Dataflow Abstract Machine simulation framework.","Second, we implement the naive SDPA algorithm on this abstract hardware and show it requires linear (O(N))","intermediate memory.","Third, we then modify the naive algorithm, taking inspiration from prior processor-oriented works, by reordering the multiplication and division operations.","Finally, we map the modified algorithm to abstract hardware, and confirm that the implementation computes SDPA at full throughput while only using a constant amount (O(1)) of intermediate memory."],"url":"http://arxiv.org/abs/2404.16629v1","category":"cs.AR"}
{"created":"2024-04-25 14:09:22","title":"Development of parallel programs on shared data-structures -- Revised version","abstract":"A syntax-directed formal system for the development of totally correct programs with respect to an unfair shared-state parallel while-language is proposed. The system can be understood as a compositional reformulation of the Owicki/Gries method for verification of parallel programs. Auxiliary variables are used both as a specification tool to eliminate undesirable implementations, and as a verification tool to make it possible to prove that an already finished program satisfies a particular specification. Auxiliary variables may be of any sort, and it is up to the user to define the auxiliary structure he prefers. Moreover, the auxiliary structure is only a part of the logic. This means that auxiliary variables do not have to be implemented as if they were ordinary programming variables. The system is proved sound and relatively complete with respect to an operational semantics and employed to develop three nontrivial algorithms: the Dining-Philosophers, the Bubble-Lattice-Sort and the Set-Partition algorithms. Finally, a related method for the development of (possibly nonterminating) programs with respect to four properties is described. This approach is then used to develop Dekker's algorithm.","sentences":["A syntax-directed formal system for the development of totally correct programs with respect to an unfair shared-state parallel while-language is proposed.","The system can be understood as a compositional reformulation of the Owicki/Gries method for verification of parallel programs.","Auxiliary variables are used both as a specification tool to eliminate undesirable implementations, and as a verification tool to make it possible to prove that an already finished program satisfies a particular specification.","Auxiliary variables may be of any sort, and it is up to the user to define the auxiliary structure he prefers.","Moreover, the auxiliary structure is only a part of the logic.","This means that auxiliary variables do not have to be implemented as if they were ordinary programming variables.","The system is proved sound and relatively complete with respect to an operational semantics and employed to develop three nontrivial algorithms: the Dining-Philosophers, the Bubble-Lattice-Sort and the Set-Partition algorithms.","Finally, a related method for the development of (possibly nonterminating) programs with respect to four properties is described.","This approach is then used to develop Dekker's algorithm."],"url":"http://arxiv.org/abs/2404.16624v1","category":"cs.FL"}
{"created":"2024-04-25 14:02:25","title":"The THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System for LIMMITS'24 Challenge","abstract":"This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge. To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements. For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks. In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers. The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable naturalness MOS of 3.97.","sentences":["This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge.","To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements.","For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks.","In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers.","The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable","naturalness MOS of 3.97."],"url":"http://arxiv.org/abs/2404.16619v1","category":"cs.SD"}
{"created":"2024-04-25 13:51:01","title":"Towards Symbiotic SAGIN Through Inter-operator Resource and Service Sharing: Joint Orchestration of User Association and Radio Resources","abstract":"The space-air-ground integrated network (SAGIN) is a pivotal architecture to support ubiquitous connectivity in the upcoming 6G era. Inter-operator resource and service sharing is a promising way to realize such a huge network, utilizing resources efficiently and reducing construction costs. Given the rationality of operators, the configuration of resources and services in SAGIN should focus on both the overall system performance and individual benefits of operators. Motivated by emerging symbiotic communication facilitating mutual benefits across different radio systems, we investigate the resource and service sharing in SAGIN from a symbiotic communication perspective in this paper. In particular, we consider a SAGIN consisting of a ground network operator (GNO) and a satellite network operator (SNO). Specifically, we aim to maximize the weighted sum rate (WSR) of the whole SAGIN by jointly optimizing the user association, resource allocation, and beamforming. Besides, we introduce a sharing coefficient to characterize the revenue of operators. Operators may suffer revenue loss when only focusing on maximizing the WSR. In pursuit of mutual benefits, we propose a mutual benefit constraint (MBC) to ensure that each operator obtains revenue gains. Then, we develop a centralized algorithm based on the successive convex approximation (SCA) method. Considering that the centralized algorithm is difficult to implement, we propose a distributed algorithm based on Lagrangian dual decomposition and the consensus alternating direction method of multipliers (ADMM). Finally, we provide extensive numerical simulations to demonstrate the effectiveness of the two proposed algorithms, and the distributed optimization algorithm can approach the performance of the centralized one.","sentences":["The space-air-ground integrated network (SAGIN) is a pivotal architecture to support ubiquitous connectivity in the upcoming 6G era.","Inter-operator resource and service sharing is a promising way to realize such a huge network, utilizing resources efficiently and reducing construction costs.","Given the rationality of operators, the configuration of resources and services in SAGIN should focus on both the overall system performance and individual benefits of operators.","Motivated by emerging symbiotic communication facilitating mutual benefits across different radio systems, we investigate the resource and service sharing in SAGIN from a symbiotic communication perspective in this paper.","In particular, we consider a SAGIN consisting of a ground network operator (GNO) and a satellite network operator (SNO).","Specifically, we aim to maximize the weighted sum rate (WSR) of the whole SAGIN by jointly optimizing the user association, resource allocation, and beamforming.","Besides, we introduce a sharing coefficient to characterize the revenue of operators.","Operators may suffer revenue loss when only focusing on maximizing the WSR.","In pursuit of mutual benefits, we propose a mutual benefit constraint (MBC) to ensure that each operator obtains revenue gains.","Then, we develop a centralized algorithm based on the successive convex approximation (SCA) method.","Considering that the centralized algorithm is difficult to implement, we propose a distributed algorithm based on Lagrangian dual decomposition and the consensus alternating direction method of multipliers (ADMM).","Finally, we provide extensive numerical simulations to demonstrate the effectiveness of the two proposed algorithms, and the distributed optimization algorithm can approach the performance of the centralized one."],"url":"http://arxiv.org/abs/2404.16611v1","category":"cs.IT"}
{"created":"2024-04-25 13:48:10","title":"A holographic bottom-up approach to $\u03a3$ baryons","abstract":"In this work, we discuss the description of neutral $\\Sigma$ baryons with $I(J^P)=1(1/2^+)$ and $I(J^P)=1(3/2^+)$ using two bottom-up approaches: the deformed background and the static dilaton. In both models, we consider a non-linear Regge trajectory extension motivated by the \\emph{strange} nature that $\\Sigma$ baryons have. We found that both models describe these systems with an RMS error smaller than 10 $\\%$. We also perform a configurational entropy calculation in both models to discuss hadronic stability.","sentences":["In this work, we discuss the description of neutral $\\Sigma$ baryons with $I(J^P)=1(1/2^+)$ and $I(J^P)=1(3/2^+)$ using two bottom-up approaches: the deformed background and the static dilaton.","In both models, we consider a non-linear Regge trajectory extension motivated by the \\emph{strange} nature that $\\Sigma$ baryons have.","We found that both models describe these systems with an RMS error smaller than 10 $\\%$. We also perform a configurational entropy calculation in both models to discuss hadronic stability."],"url":"http://arxiv.org/abs/2404.16608v1","category":"hep-ph"}
{"created":"2024-04-25 13:26:51","title":"Electronic structure, self-doping, and superconducting instability in the alternating single-layer trilayer stacking nickelates La$_3$Ni$_2$O$_7$","abstract":"Motivated by the recently proposed alternating single-layer trilayer stacking structure for the nickelate La$_3$Ni$_2$O$_7$, we comprehensively study this system using {\\it ab initio} and random-phase approximation techniques. Our analysis unveils similarities between this novel La$_3$Ni$_2$O$_7$ structure and other Ruddlesden-Popper nickelate superconductors, such as a similar charge-transfer gap value and orbital-selective behavior of the $e_g$ orbitals. However, different from other Ruddlesden-Popper nickelate superconductors, we do not observe any obvious reconstruction of the Fermi surface from ambient conditions (Cmmm phase) to high pressures (P4/mmm phase). Pressure primarily increases the bandwidths of the Ni $e_g$ bands, suggesting an enhancement of the itinerant properties of those $e_g$ states. Furthermore, the $d_{3z^2-r^2}$ orbital also has a layer-selective behavior because the antibonding-bonding-nonbonding splitting can only be obtained in the trilayer. In addition, we observe a \"self-doping\" effect from the trilayer to the single-layer sublattices and this effect will be enhanced by overall electron doping. Moreover, we find a leading $d_{x^2-y^2}$-wave pairing state that is restricted to the single-layer. Because the effective coupling between the single layers is very weak -- due to the non-superconducting trilayer in between -- this suggests that the superconducting transition temperature $T_c$ in this structure should be much lower than in the bilayer structure.","sentences":["Motivated by the recently proposed alternating single-layer trilayer stacking structure for the nickelate La$_3$Ni$_2$O$_7$, we comprehensively study this system using {\\it ab initio} and random-phase approximation techniques.","Our analysis unveils similarities between this novel La$_3$Ni$_2$O$_7$ structure and other Ruddlesden-Popper nickelate superconductors, such as a similar charge-transfer gap value and orbital-selective behavior of the $e_g$ orbitals.","However, different from other Ruddlesden-Popper nickelate superconductors, we do not observe any obvious reconstruction of the Fermi surface from ambient conditions (Cmmm phase) to high pressures (P4/mmm phase).","Pressure primarily increases the bandwidths of the Ni $e_g$ bands, suggesting an enhancement of the itinerant properties of those $e_g$ states.","Furthermore, the $d_{3z^2-r^2}$ orbital also has a layer-selective behavior because the antibonding-bonding-nonbonding splitting can only be obtained in the trilayer.","In addition, we observe a \"self-doping\" effect from the trilayer to the single-layer sublattices and this effect will be enhanced by overall electron doping.","Moreover, we find a leading $d_{x^2-y^2}$-wave pairing state that is restricted to the single-layer.","Because the effective coupling between the single layers is very weak -- due to the non-superconducting trilayer in between -- this suggests that the superconducting transition temperature $T_c$ in this structure should be much lower than in the bilayer structure."],"url":"http://arxiv.org/abs/2404.16600v1","category":"cond-mat.supr-con"}
{"created":"2024-04-25 13:25:06","title":"Uncovering Data Across Continua: An Introduction to Functional Data Analysis","abstract":"In a world increasingly awash with data, the need to extract meaningful insights from data has never been more crucial. Functional Data Analysis (FDA) goes beyond traditional data points, treating data as dynamic, continuous functions, capturing ever-changing phenomena nuances. This article introduces FDA, merging statistics with real-world complexity, ideal for those with mathematical skills but no FDA background.","sentences":["In a world increasingly awash with data, the need to extract meaningful insights from data has never been more crucial.","Functional Data Analysis (FDA) goes beyond traditional data points, treating data as dynamic, continuous functions, capturing ever-changing phenomena nuances.","This article introduces FDA, merging statistics with real-world complexity, ideal for those with mathematical skills but no FDA background."],"url":"http://arxiv.org/abs/2404.16598v1","category":"math.ST"}
{"created":"2024-04-25 13:14:48","title":"Uninterrupted Maximum Flow on Signalized Traffic Networks","abstract":"This paper describes a traffic signal control procedure that allows motorists who travel at a recommended speed on suburban arterial two-way roads with a common cycle-time to make every traffic signal. A road-to-traveler-feedback-device advises motorists how fast they should travel to do this. Signalized arterial roads where vehicles that travel at the recommended speed make every traffic signal are termed Ride-the-Green-Wave (RGW) roads. Left-turn-arounds allow vehicles to turn left from one two-way RGW-road to an intersecting/orthogonal two-way RGW-road while allowing maximum flow on the intersecting RGW-roads. In addition to introducing novel traffic signal control strategies, the methods presented in this paper have implications for: road network design, public transport control, connected and automated vehicles and environmental impacts.","sentences":["This paper describes a traffic signal control procedure that allows motorists who travel at a recommended speed on suburban arterial two-way roads with a common cycle-time to make every traffic signal.","A road-to-traveler-feedback-device advises motorists how fast they should travel to do this.","Signalized arterial roads where vehicles that travel at the recommended speed make every traffic signal are termed Ride-the-Green-Wave (RGW) roads.","Left-turn-arounds allow vehicles to turn left from one two-way RGW-road to an intersecting/orthogonal two-way RGW-road while allowing maximum flow on the intersecting RGW-roads.","In addition to introducing novel traffic signal control strategies, the methods presented in this paper have implications for: road network design, public transport control, connected and automated vehicles and environmental impacts."],"url":"http://arxiv.org/abs/2404.16592v1","category":"eess.SY"}
{"created":"2024-04-25 13:13:30","title":"SB-ETAS: using simulation based inference for scalable, likelihood-free inference for the ETAS model of earthquake occurrences","abstract":"Performing Bayesian inference for the Epidemic-Type Aftershock Sequence (ETAS) model of earthquakes typically requires MCMC sampling using the likelihood function or estimating the latent branching structure. These tasks have computational complexity $O(n^2)$ with the number of earthquakes and therefore do not scale well with new enhanced catalogs, which can now contain an order of $10^6$ events. On the other hand, simulation from the ETAS model can be done more quickly $O(n \\log n )$. We present SB-ETAS: simulation-based inference for the ETAS model. This is an approximate Bayesian method which uses Sequential Neural Posterior Estimation (SNPE), a machine learning based algorithm for learning posterior distributions from simulations. SB-ETAS can successfully approximate ETAS posterior distributions on shorter catalogues where it is computationally feasible to compare with MCMC sampling. Furthermore, the scaling of SB-ETAS makes it feasible to fit to very large earthquake catalogs, such as one for Southern California dating back to 1932. SB-ETAS can find Bayesian estimates of ETAS parameters for this catalog in less than 10 hours on a standard laptop, which would have taken over 2 weeks using MCMC. Looking beyond the standard ETAS model, this simulation based framework would allow earthquake modellers to define and infer parameters for much more complex models that have intractable likelihood functions.","sentences":["Performing Bayesian inference for the Epidemic-Type Aftershock Sequence (ETAS) model of earthquakes typically requires MCMC sampling using the likelihood function or estimating the latent branching structure.","These tasks have computational complexity $O(n^2)$ with the number of earthquakes and therefore do not scale well with new enhanced catalogs, which can now contain an order of $10^6$ events.","On the other hand, simulation from the ETAS model can be done more quickly $O(n \\log n )$.","We present SB-ETAS: simulation-based inference for the ETAS model.","This is an approximate Bayesian method which uses Sequential Neural Posterior Estimation (SNPE), a machine learning based algorithm for learning posterior distributions from simulations.","SB-ETAS can successfully approximate ETAS posterior distributions on shorter catalogues where it is computationally feasible to compare with MCMC sampling.","Furthermore, the scaling of SB-ETAS makes it feasible to fit to very large earthquake catalogs, such as one for Southern California dating back to 1932.","SB-ETAS can find Bayesian estimates of ETAS parameters for this catalog in less than 10 hours on a standard laptop, which would have taken over 2 weeks using MCMC.","Looking beyond the standard ETAS model, this simulation based framework would allow earthquake modellers to define and infer parameters for much more complex models that have intractable likelihood functions."],"url":"http://arxiv.org/abs/2404.16590v1","category":"stat.AP"}
{"created":"2024-04-25 13:13:21","title":"Proving Behavioural Apartness","abstract":"Bisimilarity is a central notion for coalgebras. In recent work, Geuvers and Jacobs suggest to focus on apartness, which they define by dualising coalgebraic bisimulations. This yields the possibility of finite proofs of distinguishability for a wide variety of state-based systems.   We propose behavioural apartness, defined by dualising behavioural equivalence rather than bisimulations. A motivating example is the subdistribution functor, where the proof system based on bisimilarity requires an infinite quantification over couplings, whereas behavioural apartness instantiates to a finite rule. In addition, we provide optimised proof rules for behavioural apartness and show their use in several examples.","sentences":["Bisimilarity is a central notion for coalgebras.","In recent work, Geuvers and Jacobs suggest to focus on apartness, which they define by dualising coalgebraic bisimulations.","This yields the possibility of finite proofs of distinguishability for a wide variety of state-based systems.   ","We propose behavioural apartness, defined by dualising behavioural equivalence rather than bisimulations.","A motivating example is the subdistribution functor, where the proof system based on bisimilarity requires an infinite quantification over couplings, whereas behavioural apartness instantiates to a finite rule.","In addition, we provide optimised proof rules for behavioural apartness and show their use in several examples."],"url":"http://arxiv.org/abs/2404.16588v1","category":"cs.LO"}
{"created":"2024-04-25 13:08:25","title":"Reduced density matrix formulation of quantum linear response","abstract":"The prediction of spectral properties via linear response (LR) theory is an important tool in quantum chemistry for understanding photo-induced processes in molecular systems. With the advances of quantum computing, we recently adapted this method for near-term quantum hardware using a truncated active space approximation with orbital rotation, named quantum linear response (qLR). In an effort to reduce the classic cost of this hybrid approach, we here derive and implement a reduced density matrix (RDM) driven approach of qLR. This allows for the calculation of spectral properties of moderately sized molecules with much larger basis sets than so far possible. We report qLR results for benzene and $R$-methyloxirane with a cc-pVTZ basis set and study the effect of shot noise on the valence and oxygen K-edge absorption spectra of H$_2$O in the cc-pVTZ basis.","sentences":["The prediction of spectral properties via linear response (LR) theory is an important tool in quantum chemistry for understanding photo-induced processes in molecular systems.","With the advances of quantum computing, we recently adapted this method for near-term quantum hardware using a truncated active space approximation with orbital rotation, named quantum linear response (qLR).","In an effort to reduce the classic cost of this hybrid approach, we here derive and implement a reduced density matrix (RDM) driven approach of qLR.","This allows for the calculation of spectral properties of moderately sized molecules with much larger basis sets than so far possible.","We report qLR results for benzene and $R$-methyloxirane with a cc-pVTZ basis set and study the effect of shot noise on the valence and oxygen K-edge absorption spectra of H$_2$O in the cc-pVTZ basis."],"url":"http://arxiv.org/abs/2404.16586v1","category":"physics.chem-ph"}
{"created":"2024-04-25 12:56:23","title":"Directional intermodular coupling enriches functional complexity in biological neuronal networks","abstract":"Hierarchically modular organization is a canonical network topology that is evolutionarily conserved in the nervous systems of animals. Within the network, neurons form directional connections defined by the growth of their axonal terminals. However, this topology is dissimilar to the network formed by dissociated neurons in culture because they form randomly connected networks on homogeneous substrates. In this study, we fabricated microfluidic devices to reconstitute hierarchically modular neuronal networks in culture (in vitro) and investigated how non-random structures, such as directional connectivity between modules, affect global network dynamics. Embedding directional connections in a pseudo-feedforward manner suppressed excessive synchrony in cultured neuronal networks and enhanced the integration-segregation balance. Modeling the behavior of biological neuronal networks using spiking neural networks (SNNs) further revealed that modularity and directionality cooperate to shape such network dynamics. Finally, we demonstrate that for a given network topology, the statistics of network dynamics, such as global network activation, correlation coefficient, and functional complexity, can be analytically predicted based on eigendecomposition of the transition matrix in the state-transition model. Hence, the integration of bioengineering and cell culture technologies enables us not only to reconstitute complex network circuitry in the nervous system but also to understand the structure-function relationships in biological neuronal networks by bridging theoretical modeling with in vitro experiments.","sentences":["Hierarchically modular organization is a canonical network topology that is evolutionarily conserved in the nervous systems of animals.","Within the network, neurons form directional connections defined by the growth of their axonal terminals.","However, this topology is dissimilar to the network formed by dissociated neurons in culture because they form randomly connected networks on homogeneous substrates.","In this study, we fabricated microfluidic devices to reconstitute hierarchically modular neuronal networks in culture (in vitro) and investigated how non-random structures, such as directional connectivity between modules, affect global network dynamics.","Embedding directional connections in a pseudo-feedforward manner suppressed excessive synchrony in cultured neuronal networks and enhanced the integration-segregation balance.","Modeling the behavior of biological neuronal networks using spiking neural networks (SNNs) further revealed that modularity and directionality cooperate to shape such network dynamics.","Finally, we demonstrate that for a given network topology, the statistics of network dynamics, such as global network activation, correlation coefficient, and functional complexity, can be analytically predicted based on eigendecomposition of the transition matrix in the state-transition model.","Hence, the integration of bioengineering and cell culture technologies enables us not only to reconstitute complex network circuitry in the nervous system but also to understand the structure-function relationships in biological neuronal networks by bridging theoretical modeling with in vitro experiments."],"url":"http://arxiv.org/abs/2404.16582v1","category":"q-bio.NC"}
{"created":"2024-04-25 12:46:22","title":"Stokes-Brinkman-Darcy models for fluid-porous systems: derivation, analysis and validation","abstract":"Flow interaction between a plain-fluid region in contact with a porous layer attracted significant attention from modelling and analysis sides due to numerous applications in biology, environment and industry. In the most widely used coupled model, fluid flow is described by the Stokes equations in the free-flow domain and Darcy's law in the porous medium, and complemented by the appropriate interface conditions. However, traditional coupling concepts are restricted, with a few exceptions, to one-dimensional flows parallel to the fluid-porous interface. In this work, we use an alternative approach to model interaction between the plain-fluid domain and porous medium by considering a transition zone, and propose the full- and hybrid-dimensional Stokes-Brinkman-Darcy models. In the first case, the equi-dimensional Brinkman equations are considered in the transition region, and the appropriate interface conditions are set on the top and bottom of the transition zone. In the latter case, we perform a dimensional model reduction by averaging the Brinkman equations in the normal direction and using the proposed transmission conditions. The well-posedness of both coupled problems is proved, and some numerical simulations are carried out in order to validate the concepts.","sentences":["Flow interaction between a plain-fluid region in contact with a porous layer attracted significant attention from modelling and analysis sides due to numerous applications in biology, environment and industry.","In the most widely used coupled model, fluid flow is described by the Stokes equations in the free-flow domain and Darcy's law in the porous medium, and complemented by the appropriate interface conditions.","However, traditional coupling concepts are restricted, with a few exceptions, to one-dimensional flows parallel to the fluid-porous interface.","In this work, we use an alternative approach to model interaction between the plain-fluid domain and porous medium by considering a transition zone, and propose the full- and hybrid-dimensional Stokes-Brinkman-Darcy models.","In the first case, the equi-dimensional Brinkman equations are considered in the transition region, and the appropriate interface conditions are set on the top and bottom of the transition zone.","In the latter case, we perform a dimensional model reduction by averaging the Brinkman equations in the normal direction and using the proposed transmission conditions.","The well-posedness of both coupled problems is proved, and some numerical simulations are carried out in order to validate the concepts."],"url":"http://arxiv.org/abs/2404.16577v1","category":"math.NA"}
{"created":"2024-04-25 12:32:01","title":"Nucleation transitions in polycontextural networks towards consensus","abstract":"Recently, we proposed polycontextural networks as a model of evolving systems of interacting beliefs. Here, we present an analysis of the phase transition as well as the scaling properties. The model contains interacting agents that strive for consensus, each with only subjective perception. Depending on a parameter that governs how responsive the agents are to changing their belief systems the model exhibits a phase transition that mediates between an active phase where the agents constantly change their beliefs and a frozen phase, where almost no changes appear. We observe the build-up of convention-aligned clusters only in the intermediate regime of diverging susceptibility. Here, we analyze in detail the behavior of polycontextural networks close to this transition. We provide an analytical estimate of the critical point and show that the scaling properties and the space-time structure of these clusters show self-similar behavior. Our results not only contribute to a better understanding of the emergence of consensus in systems of distributed beliefs but also show that polycontextural networks are models, motivated by social systems, where susceptibility -- the sensitivity to change own beliefs -- drives the growth of consensus clusters.","sentences":["Recently, we proposed polycontextural networks as a model of evolving systems of interacting beliefs.","Here, we present an analysis of the phase transition as well as the scaling properties.","The model contains interacting agents that strive for consensus, each with only subjective perception.","Depending on a parameter that governs how responsive the agents are to changing their belief systems the model exhibits a phase transition that mediates between an active phase where the agents constantly change their beliefs and a frozen phase, where almost no changes appear.","We observe the build-up of convention-aligned clusters only in the intermediate regime of diverging susceptibility.","Here, we analyze in detail the behavior of polycontextural networks close to this transition.","We provide an analytical estimate of the critical point and show that the scaling properties and the space-time structure of these clusters show self-similar behavior.","Our results not only contribute to a better understanding of the emergence of consensus in systems of distributed beliefs but also show that polycontextural networks are models, motivated by social systems, where susceptibility -- the sensitivity to change own beliefs -- drives the growth of consensus clusters."],"url":"http://arxiv.org/abs/2404.16569v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 12:29:04","title":"J-PLUS: Bayesian object classification with a strum of BANNJOS","abstract":"With its 12 optical filters, the Javalambre-Photometric Local Universe Survey (J-PLUS) provides an unprecedented multicolor view of the local Universe. The third data release (DR3) covers 3,192 deg$^2$ and contains 47.4 million objects. However, the classification algorithms currently implemented in its pipeline are deterministic and based solely on the sources morphology. Our goal is classify the sources identified in the J-PLUS DR3 images into stars, quasi-stellar objects (QSOs), and galaxies. For this task, we present BANNJOS, a machine learning pipeline that uses Bayesian neural networks to provide the probability distribution function (PDF) of the classification. BANNJOS is trained on photometric, astrometric, and morphological data from J-PLUS DR3, Gaia DR3, and CatWISE2020, using over 1.2 million objects with spectroscopic classification from SDSS DR18, LAMOST DR9, DESI EDR, and Gaia DR3. Results are validated using $1.4 10^5$ objects and cross-checked against theoretical model predictions. BANNJOS outperforms all previous classifiers in terms of accuracy, precision, and completeness across the entire magnitude range. It delivers over 95% accuracy for objects brighter than $r = 21.5$ mag, and ~90% accuracy for those up to $r = 22$ mag, where J-PLUS completeness is < 25%. BANNJOS is also the first object classifier to provide the full probability distribution function (PDF) of the classification, enabling precise object selection for high purity or completeness, and for identifying objects with complex features, like active galactic nuclei with resolved host galaxies. BANNJOS has effectively classified J-PLUS sources into around 20 million galaxies, 1 million QSOs, and 26 million stars, with full PDFs for each, which allow for later refinement of the sample. The upcoming J-PAS survey, with its 56 color bands, will further enhance BANNJOS's ability to detail each source's nature.","sentences":["With its 12 optical filters, the Javalambre-Photometric Local Universe Survey (J-PLUS) provides an unprecedented multicolor view of the local Universe.","The third data release (DR3) covers 3,192 deg$^2$ and contains 47.4 million objects.","However, the classification algorithms currently implemented in its pipeline are deterministic and based solely on the sources morphology.","Our goal is classify the sources identified in the J-PLUS DR3 images into stars, quasi-stellar objects (QSOs), and galaxies.","For this task, we present BANNJOS, a machine learning pipeline that uses Bayesian neural networks to provide the probability distribution function (PDF) of the classification.","BANNJOS is trained on photometric, astrometric, and morphological data from J-PLUS DR3, Gaia DR3, and CatWISE2020, using over 1.2 million objects with spectroscopic classification from SDSS DR18, LAMOST DR9, DESI EDR, and Gaia DR3.","Results are validated using $1.4 10^5$ objects and cross-checked against theoretical model predictions.","BANNJOS outperforms all previous classifiers in terms of accuracy, precision, and completeness across the entire magnitude range.","It delivers over 95% accuracy for objects brighter than $r = 21.5$ mag, and ~90% accuracy for those up to $r = 22$ mag, where J-PLUS completeness is < 25%.","BANNJOS is also the first object classifier to provide the full probability distribution function (PDF) of the classification, enabling precise object selection for high purity or completeness, and for identifying objects with complex features, like active galactic nuclei with resolved host galaxies.","BANNJOS has effectively classified J-PLUS sources into around 20 million galaxies, 1 million QSOs, and 26 million stars, with full PDFs for each, which allow for later refinement of the sample.","The upcoming J-PAS survey, with its 56 color bands, will further enhance BANNJOS's ability to detail each source's nature."],"url":"http://arxiv.org/abs/2404.16567v1","category":"astro-ph.GA"}
{"created":"2024-04-25 12:16:22","title":"Conductivity of lattice bosons at high temperatures","abstract":"Quantum simulations are quickly becoming an indispensable tool for studying particle transport in correlated lattice models. One of the central topics in the study of transport is the bad-metal behavior, characterized by the direct current (dc) resistivity linear in temperature. In the fermionic Hubbard model, optical conductivity has been studied extensively, and a recent optical lattice experiment has demonstrated bad metal behavior in qualitative agreement with theory. Far less is known about transport in the bosonic Hubbard model. We investigate the conductivity in the Bose-Hubbard model, and focus on the regime of strong interactions and high-temperatures. We use numerically exact calculations for small lattice sizes. At weak tunneling, we find multiple peaks in the optical conductivity that stem from the Hubbard bands present in the many-body spectrum. This feature slowly washes out as the tunneling rate gets stronger. At high temperature, we identify a regime of $T$-linear resistivity, as expected. When the interactions are very strong, the leading inverse-temperature coefficient in conductivity is proportional to the tunneling amplitude. As the tunneling becomes stronger, this dependence takes quadratic form. At very strong coupling and half filling, we identify a separate linear resistivity regime at lower temperature, corresponding to the hard-core boson regime. Additionally, we unexpectedly observe that at half filling, in a big part of the phase diagram, conductivity is an increasing function of the coupling constant before it saturates at the hard-core-boson result. We explain this feature based on the analysis of the many-body energy spectrum and the contributions to conductivity of individual eigenstates of the system.","sentences":["Quantum simulations are quickly becoming an indispensable tool for studying particle transport in correlated lattice models.","One of the central topics in the study of transport is the bad-metal behavior, characterized by the direct current (dc) resistivity linear in temperature.","In the fermionic Hubbard model, optical conductivity has been studied extensively, and a recent optical lattice experiment has demonstrated bad metal behavior in qualitative agreement with theory.","Far less is known about transport in the bosonic Hubbard model.","We investigate the conductivity in the Bose-Hubbard model, and focus on the regime of strong interactions and high-temperatures.","We use numerically exact calculations for small lattice sizes.","At weak tunneling, we find multiple peaks in the optical conductivity that stem from the Hubbard bands present in the many-body spectrum.","This feature slowly washes out as the tunneling rate gets stronger.","At high temperature, we identify a regime of $T$-linear resistivity, as expected.","When the interactions are very strong, the leading inverse-temperature coefficient in conductivity is proportional to the tunneling amplitude.","As the tunneling becomes stronger, this dependence takes quadratic form.","At very strong coupling and half filling, we identify a separate linear resistivity regime at lower temperature, corresponding to the hard-core boson regime.","Additionally, we unexpectedly observe that at half filling, in a big part of the phase diagram, conductivity is an increasing function of the coupling constant before it saturates at the hard-core-boson result.","We explain this feature based on the analysis of the many-body energy spectrum and the contributions to conductivity of individual eigenstates of the system."],"url":"http://arxiv.org/abs/2404.16559v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-25 12:04:31","title":"Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System","abstract":"In this paper, we propose a novel approach to address the problem of camera and radar sensor fusion for 3D object detection in autonomous vehicle perception systems. Our approach builds on recent advances in deep learning and leverages the strengths of both sensors to improve object detection performance. Precisely, we extract 2D features from camera images using a state-of-the-art deep learning architecture and then apply a novel Cross-Domain Spatial Matching (CDSM) transformation method to convert these features into 3D space. We then fuse them with extracted radar data using a complementary fusion strategy to produce a final 3D object representation. To demonstrate the effectiveness of our approach, we evaluate it on the NuScenes dataset. We compare our approach to both single-sensor performance and current state-of-the-art fusion methods. Our results show that the proposed approach achieves superior performance over single-sensor solutions and could directly compete with other top-level fusion methods.","sentences":["In this paper, we propose a novel approach to address the problem of camera and radar sensor fusion for 3D object detection in autonomous vehicle perception systems.","Our approach builds on recent advances in deep learning and leverages the strengths of both sensors to improve object detection performance.","Precisely, we extract 2D features from camera images using a state-of-the-art deep learning architecture and then apply a novel Cross-Domain Spatial Matching (CDSM) transformation method to convert these features into 3D space.","We then fuse them with extracted radar data using a complementary fusion strategy to produce a final 3D object representation.","To demonstrate the effectiveness of our approach, we evaluate it on the NuScenes dataset.","We compare our approach to both single-sensor performance and current state-of-the-art fusion methods.","Our results show that the proposed approach achieves superior performance over single-sensor solutions and could directly compete with other top-level fusion methods."],"url":"http://arxiv.org/abs/2404.16548v1","category":"cs.CV"}
{"created":"2024-04-25 12:00:27","title":"On CR maps between hyperquadrics and Winkelmann hypersurfaces","abstract":"In this paper, we study CR maps between hyperquadrics and Winkelmann hypersurfaces. Based on a previous study on the CR Ahlfors derivative of Lamel-Son and a recent result of Huang-Lu-Tang-Xiao on CR maps between hyperquadrics, we prove that a transversal CR map from a hyperquadric into a hyperquadric or a Winkelmann hypersurface extends to a local holomorphic isometric embedding with respect to certain K\\\"ahler metrics if and only if the Hermitian part of its CR Ahlfors derivative vanishes on an open set of the source. Our proof is based on relating the geometric rank of a CR map into a hyperquadric and its CR Ahlfors derivative.","sentences":["In this paper, we study CR maps between hyperquadrics and Winkelmann hypersurfaces.","Based on a previous study on the CR Ahlfors derivative of Lamel-Son and a recent result of Huang-Lu-Tang-Xiao on CR maps between hyperquadrics, we prove that a transversal CR map from a hyperquadric into a hyperquadric or a Winkelmann hypersurface extends to a local holomorphic isometric embedding with respect to certain K\\\"ahler metrics if and only if the Hermitian part of its CR Ahlfors derivative vanishes on an open set of the source.","Our proof is based on relating the geometric rank of a CR map into a hyperquadric and its CR Ahlfors derivative."],"url":"http://arxiv.org/abs/2404.16543v1","category":"math.CV"}
{"created":"2024-04-25 11:47:28","title":"AMEP: The Active Matter Evaluation Package for Python","abstract":"The Active Matter Evaluation Package (AMEP) is a Python library for analyzing simulation data of particle-based and continuum simulations. It provides a powerful and simple interface for handling large data sets and for calculating and visualizing a broad variety of observables that are relevant to active matter systems. Examples range from the mean-square displacement and the structure factor to cluster-size distributions, binder cumulants, and growth exponents. AMEP is written in pure Python and is based on powerful libraries such as NumPy, SciPy, Matplotlib, and scikit-image. Computationally expensive methods are parallelized and optimized to run efficiently on workstations, laptops, and high-performance computing architectures, and an HDF5-based data format is used in the backend to store and handle simulation data as well as analysis results. AMEP provides the first comprehensive framework for analyzing simulation results of both particle-based and continuum simulations (as well as experimental data) of active matter systems. In particular, AMEP also allows it to analyze simulations that combine particle-based and continuum techniques such as used to study the motion of bacteria in chemical fields or for modeling particle motion in a flow field. AMEP is available at https://amepproject.de and can be installed via conda and pip.","sentences":["The Active Matter Evaluation Package (AMEP) is a Python library for analyzing simulation data of particle-based and continuum simulations.","It provides a powerful and simple interface for handling large data sets and for calculating and visualizing a broad variety of observables that are relevant to active matter systems.","Examples range from the mean-square displacement and the structure factor to cluster-size distributions, binder cumulants, and growth exponents.","AMEP is written in pure Python and is based on powerful libraries such as NumPy, SciPy, Matplotlib, and scikit-image.","Computationally expensive methods are parallelized and optimized to run efficiently on workstations, laptops, and high-performance computing architectures, and an HDF5-based data format is used in the backend to store and handle simulation data as well as analysis results.","AMEP provides the first comprehensive framework for analyzing simulation results of both particle-based and continuum simulations (as well as experimental data) of active matter systems.","In particular, AMEP also allows it to analyze simulations that combine particle-based and continuum techniques such as used to study the motion of bacteria in chemical fields or for modeling particle motion in a flow field.","AMEP is available at https://amepproject.de and can be installed via conda and pip."],"url":"http://arxiv.org/abs/2404.16533v1","category":"cond-mat.soft"}
{"created":"2024-04-25 11:42:59","title":"Nonlinear dynamics of a hanging string with a freely pivoting attached mass","abstract":"We show that the natural resonant frequency of a suspended flexible string is significantly modified (by one order of magnitude) by adding a freely pivoting attached mass at its lower end. This articulated system then exhibits complex nonlinear dynamics such as bending oscillations, similar to those of a swing becoming slack, thereby strongly modifying the system resonance that is found to be controlled by the length of the pivoting mass. The dynamics is experimentally studied using a remote and noninvasive magnetic parametric forcing. To do so, a permanent magnet is suspended by a flexible string above a vertically oscillating conductive plate. Harmonic and period-doubling instabilities are experimentally reported and are modeled using the Hill equation, leading to analytical solutions that accurately describe the experimentally observed tonguelike instability curves.","sentences":["We show that the natural resonant frequency of a suspended flexible string is significantly modified (by one order of magnitude) by adding a freely pivoting attached mass at its lower end.","This articulated system then exhibits complex nonlinear dynamics such as bending oscillations, similar to those of a swing becoming slack, thereby strongly modifying the system resonance that is found to be controlled by the length of the pivoting mass.","The dynamics is experimentally studied using a remote and noninvasive magnetic parametric forcing.","To do so, a permanent magnet is suspended by a flexible string above a vertically oscillating conductive plate.","Harmonic and period-doubling instabilities are experimentally reported and are modeled using the Hill equation, leading to analytical solutions that accurately describe the experimentally observed tonguelike instability curves."],"url":"http://arxiv.org/abs/2404.16531v1","category":"nlin.PS"}
{"created":"2024-04-25 11:42:43","title":"On the Political Economy of Link-based Web Search","abstract":"Web search engines arguably form the most popular data-driven systems in contemporary society. They wield a considerable power by functioning as gatekeepers of the Web, with most user journeys on the Web beginning with them. Starting from the late 1990s, search engines have been dominated by the paradigm of link-based web search. In this paper, we critically analyze the political economy of the paradigm of link-based web search, drawing upon insights and methodologies from critical political economy. We draw several insights on how link-based web search has led to phenomena that favor capital through long-term structural changes on the Web, and how it has led to accentuating unpaid digital labor and ecologically unsustainable practices, among several others. We show how contemporary observations on the degrading quality of link-based web search can be traced back to the internal contradictions with the paradigm, and how such socio-technical phenomena may lead to a disutility of the link-based web search model. Our contribution is primarily on enhancing the understanding of the political economy of link-based web search, and laying bare the phenomena at work, and implicitly catalyze the search for alternative models.","sentences":["Web search engines arguably form the most popular data-driven systems in contemporary society.","They wield a considerable power by functioning as gatekeepers of the Web, with most user journeys on the Web beginning with them.","Starting from the late 1990s, search engines have been dominated by the paradigm of link-based web search.","In this paper, we critically analyze the political economy of the paradigm of link-based web search, drawing upon insights and methodologies from critical political economy.","We draw several insights on how link-based web search has led to phenomena that favor capital through long-term structural changes on the Web, and how it has led to accentuating unpaid digital labor and ecologically unsustainable practices, among several others.","We show how contemporary observations on the degrading quality of link-based web search can be traced back to the internal contradictions with the paradigm, and how such socio-technical phenomena may lead to a disutility of the link-based web search model.","Our contribution is primarily on enhancing the understanding of the political economy of link-based web search, and laying bare the phenomena at work, and implicitly catalyze the search for alternative models."],"url":"http://arxiv.org/abs/2404.16530v1","category":"cs.CY"}
{"created":"2024-04-25 11:42:32","title":"Vision-based robot manipulation of transparent liquid containers in a laboratory setting","abstract":"Laboratory processes involving small volumes of solutions and active ingredients are often performed manually due to challenges in automation, such as high initial costs, semi-structured environments and protocol variability. In this work, we develop a flexible and cost-effective approach to address this gap by introducing a vision-based system for liquid volume estimation and a simulation-driven pouring method particularly designed for containers with small openings. We evaluate both components individually, followed by an applied real-world integration of cell culture automation using a UR5 robotic arm. Our work is fully reproducible: we share our code at at \\url{https://github.com/DaniSchober/LabLiquidVision} and the newly introduced dataset LabLiquidVolume is available at https://data.dtu.dk/articles/dataset/LabLiquidVision/25103102.","sentences":["Laboratory processes involving small volumes of solutions and active ingredients are often performed manually due to challenges in automation, such as high initial costs, semi-structured environments and protocol variability.","In this work, we develop a flexible and cost-effective approach to address this gap by introducing a vision-based system for liquid volume estimation and a simulation-driven pouring method particularly designed for containers with small openings.","We evaluate both components individually, followed by an applied real-world integration of cell culture automation using a UR5 robotic arm.","Our work is fully reproducible: we share our code at at \\url{https://github.com/DaniSchober/LabLiquidVision} and the newly introduced dataset LabLiquidVolume is available at https://data.dtu.dk/articles/dataset/LabLiquidVision/25103102."],"url":"http://arxiv.org/abs/2404.16529v1","category":"cs.RO"}
{"created":"2024-04-25 11:38:30","title":"An efficient approach for searching three-body periodic orbits passing through Eulerian configuration","abstract":"A new efficient approach for searching three-body periodic equal-mass collisionless orbits passing through Eulerian configuration is presented. The approach is based on a symmetry property of the solutions at the half period. Depending on two previously established symmetry types on the shape sphere, each solution is presented by one or two distinct initial conditions (one or two points in the search domain). A numerical search based on Newton's method on a relatively coarse search grid for solutions with relatively small scale-invariant periods $T^{*}<70$ is conducted. The linear systems at each Newton's iteration are computed by high order high precision Taylor series method. The search produced 12,431 initial conditions (i.c.s) corresponding to 6,333 distinct solutions. In addition to passing through the Eulerian configuration, 35 of the solutions are also free-fall ones. Although most of the found solutions are new, all linearly stable solutions among them (only 7) are old ones. Particular attention is paid to the details of the high precision computations and the analysis of accuracy. All i.c.s are given with 100 correct digits.","sentences":["A new efficient approach for searching three-body periodic equal-mass collisionless orbits passing through Eulerian configuration is presented.","The approach is based on a symmetry property of the solutions at the half period.","Depending on two previously established symmetry types on the shape sphere, each solution is presented by one or two distinct initial conditions (one or two points in the search domain).","A numerical search based on Newton's method on a relatively coarse search grid for solutions with relatively small scale-invariant periods $T^{*}<70$ is conducted.","The linear systems at each Newton's iteration are computed by high order high precision Taylor series method.","The search produced 12,431 initial conditions (i.c.s) corresponding to 6,333 distinct solutions.","In addition to passing through the Eulerian configuration, 35 of the solutions are also free-fall ones.","Although most of the found solutions are new, all linearly stable solutions among them (only 7) are old ones.","Particular attention is paid to the details of the high precision computations and the analysis of accuracy.","All i.c.s are given with 100 correct digits."],"url":"http://arxiv.org/abs/2404.16526v1","category":"physics.class-ph"}
{"created":"2024-04-25 11:35:26","title":"Anomalous Directed Percolation on a Dynamic Network using Rydberg Facilitation","abstract":"The facilitation of Rydberg excitations in a gas of atoms provides an ideal model system to study epidemic evolution on (dynamic) networks and self organization of complex systems to the critical point of a non-equilibrium phase transition. Using Monte-Carlo simulations and a machine learning algorithm we show that the universality class of this phase transition can be tuned. The classes include directed percolation (DP), the most common class in short-range spreading models, and mean-field (MF) behavior, but also different types of anomalous directed percolation (ADP), characterized by rare long-range excitation processes. In a frozen gas, ground state atoms that can facilitate each other form a static network, for which we predict DP universality. Atomic motion then turns the network into a dynamic one with long-range (Levy-flight type) excitations. This leads to continuously varying critical exponents corresponding to the ADP universality class, eventually reaching MF behavior. These findings also explain the recently observed critical exponent of Rydberg facilitation in an ultra-cold gas experiment [Helmrich et al., Nature 577, 481 (2020)], which was in between DP and MF values.","sentences":["The facilitation of Rydberg excitations in a gas of atoms provides an ideal model system to study epidemic evolution on (dynamic) networks and self organization of complex systems to the critical point of a non-equilibrium phase transition.","Using Monte-Carlo simulations and a machine learning algorithm we show that the universality class of this phase transition can be tuned.","The classes include directed percolation (DP), the most common class in short-range spreading models, and mean-field (MF) behavior, but also different types of anomalous directed percolation (ADP), characterized by rare long-range excitation processes.","In a frozen gas, ground state atoms that can facilitate each other form a static network, for which we predict DP universality.","Atomic motion then turns the network into a dynamic one with long-range (Levy-flight type) excitations.","This leads to continuously varying critical exponents corresponding to the ADP universality class, eventually reaching MF behavior.","These findings also explain the recently observed critical exponent of Rydberg facilitation in an ultra-cold gas experiment [Helmrich et al., Nature 577, 481 (2020)], which was in between DP and MF values."],"url":"http://arxiv.org/abs/2404.16523v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-25 11:23:11","title":"Topological properties of finite-size heterostructures of magnetic topological insulators and superconductors","abstract":"Heterostructures of magnetic topological insulators (MTIs) and superconductors (SCs) in two-dimensional (2D) slab and one-dimensional (1D) nanoribbon geometries have been predicted to host, respectively, chiral Majorana edge states (CMESs) and Majorana bound states (MBSs). We study the topological properties of such MTI/SC heterostructures upon variation of the geometry from wide slabs to quasi-1D nanoribbon systems and as a function of the chemical potential, the magnetic doping, and the induced superconducting pairing potential. To do so, we construct effective symmetry-constrained low-energy Hamiltonians accounting for the real-space confinement. For a nanoribbon geometry with finite width and length, we observe different phases characterized by CMESs, MBSs, as well as coexisting CMESs and MBSs, as the chemical potential, the magnetic doping and/or the width are varied.","sentences":["Heterostructures of magnetic topological insulators (MTIs) and superconductors (SCs) in two-dimensional (2D) slab and one-dimensional (1D) nanoribbon geometries have been predicted to host, respectively, chiral Majorana edge states (CMESs) and Majorana bound states (MBSs).","We study the topological properties of such MTI/SC heterostructures upon variation of the geometry from wide slabs to quasi-1D nanoribbon systems and as a function of the chemical potential, the magnetic doping, and the induced superconducting pairing potential.","To do so, we construct effective symmetry-constrained low-energy Hamiltonians accounting for the real-space confinement.","For a nanoribbon geometry with finite width and length, we observe different phases characterized by CMESs, MBSs, as well as coexisting CMESs and MBSs, as the chemical potential, the magnetic doping and/or the width are varied."],"url":"http://arxiv.org/abs/2404.16520v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 11:15:26","title":"Adaptive Learning-based Model Predictive Control for Uncertain Interconnected Systems: A Set Membership Identification Approach","abstract":"We propose a novel adaptive learning-based model predictive control (MPC) scheme for interconnected systems which can be decomposed into several smaller dynamically coupled subsystems with uncertain coupling. The proposed scheme is mainly divided into two main online phases; a learning phase and an adaptation phase. Set membership identification is used in the learning phase to learn an uncertainty set that contains the coupling strength using online data. In the adaptation phase, rigid tube-based robust MPC is used to compute the optimal predicted states and inputs. Besides computing the optimal trajectories, the MPC ingredients are adapted in the adaptation phase taking the learnt uncertainty set into account. These MPC ingredients include the prestabilizing controller, the rigid tube, the tightened constraints and the terminal ingredients. The recursive feasibility of the proposed scheme as well as the stability of the corresponding closed-loop system are discussed. The developed scheme is compared in simulations to existing schemes including robust, adaptive and learning-based MPC.","sentences":["We propose a novel adaptive learning-based model predictive control (MPC) scheme for interconnected systems which can be decomposed into several smaller dynamically coupled subsystems with uncertain coupling.","The proposed scheme is mainly divided into two main online phases; a learning phase and an adaptation phase.","Set membership identification is used in the learning phase to learn an uncertainty set that contains the coupling strength using online data.","In the adaptation phase, rigid tube-based robust MPC is used to compute the optimal predicted states and inputs.","Besides computing the optimal trajectories, the MPC ingredients are adapted in the adaptation phase taking the learnt uncertainty set into account.","These MPC ingredients include the prestabilizing controller, the rigid tube, the tightened constraints and the terminal ingredients.","The recursive feasibility of the proposed scheme as well as the stability of the corresponding closed-loop system are discussed.","The developed scheme is compared in simulations to existing schemes including robust, adaptive and learning-based MPC."],"url":"http://arxiv.org/abs/2404.16514v1","category":"eess.SY"}
{"created":"2024-04-25 11:01:40","title":"Semantic-aware Next-Best-View for Multi-DoFs Mobile System in Search-and-Acquisition based Visual Perception","abstract":"Efficient visual perception using mobile systems is crucial, particularly in unknown environments such as search and rescue operations, where swift and comprehensive perception of objects of interest is essential. In such real-world applications, objects of interest are often situated in complex environments, making the selection of the 'Next Best' view based solely on maximizing visibility gain suboptimal. Semantics, providing a higher-level interpretation of perception, should significantly contribute to the selection of the next viewpoint for various perception tasks. In this study, we formulate a novel information gain that integrates both visibility gain and semantic gain in a unified form to select the semantic-aware Next-Best-View. Additionally, we design an adaptive strategy with termination criterion to support a two-stage search-and-acquisition manoeuvre on multiple objects of interest aided by a multi-degree-of-freedoms (Multi-DoFs) mobile system. Several semantically relevant reconstruction metrics, including perspective directivity and region of interest (ROI)-to-full reconstruction volume ratio, are introduced to evaluate the performance of the proposed approach. Simulation experiments demonstrate the advantages of the proposed approach over existing methods, achieving improvements of up to 27.13% for the ROI-to-full reconstruction volume ratio and a 0.88234 average perspective directivity. Furthermore, the planned motion trajectory exhibits better perceiving coverage toward the target.","sentences":["Efficient visual perception using mobile systems is crucial, particularly in unknown environments such as search and rescue operations, where swift and comprehensive perception of objects of interest is essential.","In such real-world applications, objects of interest are often situated in complex environments, making the selection of the 'Next Best' view based solely on maximizing visibility gain suboptimal.","Semantics, providing a higher-level interpretation of perception, should significantly contribute to the selection of the next viewpoint for various perception tasks.","In this study, we formulate a novel information gain that integrates both visibility gain and semantic gain in a unified form to select the semantic-aware Next-Best-View.","Additionally, we design an adaptive strategy with termination criterion to support a two-stage search-and-acquisition manoeuvre on multiple objects of interest aided by a multi-degree-of-freedoms (Multi-DoFs) mobile system.","Several semantically relevant reconstruction metrics, including perspective directivity and region of interest (ROI)-to-full reconstruction volume ratio, are introduced to evaluate the performance of the proposed approach.","Simulation experiments demonstrate the advantages of the proposed approach over existing methods, achieving improvements of up to 27.13% for the ROI-to-full reconstruction volume ratio and a 0.88234 average perspective directivity.","Furthermore, the planned motion trajectory exhibits better perceiving coverage toward the target."],"url":"http://arxiv.org/abs/2404.16507v1","category":"cs.CV"}
{"created":"2024-04-25 10:52:28","title":"A Prototypical Expert-Driven Approach Towards Capability-Based Monitoring of Automated Driving Systems","abstract":"Supervising the safe operation of automated vehicles is a key requirement in order to unleash their full potential in future transportation systems. In particular, previous publications have argued that SAE Level 4 vehicles should be aware of their capabilities at runtime to make appropriate behavioral decisions. In this paper, we present a framework that enables the implementation of an online capability monitor. We derive a graphical system model that captures the relationships between the quality of system elements across different architectural views. In an expert-driven approach, we parameterize Bayesian Networks based on this structure using Fuzzy Logic. Using the online monitor, we infer the quality of the system's capabilities based on technical measurements acquired at runtime. Our approach is demonstrated in the context of the UNICAR.agil research project in an urban example scenario.","sentences":["Supervising the safe operation of automated vehicles is a key requirement in order to unleash their full potential in future transportation systems.","In particular, previous publications have argued that SAE Level 4 vehicles should be aware of their capabilities at runtime to make appropriate behavioral decisions.","In this paper, we present a framework that enables the implementation of an online capability monitor.","We derive a graphical system model that captures the relationships between the quality of system elements across different architectural views.","In an expert-driven approach, we parameterize Bayesian Networks based on this structure using Fuzzy Logic.","Using the online monitor, we infer the quality of the system's capabilities based on technical measurements acquired at runtime.","Our approach is demonstrated in the context of the UNICAR.agil research project in an urban example scenario."],"url":"http://arxiv.org/abs/2404.16502v1","category":"eess.SY"}
{"created":"2024-04-25 10:45:04","title":"Violation of Bell inequalities in an analogue black hole","abstract":"Signals of entanglement and nonlocality are quantitatively evaluated at zero and finite temperature in an analogue black hole realized in the flow of a quasi one-dimensional Bose-Einstein condensate. The violation of Lorentz invariance inherent to this analog system opens the prospect to observe 3-mode quantum correlations and we study the corresponding violation of bipartite and tripartite Bell inequalities. It is shown that the long wavelength modes of the system are maximally entangled, in the sense that they realize a superposition of continuous variable versions of Greenberger-Horne-Zeilinger states whose entanglement resists partial tracing.","sentences":["Signals of entanglement and nonlocality are quantitatively evaluated at zero and finite temperature in an analogue black hole realized in the flow of a quasi one-dimensional Bose-Einstein condensate.","The violation of Lorentz invariance inherent to this analog system opens the prospect to observe 3-mode quantum correlations and we study the corresponding violation of bipartite and tripartite Bell inequalities.","It is shown that the long wavelength modes of the system are maximally entangled, in the sense that they realize a superposition of continuous variable versions of Greenberger-Horne-Zeilinger states whose entanglement resists partial tracing."],"url":"http://arxiv.org/abs/2404.16497v1","category":"quant-ph"}
{"created":"2024-04-25 10:41:12","title":"Probabilistic Multi-Layer Perceptrons for Wind Farm Condition Monitoring","abstract":"We provide a condition monitoring system for wind farms, based on normal behaviour modelling using a probabilistic multi-layer perceptron with transfer learning via fine-tuning. The model predicts the output power of the wind turbine under normal behaviour based on features retrieved from supervisory control and data acquisition (SCADA) systems. Its advantages are that (i) it can be trained with SCADA data of at least a few years, (ii) it can incorporate all SCADA data of all wind turbines in a wind farm as features, (iii) it assumes that the output power follows a normal density with heteroscedastic variance and (iv) it can predict the output of one wind turbine by borrowing strength from the data of all other wind turbines in a farm. Probabilistic guidelines for condition monitoring are given via a CUSUM control chart. We illustrate the performance of our model in a real SCADA data example which provides evidence that it outperforms other probabilistic prediction models.","sentences":["We provide a condition monitoring system for wind farms, based on normal behaviour modelling using a probabilistic multi-layer perceptron with transfer learning via fine-tuning.","The model predicts the output power of the wind turbine under normal behaviour based on features retrieved from supervisory control and data acquisition (SCADA) systems.","Its advantages are that (i) it can be trained with SCADA data of at least a few years, (ii) it can incorporate all SCADA data of all wind turbines in a wind farm as features, (iii) it assumes that the output power follows a normal density with heteroscedastic variance and (iv) it can predict the output of one wind turbine by borrowing strength from the data of all other wind turbines in a farm.","Probabilistic guidelines for condition monitoring are given via a CUSUM control chart.","We illustrate the performance of our model in a real SCADA data example which provides evidence that it outperforms other probabilistic prediction models."],"url":"http://arxiv.org/abs/2404.16496v1","category":"cs.LG"}
{"created":"2024-04-25 10:32:24","title":"On the topology of concurrent systems","abstract":"Higher-dimensional automata, i.e., pointed labeled precubical sets, are a powerful combinatorial-topological model for concurrent systems. In this paper, we show that for every (nonempty) connected polyhedron there exists a shared-variable system such that the higher-dimensional automaton modeling the state space of the system has the homotopy type of the polyhedron.","sentences":["Higher-dimensional automata, i.e., pointed labeled precubical sets, are a powerful combinatorial-topological model for concurrent systems.","In this paper, we show that for every (nonempty) connected polyhedron there exists a shared-variable system such that the higher-dimensional automaton modeling the state space of the system has the homotopy type of the polyhedron."],"url":"http://arxiv.org/abs/2404.16492v1","category":"cs.FL"}
{"created":"2024-04-25 10:28:06","title":"Ascent and Descent of Weighted Composition Operators on Lorentz spaces","abstract":"The aim of this article is to detect the ascent and descent of weighted composition operators on Lorentz spaces. We investigate the conditions on the measurable transformation $T$ and the complex-valued measurable function $u$ defined on measure space $(X, \\mathcal{A}, \\mu)$ that cause the weighted composition operators on Lorentz space $L(p, q)$, $1 < p \\leq \\infty, 1 \\leq q \\leq \\infty$ to have finite or infinite ascent (descent). We also give a number of examples in support of our findings.","sentences":["The aim of this article is to detect the ascent and descent of weighted composition operators on Lorentz spaces.","We investigate the conditions on the measurable transformation $T$ and the complex-valued measurable function $u$ defined on measure space $(X, \\mathcal{A}, \\mu)$ that cause the weighted composition operators on Lorentz space $L(p, q)$, $1 < p \\leq \\infty, 1 \\leq q \\leq \\infty$ to have finite or infinite ascent (descent).","We also give a number of examples in support of our findings."],"url":"http://arxiv.org/abs/2404.16491v1","category":"math.FA"}
{"created":"2024-04-25 10:26:18","title":"Cost-Driven Data Replication with Predictions","abstract":"This paper studies an online replication problem for distributed data access. The goal is to dynamically create and delete data copies in a multi-server system as time passes to minimize the total storage and network cost of serving access requests. We study the problem in the emergent learning-augmented setting, assuming simple binary predictions about inter-request times at individual servers. We develop an online algorithm and prove that it is ($\\frac{5+\\alpha}{3}$)-consistent (competitiveness under perfect predictions) and ($1 + \\frac{1}{\\alpha}$)-robust (competitiveness under terrible predictions), where $\\alpha \\in (0, 1]$ is a hyper-parameter representing the level of distrust in the predictions. We also study the impact of mispredictions on the competitive ratio of the proposed algorithm and adapt it to achieve a bounded robustness while retaining its consistency. We further establish a lower bound of $\\frac{3}{2}$ on the consistency of any deterministic learning-augmented algorithm. Experimental evaluations are carried out to evaluate our algorithms using real data access traces.","sentences":["This paper studies an online replication problem for distributed data access.","The goal is to dynamically create and delete data copies in a multi-server system as time passes to minimize the total storage and network cost of serving access requests.","We study the problem in the emergent learning-augmented setting, assuming simple binary predictions about inter-request times at individual servers.","We develop an online algorithm and prove that it is ($\\frac{5+\\alpha}{3}$)-consistent (competitiveness under perfect predictions) and ($1 + \\frac{1}{\\alpha}$)-robust (competitiveness under terrible predictions), where $\\alpha \\in (0, 1]$ is a hyper-parameter representing the level of distrust in the predictions.","We also study the impact of mispredictions on the competitive ratio of the proposed algorithm and adapt it to achieve a bounded robustness while retaining its consistency.","We further establish a lower bound of $\\frac{3}{2}$ on the consistency of any deterministic learning-augmented algorithm.","Experimental evaluations are carried out to evaluate our algorithms using real data access traces."],"url":"http://arxiv.org/abs/2404.16489v1","category":"cs.DS"}
{"created":"2024-04-25 10:18:16","title":"OpenIVM: a SQL-to-SQL Compiler for Incremental Computations","abstract":"This demonstration presents a new Open Source SQL-to-SQL compiler for Incremental View Maintenance (IVM). While previous systems, such as DBToaster, implemented computational functionality for IVM in a separate system, the core principle of OpenIVM is to make use of existing SQL query processing engines and perform all IVM computations via SQL. This approach enables the integration of IVM in these systems without code duplication. Also, it eases its use in cross-system IVM, i.e. to orchestrate an HTAP system in which one (OLTP) DBMS provides insertions/updates/deletes (deltas), which are propagated using SQL into another (OLAP) DBMS, hosting materialized views. Our system compiles view definitions into SQL to eventually propagate deltas into the table that materializes the view, following the principles of DBSP. Under the hood, OpenIVM uses the DuckDB library to compile (parse, transform, optimize) the materialized view maintenance logic. We demonstrate OpenIVM in action (i) as the core of a DuckDB extension module that adds IVM functionality to it and (ii) powering cross-system IVM for HTAP, with PostgreSQL handling updates on base tables and DuckDB hosting materialized views on these.","sentences":["This demonstration presents a new Open Source SQL-to-SQL compiler for Incremental View Maintenance (IVM).","While previous systems, such as DBToaster, implemented computational functionality for IVM in a separate system, the core principle of OpenIVM is to make use of existing SQL query processing engines and perform all IVM computations via SQL.","This approach enables the integration of IVM in these systems without code duplication.","Also, it eases its use in cross-system IVM, i.e. to orchestrate an HTAP system in which one (OLTP) DBMS provides insertions/updates/deletes (deltas), which are propagated using SQL into another (OLAP) DBMS, hosting materialized views.","Our system compiles view definitions into SQL to eventually propagate deltas into the table that materializes the view, following the principles of DBSP.","Under the hood, OpenIVM uses the DuckDB library to compile (parse, transform, optimize)","the materialized view maintenance logic.","We demonstrate OpenIVM in action (i) as the core of a DuckDB extension module that adds IVM functionality to it and (ii) powering cross-system IVM for HTAP, with PostgreSQL handling updates on base tables and DuckDB hosting materialized views on these."],"url":"http://arxiv.org/abs/2404.16486v1","category":"cs.DB"}
{"created":"2024-04-25 10:12:31","title":"Leveraging Pretrained Latent Representations for Few-Shot Imitation Learning on a Dexterous Robotic Hand","abstract":"In the context of imitation learning applied to dexterous robotic hands, the high complexity of the systems makes learning complex manipulation tasks challenging. However, the numerous datasets depicting human hands in various different tasks could provide us with better knowledge regarding human hand motion. We propose a method to leverage multiple large-scale task-agnostic datasets to obtain latent representations that effectively encode motion subtrajectories that we included in a transformer-based behavior cloning method. Our results demonstrate that employing latent representations yields enhanced performance compared to conventional behavior cloning methods, particularly regarding resilience to errors and noise in perception and proprioception. Furthermore, the proposed approach solely relies on human demonstrations, eliminating the need for teleoperation and, therefore, accelerating the data acquisition process. Accurate inverse kinematics for fingertip retargeting ensures precise transfer from human hand data to the robot, facilitating effective learning and deployment of manipulation policies. Finally, the trained policies have been successfully transferred to a real-world 23Dof robotic system.","sentences":["In the context of imitation learning applied to dexterous robotic hands, the high complexity of the systems makes learning complex manipulation tasks challenging.","However, the numerous datasets depicting human hands in various different tasks could provide us with better knowledge regarding human hand motion.","We propose a method to leverage multiple large-scale task-agnostic datasets to obtain latent representations that effectively encode motion subtrajectories that we included in a transformer-based behavior cloning method.","Our results demonstrate that employing latent representations yields enhanced performance compared to conventional behavior cloning methods, particularly regarding resilience to errors and noise in perception and proprioception.","Furthermore, the proposed approach solely relies on human demonstrations, eliminating the need for teleoperation and, therefore, accelerating the data acquisition process.","Accurate inverse kinematics for fingertip retargeting ensures precise transfer from human hand data to the robot, facilitating effective learning and deployment of manipulation policies.","Finally, the trained policies have been successfully transferred to a real-world 23Dof robotic system."],"url":"http://arxiv.org/abs/2404.16483v1","category":"cs.RO"}
{"created":"2024-04-25 10:00:16","title":"A Novel Channel Coding Scheme for Digital Multiple Access Computing","abstract":"In this paper, we consider the ChannelComp framework, which facilitates the computation of desired functions by multiple transmitters over a common receiver using digital modulations across a multiple access channel. While ChannelComp currently offers a broad framework for computation by designing digital constellations for over-the-air computation and employing symbol-level encoding, encoding the repeated transmissions of the same symbol and using the corresponding received sequence may significantly improve the computation performance and reduce the encoding complexity. In this paper, we propose an enhancement involving the encoding of the repetitive transmission of the same symbol at each transmitter over multiple time slots and the design of constellation diagrams, with the aim of minimizing computational errors. We frame this enhancement as an optimization problem, which jointly identifies the constellation diagram and the channel code for repetition, which we call ReChCompCode. To manage the computational complexity of the optimization, we divide it into two tractable subproblems. Through numerical experiments, we evaluate the performance of ReChCompCode. The simulation results reveal that ReChCompCode can reduce the computation error by approximately up to 30 dB compared to standard ChannelComp, particularly for product functions.","sentences":["In this paper, we consider the ChannelComp framework, which facilitates the computation of desired functions by multiple transmitters over a common receiver using digital modulations across a multiple access channel.","While ChannelComp currently offers a broad framework for computation by designing digital constellations for over-the-air computation and employing symbol-level encoding, encoding the repeated transmissions of the same symbol and using the corresponding received sequence may significantly improve the computation performance and reduce the encoding complexity.","In this paper, we propose an enhancement involving the encoding of the repetitive transmission of the same symbol at each transmitter over multiple time slots and the design of constellation diagrams, with the aim of minimizing computational errors.","We frame this enhancement as an optimization problem, which jointly identifies the constellation diagram and the channel code for repetition, which we call ReChCompCode.","To manage the computational complexity of the optimization, we divide it into two tractable subproblems.","Through numerical experiments, we evaluate the performance of ReChCompCode.","The simulation results reveal that ReChCompCode can reduce the computation error by approximately up to 30 dB compared to standard ChannelComp, particularly for product functions."],"url":"http://arxiv.org/abs/2404.16476v1","category":"eess.SP"}
{"created":"2024-04-25 09:54:21","title":"A finite-time quantum Otto engine with tunnel coupled one-dimensional Bose gases","abstract":"We undertake a theoretical study of a finite-time quantum Otto engine cycle driven by inter-particle interactions in a weakly interacting one-dimensional Bose gas in the quasicondensate regime. Utilizing a $c$-field approach, we simulate the entire Otto cycle, i.e. the two work strokes and the two equilibration strokes. More specifically, the interaction-induced work strokes are modelled by treating the working fluid as an isolated quantum many-body system undergoing unitary evolution. The equilibration strokes, on the other hand, are modelled by treating the working fluid as an open quantum system tunnel-coupled to another quasicondensate which acts as either the hot or cold reservoir, albeit of finite size. We find that, unlike a uniform 1D Bose gas, a harmonically trapped quasicondensate cannot operate purely as a \\emph{heat} engine; instead, the engine operation is enabled by additional \\emph{chemical} work performed on the working fluid, facilitated by the inflow of particles from the hot reservoir. The microscopic treatment of dynamics during equilibration strokes enables us to evaluate the characteristic operational time scales of this Otto chemical engine, crucial for characterizing its power output, without any \\emph{ad hoc} assumptions about typical thermalization timescales. We analyse the performance and quantify the figures of merit of the proposed Otto chemical engine, finding that it offers a favourable trade-off between efficiency and power output, particularly when the interaction-induced work strokes are implemented via a sudden quench. We further demonstrate that in the sudden quench regime, the engine operates with an efficiency close to the near-adiabatic (near maximum efficiency) limit, while concurrently achieving maximum power output.","sentences":["We undertake a theoretical study of a finite-time quantum Otto engine cycle driven by inter-particle interactions in a weakly interacting one-dimensional Bose gas in the quasicondensate regime.","Utilizing a $c$-field approach, we simulate the entire Otto cycle, i.e. the two work strokes and the two equilibration strokes.","More specifically, the interaction-induced work strokes are modelled by treating the working fluid as an isolated quantum many-body system undergoing unitary evolution.","The equilibration strokes, on the other hand, are modelled by treating the working fluid as an open quantum system tunnel-coupled to another quasicondensate which acts as either the hot or cold reservoir, albeit of finite size.","We find that, unlike a uniform 1D Bose gas, a harmonically trapped quasicondensate cannot operate purely as a \\emph{heat} engine; instead, the engine operation is enabled by additional \\emph{chemical} work performed on the working fluid, facilitated by the inflow of particles from the hot reservoir.","The microscopic treatment of dynamics during equilibration strokes enables us to evaluate the characteristic operational time scales of this Otto chemical engine, crucial for characterizing its power output, without any \\emph{ad hoc} assumptions about typical thermalization timescales.","We analyse the performance and quantify the figures of merit of the proposed Otto chemical engine, finding that it offers a favourable trade-off between efficiency and power output, particularly when the interaction-induced work strokes are implemented via a sudden quench.","We further demonstrate that in the sudden quench regime, the engine operates with an efficiency close to the near-adiabatic (near maximum efficiency) limit, while concurrently achieving maximum power output."],"url":"http://arxiv.org/abs/2404.16470v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-25 09:48:37","title":"Epidemic risk perception and social interactions lead to awareness cascades on multiplex networks","abstract":"The course of an epidemic is not only shaped by infection transmission over face-to-face contacts, but also by preventive behaviour caused by risk perception and social interactions. This study explores the dynamics of coupled awareness and biological infection spread within a two-layer multiplex network framework. One layer embodies face-to-face contacts, with a biological infection transmission following a simple contagion model, the SIR process. Awareness, modelled by the linear threshold model, a complex contagion, spreads over a social layer and induces behaviour that lowers the chance of a biological infection occurring. It may be provoked by the presence of either aware or infectious neighbours. We introduce a novel model combining these influences through a convex combination, creating a continuum between pure social contagion and local risk perception. Simulation of the model shows distinct effects arising from the awareness sources. Also, for convex combinations where both input sources are of importance, awareness cascades that are not attributable to only one of these sources, emerge. Under these conditions, the combination of a small-world face-to-face and a scale-free social layer, but not vice versa, make that the extent of the infections decreases with increasing transmission probability.","sentences":["The course of an epidemic is not only shaped by infection transmission over face-to-face contacts, but also by preventive behaviour caused by risk perception and social interactions.","This study explores the dynamics of coupled awareness and biological infection spread within a two-layer multiplex network framework.","One layer embodies face-to-face contacts, with a biological infection transmission following a simple contagion model, the SIR process.","Awareness, modelled by the linear threshold model, a complex contagion, spreads over a social layer and induces behaviour that lowers the chance of a biological infection occurring.","It may be provoked by the presence of either aware or infectious neighbours.","We introduce a novel model combining these influences through a convex combination, creating a continuum between pure social contagion and local risk perception.","Simulation of the model shows distinct effects arising from the awareness sources.","Also, for convex combinations where both input sources are of importance, awareness cascades that are not attributable to only one of these sources, emerge.","Under these conditions, the combination of a small-world face-to-face and a scale-free social layer, but not vice versa, make that the extent of the infections decreases with increasing transmission probability."],"url":"http://arxiv.org/abs/2404.16466v1","category":"physics.soc-ph"}
{"created":"2024-04-25 09:48:09","title":"In Situ Characterisation of Graphene Growth on Liquid Copper-Gallium Alloys: Paving the Path for Cost-Effective Synthesis","abstract":"Liquid metal catalysts (LMCats), primarily molten copper, have demonstrated their efficiency in the chemical vapour deposition (CVD) approach for synthesising high-quality, large-area graphene. However, their high melting temperatures limit broader applications. Reducing the temperature of graphene production on LMCats would lead to a more efficient and cost-effective process. Here, we investigated the effects of alloying copper with a low-melting temperature metal on graphene growth in real-time. We examined a set of liquid copper-gallium alloy systems using two complementary in situ techniques: radiation-mode optical microscopy and synchrotron X-ray reflectivity (XRR). Microscopy observations revealed reduced catalytic activity and graphene quality degradation in compositions with gallium domination. The XRR confirmed the formation of single-layer graphene on alloys with up to 60 wt% of gallium. Additionally, we detected a systematic increase in adsorption height on the alloys' surface, suggesting a weaker graphene adhesion on gallium. These findings propose a trade-off between layer quality and production cost reduction is feasible. Our results offer insights into the CVD synthesis of graphene on bimetallic liquid surfaces and underscore the potential of gallium-copper alloys for enabling the direct transfer of graphene from a liquid substrate, thereby addressing the limitations imposed by high melting temperatures of conventional LMCats.","sentences":["Liquid metal catalysts (LMCats), primarily molten copper, have demonstrated their efficiency in the chemical vapour deposition (CVD) approach for synthesising high-quality, large-area graphene.","However, their high melting temperatures limit broader applications.","Reducing the temperature of graphene production on LMCats would lead to a more efficient and cost-effective process.","Here, we investigated the effects of alloying copper with a low-melting temperature metal on graphene growth in real-time.","We examined a set of liquid copper-gallium alloy systems using two complementary in situ techniques: radiation-mode optical microscopy and synchrotron X-ray reflectivity (XRR).","Microscopy observations revealed reduced catalytic activity and graphene quality degradation in compositions with gallium domination.","The XRR confirmed the formation of single-layer graphene on alloys with up to 60 wt% of gallium.","Additionally, we detected a systematic increase in adsorption height on the alloys' surface, suggesting a weaker graphene adhesion on gallium.","These findings propose a trade-off between layer quality and production cost reduction is feasible.","Our results offer insights into the CVD synthesis of graphene on bimetallic liquid surfaces and underscore the potential of gallium-copper alloys for enabling the direct transfer of graphene from a liquid substrate, thereby addressing the limitations imposed by high melting temperatures of conventional LMCats."],"url":"http://arxiv.org/abs/2404.16465v1","category":"physics.app-ph"}
{"created":"2024-04-25 09:47:30","title":"Quantum-assisted trustworthiness for the Quantum Internet","abstract":"Device redundancy is one of the most well-known mechanisms in distributed systems to increase the overall system fault tolerance and, consequently, trustworthiness. Existing algorithms in this regard aim to exchange a significant number of messages among nodes to identify and agree which communication links or nodes are faulty. This approach greatly degrades the performance of those wireless communication networks exposed to limited available bandwidth and/or energy consumption due to messages flooding. Lately, quantum-assisted mechanisms have been envisaged as an appealing alternative to improve the performance in this kind of communication networks and have been shown to obtain levels of performance close to the ones achieved in ideal conditions. The purpose of this paper is to further explore this approach by using super-additivity and superposed quantum trajectories in quantum Internet to obtain a higher system trustworthiness. More specifically, the wireless communication network that supports the permafrost telemetry service for the Antarctica together with five operational modes (three of them using classical techniques and two of them using quantum-assisted mechanisms) have been simulated. Obtained results show that the new quantum-assisted mechanisms can increase the system performance by up to a 28%.","sentences":["Device redundancy is one of the most well-known mechanisms in distributed systems to increase the overall system fault tolerance and, consequently, trustworthiness.","Existing algorithms in this regard aim to exchange a significant number of messages among nodes to identify and agree which communication links or nodes are faulty.","This approach greatly degrades the performance of those wireless communication networks exposed to limited available bandwidth and/or energy consumption due to messages flooding.","Lately, quantum-assisted mechanisms have been envisaged as an appealing alternative to improve the performance in this kind of communication networks and have been shown to obtain levels of performance close to the ones achieved in ideal conditions.","The purpose of this paper is to further explore this approach by using super-additivity and superposed quantum trajectories in quantum Internet to obtain a higher system trustworthiness.","More specifically, the wireless communication network that supports the permafrost telemetry service for the Antarctica together with five operational modes (three of them using classical techniques and two of them using quantum-assisted mechanisms) have been simulated.","Obtained results show that the new quantum-assisted mechanisms can increase the system performance by up to a 28%."],"url":"http://arxiv.org/abs/2404.16463v1","category":"quant-ph"}
{"created":"2024-04-25 09:39:22","title":"Impacts of Energetic Particles from T Tauri Flares on Inner Protoplanetary Discs","abstract":"T Tauri stars are known to be magnetically active stars subject to strong flares observed in X-rays. These flares are likely due to intense magnetic reconnection events during which a part of the stored magnetic energy is converted into kinetic energy of supra-thermal particles. Since T Tauri stars are surrounded by an accretion disc, these particles may influence the disc dynamics and chemistry. This work continues on a previous stationary model, which showed that energetic particles accelerated during flares can produce a strong ionisation rate at high column densities in the inner accretion disc. The present model includes non-stationary sequences of flaring events sampled by a Chandra X-ray survey of nearby young stellar objects. We calculate the averaged ionisation rate expected in a radius range from 0.08 to 0.6 au from the central star. We confirm that energetic particles produced by the flares dominate the ionisation of the disc up to column densities of $10^{25}~\\rm{cm^{-2}}$. We further study the main consequences of this additional source of ionisation on the viscosity, the accretion rate, the volumetric heating rate and the chemical complexity of inner protoplanetary discs.","sentences":["T Tauri stars are known to be magnetically active stars subject to strong flares observed in X-rays.","These flares are likely due to intense magnetic reconnection events during which a part of the stored magnetic energy is converted into kinetic energy of supra-thermal particles.","Since T Tauri stars are surrounded by an accretion disc, these particles may influence the disc dynamics and chemistry.","This work continues on a previous stationary model, which showed that energetic particles accelerated during flares can produce a strong ionisation rate at high column densities in the inner accretion disc.","The present model includes non-stationary sequences of flaring events sampled by a Chandra X-ray survey of nearby young stellar objects.","We calculate the averaged ionisation rate expected in a radius range from 0.08 to 0.6 au from the central star.","We confirm that energetic particles produced by the flares dominate the ionisation of the disc up to column densities of $10^{25}~\\rm{cm^{-2}}$. We further study the main consequences of this additional source of ionisation on the viscosity, the accretion rate, the volumetric heating rate and the chemical complexity of inner protoplanetary discs."],"url":"http://arxiv.org/abs/2404.16459v1","category":"astro-ph.HE"}
{"created":"2024-04-25 09:38:11","title":"On an infinite commuting ODE system associated to a simple Lie algebra","abstract":"Inspired by a recent work of Dubrovin [7], for each simple Lie algebra $\\mathfrak{g}$, we introduce an infinite family of pairwise commuting ODEs and define their $\\tau$-functions. We show that these $\\tau$-functions can be identified with the $\\tau$-functions for the Drinfeld--Sokolov hierarchy of $\\mathfrak{g}$-type. Explicit examples for $\\mathfrak{g}=A_1$ and $A_2$ are provided, which are connected to the KdV hierarchy and the Boussinesq hierarchy respectively.","sentences":["Inspired by a recent work of Dubrovin","[7], for each simple Lie algebra $\\mathfrak{g}$, we introduce an infinite family of pairwise commuting ODEs and define their $\\tau$-functions.","We show that these $\\tau$-functions can be identified with the $\\tau$-functions for the Drinfeld--Sokolov hierarchy of $\\mathfrak{g}$-type.","Explicit examples for $\\mathfrak{g}=A_1$ and $A_2$ are provided, which are connected to the KdV hierarchy and the Boussinesq hierarchy respectively."],"url":"http://arxiv.org/abs/2404.16458v1","category":"nlin.SI"}
{"created":"2024-04-25 09:34:34","title":"Stabilizing quantum simulations of lattice gauge theories by dissipation","abstract":"Simulations of lattice gauge theories on noisy quantum hardware inherently suffer from violations of the gauge symmetry due to coherent and incoherent errors of the underlying physical system that implements the simulation. These gauge violations cause the simulations to become unphysical requiring the result of the simulation to be discarded. We investigate an active correction scheme that relies on detecting gauge violations locally and subsequently correcting them by dissipatively driving the system back into the physical gauge sector. We show that the correction scheme not only ensures the protection of the gauge symmetry, but it also leads to a longer validity of the simulation results even within the gauge-invariant sector. Finally, we discuss further applications of the scheme such as preparation of the many-body ground state of the simulated system.","sentences":["Simulations of lattice gauge theories on noisy quantum hardware inherently suffer from violations of the gauge symmetry due to coherent and incoherent errors of the underlying physical system that implements the simulation.","These gauge violations cause the simulations to become unphysical requiring the result of the simulation to be discarded.","We investigate an active correction scheme that relies on detecting gauge violations locally and subsequently correcting them by dissipatively driving the system back into the physical gauge sector.","We show that the correction scheme not only ensures the protection of the gauge symmetry, but it also leads to a longer validity of the simulation results even within the gauge-invariant sector.","Finally, we discuss further applications of the scheme such as preparation of the many-body ground state of the simulated system."],"url":"http://arxiv.org/abs/2404.16454v1","category":"quant-ph"}
{"created":"2024-04-25 09:30:19","title":"Unconditional correctness of recent quantum algorithms for factoring and computing discrete logarithms","abstract":"In 1994, Shor introduced his famous quantum algorithm to factor integers and compute discrete logarithms in polynomial time. In 2023, Regev proposed a multi-dimensional version of Shor's algorithm that requires far fewer quantum gates. His algorithm relies on a number-theoretic conjecture on the elements in $(\\mathbb{Z}/N\\mathbb{Z})^{\\times}$ that can be written as short products of very small prime numbers. We prove a version of this conjecture using tools from analytic number theory such as zero-density estimates. As a result, we obtain an unconditional proof of correctness of this improved quantum algorithm and of subsequent variants.","sentences":["In 1994, Shor introduced his famous quantum algorithm to factor integers and compute discrete logarithms in polynomial time.","In 2023, Regev proposed a multi-dimensional version of Shor's algorithm that requires far fewer quantum gates.","His algorithm relies on a number-theoretic conjecture on the elements in $(\\mathbb{Z}/N\\mathbb{Z})^{\\times}$ that can be written as short products of very small prime numbers.","We prove a version of this conjecture using tools from analytic number theory such as zero-density estimates.","As a result, we obtain an unconditional proof of correctness of this improved quantum algorithm and of subsequent variants."],"url":"http://arxiv.org/abs/2404.16450v1","category":"math.NT"}
{"created":"2024-04-25 09:29:10","title":"Hardy decomposition of higher order Lipschitz classes by polymonogenic functions","abstract":"In this paper we find a decomposition of higher order Lipschitz functions into the traces of a polymonogenic function and solve a related Riemann-Hilbert problem. Our approach lies in using a cliffordian Cauchy-type operator, which behaves as an involution operator on higher order Lipschitz spaces. The result obtained is a multidimensional sharpened version of the Hardy decomposition of H\\\"older continuous functions on a simple closed curve in the complex plane.","sentences":["In this paper we find a decomposition of higher order Lipschitz functions into the traces of a polymonogenic function and solve a related Riemann-Hilbert problem.","Our approach lies in using a cliffordian Cauchy-type operator, which behaves as an involution operator on higher order Lipschitz spaces.","The result obtained is a multidimensional sharpened version of the Hardy decomposition of H\\\"older continuous functions on a simple closed curve in the complex plane."],"url":"http://arxiv.org/abs/2404.16447v1","category":"math.CV"}
{"created":"2024-04-25 09:22:31","title":"Tightening I/O Lower Bounds through the Hourglass Dependency Pattern","abstract":"When designing an algorithm, one cares about arithmetic/computational complexity, but data movement (I/O) complexity plays an increasingly important role that highly impacts performance and energy consumption. For a given algorithm and a given I/O model, scheduling strategies such as loop tiling can reduce the required I/O down to a limit, called the I/O complexity, inherent to the algorithm itself. The objective of I/O complexity analysis is to compute, for a given program, its minimal I/O requirement among all valid schedules. We consider a sequential execution model with two memories, an infinite one, and a small one of size S on which the computations retrieve and produce data. The I/O is the number of reads and writes between the two memories. We identify a common \"hourglass pattern\" in the dependency graphs of several common linear algebra kernels. Using the properties of this pattern, we mathematically prove tighter lower bounds on their I/O complexity, which improves the previous state-of-the-art bound by a parametric ratio. This proof was integrated inside the IOLB automatic lower bound derivation tool.","sentences":["When designing an algorithm, one cares about arithmetic/computational complexity, but data movement (I/O) complexity plays an increasingly important role that highly impacts performance and energy consumption.","For a given algorithm and a given I/O model, scheduling strategies such as loop tiling can reduce the required I/O down to a limit, called the I/O complexity, inherent to the algorithm itself.","The objective of I/O complexity analysis is to compute, for a given program, its minimal I/O requirement among all valid schedules.","We consider a sequential execution model with two memories, an infinite one, and a small one of size S on which the computations retrieve and produce data.","The I/O is the number of reads and writes between the two memories.","We identify a common \"hourglass pattern\" in the dependency graphs of several common linear algebra kernels.","Using the properties of this pattern, we mathematically prove tighter lower bounds on their I/O complexity, which improves the previous state-of-the-art bound by a parametric ratio.","This proof was integrated inside the IOLB automatic lower bound derivation tool."],"url":"http://arxiv.org/abs/2404.16443v1","category":"cs.CC"}
{"created":"2024-04-25 09:20:34","title":"Quantification of 2D Interfaces: Quality of heterostructures, and what is inside a nanobubble","abstract":"Trapped materials at the interfaces of two-dimensional heterostructures (HS) lead to reduced coupling between the layers, resulting in degraded optoelectronic performance and device variability. Further, nanobubbles can form at the interface during transfer or after annealing. The question of what is inside a nanobubble, i.e. the trapped material, remains unanswered, limiting the studies and applications of these nanobubble systems. In this work, we report two key advances. Firstly, we quantify the interface quality using RAW-format optical imaging, and distinguish between ideal and non-ideal interfaces. The HS-substrate ratio value is calculated using a transfer matrix model, and is able to detect the presence of trapped layers. The second key advance is identification of water as the trapped material inside a nanobubble. To the best of our knowledge, this is the first study to show that optical imaging alone can quantify interface quality, and find the type of trapped material inside spontaneously formed nanobubbles. We also define a quality index parameter to quantify the interface quality of HS. Quantitative measurement of the interface will help answer the question whether annealing is necessary during HS preparation, and will enable creation of complex HS with small twist angles. Identification of the trapped materials will pave the way towards using nanobubbles for novel optical and engineering applications.","sentences":["Trapped materials at the interfaces of two-dimensional heterostructures (HS) lead to reduced coupling between the layers, resulting in degraded optoelectronic performance and device variability.","Further, nanobubbles can form at the interface during transfer or after annealing.","The question of what is inside a nanobubble, i.e. the trapped material, remains unanswered, limiting the studies and applications of these nanobubble systems.","In this work, we report two key advances.","Firstly, we quantify the interface quality using RAW-format optical imaging, and distinguish between ideal and non-ideal interfaces.","The HS-substrate ratio value is calculated using a transfer matrix model, and is able to detect the presence of trapped layers.","The second key advance is identification of water as the trapped material inside a nanobubble.","To the best of our knowledge, this is the first study to show that optical imaging alone can quantify interface quality, and find the type of trapped material inside spontaneously formed nanobubbles.","We also define a quality index parameter to quantify the interface quality of HS.","Quantitative measurement of the interface will help answer the question whether annealing is necessary during HS preparation, and will enable creation of complex HS with small twist angles.","Identification of the trapped materials will pave the way towards using nanobubbles for novel optical and engineering applications."],"url":"http://arxiv.org/abs/2404.16441v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 09:20:22","title":"Toeplitz Operators on Weighted Bergman Spaces over Tubular Domains","abstract":"In this paper, we mainly study the necessary and sufficient conditions for the boundedness and compactness of Toeplitz operators on weighted Bergman spaces over a tubular domains by using the Carlson measures on tubular domains. We also give some related results about Carlson measures.","sentences":["In this paper, we mainly study the necessary and sufficient conditions for the boundedness and compactness of Toeplitz operators on weighted Bergman spaces over a tubular domains by using the Carlson measures on tubular domains.","We also give some related results about Carlson measures."],"url":"http://arxiv.org/abs/2404.16439v1","category":"math.CV"}
{"created":"2024-04-25 09:10:53","title":"Ionic self-phoresis maps onto correlation-induced self-phoresis","abstract":"We re-examine the self-phoresis of a particle that releases(removes) pairs of ions into(from) the electrolyte solution. We show analytically that in the linear regime the mathematical description of this system maps onto that of the correlation-induced (self-)chemophoresis (CICP). This connection provides a unifying perspective of the two phenomena, within which one recovers and extends recent predictions as particular instances of CICP. Conversely, ion-phoretic particles are identified as candidates for experimental investigations into the rich variety of motility patterns predicted by CICP.","sentences":["We re-examine the self-phoresis of a particle that releases(removes) pairs of ions into(from) the electrolyte solution.","We show analytically that in the linear regime the mathematical description of this system maps onto that of the correlation-induced (self-)chemophoresis (CICP).","This connection provides a unifying perspective of the two phenomena, within which one recovers and extends recent predictions as particular instances of CICP.","Conversely, ion-phoretic particles are identified as candidates for experimental investigations into the rich variety of motility patterns predicted by CICP."],"url":"http://arxiv.org/abs/2404.16435v1","category":"cond-mat.soft"}
{"created":"2024-04-25 09:07:38","title":"Global existence of a strong solution to the initial value problem for the Nernst-Planck-Navier-Stokes system in $\\mathbb{R}^N$","abstract":"We study the existence of a strong solution to the initial value problem for the Nernst-Planck-Navier-Stokes (NPNS) system in $\\mathbb{R}^N, N\\geq 3$. We obtain a global in-time strong solution without any smallness assumptions on the initial data.","sentences":["We study the existence of a strong solution to the initial value problem for the Nernst-Planck-Navier-Stokes (NPNS) system in $\\mathbb{R}^N, N\\geq 3$.","We obtain a global in-time strong solution without any smallness assumptions on the initial data."],"url":"http://arxiv.org/abs/2404.16433v1","category":"math.AP"}
{"created":"2024-04-25 09:05:39","title":"Secure Coded Distributed Computing","abstract":"In this paper, we consider two critical aspects of security in the \\textit{distributed computing (DC)} model: \\textit{secure data shuffling} and \\textit{secure coded computing}. It is imperative that any external entity overhearing the transmissions does not gain any information about the \\textit{intermediate values (IVs)} exchanged during the shuffling phase of the DC model. Our approach ensures IV confidentiality during data shuffling. Moreover, each node in the system must be able to recover the IVs necessary for computing its output functions but must also remain oblivious to the IVs associated with output functions not assigned to it. We design secure DC methods and establish achievable limits on the tradeoffs between the communication and computation loads to contribute to the advancement of secure data processing in distributed systems.","sentences":["In this paper, we consider two critical aspects of security in the \\textit{distributed computing (DC)} model: \\textit{secure data shuffling} and \\textit{secure coded computing}.","It is imperative that any external entity overhearing the transmissions does not gain any information about the \\textit{intermediate values (IVs)} exchanged during the shuffling phase of the DC model.","Our approach ensures IV confidentiality during data shuffling.","Moreover, each node in the system must be able to recover the IVs necessary for computing its output functions but must also remain oblivious to the IVs associated with output functions not assigned to it.","We design secure DC methods and establish achievable limits on the tradeoffs between the communication and computation loads to contribute to the advancement of secure data processing in distributed systems."],"url":"http://arxiv.org/abs/2404.16431v1","category":"cs.IT"}
{"created":"2024-04-25 08:50:00","title":"Magnetocapacitance oscillations dominated by giant Rashba spin orbit interaction in InAs/GaSb quantum wells separated by AlSb barrier","abstract":"We observed magnetocapacitance oscillations in InAs/GaSb quantum wells separated by a $20$\\,nm AlSb middle barrier. By realizing independent ohmic contacts for electrons in InAs and holes in the GaSb layer, we found an out-of-plane oscillatory response in capacitance representing the density of states of this system. We were able to tune the charge carrier densities by applying a DC bias voltage, identifying the formation of beating signatures for forward bias. The coexistence of two distinguishable two dimensional charge carrier systems of unequal densities was verified. The corresponding Landau phase diagram presents distinct features originating from the two observed densities. A giant Rashba coefficient ranging from $430-612$\\,meV$\\text{\\AA}$ and large \\textit{g}-factor value underlines the influence of spin orbit interaction.","sentences":["We observed magnetocapacitance oscillations in InAs/GaSb quantum wells separated by a $20$\\,nm AlSb middle barrier.","By realizing independent ohmic contacts for electrons in InAs and holes in the GaSb layer, we found an out-of-plane oscillatory response in capacitance representing the density of states of this system.","We were able to tune the charge carrier densities by applying a DC bias voltage, identifying the formation of beating signatures for forward bias.","The coexistence of two distinguishable two dimensional charge carrier systems of unequal densities was verified.","The corresponding Landau phase diagram presents distinct features originating from the two observed densities.","A giant Rashba coefficient ranging from $430-612$\\,meV$\\text{\\AA}$ and large \\textit{g}-factor value underlines the influence of spin orbit interaction."],"url":"http://arxiv.org/abs/2404.16419v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 08:47:52","title":"A quantitative theory for heterogeneous combustion of nonvolatile metal particles in the diffusion-limited regime","abstract":"The paper presents an analytical theory quantitatively describing the heterogeneous combustion of nonvolatile (metal) particles in the diffusion-limited regime. It is assumed that the particle is suspended in an unconfined, isobaric, quiescent gaseous mixture and the chemisorption of the oxygen takes place evenly on the particle surface. The exact solution of the particle burn time is derived from the conservation equations of the gas-phase described in a spherical coordinate system with the utilization of constant thermophysical properties, evaluated at a reference film layer. This solution inherently takes the Stefan flow into account. The approximate expression of the time-dependent particle temperature is solved from the conservation of the particle enthalpy by neglecting the higher order terms in the Taylor expansion of the product of the transient particle density and diameter squared. Coupling the solutions for the burn time and time-dependent particle temperature provides quantitative results when initial and boundary conditions are specified. The theory is employed to predict the burn time and temperature of micro-sized iron particles, which are then compared with measurements, as the first validation case. The theoretical burn time agrees with the experiments almost perfectly at both low and high oxygen levels. The calculated particle temperature matches the measurements fairly well at relatively low oxygen mole fractions, whereas the theory overpredict the particle peak temperature due to the neglect of evaporation and the possible transition of the combustion regime.","sentences":["The paper presents an analytical theory quantitatively describing the heterogeneous combustion of nonvolatile (metal) particles in the diffusion-limited regime.","It is assumed that the particle is suspended in an unconfined, isobaric, quiescent gaseous mixture and the chemisorption of the oxygen takes place evenly on the particle surface.","The exact solution of the particle burn time is derived from the conservation equations of the gas-phase described in a spherical coordinate system with the utilization of constant thermophysical properties, evaluated at a reference film layer.","This solution inherently takes the Stefan flow into account.","The approximate expression of the time-dependent particle temperature is solved from the conservation of the particle enthalpy by neglecting the higher order terms in the Taylor expansion of the product of the transient particle density and diameter squared.","Coupling the solutions for the burn time and time-dependent particle temperature provides quantitative results when initial and boundary conditions are specified.","The theory is employed to predict the burn time and temperature of micro-sized iron particles, which are then compared with measurements, as the first validation case.","The theoretical burn time agrees with the experiments almost perfectly at both low and high oxygen levels.","The calculated particle temperature matches the measurements fairly well at relatively low oxygen mole fractions, whereas the theory overpredict the particle peak temperature due to the neglect of evaporation and the possible transition of the combustion regime."],"url":"http://arxiv.org/abs/2404.16415v1","category":"physics.flu-dyn"}
{"created":"2024-04-25 08:46:08","title":"Validating a lutetium frequency reference","abstract":"We review our progress in developing a frequency reference with singly ionized lutetium and give estimates of the levels of inaccuracy we expect to achieve in the near future with both the $^1S_0\\leftrightarrow{}^3D_1$ and $^1S_0\\leftrightarrow{}^3D_2$ transitions. Based on established experimental results, we show that inaccuracies at the low $10^{-19}$ level are readily achievable for the $^1S_0\\leftrightarrow{}^3D_1$ transition, and the frequency ratio between the two transitions is limited almost entirely by the BBR shift. We argue that the frequency ratio measured within the one apparatus provides a well-defined metric to compare and establish the performance of remotely located systems. For the measurement of an in situ frequency ratio, relativistic shifts drop out and both transitions experience the same electromagnetic environment. Consequently, the uncertainty budget for the ratio is practically identical to the uncertainty budgets for the individual transitions. If the ratios for two or more systems disagree we can be certain at least one of the clock assessments is incorrect. If they agree, subsequent comparisons on one transition would only differ by relativistic effects. Since motional effects are easily assessed and typically small for a heavy ion, only the differential gravitational red-shift will significantly contribute and this can be confirmed by comparison on the second transition.","sentences":["We review our progress in developing a frequency reference with singly ionized lutetium and give estimates of the levels of inaccuracy we expect to achieve in the near future with both the $^1S_0\\leftrightarrow{}^3D_1$ and $^1S_0\\leftrightarrow{}^3D_2$ transitions.","Based on established experimental results, we show that inaccuracies at the low $10^{-19}$ level are readily achievable for the $^1S_0\\leftrightarrow{}^3D_1$ transition, and the frequency ratio between the two transitions is limited almost entirely by the BBR shift.","We argue that the frequency ratio measured within the one apparatus provides a well-defined metric to compare and establish the performance of remotely located systems.","For the measurement of an in situ frequency ratio, relativistic shifts drop out and both transitions experience the same electromagnetic environment.","Consequently, the uncertainty budget for the ratio is practically identical to the uncertainty budgets for the individual transitions.","If the ratios for two or more systems disagree we can be certain at least one of the clock assessments is incorrect.","If they agree, subsequent comparisons on one transition would only differ by relativistic effects.","Since motional effects are easily assessed and typically small for a heavy ion, only the differential gravitational red-shift will significantly contribute and this can be confirmed by comparison on the second transition."],"url":"http://arxiv.org/abs/2404.16414v1","category":"physics.atom-ph"}
{"created":"2024-04-25 08:42:16","title":"Distributed Matrix Pencil Formulations for Prescribed-Time Leader-Following Consensus of MASs with Unknown Sensor Sensitivity","abstract":"In this paper, we address the problem of prescribed-time leader-following consensus of heterogeneous multi-agent systems (MASs) in the presence of unknown sensor sensitivity. Under a connected undirected topology, we propose a time-varying dual observer/controller design framework that makes use of regular local and inaccurate feedback to achieve consensus tracking within a prescribed time. In particular, the developed analysis framework is applicable to MASs equipped with sensors of different sensitivities. One of the design innovations involves constructing a distributed matrix pencil formulation based on worst-case sensors, yielding control parameters with sufficient robustness yet relatively low conservatism. Another novelty is the construction of the control gains, which consists of the product of a proportional coefficient obtained from the matrix pencil formulation and a classic time-varying function that grows to infinity or a novel bounded time-varying function. Furthermore, it is possible to extend the prescribed-time distributed protocol to infinite time domain by introducing the bounded time-varying gain technique without sacrificing the ultimate control accuracy, and the corresponding technical proof is comprehensive. The effectiveness of the method is demonstrated through a group of 5 single-link robot manipulators.","sentences":["In this paper, we address the problem of prescribed-time leader-following consensus of heterogeneous multi-agent systems (MASs) in the presence of unknown sensor sensitivity.","Under a connected undirected topology, we propose a time-varying dual observer/controller design framework that makes use of regular local and inaccurate feedback to achieve consensus tracking within a prescribed time.","In particular, the developed analysis framework is applicable to MASs equipped with sensors of different sensitivities.","One of the design innovations involves constructing a distributed matrix pencil formulation based on worst-case sensors, yielding control parameters with sufficient robustness yet relatively low conservatism.","Another novelty is the construction of the control gains, which consists of the product of a proportional coefficient obtained from the matrix pencil formulation and a classic time-varying function that grows to infinity or a novel bounded time-varying function.","Furthermore, it is possible to extend the prescribed-time distributed protocol to infinite time domain by introducing the bounded time-varying gain technique without sacrificing the ultimate control accuracy, and the corresponding technical proof is comprehensive.","The effectiveness of the method is demonstrated through a group of 5 single-link robot manipulators."],"url":"http://arxiv.org/abs/2404.16412v1","category":"eess.SY"}
{"created":"2024-04-25 08:37:55","title":"Detecting self-organising patterns in crowd motion: Effect of optimisation algorithms","abstract":"The escalating process of urbanization has raised concerns about incidents arising from overcrowding, necessitating a deep understanding of large human crowd behavior and the development of effective crowd management strategies. This study employs computational methods to analyze real-world crowd behaviors, emphasizing self-organizing patterns. Notably, the intersection of two streams of individuals triggers the spontaneous emergence of striped patterns, validated through both simulations and live human experiments. Addressing a gap in computational methods for studying these patterns, previous research utilized the pattern-matching technique, employing the Nelder-Mead Simplex algorithm for fitting a two-dimensional sinusoidal function to pedestrian coordinates. This paper advances the pattern-matching procedure by introducing Simulated Annealing as the optimization algorithm and employing a two-dimensional square wave for data fitting. The amalgamation of Simulated Annealing and the square wave significantly enhances pattern fitting quality, validated through statistical hypothesis tests. The study concludes by outlining potential applications of this method across diverse scenarios.","sentences":["The escalating process of urbanization has raised concerns about incidents arising from overcrowding, necessitating a deep understanding of large human crowd behavior and the development of effective crowd management strategies.","This study employs computational methods to analyze real-world crowd behaviors, emphasizing self-organizing patterns.","Notably, the intersection of two streams of individuals triggers the spontaneous emergence of striped patterns, validated through both simulations and live human experiments.","Addressing a gap in computational methods for studying these patterns, previous research utilized the pattern-matching technique, employing the Nelder-Mead Simplex algorithm for fitting a two-dimensional sinusoidal function to pedestrian coordinates.","This paper advances the pattern-matching procedure by introducing Simulated Annealing as the optimization algorithm and employing a two-dimensional square wave for data fitting.","The amalgamation of Simulated Annealing and the square wave significantly enhances pattern fitting quality, validated through statistical hypothesis tests.","The study concludes by outlining potential applications of this method across diverse scenarios."],"url":"http://arxiv.org/abs/2404.16410v1","category":"math.OC"}
{"created":"2024-04-25 08:33:08","title":"Lost in Recursion: Mining Rich Event Semantics in Knowledge Graphs","abstract":"Our world is shaped by events of various complexity. This includes both small-scale local events like local farmer markets and large complex events like political and military conflicts. The latter are typically not observed directly but through the lenses of intermediaries like newspapers or social media. In other words, we do not witness the unfolding of such events directly but are confronted with narratives surrounding them. Such narratives capture different aspects of a complex event and may also differ with respect to the narrator. Thus, they provide a rich semantics concerning real-world events. In this paper, we show how narratives concerning complex events can be constructed and utilized. We provide a formal representation of narratives based on recursive nodes to represent multiple levels of detail and discuss how narratives can be bound to event-centric knowledge graphs. Additionally, we provide an algorithm based on incremental prompting techniques that mines such narratives from texts to account for different perspectives on complex events. Finally, we show the effectiveness and future research directions in a proof of concept.","sentences":["Our world is shaped by events of various complexity.","This includes both small-scale local events like local farmer markets and large complex events like political and military conflicts.","The latter are typically not observed directly but through the lenses of intermediaries like newspapers or social media.","In other words, we do not witness the unfolding of such events directly but are confronted with narratives surrounding them.","Such narratives capture different aspects of a complex event and may also differ with respect to the narrator.","Thus, they provide a rich semantics concerning real-world events.","In this paper, we show how narratives concerning complex events can be constructed and utilized.","We provide a formal representation of narratives based on recursive nodes to represent multiple levels of detail and discuss how narratives can be bound to event-centric knowledge graphs.","Additionally, we provide an algorithm based on incremental prompting techniques that mines such narratives from texts to account for different perspectives on complex events.","Finally, we show the effectiveness and future research directions in a proof of concept."],"url":"http://arxiv.org/abs/2404.16405v1","category":"cs.CL"}
{"created":"2024-04-25 08:28:50","title":"Anomalous magnetic transition in a disordered quasicrystal approximant with heavy-fermion nature","abstract":"Quasicrystal approximant (CexY1-x)Cd6 (0 < x < 1) forms a network of corner-sharing octahedra. We report that (Ce0.8Y0.2)Cd6 exhibits an anomalous magnetic transition which can be classified neither into the conventional static magnetic ordering nor into spin glasses. The anomalous transition is characterized by the coexistence of a static order and a frequency-dependent sharp positive anomaly in the 3rd-harmonic susceptibility. Based on the investigation of the reference systems CeCd6 and (Ce0.05Y0.95)Cd6, we speculate that the anomalous transition could be induced by disorder in the possible frustrated Ce-network in the presence of the Kondo effect.","sentences":["Quasicrystal approximant (CexY1-x)Cd6 (0 < x < 1) forms a network of corner-sharing octahedra.","We report that (Ce0.8Y0.2)Cd6 exhibits an anomalous magnetic transition which can be classified neither into the conventional static magnetic ordering nor into spin glasses.","The anomalous transition is characterized by the coexistence of a static order and a frequency-dependent sharp positive anomaly in the 3rd-harmonic susceptibility.","Based on the investigation of the reference systems CeCd6 and (Ce0.05Y0.95)Cd6, we speculate that the anomalous transition could be induced by disorder in the possible frustrated Ce-network in the presence of the Kondo effect."],"url":"http://arxiv.org/abs/2404.16402v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 17:56:45","title":"Meta-Transfer Derm-Diagnosis: Exploring Few-Shot Learning and Transfer Learning for Skin Disease Classification in Long-Tail Distribution","abstract":"Addressing the challenges of rare diseases is difficult, especially with the limited number of reference images and a small patient population. This is more evident in rare skin diseases, where we encounter long-tailed data distributions that make it difficult to develop unbiased and broadly effective models. The diverse ways in which image datasets are gathered and their distinct purposes also add to these challenges. Our study conducts a detailed examination of the benefits and drawbacks of episodic and conventional training methodologies, adopting a few-shot learning approach alongside transfer learning. We evaluated our models using the ISIC2018, Derm7pt, and SD-198 datasets. With minimal labeled examples, our models showed substantial information gains and better performance compared to previously trained models. Our research emphasizes the improved ability to represent features in DenseNet121 and MobileNetV2 models, achieved by using pre-trained models on ImageNet to increase similarities within classes. Moreover, our experiments, ranging from 2-way to 5-way classifications with up to 10 examples, showed a growing success rate for traditional transfer learning methods as the number of examples increased. The addition of data augmentation techniques significantly improved our transfer learning based model performance, leading to higher performances than existing methods, especially in the SD-198 and ISIC2018 datasets. All source code related to this work will be made publicly available soon at the provided URL.","sentences":["Addressing the challenges of rare diseases is difficult, especially with the limited number of reference images and a small patient population.","This is more evident in rare skin diseases, where we encounter long-tailed data distributions that make it difficult to develop unbiased and broadly effective models.","The diverse ways in which image datasets are gathered and their distinct purposes also add to these challenges.","Our study conducts a detailed examination of the benefits and drawbacks of episodic and conventional training methodologies, adopting a few-shot learning approach alongside transfer learning.","We evaluated our models using the ISIC2018, Derm7pt, and SD-198 datasets.","With minimal labeled examples, our models showed substantial information gains and better performance compared to previously trained models.","Our research emphasizes the improved ability to represent features in DenseNet121 and MobileNetV2 models, achieved by using pre-trained models on ImageNet to increase similarities within classes.","Moreover, our experiments, ranging from 2-way to 5-way classifications with up to 10 examples, showed a growing success rate for traditional transfer learning methods as the number of examples increased.","The addition of data augmentation techniques significantly improved our transfer learning based model performance, leading to higher performances than existing methods, especially in the SD-198 and ISIC2018 datasets.","All source code related to this work will be made publicly available soon at the provided URL."],"url":"http://arxiv.org/abs/2404.16814v1","category":"cs.CV"}
{"created":"2024-04-25 17:49:51","title":"Non-supersymmetric duality cascade of QCD(BF) via semiclassics on $\\mathbb{R}^2\\times T^2$ with the baryon-'t Hooft flux","abstract":"We study the phase diagrams of the bifundamental QCD (QCD(BF)) of different ranks, which is the $4$d $SU(N_1) \\times SU(N_2)$ gauge theory coupled with a bifundamental Dirac fermion. After discussing the anomaly constraints on possible vacuum structures, we apply a novel semiclassical approach on $\\mathbb{R}^2\\times T^2$ with the baryon-'t Hooft flux to obtain the concrete dynamics. The $2$d effective theory is derived by the dilute gas approximation of center vortices, and it serves as the basis for determining the phase diagram of the model under the assumption of adiabatic continuity. As an application, we justify the non-supersymmetric duality cascade between different QCD(BF), which has been conjectured in the large-${N}$ argument. Combined with the semiclassics and the large-$N_{1,2}$ limit, we construct the explicit duality map from the parent theory, $SU(N_1) \\times SU(N_2)$ QCD(BF), to the daughter theory, $SU(N_1) \\times SU(N_2-N_1)$ QCD(BF), including the correspondence of the coupling constants. We numerically examine the validity of the duality also for finite $N_{1,2}$ within our semiclassics, finding a remarkable agreement of the phase diagrams between the parent and daughter sides.","sentences":["We study the phase diagrams of the bifundamental QCD (QCD(BF)) of different ranks, which is the $4$d $SU(N_1)","\\times SU(N_2)$ gauge theory coupled with a bifundamental Dirac fermion.","After discussing the anomaly constraints on possible vacuum structures, we apply a novel semiclassical approach on $\\mathbb{R}^2\\times T^2$ with the baryon-'t Hooft flux to obtain the concrete dynamics.","The $2$d effective theory is derived by the dilute gas approximation of center vortices, and it serves as the basis for determining the phase diagram of the model under the assumption of adiabatic continuity.","As an application, we justify the non-supersymmetric duality cascade between different QCD(BF), which has been conjectured in the large-${N}$ argument.","Combined with the semiclassics and the large-$N_{1,2}$ limit, we construct the explicit duality map from the parent theory, $SU(N_1)","\\times SU(N_2)$ QCD(BF), to the daughter theory, $SU(N_1)","\\times SU(N_2-N_1)$ QCD(BF), including the correspondence of the coupling constants.","We numerically examine the validity of the duality also for finite $N_{1,2}$ within our semiclassics, finding a remarkable agreement of the phase diagrams between the parent and daughter sides."],"url":"http://arxiv.org/abs/2404.16803v1","category":"hep-th"}
{"created":"2024-04-25 17:49:28","title":"Transformer-Based Local Feature Matching for Multimodal Image Registration","abstract":"Ultrasound imaging is a cost-effective and radiation-free modality for visualizing anatomical structures in real-time, making it ideal for guiding surgical interventions. However, its limited field-of-view, speckle noise, and imaging artifacts make it difficult to interpret the images for inexperienced users. In this paper, we propose a new 2D ultrasound to 3D CT registration method to improve surgical guidance during ultrasound-guided interventions. Our approach adopts a dense feature matching method called LoFTR to our multimodal registration problem. We learn to predict dense coarse-to-fine correspondences using a Transformer-based architecture to estimate a robust rigid transformation between a 2D ultrasound frame and a CT scan. Additionally, a fully differentiable pose estimation method is introduced, optimizing LoFTR on pose estimation error during training. Experiments conducted on a multimodal dataset of ex vivo porcine kidneys demonstrate the method's promising results for intraoperative, trackerless ultrasound pose estimation. By mapping 2D ultrasound frames into the 3D CT volume space, the method provides intraoperative guidance, potentially improving surgical workflows and image interpretation.","sentences":["Ultrasound imaging is a cost-effective and radiation-free modality for visualizing anatomical structures in real-time, making it ideal for guiding surgical interventions.","However, its limited field-of-view, speckle noise, and imaging artifacts make it difficult to interpret the images for inexperienced users.","In this paper, we propose a new 2D ultrasound to 3D CT registration method to improve surgical guidance during ultrasound-guided interventions.","Our approach adopts a dense feature matching method called LoFTR to our multimodal registration problem.","We learn to predict dense coarse-to-fine correspondences using a Transformer-based architecture to estimate a robust rigid transformation between a 2D ultrasound frame and a CT scan.","Additionally, a fully differentiable pose estimation method is introduced, optimizing LoFTR on pose estimation error during training.","Experiments conducted on a multimodal dataset of ex vivo porcine kidneys demonstrate the method's promising results for intraoperative, trackerless ultrasound pose estimation.","By mapping 2D ultrasound frames into the 3D CT volume space, the method provides intraoperative guidance, potentially improving surgical workflows and image interpretation."],"url":"http://arxiv.org/abs/2404.16802v1","category":"eess.IV"}
{"created":"2024-04-25 17:40:50","title":"Structure-Preserving Oscillation-Eliminating Discontinuous Galerkin Schemes for Ideal MHD Equations: Locally Divergence-Free and Positivity-Preserving","abstract":"This paper develops structure-preserving, oscillation-eliminating discontinuous Galerkin (OEDG) schemes for ideal magnetohydrodynamics (MHD), as a sequel to our recent work [Peng, Sun, and Wu, OEDG: Oscillation-eliminating discontinuous Galerkin method for hyperbolic conservation laws, 2023]. The schemes are based on a locally divergence-free (LDF) oscillation-eliminating (OE) procedure to suppress spurious oscillations while maintaining many of the good properties of original DG schemes, such as conservation, local compactness, and optimal convergence rates. The OE procedure is built on the solution operator of a novel damping equation -- a simple linear ordinary differential equation (ODE) whose exact solution can be exactly formulated. Because this OE procedure does not interfere with DG spatial discretization and RK stage update, it can be easily incorporated to existing DG codes as an independent module. These features make the proposed LDF OEDG schemes highly efficient and easy to implement.In addition, we present a positivity-preserving (PP) analysis of the LDF OEDG schemes on Cartesian meshes via the optimal convex decomposition technique and the geometric quasi-linearization (GQL) approach. Efficient PP LDF OEDG schemes are obtained with the HLL flux under a condition accessible by the simple local scaling PP limiter.Several one- and two-dimensional MHD tests confirm the accuracy, effectiveness, and robustness of the proposed structure-preserving OEDG schemes.","sentences":["This paper develops structure-preserving, oscillation-eliminating discontinuous Galerkin (OEDG) schemes for ideal magnetohydrodynamics (MHD), as a sequel to our recent work [Peng, Sun, and Wu, OEDG: Oscillation-eliminating discontinuous Galerkin method for hyperbolic conservation laws, 2023].","The schemes are based on a locally divergence-free (LDF) oscillation-eliminating (OE) procedure to suppress spurious oscillations while maintaining many of the good properties of original DG schemes, such as conservation, local compactness, and optimal convergence rates.","The OE procedure is built on the solution operator of a novel damping equation -- a simple linear ordinary differential equation (ODE) whose exact solution can be exactly formulated.","Because this OE procedure does not interfere with DG spatial discretization and RK stage update, it can be easily incorporated to existing DG codes as an independent module.","These features make the proposed LDF OEDG schemes highly efficient and easy to implement.","In addition, we present a positivity-preserving (PP) analysis of the LDF OEDG schemes on Cartesian meshes via the optimal convex decomposition technique and the geometric quasi-linearization (GQL) approach.","Efficient PP LDF OEDG schemes are obtained with the HLL flux under a condition accessible by the simple local scaling PP limiter.","Several one-","and two-dimensional MHD tests confirm the accuracy, effectiveness, and robustness of the proposed structure-preserving OEDG schemes."],"url":"http://arxiv.org/abs/2404.16794v1","category":"math.NA"}
{"created":"2024-04-25 17:26:58","title":"Estimating Metocean Environments Associated with Extreme Structural Response","abstract":"Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures. Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis. Environmental contours provide an alternate approach to estimating structural integrity, without requiring structural knowledge. These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space. These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design. We introduce a methodology for full probabilistic analysis to estimate the joint probability density of the environment, conditional on the occurrence of an extreme structural response, for simple structures. We use this conditional density of the environment as a basis to assess the performance of different environmental contour methods. We demonstrate the difficulty of estimating the contour boundary in the environment space for typical data samples, as well as the dependence of the performance of the environmental contour on the structure being considered.","sentences":["Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures.","Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis.","Environmental contours provide an alternate approach to estimating structural integrity, without requiring structural knowledge.","These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space.","These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design.","We introduce a methodology for full probabilistic analysis to estimate the joint probability density of the environment, conditional on the occurrence of an extreme structural response, for simple structures.","We use this conditional density of the environment as a basis to assess the performance of different environmental contour methods.","We demonstrate the difficulty of estimating the contour boundary in the environment space for typical data samples, as well as the dependence of the performance of the environmental contour on the structure being considered."],"url":"http://arxiv.org/abs/2404.16775v1","category":"stat.ME"}
{"created":"2024-04-25 17:23:31","title":"Pseudogap phase as fluctuating pair density wave","abstract":"The physical nature of pseudogap phase is one of the most important and intriguing problems towards understanding the key mechanism of high temperature superconductivity in cuprates. Theoretically, the square-lattice $t$-$J$ model is widely believed to be the simplest toy model that captures the essential physics of cuprate superconductors. We employ the Grassmann tensor product state approach to investigate uniform states in the underdoped ($\\delta \\lesssim 0.1$) region. In addition to the previously known uniform $d$-wave state, we discover a strongly fluctuating pair density wave (PDW) state with wave vector $Q = (\\pi, \\pi)$. This fluctuating PDW state weakly breaks the $C_4$ rotational symmetry of the square lattice and has a lower or comparable energy to the $d$-wave state (depending on doping and the $t/J$ ratio), making it a promising candidate state for describing the pseudogap phase.","sentences":["The physical nature of pseudogap phase is one of the most important and intriguing problems towards understanding the key mechanism of high temperature superconductivity in cuprates.","Theoretically, the square-lattice $t$-$J$ model is widely believed to be the simplest toy model that captures the essential physics of cuprate superconductors.","We employ the Grassmann tensor product state approach to investigate uniform states in the underdoped ($\\delta \\lesssim 0.1$) region.","In addition to the previously known uniform $d$-wave state, we discover a strongly fluctuating pair density wave (PDW) state with wave vector $Q = (\\pi, \\pi)$.","This fluctuating PDW state weakly breaks the $C_4$ rotational symmetry of the square lattice and has a lower or comparable energy to the $d$-wave state (depending on doping and the $t/J$ ratio), making it a promising candidate state for describing the pseudogap phase."],"url":"http://arxiv.org/abs/2404.16770v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 17:00:24","title":"Estimating the Number of Components in Finite Mixture Models via Variational Approximation","abstract":"This work introduces a new method for selecting the number of components in finite mixture models (FMMs) using variational Bayes, inspired by the large-sample properties of the Evidence Lower Bound (ELBO) derived from mean-field (MF) variational approximation. Specifically, we establish matching upper and lower bounds for the ELBO without assuming conjugate priors, suggesting the consistency of model selection for FMMs based on maximizing the ELBO. As a by-product of our proof, we demonstrate that the MF approximation inherits the stable behavior (benefited from model singularity) of the posterior distribution, which tends to eliminate the extra components under model misspecification where the number of mixture components is over-specified. This stable behavior also leads to the $n^{-1/2}$ convergence rate for parameter estimation, up to a logarithmic factor, under this model overspecification. Empirical experiments are conducted to validate our theoretical findings and compare with other state-of-the-art methods for selecting the number of components in FMMs.","sentences":["This work introduces a new method for selecting the number of components in finite mixture models (FMMs) using variational Bayes, inspired by the large-sample properties of the Evidence Lower Bound (ELBO) derived from mean-field (MF) variational approximation.","Specifically, we establish matching upper and lower bounds for the ELBO without assuming conjugate priors, suggesting the consistency of model selection for FMMs based on maximizing the ELBO.","As a by-product of our proof, we demonstrate that the MF approximation inherits the stable behavior (benefited from model singularity) of the posterior distribution, which tends to eliminate the extra components under model misspecification where the number of mixture components is over-specified.","This stable behavior also leads to the $n^{-1/2}$ convergence rate for parameter estimation, up to a logarithmic factor, under this model overspecification.","Empirical experiments are conducted to validate our theoretical findings and compare with other state-of-the-art methods for selecting the number of components in FMMs."],"url":"http://arxiv.org/abs/2404.16746v1","category":"stat.ME"}
{"created":"2024-04-25 16:42:28","title":"The MOPYS project: A survey of 70 planets in search of extended He I and H atmospheres. No evidence of enhanced evaporation in young planets","abstract":"During the first Gyr of their life, exoplanet atmospheres suffer from different atmospheric escape phenomena that can strongly affect the shape and morphology of the exoplanet itself. These processes can be studied with Ly$\\alpha$, H$\\alpha$ and/or He I triplet observations. We present high-resolution spectroscopy observations from CARMENES and GIARPS checking for He I and H$\\alpha$ signals in 20 exoplanetary atmospheres: V1298Tau c, K2-100b, HD63433b, HD63433c, HD73583b, HD73583c, K2-77b, TOI-2076b, TOI-2048b, HD235088b, TOI-1807b, TOI-1136d, TOI-1268b, TOI-1683b, TOI-2018b, MASCARA-2b, WASP-189b, TOI-2046b, TOI-1431b, and HAT-P-57b. We report two new high-resolution spectroscopy He I detections for TOI-1268b and TOI-2018b, and an H$\\alpha$ detection for TOI-1136d. The MOPYS (Measuring Out-flows in Planets orbiting Young Stars) project aims to understand the evaporating phenomena and test their predictions from the current observations. We compiled a list of 70 exoplanets with He I and/or H$\\alpha$ observations, from this work and the literature, and we considered the He I and H$\\alpha$ results as proxy for atmospheric escape. Our principal results are that 0.1-1Gyr-old planets do not exhibit more He I or H$\\alpha$ detections than older planets, and evaporation signals are more frequent for planets orbiting $\\sim$1-3Gyr-old stars. We provide new constrains to the cosmic shoreline, the empirical division between rocky planets and planets with atmosphere, by using the evaporation detections and explore the capabilities of a new dimensionless parameter, $R_{\\rm He}/R_{\\rm Hill}$, to explain the He I triplet detections. Furthermore, we present a statistically significant upper boundary for the He I triplet detections in the $T_{\\rm eq}$ vs $\\rho_{\\rm p}$ parameter space. Planets located above that boundary are unlikely to show He I absorption signals.","sentences":["During the first Gyr of their life, exoplanet atmospheres suffer from different atmospheric escape phenomena that can strongly affect the shape and morphology of the exoplanet itself.","These processes can be studied with Ly$\\alpha$, H$\\alpha$ and/or He I triplet observations.","We present high-resolution spectroscopy observations from CARMENES and GIARPS checking for He I and H$\\alpha$ signals in 20 exoplanetary atmospheres: V1298Tau c, K2-100b, HD63433b, HD63433c, HD73583b, HD73583c, K2-77b, TOI-2076b, TOI-2048b, HD235088b, TOI-1807b, TOI-1136d, TOI-1268b, TOI-1683b, TOI-2018b, MASCARA-2b, WASP-189b, TOI-2046b, TOI-1431b, and HAT-P-57b.","We report two new high-resolution spectroscopy He I detections for TOI-1268b and TOI-2018b, and an H$\\alpha$ detection for TOI-1136d.","The MOPYS (Measuring Out-flows in Planets orbiting Young Stars) project aims to understand the evaporating phenomena and test their predictions from the current observations.","We compiled a list of 70 exoplanets with He I and/or H$\\alpha$ observations, from this work and the literature, and we considered the He I and H$\\alpha$ results as proxy for atmospheric escape.","Our principal results are that 0.1-1Gyr-old planets do not exhibit more He I or H$\\alpha$ detections than older planets, and evaporation signals are more frequent for planets orbiting $\\sim$1-3Gyr-old stars.","We provide new constrains to the cosmic shoreline, the empirical division between rocky planets and planets with atmosphere, by using the evaporation detections and explore the capabilities of a new dimensionless parameter, $R_{\\rm He}/R_{\\rm Hill}$, to explain the He I triplet detections.","Furthermore, we present a statistically significant upper boundary for the He I triplet detections in the $T_{\\rm eq}$ vs $\\rho_{\\rm p}$ parameter space.","Planets located above that boundary are unlikely to show He I absorption signals."],"url":"http://arxiv.org/abs/2404.16732v1","category":"astro-ph.EP"}
{"created":"2024-04-25 15:59:01","title":"High-Coherence Kerr-cat qubit in 2D architecture","abstract":"The Kerr-cat qubit is a bosonic qubit in which multi-photon Schrodinger cat states are stabilized by applying a two-photon drive to an oscillator with a Kerr nonlinearity. The suppressed bit-flip rate with increasing cat size makes this qubit a promising candidate to implement quantum error correction codes tailored for noise-biased qubits. However, achieving strong light-matter interactions necessary for stabilizing and controlling this qubit has traditionally required strong microwave drives that heat the qubit and degrade its performance. In contrast, increasing the coupling to the drive port removes the need for strong drives at the expense of large Purcell decay. By integrating an effective band-block filter on-chip, we overcome this trade-off and realize a Kerr-cat qubit in a scalable 2D superconducting circuit with high coherence. This filter provides 30 dB of isolation at the qubit frequency with negligible attenuation at the frequencies required for stabilization and readout. We experimentally demonstrate quantum non-demolition readout fidelity of 99.6% for a cat with 8 photons. Also, to have high-fidelity universal control over this qubit, we combine fast Rabi oscillations with a new demonstration of the X(90) gate through phase modulation of the stabilization drive. Finally, the lifetime in this architecture is examined as a function of the cat size of up to 10 photons in the oscillator achieving a bit-flip time higher than 1 ms and only a linear decrease in the phase-flip time, in good agreement with the theoretical analysis of the circuit. Our qubit shows promise as a building block for fault-tolerant quantum processors with a small footprint.","sentences":["The Kerr-cat qubit is a bosonic qubit in which multi-photon Schrodinger cat states are stabilized by applying a two-photon drive to an oscillator with a Kerr nonlinearity.","The suppressed bit-flip rate with increasing cat size makes this qubit a promising candidate to implement quantum error correction codes tailored for noise-biased qubits.","However, achieving strong light-matter interactions necessary for stabilizing and controlling this qubit has traditionally required strong microwave drives that heat the qubit and degrade its performance.","In contrast, increasing the coupling to the drive port removes the need for strong drives at the expense of large Purcell decay.","By integrating an effective band-block filter on-chip, we overcome this trade-off and realize a Kerr-cat qubit in a scalable 2D superconducting circuit with high coherence.","This filter provides 30 dB of isolation at the qubit frequency with negligible attenuation at the frequencies required for stabilization and readout.","We experimentally demonstrate quantum non-demolition readout fidelity of 99.6% for a cat with 8 photons.","Also, to have high-fidelity universal control over this qubit, we combine fast Rabi oscillations with a new demonstration of the X(90) gate through phase modulation of the stabilization drive.","Finally, the lifetime in this architecture is examined as a function of the cat size of up to 10 photons in the oscillator achieving a bit-flip time higher than 1 ms and only a linear decrease in the phase-flip time, in good agreement with the theoretical analysis of the circuit.","Our qubit shows promise as a building block for fault-tolerant quantum processors with a small footprint."],"url":"http://arxiv.org/abs/2404.16697v1","category":"quant-ph"}
{"created":"2024-04-25 15:16:53","title":"Magnetic Resonance Frequency Shift Caused by Nonuniform Field and Boundary Relaxation","abstract":"Magnetic field inhomogeneity is usually detrimental to magnetic resonance (MR) experiments. It is widely recognized that a nonuniform magnetic field can lead to an increase in the resonance line width, as well as a reduction in sensitivity and spectral resolution. However, nonuniform magnetic field can also cause shift in resonance frequency, which received far less attention. In this work, we investigate the frequency shift under arbitrary nonuniform magnetic field and boundary relaxation by applying perturbation theory to the Torrey equation. Several compact frequency shift formulas are reported. We find that the frequency shift is mainly determined by $B_z$ distribution (rather than the transverse field components in previous study) and has important dependence on boundary relaxation. Furthermore, due to the difference of boundary relaxation and high order perturbation correction, this frequency shift is spin-species dependent, which implies a systematic error in many MR based precision measurements such as NMR gyroscope and comagnetometers. This insight provides a potential tool for understanding the unexplained isotope shifts in recent NMR gyroscope and new physics searching experiments that utilize comagnetometers. Finally, we propose a new tool for wall interaction research based on the frequency shift's dependency on boundary relaxation.","sentences":["Magnetic field inhomogeneity is usually detrimental to magnetic resonance (MR) experiments.","It is widely recognized that a nonuniform magnetic field can lead to an increase in the resonance line width, as well as a reduction in sensitivity and spectral resolution.","However, nonuniform magnetic field can also cause shift in resonance frequency, which received far less attention.","In this work, we investigate the frequency shift under arbitrary nonuniform magnetic field and boundary relaxation by applying perturbation theory to the Torrey equation.","Several compact frequency shift formulas are reported.","We find that the frequency shift is mainly determined by $B_z$ distribution (rather than the transverse field components in previous study) and has important dependence on boundary relaxation.","Furthermore, due to the difference of boundary relaxation and high order perturbation correction, this frequency shift is spin-species dependent, which implies a systematic error in many MR based precision measurements such as NMR gyroscope and comagnetometers.","This insight provides a potential tool for understanding the unexplained isotope shifts in recent NMR gyroscope and new physics searching experiments that utilize comagnetometers.","Finally, we propose a new tool for wall interaction research based on the frequency shift's dependency on boundary relaxation."],"url":"http://arxiv.org/abs/2404.16671v1","category":"quant-ph"}
{"created":"2024-04-25 15:06:58","title":"PhyRecon: Physically Plausible Neural Scene Reconstruction","abstract":"While neural implicit representations have gained popularity in multi-view 3D reconstruction, previous work struggles to yield physically plausible results, thereby limiting their applications in physics-demanding domains like embodied AI and robotics. The lack of plausibility originates from both the absence of physics modeling in the existing pipeline and their inability to recover intricate geometrical structures. In this paper, we introduce PhyRecon, which stands as the first approach to harness both differentiable rendering and differentiable physics simulation to learn implicit surface representations. Our framework proposes a novel differentiable particle-based physical simulator seamlessly integrated with the neural implicit representation. At its core is an efficient transformation between SDF-based implicit representation and explicit surface points by our proposed algorithm, Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses. Moreover, we model both rendering and physical uncertainty to identify and compensate for the inconsistent and inaccurate monocular geometric priors. The physical uncertainty additionally enables a physics-guided pixel sampling to enhance the learning of slender structures. By amalgamating these techniques, our model facilitates efficient joint modeling with appearance, geometry, and physics. Extensive experiments demonstrate that PhyRecon significantly outperforms all state-of-the-art methods in terms of reconstruction quality. Our reconstruction results also yield superior physical stability, verified by Isaac Gym, with at least a 40% improvement across all datasets, opening broader avenues for future physics-based applications.","sentences":["While neural implicit representations have gained popularity in multi-view 3D reconstruction, previous work struggles to yield physically plausible results, thereby limiting their applications in physics-demanding domains like embodied AI and robotics.","The lack of plausibility originates from both the absence of physics modeling in the existing pipeline and their inability to recover intricate geometrical structures.","In this paper, we introduce PhyRecon, which stands as the first approach to harness both differentiable rendering and differentiable physics simulation to learn implicit surface representations.","Our framework proposes a novel differentiable particle-based physical simulator seamlessly integrated with the neural implicit representation.","At its core is an efficient transformation between SDF-based implicit representation and explicit surface points by our proposed algorithm, Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses.","Moreover, we model both rendering and physical uncertainty to identify and compensate for the inconsistent and inaccurate monocular geometric priors.","The physical uncertainty additionally enables a physics-guided pixel sampling to enhance the learning of slender structures.","By amalgamating these techniques, our model facilitates efficient joint modeling with appearance, geometry, and physics.","Extensive experiments demonstrate that PhyRecon significantly outperforms all state-of-the-art methods in terms of reconstruction quality.","Our reconstruction results also yield superior physical stability, verified by Isaac Gym, with at least a 40% improvement across all datasets, opening broader avenues for future physics-based applications."],"url":"http://arxiv.org/abs/2404.16666v1","category":"cs.CV"}
{"created":"2024-04-25 13:39:25","title":"High-Order regularity for fully nonlinear elliptic transmission problems under weak convexity assumption","abstract":"This paper studies Schauder theory to transmission problems modelled by fully nonlinear uniformly elliptic equations of second order. We focus on operators F that fails to be concave or convex in the space of symmetric matrices. In a first scenario, it is considered that F enjoys a small ellipticity aperture. In our second case, we study regularity results where the convexity of the superlevel (or sublevel) sets is verified, implying that the operator F is quasiconcave (or quasiconvex).","sentences":["This paper studies Schauder theory to transmission problems modelled by fully nonlinear uniformly elliptic equations of second order.","We focus on operators F that fails to be concave or convex in the space of symmetric matrices.","In a first scenario, it is considered that F enjoys a small ellipticity aperture.","In our second case, we study regularity results where the convexity of the superlevel (or sublevel) sets is verified, implying that the operator F is quasiconcave (or quasiconvex)."],"url":"http://arxiv.org/abs/2404.16602v1","category":"math.AP"}
{"created":"2024-04-25 13:14:37","title":"The hunt of PeVatrons as the origin of the most energetic photons observed in our Galaxy","abstract":"Ultrarelativistic particles called cosmic rays permeate the Milky Way, propagating through the Galactic turbulent magnetic fields. The mechanisms under which these particles increase their energy can be reasonably described by current theories of acceleration and propagation of cosmic rays. There are, however, still many open questions as to how to reach petaelectronvolt (PeV) energies, the maximum energy believed to be attained in our Galaxy, and in which astrophysical sources (dubbed {\\it PeVatrons}) this ultra-high energy acceleration happens. In this article, we describe the theoretical conditions for plasma acceleration to these energies, and the Galactic sources in which these conditions are possible. These theoretical predictions are then confronted with the latest experimental results, summarising the state-of-the-art of our current knowledge of PeVatrons. We finally describe the prospects to keep advancing the understanding of these elusive objects, still unidentified more than one hundred years after the discovery of cosmic rays.","sentences":["Ultrarelativistic particles called cosmic rays permeate the Milky Way, propagating through the Galactic turbulent magnetic fields.","The mechanisms under which these particles increase their energy can be reasonably described by current theories of acceleration and propagation of cosmic rays.","There are, however, still many open questions as to how to reach petaelectronvolt (PeV) energies, the maximum energy believed to be attained in our Galaxy, and in which astrophysical sources (dubbed {\\it PeVatrons}) this ultra-high energy acceleration happens.","In this article, we describe the theoretical conditions for plasma acceleration to these energies, and the Galactic sources in which these conditions are possible.","These theoretical predictions are then confronted with the latest experimental results, summarising the state-of-the-art of our current knowledge of PeVatrons.","We finally describe the prospects to keep advancing the understanding of these elusive objects, still unidentified more than one hundred years after the discovery of cosmic rays."],"url":"http://arxiv.org/abs/2404.16591v1","category":"astro-ph.HE"}
{"created":"2024-04-25 12:34:23","title":"MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth Estimation of Endoscopic Images","abstract":"Photometric constraint is indispensable for self-supervised monocular depth estimation. It involves warping a source image onto a target view using estimated depth&pose, and then minimizing the difference between the warped and target images. However, the endoscopic built-in light causes significant brightness fluctuations, and thus makes the photometric constraint unreliable. Previous efforts only mitigate this relying on extra models to calibrate image brightness. In this paper, we propose MonoPCC to address the brightness inconsistency radically by reshaping the photometric constraint into a cycle form. Instead of only warping the source image, MonoPCC constructs a closed loop consisting of two opposite forward-backward warping paths: from target to source and then back to target. Thus, the target image finally receives an image cycle-warped from itself, which naturally makes the constraint invariant to brightness changes. Moreover, MonoPCC transplants the source image's phase-frequency into the intermediate warped image to avoid structure lost, and also stabilizes the training via an exponential moving average (EMA) strategy to avoid frequent changes in the forward warping. The comprehensive and extensive experimental results on three datasets demonstrate that our proposed MonoPCC shows a great robustness to the brightness inconsistency, and exceeds other state-of-the-arts by reducing the absolute relative error by at least 7.27%.","sentences":["Photometric constraint is indispensable for self-supervised monocular depth estimation.","It involves warping a source image onto a target view using estimated depth&pose, and then minimizing the difference between the warped and target images.","However, the endoscopic built-in light causes significant brightness fluctuations, and thus makes the photometric constraint unreliable.","Previous efforts only mitigate this relying on extra models to calibrate image brightness.","In this paper, we propose MonoPCC to address the brightness inconsistency radically by reshaping the photometric constraint into a cycle form.","Instead of only warping the source image, MonoPCC constructs a closed loop consisting of two opposite forward-backward warping paths: from target to source and then back to target.","Thus, the target image finally receives an image cycle-warped from itself, which naturally makes the constraint invariant to brightness changes.","Moreover, MonoPCC transplants the source image's phase-frequency into the intermediate warped image to avoid structure lost, and also stabilizes the training via an exponential moving average (EMA) strategy to avoid frequent changes in the forward warping.","The comprehensive and extensive experimental results on three datasets demonstrate that our proposed MonoPCC shows a great robustness to the brightness inconsistency, and exceeds other state-of-the-arts by reducing the absolute relative error by at least 7.27%."],"url":"http://arxiv.org/abs/2404.16571v1","category":"cs.CV"}
{"created":"2024-04-25 10:52:08","title":"360SFUDA++: Towards Source-free UDA for Panoramic Segmentation by Learning Reliable Category Prototypes","abstract":"In this paper, we address the challenging source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation, given only a pinhole image pre-trained model (i.e., source) and unlabeled panoramic images (i.e., target). Tackling this problem is non-trivial due to three critical challenges: 1) semantic mismatches from the distinct Field-of-View (FoV) between domains, 2) style discrepancies inherent in the UDA problem, and 3) inevitable distortion of the panoramic images. To tackle these problems, we propose 360SFUDA++ that effectively extracts knowledge from the source pinhole model with only unlabeled panoramic images and transfers the reliable knowledge to the target panoramic domain. Specifically, we first utilize Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) to patches with fixed FoV projection (FFP) to mimic the pinhole images. Both projections are shown effective in extracting knowledge from the source model. However, as the distinct projections make it less possible to directly transfer knowledge between domains, we then propose Reliable Panoramic Prototype Adaptation Module (RP2AM) to transfer knowledge at both prediction and prototype levels. RP$^2$AM selects the confident knowledge and integrates panoramic prototypes for reliable knowledge adaptation. Moreover, we introduce Cross-projection Dual Attention Module (CDAM), which better aligns the spatial and channel characteristics across projections at the feature level between domains. Both knowledge extraction and transfer processes are synchronously updated to reach the best performance. Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our 360SFUDA++ achieves significantly better performance than prior SFUDA methods.","sentences":["In this paper, we address the challenging source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation, given only a pinhole image pre-trained model (i.e., source) and unlabeled panoramic images (i.e., target).","Tackling this problem is non-trivial due to three critical challenges: 1) semantic mismatches from the distinct Field-of-View (FoV) between domains, 2) style discrepancies inherent in the UDA problem, and 3) inevitable distortion of the panoramic images.","To tackle these problems, we propose 360SFUDA++ that effectively extracts knowledge from the source pinhole model with only unlabeled panoramic images and transfers the reliable knowledge to the target panoramic domain.","Specifically, we first utilize Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) to patches with fixed FoV projection (FFP) to mimic the pinhole images.","Both projections are shown effective in extracting knowledge from the source model.","However, as the distinct projections make it less possible to directly transfer knowledge between domains, we then propose Reliable Panoramic Prototype Adaptation Module (RP2AM) to transfer knowledge at both prediction and prototype levels.","RP$^2$AM selects the confident knowledge and integrates panoramic prototypes for reliable knowledge adaptation.","Moreover, we introduce Cross-projection Dual Attention Module (CDAM), which better aligns the spatial and channel characteristics across projections at the feature level between domains.","Both knowledge extraction and transfer processes are synchronously updated to reach the best performance.","Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our 360SFUDA++ achieves significantly better performance than prior SFUDA methods."],"url":"http://arxiv.org/abs/2404.16501v1","category":"cs.CV"}
{"created":"2024-04-25 10:21:35","title":"Two-Dimensional Eclipse Mapping of the Hot Jupiter WASP-43b with JWST MIRI/LRS","abstract":"We present eclipse maps of the two-dimensional thermal emission from the dayside of the hot Jupiter WASP-43b, derived from an observation of a phase curve with the JWST MIRI/LRS instrument. The observed eclipse shapes deviate significantly from those expected for a planet emitting uniformly over its surface. We fit a map to this deviation, constructed from spherical harmonics up to order $\\ell_{\\rm max}=2$, alongside the planetary, orbital, stellar, and systematic parameters. This yields a map with a meridionally-averaged eastward hot-spot shift of $(7.75 \\pm 0.36)^{\\circ}$, with no significant degeneracy between the map and the additional parameters. We show the latitudinal and longitudinal contributions of the day-side emission structure to the eclipse shape, finding a latitudinal signal of $\\sim$200 ppm and a longitudinal signal of $\\sim$250 ppm. To investigate the sensitivity of the map to the method, we fix the non-mapping parameters and derive an \"eigenmap\" fitted with an optimised number of orthogonal phase curves, which yields a similar map to the $\\ell_{\\rm max}=2$ map. We also fit a map up to $\\ell_{\\rm max}=3$, which shows a smaller hot-spot shift, with a larger uncertainty. These maps are similar to those produced by atmospheric simulations. We conclude that there is a significant mapping signal which constrains the spherical harmonic components of our model up to $\\ell_{\\rm max}=2$. Alternative mapping models may derive different structures with smaller-scale features; we suggest that further observations of WASP-43b and other planets will drive the development of more robust methods and more accurate maps.","sentences":["We present eclipse maps of the two-dimensional thermal emission from the dayside of the hot Jupiter WASP-43b, derived from an observation of a phase curve with the JWST MIRI/LRS instrument.","The observed eclipse shapes deviate significantly from those expected for a planet emitting uniformly over its surface.","We fit a map to this deviation, constructed from spherical harmonics up to order $\\ell_{\\rm max}=2$, alongside the planetary, orbital, stellar, and systematic parameters.","This yields a map with a meridionally-averaged eastward hot-spot shift of $(7.75 \\pm 0.36)^{\\circ}$, with no significant degeneracy between the map and the additional parameters.","We show the latitudinal and longitudinal contributions of the day-side emission structure to the eclipse shape, finding a latitudinal signal of $\\sim$200 ppm and a longitudinal signal of $\\sim$250 ppm.","To investigate the sensitivity of the map to the method, we fix the non-mapping parameters and derive an \"eigenmap\" fitted with an optimised number of orthogonal phase curves, which yields a similar map to the $\\ell_{\\rm max}=2$ map.","We also fit a map up to $\\ell_{\\rm max}=3$, which shows a smaller hot-spot shift, with a larger uncertainty.","These maps are similar to those produced by atmospheric simulations.","We conclude that there is a significant mapping signal which constrains the spherical harmonic components of our model up to $\\ell_{\\rm max}=2$. Alternative mapping models may derive different structures with smaller-scale features; we suggest that further observations of WASP-43b and other planets will drive the development of more robust methods and more accurate maps."],"url":"http://arxiv.org/abs/2404.16488v1","category":"astro-ph.EP"}
{"created":"2024-04-25 10:12:42","title":"Real-Time 4K Super-Resolution of Compressed AVIF Images. AIS 2024 Challenge Survey","abstract":"This paper introduces a novel benchmark as part of the AIS 2024 Real-Time Image Super-Resolution (RTSR) Challenge, which aims to upscale compressed images from 540p to 4K resolution (4x factor) in real-time on commercial GPUs. For this, we use a diverse test set containing a variety of 4K images ranging from digital art to gaming and photography. The images are compressed using the modern AVIF codec, instead of JPEG. All the proposed methods improve PSNR fidelity over Lanczos interpolation, and process images under 10ms. Out of the 160 participants, 25 teams submitted their code and models. The solutions present novel designs tailored for memory-efficiency and runtime on edge devices. This survey describes the best solutions for real-time SR of compressed high-resolution images.","sentences":["This paper introduces a novel benchmark as part of the AIS 2024 Real-Time Image Super-Resolution (RTSR) Challenge, which aims to upscale compressed images from 540p to 4K resolution (4x factor) in real-time on commercial GPUs.","For this, we use a diverse test set containing a variety of 4K images ranging from digital art to gaming and photography.","The images are compressed using the modern AVIF codec, instead of JPEG.","All the proposed methods improve PSNR fidelity over Lanczos interpolation, and process images under 10ms.","Out of the 160 participants, 25 teams submitted their code and models.","The solutions present novel designs tailored for memory-efficiency and runtime on edge devices.","This survey describes the best solutions for real-time SR of compressed high-resolution images."],"url":"http://arxiv.org/abs/2404.16484v1","category":"cs.CV"}
{"created":"2024-04-25 10:04:36","title":"The Impact of Social Environment and Interaction Focus on User Experience and Social Acceptability of an Augmented Reality Game","abstract":"One of the most promising technologies inside the Extended Reality (XR) spectrum is Augmented Reality. This technology is already in people's pockets regarding Mobile Augmented Reality with their smartphones. The scientific community still needs answers about how humans could and should interact in environments where perceived stimuli are different from fully physical or digital circumstances. Moreover, it is still being determined if people accept these new technologies in different social environments and interaction settings or if some obstacles could exist. This paper explores the impact of the Social Environment and the Focus of social interaction on users while playing a location-based augmented reality game, measuring it with user experience and social acceptance indicators. An empirical study in a within-subject fashion was performed in different social environments and under different settings of social interaction focus with N = 28 participants compiling self-reported questionnaires after playing a Scavenger Hunt in Augmented Reality. The measures from two different Social Environments (Crowded vs. Uncrowded) resulted in statistically relevant mean differences with indicators from the Social Acceptability dimension. Moreover, the analyses show statistically relevant differences between the variances from different degrees of Social Interaction Focus with Overall Social Presence, Perceived Psychological Engagement, Perceived Attentional Engagement, and Perceived Emotional Contagion. The results suggest that a location-based AR game played in different social environments and settings can influence the user experience's social dimension. Therefore, they should be carefully considered while designing immersive technological experiences in public spaces involving social interactions between players.","sentences":["One of the most promising technologies inside the Extended Reality (XR) spectrum is Augmented Reality.","This technology is already in people's pockets regarding Mobile Augmented Reality with their smartphones.","The scientific community still needs answers about how humans could and should interact in environments where perceived stimuli are different from fully physical or digital circumstances.","Moreover, it is still being determined if people accept these new technologies in different social environments and interaction settings or if some obstacles could exist.","This paper explores the impact of the Social Environment and the Focus of social interaction on users while playing a location-based augmented reality game, measuring it with user experience and social acceptance indicators.","An empirical study in a within-subject fashion was performed in different social environments and under different settings of social interaction focus with N = 28 participants compiling self-reported questionnaires after playing a Scavenger Hunt in Augmented Reality.","The measures from two different Social Environments (Crowded vs. Uncrowded) resulted in statistically relevant mean differences with indicators from the Social Acceptability dimension.","Moreover, the analyses show statistically relevant differences between the variances from different degrees of Social Interaction Focus with Overall Social Presence, Perceived Psychological Engagement, Perceived Attentional Engagement, and Perceived Emotional Contagion.","The results suggest that a location-based AR game played in different social environments and settings can influence the user experience's social dimension.","Therefore, they should be carefully considered while designing immersive technological experiences in public spaces involving social interactions between players."],"url":"http://arxiv.org/abs/2404.16479v1","category":"cs.HC"}
{"created":"2024-04-25 09:25:17","title":"Wavefunction collapse driven by non-Hermitian disturbance","abstract":"In the context of the measurement problem, we propose to model the interaction between a quantum particle and an \"apparatus\" through a non-Hermitian Hamiltonian term. We simulate the time evolution of a normalized quantum state split into two spin components (via a Stern-Gerlach experiment) and that undergoes a wave-function collapse driven by a non-Hermitian Hatano-Nelson Hamiltonian. We further analyze how the strength and other parameters of the non-Hermitian perturbation influence the time-to-collapse of the wave function obtained under a Schr\\\"{o}dinger-type evolution. We finally discuss a thought experiment where manipulation of the apparatus could challenge standard quantum mechanics predictions.","sentences":["In the context of the measurement problem, we propose to model the interaction between a quantum particle and an \"apparatus\" through a non-Hermitian Hamiltonian term.","We simulate the time evolution of a normalized quantum state split into two spin components (via a Stern-Gerlach experiment) and that undergoes a wave-function collapse driven by a non-Hermitian Hatano-Nelson Hamiltonian.","We further analyze how the strength and other parameters of the non-Hermitian perturbation influence the time-to-collapse of the wave function obtained under a Schr\\\"{o}dinger-type evolution.","We finally discuss a thought experiment where manipulation of the apparatus could challenge standard quantum mechanics predictions."],"url":"http://arxiv.org/abs/2404.16445v1","category":"quant-ph"}
{"created":"2024-04-25 08:58:30","title":"On algebraic independence of Taylor coefficients of certain Anderson-Thakur series","abstract":"We study algebraic independence problem for the Taylor coefficients of the Anderson-Thakur series arisen as deformation series of positive characteristic multiple zeta values (abbreviated as MZV's). These Taylor coefficients are simply specialization of hyperderivatives of the Anderson-Thakur series. We consider the prolongation of t-motives associated with MZV's, and then determine the dimension of the t-motivic Galois groups in question under certain hypothesis. By using Papanikolas' theory, it enables us to obtain the desired algebraic independence result.","sentences":["We study algebraic independence problem for the Taylor coefficients of the Anderson-Thakur series arisen as deformation series of positive characteristic multiple zeta values (abbreviated as MZV's).","These Taylor coefficients are simply specialization of hyperderivatives of the Anderson-Thakur series.","We consider the prolongation of t-motives associated with MZV's, and then determine the dimension of the t-motivic Galois groups in question under certain hypothesis.","By using Papanikolas' theory, it enables us to obtain the desired algebraic independence result."],"url":"http://arxiv.org/abs/2404.16427v1","category":"math.NT"}
{"created":"2024-04-25 08:52:25","title":"Robust Fine-tuning for Pre-trained 3D Point Cloud Models","abstract":"This paper presents a robust fine-tuning method designed for pre-trained 3D point cloud models, to enhance feature robustness in downstream fine-tuned models. We highlight the limitations of current fine-tuning methods and the challenges of learning robust models. The proposed method, named Weight-Space Ensembles for Fine-Tuning then Linear Probing (WiSE-FT-LP), integrates the original pre-training and fine-tuning models through weight space integration followed by Linear Probing. This approach significantly enhances the performance of downstream fine-tuned models under distribution shifts, improving feature robustness while maintaining high performance on the target distribution. We apply this robust fine-tuning method to mainstream 3D point cloud pre-trained models and evaluate the quality of model parameters and the degradation of downstream task performance. Experimental results demonstrate the effectiveness of WiSE-FT-LP in enhancing model robustness, effectively balancing downstream task performance and model feature robustness without altering the model structures.","sentences":["This paper presents a robust fine-tuning method designed for pre-trained 3D point cloud models, to enhance feature robustness in downstream fine-tuned models.","We highlight the limitations of current fine-tuning methods and the challenges of learning robust models.","The proposed method, named Weight-Space Ensembles for Fine-Tuning then Linear Probing (WiSE-FT-LP), integrates the original pre-training and fine-tuning models through weight space integration followed by Linear Probing.","This approach significantly enhances the performance of downstream fine-tuned models under distribution shifts, improving feature robustness while maintaining high performance on the target distribution.","We apply this robust fine-tuning method to mainstream 3D point cloud pre-trained models and evaluate the quality of model parameters and the degradation of downstream task performance.","Experimental results demonstrate the effectiveness of WiSE-FT-LP in enhancing model robustness, effectively balancing downstream task performance and model feature robustness without altering the model structures."],"url":"http://arxiv.org/abs/2404.16422v1","category":"cs.CV"}
{"created":"2024-04-25 08:49:08","title":"Learning Discriminative Spatio-temporal Representations for Semi-supervised Action Recognition","abstract":"Semi-supervised action recognition aims to improve spatio-temporal reasoning ability with a few labeled data in conjunction with a large amount of unlabeled data. Albeit recent advancements, existing powerful methods are still prone to making ambiguous predictions under scarce labeled data, embodied as the limitation of distinguishing different actions with similar spatio-temporal information. In this paper, we approach this problem by empowering the model two aspects of capability, namely discriminative spatial modeling and temporal structure modeling for learning discriminative spatio-temporal representations. Specifically, we propose an Adaptive Contrastive Learning~(ACL) strategy. It assesses the confidence of all unlabeled samples by the class prototypes of the labeled data, and adaptively selects positive-negative samples from a pseudo-labeled sample bank to construct contrastive learning. Additionally, we introduce a Multi-scale Temporal Learning~(MTL) strategy. It could highlight informative semantics from long-term clips and integrate them into the short-term clip while suppressing noisy information. Afterwards, both of these two new techniques are integrated in a unified framework to encourage the model to make accurate predictions. Extensive experiments on UCF101, HMDB51 and Kinetics400 show the superiority of our method over prior state-of-the-art approaches.","sentences":["Semi-supervised action recognition aims to improve spatio-temporal reasoning ability with a few labeled data in conjunction with a large amount of unlabeled data.","Albeit recent advancements, existing powerful methods are still prone to making ambiguous predictions under scarce labeled data, embodied as the limitation of distinguishing different actions with similar spatio-temporal information.","In this paper, we approach this problem by empowering the model two aspects of capability, namely discriminative spatial modeling and temporal structure modeling for learning discriminative spatio-temporal representations.","Specifically, we propose an Adaptive Contrastive Learning~(ACL) strategy.","It assesses the confidence of all unlabeled samples by the class prototypes of the labeled data, and adaptively selects positive-negative samples from a pseudo-labeled sample bank to construct contrastive learning.","Additionally, we introduce a Multi-scale Temporal Learning~(MTL) strategy.","It could highlight informative semantics from long-term clips and integrate them into the short-term clip while suppressing noisy information.","Afterwards, both of these two new techniques are integrated in a unified framework to encourage the model to make accurate predictions.","Extensive experiments on UCF101, HMDB51 and Kinetics400 show the superiority of our method over prior state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.16416v1","category":"cs.CV"}
{"created":"2024-04-25 08:29:33","title":"Ground state properties and bubble structure of the isotopic chains of Z = 125 and 126 using the relativistic mean-field formalism","abstract":"The ground state properties of Z = 125 and 126 nuclei are investigated, taking the isotopic series from the proton to neutron drip-lines. This analysis is conducted using the relativistic mean-field approach with NL3 and the Relativistic-Hartree-Bogoliubov model with DD-ME2 parameterization. The bulk properties under examination include the binding energy, the neutron separation energies, the differential variation of the separation energy, the quadrupole deformation parameter $\\beta_2$, and the single-particle energy. We observed the stability at N = 172 and 184 over the isotopic chain for both parameter sets. The quadrupole deformation parameter reveals a shape transition from prolate to spherical and back to prolate with mass number. No signature of a super- and/or hyper-deformed structure is found over the isotopic chain. Furthermore, the analysis is extended to examine the bubble structure, revealing a bubble/semi-bubble structure for a few neutron-rich isotopes.","sentences":["The ground state properties of Z = 125 and 126 nuclei are investigated, taking the isotopic series from the proton to neutron drip-lines.","This analysis is conducted using the relativistic mean-field approach with NL3 and the Relativistic-Hartree-Bogoliubov model with DD-ME2 parameterization.","The bulk properties under examination include the binding energy, the neutron separation energies, the differential variation of the separation energy, the quadrupole deformation parameter $\\beta_2$, and the single-particle energy.","We observed the stability at N = 172 and 184 over the isotopic chain for both parameter sets.","The quadrupole deformation parameter reveals a shape transition from prolate to spherical and back to prolate with mass number.","No signature of a super- and/or hyper-deformed structure is found over the isotopic chain.","Furthermore, the analysis is extended to examine the bubble structure, revealing a bubble/semi-bubble structure for a few neutron-rich isotopes."],"url":"http://arxiv.org/abs/2404.16403v1","category":"nucl-th"}
{"created":"2024-04-25 17:56:21","title":"ESG: Pipeline-Conscious Efficient Scheduling of DNN Workflows on Serverless Platforms with Shareable GPUs","abstract":"Recent years have witnessed increasing interest in machine learning inferences on serverless computing for its auto-scaling and cost effective properties. Existing serverless computing, however, lacks effective job scheduling methods to handle the schedule space dramatically expanded by GPU sharing, task batching, and inter-task relations. Prior solutions have dodged the issue by neglecting some important factors, leaving some large performance potential locked. This paper presents ESG, a new scheduling algorithm that directly addresses the difficulties. ESG treats sharable GPU as a first-order factor in scheduling. It employs an optimality-guided adaptive method by combining A*-search and a novel dual-blade pruning to dramatically prune the scheduling space without compromising the quality. It further introduces a novel method, dominator-based SLO distribution, to ensure the scalability of the scheduler. The results show that ESG can significantly improve the SLO hit rates 61%-80% while saving 47%-187% costs over prior work.","sentences":["Recent years have witnessed increasing interest in machine learning inferences on serverless computing for its auto-scaling and cost effective properties.","Existing serverless computing, however, lacks effective job scheduling methods to handle the schedule space dramatically expanded by GPU sharing, task batching, and inter-task relations.","Prior solutions have dodged the issue by neglecting some important factors, leaving some large performance potential locked.","This paper presents ESG, a new scheduling algorithm that directly addresses the difficulties.","ESG treats sharable GPU as a first-order factor in scheduling.","It employs an optimality-guided adaptive method by combining A*-search and a novel dual-blade pruning to dramatically prune the scheduling space without compromising the quality.","It further introduces a novel method, dominator-based SLO distribution, to ensure the scalability of the scheduler.","The results show that ESG can significantly improve the SLO hit rates 61%-80% while saving 47%-187% costs over prior work."],"url":"http://arxiv.org/abs/2404.16812v1","category":"cs.DC"}
{"created":"2024-04-25 17:24:35","title":"ConKeD++ -- Improving descriptor learning for retinal image registration: A comprehensive study of contrastive losses","abstract":"Self-supervised contrastive learning has emerged as one of the most successful deep learning paradigms. In this regard, it has seen extensive use in image registration and, more recently, in the particular field of medical image registration. In this work, we propose to test and extend and improve a state-of-the-art framework for color fundus image registration, ConKeD. Using the ConKeD framework we test multiple loss functions, adapting them to the framework and the application domain. Furthermore, we evaluate our models using the standarized benchmark dataset FIRE as well as several datasets that have never been used before for color fundus registration, for which we are releasing the pairing data as well as a standardized evaluation approach. Our work demonstrates state-of-the-art performance across all datasets and metrics demonstrating several advantages over current SOTA color fundus registration methods","sentences":["Self-supervised contrastive learning has emerged as one of the most successful deep learning paradigms.","In this regard, it has seen extensive use in image registration and, more recently, in the particular field of medical image registration.","In this work, we propose to test and extend and improve a state-of-the-art framework for color fundus image registration, ConKeD. Using the ConKeD framework we test multiple loss functions, adapting them to the framework and the application domain.","Furthermore, we evaluate our models using the standarized benchmark dataset FIRE as well as several datasets that have never been used before for color fundus registration, for which we are releasing the pairing data as well as a standardized evaluation approach.","Our work demonstrates state-of-the-art performance across all datasets and metrics demonstrating several advantages over current SOTA color fundus registration methods"],"url":"http://arxiv.org/abs/2404.16773v1","category":"cs.CV"}
{"created":"2024-04-25 17:09:14","title":"TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation","abstract":"We address the problem of regressing 3D human pose and shape from a single image, with a focus on 3D accuracy. The current best methods leverage large datasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust performance. With such methods, we observe a paradoxical decline in 3D pose accuracy with increasing 2D accuracy. This is caused by biases in the p-GT and the use of an approximate camera projection model. We quantify the error induced by current camera models and show that fitting 2D keypoints and p-GT accurately causes incorrect 3D poses. Our analysis defines the invalid distances within which minimizing 2D and p-GT losses is detrimental. We use this to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) that penalizes gross 2D and p-GT losses but not smaller ones. With such a loss, there are many 3D poses that could equally explain the 2D evidence. To reduce this ambiguity we need a prior over valid human poses but such priors can introduce unwanted bias. To address this, we exploit a tokenized representation of human pose and reformulate the problem as token prediction. This restricts the estimated poses to the space of valid poses, effectively providing a uniform prior. Extensive experiments on the EMDB and 3DPW datasets show that our reformulated keypoint loss and tokenization allows us to train on in-the-wild data while improving 3D accuracy over the state-of-the-art. Our models and code are available for research at https://tokenhmr.is.tue.mpg.de.","sentences":["We address the problem of regressing 3D human pose and shape from a single image, with a focus on 3D accuracy.","The current best methods leverage large datasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust performance.","With such methods, we observe a paradoxical decline in 3D pose accuracy with increasing 2D accuracy.","This is caused by biases in the p-GT and the use of an approximate camera projection model.","We quantify the error induced by current camera models and show that fitting 2D keypoints and p-GT accurately causes incorrect 3D poses.","Our analysis defines the invalid distances within which minimizing 2D and p-GT losses is detrimental.","We use this to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) that penalizes gross 2D and p-GT losses but not smaller ones.","With such a loss, there are many 3D poses that could equally explain the 2D evidence.","To reduce this ambiguity we need a prior over valid human poses but such priors can introduce unwanted bias.","To address this, we exploit a tokenized representation of human pose and reformulate the problem as token prediction.","This restricts the estimated poses to the space of valid poses, effectively providing a uniform prior.","Extensive experiments on the EMDB and 3DPW datasets show that our reformulated keypoint loss and tokenization allows us to train on in-the-wild data while improving 3D accuracy over the state-of-the-art.","Our models and code are available for research at https://tokenhmr.is.tue.mpg.de."],"url":"http://arxiv.org/abs/2404.16752v1","category":"cs.CV"}
{"created":"2024-04-25 14:08:02","title":"Layered List Labeling","abstract":"The list-labeling problem is one of the most basic and well-studied algorithmic primitives in data structures, with an extensive literature spanning upper bounds, lower bounds, and data management applications. The classical algorithm for this problem, dating back to 1981, has amortized cost $O(\\log^2 n)$. Subsequent work has led to improvements in three directions: \\emph{low-latency} (worst-case) bounds; \\emph{high-throughput} (expected) bounds; and (adaptive) bounds for \\emph{important workloads}.   Perhaps surprisingly, these three directions of research have remained almost entirely disjoint -- this is because, so far, the techniques that allow for progress in one direction have forced worsening bounds in the others. Thus there would appear to be a tension between worst-case, adaptive, and expected bounds. List labeling has been proposed for use in databases at least as early as PODS'99, but a database needs good throughput, response time, and needs to adapt to common workloads (e.g., bulk loads), and no current list-labeling algorithm achieve good bounds for all three.   We show that this tension is not fundamental. In fact, with the help of new data-structural techniques, one can actually \\emph{combine} any three list-labeling solutions in order to cherry-pick the best worst-case, adaptive, and expected bounds from each of them.","sentences":["The list-labeling problem is one of the most basic and well-studied algorithmic primitives in data structures, with an extensive literature spanning upper bounds, lower bounds, and data management applications.","The classical algorithm for this problem, dating back to 1981, has amortized cost $O(\\log^2 n)$.","Subsequent work has led to improvements in three directions: \\emph{low-latency} (worst-case) bounds; \\emph{high-throughput} (expected) bounds; and (adaptive) bounds for \\emph{important workloads}.   ","Perhaps surprisingly, these three directions of research have remained almost entirely disjoint -- this is because, so far, the techniques that allow for progress in one direction have forced worsening bounds in the others.","Thus there would appear to be a tension between worst-case, adaptive, and expected bounds.","List labeling has been proposed for use in databases at least as early as PODS'99, but a database needs good throughput, response time, and needs to adapt to common workloads (e.g., bulk loads), and no current list-labeling algorithm achieve good bounds for all three.   ","We show that this tension is not fundamental.","In fact, with the help of new data-structural techniques, one can actually \\emph{combine} any three list-labeling solutions in order to cherry-pick the best worst-case, adaptive, and expected bounds from each of them."],"url":"http://arxiv.org/abs/2404.16623v1","category":"cs.DS"}
{"created":"2024-04-25 13:53:42","title":"Stochastic Dissipative Euler's equations for a free body","abstract":"Intrinsic thermal fluctuations within a real solid challenge the rigid body assumption that is central to Euler's equations for the motion of a free body. Recently, we have introduced a dissipative and stochastic version of Euler's equations in a thermodynamically consistent way (European Journal of Mechanics - A/Solids 103, 105184 (2024)). This framework describes the evolution of both orientation and shape of a free body, incorporating internal thermal fluctuations and their concomitant dissipative mechanisms. In the present work, we demonstrate that, in the absence of angular momentum, the theory predicts that principal axis unit vectors of a body undergo an anisotropic Brownian motion on the unit sphere, with the anisotropy arising from the body's varying moments of inertia. The resulting equilibrium time correlation function of the principal eigenvectors decays exponentially. This theoretical prediction is confirmed in molecular dynamics simulations of small bodies. The comparison of theory and equilibrium MD simulations allow us to measure the orientational diffusion tensor. We then use this information in the Stochastic Dissipative Euler's Equations, to describe a non-equilibrium situation of a body spinning around the unstable intermediate axis. The agreement between theory and simulations is excellent, offering a validation of the theoretical framework.","sentences":["Intrinsic thermal fluctuations within a real solid challenge the rigid body assumption that is central to Euler's equations for the motion of a free body.","Recently, we have introduced a dissipative and stochastic version of Euler's equations in a thermodynamically consistent way (European Journal of Mechanics - A/Solids 103, 105184 (2024)).","This framework describes the evolution of both orientation and shape of a free body, incorporating internal thermal fluctuations and their concomitant dissipative mechanisms.","In the present work, we demonstrate that, in the absence of angular momentum, the theory predicts that principal axis unit vectors of a body undergo an anisotropic Brownian motion on the unit sphere, with the anisotropy arising from the body's varying moments of inertia.","The resulting equilibrium time correlation function of the principal eigenvectors decays exponentially.","This theoretical prediction is confirmed in molecular dynamics simulations of small bodies.","The comparison of theory and equilibrium MD simulations allow us to measure the orientational diffusion tensor.","We then use this information in the Stochastic Dissipative Euler's Equations, to describe a non-equilibrium situation of a body spinning around the unstable intermediate axis.","The agreement between theory and simulations is excellent, offering a validation of the theoretical framework."],"url":"http://arxiv.org/abs/2404.16613v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 12:59:58","title":"Stability of Navier-Stokes equations with a free surface","abstract":"We consider the viscous incompressible fluids in a three-dimensional horizontally periodic domain bounded below by a fixed smooth boundary and above by a free moving surface. The fluid dynamics are governed by the Navier-Stokes equations with the effect of gravity and surface tension on the free surface. We develop a global well-posedness theory by a nonlinear energy method in low regular Sobolev spaces with several techniques, including: the horizontal energy-dissipation estimates, a new tripled bootstrap argument inspired by Guo and Tice [Arch. Ration. Mech. Anal.(2018)]. Moreover, the solution decays asymptotically to the equilibrium in an exponential rate.","sentences":["We consider the viscous incompressible fluids in a three-dimensional horizontally periodic domain bounded below by a fixed smooth boundary and above by a free moving surface.","The fluid dynamics are governed by the Navier-Stokes equations with the effect of gravity and surface tension on the free surface.","We develop a global well-posedness theory by a nonlinear energy method in low regular Sobolev spaces with several techniques, including: the horizontal energy-dissipation estimates, a new tripled bootstrap argument inspired by Guo and Tice","[Arch.","Ration.","Mech.","Anal.(2018)].","Moreover, the solution decays asymptotically to the equilibrium in an exponential rate."],"url":"http://arxiv.org/abs/2404.16585v1","category":"math.AP"}
{"created":"2024-04-25 12:02:20","title":"Image registration based automated lesion correspondence pipeline for longitudinal CT data","abstract":"Patients diagnosed with metastatic breast cancer (mBC) typically undergo several radiographic assessments during their treatment. mBC often involves multiple metastatic lesions in different organs, it is imperative to accurately track and assess these lesions to gain a comprehensive understanding of the disease's response to treatment. Computerized analysis methods that rely on lesion-level tracking have often used manual matching of corresponding lesions, a time-consuming process that is prone to errors. This paper introduces an automated lesion correspondence algorithm designed to precisely track both targets' lesions and non-targets' lesions in longitudinal data. Here we demonstrate the applicability of our algorithm on the anonymized data from two Phase III trials. The dataset contains imaging data of patients for different follow-up timepoints and the radiologist annotations for the patients enrolled in the trials. Target and non-target lesions are annotated by either one or two groups of radiologists. To facilitate accurate tracking, we have developed a registration-assisted lesion correspondence algorithm. The algorithm employs a sequential two-step pipeline: (a) Firstly, an adaptive Hungarian algorithm is used to establish correspondence among lesions within a single volumetric image series which have been annotated by multiple radiologists at a specific timepoint. (b) Secondly, after establishing correspondence and assigning unique names to the lesions, three-dimensional rigid registration is applied to various image series at the same timepoint. Registration is followed by ongoing lesion correspondence based on the adaptive Hungarian algorithm and updating lesion names for accurate tracking. Validation of our automated lesion correspondence algorithm is performed through triaxial plots based on axial, sagittal, and coronal views, confirming its efficacy in matching lesions.","sentences":["Patients diagnosed with metastatic breast cancer (mBC) typically undergo several radiographic assessments during their treatment.","mBC often involves multiple metastatic lesions in different organs, it is imperative to accurately track and assess these lesions to gain a comprehensive understanding of the disease's response to treatment.","Computerized analysis methods that rely on lesion-level tracking have often used manual matching of corresponding lesions, a time-consuming process that is prone to errors.","This paper introduces an automated lesion correspondence algorithm designed to precisely track both targets' lesions and non-targets' lesions in longitudinal data.","Here we demonstrate the applicability of our algorithm on the anonymized data from two Phase III trials.","The dataset contains imaging data of patients for different follow-up timepoints and the radiologist annotations for the patients enrolled in the trials.","Target and non-target lesions are annotated by either one or two groups of radiologists.","To facilitate accurate tracking, we have developed a registration-assisted lesion correspondence algorithm.","The algorithm employs a sequential two-step pipeline: (a) Firstly, an adaptive Hungarian algorithm is used to establish correspondence among lesions within a single volumetric image series which have been annotated by multiple radiologists at a specific timepoint.","(b) Secondly, after establishing correspondence and assigning unique names to the lesions, three-dimensional rigid registration is applied to various image series at the same timepoint.","Registration is followed by ongoing lesion correspondence based on the adaptive Hungarian algorithm and updating lesion names for accurate tracking.","Validation of our automated lesion correspondence algorithm is performed through triaxial plots based on axial, sagittal, and coronal views, confirming its efficacy in matching lesions."],"url":"http://arxiv.org/abs/2404.16544v1","category":"eess.IV"}
{"created":"2024-04-25 11:51:43","title":"Local training and enrichment based on a residual localization strategy","abstract":"To efficiently tackle parametrized multi and/or large scale problems, we propose an adaptive localized model order reduction framework combining both local offline training and local online enrichment with localized error control. For the latter, we adapt the residual localization strategy introduced in [Buhr, Engwer, Ohlberger, Rave, SIAM J. Sci. Comput., 2017] which allows to derive a localized a posteriori error estimator that can be employed to adaptively enrich the reduced solution space locally where needed. Numerical experiments demonstrate the potential of the proposed approach.","sentences":["To efficiently tackle parametrized multi and/or large scale problems, we propose an adaptive localized model order reduction framework combining both local offline training and local online enrichment with localized error control.","For the latter, we adapt the residual localization strategy introduced in [Buhr, Engwer, Ohlberger, Rave, SIAM J. Sci.","Comput., 2017] which allows to derive a localized a posteriori error estimator that can be employed to adaptively enrich the reduced solution space locally where needed.","Numerical experiments demonstrate the potential of the proposed approach."],"url":"http://arxiv.org/abs/2404.16537v1","category":"math.NA"}
{"created":"2024-04-25 17:52:03","title":"Accelerated inference on accelerated cosmic expansion: New constraints on axion-like early dark energy with DESI BAO and ACT DR6 CMB lensing","abstract":"The early dark energy (EDE) extension to $\\Lambda$CDM has been proposed as a candidate scenario to resolve the \"Hubble tension\". We present new constraints on the EDE model by incorporating new data from the Dark Energy Spectroscopic Instrument (DESI) Baryon Acoustic Oscillation (BAO) survey and CMB lensing measurements from the Atacama Cosmology Telescope (ACT) DR6 and \\textit{Planck} NPIPE data. We do not find evidence for EDE. The maximum fractional contribution of EDE to the total energy density is $f_\\mathrm{EDE}< 0.091 \\; (95\\% \\; \\mathrm{CL} )$ from our baseline combination of \\textit{Planck} CMB, CMB lensing, and DESI BAO. Our strongest constraints on EDE come from the combination of \\textit{Planck} CMB and CMB lensing alone, yielding $f_\\mathrm{EDE}< 0.070 \\; (95\\% \\; \\mathrm{CL} )$. We also explore extensions of $\\Lambda$CDM beyond the EDE parameters by treating the total neutrino mass as a free parameter, finding $\\sum m_\\nu < 0.096 \\,\\, {\\rm eV} \\; (95\\% \\; \\mathrm{CL} )$ and $f_\\mathrm{EDE}< 0.087 \\; (95\\% \\; \\mathrm{CL} )$. For the first time in EDE analyses, we perform Bayesian parameter estimation using neural network emulators of cosmological observables, which are on the order of a hundred times faster than full Boltzmann solutions.","sentences":["The early dark energy (EDE) extension to $\\Lambda$CDM has been proposed as a candidate scenario to resolve the \"Hubble tension\".","We present new constraints on the EDE model by incorporating new data from the Dark Energy Spectroscopic Instrument (DESI)","Baryon Acoustic Oscillation (BAO) survey and CMB lensing measurements from the Atacama Cosmology Telescope (ACT) DR6 and \\textit{Planck} NPIPE data.","We do not find evidence for EDE.","The maximum fractional contribution of EDE to the total energy density is $f_\\mathrm{EDE}< 0.091 \\; (95\\% \\; \\mathrm{CL} )$ from our baseline combination of \\textit{Planck} CMB, CMB lensing, and DESI BAO.","Our strongest constraints on EDE come from the combination of \\textit{Planck} CMB and CMB lensing alone, yielding $f_\\mathrm{EDE}< 0.070 \\; (95\\% \\; \\mathrm{CL} )$.","We also explore extensions of $\\Lambda$CDM beyond the EDE parameters by treating the total neutrino mass as a free parameter, finding $\\sum m_\\nu < 0.096 \\,\\, {\\rm eV} \\; (95\\% \\; \\mathrm{CL} )$ and $f_\\mathrm{EDE}< 0.087 \\; (95\\% \\; \\mathrm{CL} )$.","For the first time in EDE analyses, we perform Bayesian parameter estimation using neural network emulators of cosmological observables, which are on the order of a hundred times faster than full Boltzmann solutions."],"url":"http://arxiv.org/abs/2404.16805v1","category":"astro-ph.CO"}
{"created":"2024-04-25 17:40:52","title":"In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization","abstract":"With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations. However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters. In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass. Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work. Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work.","sentences":["With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations.","Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations.","However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters.","In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO.","FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass.","Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work.","Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work."],"url":"http://arxiv.org/abs/2404.16795v1","category":"cs.LG"}
{"created":"2024-04-25 17:38:37","title":"Rectifying submanifolds of Riemannian manifolds with anti-torqued axis","abstract":"In this paper we study rectifying submanifolds of a Riemannian manifold endowed with an anti-torqued vector field. For this, we first determine a necessary and sufficient condition for the ambient space to admit such a vector field. Then we characterize submanifolds for which an anti-torqued vector field is always assumed to be tangent or normal. A similar characterization is also done in the case of the torqued vector fields. Finally, we obtain that the rectifying submanifolds with anti-torqued axis are the warped products whose warping function is a first integration of the conformal scalar of the axis.","sentences":["In this paper we study rectifying submanifolds of a Riemannian manifold endowed with an anti-torqued vector field.","For this, we first determine a necessary and sufficient condition for the ambient space to admit such a vector field.","Then we characterize submanifolds for which an anti-torqued vector field is always assumed to be tangent or normal.","A similar characterization is also done in the case of the torqued vector fields.","Finally, we obtain that the rectifying submanifolds with anti-torqued axis are the warped products whose warping function is a first integration of the conformal scalar of the axis."],"url":"http://arxiv.org/abs/2404.16788v1","category":"math.DG"}
{"created":"2024-04-25 17:23:29","title":"Multi-scale modeling of Snail-mediated response to hypoxia in tumor progression","abstract":"Tumor cell migration within the microenvironment is a crucial aspect for cancer progression and, in this context, hypoxia has a significant role. An inadequate oxygen supply acts as an environmental stressor inducing migratory bias and phenotypic changes. In this paper, we propose a novel multi-scale mathematical model to analyze the pivotal role of Snail protein expression in the cellular responses to hypoxia. Starting from the description of single-cell dynamics driven by the Snail protein, we construct the corresponding kinetic transport equation that describes the evolution of the cell distribution. Subsequently, we employ proper scaling arguments to formally derive the equations for the statistical moments of the cell distribution, which govern the macroscopic tumor dynamics. Numerical simulations of the model are performed in various scenarios with biological relevance to provide insights into the role of the multiple tactic terms, the impact of Snail expression on cell proliferation, and the emergence of hypoxia-induced migration patterns. Moreover, quantitative comparison with experimental data shows the model's reliability in measuring the impact of Snail transcription on cell migratory potential. Through our findings, we shed light on the potential of our mathematical framework in advancing the understanding of the biological mechanisms driving tumor progression.","sentences":["Tumor cell migration within the microenvironment is a crucial aspect for cancer progression and, in this context, hypoxia has a significant role.","An inadequate oxygen supply acts as an environmental stressor inducing migratory bias and phenotypic changes.","In this paper, we propose a novel multi-scale mathematical model to analyze the pivotal role of Snail protein expression in the cellular responses to hypoxia.","Starting from the description of single-cell dynamics driven by the Snail protein, we construct the corresponding kinetic transport equation that describes the evolution of the cell distribution.","Subsequently, we employ proper scaling arguments to formally derive the equations for the statistical moments of the cell distribution, which govern the macroscopic tumor dynamics.","Numerical simulations of the model are performed in various scenarios with biological relevance to provide insights into the role of the multiple tactic terms, the impact of Snail expression on cell proliferation, and the emergence of hypoxia-induced migration patterns.","Moreover, quantitative comparison with experimental data shows the model's reliability in measuring the impact of Snail transcription on cell migratory potential.","Through our findings, we shed light on the potential of our mathematical framework in advancing the understanding of the biological mechanisms driving tumor progression."],"url":"http://arxiv.org/abs/2404.16769v1","category":"q-bio.CB"}
{"created":"2024-04-25 17:06:21","title":"Hydrodynamics of a Discrete Conservation Law","abstract":"The Riemann problem for the discrete conservation law $2 \\dot{u}_n + u^2_{n+1} - u^2_{n-1} = 0$ is classified using Whitham modulation theory, a quasi-continuum approximation, and numerical simulations. A surprisingly elaborate set of solutions to this simple discrete regularization of the inviscid Burgers' equation is obtained. In addition to discrete analogues of well-known dispersive hydrodynamic solutions -- rarefaction waves (RWs) and dispersive shock waves (DSWs) -- additional unsteady solution families and finite time blow-up are observed. Two solution types exhibit no known conservative continuum correlates: (i) a counterpropagating DSW and RW solution separated by a symmetric, stationary shock and (ii) an unsteady shock emitting two counter-propagating periodic wavetrains with the same frequency connected to a partial DSW or a RW. Another class of solutions called traveling DSWs, (iii), consists of a partial DSW connected to a traveling wave comprised of a periodic wavetrain with a rapid transition to a constant. Portions of solutions (ii) and (iii) are interpreted as shock solutions of the Whitham modulation equations.","sentences":["The Riemann problem for the discrete conservation law $2 \\dot{u}_n + u^2_{n+1} - u^2_{n-1} = 0$ is classified using Whitham modulation theory, a quasi-continuum approximation, and numerical simulations.","A surprisingly elaborate set of solutions to this simple discrete regularization of the inviscid Burgers' equation is obtained.","In addition to discrete analogues of well-known dispersive hydrodynamic solutions -- rarefaction waves (RWs) and dispersive shock waves (DSWs) -- additional unsteady solution families and finite time blow-up are observed.","Two solution types exhibit no known conservative continuum correlates: (i) a counterpropagating DSW and RW solution separated by a symmetric, stationary shock and (ii) an unsteady shock emitting two counter-propagating periodic wavetrains with the same frequency connected to a partial DSW or a RW.","Another class of solutions called traveling DSWs, (iii), consists of a partial DSW connected to a traveling wave comprised of a periodic wavetrain with a rapid transition to a constant.","Portions of solutions (ii) and (iii) are interpreted as shock solutions of the Whitham modulation equations."],"url":"http://arxiv.org/abs/2404.16750v1","category":"nlin.PS"}
{"created":"2024-04-25 16:47:53","title":"Superconducting Klein and anti-Klein tunneling in Weyl junctions","abstract":"Klein tunneling is an old topic in relativistic quantum physics, and has been observed recently in graphene where massless particles reside. Here, we propose a new heterostructure platform for Klein tunneling to occur, which consists of a Weyl-semimetal-based normal state/superconductor (NS) junction. By developing a Blonder-Tinkham-Klapwijk-like theory, we find that Klein tunneling occurs at normal incidence, which can lead to differential conductance doubling. If the (single) Weyl semimeltals are replaced by double Weyl semimetals, anti-Klein tunneling will take place of Klein tunneling. Our work provides a theoretical guide for the detection of (anti-)Klein tunneling in three-dimensional chiral NS junctions.","sentences":["Klein tunneling is an old topic in relativistic quantum physics, and has been observed recently in graphene where massless particles reside.","Here, we propose a new heterostructure platform for Klein tunneling to occur, which consists of a Weyl-semimetal-based normal state/superconductor (NS) junction.","By developing a Blonder-Tinkham-Klapwijk-like theory, we find that Klein tunneling occurs at normal incidence, which can lead to differential conductance doubling.","If the (single) Weyl semimeltals are replaced by double Weyl semimetals, anti-Klein tunneling will take place of Klein tunneling.","Our work provides a theoretical guide for the detection of (anti-)Klein tunneling in three-dimensional chiral NS junctions."],"url":"http://arxiv.org/abs/2404.16738v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 16:07:55","title":"Conformal para quaternionic contact curvature and the local flatness theorem","abstract":"A tensor invariant is defined on a para quaternionic contact manifold in terms of the curvature and torsion of the canonical para quaternionic connection involving derivatives up to third order of the contact form. This tensor, called para quaternionic contact conformal curvature, is similar to the Weyl conformal curvature in Riemannian geometry, the Chern-Moser tensor in CR geometry, the para contact curvature in para CR geometry and to the quaternionic contact conformal curvature in quaternionic contact geometry.   It is shown that a para quaternionic contact manifold is locally para quaternionic contact conformal to the standard flat para quaternionic contact structure on the para quaternionic Heisenberg group, or equivalently, to the standard para 3-Sasakian structure on the para quaternionic pseudo-sphere iff the para quaternionic contact conformal curvature vanishes.","sentences":["A tensor invariant is defined on a para quaternionic contact manifold in terms of the curvature and torsion of the canonical para quaternionic connection involving derivatives up to third order of the contact form.","This tensor, called para quaternionic contact conformal curvature, is similar to the Weyl conformal curvature in Riemannian geometry, the Chern-Moser tensor in CR geometry, the para contact curvature in para CR geometry and to the quaternionic contact conformal curvature in quaternionic contact geometry.   ","It is shown that a para quaternionic contact manifold is locally para quaternionic contact conformal to the standard flat para quaternionic contact structure on the para quaternionic Heisenberg group, or equivalently, to the standard para 3-Sasakian structure on the para quaternionic pseudo-sphere iff the para quaternionic contact conformal curvature vanishes."],"url":"http://arxiv.org/abs/2404.16703v1","category":"math.DG"}
{"created":"2024-04-25 15:06:48","title":"A new way of deriving implicit Runge-Kutta methods based on repeated integrals","abstract":"Runge-Kutta methods have an irreplaceable position among numerical methods designed to solve ordinary differential equations. Especially, implicit ones are suitable for approximating solutions of stiff initial value problems. We propose a new way of deriving coefficients of implicit Runge-Kutta methods. This approach based on repeated integrals yields both new and well-known Butcher's tableaux. We discuss the properties of newly derived methods and compare them with standard collocation implicit Runge-Kutta methods in a series of numerical experiments. In particular, we observe higher accuracy and higher experimental order of convergence of some newly derived methods.","sentences":["Runge-Kutta methods have an irreplaceable position among numerical methods designed to solve ordinary differential equations.","Especially, implicit ones are suitable for approximating solutions of stiff initial value problems.","We propose a new way of deriving coefficients of implicit Runge-Kutta methods.","This approach based on repeated integrals yields both new and well-known Butcher's tableaux.","We discuss the properties of newly derived methods and compare them with standard collocation implicit Runge-Kutta methods in a series of numerical experiments.","In particular, we observe higher accuracy and higher experimental order of convergence of some newly derived methods."],"url":"http://arxiv.org/abs/2404.16665v1","category":"math.NA"}
{"created":"2024-04-25 14:29:39","title":"Higher H\u00f6lder regularity for a subquadratic nonlocal parabolic equation","abstract":"In this paper, we are concerned with the H\\\"older regularity for solutions of the nonlocal evolutionary equation $$ \\partial_t u+(-\\Delta_p)^s u = 0. $$ Here, $(-\\Delta_p)^s$ is the fractional $p$-Laplacian, $0<s<1$ and $1<p<2$. We establish H\\\"older regularity with explicit H\\\"older exponents. We also include the inhomogeneous equation with a bounded inhomogeneity. In some cases, the obtained H\\\"older exponents are almost sharp. Our results complement the previous results for the superquadratic case when $p\\geq 2$.","sentences":["In this paper, we are concerned with the H\\\"older regularity for solutions of the nonlocal evolutionary equation $$ \\partial_t u+(-\\Delta_p)^s u = 0.","$$ Here, $(-\\Delta_p)^s$ is the fractional $p$-Laplacian, $0<s<1$ and $1<p<2$. We establish H\\\"older regularity with explicit H\\\"older exponents.","We also include the inhomogeneous equation with a bounded inhomogeneity.","In some cases, the obtained H\\\"older exponents are almost sharp.","Our results complement the previous results for the superquadratic case when $p\\geq 2$."],"url":"http://arxiv.org/abs/2404.16640v1","category":"math.AP"}
{"created":"2024-04-25 14:09:41","title":"Long-term stellar activity of M dwarfs: A combined K2 and TESS study of two early M-type stars","abstract":"Studies of the rotation and activity of M type stars are essential to enhance our understanding of stellar dynamos and angular momentum evolution. Using the outstanding photometric capabilities of space telescopes rotation signals even with low amplitudes can be investigated in up to now unrivaled detail. By combining data of K2 and the TESS prime mission the star spot activity of M dwarfs can be monitored on half a decade timescale. In the framework of our study on the rotation-activity relation for bright and nearby M dwarfs we also aim at an investigation of the long-term activity. While K2 was observing fields distributed around the ecliptic plane, the TESS prime mission was oriented along a line of ecliptic longitude with one camera centered on an ecliptic pole. Due to these different observing strategies, the overlap between K2 and the TESS prime mission is marginal. However, 45 stars from our sample were observed with both missions of which two early M-type stars that fulfill our selection criteria, EPIC 202059229 and EPIC 245919787, were analyzed in more detail. We found that for both stars the rotation period did not change while the rotational phase did change for EPIC 245919787 by ~0.2. The amplitude of the spot induced variability changed for both stars but more significant for EPIC 245919787. By comparing the cumulative flare frequency distributions we found that the flare activity for EPIC 202059229 is unchanged while it slightly changes for EPIC 245919787 between the K2 and TESS epochs. Using a combination of light curves from K2 and TESS that span a baseline up to 4.5 years we could measure significant differential rotation for EPIC 245919787. Furthermore, we show that combining missions like K2 and TESS is a promising method for detecting stellar activity cycles.","sentences":["Studies of the rotation and activity of M type stars are essential to enhance our understanding of stellar dynamos and angular momentum evolution.","Using the outstanding photometric capabilities of space telescopes rotation signals even with low amplitudes can be investigated in up to now unrivaled detail.","By combining data of K2 and the TESS prime mission the star spot activity of M dwarfs can be monitored on half a decade timescale.","In the framework of our study on the rotation-activity relation for bright and nearby M dwarfs we also aim at an investigation of the long-term activity.","While K2 was observing fields distributed around the ecliptic plane, the TESS prime mission was oriented along a line of ecliptic longitude with one camera centered on an ecliptic pole.","Due to these different observing strategies, the overlap between K2 and the TESS prime mission is marginal.","However, 45 stars from our sample were observed with both missions of which two early M-type stars that fulfill our selection criteria, EPIC 202059229 and EPIC 245919787, were analyzed in more detail.","We found that for both stars the rotation period did not change while the rotational phase did change for EPIC 245919787 by ~0.2.","The amplitude of the spot induced variability changed for both stars but more significant for EPIC 245919787.","By comparing the cumulative flare frequency distributions we found that the flare activity for EPIC 202059229 is unchanged while it slightly changes for EPIC 245919787 between the K2 and TESS epochs.","Using a combination of light curves from K2 and TESS that span a baseline up to 4.5 years we could measure significant differential rotation for EPIC 245919787.","Furthermore, we show that combining missions like K2 and TESS is a promising method for detecting stellar activity cycles."],"url":"http://arxiv.org/abs/2404.16625v1","category":"astro-ph.SR"}
{"created":"2024-04-25 13:41:18","title":"A Mathematical Framework for Spatio-Temporal Control in Industrial Drying","abstract":"We introduce two models of industrial drying - a simplified one-equation model, and a detailed three-equation model. The purpose of the simplified model is rigorous validation of numerical methods for PDE-constrained optimal control. The purpose of the detailed model is to be able to predict and control the behaviour of an industrial disk drier. For both models, we introduce a fully validated numerical method to compute the optimal source term to maintain the outlet temperature as close as possible to the set-point temperature. By performing simulations using realistic parameters for industrial driers, we illustrate potential applications of the method.","sentences":["We introduce two models of industrial drying - a simplified one-equation model, and a detailed three-equation model.","The purpose of the simplified model is rigorous validation of numerical methods for PDE-constrained optimal control.","The purpose of the detailed model is to be able to predict and control the behaviour of an industrial disk drier.","For both models, we introduce a fully validated numerical method to compute the optimal source term to maintain the outlet temperature as close as possible to the set-point temperature.","By performing simulations using realistic parameters for industrial driers, we illustrate potential applications of the method."],"url":"http://arxiv.org/abs/2404.16604v1","category":"math.OC"}
{"created":"2024-04-25 13:14:59","title":"Andreev reflection, Andreev states, and long ballistic SNS junction","abstract":"The analysis in the present paper is based on the most known concept introduced by the brilliant physicist Alexander Andreev: Andreev bound states in a normal metal sandwiched between two superconductors. The paper presents results of direct calculations of {\\em ab initio} expressions for the currents in a long ballistic SNS junction. The expressions are expanded in $1/L$ ($L$ is the thickness of the normal layer). The main contribution $\\propto 1/L$ to the current agrees with the results obtained in the past, but the analysis suggests a new physical picture of the charge transport through the junction free from the problem with the charge conservation law. The saw-tooth current-phase relation at $T=0$ directly follows from the Galilean invariance of the Bogolyubov-de Gennes equations proved in the paper. The proof is valid for any variation of the energy gap in space if the Andreev reflection is the only scattering process. The respective roles of the contributions of bound and continuum states to the current are clarified. They depend on the junction dimensionality.","sentences":["The analysis in the present paper is based on the most known concept introduced by the brilliant physicist Alexander Andreev:","Andreev bound states in a normal metal sandwiched between two superconductors.","The paper presents results of direct calculations of {\\em ab initio} expressions for the currents in a long ballistic SNS junction.","The expressions are expanded in $1/L$ ($L$ is the thickness of the normal layer).","The main contribution $\\propto 1/L$ to the current agrees with the results obtained in the past, but the analysis suggests a new physical picture of the charge transport through the junction free from the problem with the charge conservation law.","The saw-tooth current-phase relation at $T=0$ directly follows from the Galilean invariance of the Bogolyubov-de Gennes equations proved in the paper.","The proof is valid for any variation of the energy gap in space if the Andreev reflection is the only scattering process.","The respective roles of the contributions of bound and continuum states to the current are clarified.","They depend on the junction dimensionality."],"url":"http://arxiv.org/abs/2404.16593v1","category":"cond-mat.supr-con"}
{"created":"2024-04-25 12:07:41","title":"Surprisingly Strong Performance Prediction with Neural Graph Features","abstract":"Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training. Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks. Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training. However, they are still poorly understood, exhibit biases with network properties, and their performance is limited. Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs. GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings. In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost.","sentences":["Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training.","Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks.","Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training.","However, they are still poorly understood, exhibit biases with network properties, and their performance is limited.","Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs.","GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings.","In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost."],"url":"http://arxiv.org/abs/2404.16551v1","category":"cs.LG"}
{"created":"2024-04-25 11:27:58","title":"A Deep Learning-Driven Pipeline for Differentiating Hypertrophic Cardiomyopathy from Cardiac Amyloidosis Using 2D Multi-View Echocardiography","abstract":"Hypertrophic cardiomyopathy (HCM) and cardiac amyloidosis (CA) are both heart conditions that can progress to heart failure if untreated. They exhibit similar echocardiographic characteristics, often leading to diagnostic challenges. This paper introduces a novel multi-view deep learning approach that utilizes 2D echocardiography for differentiating between HCM and CA. The method begins by classifying 2D echocardiography data into five distinct echocardiographic views: apical 4-chamber, parasternal long axis of left ventricle, parasternal short axis at levels of the mitral valve, papillary muscle, and apex. It then extracts features of each view separately and combines five features for disease classification. A total of 212 patients diagnosed with HCM, and 30 patients diagnosed with CA, along with 200 individuals with normal cardiac function(Normal), were enrolled in this study from 2018 to 2022. This approach achieved a precision, recall of 0.905, and micro-F1 score of 0.904, demonstrating its effectiveness in accurately identifying HCM and CA using a multi-view analysis.","sentences":["Hypertrophic cardiomyopathy (HCM) and cardiac amyloidosis (CA) are both heart conditions that can progress to heart failure if untreated.","They exhibit similar echocardiographic characteristics, often leading to diagnostic challenges.","This paper introduces a novel multi-view deep learning approach that utilizes 2D echocardiography for differentiating between HCM and CA.","The method begins by classifying 2D echocardiography data into five distinct echocardiographic views: apical 4-chamber, parasternal long axis of left ventricle, parasternal short axis at levels of the mitral valve, papillary muscle, and apex.","It then extracts features of each view separately and combines five features for disease classification.","A total of 212 patients diagnosed with HCM, and 30 patients diagnosed with CA, along with 200 individuals with normal cardiac function(Normal), were enrolled in this study from 2018 to 2022.","This approach achieved a precision, recall of 0.905, and micro-F1 score of 0.904, demonstrating its effectiveness in accurately identifying HCM and CA using a multi-view analysis."],"url":"http://arxiv.org/abs/2404.16522v1","category":"eess.IV"}
{"created":"2024-04-25 09:29:14","title":"Inverse Spectral Problems for Collapsing Manifolds II: Quantitative Stability of Reconstruction for Orbifolds","abstract":"We consider the inverse problem of determining the metric-measure structure of collapsing manifolds from local measurements of spectral data. In the part I of the paper, we proved the uniqueness of the inverse problem and a continuity result for the stability in the closure of Riemannian manifolds with bounded diameter and sectional curvature in the measured Gromov-Hausdorff topology. In this paper we show that when the collapse of dimension is $1$-dimensional, it is possible to obtain quantitative stability of the inverse problem for Riemannian orbifolds. The proof is based on an improved version of the quantitative unique continuation for the wave operator on Riemannian manifolds by removing assumptions on the covariant derivatives of the curvature tensor.","sentences":["We consider the inverse problem of determining the metric-measure structure of collapsing manifolds from local measurements of spectral data.","In the part I of the paper, we proved the uniqueness of the inverse problem and a continuity result for the stability in the closure of Riemannian manifolds with bounded diameter and sectional curvature in the measured Gromov-Hausdorff topology.","In this paper we show that when the collapse of dimension is $1$-dimensional, it is possible to obtain quantitative stability of the inverse problem for Riemannian orbifolds.","The proof is based on an improved version of the quantitative unique continuation for the wave operator on Riemannian manifolds by removing assumptions on the covariant derivatives of the curvature tensor."],"url":"http://arxiv.org/abs/2404.16448v1","category":"math.AP"}
{"created":"2024-04-25 09:16:23","title":"Exponential decay for fractional Schr\u00f6dinger parabolic problems","abstract":"We discuss exponential decay in $L^p(R^N)$, $1\\leq p\\leq \\infty$,of solutions of a fractional Schr\\\"odinger parabolic equation with a locally uniformly integrable potential. The exponential type of the semigroup of solutions is considered and its independence in of $1\\leq p\\leq \\infty$ is addressed. We characterise a large class of potentials for which solutions decay exponentially.","sentences":["We discuss exponential decay in $L^p(R^N)$, $1\\leq p\\leq \\infty$,of solutions of a fractional Schr\\\"odinger parabolic equation with a locally uniformly integrable potential.","The exponential type of the semigroup of solutions is considered and its independence in of $1\\leq p\\leq \\infty$ is addressed.","We characterise a large class of potentials for which solutions decay exponentially."],"url":"http://arxiv.org/abs/2404.16438v1","category":"math.AP"}
{"created":"2024-04-25 09:02:11","title":"Depth Supervised Neural Surface Reconstruction from Airborne Imagery","abstract":"While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS). Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches. However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing. For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows. Furthermore, training such networks is computationally expensive. Thus, the aim of our work is twofold: First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery. Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment. Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs. For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images.","sentences":["While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS).","Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches.","However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing.","For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows.","Furthermore, training such networks is computationally expensive.","Thus, the aim of our work is twofold:","First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery.","Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment.","Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs.","For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images."],"url":"http://arxiv.org/abs/2404.16429v1","category":"cs.CV"}
{"created":"2024-04-25 08:56:35","title":"Instanton's Insertions to arbitrary non flat Connections in $\\mathbb{R}^4$","abstract":"Given a connection $A$ on a $SU(2)$-bundle $P$ over $\\mathbb{R}^4$ with finite Yang-Mills energy $YM(A)$ and nonzero curvature $F_A(0)$ at the origin, and given $\\rho>0$ small enough, we construct a new connection $\\hat A$ on a bundle $\\hat P$ of different Chern class ($|c_2(A)-c_2(\\hat A)|=8\\pi^2$), in such a way that $\\hat A$ is gauge equivalent to $A$ in $\\mathbb{R}^4\\setminus B_\\rho(0)$ and $$YM(\\hat A)\\le YM(A)+8\\pi^2-\\varepsilon_0\\rho^4|F_A(0)|^2$$ for a universal constant $\\varepsilon_0>0$.","sentences":["Given a connection $A$ on a $SU(2)$-bundle $P$ over $\\mathbb{R}^4$ with finite Yang-Mills energy $YM(A)$ and nonzero curvature $F_A(0)$ at the origin, and given $\\rho>0$ small enough, we construct a new connection $\\hat A$ on a bundle $\\hat P$ of different Chern class ($|c_2(A)-c_2(\\hat A)|=8\\pi^2$), in such a way that $\\hat A$ is gauge equivalent to $A$ in $\\mathbb{R}^4\\setminus B_\\rho(0)$ and $$YM(\\hat A)\\le YM(A)+8\\pi^2-\\varepsilon_0\\rho^4|F_A(0)|^2$$ for a universal constant $\\varepsilon_0>0$."],"url":"http://arxiv.org/abs/2404.16426v1","category":"math.DG"}
{"created":"2024-04-25 17:58:09","title":"Boosting Unsupervised Semantic Segmentation with Principal Mask Proposals","abstract":"Unsupervised semantic segmentation aims to automatically partition images into semantically meaningful regions by identifying global categories within an image corpus without any form of annotation. Building upon recent advances in self-supervised representation learning, we focus on how to leverage these large pre-trained models for the downstream task of unsupervised segmentation. We present PriMaPs - Principal Mask Proposals - decomposing images into semantically meaningful masks based on their feature representation. This allows us to realize unsupervised semantic segmentation by fitting class prototypes to PriMaPs with a stochastic expectation-maximization algorithm, PriMaPs-EM. Despite its conceptual simplicity, PriMaPs-EM leads to competitive results across various pre-trained backbone models, including DINO and DINOv2, and across datasets, such as Cityscapes, COCO-Stuff, and Potsdam-3. Importantly, PriMaPs-EM is able to boost results when applied orthogonally to current state-of-the-art unsupervised semantic segmentation pipelines.","sentences":["Unsupervised semantic segmentation aims to automatically partition images into semantically meaningful regions by identifying global categories within an image corpus without any form of annotation.","Building upon recent advances in self-supervised representation learning, we focus on how to leverage these large pre-trained models for the downstream task of unsupervised segmentation.","We present PriMaPs - Principal Mask Proposals - decomposing images into semantically meaningful masks based on their feature representation.","This allows us to realize unsupervised semantic segmentation by fitting class prototypes to PriMaPs with a stochastic expectation-maximization algorithm, PriMaPs-EM.","Despite its conceptual simplicity, PriMaPs-EM leads to competitive results across various pre-trained backbone models, including DINO and DINOv2, and across datasets, such as Cityscapes, COCO-Stuff, and Potsdam-3.","Importantly, PriMaPs-EM is able to boost results when applied orthogonally to current state-of-the-art unsupervised semantic segmentation pipelines."],"url":"http://arxiv.org/abs/2404.16818v1","category":"cs.CV"}
{"created":"2024-04-25 17:30:38","title":"Registration by Regression (RbR): a framework for interpretable and flexible atlas registration","abstract":"In human neuroimaging studies, atlas registration enables mapping MRI scans to a common coordinate frame, which is necessary to aggregate data from multiple subjects. Machine learning registration methods have achieved excellent speed and accuracy but lack interpretability. More recently, keypoint-based methods have been proposed to tackle this issue, but their accuracy is still subpar, particularly when fitting nonlinear transforms. Here we propose Registration by Regression (RbR), a novel atlas registration framework that is highly robust and flexible, conceptually simple, and can be trained with cheaply obtained data. RbR predicts the (x,y,z) atlas coordinates for every voxel of the input scan (i.e., every voxel is a keypoint), and then uses closed-form expressions to quickly fit transforms using a wide array of possible deformation models, including affine and nonlinear (e.g., Bspline, Demons, invertible diffeomorphic models, etc.). Robustness is provided by the large number of voxels informing the registration and can be further increased by robust estimators like RANSAC. Experiments on independent public datasets show that RbR yields more accurate registration than competing keypoint approaches, while providing full control of the deformation model.","sentences":["In human neuroimaging studies, atlas registration enables mapping MRI scans to a common coordinate frame, which is necessary to aggregate data from multiple subjects.","Machine learning registration methods have achieved excellent speed and accuracy but lack interpretability.","More recently, keypoint-based methods have been proposed to tackle this issue, but their accuracy is still subpar, particularly when fitting nonlinear transforms.","Here we propose Registration by Regression (RbR), a novel atlas registration framework that is highly robust and flexible, conceptually simple, and can be trained with cheaply obtained data.","RbR predicts the (x,y,z) atlas coordinates for every voxel of the input scan (i.e., every voxel is a keypoint), and then uses closed-form expressions to quickly fit transforms using a wide array of possible deformation models, including affine and nonlinear (e.g., Bspline, Demons, invertible diffeomorphic models, etc.).","Robustness is provided by the large number of voxels informing the registration and can be further increased by robust estimators like RANSAC.","Experiments on independent public datasets show that RbR yields more accurate registration than competing keypoint approaches, while providing full control of the deformation model."],"url":"http://arxiv.org/abs/2404.16781v1","category":"cs.CV"}
{"created":"2024-04-25 16:39:32","title":"History repeats itself: A Baseline for Temporal Knowledge Graph Forecasting","abstract":"Temporal Knowledge Graph (TKG) Forecasting aims at predicting links in Knowledge Graphs for future timesteps based on a history of Knowledge Graphs. To this day, standardized evaluation protocols and rigorous comparison across TKG models are available, but the importance of simple baselines is often neglected in the evaluation, which prevents researchers from discerning actual and fictitious progress. We propose to close this gap by designing an intuitive baseline for TKG Forecasting based on predicting recurring facts. Compared to most TKG models, it requires little hyperparameter tuning and no iterative training. Further, it can help to identify failure modes in existing approaches. The empirical findings are quite unexpected: compared to 11 methods on five datasets, our baseline ranks first or third in three of them, painting a radically different picture of the predictive quality of the state of the art.","sentences":["Temporal Knowledge Graph (TKG) Forecasting aims at predicting links in Knowledge Graphs for future timesteps based on a history of Knowledge Graphs.","To this day, standardized evaluation protocols and rigorous comparison across TKG models are available, but the importance of simple baselines is often neglected in the evaluation, which prevents researchers from discerning actual and fictitious progress.","We propose to close this gap by designing an intuitive baseline for TKG Forecasting based on predicting recurring facts.","Compared to most TKG models, it requires little hyperparameter tuning and no iterative training.","Further, it can help to identify failure modes in existing approaches.","The empirical findings are quite unexpected: compared to 11 methods on five datasets, our baseline ranks first or third in three of them, painting a radically different picture of the predictive quality of the state of the art."],"url":"http://arxiv.org/abs/2404.16726v1","category":"cs.LG"}
{"created":"2024-04-25 16:37:58","title":"Tverberg's theorem and multi-class support vector machines","abstract":"We show how, using linear-algebraic tools developed to prove Tverberg's theorem in combinatorial geometry, we can design new models of multi-class support vector machines (SVMs). These supervised learning protocols require fewer conditions to classify sets of points, and can be computed using existing binary SVM algorithms in higher-dimensional spaces, including soft-margin SVM algorithms. We describe how the theoretical guarantees of standard support vector machines transfer to these new classes of multi-class support vector machines. We give a new simple proof of a geometric characterization of support vectors for largest margin SVMs by Veelaert.","sentences":["We show how, using linear-algebraic tools developed to prove Tverberg's theorem in combinatorial geometry, we can design new models of multi-class support vector machines (SVMs).","These supervised learning protocols require fewer conditions to classify sets of points, and can be computed using existing binary SVM algorithms in higher-dimensional spaces, including soft-margin SVM algorithms.","We describe how the theoretical guarantees of standard support vector machines transfer to these new classes of multi-class support vector machines.","We give a new simple proof of a geometric characterization of support vectors for largest margin SVMs by Veelaert."],"url":"http://arxiv.org/abs/2404.16724v1","category":"cs.LG"}
{"created":"2024-04-25 14:10:52","title":"Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer","abstract":"Unsupervised cross-lingual transfer involves transferring knowledge between languages without explicit supervision. Although numerous studies have been conducted to improve performance in such tasks by focusing on cross-lingual knowledge, particularly lexical and syntactic knowledge, current approaches are limited as they only incorporate syntactic or lexical information. Since each type of information offers unique advantages and no previous attempts have combined both, we attempt to explore the potential of this approach. In this paper, we present a novel framework called \"Lexicon-Syntax Enhanced Multilingual BERT\" that combines both lexical and syntactic knowledge. Specifically, we use Multilingual BERT (mBERT) as the base model and employ two techniques to enhance its learning capabilities. The code-switching technique is used to implicitly teach the model lexical alignment information, while a syntactic-based graph attention network is designed to help the model encode syntactic structure. To integrate both types of knowledge, we input code-switched sequences into both the syntactic module and the mBERT base model simultaneously. Our extensive experimental results demonstrate this framework can consistently outperform all baselines of zero-shot cross-lingual transfer, with the gains of 1.0~3.7 points on text classification, named entity recognition (ner), and semantic parsing tasks. Keywords:cross-lingual transfer, lexicon, syntax, code-switching, graph attention network","sentences":["Unsupervised cross-lingual transfer involves transferring knowledge between languages without explicit supervision.","Although numerous studies have been conducted to improve performance in such tasks by focusing on cross-lingual knowledge, particularly lexical and syntactic knowledge, current approaches are limited as they only incorporate syntactic or lexical information.","Since each type of information offers unique advantages and no previous attempts have combined both, we attempt to explore the potential of this approach.","In this paper, we present a novel framework called \"Lexicon-Syntax Enhanced Multilingual BERT\" that combines both lexical and syntactic knowledge.","Specifically, we use Multilingual BERT (mBERT) as the base model and employ two techniques to enhance its learning capabilities.","The code-switching technique is used to implicitly teach the model lexical alignment information, while a syntactic-based graph attention network is designed to help the model encode syntactic structure.","To integrate both types of knowledge, we input code-switched sequences into both the syntactic module and the mBERT base model simultaneously.","Our extensive experimental results demonstrate this framework can consistently outperform all baselines of zero-shot cross-lingual transfer, with the gains of 1.0~3.7 points on text classification, named entity recognition (ner), and semantic parsing tasks.","Keywords:cross-lingual transfer, lexicon, syntax, code-switching, graph attention network"],"url":"http://arxiv.org/abs/2404.16627v1","category":"cs.CL"}
{"created":"2024-04-25 13:56:05","title":"Robust Capped lp-Norm Support Vector Ordinal Regression","abstract":"Ordinal regression is a specialized supervised problem where the labels show an inherent order. The order distinguishes it from normal multi-class problem. Support Vector Ordinal Regression, as an outstanding ordinal regression model, is widely used in many ordinal regression tasks. However, like most supervised learning algorithms, the design of SVOR is based on the assumption that the training data are real and reliable, which is difficult to satisfy in real-world data. In many practical applications, outliers are frequently present in the training set, potentially leading to misguide the learning process, such that the performance is non-optimal. In this paper, we propose a novel capped $\\ell_{p}$-norm loss function that is theoretically robust to both light and heavy outliers. The capped $\\ell_{p}$-norm loss can help the model detect and eliminate outliers during training process. Adhering to this concept, we introduce a new model, Capped $\\ell_{p}$-Norm Support Vector Ordinal Regression(CSVOR), that is robust to outliers. CSVOR uses a weight matrix to detect and eliminate outliers during the training process to improve the robustness to outliers. Moreover, a Re-Weighted algorithm algorithm which is illustrated convergence by our theoretical results is proposed to effectively minimize the corresponding problem. Extensive experimental results demonstrate that our model outperforms state-of-the-art(SOTA) methods, particularly in the presence of outliers.","sentences":["Ordinal regression is a specialized supervised problem where the labels show an inherent order.","The order distinguishes it from normal multi-class problem.","Support Vector Ordinal Regression, as an outstanding ordinal regression model, is widely used in many ordinal regression tasks.","However, like most supervised learning algorithms, the design of SVOR is based on the assumption that the training data are real and reliable, which is difficult to satisfy in real-world data.","In many practical applications, outliers are frequently present in the training set, potentially leading to misguide the learning process, such that the performance is non-optimal.","In this paper, we propose a novel capped $\\ell_{p}$-norm loss function that is theoretically robust to both light and heavy outliers.","The capped $\\ell_{p}$-norm loss can help the model detect and eliminate outliers during training process.","Adhering to this concept, we introduce a new model, Capped $\\ell_{p}$-Norm Support Vector Ordinal Regression(CSVOR), that is robust to outliers.","CSVOR uses a weight matrix to detect and eliminate outliers during the training process to improve the robustness to outliers.","Moreover, a Re-Weighted algorithm algorithm which is illustrated convergence by our theoretical results is proposed to effectively minimize the corresponding problem.","Extensive experimental results demonstrate that our model outperforms state-of-the-art(SOTA) methods, particularly in the presence of outliers."],"url":"http://arxiv.org/abs/2404.16616v1","category":"cs.LG"}
{"created":"2024-04-25 12:36:19","title":"Exploring Internal Numeracy in Language Models: A Case Study on ALBERT","abstract":"It has been found that Transformer-based language models have the ability to perform basic quantitative reasoning. In this paper, we propose a method for studying how these models internally represent numerical data, and use our proposal to analyze the ALBERT family of language models. Specifically, we extract the learned embeddings these models use to represent tokens that correspond to numbers and ordinals, and subject these embeddings to Principal Component Analysis (PCA). PCA results reveal that ALBERT models of different sizes, trained and initialized separately, consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts. Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D space. Our findings illustrate that language models, trained purely to model text, can intuit basic mathematical concepts, opening avenues for NLP applications that intersect with quantitative reasoning.","sentences":["It has been found that Transformer-based language models have the ability to perform basic quantitative reasoning.","In this paper, we propose a method for studying how these models internally represent numerical data, and use our proposal to analyze the ALBERT family of language models.","Specifically, we extract the learned embeddings these models use to represent tokens that correspond to numbers and ordinals, and subject these embeddings to Principal Component Analysis (PCA).","PCA results reveal that ALBERT models of different sizes, trained and initialized separately, consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts.","Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D space.","Our findings illustrate that language models, trained purely to model text, can intuit basic mathematical concepts, opening avenues for NLP applications that intersect with quantitative reasoning."],"url":"http://arxiv.org/abs/2404.16574v1","category":"cs.CL"}
{"created":"2024-04-25 12:35:27","title":"Multi-Scale Representations by Varying Window Attention for Semantic Segmentation","abstract":"Multi-scale learning is central to semantic segmentation. We visualize the effective receptive field (ERF) of canonical multi-scale representations and point out two risks in learning them: scale inadequacy and field inactivation. A novel multi-scale learner, varying window attention (VWA), is presented to address these issues. VWA leverages the local window attention (LWA) and disentangles LWA into the query window and context window, allowing the context's scale to vary for the query to learn representations at multiple scales. However, varying the context to large-scale windows (enlarging ratio R) can significantly increase the memory footprint and computation cost (R^2 times larger than LWA). We propose a simple but professional re-scaling strategy to zero the extra induced cost without compromising performance. Consequently, VWA uses the same cost as LWA to overcome the receptive limitation of the local window. Furthermore, depending on VWA and employing various MLPs, we introduce a multi-scale decoder (MSD), VWFormer, to improve multi-scale representations for semantic segmentation. VWFormer achieves efficiency competitive with the most compute-friendly MSDs, like FPN and MLP decoder, but performs much better than any MSDs. For instance, using nearly half of UPerNet's computation, VWFormer outperforms it by 1.0%-2.5% mIoU on ADE20K. With little extra overhead, ~10G FLOPs, Mask2Former armed with VWFormer improves by 1.0%-1.3%.","sentences":["Multi-scale learning is central to semantic segmentation.","We visualize the effective receptive field (ERF) of canonical multi-scale representations and point out two risks in learning them: scale inadequacy and field inactivation.","A novel multi-scale learner, varying window attention (VWA), is presented to address these issues.","VWA leverages the local window attention (LWA) and disentangles LWA into the query window and context window, allowing the context's scale to vary for the query to learn representations at multiple scales.","However, varying the context to large-scale windows (enlarging ratio R) can significantly increase the memory footprint and computation cost (R^2 times larger than LWA).","We propose a simple but professional re-scaling strategy to zero the extra induced cost without compromising performance.","Consequently, VWA uses the same cost as LWA to overcome the receptive limitation of the local window.","Furthermore, depending on VWA and employing various MLPs, we introduce a multi-scale decoder (MSD), VWFormer, to improve multi-scale representations for semantic segmentation.","VWFormer achieves efficiency competitive with the most compute-friendly MSDs, like FPN and MLP decoder, but performs much better than any MSDs.","For instance, using nearly half of UPerNet's computation, VWFormer outperforms it by 1.0%-2.5% mIoU on ADE20K. With little extra overhead, ~10G FLOPs, Mask2Former armed with VWFormer improves by 1.0%-1.3%."],"url":"http://arxiv.org/abs/2404.16573v1","category":"cs.CV"}
{"created":"2024-04-25 12:27:59","title":"PyRadar: Towards Automatically Retrieving and Validating Source Code Repository Information for PyPI Packages","abstract":"A package's source code repository records the development history of the package, providing indispensable information for the use and risk monitoring of the package. However, a package release often misses its source code repository due to the separation of the package's development platform from its distribution platform. Existing tools retrieve the release's repository information from its metadata, which suffers from two limitations: the metadata may not contain or contain wrong information. Our analysis shows that existing tools can only retrieve repository information for up to 70.5% of PyPI releases. To address the limitations, this paper proposes PyRadar, a novel framework that utilizes the metadata and source distribution to retrieve and validate the repository information for PyPI releases. We start with an empirical study to compare four existing tools on 4,227,425 PyPI releases and analyze phantom files (files appearing in the release's distribution but not in the release's repository) in 14,375 correct package-repository links and 2,064 incorrect links. Based on the findings, we design PyRadar with three components, i.e., Metadata-based Retriever, Source Code Repository Validator, and Source Code-based Retriever. In particular, the Metadata-based Retriever combines best practices of existing tools and successfully retrieves repository information from the metadata for 72.1% of PyPI releases. The Source Code Repository Validator applies common machine learning algorithms on six crafted features and achieves an AUC of up to 0.995. The Source Code-based Retriever queries World of Code with the SHA-1 hashes of all Python files in the release's source distribution and retrieves repository information for 90.2% of packages in our dataset with an accuracy of 0.970. Both practitioners and researchers can employ the PyRadar to better use PyPI packages.","sentences":["A package's source code repository records the development history of the package, providing indispensable information for the use and risk monitoring of the package.","However, a package release often misses its source code repository due to the separation of the package's development platform from its distribution platform.","Existing tools retrieve the release's repository information from its metadata, which suffers from two limitations: the metadata may not contain or contain wrong information.","Our analysis shows that existing tools can only retrieve repository information for up to 70.5% of PyPI releases.","To address the limitations, this paper proposes PyRadar, a novel framework that utilizes the metadata and source distribution to retrieve and validate the repository information for PyPI releases.","We start with an empirical study to compare four existing tools on 4,227,425 PyPI releases and analyze phantom files (files appearing in the release's distribution but not in the release's repository) in 14,375 correct package-repository links and 2,064 incorrect links.","Based on the findings, we design PyRadar with three components, i.e., Metadata-based Retriever, Source Code Repository Validator, and Source Code-based Retriever.","In particular, the Metadata-based Retriever combines best practices of existing tools and successfully retrieves repository information from the metadata for 72.1% of PyPI releases.","The Source Code Repository Validator applies common machine learning algorithms on six crafted features and achieves an AUC of up to 0.995.","The Source Code-based Retriever queries World of Code with the SHA-1 hashes of all Python files in the release's source distribution and retrieves repository information for 90.2% of packages in our dataset with an accuracy of 0.970.","Both practitioners and researchers can employ the PyRadar to better use PyPI packages."],"url":"http://arxiv.org/abs/2404.16565v1","category":"cs.SE"}
{"created":"2024-04-25 11:36:10","title":"3D deep learning for enhanced atom probe tomography analysis of nanoscale microstructures","abstract":"Quantitative analysis of microstructural features on the nanoscale, including precipitates, local chemical orderings (LCOs) or structural defects (e.g. stacking faults) plays a pivotal role in understanding the mechanical and physical responses of engineering materials. Atom probe tomography (APT), known for its exceptional combination of chemical sensitivity and sub-nanometer resolution, primarily identifies microstructures through compositional segregations. However, this fails when there is no significant segregation, as can be the case for LCOs and stacking faults. Here, we introduce a 3D deep learning approach, AtomNet, designed to process APT point cloud data at the single-atom level for nanoscale microstructure extraction, simultaneously considering compositional and structural information. AtomNet is showcased in segmenting L12-type nanoprecipitates from the matrix in an AlLiMg alloy, irrespective of crystallographic orientations, which outperforms previous methods. AtomNet also allows for 3D imaging of L10-type LCOs in an AuCu alloy, a challenging task for conventional analysis due to their small size and subtle compositional differences. Finally, we demonstrate the use of AtomNet for revealing 2D stacking faults in a Co-based superalloy, without any defected training data, expanding the capabilities of APT for automated exploration of hidden microstructures. AtomNet pushes the boundaries of APT analysis, and holds promise in establishing precise quantitative microstructure-property relationships across a diverse range of metallic materials.","sentences":["Quantitative analysis of microstructural features on the nanoscale, including precipitates, local chemical orderings (LCOs) or structural defects (e.g. stacking faults) plays a pivotal role in understanding the mechanical and physical responses of engineering materials.","Atom probe tomography (APT), known for its exceptional combination of chemical sensitivity and sub-nanometer resolution, primarily identifies microstructures through compositional segregations.","However, this fails when there is no significant segregation, as can be the case for LCOs and stacking faults.","Here, we introduce a 3D deep learning approach, AtomNet, designed to process APT point cloud data at the single-atom level for nanoscale microstructure extraction, simultaneously considering compositional and structural information.","AtomNet is showcased in segmenting L12-type nanoprecipitates from the matrix in an AlLiMg alloy, irrespective of crystallographic orientations, which outperforms previous methods.","AtomNet also allows for 3D imaging of L10-type LCOs in an AuCu alloy, a challenging task for conventional analysis due to their small size and subtle compositional differences.","Finally, we demonstrate the use of AtomNet for revealing 2D stacking faults in a Co-based superalloy, without any defected training data, expanding the capabilities of APT for automated exploration of hidden microstructures.","AtomNet pushes the boundaries of APT analysis, and holds promise in establishing precise quantitative microstructure-property relationships across a diverse range of metallic materials."],"url":"http://arxiv.org/abs/2404.16524v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 10:57:50","title":"Efficient algorithms for regularized Poisson Non-negative Matrix Factorization","abstract":"We consider the problem of regularized Poisson Non-negative Matrix Factorization (NMF) problem, encompassing various regularization terms such as Lipschitz and relatively smooth functions, alongside linear constraints. This problem holds significant relevance in numerous Machine Learning applications, particularly within the domain of physical linear unmixing problems. A notable challenge arises from the main loss term in the Poisson NMF problem being a KL divergence, which is non-Lipschitz, rendering traditional gradient descent-based approaches inefficient. In this contribution, we explore the utilization of Block Successive Upper Minimization (BSUM) to overcome this challenge. We build approriate majorizing function for Lipschitz and relatively smooth functions, and show how to introduce linear constraints into the problem. This results in the development of two novel algorithms for regularized Poisson NMF. We conduct numerical simulations to showcase the effectiveness of our approach.","sentences":["We consider the problem of regularized Poisson Non-negative Matrix Factorization (NMF) problem, encompassing various regularization terms such as Lipschitz and relatively smooth functions, alongside linear constraints.","This problem holds significant relevance in numerous Machine Learning applications, particularly within the domain of physical linear unmixing problems.","A notable challenge arises from the main loss term in the Poisson NMF problem being a KL divergence, which is non-Lipschitz, rendering traditional gradient descent-based approaches inefficient.","In this contribution, we explore the utilization of Block Successive Upper Minimization (BSUM) to overcome this challenge.","We build approriate majorizing function for Lipschitz and relatively smooth functions, and show how to introduce linear constraints into the problem.","This results in the development of two novel algorithms for regularized Poisson NMF.","We conduct numerical simulations to showcase the effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.16505v1","category":"cs.LG"}
{"created":"2024-04-25 09:07:19","title":"Point-JEPA: A Joint Embedding Predictive Architecture for Self-Supervised Learning on Point Cloud","abstract":"Recent advancements in self-supervised learning in the point cloud domain have demonstrated significant potential. However, these methods often suffer from drawbacks, including lengthy pre-training time, the necessity of reconstruction in the input space, or the necessity of additional modalities. In order to address these issues, we introduce Point-JEPA, a joint embedding predictive architecture designed specifically for point cloud data. To this end, we introduce a sequencer that orders point cloud tokens to efficiently compute and utilize tokens proximity based on their indices during target and context selection. The sequencer also allows shared computations of the tokens proximity between context and target selection, further improving the efficiency. Experimentally, our method achieves competitive results with state-of-the-art methods while avoiding the reconstruction in the input space or additional modality.","sentences":["Recent advancements in self-supervised learning in the point cloud domain have demonstrated significant potential.","However, these methods often suffer from drawbacks, including lengthy pre-training time, the necessity of reconstruction in the input space, or the necessity of additional modalities.","In order to address these issues, we introduce Point-JEPA, a joint embedding predictive architecture designed specifically for point cloud data.","To this end, we introduce a sequencer that orders point cloud tokens to efficiently compute and utilize tokens proximity based on their indices during target and context selection.","The sequencer also allows shared computations of the tokens proximity between context and target selection, further improving the efficiency.","Experimentally, our method achieves competitive results with state-of-the-art methods while avoiding the reconstruction in the input space or additional modality."],"url":"http://arxiv.org/abs/2404.16432v1","category":"cs.CV"}
{"created":"2024-04-25 13:55:03","title":"Derandomization with Pseudorandomness","abstract":"Derandomization techniques are often used within advanced randomized algorithms. In particular, pseudorandom objects, such as hash families and expander graphs, are key components of such algorithms, but their verification presents a challenge. This work shows how such algorithms can be expressed and verified in Isabelle and presents a pseudorandom objects library that abstracts away the involved deep algebraic/analytic results. Moreover, it presents examples that show how the library eases and enables the verification of advanced randomized algorithms. Highlighting the value of this framework is that it was recently used to verify the optimal-space distinct elements algorithm by Blasiok from 2018, which relies on the combination of many derandomization techniques to achieve its optimality.","sentences":["Derandomization techniques are often used within advanced randomized algorithms.","In particular, pseudorandom objects, such as hash families and expander graphs, are key components of such algorithms, but their verification presents a challenge.","This work shows how such algorithms can be expressed and verified in Isabelle and presents a pseudorandom objects library that abstracts away the involved deep algebraic/analytic results.","Moreover, it presents examples that show how the library eases and enables the verification of advanced randomized algorithms.","Highlighting the value of this framework is that it was recently used to verify the optimal-space distinct elements algorithm by Blasiok from 2018, which relies on the combination of many derandomization techniques to achieve its optimality."],"url":"http://arxiv.org/abs/2404.16614v1","category":"cs.LO"}
{"created":"2024-04-25 12:52:57","title":"A New Two-Sided Sketching Algorithm for Large-Scale Tensor Decomposition Based on Discrete Cosine Transformation","abstract":"Large tensors are frequently encountered in various fields such as computer vision, scientific simulations, sensor networks, and data mining. However, these tensors are often too large for convenient processing, transfer, or storage. Fortunately, they typically exhibit a low-rank structure that can be leveraged through tensor decomposition. Despite this, performing large-scale tensor decomposition can be time-consuming. Sketching is a useful technique to reduce the dimensionality of the data. In this study, we introduce a novel two-sided sketching method based on the $t$-product decomposition and the discrete cosine transformation. We conduct a thorough theoretical analysis to assess the approximation error of the proposed method. Specifically, we enhance the algorithm with power iteration to achieve more precise approximate solutions. Extensive numerical experiments and comparisons on low-rank approximation of color images and grayscale videos illustrate the efficiency and effectiveness of the proposed approach in terms of both CPU time and approximation accuracy.","sentences":["Large tensors are frequently encountered in various fields such as computer vision, scientific simulations, sensor networks, and data mining.","However, these tensors are often too large for convenient processing, transfer, or storage.","Fortunately, they typically exhibit a low-rank structure that can be leveraged through tensor decomposition.","Despite this, performing large-scale tensor decomposition can be time-consuming.","Sketching is a useful technique to reduce the dimensionality of the data.","In this study, we introduce a novel two-sided sketching method based on the $t$-product decomposition and the discrete cosine transformation.","We conduct a thorough theoretical analysis to assess the approximation error of the proposed method.","Specifically, we enhance the algorithm with power iteration to achieve more precise approximate solutions.","Extensive numerical experiments and comparisons on low-rank approximation of color images and grayscale videos illustrate the efficiency and effectiveness of the proposed approach in terms of both CPU time and approximation accuracy."],"url":"http://arxiv.org/abs/2404.16580v1","category":"math.OC"}
{"created":"2024-04-25 12:03:00","title":"Implementation of matrix compression in the coupling of JOREK to realistic 3D conducting wall structures","abstract":"JOREK is an advanced non-linear simulation code for studying MHD instabilities in magnetically confined fusion plasmas and their control and/or mitigation. A free-boundary and resistive wall extension was introduced via coupling to the STARWALL and CARIDDI codes, both able to provide dense response matrices describing the electromagnetic interactions between plasma and conducting structures. For detailed CAD representations of the conducting structures and high resolutions for the plasma region, memory and computing time limitations restrict the possibility of simulating the ITER tokamak. In the present work, the Singular Value Decomposition provided by routines from the ScaLAPACK library has been successfully applied to compress some of the dense response matrices and thus optimize memory usage. This is demonstrated for simulations of Tearing Mode and Vertical Displacement Event instabilities. An outlook to future applications on large production cases and further extensions of the method are discussed.","sentences":["JOREK is an advanced non-linear simulation code for studying MHD instabilities in magnetically confined fusion plasmas and their control and/or mitigation.","A free-boundary and resistive wall extension was introduced via coupling to the STARWALL and CARIDDI codes, both able to provide dense response matrices describing the electromagnetic interactions between plasma and conducting structures.","For detailed CAD representations of the conducting structures and high resolutions for the plasma region, memory and computing time limitations restrict the possibility of simulating the ITER tokamak.","In the present work, the Singular Value Decomposition provided by routines from the ScaLAPACK library has been successfully applied to compress some of the dense response matrices and thus optimize memory usage.","This is demonstrated for simulations of Tearing Mode and Vertical Displacement Event instabilities.","An outlook to future applications on large production cases and further extensions of the method are discussed."],"url":"http://arxiv.org/abs/2404.16546v1","category":"physics.plasm-ph"}
{"created":"2024-04-25 11:55:29","title":"Approximation Algorithm of Minimum All-Ones Problem for Arbitrary Graphs","abstract":"Let $G=(V, E)$ be a graph and let each vertex of $G$ has a lamp and a button. Each button can be of $\\sigma^+$-type or $\\sigma$-type.   Assume that initially some lamps are on and others are off. The button on vertex $x$ is of $\\sigma^+$-type ($\\sigma$-type, respectively) if pressing the button changes the lamp states on $x$ and on its neighbors in $G$ (the lamp states on the neighbors of $x$ only, respectively). Assume that there is a set $X\\subseteq V$ such that pressing buttons on vertices of $X$ lights all lamps on vertices of $G$. In particular, it is known to hold when initially all lamps are off and all buttons are of $\\sigma^+$-type.   Finding such a set $X$ of the smallest size is NP-hard even if initially all lamps are off and all buttons are of $\\sigma^+$-type. Using a linear algebraic approach we design a polynomial-time approximation algorithm for the problem such that for the set $X$ constructed by the algorithm, we have $|X|\\le \\min\\{r,(|V|+{\\rm opt})/2\\},$ where $r$ is the rank of a (modified) adjacent matrix of $G$ and ${\\rm opt}$ is the size of an optimal solution to the problem.   To the best of our knowledge, this is the first polynomial-time approximation algorithm for the problem with a nontrivial approximation guarantee.","sentences":["Let $G=(V, E)$ be a graph and let each vertex of $G$ has a lamp and a button.","Each button can be of $\\sigma^+$-type or $\\sigma$-type.   ","Assume that initially some lamps are on and others are off.","The button on vertex $x$ is of $\\sigma^+$-type ($\\sigma$-type, respectively) if pressing the button changes the lamp states on $x$ and on its neighbors in $G$ (the lamp states on the neighbors of $x$ only, respectively).","Assume that there is a set $X\\subseteq V$ such that pressing buttons on vertices of $X$ lights all lamps on vertices of $G$. In particular, it is known to hold when initially all lamps are off and all buttons are of $\\sigma^+$-type.   ","Finding such a set $X$ of the smallest size is NP-hard even if initially all lamps are off and all buttons are of $\\sigma^+$-type.","Using a linear algebraic approach we design a polynomial-time approximation algorithm for the problem such that for the set $X$ constructed by the algorithm, we have $|X|\\le \\min\\{r,(|V|+{\\rm opt})/2\\},$ where $r$ is the rank of a (modified) adjacent matrix of $G$ and ${\\rm opt}$ is the size of an optimal solution to the problem.   ","To the best of our knowledge, this is the first polynomial-time approximation algorithm for the problem with a nontrivial approximation guarantee."],"url":"http://arxiv.org/abs/2404.16540v1","category":"cs.DS"}
{"created":"2024-04-25 10:40:09","title":"Theoretical Insights into Inorganic Antiperovskite Nitrides (X$_3$NA; X = Mg, Sr, Ca, Ba; A = Sb, As): An Emerging Class of Materials for Photovoltaics","abstract":"Antiperovskite nitrides are potential candidates for applications harvesting solar light. With a comprehensive state-of-the-art approach combining hybrid density-functional theory, many-body perturbation theory, the Wannier-Mott model, density-functional perturbation theory, and the Feynman polaron model, we explore excitonic and polaronic effects in X$_3$NA (X: Mg, Ca, Sr, Ba, A = Sb, As). For all of them, we uncover a significant influence of the ionic dielectric screening on the static dielectric constant. Small exciton binding energies, weak electron-phonon coupling, and high charge-carrier mobilities facilitate enhanced charge transport in Mg$_3$NSb, Sr$_3$NSb, and Ba$_3$NSb. Our results highlight the potential of these nitrides as optimal candidates for efficient photovoltaic absorbers.","sentences":["Antiperovskite nitrides are potential candidates for applications harvesting solar light.","With a comprehensive state-of-the-art approach combining hybrid density-functional theory, many-body perturbation theory, the Wannier-Mott model, density-functional perturbation theory, and the Feynman polaron model, we explore excitonic and polaronic effects in X$_3$NA (X: Mg, Ca, Sr, Ba, A = Sb, As).","For all of them, we uncover a significant influence of the ionic dielectric screening on the static dielectric constant.","Small exciton binding energies, weak electron-phonon coupling, and high charge-carrier mobilities facilitate enhanced charge transport in Mg$_3$NSb, Sr$_3$NSb, and Ba$_3$NSb.","Our results highlight the potential of these nitrides as optimal candidates for efficient photovoltaic absorbers."],"url":"http://arxiv.org/abs/2404.16494v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 10:18:01","title":"Concentration estimates for SPDEs driven by fractional Brownian motion","abstract":"The main goal of this work is to provide sample-path estimates for the solution of slowly time-dependent SPDEs perturbed by a cylindrical fractional Brownian motion. Our strategy is similar to the approach by Berglund and Nader for space-time white noise. However, the setting of fractional Brownian motion does not allow us to use any martingale methods. Using instead optimal estimates for the probability that the supremum of a Gaussian process exceeds a certain level, we derive concentration estimates for the solution of the SPDE, provided that the Hurst index $H$ of the fractional Brownian motion satisfies $H>\\frac14$. As a by-product, we also obtain concentration estimates for one-dimensional fractional SDEs valid for any $H\\in(0,1)$.","sentences":["The main goal of this work is to provide sample-path estimates for the solution of slowly time-dependent SPDEs perturbed by a cylindrical fractional Brownian motion.","Our strategy is similar to the approach by Berglund and Nader for space-time white noise.","However, the setting of fractional Brownian motion does not allow us to use any martingale methods.","Using instead optimal estimates for the probability that the supremum of a Gaussian process exceeds a certain level, we derive concentration estimates for the solution of the SPDE, provided that the Hurst index $H$ of the fractional Brownian motion satisfies $H>\\frac14$. As a by-product, we also obtain concentration estimates for one-dimensional fractional SDEs valid for any $H\\in(0,1)$."],"url":"http://arxiv.org/abs/2404.16485v1","category":"math.PR"}
{"created":"2024-04-25 17:57:09","title":"Quantum effects on the evaporation of PBHs: contributions to dark matter","abstract":"We compute the relic abundance of dark matter in the presence of Primordial Black Holes (PBHs) beyond the semiclassical approximation. We take into account the quantum corrections due to the memory burden effect, which is assumed to suppress the black hole evaporation rate by the inverse power of its own entropy. Such quantum effect significantly enhances the lifetime, rendering the possibility of PBH mass $\\lesssim 10^{9}$ g being the sole dark matter (DM) candidate. However, Nature can not rule out the existence of fundamental particles such as DM. We, therefore, include the possibility of populating the dark sector by the decay of PBHs to those fundamental particles, adding the contribution to stable PBH whose lifetime is extended due to the quantum corrections. Depending on the strength of the burden effect, we show that a wide range of parameter space opens up in the initial PBH mass and fundamental dark matter mass plane that respects the correct relic abundance.","sentences":["We compute the relic abundance of dark matter in the presence of Primordial Black Holes (PBHs) beyond the semiclassical approximation.","We take into account the quantum corrections due to the memory burden effect, which is assumed to suppress the black hole evaporation rate by the inverse power of its own entropy.","Such quantum effect significantly enhances the lifetime, rendering the possibility of PBH mass $\\lesssim 10^{9}$ g being the sole dark matter (DM) candidate.","However, Nature can not rule out the existence of fundamental particles such as DM.","We, therefore, include the possibility of populating the dark sector by the decay of PBHs to those fundamental particles, adding the contribution to stable PBH whose lifetime is extended due to the quantum corrections.","Depending on the strength of the burden effect, we show that a wide range of parameter space opens up in the initial PBH mass and fundamental dark matter mass plane that respects the correct relic abundance."],"url":"http://arxiv.org/abs/2404.16815v1","category":"hep-ph"}
{"created":"2024-04-25 17:33:43","title":"A note on the order of the Tate--Shafarevich group modulo squares","abstract":"We investigate the order of the Tate--Shafarevich group of abelian varieties modulo rational squares. Our main result shows that every square-free natural number appears as the non square-free part of the Tate--Shafarevich group of some abelian variety, thereby validating a conjecture of W. Stein.","sentences":["We investigate the order of the Tate--Shafarevich group of abelian varieties modulo rational squares.","Our main result shows that every square-free natural number appears as the non square-free part of the Tate--Shafarevich group of some abelian variety, thereby validating a conjecture of W. Stein."],"url":"http://arxiv.org/abs/2404.16785v1","category":"math.NT"}
{"created":"2024-04-25 17:27:02","title":"Subset SSD for enhanced indexation with sector constraints","abstract":"In this paper we apply second order stochastic dominance (SSD) to the problem of enhanced indexation with asset subset (sector) constraints. The problem we consider is how to construct a portfolio that is designed to outperform a given market index whilst having regard to the proportion of the portfolio invested in distinct market sectors. In our approach, subset SSD, the portfolio associated with each sector is treated in a SSD manner. In other words in subset SSD we actively try to find sector portfolios that SSD dominate their respective sector indices. However the proportion of the overall portfolio invested in each sector is not pre-specified, rather it is decided via optimisation. Computational results are given for our approach as applied to the S\\&P~500 over the period $29^{\\text{th}}$ August 2018 to $29^{\\text{th}}$ December 2023. This period, over 5 years, includes the Covid pandemic, which had a significant effect on stock prices. Our results indicate that the scaled version of our subset SSD approach significantly outperforms the S\\&P~500 over the period considered. Our approach also outperforms the standard SSD based approach to the problem.","sentences":["In this paper we apply second order stochastic dominance (SSD) to the problem of enhanced indexation with asset subset (sector) constraints.","The problem we consider is how to construct a portfolio that is designed to outperform a given market index whilst having regard to the proportion of the portfolio invested in distinct market sectors.","In our approach, subset SSD, the portfolio associated with each sector is treated in a SSD manner.","In other words in subset SSD we actively try to find sector portfolios that SSD dominate their respective sector indices.","However the proportion of the overall portfolio invested in each sector is not pre-specified, rather it is decided via optimisation.","Computational results are given for our approach as applied to the S\\&P~500 over the period $29^{\\text{th}}$ August 2018 to $29^{\\text{th}}$ December 2023.","This period, over 5 years, includes the Covid pandemic, which had a significant effect on stock prices.","Our results indicate that the scaled version of our subset SSD approach significantly outperforms the S\\&P~500 over the period considered.","Our approach also outperforms the standard SSD based approach to the problem."],"url":"http://arxiv.org/abs/2404.16777v1","category":"q-fin.CP"}
{"created":"2024-04-25 17:26:59","title":"Modeling Selective Feature Attention for Representation-based Siamese Text Matching","abstract":"Representation-based Siamese networks have risen to popularity in lightweight text matching due to their low deployment and inference costs. While word-level attention mechanisms have been implemented within Siamese networks to improve performance, we propose Feature Attention (FA), a novel downstream block designed to enrich the modeling of dependencies among embedding features. Employing \"squeeze-and-excitation\" techniques, the FA block dynamically adjusts the emphasis on individual features, enabling the network to concentrate more on features that significantly contribute to the final classification. Building upon FA, we introduce a dynamic \"selection\" mechanism called Selective Feature Attention (SFA), which leverages a stacked BiGRU Inception structure. The SFA block facilitates multi-scale semantic extraction by traversing different stacked BiGRU layers, encouraging the network to selectively concentrate on semantic information and embedding features across varying levels of abstraction. Both the FA and SFA blocks offer a seamless integration capability with various Siamese networks, showcasing a plug-and-play characteristic. Experimental evaluations conducted across diverse text matching baselines and benchmarks underscore the indispensability of modeling feature attention and the superiority of the \"selection\" mechanism.","sentences":["Representation-based Siamese networks have risen to popularity in lightweight text matching due to their low deployment and inference costs.","While word-level attention mechanisms have been implemented within Siamese networks to improve performance, we propose Feature Attention (FA), a novel downstream block designed to enrich the modeling of dependencies among embedding features.","Employing \"squeeze-and-excitation\" techniques, the FA block dynamically adjusts the emphasis on individual features, enabling the network to concentrate more on features that significantly contribute to the final classification.","Building upon FA, we introduce a dynamic \"selection\" mechanism called Selective Feature Attention (SFA), which leverages a stacked BiGRU Inception structure.","The SFA block facilitates multi-scale semantic extraction by traversing different stacked BiGRU layers, encouraging the network to selectively concentrate on semantic information and embedding features across varying levels of abstraction.","Both the FA and SFA blocks offer a seamless integration capability with various Siamese networks, showcasing a plug-and-play characteristic.","Experimental evaluations conducted across diverse text matching baselines and benchmarks underscore the indispensability of modeling feature attention and the superiority of the \"selection\" mechanism."],"url":"http://arxiv.org/abs/2404.16776v1","category":"cs.CL"}
{"created":"2024-04-25 17:24:12","title":"$R_{D^{(*)}}$ and survival of the fittest scalar leptoquark","abstract":"Motivated by the long-standing discrepancy in lepton flavor universality ratios $R_D$ and $R_{D^{\\ast}}$ we assess the status of scalar leptoquark states $R_2$, $\\widetilde R_2$ and $S_1$ which can in principle provide a desired enhancement of $\\mathcal{B}(B\\to D^{(\\ast )}\\tau \\nu)$ in a minimal setup with two Yukawa couplings only. We consider unavoidable low-energy constraints, $Z$-pole measurements as well as high-$p_T$ constraints. After setting mass of each leptoquark to $1.5$ TeV we find that of all considered states only $S_1$ leptoquark, coupled to both chiralities of leptons and quarks, is still a completely viable solution while the scenario with $R_2$ is in growing tension with $\\Gamma(Z \\to \\tau \\tau)$ and with the LHC constraints on the di-tau tails at high-$p_T$. We comment on the future experimental tests of $S_1$ scenario.","sentences":["Motivated by the long-standing discrepancy in lepton flavor universality ratios $R_D$ and $R_{D^{\\ast}}$ we assess the status of scalar leptoquark states $R_2$, $\\widetilde R_2$ and $S_1$ which can in principle provide a desired enhancement of $\\mathcal{B}(B\\to D^{(\\ast )}\\tau \\nu)$ in a minimal setup with two Yukawa couplings only.","We consider unavoidable low-energy constraints, $Z$-pole measurements as well as high-$p_T$ constraints.","After setting mass of each leptoquark to $1.5$ TeV we find that of all considered states only $S_1$ leptoquark, coupled to both chiralities of leptons and quarks, is still a completely viable solution while the scenario with $R_2$ is in growing tension with $\\Gamma(Z \\to \\tau \\tau)$ and with the LHC constraints on the di-tau tails at high-$p_T$. We comment on the future experimental tests of $S_1$ scenario."],"url":"http://arxiv.org/abs/2404.16772v1","category":"hep-ph"}
{"created":"2024-04-25 17:16:41","title":"Dichalcogenides and difulfides nanostructures for hydrogen storage","abstract":"Hydrogen energy is a high-efficiency and clean energy. Large-surface-area, two-dimensional (2D) layered materials are expected to have an advantage in hydrogen storage applications. Among a large number of 2D materials, monolayer dichacogenides have emerged as promising candidate for hydrogen clean energy. In the present work, first-principles calculations and molecular dynamics simulations are carried out to investigate the adsorption behaviors of hydrogen molecules on transition metal dichalcogenides and disulphides. Furthermore we propose novel structures which may suitable for hydrogen storage. As the concentration increaes, 2D shows thermodynamic stability even at room temperature, suggesting a possible aplicability for hydrogen storage.","sentences":["Hydrogen energy is a high-efficiency and clean energy.","Large-surface-area, two-dimensional (2D) layered materials are expected to have an advantage in hydrogen storage applications.","Among a large number of 2D materials, monolayer dichacogenides have emerged as promising candidate for hydrogen clean energy.","In the present work, first-principles calculations and molecular dynamics simulations are carried out to investigate the adsorption behaviors of hydrogen molecules on transition metal dichalcogenides and disulphides.","Furthermore we propose novel structures which may suitable for hydrogen storage.","As the concentration increaes, 2D shows thermodynamic stability even at room temperature, suggesting a possible aplicability for hydrogen storage."],"url":"http://arxiv.org/abs/2404.16761v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 17:11:52","title":"Links and the Diaconis-Graham Inequality","abstract":"In 1977 Diaconis and Graham proved two inequalities relating different measures of disarray in permutations, and asked for a characterization of those permutations for which equality holds in one of these inequalities. Such a characterization was first given in 2013. Recently, another characterization was given by Woo, using a topological link in $\\mathbb R^3$ that can be associated to the cycle diagram of a permutation. We show that Woo's characterization extends much further: for any permutation, the discrepancy in Diaconis and Graham's inequality is directly related to the Euler characteristic of the associated link. This connection provides a new proof of the original result of Diaconis and Graham. We also characterize permutations with a fixed discrepancy in terms of their associated links and find that the stabilized-interval-free permutations are precisely those whose associated links are nonsplit.","sentences":["In 1977 Diaconis and Graham proved two inequalities relating different measures of disarray in permutations, and asked for a characterization of those permutations for which equality holds in one of these inequalities.","Such a characterization was first given in 2013.","Recently, another characterization was given by Woo, using a topological link in $\\mathbb R^3$ that can be associated to the cycle diagram of a permutation.","We show that Woo's characterization extends much further: for any permutation, the discrepancy in Diaconis and Graham's inequality is directly related to the Euler characteristic of the associated link.","This connection provides a new proof of the original result of Diaconis and Graham.","We also characterize permutations with a fixed discrepancy in terms of their associated links and find that the stabilized-interval-free permutations are precisely those whose associated links are nonsplit."],"url":"http://arxiv.org/abs/2404.16755v1","category":"math.CO"}
{"created":"2024-04-25 16:50:44","title":"Calculable neutrino Dirac mass matrix and one-loop $\\bar \u03b8$ in the minimal left-right symmetric model","abstract":"We revisit the contribution to the strong CP parameter $\\bar \\theta$ from leptonic CP violation at one-loop level in the minimal left-right symmetric model in the case of parity as the left-right symmetry. The Hermitian neutrino Dirac mass matrix $M_D$ can be calculated using the light and heavy neutrino masses and mixings. We propose a parameterization of the right-handed neutrino mixing matrix $V_R$ and construct the heavy neutrino mass that maintains the Hermiticity of $M_D$. We further apply it to evaluate the one-loop $\\bar\\theta$, denoted as $\\bar \\theta_{loop}$, as a function of the sterile neutrino masses for explicit examples of $V_R$. By requiring the magnitude of $\\bar \\theta_{loop}\\lesssim 10^{-10}$, we derive the upper limits on the sterile neutrino masses, which are within reach of direct searches at the Large Hadron Collider and neutrinoless double beta decay experiments. Furthermore, our parameterization is applicable to other phenomenological studies.","sentences":["We revisit the contribution to the strong CP parameter $\\bar \\theta$ from leptonic CP violation at one-loop level in the minimal left-right symmetric model in the case of parity as the left-right symmetry.","The Hermitian neutrino Dirac mass matrix $M_D$ can be calculated using the light and heavy neutrino masses and mixings.","We propose a parameterization of the right-handed neutrino mixing matrix $V_R$ and construct the heavy neutrino mass that maintains the Hermiticity of $M_D$. We further apply it to evaluate the one-loop $\\bar\\theta$, denoted as $\\bar \\theta_{loop}$, as a function of the sterile neutrino masses for explicit examples of $V_R$. By requiring the magnitude of $\\bar \\theta_{loop}\\lesssim 10^{-10}$, we derive the upper limits on the sterile neutrino masses, which are within reach of direct searches at the Large Hadron Collider and neutrinoless double beta decay experiments.","Furthermore, our parameterization is applicable to other phenomenological studies."],"url":"http://arxiv.org/abs/2404.16740v1","category":"hep-ph"}
{"created":"2024-04-25 16:47:34","title":"Open Source Software (OSS) Transparency for DoD Acquisition","abstract":"Caveat emptor, or let the buyer beware, is commonly attributed to open source software (OSS)-the onus is on the OSS consumer to ensure that it is fit for use in the consumer's context. OSS has been compared to an open market bazaar where consumers are free to browse all the source code and take a copy. In this paper, we observe challenges for the OSS consumer to obtain information about the process(es), project(s) used to produce a product and the protection(s) employed by those projects. We discuss the need for more transparency by OSS projects, where possible and introduce a framework for reasoning about those OSS projects and their products for use by the OSS consumer.","sentences":["Caveat emptor, or let the buyer beware, is commonly attributed to open source software (OSS)-the onus is on the OSS consumer to ensure that it is fit for use in the consumer's context.","OSS has been compared to an open market bazaar where consumers are free to browse all the source code and take a copy.","In this paper, we observe challenges for the OSS consumer to obtain information about the process(es), project(s) used to produce a product and the protection(s) employed by those projects.","We discuss the need for more transparency by OSS projects, where possible and introduce a framework for reasoning about those OSS projects and their products for use by the OSS consumer."],"url":"http://arxiv.org/abs/2404.16737v1","category":"cs.SE"}
{"created":"2024-04-25 16:24:17","title":"Log-normal glide and the formation of misfit dislocation networks in heteroepitaxial ZnS on GaP","abstract":"Scanning electron microscopy (SEM) based electron channeling contrast imaging (ECCI) is used to observe and quantify misfit dislocation (MD) networks formed at the heteroepitaxial interface between ZnS and GaP grown by molecular beam epitaxy (MBE). Below a critical thickness of 15-20 nm, no MDs are observed. However, crystallographic features with strong dipole contrast, consistent with unexpanded dislocation half-loops, are observed prior to the formation of visible interfacial MD segments and any notable strain relaxation. At higher film thicknesses (20 to 50 nm), interfacial MD lengths increase anisotropically in the two orthogonal in-plane <110> line directions, threading dislocation (TD) density increases, and a roughening transition is observed from atomically smooth two-dimensional (2D) to a multi-stepped three-dimensional (3D) morphology, providing evidence for step edge pinning via surface terminating dislocations. The ZnS strain relaxation, calculated from the total MD content observed via ECCI, matches the average strain relaxation measured by high-resolution x-ray diffraction (HRXRD). The MD lengths are found to follow a log-normal distribution, indicating that the combined MD nucleation and TD glide processes must have a normal distribution of activation energies. The estimated TD glide velocity ($v_{g}$) along [$\\bar{1}$10] is almost twice that along [110], but in both directions shows a maximum as a function of film thickness, indicating an initial burst of plasticity followed by dislocation pinning.","sentences":["Scanning electron microscopy (SEM) based electron channeling contrast imaging (ECCI) is used to observe and quantify misfit dislocation (MD) networks formed at the heteroepitaxial interface between ZnS and GaP grown by molecular beam epitaxy (MBE).","Below a critical thickness of 15-20 nm, no MDs are observed.","However, crystallographic features with strong dipole contrast, consistent with unexpanded dislocation half-loops, are observed prior to the formation of visible interfacial MD segments and any notable strain relaxation.","At higher film thicknesses (20 to 50 nm), interfacial MD lengths increase anisotropically in the two orthogonal in-plane <110> line directions, threading dislocation (TD) density increases, and a roughening transition is observed from atomically smooth two-dimensional (2D) to a multi-stepped three-dimensional (3D) morphology, providing evidence for step edge pinning via surface terminating dislocations.","The ZnS strain relaxation, calculated from the total MD content observed via ECCI, matches the average strain relaxation measured by high-resolution x-ray diffraction (HRXRD).","The MD lengths are found to follow a log-normal distribution, indicating that the combined MD nucleation and TD glide processes must have a normal distribution of activation energies.","The estimated TD glide velocity ($v_{g}$) along [$\\bar{1}$10] is almost twice that along [110], but in both directions shows a maximum as a function of film thickness, indicating an initial burst of plasticity followed by dislocation pinning."],"url":"http://arxiv.org/abs/2404.16714v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 15:51:29","title":"Deep Hard X-ray Survey of the M81 Field Based on INTEGRAL Data","abstract":"We have carried out a deep survey of the M81 field in the 25-60 keV energy band based on long-term (2003-2023) INTEGRAL observations. A record sensitivity of 0.16 mCrab at a detection significance of 4 sigma has been achieved in the central part of the field owing to the long accumulated exposure (19.2 Ms). The total area of the survey is 1004 deg^2 at a sensitivity level better than 0.72 mCrab. We have produced a catalog of sources detected at a significance level higher than 4 sigma. It contains 51 objects most of which are active galactic nuclei (AGNs). The median redshift of the Seyfert galaxies in the catalog is z=0.0366. Six sources have not been detected previously in any of the X-ray surveys. According to the available indirect data, all of them and two more sources that have already been entered previously into the INTEGRAL survey catalogs can also be AGNs, including those with strong internal absorption.","sentences":["We have carried out a deep survey of the M81 field in the 25-60 keV energy band based on long-term (2003-2023) INTEGRAL observations.","A record sensitivity of 0.16 mCrab at a detection significance of 4 sigma has been achieved in the central part of the field owing to the long accumulated exposure (19.2 Ms).","The total area of the survey is 1004 deg^2 at a sensitivity level better than 0.72 mCrab.","We have produced a catalog of sources detected at a significance level higher than 4 sigma.","It contains 51 objects most of which are active galactic nuclei (AGNs).","The median redshift of the Seyfert galaxies in the catalog is z=0.0366.","Six sources have not been detected previously in any of the X-ray surveys.","According to the available indirect data, all of them and two more sources that have already been entered previously into the INTEGRAL survey catalogs can also be AGNs, including those with strong internal absorption."],"url":"http://arxiv.org/abs/2404.16691v1","category":"astro-ph.HE"}
{"created":"2024-04-25 15:33:39","title":"Calibrating non-parametric morphological indicators from {\\it JWST} images for galaxies over $0.5<z<3$","abstract":"The measurements of morphological indicators of galaxies are often influenced by a series of observational effects. In this study, we utilize a sample of over 800 TNG50 simulated galaxies with log($M_*$/M$_\\odot$)$>9$ at $0.5<z<3$ to investigate the differences in non-parametric morphological indicators ($C$, $S$, $Gini$, $M_{\\rm 20}$, $A_{\\rm O}$, and $D_{\\rm O}$) derived from noise-free and high-resolution TNG50 images and mock images simulated to have the same observational conditions as {\\it JWST}/NIRCam. We quantify the relationship between intrinsic and observed values of the morphological indicators and accordingly apply this calibration to over 4600 galaxies in the same stellar mass and redshift ranges observed in {\\it JWST} CEERS and JADES surveys. We find a significant evolution of morphological indicators with rest-frame wavelength ($\\lambda_{\\rm rf}$) at $\\lambda_{\\rm rf}<1$\\,$\\mu$m, while essentially no obvious variations occur at $\\lambda_{\\rm rf}>1$\\,$\\mu$m. The morphological indicators of star-forming galaxies (SFGs) and quiescent galaxies (QGs) are significantly different. The morphologies of QGs exhibit a higher sensitivity to rest-frame wavelength than SFGs. After analyzing the evolution of morphological indicators in the rest-frame V-band (0.5-0.7\\,$\\mu$m) and rest-frame J-band (1.1-1.4\\,$\\mu$m), we find that the morphologies of QGs evolve substantially with both redshift and stellar mass. For SFGs, the $C$, $Gini$ and $M_{\\rm 20}$ show a rapid evolution with stellar mass at log($M_*$/M$_\\odot$)$\\geq10.5$, while the $A_{\\rm O}$, $D_{\\rm O}$ and $A$ evolve with both redshift and stellar mass. Our comparison shows that TNG50 simulations effectively reproduce the morphological indicators we measured from {\\it JWST} observations when the impact of dust attenuation is considered.","sentences":["The measurements of morphological indicators of galaxies are often influenced by a series of observational effects.","In this study, we utilize a sample of over 800 TNG50 simulated galaxies with log($M_*$/M$_\\odot$)$>9$ at $0.5<z<3$ to investigate the differences in non-parametric morphological indicators ($C$, $S$, $Gini$, $M_{\\rm 20}$, $A_{\\rm O}$, and $D_{\\rm O}$) derived from noise-free and high-resolution TNG50 images and mock images simulated to have the same observational conditions as {\\it JWST}/NIRCam.","We quantify the relationship between intrinsic and observed values of the morphological indicators and accordingly apply this calibration to over 4600 galaxies in the same stellar mass and redshift ranges observed in {\\it JWST} CEERS and JADES surveys.","We find a significant evolution of morphological indicators with rest-frame wavelength ($\\lambda_{\\rm rf}$) at $\\lambda_{\\rm rf}<1$\\,$\\mu$m, while essentially no obvious variations occur at $\\lambda_{\\rm rf}>1$\\,$\\mu$m.","The morphological indicators of star-forming galaxies (SFGs) and quiescent galaxies (QGs) are significantly different.","The morphologies of QGs exhibit a higher sensitivity to rest-frame wavelength than SFGs.","After analyzing the evolution of morphological indicators in the rest-frame V-band (0.5-0.7\\,$\\mu$m) and rest-frame J-band (1.1-1.4\\,$\\mu$m), we find that the morphologies of QGs evolve substantially with both redshift and stellar mass.","For SFGs, the $C$, $Gini$ and $M_{\\rm 20}$ show a rapid evolution with stellar mass at log($M_*$/M$_\\odot$)$\\geq10.5$, while the $A_{\\rm O}$, $D_{\\rm O}$ and $A$ evolve with both redshift and stellar mass.","Our comparison shows that TNG50 simulations effectively reproduce the morphological indicators we measured from {\\it JWST} observations when the impact of dust attenuation is considered."],"url":"http://arxiv.org/abs/2404.16686v1","category":"astro-ph.GA"}
{"created":"2024-04-25 15:11:31","title":"Simulation of depth-dose curves and water equivalent ratios of energetic proton beams in cortical bone","abstract":"We have determined the depth-dose curve, the penetration range, and the water equivalent ratio (WER), for proton beams of clinical energies in cortical bone, by means of a detailed and accurate simulation that combines molecular dynamics and Monte Carlo techniques. The fundamental input quantities (stopping power and energy loss straggling) for the simulation were obtained from a reliable electronic excitation spectrum of the condensed-phase target, which takes into account the organic and mineral phases that form it. Our simulations with these inputs, that are in excellent agreement with the scarce data available for a cortical bone target, deviate from simulations performed using other stopping quantities, such as those provided in the widely used ICRU Report 49. The results of this work emphasize the importance of an accurate determination of the stopping quantities of cortical bone in order to advance towards the millimetric precision for the proton penetration ranges and deposited dose needed in radiotherapy.","sentences":["We have determined the depth-dose curve, the penetration range, and the water equivalent ratio (WER), for proton beams of clinical energies in cortical bone, by means of a detailed and accurate simulation that combines molecular dynamics and Monte Carlo techniques.","The fundamental input quantities (stopping power and energy loss straggling) for the simulation were obtained from a reliable electronic excitation spectrum of the condensed-phase target, which takes into account the organic and mineral phases that form it.","Our simulations with these inputs, that are in excellent agreement with the scarce data available for a cortical bone target, deviate from simulations performed using other stopping quantities, such as those provided in the widely used ICRU Report 49.","The results of this work emphasize the importance of an accurate determination of the stopping quantities of cortical bone in order to advance towards the millimetric precision for the proton penetration ranges and deposited dose needed in radiotherapy."],"url":"http://arxiv.org/abs/2404.16667v1","category":"physics.med-ph"}
{"created":"2024-04-25 14:58:48","title":"Time-domain analysis of multi-waveband flares from AD Leonis","abstract":"Radio bursts of magnetically active stars reveal the intensity and activity of the stellar magnetic field. They may also be related to the planets around the stars. We monitored a radio-active star, AD Leonis, 3000 seconds per day for 17 days in November 2020, and 5000 seconds per day for 5 days in July 2023 with the Five-hundred-meter Aperture Spherical radio Telescope (FAST). Based on the simultaneous flux increases in Stokes I and Stokes V, one left-hand circular polarized radio burst is identified. The $\\sim50\\%$ degree of circular polarization indicates the burst being originated from non-thermal radiation related to the stellar magnetic field. Combining the newly discovered burst with previous observations of radio and X-ray bursts from AD Leonis, we did a periodicity analysis for the 49 bursts in total. No periodicity with confidence level $>3\\sigma$ is found, while a candidate period of 3.04 days at $\\approx 2\\sigma$ confidence level is presented and discussed. Results of recent FAST observations and the periodicity analysis suggest a more compact campaign of observation toward this source, from which a more optimistic result of period search could be achieved.","sentences":["Radio bursts of magnetically active stars reveal the intensity and activity of the stellar magnetic field.","They may also be related to the planets around the stars.","We monitored a radio-active star, AD Leonis, 3000 seconds per day for 17 days in November 2020, and 5000 seconds per day for 5 days in July 2023 with the Five-hundred-meter Aperture Spherical radio Telescope (FAST).","Based on the simultaneous flux increases in Stokes I and Stokes V, one left-hand circular polarized radio burst is identified.","The $\\sim50\\%$ degree of circular polarization indicates the burst being originated from non-thermal radiation related to the stellar magnetic field.","Combining the newly discovered burst with previous observations of radio and X-ray bursts from AD Leonis, we did a periodicity analysis for the 49 bursts in total.","No periodicity with confidence level $>3\\sigma$ is found, while a candidate period of 3.04 days at $\\approx 2\\sigma$ confidence level is presented and discussed.","Results of recent FAST observations and the periodicity analysis suggest a more compact campaign of observation toward this source, from which a more optimistic result of period search could be achieved."],"url":"http://arxiv.org/abs/2404.16661v1","category":"astro-ph.SR"}
{"created":"2024-04-25 14:47:46","title":"Rational Designing of Anthocyanidins-Directed Near-Infrared Two-Photon Fluorescence Probes","abstract":"Recently, two-photon fluorescent probes based on anthocyanidins molecules have attracted extensive attention due to their outstanding photophysical properties. However, there are only a few two-photon excited fluorescent probes that really meet the requirements of relatively long emission wavelengths (>600 nm), large two-photon absorption (TPA) cross sections (300 GM), significant Stokes shift (>80 nm), and high fluorescence intensity. Herein, the photophysical properties of a series of anthocyanidins with the same substituents but different fluorophore skeletons were investigated in detail. Compared with b-series molecules, a-series molecules with a six-membered ring in the backbone have a slightly higher reorganization energy. This results in more energy loss upon light excitation, enabling the reaction products to detect NTR through a larger Stokes shift. More importantly, there is very little decrease in fluorescence intensity as the Stokes shift increases. These features are extremely valuable for high-resolution NTR detection. In light of this, novel 2a-n (n=1-5) compounds are designed, which are accomplished by inhibiting the twisted intramolecular charge transfer (TICT) effect through alkyl cyclization, azetidine ring and extending {\\pi} conjugation. Among them, 2a-3 gains long emission spectrum ({\\lambda}em=691.42 nm), noticeable TPA cross section (957.36 GM), and large Stokes shift (110.88 nm), indicating that it serves as a promising candidate for two-photon fluorescent dyes. It is hoped that this work will offer some insightful theoretical direction for the development of novel high performance anthocyanin fluorescent materials.","sentences":["Recently, two-photon fluorescent probes based on anthocyanidins molecules have attracted extensive attention due to their outstanding photophysical properties.","However, there are only a few two-photon excited fluorescent probes that really meet the requirements of relatively long emission wavelengths (>600 nm), large two-photon absorption (TPA) cross sections (300 GM), significant Stokes shift (>80 nm), and high fluorescence intensity.","Herein, the photophysical properties of a series of anthocyanidins with the same substituents but different fluorophore skeletons were investigated in detail.","Compared with b-series molecules, a-series molecules with a six-membered ring in the backbone have a slightly higher reorganization energy.","This results in more energy loss upon light excitation, enabling the reaction products to detect NTR through a larger Stokes shift.","More importantly, there is very little decrease in fluorescence intensity as the Stokes shift increases.","These features are extremely valuable for high-resolution NTR detection.","In light of this, novel 2a-n (n=1-5) compounds are designed, which are accomplished by inhibiting the twisted intramolecular charge transfer (TICT) effect through alkyl cyclization, azetidine ring and extending {\\pi} conjugation.","Among them, 2a-3 gains long emission spectrum ({\\lambda}em=691.42 nm), noticeable TPA cross section (957.36 GM), and large Stokes shift (110.88 nm), indicating that it serves as a promising candidate for two-photon fluorescent dyes.","It is hoped that this work will offer some insightful theoretical direction for the development of novel high performance anthocyanin fluorescent materials."],"url":"http://arxiv.org/abs/2404.16655v1","category":"physics.chem-ph"}
{"created":"2024-04-25 14:31:02","title":"Gaussian free field and Liouville quantum gravity","abstract":"Over fourty years ago, the physicist Polyakov proposed a bold framework for string theory, in which the problem was reduced to the study of certain \"random surfaces\". He further made the tantalising suggestion that this theory could be explicitly solved. Recent breakthroughs from the last fifteen years have not only given a concrete mathematical basis for this theory but also verified some of its most striking predictions, as well as Polyakov's original vision. This theory, now known in the mathematics literature either as Liouville quantum gravity or Liouville conformal field theory, is based on a remarkable combination of ideas coming from different fields, above all probability and geometry. This book is intended to be an introduction to these developments assuming as few prerequisites as possible.","sentences":["Over fourty years ago, the physicist Polyakov proposed a bold framework for string theory, in which the problem was reduced to the study of certain \"random surfaces\".","He further made the tantalising suggestion that this theory could be explicitly solved.","Recent breakthroughs from the last fifteen years have not only given a concrete mathematical basis for this theory but also verified some of its most striking predictions, as well as Polyakov's original vision.","This theory, now known in the mathematics literature either as Liouville quantum gravity or Liouville conformal field theory, is based on a remarkable combination of ideas coming from different fields, above all probability and geometry.","This book is intended to be an introduction to these developments assuming as few prerequisites as possible."],"url":"http://arxiv.org/abs/2404.16642v1","category":"math.PR"}
{"created":"2024-04-25 14:30:38","title":"Extended high-ionization [MgIV] emission tracing widespread shocks in starbursts seen by JWST /NIRSpec","abstract":"We report the detection of extended (>0.5-1kpc) high-ionization [MgIV] 4.487 $\\mu$m (80 eV) emission in four local luminous infrared galaxies observed with JWST/NIRSpec. Excluding the nucleus and outflow of the Type 1 active galactic nucleus (AGN) in the sample, we find that the [MgIV] luminosity is well correlated with that of H recombination lines, which mainly trace star forming clumps in these objects, and that the [ArVI] 4.530 $\\mu$m (75 eV), usually seen in AGN, is undetected. On 100-400pc scales, the [MgIV] line profiles are broader (sigma([MgIV])=90 +- 25 km/s) and shifted (Delta_v up to +- 50 km/s) compared to those of the H recombination lines and lower ionization transitions (e.g., sigma(Hu-12)=57 +- 15 km/s). The [MgIV] kinematics follow the large scale rotating velocity field of these galaxies and the broad [MgIV] profiles are compatible with the broad wings detected in the H recombination lines. Based on these observational results, extended highly ionized gas more turbulent than the ambient interstellar medium, possibly as a result of ionizing shocks associated with star-formation, is the most likely origin of the [MgIV] emission. We also computed new grids of photoionization and shock models to investigate where the [MgIV] line originates. Shocks with velocities of 100-130 km/s reproduce the observed line ratios and the [MgIV] luminosity agrees with that expected from the mechanical energy released by supernove (SNe) in these regions. Therefore, these models support shocks induced by SNe as the origin of the [MgIV] line. Future studies on the stellar feedback from SNe will benefit from the [MgIV] line that is little affected by obscuration and, in absence of an AGN, can only be produced by shocks due to its high ionization potential.","sentences":["We report the detection of extended (>0.5-1kpc) high-ionization [MgIV] 4.487 $\\mu$m (80 eV) emission in four local luminous infrared galaxies observed with JWST/NIRSpec.","Excluding the nucleus and outflow of the Type 1 active galactic nucleus (AGN) in the sample, we find that the [MgIV] luminosity is well correlated with that of H recombination lines, which mainly trace star forming clumps in these objects, and that the [ArVI] 4.530 $\\mu$m (75 eV), usually seen in AGN, is undetected.","On 100-400pc scales, the [MgIV] line profiles are broader (sigma([MgIV])=90 +- 25 km/s) and shifted (Delta_v up to +- 50 km/s) compared to those of the H recombination lines and lower ionization transitions (e.g., sigma(Hu-12)=57 +- 15 km/s).","The [MgIV] kinematics follow the large scale rotating velocity field of these galaxies and the broad [MgIV] profiles are compatible with the broad wings detected in the H recombination lines.","Based on these observational results, extended highly ionized gas more turbulent than the ambient interstellar medium, possibly as a result of ionizing shocks associated with star-formation, is the most likely origin of the [MgIV] emission.","We also computed new grids of photoionization and shock models to investigate where the [MgIV] line originates.","Shocks with velocities of 100-130 km/s reproduce the observed line ratios and the [MgIV] luminosity agrees with that expected from the mechanical energy released by supernove (SNe) in these regions.","Therefore, these models support shocks induced by SNe as the origin of the [MgIV] line.","Future studies on the stellar feedback from SNe will benefit from the [MgIV] line that is little affected by obscuration and, in absence of an AGN, can only be produced by shocks due to its high ionization potential."],"url":"http://arxiv.org/abs/2404.16641v1","category":"astro-ph.GA"}
{"created":"2024-04-25 13:41:06","title":"Freezing density scaling of transport coefficients in the Weeks-Chandler-Andersen fluid","abstract":"It is shown that the transport coefficients (self-diffusion, shear viscosity, and thermal conductivity) of the Weeks-Chandler-Anderson (WCA) fluid along isotherms exhibit a freezing density scaling (FDS). The functional form of this FDS is essentially the same or closely related to those in the Lennard-Jones fluid, hard-sphere fluid, and some liquefied noble gases. This proves that this FDS represents a quasi-universal corresponding state principle for simple classical fluids with steep interactions. Some related aspects such as Stokes-Einstein relation without a hydrodynamic diameter and gas-to-liquid dynamical crossover are briefly discussed. Simple fitting formula for the transport coefficients of the dense WCA fluid are suggested.","sentences":["It is shown that the transport coefficients (self-diffusion, shear viscosity, and thermal conductivity) of the Weeks-Chandler-Anderson (WCA) fluid along isotherms exhibit a freezing density scaling (FDS).","The functional form of this FDS is essentially the same or closely related to those in the Lennard-Jones fluid, hard-sphere fluid, and some liquefied noble gases.","This proves that this FDS represents a quasi-universal corresponding state principle for simple classical fluids with steep interactions.","Some related aspects such as Stokes-Einstein relation without a hydrodynamic diameter and gas-to-liquid dynamical crossover are briefly discussed.","Simple fitting formula for the transport coefficients of the dense WCA fluid are suggested."],"url":"http://arxiv.org/abs/2404.16603v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 12:42:48","title":"Probing the pole origin of $X(3872)$ with the coupled-channel dynamics","abstract":"The $X(3872)$, as the first and the most crucial member in the exotic charmoniumlike $XYZ$ family, has been studied for a long time. However, its dynamical origin, whether stemming from a $D\\bar{D}^*$ hadronic molecule or the first excited $P$-wave charmonium $\\chi_{c1}(2P)$, remains controversial. In this Letter, we demonstrate that the $X(3872)$ definitely does not result from the mass shift of the higher bare $\\chi_{c1}(2P)$ resonance pole in the coupled-channel dynamics involving a short-distance $c\\bar{c}$ core and the long-distance $D\\bar{D}^*$ channels. Instead, it originates from either the $D\\bar{D}^*$ molecular pole or the shadow pole associated with the $P$-wave charmonium, which depends on the concrete coupling mode between the $c\\bar{c}$ and $D\\bar{D}^*$. In order to further exploit the nature of $X(3872)$, we carefully investigate potential mechanisms that contribute to its pole width, which suggests that the coupled-channel dynamics plays a critical role in causing a noticeable discrepancy between the pole widths of $X(3872)$ and $T_{cc}^+$. Interestingly, we bridge the quantitative connection among the dynamics origin of $X(3872)$, its pole width and the properties of the predicted new resonance. The precise measurement of the pole width of $X(3872)$ and the search for the new charmoniumlike resonance become highly significant and can be anticipated in future LHCb, BESIII and Belle II experiments.","sentences":["The $X(3872)$, as the first and the most crucial member in the exotic charmoniumlike $XYZ$ family, has been studied for a long time.","However, its dynamical origin, whether stemming from a $D\\bar{D}^*$ hadronic molecule or the first excited $P$-wave charmonium $\\chi_{c1}(2P)$, remains controversial.","In this Letter, we demonstrate that the $X(3872)$ definitely does not result from the mass shift of the higher bare $\\chi_{c1}(2P)$ resonance pole in the coupled-channel dynamics involving a short-distance $c\\bar{c}$ core and the long-distance $D\\bar{D}^*$ channels.","Instead, it originates from either the $D\\bar{D}^*$ molecular pole or the shadow pole associated with the $P$-wave charmonium, which depends on the concrete coupling mode between the $c\\bar{c}$ and $D\\bar{D}^*$. In order to further exploit the nature of $X(3872)$, we carefully investigate potential mechanisms that contribute to its pole width, which suggests that the coupled-channel dynamics plays a critical role in causing a noticeable discrepancy between the pole widths of $X(3872)$ and $T_{cc}^+$. Interestingly, we bridge the quantitative connection among the dynamics origin of $X(3872)$, its pole width and the properties of the predicted new resonance.","The precise measurement of the pole width of $X(3872)$ and the search for the new charmoniumlike resonance become highly significant and can be anticipated in future LHCb, BESIII and Belle II experiments."],"url":"http://arxiv.org/abs/2404.16575v1","category":"hep-ph"}
{"created":"2024-04-25 12:32:24","title":"STELLA lightcurves of energetic pair instability supernovae in the context of SN2018ibb","abstract":"SN2018ibb is a recently observed hydrogen poor super-luminous supernova which appears to be powered by the decay of $30\\;\\rm{M_\\odot}$ of radioactive nickel. This supernova has been suggested to show hybrid signatures of a pair instability supernova and an interacting supernova. In a previous paper, we found that rotating, metal enriched pair instability supernova progenitors appeared to check both of these boxes. In this paper, we model the lightcurves of the pair instability supernovae using STELLA. We find that the STELLA models can explain the overall shape of the bolometric lightcurve of SN2018ibb, though not specific morphological features such as the luminosity peak or the bump at roughly three hundred days after the peak. We also estimate the contribution from interaction, and find that with relatively low wind velocities, the circum-stellar medium originating from the stellar winds is consistent with the evidence for interaction in the spectra. The observed values of the photosphere velocity in the hundred days after peak luminosity are similar to the STELLA models, but the deceleration is lower. This leads to the biggest inconsistency which is the black body temperature of SN2018ibb being much hotter than any of the STELLA models. We note that this high temperature (and the flat velocity) may be difficult to reconcile with the long rise time of SN2018ibb, but nevertheless conclude that if it is accurate, this discrepancy represents a challenge for SN2018ibb being a robust PISN candidate. This result is noteworthy given the lack of other scenarios for this supernova.","sentences":["SN2018ibb is a recently observed hydrogen poor super-luminous supernova which appears to be powered by the decay of $30\\;\\rm{M_\\odot}$ of radioactive nickel.","This supernova has been suggested to show hybrid signatures of a pair instability supernova and an interacting supernova.","In a previous paper, we found that rotating, metal enriched pair instability supernova progenitors appeared to check both of these boxes.","In this paper, we model the lightcurves of the pair instability supernovae using STELLA.","We find that the STELLA models can explain the overall shape of the bolometric lightcurve of SN2018ibb, though not specific morphological features such as the luminosity peak or the bump at roughly three hundred days after the peak.","We also estimate the contribution from interaction, and find that with relatively low wind velocities, the circum-stellar medium originating from the stellar winds is consistent with the evidence for interaction in the spectra.","The observed values of the photosphere velocity in the hundred days after peak luminosity are similar to the STELLA models, but the deceleration is lower.","This leads to the biggest inconsistency which is the black body temperature of SN2018ibb being much hotter than any of the STELLA models.","We note that this high temperature (and the flat velocity) may be difficult to reconcile with the long rise time of SN2018ibb, but nevertheless conclude that if it is accurate, this discrepancy represents a challenge for SN2018ibb being a robust PISN candidate.","This result is noteworthy given the lack of other scenarios for this supernova."],"url":"http://arxiv.org/abs/2404.16570v1","category":"astro-ph.HE"}
{"created":"2024-04-25 12:29:10","title":"Simulating Ultrafast Transient Absorption Spectra from First Principles using a Time-Dependent Configuration Interaction Probe","abstract":"Transient absorption spectroscopy (TAS) is among the most common ultrafast photochemical experiments, but its interpretation remains challenging. In this work, we present an efficient and robust method for simulating TAS signals from first principles. Excited-state absorption and stimulated emission (SE) signals are computed using time-dependent complete active space configuration interaction (TD-CASCI) simulations, leveraging the robustness of time-domain simulation to minimize electronic structure failure. We demonstrate our approach by simulating the TAS signal of 1$^\\prime$-hydroxy-2$^\\prime$-acetonapthone (HAN) from ab initio multiple spawning nonadiabatic molecular dynamics simulations. Our results are compared to gas-phase TAS data recorded from both jet-cooled ($T\\sim 40$ K) and hot ($\\sim 403$ K) molecules via cavity-enhanced transient absorption spectroscopy (CE-TAS). Decomposition of the computed spectrum allows us to assign a rise in the SE signal to excited-state proton transfer and the ultimate decay of the signal to relaxation through a twisted conical intersection. The total cost of computing the observable signal ($\\sim$1700 graphics processing unit hours for $\\sim$4 ns of electron dynamics) was markedly less than that of the {\\em ab initio} multiple spawning calculations used to compute the underlying nonadiabatic dynamics.","sentences":["Transient absorption spectroscopy (TAS) is among the most common ultrafast photochemical experiments, but its interpretation remains challenging.","In this work, we present an efficient and robust method for simulating TAS signals from first principles.","Excited-state absorption and stimulated emission (SE) signals are computed using time-dependent complete active space configuration interaction (TD-CASCI) simulations, leveraging the robustness of time-domain simulation to minimize electronic structure failure.","We demonstrate our approach by simulating the TAS signal of 1$^\\prime$-hydroxy-2$^\\prime$-acetonapthone (HAN) from ab initio multiple spawning nonadiabatic molecular dynamics simulations.","Our results are compared to gas-phase TAS data recorded from both jet-cooled ($T\\sim 40$ K) and hot ($\\sim 403$ K) molecules via cavity-enhanced transient absorption spectroscopy (CE-TAS).","Decomposition of the computed spectrum allows us to assign a rise in the SE signal to excited-state proton transfer and the ultimate decay of the signal to relaxation through a twisted conical intersection.","The total cost of computing the observable signal ($\\sim$1700 graphics processing unit hours for $\\sim$4 ns of electron dynamics) was markedly less than that of the {\\em ab initio} multiple spawning calculations used to compute the underlying nonadiabatic dynamics."],"url":"http://arxiv.org/abs/2404.16568v1","category":"physics.chem-ph"}
{"created":"2024-04-25 12:24:37","title":"Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark","abstract":"Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length.","sentences":["Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more.","In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms.","We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data.","Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features.","This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series.","Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter.","In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length."],"url":"http://arxiv.org/abs/2404.16563v1","category":"cs.CL"}
{"created":"2024-04-25 09:34:21","title":"Potential energy surfaces from many-body functionals: analytical benchmarks and conserving many-body approximations","abstract":"We investigate analytically the performance of many-body energy functionals, derived respectively by Klein and Luttinger and Ward, at different levels of diagrammatic approximations, ranging from second Born, to GW, to the so-called T-matrix, for the calculation of total energies and potential energy surfaces. We benchmark our theoretical results on the extended two-site Hubbard model, which is analytically solvable and for which several exact properties can be calculated. Despite its simplicity, this model displays the physics of strongly correlated electrons: it is prototypical of the H$_2$ dissociation, a notoriously difficult problem to solve accurately for the majority of mean-field based approaches. We show that both functionals exhibit good to excellent variational properties, particularly in the case of the Luttinger-Ward one, which is in close agreement with fully self-consistent calculations, and elucidate the relation between the accuracy of the results and the different input one-body Green's functions. Provided that these are wisely chosen, we show how the Luttinger-Ward functional can be used as a computationally inexpensive alternative to fully self-consistent many-body calculations, without sacrificing the precision of the results obtained. Furthermore, in virtue of this accuracy, we argue that this functional can also be used to rank different many-body approximations at different regimes of electronic correlation, once again bypassing the need for self-consistency.","sentences":["We investigate analytically the performance of many-body energy functionals, derived respectively by Klein and Luttinger and Ward, at different levels of diagrammatic approximations, ranging from second Born, to GW, to the so-called T-matrix, for the calculation of total energies and potential energy surfaces.","We benchmark our theoretical results on the extended two-site Hubbard model, which is analytically solvable and for which several exact properties can be calculated.","Despite its simplicity, this model displays the physics of strongly correlated electrons: it is prototypical of the H$_2$ dissociation, a notoriously difficult problem to solve accurately for the majority of mean-field based approaches.","We show that both functionals exhibit good to excellent variational properties, particularly in the case of the Luttinger-Ward one, which is in close agreement with fully self-consistent calculations, and elucidate the relation between the accuracy of the results and the different input one-body Green's functions.","Provided that these are wisely chosen, we show how the Luttinger-Ward functional can be used as a computationally inexpensive alternative to fully self-consistent many-body calculations, without sacrificing the precision of the results obtained.","Furthermore, in virtue of this accuracy, we argue that this functional can also be used to rank different many-body approximations at different regimes of electronic correlation, once again bypassing the need for self-consistency."],"url":"http://arxiv.org/abs/2404.16453v1","category":"cond-mat.str-el"}
{"created":"2024-04-25 08:56:05","title":"Soft X-ray prompt emission from a high-redshift gamma-ray burst EP240315a","abstract":"Long gamma-ray bursts (GRBs) are believed to originate from core collapse of massive stars. High-redshift GRBs can probe the star formation and reionization history of the early universe, but their detection remains rare. Here we report the detection of a GRB triggered in the 0.5--4 keV band by the Wide-field X-ray Telescope (WXT) on board the Einstein Probe (EP) mission, designated as EP240315a, whose bright peak was also detected by the Swift Burst Alert Telescope and Konus-Wind through off-line analyses. At a redshift of $z=4.859$, EP240315a showed a much longer and more complicated light curve in the soft X-ray band than in gamma-rays. Benefiting from a large field-of-view ($\\sim$3600 deg$^2$) and a high sensitivity, EP-WXT captured the earlier engine activation and extended late engine activity through a continuous detection. With a peak X-ray flux at the faint end of previously known high-$z$ GRBs, the detection of EP240315a demonstrates the great potential for EP to study the early universe via GRBs.","sentences":["Long gamma-ray bursts (GRBs) are believed to originate from core collapse of massive stars.","High-redshift GRBs can probe the star formation and reionization history of the early universe, but their detection remains rare.","Here we report the detection of a GRB triggered in the 0.5--4 keV band by the Wide-field X-ray Telescope (WXT) on board the Einstein Probe (EP) mission, designated as EP240315a, whose bright peak was also detected by the Swift Burst Alert Telescope and Konus-Wind through off-line analyses.","At a redshift of $z=4.859$, EP240315a showed a much longer and more complicated light curve in the soft X-ray band than in gamma-rays.","Benefiting from a large field-of-view ($\\sim$3600 deg$^2$) and a high sensitivity, EP-WXT captured the earlier engine activation and extended late engine activity through a continuous detection.","With a peak X-ray flux at the faint end of previously known high-$z$ GRBs, the detection of EP240315a demonstrates the great potential for EP to study the early universe via GRBs."],"url":"http://arxiv.org/abs/2404.16425v1","category":"astro-ph.HE"}
{"created":"2024-04-25 08:23:48","title":"Antibacterial size effect of ZnO nanoparticles and their role as additives in emulsion waterborne paint","abstract":"Nosocomial infections (NIs) are prevalent in intensive care units due to antibiotic overuse. Metal oxide nanoparticles (NPs), like ZnO, offer potential solutions, yet understanding how NPs size impacts their antibacterial efficacy are lacking. This study focuses on the effect of nanoparticle size on kinetics of bacterial strains growth. NPs were synthesized using a sol-gel process with monoethanolamine (MEA) and water, characterized using X-ray diffraction (XRD), transmission electron microscopy (TEM), and Raman spectroscopy, confirming crystallization and size variations. ZnO NPs with mean size of 22, 35 and 66 nm were used against the most common nosocomial bacteria strains Escherichia coli (Gram-negative), Pseudomonas aeruginosa (Gram-negative), and Staphylococcus aureus (Gram-positive). The evaluation of NPs minimal inhibitory concentration (MIC) and bactericidal concentration (MBC) revealed superior antibacterial activity in smaller NPs. The bacterial population was monitored via optical absorbance, showing reduced specific growth rate, prolonged latency period, and increased inhibition percentage with smaller NPs, indicating a substantial deceleration in the growth of microorganisms. Pseudomonas aeruginosa exhibited the smallest sensitivity to ZnO NPs, attributed to its environmental stress resistance. Furthermore, the antibacterial efficacy of paint containing 1 wt% of 22 nm ZnO NPs was assessed and displayed activity against E. coli and S. aureus.","sentences":["Nosocomial infections (NIs) are prevalent in intensive care units due to antibiotic overuse.","Metal oxide nanoparticles (NPs), like ZnO, offer potential solutions, yet understanding how NPs size impacts their antibacterial efficacy are lacking.","This study focuses on the effect of nanoparticle size on kinetics of bacterial strains growth.","NPs were synthesized using a sol-gel process with monoethanolamine (MEA) and water, characterized using X-ray diffraction (XRD), transmission electron microscopy (TEM), and Raman spectroscopy, confirming crystallization and size variations.","ZnO NPs with mean size of 22, 35 and 66 nm were used against the most common nosocomial bacteria strains Escherichia coli (Gram-negative), Pseudomonas aeruginosa (Gram-negative), and Staphylococcus aureus (Gram-positive).","The evaluation of NPs minimal inhibitory concentration (MIC) and bactericidal concentration (MBC) revealed superior antibacterial activity in smaller NPs.","The bacterial population was monitored via optical absorbance, showing reduced specific growth rate, prolonged latency period, and increased inhibition percentage with smaller NPs, indicating a substantial deceleration in the growth of microorganisms.","Pseudomonas aeruginosa exhibited the smallest sensitivity to ZnO NPs, attributed to its environmental stress resistance.","Furthermore, the antibacterial efficacy of paint containing 1 wt% of 22 nm ZnO NPs was assessed and displayed activity against E. coli and S. aureus."],"url":"http://arxiv.org/abs/2404.16400v1","category":"cond-mat.mtrl-sci"}
