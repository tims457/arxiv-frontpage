{"created":"2024-04-16 17:59:11","title":"COMBO: Compositional World Models for Embodied Multi-Agent Cooperation","abstract":"In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only partial egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents' actions given only partial egocentric visual observations of the world. To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations. To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video. By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning. To evaluate the efficacy of our methods, we create two challenging embodied multi-agent long-horizon cooperation tasks using the ThreeDWorld simulator and conduct experiments with 2-4 agents. The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed framework. More videos can be found at https://vis-www.cs.umass.edu/combo/.","sentences":["In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only partial egocentric views of the world.","To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents' actions given only partial egocentric visual observations of the world.","To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations.","To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video.","By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning.","To evaluate the efficacy of our methods, we create two challenging embodied multi-agent long-horizon cooperation tasks using the ThreeDWorld simulator and conduct experiments with 2-4 agents.","The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed framework.","More videos can be found at https://vis-www.cs.umass.edu/combo/."],"url":"http://arxiv.org/abs/2404.10775v1","category":"cs.CV"}
{"created":"2024-04-16 17:59:10","title":"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents","abstract":"Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.","sentences":["Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more.","Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM.","However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response.","In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost.","We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure.","Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences.","For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations.","Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy.","We release LLM-AggreFact, code for data synthesis, and models."],"url":"http://arxiv.org/abs/2404.10774v1","category":"cs.CL"}
{"created":"2024-04-16 17:47:16","title":"LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?","abstract":"Diffusion models have exhibited remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks. In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding. With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints. Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data. In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths. Our framework also includes a diffuser for semantic image-to-text conversion and a Back&Refine technique to enhance token interactivity during inference. LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules. This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation.","sentences":["Diffusion models have exhibited remarkable capabilities in text-to-image generation.","However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks.","In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding.","With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints.","Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data.","In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths.","Our framework also includes a diffuser for semantic image-to-text conversion and a Back&Refine technique to enhance token interactivity during inference.","LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules.","This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation."],"url":"http://arxiv.org/abs/2404.10763v1","category":"cs.AI"}
{"created":"2024-04-16 17:13:08","title":"N-Agent Ad Hoc Teamwork","abstract":"Current approaches to learning cooperative behaviors in multi-agent settings assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls \\textit{all} agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\\textit{single}$ agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards generalizing the class of scenarios that cooperative learning methods can address, we introduce $N$-agent ad hoc teamwork, in which a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates at evaluation time. This paper formalizes the problem, and proposes the $\\textit{Policy Optimization with Agent Modelling}$ (POAM) algorithm. POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors. Empirical evaluation on StarCraft II tasks shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates.","sentences":["Current approaches to learning cooperative behaviors in multi-agent settings assume relatively restrictive settings.","In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls \\textit{all} agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\\textit{single}$ agent in the scenario.","However, many cooperative settings in the real world are much less restrictive.","For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company.","Towards generalizing the class of scenarios that cooperative learning methods can address, we introduce $N$-agent ad hoc teamwork, in which a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates at evaluation time.","This paper formalizes the problem, and proposes the $\\textit{Policy Optimization with Agent Modelling}$ (POAM) algorithm.","POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors.","Empirical evaluation on StarCraft II tasks shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates."],"url":"http://arxiv.org/abs/2404.10740v1","category":"cs.AI"}
{"created":"2024-04-16 17:09:46","title":"On forcing axioms and weakenings of the Axiom of Choice","abstract":"We prove forcing axiom equivalents of two families of weakenings of the axiom of choice: a trichotomy principle for cardinals isolated by L\\'evy, ${\\rm H\\hskip0.05pt}_\\kappa$, and ${\\rm DC}_\\kappa$, the principle of dependent choices generalized to cardinals $\\kappa$, for regular cardinals $\\kappa$. Using these equivalents we obtain new forcing axiom formulations of the axiom of choice.   A point of interest is that we use a new template for forcing axioms. For the class of forcings to which we asks that the axioms apply, we do not ask that they apply to all collections of dense sets of a certain cardinality, but rather only for each particular forcing to a specific family of dense sets of the cardinality in question.","sentences":["We prove forcing axiom equivalents of two families of weakenings of the axiom of choice: a trichotomy principle for cardinals isolated by L\\'evy, ${\\rm H\\hskip0.05pt}_\\kappa$, and ${\\rm DC}_\\kappa$, the principle of dependent choices generalized to cardinals $\\kappa$, for regular cardinals $\\kappa$. Using these equivalents we obtain new forcing axiom formulations of the axiom of choice.   ","A point of interest is that we use a new template for forcing axioms.","For the class of forcings to which we asks that the axioms apply, we do not ask that they apply to all collections of dense sets of a certain cardinality, but rather only for each particular forcing to a specific family of dense sets of the cardinality in question."],"url":"http://arxiv.org/abs/2404.10736v1","category":"math.LO"}
{"created":"2024-04-16 17:05:43","title":"Bootstrapping Linear Models for Fast Online Adaptation in Human-Agent Collaboration","abstract":"Agents that assist people need to have well-initialized policies that can adapt quickly to align with their partners' reward functions. Initializing policies to maximize performance with unknown partners can be achieved by bootstrapping nonlinear models using imitation learning over large, offline datasets. Such policies can require prohibitive computation to fine-tune in-situ and therefore may miss critical run-time information about a partner's reward function as expressed through their immediate behavior. In contrast, online logistic regression using low-capacity models performs rapid inference and fine-tuning updates and thus can make effective use of immediate in-task behavior for reward function alignment. However, these low-capacity models cannot be bootstrapped as effectively by offline datasets and thus have poor initializations. We propose BLR-HAC, Bootstrapped Logistic Regression for Human Agent Collaboration, which bootstraps large nonlinear models to learn the parameters of a low-capacity model which then uses online logistic regression for updates during collaboration. We test BLR-HAC in a simulated surface rearrangement task and demonstrate that it achieves higher zero-shot accuracy than shallow methods and takes far less computation to adapt online while still achieving similar performance to fine-tuned, large nonlinear models. For code, please see our project page https://sites.google.com/view/blr-hac.","sentences":["Agents that assist people need to have well-initialized policies that can adapt quickly to align with their partners' reward functions.","Initializing policies to maximize performance with unknown partners can be achieved by bootstrapping nonlinear models using imitation learning over large, offline datasets.","Such policies can require prohibitive computation to fine-tune in-situ and therefore may miss critical run-time information about a partner's reward function as expressed through their immediate behavior.","In contrast, online logistic regression using low-capacity models performs rapid inference and fine-tuning updates and thus can make effective use of immediate in-task behavior for reward function alignment.","However, these low-capacity models cannot be bootstrapped as effectively by offline datasets and thus have poor initializations.","We propose BLR-HAC, Bootstrapped Logistic Regression for Human Agent Collaboration, which bootstraps large nonlinear models to learn the parameters of a low-capacity model which then uses online logistic regression for updates during collaboration.","We test BLR-HAC in a simulated surface rearrangement task and demonstrate that it achieves higher zero-shot accuracy than shallow methods and takes far less computation to adapt online while still achieving similar performance to fine-tuned, large nonlinear models.","For code, please see our project page https://sites.google.com/view/blr-hac."],"url":"http://arxiv.org/abs/2404.10733v1","category":"cs.AI"}
{"created":"2024-04-16 17:03:50","title":"What is Meant by AGI? On the Definition of Artificial General Intelligence","abstract":"This paper aims to establish a consensus on AGI's definition. General intelligence refers to the adaptation to open environments according to certain principles using limited resources. It emphasizes that adaptation or learning is an indispensable property of intelligence, and places the controversial part within the principles of intelligence, which can be described from different perspectives.","sentences":["This paper aims to establish a consensus on AGI's definition.","General intelligence refers to the adaptation to open environments according to certain principles using limited resources.","It emphasizes that adaptation or learning is an indispensable property of intelligence, and places the controversial part within the principles of intelligence, which can be described from different perspectives."],"url":"http://arxiv.org/abs/2404.10731v1","category":"cs.AI"}
{"created":"2024-04-16 17:02:52","title":"Insight Gained from Migrating a Machine Learning Model to Intelligence Processing Units","abstract":"The discoveries in this paper show that Intelligence Processing Units (IPUs) offer a viable accelerator alternative to GPUs for machine learning (ML) applications within the fields of materials science and battery research. We investigate the process of migrating a model from GPU to IPU and explore several optimization techniques, including pipelining and gradient accumulation, aimed at enhancing the performance of IPU-based models. Furthermore, we have effectively migrated a specialized model to the IPU platform. This model is employed for predicting effective conductivity, a parameter crucial in ion transport processes, which govern the performance of multiple charge and discharge cycles of batteries. The model utilizes a Convolutional Neural Network (CNN) architecture to perform prediction tasks for effective conductivity. The performance of this model on the IPU is found to be comparable to its execution on GPUs. We also analyze the utilization and performance of Graphcore's Bow IPU. Through benchmark tests, we observe significantly improved performance with the Bow IPU when compared to its predecessor, the Colossus IPU.","sentences":["The discoveries in this paper show that Intelligence Processing Units (IPUs) offer a viable accelerator alternative to GPUs for machine learning (ML) applications within the fields of materials science and battery research.","We investigate the process of migrating a model from GPU to IPU and explore several optimization techniques, including pipelining and gradient accumulation, aimed at enhancing the performance of IPU-based models.","Furthermore, we have effectively migrated a specialized model to the IPU platform.","This model is employed for predicting effective conductivity, a parameter crucial in ion transport processes, which govern the performance of multiple charge and discharge cycles of batteries.","The model utilizes a Convolutional Neural Network (CNN) architecture to perform prediction tasks for effective conductivity.","The performance of this model on the IPU is found to be comparable to its execution on GPUs.","We also analyze the utilization and performance of Graphcore's Bow IPU.","Through benchmark tests, we observe significantly improved performance with the Bow IPU when compared to its predecessor, the Colossus IPU."],"url":"http://arxiv.org/abs/2404.10730v1","category":"cs.LG"}
{"created":"2024-04-16 16:51:53","title":"Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study","abstract":"Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across various a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences.","Existing RLHF methods can be roughly categorized as either reward-based or reward-free.","Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO).","However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO).","Is DPO truly superior to PPO?","Why does PPO perform poorly on these benchmarks?","In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations.","Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs.","Finally, we benchmark DPO and PPO across various a collection of RLHF testbeds, ranging from dialogue to code generation.","Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions."],"url":"http://arxiv.org/abs/2404.10719v1","category":"cs.CL"}
{"created":"2024-04-16 16:51:12","title":"Mixed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation","abstract":"Recently, prototype learning has emerged in semi-supervised medical image segmentation and achieved remarkable performance. However, the scarcity of labeled data limits the expressiveness of prototypes in previous methods, potentially hindering the complete representation of prototypes for class embedding. To address this problem, we propose the Mixed Prototype Consistency Learning (MPCL) framework, which includes a Mean Teacher and an auxiliary network. The Mean Teacher generates prototypes for labeled and unlabeled data, while the auxiliary network produces additional prototypes for mixed data processed by CutMix. Through prototype fusion, mixed prototypes provide extra semantic information to both labeled and unlabeled prototypes. High-quality global prototypes for each class are formed by fusing two enhanced prototypes, optimizing the distribution of hidden embeddings used in consistency learning. Extensive experiments on the left atrium and type B aortic dissection datasets demonstrate MPCL's superiority over previous state-of-the-art approaches, confirming the effectiveness of our framework. The code will be released soon.","sentences":["Recently, prototype learning has emerged in semi-supervised medical image segmentation and achieved remarkable performance.","However, the scarcity of labeled data limits the expressiveness of prototypes in previous methods, potentially hindering the complete representation of prototypes for class embedding.","To address this problem, we propose the Mixed Prototype Consistency Learning (MPCL) framework, which includes a Mean Teacher and an auxiliary network.","The Mean Teacher generates prototypes for labeled and unlabeled data, while the auxiliary network produces additional prototypes for mixed data processed by CutMix.","Through prototype fusion, mixed prototypes provide extra semantic information to both labeled and unlabeled prototypes.","High-quality global prototypes for each class are formed by fusing two enhanced prototypes, optimizing the distribution of hidden embeddings used in consistency learning.","Extensive experiments on the left atrium and type B aortic dissection datasets demonstrate MPCL's superiority over previous state-of-the-art approaches, confirming the effectiveness of our framework.","The code will be released soon."],"url":"http://arxiv.org/abs/2404.10717v1","category":"cs.CV"}
{"created":"2024-04-16 16:30:27","title":"Cross-Language Evolution of Divergent Collective Memory Around the Arab Spring","abstract":"The Arab Spring was a historic set of protests beginning in 2011 that toppled governments and led to major conflicts. Collective memories of events like these can vary significantly across social contexts in response to political, cultural, and linguistic factors. While Wikipedia plays an important role in documenting both historic and current events, little attention has been given to how Wikipedia articles, created in the aftermath of major events, continue to evolve over years or decades. Using the archived content of Arab Spring-related topics across the Arabic and English Wikipedias between 2011 and 2024, we define and evaluate multilingual measures of event salience, deliberation, contextualization, and consolidation of collective memory surrounding the Arab Spring. Our findings about the temporal evolution of the Wikipedia articles' content similarity across languages has implications for theorizing about online collective memory processes and evaluating linguistic models trained on these data.","sentences":["The Arab Spring was a historic set of protests beginning in 2011 that toppled governments and led to major conflicts.","Collective memories of events like these can vary significantly across social contexts in response to political, cultural, and linguistic factors.","While Wikipedia plays an important role in documenting both historic and current events, little attention has been given to how Wikipedia articles, created in the aftermath of major events, continue to evolve over years or decades.","Using the archived content of Arab Spring-related topics across the Arabic and English Wikipedias between 2011 and 2024, we define and evaluate multilingual measures of event salience, deliberation, contextualization, and consolidation of collective memory surrounding the Arab Spring.","Our findings about the temporal evolution of the Wikipedia articles' content similarity across languages has implications for theorizing about online collective memory processes and evaluating linguistic models trained on these data."],"url":"http://arxiv.org/abs/2404.10706v1","category":"cs.CY"}
{"created":"2024-04-16 16:23:10","title":"Question Difficulty Ranking for Multiple-Choice Reading Comprehension","abstract":"Multiple-choice (MC) tests are an efficient method to assess English learners. It is useful for test creators to rank candidate MC questions by difficulty during exam curation. Typically, the difficulty is determined by having human test takers trial the questions in a pretesting stage. However, this is expensive and not scalable. Therefore, we explore automated approaches to rank MC questions by difficulty. However, there is limited data for explicit training of a system for difficulty scores. Hence, we compare task transfer and zero-shot approaches: task transfer adapts level classification and reading comprehension systems for difficulty ranking while zero-shot prompting of instruction finetuned language models contrasts absolute assessment against comparative. It is found that level classification transfers better than reading comprehension. Additionally, zero-shot comparative assessment is more effective at difficulty ranking than the absolute assessment and even the task transfer approaches at question difficulty ranking with a Spearman's correlation of 40.4%. Combining the systems is observed to further boost the correlation.","sentences":["Multiple-choice (MC) tests are an efficient method to assess English learners.","It is useful for test creators to rank candidate MC questions by difficulty during exam curation.","Typically, the difficulty is determined by having human test takers trial the questions in a pretesting stage.","However, this is expensive and not scalable.","Therefore, we explore automated approaches to rank MC questions by difficulty.","However, there is limited data for explicit training of a system for difficulty scores.","Hence, we compare task transfer and zero-shot approaches: task transfer adapts level classification and reading comprehension systems for difficulty ranking while zero-shot prompting of instruction finetuned language models contrasts absolute assessment against comparative.","It is found that level classification transfers better than reading comprehension.","Additionally, zero-shot comparative assessment is more effective at difficulty ranking than the absolute assessment and even the task transfer approaches at question difficulty ranking with a Spearman's correlation of 40.4%.","Combining the systems is observed to further boost the correlation."],"url":"http://arxiv.org/abs/2404.10704v1","category":"cs.CL"}
{"created":"2024-04-16 16:17:48","title":"Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs","abstract":"Modern smartphone camera quality heavily relies on the image signal processor (ISP) to enhance captured raw images, utilizing carefully designed modules to produce final output images encoded in a standard color space (e.g., sRGB). Neural-based end-to-end learnable ISPs offer promising advancements, potentially replacing traditional ISPs with their ability to adapt without requiring extensive tuning for each new camera model, as is often the case for nearly every module in traditional ISPs. However, the key challenge with the recent learning-based ISPs is the urge to collect large paired datasets for each distinct camera model due to the influence of intrinsic camera characteristics on the formation of input raw images. This paper tackles this challenge by introducing a novel method for unpaired learning of raw-to-raw translation across diverse cameras. Specifically, we propose Rawformer, an unsupervised Transformer-based encoder-decoder method for raw-to-raw translation. It accurately maps raw images captured by a certain camera to the target camera, facilitating the generalization of learnable ISPs to new unseen cameras. Our method demonstrates superior performance on real camera datasets, achieving higher accuracy compared to previous state-of-the-art techniques, and preserving a more robust correlation between the original and translated raw images.","sentences":["Modern smartphone camera quality heavily relies on the image signal processor (ISP) to enhance captured raw images, utilizing carefully designed modules to produce final output images encoded in a standard color space (e.g., sRGB).","Neural-based end-to-end learnable ISPs offer promising advancements, potentially replacing traditional ISPs with their ability to adapt without requiring extensive tuning for each new camera model, as is often the case for nearly every module in traditional ISPs.","However, the key challenge with the recent learning-based ISPs is the urge to collect large paired datasets for each distinct camera model due to the influence of intrinsic camera characteristics on the formation of input raw images.","This paper tackles this challenge by introducing a novel method for unpaired learning of raw-to-raw translation across diverse cameras.","Specifically, we propose Rawformer, an unsupervised Transformer-based encoder-decoder method for raw-to-raw translation.","It accurately maps raw images captured by a certain camera to the target camera, facilitating the generalization of learnable ISPs to new unseen cameras.","Our method demonstrates superior performance on real camera datasets, achieving higher accuracy compared to previous state-of-the-art techniques, and preserving a more robust correlation between the original and translated raw images."],"url":"http://arxiv.org/abs/2404.10700v1","category":"eess.IV"}
{"created":"2024-04-16 16:16:40","title":"ECLAIR: A High-Fidelity Aerial LiDAR Dataset for Semantic Segmentation","abstract":"We introduce ECLAIR (Extended Classification of Lidar for AI Recognition), a new outdoor large-scale aerial LiDAR dataset designed specifically for advancing research in point cloud semantic segmentation. As the most extensive and diverse collection of its kind to date, the dataset covers a total area of 10$km^2$ with close to 600 million points and features eleven distinct object categories. To guarantee the dataset's quality and utility, we have thoroughly curated the point labels through an internal team of experts, ensuring accuracy and consistency in semantic labeling. The dataset is engineered to move forward the fields of 3D urban modeling, scene understanding, and utility infrastructure management by presenting new challenges and potential applications. As a benchmark, we report qualitative and quantitative analysis of a voxel-based point cloud segmentation approach based on the Minkowski Engine.","sentences":["We introduce ECLAIR (Extended Classification of Lidar for AI Recognition), a new outdoor large-scale aerial LiDAR dataset designed specifically for advancing research in point cloud semantic segmentation.","As the most extensive and diverse collection of its kind to date, the dataset covers a total area of 10$km^2$ with close to 600 million points and features eleven distinct object categories.","To guarantee the dataset's quality and utility, we have thoroughly curated the point labels through an internal team of experts, ensuring accuracy and consistency in semantic labeling.","The dataset is engineered to move forward the fields of 3D urban modeling, scene understanding, and utility infrastructure management by presenting new challenges and potential applications.","As a benchmark, we report qualitative and quantitative analysis of a voxel-based point cloud segmentation approach based on the Minkowski Engine."],"url":"http://arxiv.org/abs/2404.10699v1","category":"cs.CV"}
{"created":"2024-04-16 16:00:59","title":"Simplex Decomposition for Portfolio Allocation Constraints in Reinforcement Learning","abstract":"Portfolio optimization tasks describe sequential decision problems in which the investor's wealth is distributed across a set of assets. Allocation constraints are used to enforce minimal or maximal investments into particular subsets of assets to control for objectives such as limiting the portfolio's exposure to a certain sector due to environmental concerns. Although methods for constrained Reinforcement Learning (CRL) can optimize policies while considering allocation constraints, it can be observed that these general methods yield suboptimal results. In this paper, we propose a novel approach to handle allocation constraints based on a decomposition of the constraint action space into a set of unconstrained allocation problems. In particular, we examine this approach for the case of two constraints. For example, an investor may wish to invest at least a certain percentage of the portfolio into green technologies while limiting the investment in the fossil energy sector. We show that the action space of the task is equivalent to the decomposed action space, and introduce a new reinforcement learning (RL) approach CAOSD, which is built on top of the decomposition. The experimental evaluation on real-world Nasdaq-100 data demonstrates that our approach consistently outperforms state-of-the-art CRL benchmarks for portfolio optimization.","sentences":["Portfolio optimization tasks describe sequential decision problems in which the investor's wealth is distributed across a set of assets.","Allocation constraints are used to enforce minimal or maximal investments into particular subsets of assets to control for objectives such as limiting the portfolio's exposure to a certain sector due to environmental concerns.","Although methods for constrained Reinforcement Learning (CRL) can optimize policies while considering allocation constraints, it can be observed that these general methods yield suboptimal results.","In this paper, we propose a novel approach to handle allocation constraints based on a decomposition of the constraint action space into a set of unconstrained allocation problems.","In particular, we examine this approach for the case of two constraints.","For example, an investor may wish to invest at least a certain percentage of the portfolio into green technologies while limiting the investment in the fossil energy sector.","We show that the action space of the task is equivalent to the decomposed action space, and introduce a new reinforcement learning (RL) approach CAOSD, which is built on top of the decomposition.","The experimental evaluation on real-world Nasdaq-100 data demonstrates that our approach consistently outperforms state-of-the-art CRL benchmarks for portfolio optimization."],"url":"http://arxiv.org/abs/2404.10683v1","category":"cs.AI"}
{"created":"2024-04-16 15:58:20","title":"HSVI-based Online Minimax Strategies for Partially Observable Stochastic Games with Neural Perception Mechanisms","abstract":"We consider a variant of continuous-state partially-observable stochastic games with neural perception mechanisms and an asymmetric information structure. One agent has partial information, with the observation function implemented as a neural network, while the other agent is assumed to have full knowledge of the state. We present, for the first time, an efficient online method to compute an $\\varepsilon$-minimax strategy profile, which requires only one linear program to be solved for each agent at every stage, instead of a complex estimation of opponent counterfactual values. For the partially-informed agent, we propose a continual resolving approach which uses lower bounds, pre-computed offline with heuristic search value iteration (HSVI), instead of opponent counterfactual values. This inherits the soundness of continual resolving at the cost of pre-computing the bound. For the fully-informed agent, we propose an inferred-belief strategy, where the agent maintains an inferred belief about the belief of the partially-informed agent based on (offline) upper bounds from HSVI, guaranteeing $\\varepsilon$-distance to the value of the game at the initial belief known to both agents.","sentences":["We consider a variant of continuous-state partially-observable stochastic games with neural perception mechanisms and an asymmetric information structure.","One agent has partial information, with the observation function implemented as a neural network, while the other agent is assumed to have full knowledge of the state.","We present, for the first time, an efficient online method to compute an $\\varepsilon$-minimax strategy profile, which requires only one linear program to be solved for each agent at every stage, instead of a complex estimation of opponent counterfactual values.","For the partially-informed agent, we propose a continual resolving approach which uses lower bounds, pre-computed offline with heuristic search value iteration (HSVI), instead of opponent counterfactual values.","This inherits the soundness of continual resolving at the cost of pre-computing the bound.","For the fully-informed agent, we propose an inferred-belief strategy, where the agent maintains an inferred belief about the belief of the partially-informed agent based on (offline) upper bounds from HSVI, guaranteeing $\\varepsilon$-distance to the value of the game at the initial belief known to both agents."],"url":"http://arxiv.org/abs/2404.10679v1","category":"cs.GT"}
{"created":"2024-04-16 15:53:41","title":"Automating REST API Postman Test Cases Using LLM","abstract":"In the contemporary landscape of technological advancements, the automation of manual processes is crucial, compelling the demand for huge datasets to effectively train and test machines. This research paper is dedicated to the exploration and implementation of an automated approach to generate test cases specifically using Large Language Models. The methodology integrates the use of Open AI to enhance the efficiency and effectiveness of test case generation for training and evaluating Large Language Models. This formalized approach with LLMs simplifies the testing process, making it more efficient and comprehensive. Leveraging natural language understanding, LLMs can intelligently formulate test cases that cover a broad range of REST API properties, ensuring comprehensive testing. The model that is developed during the research is trained using manually collected postman test cases or instances for various Rest APIs. LLMs enhance the creation of Postman test cases by automating the generation of varied and intricate test scenarios. Postman test cases offer streamlined automation, collaboration, and dynamic data handling, providing a user-friendly and efficient approach to API testing compared to traditional test cases. Thus, the model developed not only conforms to current technological standards but also holds the promise of evolving into an idea of substantial importance in future technological advancements.","sentences":["In the contemporary landscape of technological advancements, the automation of manual processes is crucial, compelling the demand for huge datasets to effectively train and test machines.","This research paper is dedicated to the exploration and implementation of an automated approach to generate test cases specifically using Large Language Models.","The methodology integrates the use of Open AI to enhance the efficiency and effectiveness of test case generation for training and evaluating Large Language Models.","This formalized approach with LLMs simplifies the testing process, making it more efficient and comprehensive.","Leveraging natural language understanding, LLMs can intelligently formulate test cases that cover a broad range of REST API properties, ensuring comprehensive testing.","The model that is developed during the research is trained using manually collected postman test cases or instances for various Rest APIs.","LLMs enhance the creation of Postman test cases by automating the generation of varied and intricate test scenarios.","Postman test cases offer streamlined automation, collaboration, and dynamic data handling, providing a user-friendly and efficient approach to API testing compared to traditional test cases.","Thus, the model developed not only conforms to current technological standards but also holds the promise of evolving into an idea of substantial importance in future technological advancements."],"url":"http://arxiv.org/abs/2404.10678v1","category":"cs.SE"}
{"created":"2024-04-16 15:50:19","title":"SCALE: Self-Correcting Visual Navigation for Mobile Robots via Anti-Novelty Estimation","abstract":"Although visual navigation has been extensively studied using deep reinforcement learning, online learning for real-world robots remains a challenging task. Recent work directly learned from offline dataset to achieve broader generalization in the real-world tasks, which, however, faces the out-of-distribution (OOD) issue and potential robot localization failures in a given map for unseen observation. This significantly drops the success rates and even induces collision. In this paper, we present a self-correcting visual navigation method, SCALE, that can autonomously prevent the robot from the OOD situations without human intervention. Specifically, we develop an image-goal conditioned offline reinforcement learning method based on implicit Q-learning (IQL). When facing OOD observation, our novel localization recovery method generates the potential future trajectories by learning from the navigation affordance, and estimates the future novelty via random network distillation (RND). A tailored cost function searches for the candidates with the least novelty that can lead the robot to the familiar places. We collect offline data and conduct evaluation experiments in three real-world urban scenarios. Experiment results show that SCALE outperforms the previous state-of-the-art methods for open-world navigation with a unique capability of localization recovery, significantly reducing the need for human intervention. Code is available at https://github.com/KubeEdge4Robotics/ScaleNav.","sentences":["Although visual navigation has been extensively studied using deep reinforcement learning, online learning for real-world robots remains a challenging task.","Recent work directly learned from offline dataset to achieve broader generalization in the real-world tasks, which, however, faces the out-of-distribution (OOD) issue and potential robot localization failures in a given map for unseen observation.","This significantly drops the success rates and even induces collision.","In this paper, we present a self-correcting visual navigation method, SCALE, that can autonomously prevent the robot from the OOD situations without human intervention.","Specifically, we develop an image-goal conditioned offline reinforcement learning method based on implicit Q-learning (IQL).","When facing OOD observation, our novel localization recovery method generates the potential future trajectories by learning from the navigation affordance, and estimates the future novelty via random network distillation (RND).","A tailored cost function searches for the candidates with the least novelty that can lead the robot to the familiar places.","We collect offline data and conduct evaluation experiments in three real-world urban scenarios.","Experiment results show that SCALE outperforms the previous state-of-the-art methods for open-world navigation with a unique capability of localization recovery, significantly reducing the need for human intervention.","Code is available at https://github.com/KubeEdge4Robotics/ScaleNav."],"url":"http://arxiv.org/abs/2404.10675v1","category":"cs.RO"}
{"created":"2024-04-16 15:39:11","title":"Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay","abstract":"We study continual offline reinforcement learning, a practical paradigm that facilitates forward transfer and mitigates catastrophic forgetting to tackle sequential offline tasks. We propose a dual generative replay framework that retains previous knowledge by concurrent replay of generated pseudo-data. First, we decouple the continual learning policy into a diffusion-based generative behavior model and a multi-head action evaluation model, allowing the policy to inherit distributional expressivity for encompassing a progressive range of diverse behaviors. Second, we train a task-conditioned diffusion model to mimic state distributions of past tasks. Generated states are paired with corresponding responses from the behavior generator to represent old tasks with high-fidelity replayed samples. Finally, by interleaving pseudo samples with real ones of the new task, we continually update the state and behavior generators to model progressively diverse behaviors, and regularize the multi-head critic via behavior cloning to mitigate forgetting. Experiments demonstrate that our method achieves better forward transfer with less forgetting, and closely approximates the results of using previous ground-truth data due to its high-fidelity replay of the sample space. Our code is available at \\href{https://github.com/NJU-RL/CuGRO}{https://github.com/NJU-RL/CuGRO}.","sentences":["We study continual offline reinforcement learning, a practical paradigm that facilitates forward transfer and mitigates catastrophic forgetting to tackle sequential offline tasks.","We propose a dual generative replay framework that retains previous knowledge by concurrent replay of generated pseudo-data.","First, we decouple the continual learning policy into a diffusion-based generative behavior model and a multi-head action evaluation model, allowing the policy to inherit distributional expressivity for encompassing a progressive range of diverse behaviors.","Second, we train a task-conditioned diffusion model to mimic state distributions of past tasks.","Generated states are paired with corresponding responses from the behavior generator to represent old tasks with high-fidelity replayed samples.","Finally, by interleaving pseudo samples with real ones of the new task, we continually update the state and behavior generators to model progressively diverse behaviors, and regularize the multi-head critic via behavior cloning to mitigate forgetting.","Experiments demonstrate that our method achieves better forward transfer with less forgetting, and closely approximates the results of using previous ground-truth data due to its high-fidelity replay of the sample space.","Our code is available at \\href{https://github.com/NJU-RL/CuGRO}{https://github.com/NJU-RL/CuGRO}."],"url":"http://arxiv.org/abs/2404.10662v1","category":"cs.LG"}
{"created":"2024-04-16 15:35:34","title":"Trajectory Planning using Reinforcement Learning for Interactive Overtaking Maneuvers in Autonomous Racing Scenarios","abstract":"Conventional trajectory planning approaches for autonomous racing are based on the sequential execution of prediction of the opposing vehicles and subsequent trajectory planning for the ego vehicle. If the opposing vehicles do not react to the ego vehicle, they can be predicted accurately. However, if there is interaction between the vehicles, the prediction loses its validity. For high interaction, instead of a planning approach that reacts exclusively to the fixed prediction, a trajectory planning approach is required that incorporates the interaction with the opposing vehicles. This paper demonstrates the limitations of a widely used conventional sampling-based approach within a highly interactive blocking scenario. We show that high success rates are achieved for less aggressive blocking behavior but that the collision rate increases with more significant interaction. We further propose a novel Reinforcement Learning (RL)-based trajectory planning approach for racing that explicitly exploits the interaction with the opposing vehicle without requiring a prediction. In contrast to the conventional approach, the RL-based approach achieves high success rates even for aggressive blocking behavior. Furthermore, we propose a novel safety layer (SL) that intervenes when the trajectory generated by the RL-based approach is infeasible. In that event, the SL generates a sub-optimal but feasible trajectory, avoiding termination of the scenario due to a not found valid solution.","sentences":["Conventional trajectory planning approaches for autonomous racing are based on the sequential execution of prediction of the opposing vehicles and subsequent trajectory planning for the ego vehicle.","If the opposing vehicles do not react to the ego vehicle, they can be predicted accurately.","However, if there is interaction between the vehicles, the prediction loses its validity.","For high interaction, instead of a planning approach that reacts exclusively to the fixed prediction, a trajectory planning approach is required that incorporates the interaction with the opposing vehicles.","This paper demonstrates the limitations of a widely used conventional sampling-based approach within a highly interactive blocking scenario.","We show that high success rates are achieved for less aggressive blocking behavior but that the collision rate increases with more significant interaction.","We further propose a novel Reinforcement Learning (RL)-based trajectory planning approach for racing that explicitly exploits the interaction with the opposing vehicle without requiring a prediction.","In contrast to the conventional approach, the RL-based approach achieves high success rates even for aggressive blocking behavior.","Furthermore, we propose a novel safety layer (SL) that intervenes when the trajectory generated by the RL-based approach is infeasible.","In that event, the SL generates a sub-optimal but feasible trajectory, avoiding termination of the scenario due to a not found valid solution."],"url":"http://arxiv.org/abs/2404.10658v1","category":"cs.RO"}
{"created":"2024-04-16 15:20:28","title":"Efficient Parking Search using Shared Fleet Data","abstract":"Finding an available on-street parking spot is a relevant problem of day-to-day life. In recent years, cities such as Melbourne and San Francisco deployed sensors that provide real-time information about the occupation of parking spots. Finding a free parking spot in such a smart environment can be modeled and solved as a Markov decision process (MDP). The problem has to consider uncertainty as available parking spots might not remain available until arrival due to other vehicles also claiming spots in the meantime. Knowing the parking intention of every vehicle in the environment would eliminate this uncertainty. Unfortunately, it does currently not seem realistic to have such data from all vehicles. In contrast, acquiring data from a subset of vehicles or a vehicle fleet appears feasible and has the potential to reduce uncertainty.   In this paper, we examine the question of how useful sharing data within a vehicle fleet might be for the search times of particular drivers. We use fleet data to better estimate the availability of parking spots at arrival. Since optimal solutions for large scenarios are infeasible, we base our method on approximate solutions, which have been shown to perform well in single-agent settings. Our experiments are conducted on a simulation using real-world and synthetic data from the city of Melbourne. The results indicate that fleet data can significantly reduce search times for an available parking spot.","sentences":["Finding an available on-street parking spot is a relevant problem of day-to-day life.","In recent years, cities such as Melbourne and San Francisco deployed sensors that provide real-time information about the occupation of parking spots.","Finding a free parking spot in such a smart environment can be modeled and solved as a Markov decision process (MDP).","The problem has to consider uncertainty as available parking spots might not remain available until arrival due to other vehicles also claiming spots in the meantime.","Knowing the parking intention of every vehicle in the environment would eliminate this uncertainty.","Unfortunately, it does currently not seem realistic to have such data from all vehicles.","In contrast, acquiring data from a subset of vehicles or a vehicle fleet appears feasible and has the potential to reduce uncertainty.   ","In this paper, we examine the question of how useful sharing data within a vehicle fleet might be for the search times of particular drivers.","We use fleet data to better estimate the availability of parking spots at arrival.","Since optimal solutions for large scenarios are infeasible, we base our method on approximate solutions, which have been shown to perform well in single-agent settings.","Our experiments are conducted on a simulation using real-world and synthetic data from the city of Melbourne.","The results indicate that fleet data can significantly reduce search times for an available parking spot."],"url":"http://arxiv.org/abs/2404.10646v1","category":"cs.AI"}
{"created":"2024-04-16 15:18:40","title":"Continuous Control Reinforcement Learning: Distributed Distributional DrQ Algorithms","abstract":"Distributed Distributional DrQ is a model-free and off-policy RL algorithm for continuous control tasks based on the state and observation of the agent, which is an actor-critic method with the data-augmentation and the distributional perspective of critic value function. Aim to learn to control the agent and master some tasks in a high-dimensional continuous space. DrQ-v2 uses DDPG as the backbone and achieves out-performance in various continuous control tasks. Here Distributed Distributional DrQ uses Distributed Distributional DDPG as the backbone, and this modification aims to achieve better performance in some hard continuous control tasks through the better expression ability of distributional value function and distributed actor policies.","sentences":["Distributed Distributional DrQ is a model-free and off-policy RL algorithm for continuous control tasks based on the state and observation of the agent, which is an actor-critic method with the data-augmentation and the distributional perspective of critic value function.","Aim to learn to control the agent and master some tasks in a high-dimensional continuous space.","DrQ-v2 uses DDPG as the backbone and achieves out-performance in various continuous control tasks.","Here Distributed Distributional DrQ uses Distributed Distributional DDPG as the backbone, and this modification aims to achieve better performance in some hard continuous control tasks through the better expression ability of distributional value function and distributed actor policies."],"url":"http://arxiv.org/abs/2404.10645v1","category":"cs.LG"}
{"created":"2024-04-16 15:03:59","title":"Constrained Object Placement Using Reinforcement Learning","abstract":"Close and precise placement of irregularly shaped objects requires a skilled robotic system. Particularly challenging is the manipulation of objects that have sensitive top surfaces and a fixed set of neighbors. To avoid damaging the surface, they have to be grasped from the side, and during placement, their neighbor relations have to be maintained. In this work, we train a reinforcement learning agent that generates smooth end-effector motions to place objects as close as possible next to each other. During the placement, our agent considers neighbor constraints defined in a given layout of the objects while trying to avoid collisions. Our approach learns to place compact object assemblies without the need for predefined spacing between objects as required by traditional methods. We thoroughly evaluated our approach using a two-finger gripper mounted to a robotic arm with six degrees of freedom. The results show that our agent outperforms two baseline approaches in terms of object assembly compactness, thereby reducing the needed space to place the objects according to the given neighbor constraints. On average, our approach reduces the distances between all placed objects by at least 60%, with fewer collisions at the same compactness compared to both baselines.","sentences":["Close and precise placement of irregularly shaped objects requires a skilled robotic system.","Particularly challenging is the manipulation of objects that have sensitive top surfaces and a fixed set of neighbors.","To avoid damaging the surface, they have to be grasped from the side, and during placement, their neighbor relations have to be maintained.","In this work, we train a reinforcement learning agent that generates smooth end-effector motions to place objects as close as possible next to each other.","During the placement, our agent considers neighbor constraints defined in a given layout of the objects while trying to avoid collisions.","Our approach learns to place compact object assemblies without the need for predefined spacing between objects as required by traditional methods.","We thoroughly evaluated our approach using a two-finger gripper mounted to a robotic arm with six degrees of freedom.","The results show that our agent outperforms two baseline approaches in terms of object assembly compactness, thereby reducing the needed space to place the objects according to the given neighbor constraints.","On average, our approach reduces the distances between all placed objects by at least 60%, with fewer collisions at the same compactness compared to both baselines."],"url":"http://arxiv.org/abs/2404.10632v1","category":"cs.RO"}
{"created":"2024-04-16 14:42:49","title":"Private Attribute Inference from Images with Vision-Language Models","abstract":"As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus. While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that the increase in models' capabilities has enabled LLMs to make accurate privacy-infringing inferences from previously unseen texts. With the rise of multimodal vision-language models (VLMs), capable of understanding both images and text, a pertinent question is whether such results transfer to the previously unexplored domain of benign images posted online. To investigate the risks associated with the image reasoning capabilities of newly emerging VLMs, we compile an image dataset with human-annotated labels of the image owner's personal attributes. In order to understand the additional privacy risk posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans. On this dataset, we evaluate the inferential capabilities of 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger adversaries, establishing an imperative for the development of adequate defenses.","sentences":["As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus.","While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that the increase in models' capabilities has enabled LLMs to make accurate privacy-infringing inferences from previously unseen texts.","With the rise of multimodal vision-language models (VLMs), capable of understanding both images and text, a pertinent question is whether such results transfer to the previously unexplored domain of benign images posted online.","To investigate the risks associated with the image reasoning capabilities of newly emerging VLMs, we compile an image dataset with human-annotated labels of the image owner's personal attributes.","In order to understand the additional privacy risk posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans.","On this dataset, we evaluate the inferential capabilities of 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy.","Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger adversaries, establishing an imperative for the development of adequate defenses."],"url":"http://arxiv.org/abs/2404.10618v1","category":"cs.AI"}
{"created":"2024-04-16 14:40:50","title":"Emergent intelligence of buckling-driven elasto-active structures","abstract":"Active systems of self-propelled agents, e.g., birds, fish, and bacteria, can organize their collective motion into myriad autonomous behaviors. Ubiquitous in nature and across length scales, such phenomena are also amenable to artificial settings, e.g., where brainless self-propelled robots orchestrate their movements into spatio-temportal patterns via the application of external cues or when confined within flexible boundaries. Very much like their natural counterparts, these approaches typically require many units to initiate collective motion such that controlling the ensuing dynamics is challenging. Here, we demonstrate a novel yet simple mechanism that leverages nonlinear elasticity to tame near-diffusive motile particles in forming structures capable of directed motion and other emergent intelligent behaviors. Our elasto-active system comprises two centimeter-sized self-propelled microbots connected with elastic beams. These microbots exert forces that suffice to buckle the beam and set the structure in motion. We first rationalize the physics of the interaction between the beam and the microbots. Then we use reduced order models to predict the interactions of our elasto-active structure with boundaries, e.g., walls and constrictions, and demonstrate how they can exhibit intelligent behaviors such as maze navigation. The findings are relevant to designing intelligent materials or soft robots capable of autonomous space exploration, adaptation, and interaction with the surrounding environment.","sentences":["Active systems of self-propelled agents, e.g., birds, fish, and bacteria, can organize their collective motion into myriad autonomous behaviors.","Ubiquitous in nature and across length scales, such phenomena are also amenable to artificial settings, e.g., where brainless self-propelled robots orchestrate their movements into spatio-temportal patterns via the application of external cues or when confined within flexible boundaries.","Very much like their natural counterparts, these approaches typically require many units to initiate collective motion such that controlling the ensuing dynamics is challenging.","Here, we demonstrate a novel yet simple mechanism that leverages nonlinear elasticity to tame near-diffusive motile particles in forming structures capable of directed motion and other emergent intelligent behaviors.","Our elasto-active system comprises two centimeter-sized self-propelled microbots connected with elastic beams.","These microbots exert forces that suffice to buckle the beam and set the structure in motion.","We first rationalize the physics of the interaction between the beam and the microbots.","Then we use reduced order models to predict the interactions of our elasto-active structure with boundaries, e.g., walls and constrictions, and demonstrate how they can exhibit intelligent behaviors such as maze navigation.","The findings are relevant to designing intelligent materials or soft robots capable of autonomous space exploration, adaptation, and interaction with the surrounding environment."],"url":"http://arxiv.org/abs/2404.10614v1","category":"cond-mat.soft"}
{"created":"2024-04-16 14:37:53","title":"Shining Light into the Tunnel: Understanding and Classifying Network Traffic of Residential Proxies","abstract":"Emerging in recent years, residential proxies (RESIPs) feature multiple unique characteristics when compared with traditional network proxies (e.g., commercial VPNs), particularly, the deployment in residential networks rather than data center networks, the worldwide distribution in tens of thousands of cities and ISPs, and the large scale of millions of exit nodes. All these factors allow RESIP users to effectively masquerade their traffic flows as ones from authentic residential users, which leads to the increasing adoption of RESIP services, especially in malicious online activities. However, regarding the (malicious) usage of RESIPs (i.e., what traffic is relayed by RESIPs), current understanding turns out to be insufficient. Particularly, previous works on RESIP traffic studied only the maliciousness of web traffic destinations and the suspicious patterns of visiting popular websites. Also, a general methodology is missing regarding capturing large-scale RESIP traffic and analyzing RESIP traffic for security risks. Furthermore, considering many RESIP nodes are found to be located in corporate networks and are deployed without proper authorization from device owners or network administrators, it is becoming increasingly necessary to detect and block RESIP traffic flows, which unfortunately is impeded by the scarcity of realistic RESIP traffic datasets and effective detection methodologies.   To fill in these gaps, multiple novel tools have been designed and implemented in this study, which include a general framework to deploy RESIP nodes and collect RESIP traffic in a distributed manner, a RESIP traffic analyzer to efficiently process RESIP traffic logs and surface out suspicious traffic flows, and multiple machine learning based RESIP traffic classifiers to timely and accurately detect whether a given traffic flow is RESIP traffic or not.","sentences":["Emerging in recent years, residential proxies (RESIPs) feature multiple unique characteristics when compared with traditional network proxies (e.g., commercial VPNs), particularly, the deployment in residential networks rather than data center networks, the worldwide distribution in tens of thousands of cities and ISPs, and the large scale of millions of exit nodes.","All these factors allow RESIP users to effectively masquerade their traffic flows as ones from authentic residential users, which leads to the increasing adoption of RESIP services, especially in malicious online activities.","However, regarding the (malicious) usage of RESIPs (i.e., what traffic is relayed by RESIPs), current understanding turns out to be insufficient.","Particularly, previous works on RESIP traffic studied only the maliciousness of web traffic destinations and the suspicious patterns of visiting popular websites.","Also, a general methodology is missing regarding capturing large-scale RESIP traffic and analyzing RESIP traffic for security risks.","Furthermore, considering many RESIP nodes are found to be located in corporate networks and are deployed without proper authorization from device owners or network administrators, it is becoming increasingly necessary to detect and block RESIP traffic flows, which unfortunately is impeded by the scarcity of realistic RESIP traffic datasets and effective detection methodologies.   ","To fill in these gaps, multiple novel tools have been designed and implemented in this study, which include a general framework to deploy RESIP nodes and collect RESIP traffic in a distributed manner, a RESIP traffic analyzer to efficiently process RESIP traffic logs and surface out suspicious traffic flows, and multiple machine learning based RESIP traffic classifiers to timely and accurately detect whether a given traffic flow is RESIP traffic or not."],"url":"http://arxiv.org/abs/2404.10610v1","category":"cs.CR"}
{"created":"2024-04-16 14:22:58","title":"Hardware-aware training of models with synaptic delays for digital event-driven neuromorphic processors","abstract":"Configurable synaptic delays are a basic feature in many neuromorphic neural network hardware accelerators. However, they have been rarely used in model implementations, despite their promising impact on performance and efficiency in tasks that exhibit complex (temporal) dynamics, as it has been unclear how to optimize them. In this work, we propose a framework to train and deploy, in digital neuromorphic hardware, highly performing spiking neural network models (SNNs) where apart from the synaptic weights, the per-synapse delays are also co-optimized. Leveraging spike-based back-propagation-through-time, the training accounts for both platform constraints, such as synaptic weight precision and the total number of parameters per core, as a function of the network size. In addition, a delay pruning technique is used to reduce memory footprint with a low cost in performance. We evaluate trained models in two neuromorphic digital hardware platforms: Intel Loihi and Imec Seneca. Loihi offers synaptic delay support using the so-called Ring-Buffer hardware structure. Seneca does not provide native hardware support for synaptic delays. A second contribution of this paper is therefore a novel area- and memory-efficient hardware structure for acceleration of synaptic delays, which we have integrated in Seneca. The evaluated benchmark involves several models for solving the SHD (Spiking Heidelberg Digits) classification task, where minimal accuracy degradation during the transition from software to hardware is demonstrated. To our knowledge, this is the first work showcasing how to train and deploy hardware-aware models parameterized with synaptic delays, on multicore neuromorphic hardware accelerators.","sentences":["Configurable synaptic delays are a basic feature in many neuromorphic neural network hardware accelerators.","However, they have been rarely used in model implementations, despite their promising impact on performance and efficiency in tasks that exhibit complex (temporal) dynamics, as it has been unclear how to optimize them.","In this work, we propose a framework to train and deploy, in digital neuromorphic hardware, highly performing spiking neural network models (SNNs) where apart from the synaptic weights, the per-synapse delays are also co-optimized.","Leveraging spike-based back-propagation-through-time, the training accounts for both platform constraints, such as synaptic weight precision and the total number of parameters per core, as a function of the network size.","In addition, a delay pruning technique is used to reduce memory footprint with a low cost in performance.","We evaluate trained models in two neuromorphic digital hardware platforms: Intel Loihi and Imec Seneca.","Loihi offers synaptic delay support using the so-called Ring-Buffer hardware structure.","Seneca does not provide native hardware support for synaptic delays.","A second contribution of this paper is therefore a novel area- and memory-efficient hardware structure for acceleration of synaptic delays, which we have integrated in Seneca.","The evaluated benchmark involves several models for solving the SHD (Spiking Heidelberg Digits) classification task, where minimal accuracy degradation during the transition from software to hardware is demonstrated.","To our knowledge, this is the first work showcasing how to train and deploy hardware-aware models parameterized with synaptic delays, on multicore neuromorphic hardware accelerators."],"url":"http://arxiv.org/abs/2404.10597v1","category":"cs.NE"}
{"created":"2024-04-16 14:20:55","title":"Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases","abstract":"Large Vision-Language Models (LVLMs), due to the remarkable visual reasoning ability to understand images and videos, have received widespread attention in the autonomous driving domain, which significantly advances the development of interpretable end-to-end autonomous driving. However, current evaluations of LVLMs primarily focus on the multi-faceted capabilities in common scenarios, lacking quantifiable and automated assessment in autonomous driving contexts, let alone severe road corner cases that even the state-of-the-art autonomous driving perception systems struggle to handle. In this paper, we propose CODA-LM, a novel vision-language benchmark for self-driving, which provides the first automatic and quantitative evaluation of LVLMs for interpretable autonomous driving including general perception, regional perception, and driving suggestions. CODA-LM utilizes the texts to describe the road images, exploiting powerful text-only large language models (LLMs) without image inputs to assess the capabilities of LVLMs in autonomous driving scenarios, which reveals stronger alignment with human preferences than LVLM judges. Experiments demonstrate that even the closed-sourced commercial LVLMs like GPT-4V cannot deal with road corner cases well, suggesting that we are still far from a strong LVLM-powered intelligent driving agent, and we hope our CODA-LM can become the catalyst to promote future development.","sentences":["Large Vision-Language Models (LVLMs), due to the remarkable visual reasoning ability to understand images and videos, have received widespread attention in the autonomous driving domain, which significantly advances the development of interpretable end-to-end autonomous driving.","However, current evaluations of LVLMs primarily focus on the multi-faceted capabilities in common scenarios, lacking quantifiable and automated assessment in autonomous driving contexts, let alone severe road corner cases that even the state-of-the-art autonomous driving perception systems struggle to handle.","In this paper, we propose CODA-LM, a novel vision-language benchmark for self-driving, which provides the first automatic and quantitative evaluation of LVLMs for interpretable autonomous driving including general perception, regional perception, and driving suggestions.","CODA-LM utilizes the texts to describe the road images, exploiting powerful text-only large language models (LLMs) without image inputs to assess the capabilities of LVLMs in autonomous driving scenarios, which reveals stronger alignment with human preferences than LVLM judges.","Experiments demonstrate that even the closed-sourced commercial LVLMs like GPT-4V cannot deal with road corner cases well, suggesting that we are still far from a strong LVLM-powered intelligent driving agent, and we hope our CODA-LM can become the catalyst to promote future development."],"url":"http://arxiv.org/abs/2404.10595v1","category":"cs.CV"}
{"created":"2024-04-16 14:16:43","title":"A Longitudinal Study of Child Wellbeing Assessment via Online Interactions with a Social Robots","abstract":"Socially Assistive Robots are studied in different Child-Robot Interaction settings. However, logistical constraints limit accessibility, particularly affecting timely support for mental wellbeing. In this work, we have investigated whether online interactions with a robot can be used for the assessment of mental wellbeing in children. The children (N=40, 20 girls and 20 boys; 8-13 years) interacted with the Nao robot (30-45 mins) over three sessions, at least a week apart. Audio-visual recordings were collected throughout the sessions that concluded with the children answering user perception questionnaires pertaining to their anxiety towards the robot, and the robot's abilities. We divided the participants into three wellbeing clusters (low, med and high tertiles) using their responses to the Short Moods and Feelings Questionnaire (SMFQ) and further analysed how their wellbeing and their perceptions of the robot changed over the wellbeing tertiles, across sessions and across participants' gender. Our primary findings suggest that (I) online mediated-interactions with robots can be effective in assessing children's mental wellbeing over time, and (II) children's overall perception of the robot either improved or remained consistent across time. Supplementary exploratory analyses have also revealed that gender affected the children's wellbeing assessments as well as their perceptions of the robot.","sentences":["Socially Assistive Robots are studied in different Child-Robot Interaction settings.","However, logistical constraints limit accessibility, particularly affecting timely support for mental wellbeing.","In this work, we have investigated whether online interactions with a robot can be used for the assessment of mental wellbeing in children.","The children (N=40, 20 girls and 20 boys; 8-13 years) interacted with the Nao robot (30-45 mins) over three sessions, at least a week apart.","Audio-visual recordings were collected throughout the sessions that concluded with the children answering user perception questionnaires pertaining to their anxiety towards the robot, and the robot's abilities.","We divided the participants into three wellbeing clusters (low, med and high tertiles) using their responses to the Short Moods and Feelings Questionnaire (SMFQ) and further analysed how their wellbeing and their perceptions of the robot changed over the wellbeing tertiles, across sessions and across participants' gender.","Our primary findings suggest that (I) online mediated-interactions with robots can be effective in assessing children's mental wellbeing over time, and (II) children's overall perception of the robot either improved or remained consistent across time.","Supplementary exploratory analyses have also revealed that gender affected the children's wellbeing assessments as well as their perceptions of the robot."],"url":"http://arxiv.org/abs/2404.10593v1","category":"cs.HC"}
{"created":"2024-04-16 14:14:34","title":"Learning Symbolic Task Representation from a Human-Led Demonstration: A Memory to Store, Retrieve, Consolidate, and Forget Experiences","abstract":"We present a symbolic learning framework inspired by cognitive-like memory functionalities (i.e., storing, retrieving, consolidating and forgetting) to generate task representations to support high-level task planning and knowledge bootstrapping. We address a scenario involving a non-expert human, who performs a single task demonstration, and a robot, which online learns structured knowledge to re-execute the task based on experiences, i.e., observations. We consider a one-shot learning process based on non-annotated data to store an intelligible representation of the task, which can be refined through interaction, e.g., via verbal or visual communication. Our general-purpose framework relies on fuzzy Description Logic, which has been used to extend the previously developed Scene Identification and Tagging algorithm. In this paper, we exploit such an algorithm to implement cognitive-like memory functionalities employing scores that rank memorised observations over time based on simple heuristics. Our main contribution is the formalisation of a framework that can be used to systematically investigate different heuristics for bootstrapping hierarchical knowledge representations based on robot observations. Through an illustrative assembly task scenario, the paper presents the performance of our framework to discuss its benefits and limitations.","sentences":["We present a symbolic learning framework inspired by cognitive-like memory functionalities (i.e., storing, retrieving, consolidating and forgetting) to generate task representations to support high-level task planning and knowledge bootstrapping.","We address a scenario involving a non-expert human, who performs a single task demonstration, and a robot, which online learns structured knowledge to re-execute the task based on experiences, i.e., observations.","We consider a one-shot learning process based on non-annotated data to store an intelligible representation of the task, which can be refined through interaction, e.g., via verbal or visual communication.","Our general-purpose framework relies on fuzzy Description Logic, which has been used to extend the previously developed Scene Identification and Tagging algorithm.","In this paper, we exploit such an algorithm to implement cognitive-like memory functionalities employing scores that rank memorised observations over time based on simple heuristics.","Our main contribution is the formalisation of a framework that can be used to systematically investigate different heuristics for bootstrapping hierarchical knowledge representations based on robot observations.","Through an illustrative assembly task scenario, the paper presents the performance of our framework to discuss its benefits and limitations."],"url":"http://arxiv.org/abs/2404.10591v1","category":"cs.RO"}
{"created":"2024-04-16 14:14:20","title":"Ray-Tracing Calibration from Channel Sounding Measurements in a Millimeter-Wave Industrial Scenario","abstract":"New-generation communication and sensing systems are gaining strong interest in the context of Industry 4.0 e.g., related to mapping techniques, environmental sensing, automation or hyper-vision. The radio propagation in confined, cluttered and heavily metalized factory environments is a critical challenge; thus an evaluation by accurate propagation channel models is necessary. Site-specific channel emulation can be obtained from Ray-tracing (RT); but RT validation for factory environments is still an on-going work. For this purpose, a measurement campaign was performed in a machine room with many metallic objects and machines, using a mmWave channel sounder. Wideband channel responses were collected and compared to RT simulations. The RT prediction tool was calibrated to minimize the error observed on some large scale statistics, thus reaching a very good agreement between the simulation and the measurement. Average error in received power, delay spread and azimuth spread is below 1.5 dB, 5 ns and 2{\\deg} respectively.","sentences":["New-generation communication and sensing systems are gaining strong interest in the context of Industry 4.0 e.g., related to mapping techniques, environmental sensing, automation or hyper-vision.","The radio propagation in confined, cluttered and heavily metalized factory environments is a critical challenge; thus an evaluation by accurate propagation channel models is necessary.","Site-specific channel emulation can be obtained from Ray-tracing (RT); but RT validation for factory environments is still an on-going work.","For this purpose, a measurement campaign was performed in a machine room with many metallic objects and machines, using a mmWave channel sounder.","Wideband channel responses were collected and compared to RT simulations.","The RT prediction tool was calibrated to minimize the error observed on some large scale statistics, thus reaching a very good agreement between the simulation and the measurement.","Average error in received power, delay spread and azimuth spread is below 1.5 dB, 5 ns and 2{\\deg} respectively."],"url":"http://arxiv.org/abs/2404.10590v1","category":"eess.SP"}
{"created":"2024-04-16 14:04:46","title":"The application of Augmented Reality (AR) in Remote Work and Education","abstract":"With the rapid advancement of technology, Augmented Reality (AR) technology, known for its ability to deeply integrate virtual information with the real world, is gradually transforming traditional work modes and teaching methods. Particularly in the realms of remote work and online education, AR technology demonstrates a broad spectrum of application prospects. This paper delves into the application potential and actual effects of AR technology in remote work and education. Through a systematic literature review, this study outlines the key features, advantages, and challenges of AR technology. Based on theoretical analysis, it discusses the scientific basis and technical support that AR technology provides for enhancing remote work efficiency and promoting innovation in educational teaching models. Additionally, by designing an empirical research plan and analyzing experimental data, this article reveals the specific performance and influencing factors of AR technology in practical applications. Finally, based on the results of the experiments, this research summarizes the application value of AR technology in remote work and education, looks forward to its future development trends, and proposes forward-looking research directions and strategic suggestions, offering empirical foundation and theoretical guidance for further promoting the in-depth application of AR technology in related fields.","sentences":["With the rapid advancement of technology, Augmented Reality (AR) technology, known for its ability to deeply integrate virtual information with the real world, is gradually transforming traditional work modes and teaching methods.","Particularly in the realms of remote work and online education, AR technology demonstrates a broad spectrum of application prospects.","This paper delves into the application potential and actual effects of AR technology in remote work and education.","Through a systematic literature review, this study outlines the key features, advantages, and challenges of AR technology.","Based on theoretical analysis, it discusses the scientific basis and technical support that AR technology provides for enhancing remote work efficiency and promoting innovation in educational teaching models.","Additionally, by designing an empirical research plan and analyzing experimental data, this article reveals the specific performance and influencing factors of AR technology in practical applications.","Finally, based on the results of the experiments, this research summarizes the application value of AR technology in remote work and education, looks forward to its future development trends, and proposes forward-looking research directions and strategic suggestions, offering empirical foundation and theoretical guidance for further promoting the in-depth application of AR technology in related fields."],"url":"http://arxiv.org/abs/2404.10579v1","category":"cs.AI"}
{"created":"2024-04-16 13:53:58","title":"EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence","abstract":"A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data. These negative samples often follow a softmax distribution which are dynamically updated during the training process. However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function. In this paper, we propose an Efficient Markov Chain Monte Carlo negative sampling method for Contrastive learning (EMC$^2$). We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization. We prove that EMC$^2$ finds an $\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost. Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms. We report the results for pre-training image encoders on STL-10 and Imagenet-100.","sentences":["A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data.","These negative samples often follow a softmax distribution which are dynamically updated during the training process.","However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function.","In this paper, we propose an Efficient Markov Chain Monte Carlo negative sampling method for Contrastive learning (EMC$^2$).","We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization.","We prove that EMC$^2$ finds an $\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations.","Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost.","Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms.","We report the results for pre-training image encoders on STL-10 and Imagenet-100."],"url":"http://arxiv.org/abs/2404.10575v1","category":"cs.LG"}
{"created":"2024-04-16 13:52:00","title":"Uncertainty-guided Open-Set Source-Free Unsupervised Domain Adaptation with Target-private Class Segregation","abstract":"Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target but usually requires simultaneous access to both source and target data. Moreover, UDA approaches commonly assume that source and target domains share the same labels space. Yet, these two assumptions are hardly satisfied in real-world scenarios. This paper considers the more challenging Source-Free Open-set Domain Adaptation (SF-OSDA) setting, where both assumptions are dropped. We propose a novel approach for SF-OSDA that exploits the granularity of target-private categories by segregating their samples into multiple unknown classes. Starting from an initial clustering-based assignment, our method progressively improves the segregation of target-private samples by refining their pseudo-labels with the guide of an uncertainty-based sample selection module. Additionally, we propose a novel contrastive loss, named NL-InfoNCELoss, that, integrating negative learning into self-supervised contrastive learning, enhances the model robustness to noisy pseudo-labels. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed method over existing approaches, establishing new state-of-the-art performance. Notably, additional analyses show that our method is able to learn the underlying semantics of novel classes, opening the possibility to perform novel class discovery.","sentences":["Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target but usually requires simultaneous access to both source and target data.","Moreover, UDA approaches commonly assume that source and target domains share the same labels space.","Yet, these two assumptions are hardly satisfied in real-world scenarios.","This paper considers the more challenging Source-Free Open-set Domain Adaptation (SF-OSDA) setting, where both assumptions are dropped.","We propose a novel approach for SF-OSDA that exploits the granularity of target-private categories by segregating their samples into multiple unknown classes.","Starting from an initial clustering-based assignment, our method progressively improves the segregation of target-private samples by refining their pseudo-labels with the guide of an uncertainty-based sample selection module.","Additionally, we propose a novel contrastive loss, named NL-InfoNCELoss, that, integrating negative learning into self-supervised contrastive learning, enhances the model robustness to noisy pseudo-labels.","Extensive experiments on benchmark datasets demonstrate the superiority of the proposed method over existing approaches, establishing new state-of-the-art performance.","Notably, additional analyses show that our method is able to learn the underlying semantics of novel classes, opening the possibility to perform novel class discovery."],"url":"http://arxiv.org/abs/2404.10574v1","category":"cs.CV"}
{"created":"2024-04-16 13:51:43","title":"AAVDiff: Experimental Validation of Enhanced Viability and Diversity in Recombinant Adeno-Associated Virus (AAV) Capsids through Diffusion Generation","abstract":"Recombinant adeno-associated virus (rAAV) vectors have revolutionized gene therapy, but their broad tropism and suboptimal transduction efficiency limit their clinical applications. To overcome these limitations, researchers have focused on designing and screening capsid libraries to identify improved vectors. However, the large sequence space and limited resources present challenges in identifying viable capsid variants. In this study, we propose an end-to-end diffusion model to generate capsid sequences with enhanced viability. Using publicly available AAV2 data, we generated 38,000 diverse AAV2 viral protein (VP) sequences, and evaluated 8,000 for viral selection. The results attested the superiority of our model compared to traditional methods. Additionally, in the absence of AAV9 capsid data, apart from one wild-type sequence, we used the same model to directly generate a number of viable sequences with up to 9 mutations. we transferred the remaining 30,000 samples to the AAV9 domain. Furthermore, we conducted mutagenesis on AAV9 VP hypervariable regions VI and V, contributing to the continuous improvement of the AAV9 VP sequence. This research represents a significant advancement in the design and functional validation of rAAV vectors, offering innovative solutions to enhance specificity and transduction efficiency in gene therapy applications.","sentences":["Recombinant adeno-associated virus (rAAV) vectors have revolutionized gene therapy, but their broad tropism and suboptimal transduction efficiency limit their clinical applications.","To overcome these limitations, researchers have focused on designing and screening capsid libraries to identify improved vectors.","However, the large sequence space and limited resources present challenges in identifying viable capsid variants.","In this study, we propose an end-to-end diffusion model to generate capsid sequences with enhanced viability.","Using publicly available AAV2 data, we generated 38,000 diverse AAV2 viral protein (VP) sequences, and evaluated 8,000 for viral selection.","The results attested the superiority of our model compared to traditional methods.","Additionally, in the absence of AAV9 capsid data, apart from one wild-type sequence, we used the same model to directly generate a number of viable sequences with up to 9 mutations.","we transferred the remaining 30,000 samples to the AAV9 domain.","Furthermore, we conducted mutagenesis on AAV9 VP hypervariable regions VI and V, contributing to the continuous improvement of the AAV9 VP sequence.","This research represents a significant advancement in the design and functional validation of rAAV vectors, offering innovative solutions to enhance specificity and transduction efficiency in gene therapy applications."],"url":"http://arxiv.org/abs/2404.10573v2","category":"cs.AI"}
{"created":"2024-04-16 13:40:22","title":"Exploring Homological Properties of Independent Complexes of Kneser Graphs","abstract":"We discuss the topological properties of the independence complex of Kneser graphs, Ind(KG$(n, k))$, with $n\\geq 3$ and $k\\geq 1$. By identifying one kind of maximal simplices through projective planes, we obtain homology generators for the $6$-dimensional homology of the complex Ind(KG$(3, k))$. Using cross-polytopal generators, we provide lower bounds for the rank of $p$-dimensional homology of the complex Ind(KG$(n, k))$ where $p=1/2\\cdot {2n+k\\choose 2n}$.   Denote $\\mathcal{F}_n^{[m]}$ to be the collection of $n$-subsets of $[m]$ equipped with the symmetric difference metric. We prove that if $\\ell$ is the minimal integer with the $q$th dimensional reduced homology $\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}^{[\\ell]}_n; 2(n-1)))$ being non-trivial, then $$\\text{rank} (\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}_n^{[m]}; 2(n-1)))\\geq \\sum_{i=\\ell}^m{i-2\\choose \\ell-2}\\cdot \\text{rank} (\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}_n^{[\\ell]}; 2(n-1))). $$ Since the independence complex Ind(KG$(n, k))$ and the Vietoris-Rips complex $\\mathcal{VR}(\\mathcal{F}^{[2n+k]}_n; 2(n-1))$ are the same, we obtain a homology propagation result in the setting of independence complexes of Kneser graphs. Connectivity of these complexes is also discussed in this paper.","sentences":["We discuss the topological properties of the independence complex of Kneser graphs, Ind(KG$(n, k))$, with $n\\geq 3$ and $k\\geq 1$. By identifying one kind of maximal simplices through projective planes, we obtain homology generators for the $6$-dimensional homology of the complex Ind(KG$(3, k))$. Using cross-polytopal generators, we provide lower bounds for the rank of $p$-dimensional homology of the complex Ind(KG$(n, k))$ where $p=1/2\\cdot {2n+k\\choose 2n}$.   Denote $\\mathcal{F}_n^{[m]}$ to be the collection of $n$-subsets of $[m]$ equipped with the symmetric difference metric.","We prove that if $\\ell$ is the minimal integer with the $q$th dimensional reduced homology $\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}^{[\\ell]}_n; 2(n-1)))$ being non-trivial, then $$\\text{rank} (\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}_n^{[m]}; 2(n-1)))\\geq \\sum_{i=\\ell}^m{i-2\\choose \\ell-2}\\cdot \\text{rank} (\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}_n^{[\\ell]}; 2(n-1))).","$$ Since the independence complex Ind(KG$(n, k))$ and the Vietoris-Rips complex $\\mathcal{VR}(\\mathcal{F}^{[2n+k]}_n; 2(n-1))$ are the same, we obtain a homology propagation result in the setting of independence complexes of Kneser graphs.","Connectivity of these complexes is also discussed in this paper."],"url":"http://arxiv.org/abs/2404.10566v1","category":"math.CO"}
{"created":"2024-04-16 13:29:11","title":"Generative AI for Advanced UAV Networking","abstract":"With the impressive achievements of chatGPT and Sora, generative artificial intelligence (GAI) has received increasing attention. Not limited to the field of content generation, GAI is also widely used to solve the problems in wireless communication scenarios due to its powerful learning and generalization capabilities. Therefore, we discuss key applications of GAI in improving unmanned aerial vehicle (UAV) communication and networking performance in this article. Specifically, we first review the key technologies of GAI and the important roles of UAV networking. Then, we show how GAI can improve the communication, networking, and security performances of UAV systems. Subsequently, we propose a novel framework of GAI for advanced UAV networking, and then present a case study of UAV-enabled spectrum map estimation and transmission rate optimization based on the proposed framework to verify the effectiveness of GAI-enabled UAV systems. Finally, we discuss some important open directions.","sentences":["With the impressive achievements of chatGPT and Sora, generative artificial intelligence (GAI) has received increasing attention.","Not limited to the field of content generation, GAI is also widely used to solve the problems in wireless communication scenarios due to its powerful learning and generalization capabilities.","Therefore, we discuss key applications of GAI in improving unmanned aerial vehicle (UAV) communication and networking performance in this article.","Specifically, we first review the key technologies of GAI and the important roles of UAV networking.","Then, we show how GAI can improve the communication, networking, and security performances of UAV systems.","Subsequently, we propose a novel framework of GAI for advanced UAV networking, and then present a case study of UAV-enabled spectrum map estimation and transmission rate optimization based on the proposed framework to verify the effectiveness of GAI-enabled UAV systems.","Finally, we discuss some important open directions."],"url":"http://arxiv.org/abs/2404.10556v1","category":"cs.NI"}
{"created":"2024-04-16 13:22:54","title":"Unveiling the Misuse Potential of Base Large Language Models via In-Context Learning","abstract":"The open-sourcing of large language models (LLMs) accelerates application development, innovation, and scientific progress. This includes both base models, which are pre-trained on extensive datasets without alignment, and aligned models, deliberately designed to align with ethical standards and human values. Contrary to the prevalent assumption that the inherent instruction-following limitations of base LLMs serve as a safeguard against misuse, our investigation exposes a critical oversight in this belief. By deploying carefully designed demonstrations, our research demonstrates that base LLMs could effectively interpret and execute malicious instructions. To systematically assess these risks, we introduce a novel set of risk evaluation metrics. Empirical results reveal that the outputs from base LLMs can exhibit risk levels on par with those of models fine-tuned for malicious purposes. This vulnerability, requiring neither specialized knowledge nor training, can be manipulated by almost anyone, highlighting the substantial risk and the critical need for immediate attention to the base LLMs' security protocols.","sentences":["The open-sourcing of large language models (LLMs) accelerates application development, innovation, and scientific progress.","This includes both base models, which are pre-trained on extensive datasets without alignment, and aligned models, deliberately designed to align with ethical standards and human values.","Contrary to the prevalent assumption that the inherent instruction-following limitations of base LLMs serve as a safeguard against misuse, our investigation exposes a critical oversight in this belief.","By deploying carefully designed demonstrations, our research demonstrates that base LLMs could effectively interpret and execute malicious instructions.","To systematically assess these risks, we introduce a novel set of risk evaluation metrics.","Empirical results reveal that the outputs from base LLMs can exhibit risk levels on par with those of models fine-tuned for malicious purposes.","This vulnerability, requiring neither specialized knowledge nor training, can be manipulated by almost anyone, highlighting the substantial risk and the critical need for immediate attention to the base LLMs' security protocols."],"url":"http://arxiv.org/abs/2404.10552v1","category":"cs.CL"}
{"created":"2024-04-16 13:19:57","title":"The Evolution of Learning: Assessing the Transformative Impact of Generative AI on Higher Education","abstract":"Generative Artificial Intelligence (GAI) models such as ChatGPT have experienced a surge in popularity, attracting 100 million active users in 2 months and generating an estimated 10 million daily queries. Despite this remarkable adoption, there remains a limited understanding to which extent this innovative technology influences higher education. This research paper investigates the impact of GAI on university students and Higher Education Institutions (HEIs). The study adopts a mixed-methods approach, combining a comprehensive survey with scenario analysis to explore potential benefits, drawbacks, and transformative changes the new technology brings. Using an online survey with 130 participants we assessed students' perspectives and attitudes concerning present ChatGPT usage in academics. Results show that students use the current technology for tasks like assignment writing and exam preparation and believe it to be a effective help in achieving academic goals. The scenario analysis afterwards projected potential future scenarios, providing valuable insights into the possibilities and challenges associated with incorporating GAI into higher education. The main motivation is to gain a tangible and precise understanding of the potential consequences for HEIs and to provide guidance responding to the evolving learning environment. The findings indicate that irresponsible and excessive use of the technology could result in significant challenges. Hence, HEIs must develop stringent policies, reevaluate learning objectives, upskill their lecturers, adjust the curriculum and reconsider examination approaches.","sentences":["Generative Artificial Intelligence (GAI) models such as ChatGPT have experienced a surge in popularity, attracting 100 million active users in 2 months and generating an estimated 10 million daily queries.","Despite this remarkable adoption, there remains a limited understanding to which extent this innovative technology influences higher education.","This research paper investigates the impact of GAI on university students and Higher Education Institutions (HEIs).","The study adopts a mixed-methods approach, combining a comprehensive survey with scenario analysis to explore potential benefits, drawbacks, and transformative changes the new technology brings.","Using an online survey with 130 participants we assessed students' perspectives and attitudes concerning present ChatGPT usage in academics.","Results show that students use the current technology for tasks like assignment writing and exam preparation and believe it to be a effective help in achieving academic goals.","The scenario analysis afterwards projected potential future scenarios, providing valuable insights into the possibilities and challenges associated with incorporating GAI into higher education.","The main motivation is to gain a tangible and precise understanding of the potential consequences for HEIs and to provide guidance responding to the evolving learning environment.","The findings indicate that irresponsible and excessive use of the technology could result in significant challenges.","Hence, HEIs must develop stringent policies, reevaluate learning objectives, upskill their lecturers, adjust the curriculum and reconsider examination approaches."],"url":"http://arxiv.org/abs/2404.10551v1","category":"cs.AI"}
{"created":"2024-04-16 13:05:23","title":"Benchmarking Machine Learning Applications on Heterogeneous Architecture using Reframe","abstract":"With the rapid increase in machine learning workloads performed on HPC systems, it is beneficial to regularly perform machine learning specific benchmarks to monitor performance and identify issues. Furthermore, as part of the Edinburgh International Data Facility, EPCC currently hosts a wide range of machine learning accelerators including Nvidia GPUs, the Graphcore Bow Pod64 and Cerebras CS-2, which are managed via Kubernetes and Slurm. We extended the Reframe framework to support the Kubernetes scheduler backend, and utilise Reframe to perform machine learning benchmarks, and we discuss the preliminary results collected and challenges involved in integrating Reframe across multiple platforms and architectures.","sentences":["With the rapid increase in machine learning workloads performed on HPC systems, it is beneficial to regularly perform machine learning specific benchmarks to monitor performance and identify issues.","Furthermore, as part of the Edinburgh International Data Facility, EPCC currently hosts a wide range of machine learning accelerators including Nvidia GPUs, the Graphcore Bow Pod64 and Cerebras CS-2, which are managed via Kubernetes and Slurm.","We extended the Reframe framework to support the Kubernetes scheduler backend, and utilise Reframe to perform machine learning benchmarks, and we discuss the preliminary results collected and challenges involved in integrating Reframe across multiple platforms and architectures."],"url":"http://arxiv.org/abs/2404.10536v1","category":"cs.DC"}
{"created":"2024-04-16 12:55:57","title":"AllTheDocks road safety dataset: A cyclist's perspective and experience","abstract":"Active travel is an essential component in intelligent transportation systems. Cycling, as a form of active travel, shares the road space with motorised traffic which often affects the cyclists' safety and comfort and therefore peoples' propensity to uptake cycling instead of driving. This paper presents a unique dataset, collected by cyclists across London, that includes video footage, accelerometer, GPS, and gyroscope data. The dataset is then labelled by an independent group of London cyclists to rank the safety level of each frame and to identify objects in the cyclist's field of vision that might affect their experience. Furthermore, in this dataset, the quality of the road is measured by the international roughness index of the surface, which indicates the comfort of cycling on the road. The dataset will be made available for open access in the hope of motivating more research in this area to underpin the requirements for cyclists' safety and comfort and encourage more people to replace vehicle travel with cycling.","sentences":["Active travel is an essential component in intelligent transportation systems.","Cycling, as a form of active travel, shares the road space with motorised traffic which often affects the cyclists' safety and comfort and therefore peoples' propensity to uptake cycling instead of driving.","This paper presents a unique dataset, collected by cyclists across London, that includes video footage, accelerometer, GPS, and gyroscope data.","The dataset is then labelled by an independent group of London cyclists to rank the safety level of each frame and to identify objects in the cyclist's field of vision that might affect their experience.","Furthermore, in this dataset, the quality of the road is measured by the international roughness index of the surface, which indicates the comfort of cycling on the road.","The dataset will be made available for open access in the hope of motivating more research in this area to underpin the requirements for cyclists' safety and comfort and encourage more people to replace vehicle travel with cycling."],"url":"http://arxiv.org/abs/2404.10528v1","category":"cs.MM"}
{"created":"2024-04-16 12:37:10","title":"CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity","abstract":"State-of-the-art performance in QA tasks is currently achieved by systems employing Large Language Models (LLMs), however these models tend to hallucinate information in their responses. One approach focuses on enhancing the generation process by incorporating attribution from the given input to the output. However, the challenge of identifying appropriate attributions and verifying their accuracy against a source is a complex task that requires significant improvements in assessing such systems. We introduce an attribution-oriented Chain-of-Thought reasoning method to enhance the accuracy of attributions. This approach focuses the reasoning process on generating an attribution-centric output. Evaluations on two context-enhanced question-answering datasets using GPT-4 demonstrate improved accuracy and correctness of attributions. In addition, the combination of our method with finetuning enhances the response and attribution accuracy of two smaller LLMs, showing their potential to outperform GPT-4 in some cases.","sentences":["State-of-the-art performance in QA tasks is currently achieved by systems employing Large Language Models (LLMs), however these models tend to hallucinate information in their responses.","One approach focuses on enhancing the generation process by incorporating attribution from the given input to the output.","However, the challenge of identifying appropriate attributions and verifying their accuracy against a source is a complex task that requires significant improvements in assessing such systems.","We introduce an attribution-oriented Chain-of-Thought reasoning method to enhance the accuracy of attributions.","This approach focuses the reasoning process on generating an attribution-centric output.","Evaluations on two context-enhanced question-answering datasets using GPT-4 demonstrate improved accuracy and correctness of attributions.","In addition, the combination of our method with finetuning enhances the response and attribution accuracy of two smaller LLMs, showing their potential to outperform GPT-4 in some cases."],"url":"http://arxiv.org/abs/2404.10513v1","category":"cs.CL"}
{"created":"2024-04-16 12:33:44","title":"Four-hour thunderstorm nowcasting using deep diffusion models of satellite","abstract":"Convection (thunderstorm) develops rapidly within hours and is highly destructive, posing a significant challenge for nowcasting and resulting in substantial losses to nature and society. After the emergence of artificial intelligence (AI)-based methods, convection nowcasting has experienced rapid advancements, with its performance surpassing that of physics-based numerical weather prediction and other conventional approaches. However, the lead time and coverage of it still leave much to be desired and hardly meet the needs of disaster emergency response. Here, we propose a deep diffusion model of satellite (DDMS) to establish an AI-based convection nowcasting system. On one hand, it employs diffusion processes to effectively simulate complicated spatiotemporal evolution patterns of convective clouds, significantly improving the forecast lead time. On the other hand, it utilizes geostationary satellite brightness temperature data, thereby achieving planetary-scale forecast coverage. During long-term tests and objective validation based on the FengYun-4A satellite, our system achieves, for the first time, effective convection nowcasting up to 4 hours, with broad coverage (about 20,000,000 km2), remarkable accuracy, and high resolution (15 minutes; 4 km). Its performance reaches a new height in convection nowcasting compared to the existing models. In terms of application, our system operates efficiently (forecasting 4 hours of convection in 8 minutes), and is highly transferable with the potential to collaborate with multiple satellites for global convection nowcasting. Furthermore, our results highlight the remarkable capabilities of diffusion models in convective clouds forecasting, as well as the significant value of geostationary satellite data when empowered by AI technologies.","sentences":["Convection (thunderstorm) develops rapidly within hours and is highly destructive, posing a significant challenge for nowcasting and resulting in substantial losses to nature and society.","After the emergence of artificial intelligence (AI)-based methods, convection nowcasting has experienced rapid advancements, with its performance surpassing that of physics-based numerical weather prediction and other conventional approaches.","However, the lead time and coverage of it still leave much to be desired and hardly meet the needs of disaster emergency response.","Here, we propose a deep diffusion model of satellite (DDMS) to establish an AI-based convection nowcasting system.","On one hand, it employs diffusion processes to effectively simulate complicated spatiotemporal evolution patterns of convective clouds, significantly improving the forecast lead time.","On the other hand, it utilizes geostationary satellite brightness temperature data, thereby achieving planetary-scale forecast coverage.","During long-term tests and objective validation based on the FengYun-4A satellite, our system achieves, for the first time, effective convection nowcasting up to 4 hours, with broad coverage (about 20,000,000 km2), remarkable accuracy, and high resolution (15 minutes; 4 km).","Its performance reaches a new height in convection nowcasting compared to the existing models.","In terms of application, our system operates efficiently (forecasting 4 hours of convection in 8 minutes), and is highly transferable with the potential to collaborate with multiple satellites for global convection nowcasting.","Furthermore, our results highlight the remarkable capabilities of diffusion models in convective clouds forecasting, as well as the significant value of geostationary satellite data when empowered by AI technologies."],"url":"http://arxiv.org/abs/2404.10512v1","category":"cs.LG"}
{"created":"2024-04-16 12:27:54","title":"White Men Lead, Black Women Help: Uncovering Gender, Racial, and Intersectional Bias in Language Agency","abstract":"Social biases can manifest in language agency. For instance, White individuals and men are often described as \"agentic\" and achievement-oriented, whereas Black individuals and women are frequently described as \"communal\" and as assisting roles. This study establishes agency as an important aspect of studying social biases in both human-written and Large Language Model (LLM)-generated texts. To accurately measure \"language agency\" at sentence level, we propose a Language Agency Classification dataset to train reliable agency classifiers. We then use an agency classifier to reveal notable language agency biases in 6 datasets of human- or LLM-written texts, including biographies, professor reviews, and reference letters. While most prior NLP research on agency biases focused on single dimensions, we comprehensively explore language agency biases in gender, race, and intersectional identities. We observe that (1) language agency biases in human-written texts align with real-world social observations; (2) LLM-generated texts demonstrate remarkably higher levels of language agency bias than human-written texts; and (3) critical biases in language agency target people of minority groups--for instance, languages used to describe Black females exhibit the lowest level of agency across datasets. Our findings reveal intricate social biases in human- and LLM-written texts through the lens of language agency, warning against using LLM generations in social contexts without scrutiny.","sentences":["Social biases can manifest in language agency.","For instance, White individuals and men are often described as \"agentic\" and achievement-oriented, whereas Black individuals and women are frequently described as \"communal\" and as assisting roles.","This study establishes agency as an important aspect of studying social biases in both human-written and Large Language Model (LLM)-generated texts.","To accurately measure \"language agency\" at sentence level, we propose a Language Agency Classification dataset to train reliable agency classifiers.","We then use an agency classifier to reveal notable language agency biases in 6 datasets of human- or LLM-written texts, including biographies, professor reviews, and reference letters.","While most prior NLP research on agency biases focused on single dimensions, we comprehensively explore language agency biases in gender, race, and intersectional identities.","We observe that (1) language agency biases in human-written texts align with real-world social observations; (2) LLM-generated texts demonstrate remarkably higher levels of language agency bias than human-written texts; and (3) critical biases in language agency target people of minority groups--for instance, languages used to describe Black females exhibit the lowest level of agency across datasets.","Our findings reveal intricate social biases in human- and LLM-written texts through the lens of language agency, warning against using LLM generations in social contexts without scrutiny."],"url":"http://arxiv.org/abs/2404.10508v1","category":"cs.CL"}
{"created":"2024-04-16 12:23:59","title":"Data Collection of Real-Life Knowledge Work in Context: The RLKWiC Dataset","abstract":"Over the years, various approaches have been employed to enhance the productivity of knowledge workers, from addressing psychological well-being to the development of personal knowledge assistants. A significant challenge in this research area has been the absence of a comprehensive, publicly accessible dataset that mirrors real-world knowledge work. Although a handful of datasets exist, many are restricted in access or lack vital information dimensions, complicating meaningful comparison and benchmarking in the domain. This paper presents RLKWiC, a novel dataset of Real-Life Knowledge Work in Context, derived from monitoring the computer interactions of eight participants over a span of two months. As the first publicly available dataset offering a wealth of essential information dimensions (such as explicated contexts, textual contents, and semantics), RLKWiC seeks to address the research gap in the personal information management domain, providing valuable insights for modeling user behavior.","sentences":["Over the years, various approaches have been employed to enhance the productivity of knowledge workers, from addressing psychological well-being to the development of personal knowledge assistants.","A significant challenge in this research area has been the absence of a comprehensive, publicly accessible dataset that mirrors real-world knowledge work.","Although a handful of datasets exist, many are restricted in access or lack vital information dimensions, complicating meaningful comparison and benchmarking in the domain.","This paper presents RLKWiC, a novel dataset of Real-Life Knowledge Work in Context, derived from monitoring the computer interactions of eight participants over a span of two months.","As the first publicly available dataset offering a wealth of essential information dimensions (such as explicated contexts, textual contents, and semantics), RLKWiC seeks to address the research gap in the personal information management domain, providing valuable insights for modeling user behavior."],"url":"http://arxiv.org/abs/2404.10505v1","category":"cs.AI"}
{"created":"2024-04-17 17:59:53","title":"InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior","abstract":"3D Gaussians have recently emerged as an efficient representation for novel view synthesis. This work studies its editability with a particular focus on the inpainting task, which aims to supplement an incomplete set of 3D Gaussians with additional points for visually harmonious rendering. Compared to 2D inpainting, the crux of inpainting 3D Gaussians is to figure out the rendering-relevant properties of the introduced points, whose optimization largely benefits from their initial 3D positions. To this end, we propose to guide the point initialization with an image-conditioned depth completion model, which learns to directly restore the depth map based on the observed image. Such a design allows our model to fill in depth values at an aligned scale with the original depth, and also to harness strong generalizability from largescale diffusion prior. Thanks to the more accurate depth completion, our approach, dubbed InFusion, surpasses existing alternatives with sufficiently better fidelity and efficiency under various complex scenarios. We further demonstrate the effectiveness of InFusion with several practical applications, such as inpainting with user-specific texture or with novel object insertion.","sentences":["3D Gaussians have recently emerged as an efficient representation for novel view synthesis.","This work studies its editability with a particular focus on the inpainting task, which aims to supplement an incomplete set of 3D Gaussians with additional points for visually harmonious rendering.","Compared to 2D inpainting, the crux of inpainting 3D Gaussians is to figure out the rendering-relevant properties of the introduced points, whose optimization largely benefits from their initial 3D positions.","To this end, we propose to guide the point initialization with an image-conditioned depth completion model, which learns to directly restore the depth map based on the observed image.","Such a design allows our model to fill in depth values at an aligned scale with the original depth, and also to harness strong generalizability from largescale diffusion prior.","Thanks to the more accurate depth completion, our approach, dubbed InFusion, surpasses existing alternatives with sufficiently better fidelity and efficiency under various complex scenarios.","We further demonstrate the effectiveness of InFusion with several practical applications, such as inpainting with user-specific texture or with novel object insertion."],"url":"http://arxiv.org/abs/2404.11613v1","category":"cs.CV"}
{"created":"2024-04-17 17:58:41","title":"Photoproduction of QED bound states in future electron-ion colliders","abstract":"In this work we perform an exploratory study of the photoproduction of singlet QED bound states $(l^-l^+)_S$ in electron - ion collisions at the EicC, EIC and LHeC energies. The total cross - sections, event rates per year and rapidity distributions associated with the parapositronium, paramuonium and paratauonium production are estimated. Moreover, we consider the decay of these states in a two - photon system and implement kinematical cuts on the rapidities and energies of the photons in the final state. We demonstrate the paramuonium can, in principle, be observed for the first time in all these colliders and that the EIC is a potential collider to discover the paratauonium state.","sentences":["In this work we perform an exploratory study of the photoproduction of singlet QED bound states $(l^-l^+)_S$ in electron - ion collisions at the EicC, EIC and LHeC energies.","The total cross - sections, event rates per year and rapidity distributions associated with the parapositronium, paramuonium and paratauonium production are estimated.","Moreover, we consider the decay of these states in a two - photon system and implement kinematical cuts on the rapidities and energies of the photons in the final state.","We demonstrate the paramuonium can, in principle, be observed for the first time in all these colliders and that the EIC is a potential collider to discover the paratauonium state."],"url":"http://arxiv.org/abs/2404.11610v1","category":"hep-ph"}
{"created":"2024-04-17 17:55:54","title":"Localized dopant motion across the 2D Ising phase transition","abstract":"I investigate the motion of a single hole in 2D spin lattices with square and triangular geometries. While the spins have nearest neighbor Ising spin couplings $J$, the hole is allowed to move only in 1D along a single line in the 2D lattice with nearest neighbor hopping amplitude $t$. The non-equilibrium hole dynamics is initialized by suddenly removing a single spin from the thermal Ising spin lattice. I find that for any nonzero spin coupling and temperature, the hole is localized. This is an extension of the thermally induced localization phenomenon [arXiv:2310.11193] to the case, where there is a phase transition to a long-range ordered ferromagnetic phase. The dynamics depends only on the ratio of the temperature to the spin coupling, $k_BT / |J|$, and on the ratio of the spin coupling to the hopping $J/t$. I characterize these dependencies in great detail. In particular, I find universal behavior at high temperatures, common features for the square and triangular lattices across the Curie temperatures for ferromagnetic interactions, and highly distinct behaviors for the two geometries in the presence of antiferromagnetic interactions due geometric frustration in the triangular lattice.","sentences":["I investigate the motion of a single hole in 2D spin lattices with square and triangular geometries.","While the spins have nearest neighbor Ising spin couplings $J$, the hole is allowed to move only in 1D along a single line in the 2D lattice with nearest neighbor hopping amplitude $t$. The non-equilibrium hole dynamics is initialized by suddenly removing a single spin from the thermal Ising spin lattice.","I find that for any nonzero spin coupling and temperature, the hole is localized.","This is an extension of the thermally induced localization phenomenon [arXiv:2310.11193] to the case, where there is a phase transition to a long-range ordered ferromagnetic phase.","The dynamics depends only on the ratio of the temperature to the spin coupling, $k_BT / |J|$, and on the ratio of the spin coupling to the hopping $J/t$. I characterize these dependencies in great detail.","In particular, I find universal behavior at high temperatures, common features for the square and triangular lattices across the Curie temperatures for ferromagnetic interactions, and highly distinct behaviors for the two geometries in the presence of antiferromagnetic interactions due geometric frustration in the triangular lattice."],"url":"http://arxiv.org/abs/2404.11608v1","category":"cond-mat.str-el"}
{"created":"2024-04-17 17:55:27","title":"Private federated discovery of out-of-vocabulary words for Gboard","abstract":"The vocabulary of language models in Gboard, Google's keyboard application, plays a crucial role for improving user experience. One way to improve the vocabulary is to discover frequently typed out-of-vocabulary (OOV) words on user devices. This task requires strong privacy protection due to the sensitive nature of user input data. In this report, we present a private OOV discovery algorithm for Gboard, which builds on recent advances in private federated analytics. The system offers local differential privacy (LDP) guarantees for user contributed words. With anonymous aggregation, the final released words satisfy central differential privacy guarantees with $\\epsilon = 0.315, \\delta = 10^{-10}$ for OOV discovery in en-US (English in United States).","sentences":["The vocabulary of language models in Gboard, Google's keyboard application, plays a crucial role for improving user experience.","One way to improve the vocabulary is to discover frequently typed out-of-vocabulary (OOV) words on user devices.","This task requires strong privacy protection due to the sensitive nature of user input data.","In this report, we present a private OOV discovery algorithm for Gboard, which builds on recent advances in private federated analytics.","The system offers local differential privacy (LDP) guarantees for user contributed words.","With anonymous aggregation, the final released words satisfy central differential privacy guarantees with $\\epsilon = 0.315, \\delta = 10^{-10}$ for OOV discovery in en-US (English in United States)."],"url":"http://arxiv.org/abs/2404.11607v1","category":"cs.DS"}
{"created":"2024-04-17 17:52:37","title":"The 33 M$_\\odot$ black hole Gaia BH3 is part of the disrupted ED-2 star cluster","abstract":"The Gaia Collaboration has recently reported the detection of a 33 M$_\\odot$ black hole in a wide binary system located in the Solar neighbourhood. Here we explore the relationship between this black hole, known as Gaia BH3, and the nearby ED-2 halo stellar stream. We study the orbital characteristics of the Gaia BH3 binary and present measurements of the chemical abundances of ED-2 member stars derived from high-resolution spectra obtained with the VLT. We find that the Galactic orbit of the Gaia BH3 system and its metallicity are entirely consistent with being part of the ED-2 stream. The characteristics of the stream, particularly its negligible spread in metallicity and in other chemical elements as well as its single stellar population, suggest that it originated from a disrupted star cluster of low mass. Its age is comparable to that of the globular cluster M92 that has been estimated to be as old as the Universe. This is the first black hole unambiguously associated with a disrupted star cluster. We infer a plausible mass range for the cluster to be relatively narrow, between $2\\times 10^3M_\\odot$ and $4.2\\times 10^4M_\\odot$. This implies that the black hole could have formed directly from the collapse of a massive very-metal-poor star, but that the alternative scenario of binary interactions inside the cluster environment also deserves to be explored.","sentences":["The Gaia Collaboration has recently reported the detection of a 33 M$_\\odot$ black hole in a wide binary system located in the Solar neighbourhood.","Here we explore the relationship between this black hole, known as Gaia BH3, and the nearby ED-2 halo stellar stream.","We study the orbital characteristics of the Gaia BH3 binary and present measurements of the chemical abundances of ED-2 member stars derived from high-resolution spectra obtained with the VLT.","We find that the Galactic orbit of the Gaia BH3 system and its metallicity are entirely consistent with being part of the ED-2 stream.","The characteristics of the stream, particularly its negligible spread in metallicity and in other chemical elements as well as its single stellar population, suggest that it originated from a disrupted star cluster of low mass.","Its age is comparable to that of the globular cluster M92 that has been estimated to be as old as the Universe.","This is the first black hole unambiguously associated with a disrupted star cluster.","We infer a plausible mass range for the cluster to be relatively narrow, between $2\\times 10^3M_\\odot$ and $4.2\\times 10^4M_\\odot$.","This implies that the black hole could have formed directly from the collapse of a massive very-metal-poor star, but that the alternative scenario of binary interactions inside the cluster environment also deserves to be explored."],"url":"http://arxiv.org/abs/2404.11604v1","category":"astro-ph.GA"}
{"created":"2024-04-17 17:51:15","title":"Interaction Techniques for Exploratory Data Visualization on Mobile Devices","abstract":"The ubiquity and on-the-go availability of mobile devices makes them central to many tasks such as interpersonal communication and media consumption. However, despite the potential of mobile devices for on-demand exploratory data visualization, existing mobile interactions are difficult, often using highly custom interactions, complex gestures, or multi-modal input. We synthesize limitations from the literature and outline four motivating principles for improved mobile interaction: leverage ubiquitous modalities, prioritize discoverability, enable rapid in-context data exploration, and promote graceful recovery. We then contribute thirteen interaction candidates and conduct a formative study with twelve participants who experienced our interactions in a testbed prototype. Based on these interviews, we discuss design considerations and tradeoffs from four main themes: precise and rapid inspection, focused navigation, single-touch and fixed orientation interaction, and judicious use of motion.","sentences":["The ubiquity and on-the-go availability of mobile devices makes them central to many tasks such as interpersonal communication and media consumption.","However, despite the potential of mobile devices for on-demand exploratory data visualization, existing mobile interactions are difficult, often using highly custom interactions, complex gestures, or multi-modal input.","We synthesize limitations from the literature and outline four motivating principles for improved mobile interaction: leverage ubiquitous modalities, prioritize discoverability, enable rapid in-context data exploration, and promote graceful recovery.","We then contribute thirteen interaction candidates and conduct a formative study with twelve participants who experienced our interactions in a testbed prototype.","Based on these interviews, we discuss design considerations and tradeoffs from four main themes: precise and rapid inspection, focused navigation, single-touch and fixed orientation interaction, and judicious use of motion."],"url":"http://arxiv.org/abs/2404.11602v1","category":"cs.HC"}
{"created":"2024-04-17 17:50:24","title":"Variational Bayesian Last Layers","abstract":"We introduce a deterministic variational formulation for training Bayesian last layer neural networks. This yields a sampling-free, single-pass model and loss that effectively improves uncertainty estimation. Our variational Bayesian last layer (VBLL) can be trained and evaluated with only quadratic complexity in last layer width, and is thus (nearly) computationally free to add to standard architectures. We experimentally investigate VBLLs, and show that they improve predictive accuracy, calibration, and out of distribution detection over baselines across both regression and classification. Finally, we investigate combining VBLL layers with variational Bayesian feature learning, yielding a lower variance collapsed variational inference method for Bayesian neural networks.","sentences":["We introduce a deterministic variational formulation for training Bayesian last layer neural networks.","This yields a sampling-free, single-pass model and loss that effectively improves uncertainty estimation.","Our variational Bayesian last layer (VBLL) can be trained and evaluated with only quadratic complexity in last layer width, and is thus (nearly) computationally free to add to standard architectures.","We experimentally investigate VBLLs, and show that they improve predictive accuracy, calibration, and out of distribution detection over baselines across both regression and classification.","Finally, we investigate combining VBLL layers with variational Bayesian feature learning, yielding a lower variance collapsed variational inference method for Bayesian neural networks."],"url":"http://arxiv.org/abs/2404.11599v1","category":"cs.LG"}
{"created":"2024-04-17 17:28:30","title":"Maximin Shares in Hereditary Set Systems","abstract":"We consider the problem of fairly allocating a set of indivisible items under the criteria of the maximin share guarantee. Specifically, we study approximation of maximin share allocations under hereditary set system valuations, in which each valuation function is based on the independent sets of an underlying hereditary set systems. Using a lone divider approach, we show the existence of $1/2$-approximate MMS allocations, improving on the $11/30$ guarantee of Li and Vetta. Moreover, we prove that ($2/3 + \\epsilon$)-approximate MMS allocations do not always exist in this model for every $\\epsilon > 0$, an improvement from the recent $3/4 + \\epsilon$ result of Li and Deng. Our existence proof is constructive, but does not directly yield a polynomial-time approximation algorithm. However, we show that a $2/5$-approximate MMS allocation can be found in polynomial time, given valuation oracles. Finally, we show that our existence and approximation results transfer to a variety of problems within constrained fair allocation, improving on existing results in some of these settings.","sentences":["We consider the problem of fairly allocating a set of indivisible items under the criteria of the maximin share guarantee.","Specifically, we study approximation of maximin share allocations under hereditary set system valuations, in which each valuation function is based on the independent sets of an underlying hereditary set systems.","Using a lone divider approach, we show the existence of $1/2$-approximate MMS allocations, improving on the $11/30$ guarantee of Li and Vetta.","Moreover, we prove that ($2/3 + \\epsilon$)-approximate MMS allocations do not always exist in this model for every $\\epsilon > 0$, an improvement from the recent $3/4 + \\epsilon$ result of Li and Deng.","Our existence proof is constructive, but does not directly yield a polynomial-time approximation algorithm.","However, we show that a $2/5$-approximate MMS allocation can be found in polynomial time, given valuation oracles.","Finally, we show that our existence and approximation results transfer to a variety of problems within constrained fair allocation, improving on existing results in some of these settings."],"url":"http://arxiv.org/abs/2404.11582v1","category":"cs.GT"}
{"created":"2024-04-17 17:25:25","title":"Spatial Heterogeneous Additive Partial Linear Model: A Joint Approach of Bivariate Spline and Forest Lasso","abstract":"Identifying spatial heterogeneous patterns has attracted a surge of research interest in recent years, due to its important applications in various scientific and engineering fields. In practice the spatially heterogeneous components are often mixed with components which are spatially smooth, making the task of identifying the heterogeneous regions more challenging. In this paper, we develop an efficient clustering approach to identify the model heterogeneity of the spatial additive partial linear model. Specifically, we aim to detect the spatially contiguous clusters based on the regression coefficients while introducing a spatially varying intercept to deal with the smooth spatial effect. On the one hand, to approximate the spatial varying intercept, we use the method of bivariate spline over triangulation, which can effectively handle the data from a complex domain. On the other hand, a novel fusion penalty termed the forest lasso is proposed to reveal the spatial clustering pattern. Our proposed fusion penalty has advantages in both the estimation and computation efficiencies when dealing with large spatial data. Theoretically properties of our estimator are established, and simulation results show that our approach can achieve more accurate estimation with a limited computation cost compared with the existing approaches. To illustrate its practical use, we apply our approach to analyze the spatial pattern of the relationship between land surface temperature measured by satellites and air temperature measured by ground stations in the United States.","sentences":["Identifying spatial heterogeneous patterns has attracted a surge of research interest in recent years, due to its important applications in various scientific and engineering fields.","In practice the spatially heterogeneous components are often mixed with components which are spatially smooth, making the task of identifying the heterogeneous regions more challenging.","In this paper, we develop an efficient clustering approach to identify the model heterogeneity of the spatial additive partial linear model.","Specifically, we aim to detect the spatially contiguous clusters based on the regression coefficients while introducing a spatially varying intercept to deal with the smooth spatial effect.","On the one hand, to approximate the spatial varying intercept, we use the method of bivariate spline over triangulation, which can effectively handle the data from a complex domain.","On the other hand, a novel fusion penalty termed the forest lasso is proposed to reveal the spatial clustering pattern.","Our proposed fusion penalty has advantages in both the estimation and computation efficiencies when dealing with large spatial data.","Theoretically properties of our estimator are established, and simulation results show that our approach can achieve more accurate estimation with a limited computation cost compared with the existing approaches.","To illustrate its practical use, we apply our approach to analyze the spatial pattern of the relationship between land surface temperature measured by satellites and air temperature measured by ground stations in the United States."],"url":"http://arxiv.org/abs/2404.11579v1","category":"stat.ME"}
{"created":"2024-04-17 17:09:40","title":"Emulators for scarce and noisy data: application to auxiliary field diffusion Monte Carlo for the deuteron","abstract":"The validation, verification, and uncertainty quantification of computationally expensive theoretical models of quantum many-body systems require the construction of fast and accurate emulators. In this work, we develop emulators for auxiliary field diffusion Monte Carlo (AFDMC), a powerful many-body method for nuclear systems. We introduce a reduced-basis method (RBM) emulator for AFDMC and study it in the simple case of the deuteron. Furthermore, we compare our RBM emulator with the recently proposed parametric matrix model (PMM) that combines elements of RBMs with machine learning. We contrast these two approaches with a traditional Gaussian Process emulator. All three emulators constructed here are based on a very limited set of 5 training points, as expected for realistic AFDMC calculations, but validated against $\\mathcal{O}(10^3)$ exact solutions. We find that the PMM, with emulator errors of only $\\approx 0.1 \\%$ and speed-up factors of $\\approx 10^7$, outperforms the other two emulators when applied to AFDMC.","sentences":["The validation, verification, and uncertainty quantification of computationally expensive theoretical models of quantum many-body systems require the construction of fast and accurate emulators.","In this work, we develop emulators for auxiliary field diffusion Monte Carlo (AFDMC), a powerful many-body method for nuclear systems.","We introduce a reduced-basis method (RBM) emulator for AFDMC and study it in the simple case of the deuteron.","Furthermore, we compare our RBM emulator with the recently proposed parametric matrix model (PMM) that combines elements of RBMs with machine learning.","We contrast these two approaches with a traditional Gaussian Process emulator.","All three emulators constructed here are based on a very limited set of 5 training points, as expected for realistic AFDMC calculations, but validated against $\\mathcal{O}(10^3)$ exact solutions.","We find that the PMM, with emulator errors of only $\\approx 0.1 \\%$ and speed-up factors of $\\approx 10^7$, outperforms the other two emulators when applied to AFDMC."],"url":"http://arxiv.org/abs/2404.11566v1","category":"nucl-th"}
{"created":"2024-04-17 17:02:27","title":"The Terwilliger algebras of doubly regular tournaments","abstract":"The Terwilliger algebras of asymmetric association schemes of rank $3$, whose nonidentity relations correspond to doubly regular tournaments, are shown to have thin irreducible modules, and to always be of dimension $4k+9$ for some positive integer $k$. It is determined that asymmetric rank $3$ association schemes of order up to $23$ are determined up to combinatorial isomorphism by the list of their complex Terwilliger algebras at each vertex, but this no longer true at order $27$. To distinguish order $27$ asymmetric rank $3$ association schemes, it is shown using computer calculations that the list of rational Terwilliger algebras at each vertex will suffice.","sentences":["The Terwilliger algebras of asymmetric association schemes of rank $3$, whose nonidentity relations correspond to doubly regular tournaments, are shown to have thin irreducible modules, and to always be of dimension $4k+9$ for some positive integer","$k$.","It is determined that asymmetric rank $3$ association schemes of order up to $23$ are determined up to combinatorial isomorphism by the list of their complex Terwilliger algebras at each vertex, but this no longer true at order $27$. To distinguish order $27$ asymmetric rank $3$ association schemes, it is shown using computer calculations that the list of rational Terwilliger algebras at each vertex will suffice."],"url":"http://arxiv.org/abs/2404.11560v1","category":"math.CO"}
{"created":"2024-04-17 17:00:12","title":"Hierarchical storage management in user space for neuroimaging applications","abstract":"Neuroimaging open-data initiatives have led to increased availability of large scientific datasets. While these datasets are shifting the processing bottleneck from compute-intensive to data-intensive, current standardized analysis tools have yet to adopt strategies that mitigate the costs associated with large data transfers. A major challenge in adapting neuroimaging applications for data-intensive processing is that they must be entirely rewritten. To facilitate data management for standardized neuroimaging tools, we developed Sea, a library that intercepts and redirects application read and write calls to minimize data transfer time. In this paper, we investigate the performance of Sea on three preprocessing pipelines implemented using standard toolboxes (FSL, SPM and AFNI), using three neuroimaging datasets of different sizes (OpenNeuro's ds001545, PREVENT-AD and the HCP dataset) on two high-performance computing clusters. Our results demonstrate that Sea provides large speedups (up to 32X) when the shared file system's (e.g. Lustre) performance is deteriorated. When the shared file system is not overburdened by other users, performance is unaffected by Sea, suggesting that Sea's overhead is minimal even in cases where its benefits are limited. Overall, Sea is beneficial, even when performance gain is minimal, as it can be used to limit the number of files created on parallel file systems.","sentences":["Neuroimaging open-data initiatives have led to increased availability of large scientific datasets.","While these datasets are shifting the processing bottleneck from compute-intensive to data-intensive, current standardized analysis tools have yet to adopt strategies that mitigate the costs associated with large data transfers.","A major challenge in adapting neuroimaging applications for data-intensive processing is that they must be entirely rewritten.","To facilitate data management for standardized neuroimaging tools, we developed Sea, a library that intercepts and redirects application read and write calls to minimize data transfer time.","In this paper, we investigate the performance of Sea on three preprocessing pipelines implemented using standard toolboxes (FSL, SPM and AFNI), using three neuroimaging datasets of different sizes (OpenNeuro's ds001545, PREVENT-AD and the HCP dataset) on two high-performance computing clusters.","Our results demonstrate that Sea provides large speedups (up to 32X) when the shared file system's (e.g. Lustre) performance is deteriorated.","When the shared file system is not overburdened by other users, performance is unaffected by Sea, suggesting that Sea's overhead is minimal even in cases where its benefits are limited.","Overall, Sea is beneficial, even when performance gain is minimal, as it can be used to limit the number of files created on parallel file systems."],"url":"http://arxiv.org/abs/2404.11556v1","category":"cs.DC"}
{"created":"2024-04-17 16:44:24","title":"Borel transforms of functions from a parameterized family of Hilbert spaces of entire functions","abstract":"We introduce a continuous scale of Hilbert spaces of entire functions $P_\\beta (D)$ for a bounded convex domain $D$ on the complex plane. For the parameters $\\beta \\in (\\frac 12;\\frac 32)$ a complete description of the spaces of Borel transforms of functions from $P_\\beta (D)$ is given.","sentences":["We introduce a continuous scale of Hilbert spaces of entire functions $P_\\beta (D)$ for a bounded convex domain $D$ on the complex plane.","For the parameters $\\beta \\in (\\frac 12;\\frac 32)$ a complete description of the spaces of Borel transforms of functions from $P_\\beta (D)$ is given."],"url":"http://arxiv.org/abs/2404.11548v1","category":"math.CV"}
{"created":"2024-04-17 16:42:55","title":"Steiner trees with infinitely many terminals on the sides of an angle","abstract":"The Euclidean Steiner problem is the problem of finding a set $St$, with the shortest length, such that $St \\cup A$ is connected, where $A$ is a given set in a Euclidean space. The solutions $St$ to the Steiner problem will be called Steiner sets while the set $A$ will be called input. Since every Steiner set is acyclic we call it Steiner tree in the case when it is connected. We say that a Steiner tree is indecomposable if it does not contain any Steiner tree for a subset of the input.   We are interested in finding the Steiner set when the input consists of infinitely many points distributed on two lines. In particular we would like to find a configuration which gives an indecomposable Steiner tree.   We consider a self-similar input, namely the set $A_{\\alpha,\\lambda}$ of points with coordinates $(\\lambda^{k-1}\\cos \\alpha,$ $\\pm \\lambda^{k-1}\\sin \\alpha)$, where $\\lambda>0$ and $\\alpha>0$ are small fixed values. These points are distributed on the two sides of an angle of size $2\\alpha$ in such a way that the distances from the points to the vertex of the angle are in a geometric progression.   To our surprise, we show that in this case the solutions to the Steiner problem for $A_{\\alpha,\\lambda}$, when $\\alpha$ and $\\lambda$ are small enough, are always decomposable trees. More precisely, any Steiner tree for $A_{\\alpha,\\lambda}$ is a countable union of Steiner trees, each one connecting 5 points from the input. By considering only a finite number of components we obtain many solutions to the Steiner problem for finite sets composed of $4k+1$ points distributed on the two lines ($2k+1$ on a line and $2k$ on the other line). These solutions are very similar to the ladders of Chung and Graham.","sentences":["The Euclidean Steiner problem is the problem of finding a set $St$, with the shortest length, such that $St \\cup A$ is connected, where $A$ is a given set in a Euclidean space.","The solutions $St$ to the Steiner problem will be called Steiner sets while the set $A$ will be called input.","Since every Steiner set is acyclic we call it Steiner tree in the case when it is connected.","We say that a Steiner tree is indecomposable if it does not contain any Steiner tree for a subset of the input.   ","We are interested in finding the Steiner set when the input consists of infinitely many points distributed on two lines.","In particular we would like to find a configuration which gives an indecomposable Steiner tree.   ","We consider a self-similar input, namely the set $A_{\\alpha,\\lambda}$ of points with coordinates $(\\lambda^{k-1}\\cos \\alpha,$ $\\pm \\lambda^{k-1}\\sin \\alpha)$, where $\\lambda>0$ and $\\alpha>0$ are small fixed values.","These points are distributed on the two sides of an angle of size $2\\alpha$ in such a way that the distances from the points to the vertex of the angle are in a geometric progression.   ","To our surprise, we show that in this case the solutions to the Steiner problem for $A_{\\alpha,\\lambda}$, when $\\alpha$ and $\\lambda$ are small enough, are always decomposable trees.","More precisely, any Steiner tree for $A_{\\alpha,\\lambda}$ is a countable union of Steiner trees, each one connecting 5 points from the input.","By considering only a finite number of components we obtain many solutions to the Steiner problem for finite sets composed of $4k+1$ points distributed on the two lines ($2k+1$ on a line and $2k$ on the other line).","These solutions are very similar to the ladders of Chung and Graham."],"url":"http://arxiv.org/abs/2404.11546v1","category":"math.MG"}
{"created":"2024-04-17 16:35:00","title":"Carbon- and Oxygen-rich stars in MaStar: identification and classification","abstract":"Carbon- and Oxygen-rich stars populating the Thermally-Pulsing Asymptotic Giant Branch (TP-AGB) phase of stellar evolution are relevant contributors to the spectra of ~1 Gyr old populations. Atmosphere models for these types are uncertain, due to complex molecules and mass-loss effects. Empirical spectra are then crucial, but samples are small due to the short (~3 Myr) TP-AGB lifetime. Here we exploit the vastness of the MaNGA Stellar library MaStar (~60,000 spectra) to identify C,O-rich type stars. We define an optical colour selection with cuts of (g-r)>2 and (g-i)<1.55(g-r)-0.07, calibrated with known C- and O- rich spectra. This identifies C-,O-rich stars along clean, separated sequences. An analogue selection is found in V,R,I bands. Our equation identifies C- and O-rich spectra with predictive performance metric F1-scores of 0.72 and 0.74 (over 1), respectively. We finally identify 41 C- and 87 O-rich type AGB stars in MaStar, 5 and 49 of which do not have a SIMBAD counterpart. We also detect a sample of non-AGB, dwarf C-stars. We further design a fitting procedure to classify the spectra into broad spectral types, by using as fitting templates empirical C and O-rich spectra. We find remarkably good fits for the majority of candidates and categorise them into C- and O-rich bins following existing classifications, which correlate to effective temperature. Our selection models can be applied to large photometric surveys (e.g. Euclid, Rubin). The classified spectra will facilitate future evolutionary population synthesis models.","sentences":["Carbon- and Oxygen-rich stars populating the Thermally-Pulsing Asymptotic Giant Branch (TP-AGB) phase of stellar evolution are relevant contributors to the spectra of ~1 Gyr old populations.","Atmosphere models for these types are uncertain, due to complex molecules and mass-loss effects.","Empirical spectra are then crucial, but samples are small due to the short (~3 Myr) TP-AGB lifetime.","Here we exploit the vastness of the MaNGA Stellar library MaStar (~60,000 spectra) to identify C,O-rich type stars.","We define an optical colour selection with cuts of (g-r)>2 and (g-i)<1.55(g-r)-0.07, calibrated with known C- and O- rich spectra.","This identifies C-,O-rich stars along clean, separated sequences.","An analogue selection is found in V,R,I bands.","Our equation identifies C- and O-rich spectra with predictive performance metric F1-scores of 0.72 and 0.74 (over 1), respectively.","We finally identify 41 C- and 87 O-rich type AGB stars in MaStar, 5 and 49 of which do not have a SIMBAD counterpart.","We also detect a sample of non-AGB, dwarf C-stars.","We further design a fitting procedure to classify the spectra into broad spectral types, by using as fitting templates empirical C and O-rich spectra.","We find remarkably good fits for the majority of candidates and categorise them into C- and O-rich bins following existing classifications, which correlate to effective temperature.","Our selection models can be applied to large photometric surveys (e.g. Euclid, Rubin).","The classified spectra will facilitate future evolutionary population synthesis models."],"url":"http://arxiv.org/abs/2404.11541v1","category":"astro-ph.GA"}
{"created":"2024-04-17 16:14:06","title":"Equitably allocating wildfire resilience investments for power grids: The curse of aggregation and vulnerability indices","abstract":"Wildfires ignited by power systems infrastructure are among the most destructive wildfires; hence some utility companies in wildfire-prone regions have pursued a proactive policy of emergency power shutoffs. These shutoffs, while mitigating the risk of disastrous ignition events, result in power outages that could negatively impacts vulnerable communities. In this paper, we consider how to equitably allocate funds to underground and effectively de-risk power lines in transmission networks. We explore the impact of the 2021 White House resource allocation policy called the Justice40 initiative, which states that 40% of the benefits of federally-funded climate-related investments should go to socially vulnerable communities. The definition of what constitutes a vulnerable community varies by organization, and we consider two major recently proposed vulnerability indices: the Justice40 index created under the 2021 White House and the Social Vulnerability Index (SVI) developed by the Center for Disease Control and Prevention (CDC). We show that allocating budget according to these two indices fails to reduce power outages for indigenous communities and those subject to high wildfire ignition risk using a high-fidelity synthetic power grid dataset that matches the key features of the Texas transmission system. We discuss how aggregation of communities and \"one size fits all\" vulnerability indices might be the reasons for the misalignment between the goals of vulnerability indices and their realized impact in this particular case study. We provide a method of achieving an equitable investment plan by adding group-level protections on percentage of load that is shed across each population group of interest.","sentences":["Wildfires ignited by power systems infrastructure are among the most destructive wildfires; hence some utility companies in wildfire-prone regions have pursued a proactive policy of emergency power shutoffs.","These shutoffs, while mitigating the risk of disastrous ignition events, result in power outages that could negatively impacts vulnerable communities.","In this paper, we consider how to equitably allocate funds to underground and effectively de-risk power lines in transmission networks.","We explore the impact of the 2021 White House resource allocation policy called the Justice40 initiative, which states that 40% of the benefits of federally-funded climate-related investments should go to socially vulnerable communities.","The definition of what constitutes a vulnerable community varies by organization, and we consider two major recently proposed vulnerability indices: the Justice40 index created under the 2021 White House and the Social Vulnerability Index (SVI) developed by the Center for Disease Control and Prevention (CDC).","We show that allocating budget according to these two indices fails to reduce power outages for indigenous communities and those subject to high wildfire ignition risk using a high-fidelity synthetic power grid dataset that matches the key features of the Texas transmission system.","We discuss how aggregation of communities and \"one size fits all\" vulnerability indices might be the reasons for the misalignment between the goals of vulnerability indices and their realized impact in this particular case study.","We provide a method of achieving an equitable investment plan by adding group-level protections on percentage of load that is shed across each population group of interest."],"url":"http://arxiv.org/abs/2404.11520v1","category":"math.OC"}
{"created":"2024-04-17 16:10:55","title":"Disentangled Cascaded Graph Convolution Networks for Multi-Behavior Recommendation","abstract":"Multi-behavioral recommender systems have emerged as a solution to address data sparsity and cold-start issues by incorporating auxiliary behaviors alongside target behaviors. However, existing models struggle to accurately capture varying user preferences across different behaviors and fail to account for diverse item preferences within behaviors. Various user preference factors (such as price or quality) entangled in the behavior may lead to sub-optimization problems. Furthermore, these models overlook the personalized nature of user behavioral preferences by employing uniform transformation networks for all users and items. To tackle these challenges, we propose the Disentangled Cascaded Graph Convolutional Network (Disen-CGCN), a novel multi-behavior recommendation model. Disen-CGCN employs disentangled representation techniques to effectively separate factors within user and item representations, ensuring their independence. In addition, it incorporates a multi-behavioral meta-network, enabling personalized feature transformation across user and item behaviors. Furthermore, an attention mechanism captures user preferences for different item factors within each behavior. By leveraging attention weights, we aggregate user and item embeddings separately for each behavior, computing preference scores that predict overall user preferences for items. Our evaluation on benchmark datasets demonstrates the superiority of Disen-CGCN over state-of-the-art models, showcasing an average performance improvement of 7.07% and 9.00% on respective datasets. These results highlight Disen-CGCN's ability to effectively leverage multi-behavioral data, leading to more accurate recommendations.","sentences":["Multi-behavioral recommender systems have emerged as a solution to address data sparsity and cold-start issues by incorporating auxiliary behaviors alongside target behaviors.","However, existing models struggle to accurately capture varying user preferences across different behaviors and fail to account for diverse item preferences within behaviors.","Various user preference factors (such as price or quality) entangled in the behavior may lead to sub-optimization problems.","Furthermore, these models overlook the personalized nature of user behavioral preferences by employing uniform transformation networks for all users and items.","To tackle these challenges, we propose the Disentangled Cascaded Graph Convolutional Network (Disen-CGCN), a novel multi-behavior recommendation model.","Disen-CGCN employs disentangled representation techniques to effectively separate factors within user and item representations, ensuring their independence.","In addition, it incorporates a multi-behavioral meta-network, enabling personalized feature transformation across user and item behaviors.","Furthermore, an attention mechanism captures user preferences for different item factors within each behavior.","By leveraging attention weights, we aggregate user and item embeddings separately for each behavior, computing preference scores that predict overall user preferences for items.","Our evaluation on benchmark datasets demonstrates the superiority of Disen-CGCN over state-of-the-art models, showcasing an average performance improvement of 7.07% and 9.00% on respective datasets.","These results highlight Disen-CGCN's ability to effectively leverage multi-behavioral data, leading to more accurate recommendations."],"url":"http://arxiv.org/abs/2404.11519v1","category":"cs.IR"}
{"created":"2024-04-17 16:10:25","title":"High-order meshless global stability analysis of Taylor-Couette flows in complex domains","abstract":"Recently, meshless methods have become popular in numerically solving partial differential equations and have been employed to solve equations governing fluid flows, heat transfer, and species transport. In the present study, a numerical solver is developed employing the meshless framework to efficiently compute the hydrodynamic stability of fluid flows in complex geometries. The developed method is tested on two cases of Taylor-Couette flows. The concentric case represents the parallel flow assumption incorporated in the Orr-Sommerfeld model and the eccentric Taylor-Couette flow incorporates a non-parallel base flow with separation bubbles. The method was validated against earlier works by Marcus [1], Oikawa et al. [2], Leclercq et al. [3], and Mittal et al. [4]. The results for the two cases and the effectiveness of the method are discussed in detail. The method is then applied to Taylor-Couette flow in an elliptical enclosure and the stability of the flow is investigated.","sentences":["Recently, meshless methods have become popular in numerically solving partial differential equations and have been employed to solve equations governing fluid flows, heat transfer, and species transport.","In the present study, a numerical solver is developed employing the meshless framework to efficiently compute the hydrodynamic stability of fluid flows in complex geometries.","The developed method is tested on two cases of Taylor-Couette flows.","The concentric case represents the parallel flow assumption incorporated in the Orr-Sommerfeld model and the eccentric Taylor-Couette flow incorporates a non-parallel base flow with separation bubbles.","The method was validated against earlier works by Marcus","[1], Oikawa et al.","[2], Leclercq et al.","[3], and Mittal et al.","[4].","The results for the two cases and the effectiveness of the method are discussed in detail.","The method is then applied to Taylor-Couette flow in an elliptical enclosure and the stability of the flow is investigated."],"url":"http://arxiv.org/abs/2404.11517v1","category":"physics.flu-dyn"}
{"created":"2024-04-17 16:06:31","title":"Central limit theorems for Green metrics on hyperbolic groups","abstract":"Recently, the first author and Tanaka obtained a large deviation principle comparing two Green metrics on a non-elementary hyperbolic group $\\Gamma$ associated to finitely supported, admissable probability measures. The aim of this article is to prove the corresponding central limit theorem. Furthermore, our results apply to various other metrics including length functions associated to Anosov representations and to group actions on hyperbolic metric spaces.   In the particular case that $\\Gamma$ is the fundamental group of a closed hyperbolic manifold our work provides a statistical characterisation for the hitting measure of a finitely supported random walk being in the same class as the Lebesgue measure.","sentences":["Recently, the first author and Tanaka obtained a large deviation principle comparing two Green metrics on a non-elementary hyperbolic group $\\Gamma$ associated to finitely supported, admissable probability measures.","The aim of this article is to prove the corresponding central limit theorem.","Furthermore, our results apply to various other metrics including length functions associated to Anosov representations and to group actions on hyperbolic metric spaces.   ","In the particular case that $\\Gamma$ is the fundamental group of a closed hyperbolic manifold our work provides a statistical characterisation for the hitting measure of a finitely supported random walk being in the same class as the Lebesgue measure."],"url":"http://arxiv.org/abs/2404.11512v1","category":"math.DS"}
{"created":"2024-04-17 15:59:25","title":"Testing Intersectingness of Uniform Families","abstract":"A set family ${\\cal F}$ is called intersecting if every two members of ${\\cal F}$ intersect, and it is called uniform if all members of ${\\cal F}$ share a common size. A uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$ of $k$-subsets of $[n]$ is $\\varepsilon$-far from intersecting if one has to remove more than $\\varepsilon \\cdot \\binom{n}{k}$ of the sets of ${\\cal F}$ to make it intersecting. We study the property testing problem that given query access to a uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$, asks to distinguish between the case that ${\\cal F}$ is intersecting and the case that it is $\\varepsilon$-far from intersecting. We prove that for every fixed integer $r$, the problem admits a non-adaptive two-sided error tester with query complexity $O(\\frac{\\ln n}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k}{n})^r)$ and a non-adaptive one-sided error tester with query complexity $O(\\frac{\\ln k}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^r)$. The query complexities are optimal up to the logarithmic terms. For $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^2)$, we further provide a non-adaptive one-sided error tester with optimal query complexity of $O(\\frac{1}{\\varepsilon})$. Our findings show that the query complexity of the problem differs substantially from that of testing intersectingness of non-uniform families, studied recently by Chen, De, Li, Nadimpalli, and Servedio (ITCS, 2024).","sentences":["A set family ${\\cal F}$ is called intersecting if every two members of ${\\cal F}$ intersect, and it is called uniform if all members of ${\\cal F}$ share a common size.","A uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$ of $k$-subsets of $[n]$ is $\\varepsilon$-far from intersecting if one has to remove more than $\\varepsilon \\cdot \\binom{n}{k}$ of the sets of ${\\cal F}$ to make it intersecting.","We study the property testing problem that given query access to a uniform family ${\\cal F} \\subseteq \\binom{[n]}{k}$, asks to distinguish between the case that ${\\cal F}$ is intersecting and the case that it is $\\varepsilon$-far from intersecting.","We prove that for every fixed integer $r$, the problem admits a non-adaptive two-sided error tester with query complexity $O(\\frac{\\ln n}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k}{n})^r)$ and a non-adaptive one-sided error tester with query complexity $O(\\frac{\\ln k}{\\varepsilon})$ for $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^r)$. The query complexities are optimal up to the logarithmic terms.","For $\\varepsilon \\geq \\Omega( (\\frac{k^2}{n})^2)$, we further provide a non-adaptive one-sided error tester with optimal query complexity of $O(\\frac{1}{\\varepsilon})$. Our findings show that the query complexity of the problem differs substantially from that of testing intersectingness of non-uniform families, studied recently by Chen, De, Li, Nadimpalli, and Servedio (ITCS, 2024)."],"url":"http://arxiv.org/abs/2404.11504v1","category":"cs.DS"}
{"created":"2024-04-17 15:52:29","title":"Runtime Verification and Field Testing for ROS-Based Robotic Systems","abstract":"Robotic systems are becoming pervasive and adopted in increasingly many domains, such as manufacturing, healthcare, and space exploration. To this end, engineering software has emerged as a crucial discipline for building maintainable and reusable robotic systems. Robotics software engineering research has received increasing attention, fostering autonomy as a fundamental goal. However, robotics developers are still challenged trying to achieve this goal given that simulation is not able to deliver solutions to realistically emulate real-world phenomena. Robots also need to operate in unpredictable and uncontrollable environments, which require safe and trustworthy self-adaptation capabilities implemented in software. Typical techniques to address the challenges are runtime verification, field-based testing, and mitigation techniques that enable fail-safe solutions. However, there is no clear guidance to architect ROS-based systems to enable and facilitate runtime verification and field-based testing. This paper aims to fill in this gap by providing guidelines that can help developers and QA teams when developing, verifying or testing their robots in the field. These guidelines are carefully tailored to address the challenges and requirements of testing robotics systems in real-world scenarios. We conducted a literature review on studies addressing runtime verification and field-based testing for robotic systems, mined ROS-based application repositories, and validated the applicability, clarity, and usefulness via two questionnaires with 55 answers. We contribute 20 guidelines formulated for researchers and practitioners in robotic software engineering. Finally, we map our guidelines to open challenges thus far in runtime verification and field-based testing for ROS-based systems and, we outline promising research directions in the field.","sentences":["Robotic systems are becoming pervasive and adopted in increasingly many domains, such as manufacturing, healthcare, and space exploration.","To this end, engineering software has emerged as a crucial discipline for building maintainable and reusable robotic systems.","Robotics software engineering research has received increasing attention, fostering autonomy as a fundamental goal.","However, robotics developers are still challenged trying to achieve this goal given that simulation is not able to deliver solutions to realistically emulate real-world phenomena.","Robots also need to operate in unpredictable and uncontrollable environments, which require safe and trustworthy self-adaptation capabilities implemented in software.","Typical techniques to address the challenges are runtime verification, field-based testing, and mitigation techniques that enable fail-safe solutions.","However, there is no clear guidance to architect ROS-based systems to enable and facilitate runtime verification and field-based testing.","This paper aims to fill in this gap by providing guidelines that can help developers and QA teams when developing, verifying or testing their robots in the field.","These guidelines are carefully tailored to address the challenges and requirements of testing robotics systems in real-world scenarios.","We conducted a literature review on studies addressing runtime verification and field-based testing for robotic systems, mined ROS-based application repositories, and validated the applicability, clarity, and usefulness via two questionnaires with 55 answers.","We contribute 20 guidelines formulated for researchers and practitioners in robotic software engineering.","Finally, we map our guidelines to open challenges thus far in runtime verification and field-based testing for ROS-based systems and, we outline promising research directions in the field."],"url":"http://arxiv.org/abs/2404.11498v1","category":"cs.SE"}
{"created":"2024-04-17 15:46:28","title":"Microscopic solutions for vortex clustering in two-band type-1.5 superconductor","abstract":"It has been proposed that two-band superconductors exhibit a distinct phase characterized by two correlation lengths, where one is smaller and the other larger than the magnetic field penetration length. This regime was coined type-1.5 superconductivity, with a number of unconventional properties, such as vortex clustering. However a fully microscopic solution for vortex clusters has remained challenging due to computational complexities beyond quasiclassical models. In this work, we present numerical solutions obtained in a fully self-consistent two-band Bogoliubov-de-Gennes model. We show the presence of discrepant correlation lengths leading to vortex clustering in two-band superconductors.","sentences":["It has been proposed that two-band superconductors exhibit a distinct phase characterized by two correlation lengths, where one is smaller and the other larger than the magnetic field penetration length.","This regime was coined type-1.5 superconductivity, with a number of unconventional properties, such as vortex clustering.","However a fully microscopic solution for vortex clusters has remained challenging due to computational complexities beyond quasiclassical models.","In this work, we present numerical solutions obtained in a fully self-consistent two-band Bogoliubov-de-Gennes model.","We show the presence of discrepant correlation lengths leading to vortex clustering in two-band superconductors."],"url":"http://arxiv.org/abs/2404.11491v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 15:46:27","title":"Variational methods for solving high dimensional quantum systems","abstract":"Variational methods are highly valuable computational tools for solving high-dimensional quantum systems. In this paper, we explore the effectiveness of three variational methods: the density matrix renormalization group (DMRG), Boltzmann machine learning, and the variational quantum eigensolver (VQE). We apply these methods to two different quantum systems: the fermi-Hubbard model in condensed matter physics and the Schwinger model in high energy physics. To facilitate the computations on quantum computers, we map each model to a spin 1/2 system using the Jordan-Wigner transformation. This transformation allows us to take advantage of the capabilities of quantum computing. We calculate the ground state of both quantum systems and compare the results obtained using the three variational methods. By doing so, we aim to demonstrate the power and effectiveness of these variational approaches in tackling complex quantum systems.","sentences":["Variational methods are highly valuable computational tools for solving high-dimensional quantum systems.","In this paper, we explore the effectiveness of three variational methods: the density matrix renormalization group (DMRG), Boltzmann machine learning, and the variational quantum eigensolver (VQE).","We apply these methods to two different quantum systems: the fermi-Hubbard model in condensed matter physics and the Schwinger model in high energy physics.","To facilitate the computations on quantum computers, we map each model to a spin 1/2 system using the Jordan-Wigner transformation.","This transformation allows us to take advantage of the capabilities of quantum computing.","We calculate the ground state of both quantum systems and compare the results obtained using the three variational methods.","By doing so, we aim to demonstrate the power and effectiveness of these variational approaches in tackling complex quantum systems."],"url":"http://arxiv.org/abs/2404.11490v1","category":"quant-ph"}
{"created":"2024-04-17 15:34:55","title":"IoTSim-Osmosis-RES: Towards autonomic renewable energy-aware osmotic computing","abstract":"Internet of Things systems exists in various areas of our everyday life. For example, sensors installed in smart cities and homes are processed in edge and cloud computing centres providing several benefits that improve our lives. The place of data processing is related to the required system response times -- processing data closer to its source results in a shorter system response time. The Osmotic Computing concept enables flexible deployment of data processing services and their possible movement, just like particles in the osmosis phenomenon move between regions of different densities. At the same time, the impact of complex computer architecture on the environment is increasingly being compensated by the use of renewable and low-carbon energy sources. However, the uncertainty of supplying green energy makes the management of Osmotic Computing demanding, and therefore their autonomy is desirable. In the paper, we present a framework enabling osmotic computing simulation based on renewable energy sources and autonomic osmotic agents, allowing the analysis of distributed management algorithms. We discuss the challenges posed to the framework and analyze various management algorithms for cooperating osmotic agents. In the evaluation we show that changing the adaptation logic of the osmotic agents, it is possible to increase the self-consumption of renewable energy sources or increase the usage of low emission ones.","sentences":["Internet of Things systems exists in various areas of our everyday life.","For example, sensors installed in smart cities and homes are processed in edge and cloud computing centres providing several benefits that improve our lives.","The place of data processing is related to the required system response times -- processing data closer to its source results in a shorter system response time.","The Osmotic Computing concept enables flexible deployment of data processing services and their possible movement, just like particles in the osmosis phenomenon move between regions of different densities.","At the same time, the impact of complex computer architecture on the environment is increasingly being compensated by the use of renewable and low-carbon energy sources.","However, the uncertainty of supplying green energy makes the management of Osmotic Computing demanding, and therefore their autonomy is desirable.","In the paper, we present a framework enabling osmotic computing simulation based on renewable energy sources and autonomic osmotic agents, allowing the analysis of distributed management algorithms.","We discuss the challenges posed to the framework and analyze various management algorithms for cooperating osmotic agents.","In the evaluation we show that changing the adaptation logic of the osmotic agents, it is possible to increase the self-consumption of renewable energy sources or increase the usage of low emission ones."],"url":"http://arxiv.org/abs/2404.11481v1","category":"cs.DC"}
{"created":"2024-04-17 15:15:54","title":"Quantum dynamics of dissipative Chern insulator","abstract":"For open quantum systems,a short-time evolution is usually well described by the effective non-Hermitian Hamiltonians,while long-time dynamics requires the Lindblad master equation,in which the Liouvillian superoperators characterize the time evolution.In this paper, we constructed an open system by adding suitable gain and loss operators to the Chen insulator to investigate the time evolution of quantum states at long times by numerical simulations.Finally,we also propose a topolectrical circuits to realize the dissipative system for experimental observation.It is found that the opening and closing of the Liouvillian gap leads to different damping behaviours of the system and that the presence of non-Hermitian skin effects leads to a phenomenon of chiral damping with sharp wavefronts.Our study deepens the understanding of quantum dynamics of dissipative system.","sentences":["For open quantum systems,a short-time evolution is usually well described by the effective non-Hermitian Hamiltonians,while long-time dynamics requires the Lindblad master equation,in which the Liouvillian superoperators characterize the time evolution.","In this paper, we constructed an open system by adding suitable gain and loss operators to the Chen insulator to investigate the time evolution of quantum states at long times by numerical simulations.","Finally,we also propose a topolectrical circuits to realize the dissipative system for experimental observation.","It is found that the opening and closing of the Liouvillian gap leads to different damping behaviours of the system and that the presence of non-Hermitian skin effects leads to a phenomenon of chiral damping with sharp wavefronts.","Our study deepens the understanding of quantum dynamics of dissipative system."],"url":"http://arxiv.org/abs/2404.11466v1","category":"quant-ph"}
{"created":"2024-04-17 15:14:45","title":"Low-Density Parity-Check Codes and Spatial Coupling for Quantitative Group Testing","abstract":"A non-adaptive quantitative group testing (GT) scheme based on sparse codes-on-graphs in combination with low-complexity peeling decoding was introduced and analyzed by Karimi et al.. In this work, we propose a variant of this scheme based on low-density parity-check codes where the BCH codes at the constraint nodes are replaced by simple single parity-check codes. Furthermore, we apply spatial coupling to both GT schemes, perform a density evolution analysis, and compare their performance with and without coupling. Our analysis shows that both schemes improve with increasing coupling memory, and for all considered cases, it is observed that the LDPC code-based scheme substantially outperforms the original scheme. Simulation results for finite block length confirm the asymptotic density evolution thresholds.","sentences":["A non-adaptive quantitative group testing (GT) scheme based on sparse codes-on-graphs in combination with low-complexity peeling decoding was introduced and analyzed by Karimi et al..","In this work, we propose a variant of this scheme based on low-density parity-check codes where the BCH codes at the constraint nodes are replaced by simple single parity-check codes.","Furthermore, we apply spatial coupling to both GT schemes, perform a density evolution analysis, and compare their performance with and without coupling.","Our analysis shows that both schemes improve with increasing coupling memory, and for all considered cases, it is observed that the LDPC code-based scheme substantially outperforms the original scheme.","Simulation results for finite block length confirm the asymptotic density evolution thresholds."],"url":"http://arxiv.org/abs/2404.11463v1","category":"cs.IT"}
{"created":"2024-04-17 15:05:00","title":"Deep Pattern Network for Click-Through Rate Prediction","abstract":"Click-through rate (CTR) prediction tasks play a pivotal role in real-world applications, particularly in recommendation systems and online advertising. A significant research branch in this domain focuses on user behavior modeling. Current research predominantly centers on modeling co-occurrence relationships between the target item and items previously interacted with by users in their historical data. However, this focus neglects the intricate modeling of user behavior patterns. In reality, the abundance of user interaction records encompasses diverse behavior patterns, indicative of a spectrum of habitual paradigms. These patterns harbor substantial potential to significantly enhance CTR prediction performance. To harness the informational potential within user behavior patterns, we extend Target Attention (TA) to Target Pattern Attention (TPA) to model pattern-level dependencies. Furthermore, three critical challenges demand attention: the inclusion of unrelated items within behavior patterns, data sparsity in behavior patterns, and computational complexity arising from numerous patterns. To address these challenges, we introduce the Deep Pattern Network (DPN), designed to comprehensively leverage information from user behavior patterns. DPN efficiently retrieves target-related user behavior patterns using a target-aware attention mechanism. Additionally, it contributes to refining user behavior patterns through a pre-training paradigm based on self-supervised learning while promoting dependency learning within sparse patterns. Our comprehensive experiments, conducted across three public datasets, substantiate the superior performance and broad compatibility of DPN.","sentences":["Click-through rate (CTR) prediction tasks play a pivotal role in real-world applications, particularly in recommendation systems and online advertising.","A significant research branch in this domain focuses on user behavior modeling.","Current research predominantly centers on modeling co-occurrence relationships between the target item and items previously interacted with by users in their historical data.","However, this focus neglects the intricate modeling of user behavior patterns.","In reality, the abundance of user interaction records encompasses diverse behavior patterns, indicative of a spectrum of habitual paradigms.","These patterns harbor substantial potential to significantly enhance CTR prediction performance.","To harness the informational potential within user behavior patterns, we extend Target Attention (TA) to Target Pattern Attention (TPA) to model pattern-level dependencies.","Furthermore, three critical challenges demand attention: the inclusion of unrelated items within behavior patterns, data sparsity in behavior patterns, and computational complexity arising from numerous patterns.","To address these challenges, we introduce the Deep Pattern Network (DPN), designed to comprehensively leverage information from user behavior patterns.","DPN efficiently retrieves target-related user behavior patterns using a target-aware attention mechanism.","Additionally, it contributes to refining user behavior patterns through a pre-training paradigm based on self-supervised learning while promoting dependency learning within sparse patterns.","Our comprehensive experiments, conducted across three public datasets, substantiate the superior performance and broad compatibility of DPN."],"url":"http://arxiv.org/abs/2404.11456v1","category":"cs.IR"}
{"created":"2024-04-17 15:01:54","title":"Solution to the iterative differential equation $-\u03b3g' = g^{-1}$","abstract":"Using a Picard-like operator $T$, we prove that the iterative differential equation $-\\gamma g' = g^{-1}$ with parameter $\\gamma>0$ has a solution $g=h\\colon[0,1]\\to[0,1]$ for only one value $\\gamma=\\kappa\\approx0.278877$, and that this solution $h$ is unique. As an even stronger result, we exhibit $h$ as the global limit of the operator $T$.","sentences":["Using a Picard-like operator $T$, we prove that the iterative differential equation $-\\gamma g' = g^{-1}$ with parameter $\\gamma>0$ has a solution $g=h\\colon[0,1]\\to[0,1]$ for only one value $\\gamma=\\kappa\\approx0.278877$, and that this solution $h$ is unique.","As an even stronger result, we exhibit $h$ as the global limit of the operator $T$."],"url":"http://arxiv.org/abs/2404.11455v1","category":"math.CA"}
{"created":"2024-04-17 14:56:06","title":"Efficient system for bulk characterization of cryogenic CMOS components","abstract":"Semiconductor integrated circuits operated at cryogenic temperature will play an essential role in quantum computing architectures. These can offer equivalent or superior performance to their room-temperature counterparts while enabling a scaling up of the total number of qubits under control. Silicon integrated circuits can be operated at a temperature stage of a cryogenic system where cooling power is sufficient ($\\sim$3.5+ K) to allow for analog signal chain components (e.g. amplifiers and mixers), local signal synthesis, signal digitization, and control logic. A critical stage in cryo-electronics development is the characterization of individual transistor devices in a particular technology node at cryogenic temperatures. This data enables the creation of a process design kit (PDK) to model devices and simulate integrated circuits operating well below the minimum standard temperature ranges covered by foundry-released models (e.g. -55 {\\deg}C). Here, an efficient approach to the characterization of large numbers of components at cryogenic temperature is reported. We developed a system to perform DC measurements with Kelvin sense of individual transistors at 4.2 K using integrated on-die multiplexers, enabling bulk characterization of thousands of devices with no physical change to the measurement setup.","sentences":["Semiconductor integrated circuits operated at cryogenic temperature will play an essential role in quantum computing architectures.","These can offer equivalent or superior performance to their room-temperature counterparts while enabling a scaling up of the total number of qubits under control.","Silicon integrated circuits can be operated at a temperature stage of a cryogenic system where cooling power is sufficient ($\\sim$3.5+ K) to allow for analog signal chain components (e.g. amplifiers and mixers), local signal synthesis, signal digitization, and control logic.","A critical stage in cryo-electronics development is the characterization of individual transistor devices in a particular technology node at cryogenic temperatures.","This data enables the creation of a process design kit (PDK) to model devices and simulate integrated circuits operating well below the minimum standard temperature ranges covered by foundry-released models (e.g. -55 {\\deg}C).","Here, an efficient approach to the characterization of large numbers of components at cryogenic temperature is reported.","We developed a system to perform DC measurements with Kelvin sense of individual transistors at 4.2 K using integrated on-die multiplexers, enabling bulk characterization of thousands of devices with no physical change to the measurement setup."],"url":"http://arxiv.org/abs/2404.11451v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-17 14:53:44","title":"Multi-modalities and non-commutativity/associativity in functorial linear logic: a case study","abstract":"Similar to modal connectives, the exponential ! in intuitionistic linear logic (ILL) is not canonical, in the sense that if $i\\not= j$ then $!^i F\\not\\equiv !^j F$. Intuitively, this means that we can mark the exponential with labels taken from a set I organized in a pre-order $\\preceq$, obtaining (possibly infinitely-many) exponentials ($!^i$ for $i\\in I$).   There are, however, two main differences between multi-modalities in normal modal logics and subexponentials in linear logic. i. substructural behaviour. Subexponentials carry the possibility of having different structural behaviors; ii. nature of modalities. Normal modal logics start from the weakest version, assuming only axiom K, then extensions are considered, by adding other axioms. Exponentials in linear logic \"take for granted\" the behaviors expressed by axioms T and 4.   Regarding (i), originally subexponentials could assume only weakening and contraction axioms, but later non-commutative/non-associative systems allowing commutative/ associative subexponentials were presented.   Concerning (ii), Guerrini et al unified the modal and LL approaches, with the exponentials assuming only the linear version of K, with the possibility of adding modal extensions to it. This discussion was brought to multi-modal case, where subexponentials consider not only the structural axioms for contraction and weakening, but also the subexponential version of axioms {K,4,D,T}.   In this work, we intend to join these two studies. This means that $!^{i}$ can behave classically or not, model associative and commutative systems or not, but also with exponential behaviors different from those in LL. Hence, by assigning different modal axioms one obtains, in a modular way, a class of different substructural modal logics.","sentences":["Similar to modal connectives, the exponential !","in intuitionistic linear logic (ILL) is not canonical, in the sense that if $i\\not= j$ then $!^i F\\not\\equiv !^j","F$. Intuitively, this means that we can mark the exponential with labels taken from a set I organized in a pre-order $\\preceq$, obtaining (possibly infinitely-many) exponentials ($!^i$ for $i\\in I$).   ","There are, however, two main differences between multi-modalities in normal modal logics and subexponentials in linear logic.","i. substructural behaviour.","Subexponentials carry the possibility of having different structural behaviors; ii. nature of modalities.","Normal modal logics start from the weakest version, assuming only axiom K, then extensions are considered, by adding other axioms.","Exponentials in linear logic \"take for granted\" the behaviors expressed by axioms T and 4.   ","Regarding (i), originally subexponentials could assume only weakening and contraction axioms, but later non-commutative/non-associative systems allowing commutative/ associative subexponentials were presented.   ","Concerning (ii), Guerrini et al unified the modal and LL approaches, with the exponentials assuming only the linear version of K, with the possibility of adding modal extensions to it.","This discussion was brought to multi-modal case, where subexponentials consider not only the structural axioms for contraction and weakening, but also the subexponential version of axioms {K,4,D,T}.   ","In this work, we intend to join these two studies.","This means that $!^{i}$ can behave classically or not, model associative and commutative systems or not, but also with exponential behaviors different from those in LL.","Hence, by assigning different modal axioms one obtains, in a modular way, a class of different substructural modal logics."],"url":"http://arxiv.org/abs/2404.11445v1","category":"cs.LO"}
{"created":"2024-04-17 14:53:26","title":"Fidelity decay and error accumulation in quantum volume circuits","abstract":"We provide a comprehensive analysis of fidelity decay and error accumulation in faulty quantum circuit models. We devise an analytical bound to the average fidelity between the desired and faulty output states, accounting for errors that may arise during the implementation of two-qubit gates and multi-qubit permutations. We demonstrate that fidelity decays exponentially with both the number of qubits and circuit depth, and determine the decay rates as a function of the parameterized probabilities of the two types of errors. These decay constants are intricately linked to the connectivity and dimensionality of the processor's architecture. Furthermore, we establish a robust linear relationship between fidelity and the heavy output frequency used in Quantum Volume tests to benchmark quantum processors, under the considered errors protocol. These findings pave the way for predicting fidelity trends in the presence of specific errors and offer insights into the best strategies for increasing Quantum Volume.","sentences":["We provide a comprehensive analysis of fidelity decay and error accumulation in faulty quantum circuit models.","We devise an analytical bound to the average fidelity between the desired and faulty output states, accounting for errors that may arise during the implementation of two-qubit gates and multi-qubit permutations.","We demonstrate that fidelity decays exponentially with both the number of qubits and circuit depth, and determine the decay rates as a function of the parameterized probabilities of the two types of errors.","These decay constants are intricately linked to the connectivity and dimensionality of the processor's architecture.","Furthermore, we establish a robust linear relationship between fidelity and the heavy output frequency used in Quantum Volume tests to benchmark quantum processors, under the considered errors protocol.","These findings pave the way for predicting fidelity trends in the presence of specific errors and offer insights into the best strategies for increasing Quantum Volume."],"url":"http://arxiv.org/abs/2404.11444v1","category":"quant-ph"}
{"created":"2024-04-17 14:51:46","title":"Structural properties of amorphous Na$_3$OCl electrolyte by first-principles and machine learning molecular dynamics","abstract":"Solid-state electrolytes mark a significant leap forward in the field of electrochemical energy storage, offering improved safety and efficiency compared to conventional liquid electrolytes. Among these, antiperovskite electrolytes, particularly those based on Li and Na, have emerged as promising candidates due to their superior ionic conductivity and straightforward synthesis processes. This study focuses on the amorphous phase of antiperovskite Na$_3$OCl, assessing its structural properties through a combination of first-principles molecular dynamics (FPMD) and machine learning interatomic potential (MLIP) simulations. Our comprehensive analysis spans models ranging from 135 to 3645 atoms, allowing for a detailed examination of X-ray and neutron structure factors, total and partial pair correlation functions, coordination numbers, and structural unit distributions. We demonstrate the minimal, albeit partially present, size effects on these structural features and validate the accuracy of the MLIP model in reproducing the intricate details of the amorphous Na$_3$OCl structure described at the FPMD level.","sentences":["Solid-state electrolytes mark a significant leap forward in the field of electrochemical energy storage, offering improved safety and efficiency compared to conventional liquid electrolytes.","Among these, antiperovskite electrolytes, particularly those based on Li and Na, have emerged as promising candidates due to their superior ionic conductivity and straightforward synthesis processes.","This study focuses on the amorphous phase of antiperovskite Na$_3$OCl, assessing its structural properties through a combination of first-principles molecular dynamics (FPMD) and machine learning interatomic potential (MLIP) simulations.","Our comprehensive analysis spans models ranging from 135 to 3645 atoms, allowing for a detailed examination of X-ray and neutron structure factors, total and partial pair correlation functions, coordination numbers, and structural unit distributions.","We demonstrate the minimal, albeit partially present, size effects on these structural features and validate the accuracy of the MLIP model in reproducing the intricate details of the amorphous Na$_3$OCl structure described at the FPMD level."],"url":"http://arxiv.org/abs/2404.11442v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 14:47:05","title":"Excitation Transmission through a non-Hermitian traversable wormhole","abstract":"This study explores the intricate real-time dynamics of a non-Hermitian system composed of two interconnected Sachdev-Ye-Kitaev (SYK) models. A central finding reveals that an excitation initially localized in the right SYK subsystem can be efficiently transmitted to the left subsystem subsequent to the characteristic scrambling time, a phenomenon facilitated by the intrinsic non-Hermitian nature of the system. The defining hallmark of non-Hermiticity is manifest in the asymmetric conveyance of quantum states, with the non-Hermitian parameter functioning as a tunable knob that selectively amplifies or dampens propagation modes on either side. Despite this inherent directional bias in state transfer, the system sustains two distinct phases, analogously likened to black holes and wormholes.","sentences":["This study explores the intricate real-time dynamics of a non-Hermitian system composed of two interconnected Sachdev-Ye-Kitaev (SYK) models.","A central finding reveals that an excitation initially localized in the right SYK subsystem can be efficiently transmitted to the left subsystem subsequent to the characteristic scrambling time, a phenomenon facilitated by the intrinsic non-Hermitian nature of the system.","The defining hallmark of non-Hermiticity is manifest in the asymmetric conveyance of quantum states, with the non-Hermitian parameter functioning as a tunable knob that selectively amplifies or dampens propagation modes on either side.","Despite this inherent directional bias in state transfer, the system sustains two distinct phases, analogously likened to black holes and wormholes."],"url":"http://arxiv.org/abs/2404.11436v1","category":"hep-th"}
{"created":"2024-04-17 14:34:56","title":"CarcassFormer: An End-to-end Transformer-based Framework for Simultaneous Localization, Segmentation and Classification of Poultry Carcass Defect","abstract":"In the food industry, assessing the quality of poultry carcasses during processing is a crucial step. This study proposes an effective approach for automating the assessment of carcass quality without requiring skilled labor or inspector involvement. The proposed system is based on machine learning (ML) and computer vision (CV) techniques, enabling automated defect detection and carcass quality assessment. To this end, an end-to-end framework called CarcassFormer is introduced. It is built upon a Transformer-based architecture designed to effectively extract visual representations while simultaneously detecting, segmenting, and classifying poultry carcass defects. Our proposed framework is capable of analyzing imperfections resulting from production and transport welfare issues, as well as processing plant stunner, scalder, picker, and other equipment malfunctions. To benchmark the framework, a dataset of 7,321 images was initially acquired, which contained both single and multiple carcasses per image. In this study, the performance of the CarcassFormer system is compared with other state-of-the-art (SOTA) approaches for both classification, detection, and segmentation tasks. Through extensive quantitative experiments, our framework consistently outperforms existing methods, demonstrating remarkable improvements across various evaluation metrics such as AP, AP@50, and AP@75. Furthermore, the qualitative results highlight the strengths of CarcassFormer in capturing fine details, including feathers, and accurately localizing and segmenting carcasses with high precision. To facilitate further research and collaboration, the pre-trained model and source code of CarcassFormer is available for research purposes at: \\url{https://github.com/UARK-AICV/CarcassFormer}.","sentences":["In the food industry, assessing the quality of poultry carcasses during processing is a crucial step.","This study proposes an effective approach for automating the assessment of carcass quality without requiring skilled labor or inspector involvement.","The proposed system is based on machine learning (ML) and computer vision (CV) techniques, enabling automated defect detection and carcass quality assessment.","To this end, an end-to-end framework called CarcassFormer is introduced.","It is built upon a Transformer-based architecture designed to effectively extract visual representations while simultaneously detecting, segmenting, and classifying poultry carcass defects.","Our proposed framework is capable of analyzing imperfections resulting from production and transport welfare issues, as well as processing plant stunner, scalder, picker, and other equipment malfunctions.","To benchmark the framework, a dataset of 7,321 images was initially acquired, which contained both single and multiple carcasses per image.","In this study, the performance of the CarcassFormer system is compared with other state-of-the-art (SOTA) approaches for both classification, detection, and segmentation tasks.","Through extensive quantitative experiments, our framework consistently outperforms existing methods, demonstrating remarkable improvements across various evaluation metrics such as AP, AP@50, and AP@75.","Furthermore, the qualitative results highlight the strengths of CarcassFormer in capturing fine details, including feathers, and accurately localizing and segmenting carcasses with high precision.","To facilitate further research and collaboration, the pre-trained model and source code of CarcassFormer is available for research purposes at: \\url{https://github.com/UARK-AICV/CarcassFormer}."],"url":"http://arxiv.org/abs/2404.11429v1","category":"cs.CV"}
{"created":"2024-04-17 14:31:11","title":"Spatially resolved lock-in micro-thermography (SR-LIT): A tensor analysis-enhanced method for anisotropic thermal characterization","abstract":"While high-throughput (HT) computations have streamlined the discovery of promising new materials, experimental characterization remains challenging and time-consuming. One significant bottleneck is the lack of an HT thermal characterization technique capable of analyzing advanced materials exhibiting varying surface roughness and in-plane anisotropy. To tackle these challenges, we introduce spatially resolved lock-in micro-thermography (SR-LIT), an innovative technique enhanced by tensor analysis for optical thermal characterization. Our comprehensive analysis and experimental findings showcase notable advancements: We present a novel tensor-based methodology that surpasses the limitations of vector-based analysis prevalent in existing techniques, significantly enhancing the characterization of arbitrary in-plane anisotropic thermal conductivity tensors. On the instrumental side, we introduce a straightforward camera-based detection system that, when combined with the tensor-based methodology, enables HT thermal measurements. This technique requires minimal sample preparation and enables the determination of the entire in-plane thermal conductivity tensor with a single data acquisition lasting under 40 seconds, demonstrating a time efficiency over 90 times superior to state-of-the-art HT thermology. Additionally, our method accommodates millimeter-sized samples with poor surface finish, tolerating surface roughness up to 3.5 {\\mu}m. These features highlight an innovative approach to realizing HT and accurate thermal characterization across various research areas and real-world applications.","sentences":["While high-throughput (HT) computations have streamlined the discovery of promising new materials, experimental characterization remains challenging and time-consuming.","One significant bottleneck is the lack of an HT thermal characterization technique capable of analyzing advanced materials exhibiting varying surface roughness and in-plane anisotropy.","To tackle these challenges, we introduce spatially resolved lock-in micro-thermography (SR-LIT), an innovative technique enhanced by tensor analysis for optical thermal characterization.","Our comprehensive analysis and experimental findings showcase notable advancements: We present a novel tensor-based methodology that surpasses the limitations of vector-based analysis prevalent in existing techniques, significantly enhancing the characterization of arbitrary in-plane anisotropic thermal conductivity tensors.","On the instrumental side, we introduce a straightforward camera-based detection system that, when combined with the tensor-based methodology, enables HT thermal measurements.","This technique requires minimal sample preparation and enables the determination of the entire in-plane thermal conductivity tensor with a single data acquisition lasting under 40 seconds, demonstrating a time efficiency over 90 times superior to state-of-the-art","HT thermology.","Additionally, our method accommodates millimeter-sized samples with poor surface finish, tolerating surface roughness up to 3.5 {\\mu}m.","These features highlight an innovative approach to realizing HT and accurate thermal characterization across various research areas and real-world applications."],"url":"http://arxiv.org/abs/2404.11424v1","category":"physics.app-ph"}
{"created":"2024-04-17 14:26:24","title":"Excitonic circular dichroism in boron-nitrogen clusters decorated graphene","abstract":"Within the first principle calculations, we propose a boron and nitrogen cluster incorporated graphene system for efficient valley polarization. The broken spatial inversion symmetry results in high Berry curvature at K and K' valleys of the hexagonal Brillouin zone in this semiconducting system. The consideration of excitonic quasiparticles within GW approximation along with their scattering processes within many-body Bethe-Salpeter equation gives rise to an optical gap of 1.72 eV with an excitonic binding energy of 0.65 eV. Owing to the negligible intervalley scattering, the electrons in opposite valleys are selectively excited by left- and right-handed circular polarized lights, as evident from the oscillator strength calculations. Therefore, this system can exhibit circular-dichroism valley Hall effect in the presence of the in-plane electric field. Moreover, such excitonic qubits can be exploited for information processing.","sentences":["Within the first principle calculations, we propose a boron and nitrogen cluster incorporated graphene system for efficient valley polarization.","The broken spatial inversion symmetry results in high Berry curvature at K and K' valleys of the hexagonal Brillouin zone in this semiconducting system.","The consideration of excitonic quasiparticles within GW approximation along with their scattering processes within many-body Bethe-Salpeter equation gives rise to an optical gap of 1.72 eV with an excitonic binding energy of 0.65 eV. Owing to the negligible intervalley scattering, the electrons in opposite valleys are selectively excited by left- and right-handed circular polarized lights, as evident from the oscillator strength calculations.","Therefore, this system can exhibit circular-dichroism valley Hall effect in the presence of the in-plane electric field.","Moreover, such excitonic qubits can be exploited for information processing."],"url":"http://arxiv.org/abs/2404.11421v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 14:23:28","title":"SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping","abstract":"We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment. Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences. We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy. Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views. While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry. In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces. Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution. We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy.","sentences":["We present SLAIM - Simultaneous Localization and Implicit Mapping.","We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance.","Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms.","NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment.","Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences.","We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy.","Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views.","While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry.","In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces.","Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution.","We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy."],"url":"http://arxiv.org/abs/2404.11419v1","category":"cs.CV"}
{"created":"2024-04-17 14:10:36","title":"Batch Array Codes","abstract":"Batch codes are a type of codes specifically designed for coded distributed storage systems and private information retrieval protocols. These codes have got much attention in recent years due to their ability to enable efficient and secure storage in distributed systems.   In this paper, we study an array code version of the batch codes, which is called the \\emph{batch array code} (BAC). Under the setting of BAC, each node stores a bucket containing multiple code symbols and responds with a locally computed linear combination of the symbols in its bucket during the recovery of a requested symbol. We demonstrate that BACs can support the same type of requests as the original batch codes but with reduced redundancy. Specifically, we establish information theoretic lower bounds on the code lengths and provide several code constructions that confirm the tightness of the lower bounds for certain parameter regimes.","sentences":["Batch codes are a type of codes specifically designed for coded distributed storage systems and private information retrieval protocols.","These codes have got much attention in recent years due to their ability to enable efficient and secure storage in distributed systems.   ","In this paper, we study an array code version of the batch codes, which is called the \\emph{batch array code} (BAC).","Under the setting of BAC, each node stores a bucket containing multiple code symbols and responds with a locally computed linear combination of the symbols in its bucket during the recovery of a requested symbol.","We demonstrate that BACs can support the same type of requests as the original batch codes but with reduced redundancy.","Specifically, we establish information theoretic lower bounds on the code lengths and provide several code constructions that confirm the tightness of the lower bounds for certain parameter regimes."],"url":"http://arxiv.org/abs/2404.11409v1","category":"cs.IT"}
{"created":"2024-04-17 14:03:42","title":"In situ sound absorption estimation with the discrete complex image source method","abstract":"Estimating the sound absorption in situ relies on accurately describing the measured sound field. Evidence suggests that modeling the reflection of impinging spherical waves is important, especially for compact measurement systems. This article proposes a method for estimating the sound absorption coefficient of a material sample by mapping the sound pressure, measured by a microphone array, to a distribution of monopoles along a line in the complex plane. The proposed method is compared to modeling the sound field as a superposition of two sources (a monopole and an image source). The obtained inverse problems are solved with Tikhonov regularization, with automatic choice of the regularization parameter by the L-curve criterion. The sound absorption measurement is tested with simulations of the sound field above infinite and finite porous absorbers. The approaches are compared to the plane-wave absorption coefficient and the one obtained by spherical wave incidence. Experimental analysis of two porous samples and one resonant absorber is also carried out in situ. Four arrays were tested with an increasing aperture and number of sensors. It was demonstrated that measurements are feasible even with an array with only a few microphones. The discretization of the integral equation led to a more accurate reconstruction of the sound pressure and particle velocity at the sample's surface. The resulting absorption coefficient agrees with the one obtained for spherical wave incidence, indicating that including more monopoles along the complex line is an essential feature of the sound field.","sentences":["Estimating the sound absorption in situ relies on accurately describing the measured sound field.","Evidence suggests that modeling the reflection of impinging spherical waves is important, especially for compact measurement systems.","This article proposes a method for estimating the sound absorption coefficient of a material sample by mapping the sound pressure, measured by a microphone array, to a distribution of monopoles along a line in the complex plane.","The proposed method is compared to modeling the sound field as a superposition of two sources (a monopole and an image source).","The obtained inverse problems are solved with Tikhonov regularization, with automatic choice of the regularization parameter by the L-curve criterion.","The sound absorption measurement is tested with simulations of the sound field above infinite and finite porous absorbers.","The approaches are compared to the plane-wave absorption coefficient and the one obtained by spherical wave incidence.","Experimental analysis of two porous samples and one resonant absorber is also carried out in situ.","Four arrays were tested with an increasing aperture and number of sensors.","It was demonstrated that measurements are feasible even with an array with only a few microphones.","The discretization of the integral equation led to a more accurate reconstruction of the sound pressure and particle velocity at the sample's surface.","The resulting absorption coefficient agrees with the one obtained for spherical wave incidence, indicating that including more monopoles along the complex line is an essential feature of the sound field."],"url":"http://arxiv.org/abs/2404.11399v1","category":"eess.AS"}
{"created":"2024-04-17 13:59:45","title":"Convergence rate and uniform Lipschitz estimate in periodic homogenization of high-contrast elliptic systems","abstract":"We consider the Dirichlet problem for elliptic systems with periodically distributed inclusions whose conduction parameter exhibits a significant contrast compared to the background media. We develop a unified method to quantify the convergence rates both as the periodicity of inclusions tends to zero and as the parameter approaches either zero or infinity. Based on the obtained convergence rates and a Campanato-type scheme, we also derive the regularity estimates that are uniform both in the periodicity and the contrast.","sentences":["We consider the Dirichlet problem for elliptic systems with periodically distributed inclusions whose conduction parameter exhibits a significant contrast compared to the background media.","We develop a unified method to quantify the convergence rates both as the periodicity of inclusions tends to zero and as the parameter approaches either zero or infinity.","Based on the obtained convergence rates and a Campanato-type scheme, we also derive the regularity estimates that are uniform both in the periodicity and the contrast."],"url":"http://arxiv.org/abs/2404.11396v1","category":"math.AP"}
{"created":"2024-04-17 13:52:04","title":"The extended versions of the noncommutative KP and mKP equations and Miura transformation","abstract":"Extended versions of the noncommutative(nc) KP equation and the nc mKP equation are constructed in a unified way, for which two types of quasideterminant solutions are also presented. In commutative setting, the quasideterminant solutions provide the known and unknown Wronskian and Grammian solutions for the bilinear KP equation with self-consistent sources and the bilinear mKP equation with self-consistent sources, respectively. Miura transformation is established for the extended nc KP and nc mKP equations.","sentences":["Extended versions of the noncommutative(nc) KP equation and the nc mKP equation are constructed in a unified way, for which two types of quasideterminant solutions are also presented.","In commutative setting, the quasideterminant solutions provide the known and unknown Wronskian and Grammian solutions for the bilinear KP equation with self-consistent sources and the bilinear mKP equation with self-consistent sources, respectively.","Miura transformation is established for the extended nc KP and nc mKP equations."],"url":"http://arxiv.org/abs/2404.11391v1","category":"nlin.SI"}
{"created":"2024-04-17 13:51:37","title":"A $\u03c4$-preconditioner for space fractional diffusion equation with non-separable variable coefficients","abstract":"In this paper, we study a $\\tau$-matrix approximation based preconditioner for the linear systems arising from discretization of unsteady state Riesz space fractional diffusion equation with non-separable variable coefficients. The structure of coefficient matrices of the linear systems is identity plus summation of diagonal-times-multilevel-Toeplitz matrices. In our preconditioning technique, the diagonal matrices are approximated by scalar identity matrices and the Toeplitz matrices are approximated by {\\tau}-matrices (a type of matrices diagonalizable by discrete sine transforms). The proposed preconditioner is fast invertible through the fast sine transform (FST) algorithm. Theoretically, we show that the GMRES solver for the preconditioned systems has an optimal convergence rate (a convergence rate independent of discretization stepsizes). To the best of our knowledge, this is the first preconditioning method with the optimal convergence rate for the variable-coefficients space fractional diffusion equation. Numerical results are reported to demonstrate the efficiency of the proposed method.","sentences":["In this paper, we study a $\\tau$-matrix approximation based preconditioner for the linear systems arising from discretization of unsteady state Riesz space fractional diffusion equation with non-separable variable coefficients.","The structure of coefficient matrices of the linear systems is identity plus summation of diagonal-times-multilevel-Toeplitz matrices.","In our preconditioning technique, the diagonal matrices are approximated by scalar identity matrices and the Toeplitz matrices are approximated by {\\tau}-matrices (a type of matrices diagonalizable by discrete sine transforms).","The proposed preconditioner is fast invertible through the fast sine transform (FST) algorithm.","Theoretically, we show that the GMRES solver for the preconditioned systems has an optimal convergence rate (a convergence rate independent of discretization stepsizes).","To the best of our knowledge, this is the first preconditioning method with the optimal convergence rate for the variable-coefficients space fractional diffusion equation.","Numerical results are reported to demonstrate the efficiency of the proposed method."],"url":"http://arxiv.org/abs/2404.11390v1","category":"math.NA"}
{"created":"2024-04-17 13:51:20","title":"Finding $d$-Cuts in Graphs of Bounded Diameter, Graphs of Bounded Radius and $H$-Free Graphs","abstract":"The $d$-Cut problem is to decide if a graph has an edge cut such that each vertex has at most $d$ neighbours at the opposite side of the cut. If $d=1$, we obtain the intensively studied Matching Cut problem. The $d$-Cut problem has been studied as well, but a systematic study for special graph classes was lacking. We initiate such a study and consider classes of bounded diameter, bounded radius and $H$-free graphs. We prove that for all $d\\geq 2$, $d$-Cut is polynomial-time solvable for graphs of diameter $2$, $(P_3+P_4)$-free graphs and $P_5$-free graphs. These results extend known results for $d=1$. However, we also prove several NP-hardness results for $d$-Cut that contrast known polynomial-time results for $d=1$. Our results lead to full dichotomies for bounded diameter and bounded radius and to almost-complete dichotomies for $H$-free graphs.","sentences":["The $d$-Cut problem is to decide if a graph has an edge cut such that each vertex has at most $d$ neighbours at the opposite side of the cut.","If $d=1$, we obtain the intensively studied Matching Cut problem.","The $d$-Cut problem has been studied as well, but a systematic study for special graph classes was lacking.","We initiate such a study and consider classes of bounded diameter, bounded radius and $H$-free graphs.","We prove that for all $d\\geq 2$, $d$-Cut is polynomial-time solvable for graphs of diameter $2$, $(P_3+P_4)$-free graphs and $P_5$-free graphs.","These results extend known results for $d=1$. However, we also prove several NP-hardness results for $d$-Cut that contrast known polynomial-time results for $d=1$. Our results lead to full dichotomies for bounded diameter and bounded radius and to almost-complete dichotomies for $H$-free graphs."],"url":"http://arxiv.org/abs/2404.11389v1","category":"math.CO"}
{"created":"2024-04-17 13:47:02","title":"Modified mean field ansatz for charged polarons in a Bose-Einstein condensate","abstract":"Ionic Bose polarons are quantum entities emerging from the interaction between an ion and a Bose-Einstein condensate (BEC), featuring long-ranged interactions that can compete with the gas healing length. This can result in strong interparticle correlations and enhancement of gas density around the ion. One possible approach to describe this complex system with high accuracy relies on numerical treatment such as the quantum Monte Carlo (QMC) techniques. Nevertheless, it is computationally very expensive and does not easily allow to study the system dynamics. On the other hand, a mean-field based variational ansatz in the co-moving frame can capture a sizeable change in the gas density. We apply it to the case of regularized ion-atom potential and find that it qualitatively reproduces the full numerical results. In addition, we also study the system of two pinned ions, focusing on their effective interaction induced by the bath. This approach seems to be promising for studying transport and nonequilibrium dynamics of charged (bi)polarons in condensed media.","sentences":["Ionic Bose polarons are quantum entities emerging from the interaction between an ion and a Bose-Einstein condensate (BEC), featuring long-ranged interactions that can compete with the gas healing length.","This can result in strong interparticle correlations and enhancement of gas density around the ion.","One possible approach to describe this complex system with high accuracy relies on numerical treatment such as the quantum Monte Carlo (QMC) techniques.","Nevertheless, it is computationally very expensive and does not easily allow to study the system dynamics.","On the other hand, a mean-field based variational ansatz in the co-moving frame can capture a sizeable change in the gas density.","We apply it to the case of regularized ion-atom potential and find that it qualitatively reproduces the full numerical results.","In addition, we also study the system of two pinned ions, focusing on their effective interaction induced by the bath.","This approach seems to be promising for studying transport and nonequilibrium dynamics of charged (bi)polarons in condensed media."],"url":"http://arxiv.org/abs/2404.11387v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-17 13:34:27","title":"Hexagonal quasiperiodic tilings as decorations of periodic lattices","abstract":"Symmetry sharing facilitates coherent interfaces which can transition from periodic to quasiperiodic structures. Motivated by the design and construction of such systems, we present hexagonal quasiperiodic tilings with a single edge-length which can be considered as decorations of a periodic lattice. We introduce these tilings by modifying an existing family of golden-mean trigonal and hexagonal tilings, and discuss their properties in terms of this wider family. Then, we show how the vertices of these new systems can be considered as decorations or sublattice sets of a periodic triangular lattice. We conclude by simulating a simple Ising model on one of these decorations, and compare this system to a triangular lattice with random defects.","sentences":["Symmetry sharing facilitates coherent interfaces which can transition from periodic to quasiperiodic structures.","Motivated by the design and construction of such systems, we present hexagonal quasiperiodic tilings with a single edge-length which can be considered as decorations of a periodic lattice.","We introduce these tilings by modifying an existing family of golden-mean trigonal and hexagonal tilings, and discuss their properties in terms of this wider family.","Then, we show how the vertices of these new systems can be considered as decorations or sublattice sets of a periodic triangular lattice.","We conclude by simulating a simple Ising model on one of these decorations, and compare this system to a triangular lattice with random defects."],"url":"http://arxiv.org/abs/2404.11378v1","category":"cond-mat.other"}
{"created":"2024-04-17 13:33:53","title":"A New Algorithm With Lower Complexity for Bilevel Optimization","abstract":"Many stochastic algorithms have been proposed to solve the bilevel optimization problem, where the lower level function is strongly convex and the upper level value function is nonconvex. In particular, exising Hessian inverse-free algorithms that utilize momentum recursion or variance reduction technqiues can reach an $\\epsilon$-stationary point with a complexity of $\\tilde{O}(\\epsilon^{-1.5})$ under usual smoothness conditions. However, $\\tilde{O}(\\epsilon^{-1.5})$ is a complexity higher than $O(\\epsilon^{-1.5})$. How to make a Hessian inverse-free algorithm achieve the complexity of $O(\\epsilon^{-1.5})$ under usual smoothness conditions remains an unresolved problem. In this paper, we propose a new Hessian inverse-free algorithm based on the projected stochastic gradient descent method and variance reduction technique of SPIDER. This algorithm can achieve a complexity of $O(\\epsilon^{-1.5})$ under usual smoothness conditions whether it runs in a fully single loop or double loop structure. Finally, we validate our theoretical results through synthetic experiments and demonstrate the efficiency of our algorithm in some machine learning applications.","sentences":["Many stochastic algorithms have been proposed to solve the bilevel optimization problem, where the lower level function is strongly convex and the upper level value function is nonconvex.","In particular, exising Hessian inverse-free algorithms that utilize momentum recursion or variance reduction technqiues can reach an $\\epsilon$-stationary point with a complexity of $\\tilde{O}(\\epsilon^{-1.5})$ under usual smoothness conditions.","However, $\\tilde{O}(\\epsilon^{-1.5})$ is a complexity higher than $O(\\epsilon^{-1.5})$. How to make a Hessian inverse-free algorithm achieve the complexity of $O(\\epsilon^{-1.5})$ under usual smoothness conditions remains an unresolved problem.","In this paper, we propose a new Hessian inverse-free algorithm based on the projected stochastic gradient descent method and variance reduction technique of SPIDER.","This algorithm can achieve a complexity of $O(\\epsilon^{-1.5})$ under usual smoothness conditions whether it runs in a fully single loop or double loop structure.","Finally, we validate our theoretical results through synthetic experiments and demonstrate the efficiency of our algorithm in some machine learning applications."],"url":"http://arxiv.org/abs/2404.11377v1","category":"math.OC"}
{"created":"2024-04-17 13:32:05","title":"Tensor Factorisation for Polypharmacy Side Effect Prediction","abstract":"Adverse reactions caused by drug combinations are an increasingly common phenomenon, making their accurate prediction an important challenge in modern medicine. However, the polynomial nature of this problem renders lab-based identification of adverse reactions insufficient. Dozens of computational approaches have therefore been proposed for the task in recent years, with varying degrees of success. One group of methods that has seemingly been under-utilised in this area is tensor factorisation, despite their clear applicability to this type of data. In this work, we apply three such models to a benchmark dataset in order to compare them against established techniques. We find, in contrast to previous reports, that for this task tensor factorisation models are competitive with state-of-the-art graph neural network models and we recommend that future work in this field considers cheaper methods with linear complexity before running costly deep learning processes.","sentences":["Adverse reactions caused by drug combinations are an increasingly common phenomenon, making their accurate prediction an important challenge in modern medicine.","However, the polynomial nature of this problem renders lab-based identification of adverse reactions insufficient.","Dozens of computational approaches have therefore been proposed for the task in recent years, with varying degrees of success.","One group of methods that has seemingly been under-utilised in this area is tensor factorisation, despite their clear applicability to this type of data.","In this work, we apply three such models to a benchmark dataset in order to compare them against established techniques.","We find, in contrast to previous reports, that for this task tensor factorisation models are competitive with state-of-the-art graph neural network models and we recommend that future work in this field considers cheaper methods with linear complexity before running costly deep learning processes."],"url":"http://arxiv.org/abs/2404.11374v1","category":"cs.LG"}
{"created":"2024-04-17 13:31:26","title":"The boundary of bordified Outer space","abstract":"We study the boundary of the \"Jewel space\" $\\mathcal J_n$ constructed in arXiv:1709.01296. This is an equivariant deformation retract of Outer space $CV_n$ on which $Out(F_n)$ acts properly and cocompactly, and is homeomorphic to the Bestvina-Feighn bordification of $CV_n$. In the current paper we analyze the structure of the boundary of $\\mathcal J_n$. We then use the desctiption of the simplicial closure $CV_n^*$ as the sphere complex of a connected sum of $n$ copies of $S^1\\times S^2$ to prove that this boundary is homotopy equivalent to the subcomplex of $CV_n^*$ spanned by vertices at infinity.","sentences":["We study the boundary of the \"Jewel space\" $\\mathcal J_n$ constructed in arXiv:1709.01296.","This is an equivariant deformation retract of Outer space $CV_n$ on which $Out(F_n)$ acts properly and cocompactly, and is homeomorphic to the Bestvina-Feighn bordification of $CV_n$. In the current paper we analyze the structure of the boundary of $\\mathcal J_n$. We then use the desctiption of the simplicial closure $CV_n^*$ as the sphere complex of a connected sum of $n$ copies of $S^1\\times S^2$ to prove that this boundary is homotopy equivalent to the subcomplex of $CV_n^*$ spanned by vertices at infinity."],"url":"http://arxiv.org/abs/2404.11371v1","category":"math.GR"}
{"created":"2024-04-17 13:23:11","title":"Effective one-dimension reduction of multi-compartment complex systems dynamics","abstract":"A broad class of systems, including ecological, epidemiological, and sociological ones, are characterized by populations of individuals assigned to specific categories, e.g., a chemical species, an opinion or an epidemic state, that are modeled as compartments. Due to interactions and intrinsic dynamics, individuals are allowed to change category, leading to concentrations varying over time with complex behavior, typical of reaction-diffusion systems. While compartmental modeling provides a powerful framework for studying the dynamics of such populations and describe the spatiotemporal evolution of a system, it mostly relies on deterministic mean-field descriptions to deal with systems with many degrees of freedom. Here, we propose a method to alleviate some of the limitations of compartmental models by capitalizing on tools originating from quantum physics to systematically reduce multi-dimensional systems to an effective one-dimensional representation. Using this reduced system, we are able to not only investigate the mean-field dynamics and their critical behavior, but we can additionally study stochastic representations that capture fundamental features of the system. We demonstrate the validity of our formalism by studying the critical behavior of models widely adopted to study epidemic, ecological and economic systems.","sentences":["A broad class of systems, including ecological, epidemiological, and sociological ones, are characterized by populations of individuals assigned to specific categories, e.g., a chemical species, an opinion or an epidemic state, that are modeled as compartments.","Due to interactions and intrinsic dynamics, individuals are allowed to change category, leading to concentrations varying over time with complex behavior, typical of reaction-diffusion systems.","While compartmental modeling provides a powerful framework for studying the dynamics of such populations and describe the spatiotemporal evolution of a system, it mostly relies on deterministic mean-field descriptions to deal with systems with many degrees of freedom.","Here, we propose a method to alleviate some of the limitations of compartmental models by capitalizing on tools originating from quantum physics to systematically reduce multi-dimensional systems to an effective one-dimensional representation.","Using this reduced system, we are able to not only investigate the mean-field dynamics and their critical behavior, but we can additionally study stochastic representations that capture fundamental features of the system.","We demonstrate the validity of our formalism by studying the critical behavior of models widely adopted to study epidemic, ecological and economic systems."],"url":"http://arxiv.org/abs/2404.11366v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-17 13:16:42","title":"Open-system eigenstate thermalization in a noninteracting integrable model","abstract":"We study thermalization in isolated quantum systems from an open quantum systems perspective. We argue that for a small system connected to a macroscopic bath, the system observables are thermal if the combined system-bath configuration is in an eigenstate of its Hamiltonian, even for fully integrable models (unless thermalization is suppressed by localization due to strong coupling). We illustrate our claim for a single fermionic level coupled to a noninteracting fermionic bath. We further show that upon quenching the system Hamiltonian, the system occupancy relaxes to the thermal value corresponding to the new Hamiltonian. Finally, we demonstrate that system thermalization also arises for a system coupled to a bath initialized in a typical eigenstate of its Hamiltonian. Our findings show that chaos and nonintegrability are not the sole drivers of thermalization and complementary approaches are needed to offer a more comprehensive understanding of how statistical mechanics emerges.","sentences":["We study thermalization in isolated quantum systems from an open quantum systems perspective.","We argue that for a small system connected to a macroscopic bath, the system observables are thermal if the combined system-bath configuration is in an eigenstate of its Hamiltonian, even for fully integrable models (unless thermalization is suppressed by localization due to strong coupling).","We illustrate our claim for a single fermionic level coupled to a noninteracting fermionic bath.","We further show that upon quenching the system Hamiltonian, the system occupancy relaxes to the thermal value corresponding to the new Hamiltonian.","Finally, we demonstrate that system thermalization also arises for a system coupled to a bath initialized in a typical eigenstate of its Hamiltonian.","Our findings show that chaos and nonintegrability are not the sole drivers of thermalization and complementary approaches are needed to offer a more comprehensive understanding of how statistical mechanics emerges."],"url":"http://arxiv.org/abs/2404.11360v1","category":"quant-ph"}
{"created":"2024-04-17 13:12:14","title":"Detector Collapse: Backdooring Object Detection to Catastrophic Overload or Blindness","abstract":"Object detection tasks, crucial in safety-critical systems like autonomous driving, focus on pinpointing object locations. These detectors are known to be susceptible to backdoor attacks. However, existing backdoor techniques have primarily been adapted from classification tasks, overlooking deeper vulnerabilities specific to object detection. This paper is dedicated to bridging this gap by introducing Detector Collapse} (DC), a brand-new backdoor attack paradigm tailored for object detection. DC is designed to instantly incapacitate detectors (i.e., severely impairing detector's performance and culminating in a denial-of-service). To this end, we develop two innovative attack schemes: Sponge for triggering widespread misidentifications and Blinding for rendering objects invisible. Remarkably, we introduce a novel poisoning strategy exploiting natural objects, enabling DC to act as a practical backdoor in real-world environments. Our experiments on different detectors across several benchmarks show a significant improvement ($\\sim$10\\%-60\\% absolute and $\\sim$2-7$\\times$ relative) in attack efficacy over state-of-the-art attacks.","sentences":["Object detection tasks, crucial in safety-critical systems like autonomous driving, focus on pinpointing object locations.","These detectors are known to be susceptible to backdoor attacks.","However, existing backdoor techniques have primarily been adapted from classification tasks, overlooking deeper vulnerabilities specific to object detection.","This paper is dedicated to bridging this gap by introducing Detector Collapse} (DC), a brand-new backdoor attack paradigm tailored for object detection.","DC is designed to instantly incapacitate detectors (i.e., severely impairing detector's performance and culminating in a denial-of-service).","To this end, we develop two innovative attack schemes: Sponge for triggering widespread misidentifications and Blinding for rendering objects invisible.","Remarkably, we introduce a novel poisoning strategy exploiting natural objects, enabling DC to act as a practical backdoor in real-world environments.","Our experiments on different detectors across several benchmarks show a significant improvement ($\\sim$10\\%-60\\% absolute and $\\sim$2-7$\\times$ relative) in attack efficacy over state-of-the-art attacks."],"url":"http://arxiv.org/abs/2404.11357v1","category":"cs.CV"}
{"created":"2024-04-17 13:09:29","title":"Transition between scattering regimes of 2D electron transport","abstract":"We examine 2D electron transport through a long narrow channel driven by an external electric field in presence of diffusive boundary scattering. At zero temperature, we derive an analytical solution of the transition from ballistic to diffusive transport if we increase the bulk disorder strength. This crossover yields characteristic current density profiles. Furthermore, we illustrate the current density in the transition from ballistic to hydrodynamic transport. This corresponds to the Gurzhi effect in the resistivity. We also study the influence of finite temperature on current densities and average current in this system. In particular, we analyze how different scaling laws of scattering with respect to temperature affect the current profile along the channel.","sentences":["We examine 2D electron transport through a long narrow channel driven by an external electric field in presence of diffusive boundary scattering.","At zero temperature, we derive an analytical solution of the transition from ballistic to diffusive transport if we increase the bulk disorder strength.","This crossover yields characteristic current density profiles.","Furthermore, we illustrate the current density in the transition from ballistic to hydrodynamic transport.","This corresponds to the Gurzhi effect in the resistivity.","We also study the influence of finite temperature on current densities and average current in this system.","In particular, we analyze how different scaling laws of scattering with respect to temperature affect the current profile along the channel."],"url":"http://arxiv.org/abs/2404.11353v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-17 13:09:15","title":"Accelerating Geo-distributed Machine Learning with Network-Aware Adaptive Tree and Auxiliary Route","abstract":"Distributed machine learning is becoming increasingly popular for geo-distributed data analytics, facilitating the collaborative analysis of data scattered across data centers in different regions. This paradigm eliminates the need for centralizing sensitive raw data in one location but faces the significant challenge of high parameter synchronization delays, which stems from the constraints of bandwidth-limited, heterogeneous, and fluctuating wide-area networks. Prior research has focused on optimizing the synchronization topology, evolving from starlike to tree-based structures. However, these solutions typically depend on regular tree structures and lack an adequate topology metric, resulting in limited improvements. This paper proposes NetStorm, an adaptive and highly efficient communication scheduler designed to speed up parameter synchronization across geo-distributed data centers. First, it establishes an effective metric for optimizing a multi-root FAPT synchronization topology. Second, a network awareness module is developed to acquire network knowledge, aiding in topology decisions. Third, a multipath auxiliary transmission mechanism is introduced to enhance network awareness and facilitate multipath transmissions. Lastly, we design policy consistency protocols to guarantee seamless updates of transmission policies. Empirical results demonstrate that NetStorm significantly outperforms distributed training systems like MXNET, MLNET, and TSEngine, with a speedup of 6.5~9.2 times over MXNET.","sentences":["Distributed machine learning is becoming increasingly popular for geo-distributed data analytics, facilitating the collaborative analysis of data scattered across data centers in different regions.","This paradigm eliminates the need for centralizing sensitive raw data in one location but faces the significant challenge of high parameter synchronization delays, which stems from the constraints of bandwidth-limited, heterogeneous, and fluctuating wide-area networks.","Prior research has focused on optimizing the synchronization topology, evolving from starlike to tree-based structures.","However, these solutions typically depend on regular tree structures and lack an adequate topology metric, resulting in limited improvements.","This paper proposes NetStorm, an adaptive and highly efficient communication scheduler designed to speed up parameter synchronization across geo-distributed data centers.","First, it establishes an effective metric for optimizing a multi-root FAPT synchronization topology.","Second, a network awareness module is developed to acquire network knowledge, aiding in topology decisions.","Third, a multipath auxiliary transmission mechanism is introduced to enhance network awareness and facilitate multipath transmissions.","Lastly, we design policy consistency protocols to guarantee seamless updates of transmission policies.","Empirical results demonstrate that NetStorm significantly outperforms distributed training systems like MXNET, MLNET, and TSEngine, with a speedup of 6.5~9.2 times over MXNET."],"url":"http://arxiv.org/abs/2404.11352v1","category":"cs.DC"}
{"created":"2024-04-17 13:08:49","title":"Circular Distribution of Agents using Convex Layers","abstract":"This paper considers the problem of conflict-free distribution of agents on a circular periphery encompassing all agents. The two key elements of the proposed policy include the construction of a set of convex layers (nested convex polygons) using the initial positions of the agents, and a novel search space region for each of the agents. The search space for an agent on a convex layer is defined as the region enclosed between the lines passing through the agent's position and normal to its supporting edges. Guaranteeing collision-free paths, a goal assignment policy designates a unique goal position within the search space of an agent. In contrast to the existing literature, this work presents a one-shot, collision-free solution to the circular distribution problem by utilizing only the initial positions of the agents. Illustrative examples demonstrate the effectiveness of the proposed policy.","sentences":["This paper considers the problem of conflict-free distribution of agents on a circular periphery encompassing all agents.","The two key elements of the proposed policy include the construction of a set of convex layers (nested convex polygons) using the initial positions of the agents, and a novel search space region for each of the agents.","The search space for an agent on a convex layer is defined as the region enclosed between the lines passing through the agent's position and normal to its supporting edges.","Guaranteeing collision-free paths, a goal assignment policy designates a unique goal position within the search space of an agent.","In contrast to the existing literature, this work presents a one-shot, collision-free solution to the circular distribution problem by utilizing only the initial positions of the agents.","Illustrative examples demonstrate the effectiveness of the proposed policy."],"url":"http://arxiv.org/abs/2404.11351v1","category":"cs.MA"}
{"created":"2024-04-17 13:00:23","title":"When time delays and phase lags are not the same: higher-order phase reduction unravels delay-induced synchronization in oscillator networks","abstract":"Coupled oscillators with time-delayed network interactions are critical to understand synchronization phenomena in many physical systems. Phase reductions to finite-dimensional phase oscillator networks allow for their explicit analysis. However, first-order phase reductions - where delays correspond to phase lags - fail to capture the delay-dependence of synchronization. We develop a systematic approach to derive phase reductions for delay-coupled oscillators to arbitrary order. Already the second-order reduction can predict delay-dependent (bi-)stability of synchronized states as demonstrated for Stuart-Landau oscillators.","sentences":["Coupled oscillators with time-delayed network interactions are critical to understand synchronization phenomena in many physical systems.","Phase reductions to finite-dimensional phase oscillator networks allow for their explicit analysis.","However, first-order phase reductions - where delays correspond to phase lags - fail to capture the delay-dependence of synchronization.","We develop a systematic approach to derive phase reductions for delay-coupled oscillators to arbitrary order.","Already the second-order reduction can predict delay-dependent (bi-)stability of synchronized states as demonstrated for Stuart-Landau oscillators."],"url":"http://arxiv.org/abs/2404.11340v1","category":"math.DS"}
{"created":"2024-04-17 13:00:05","title":"Best Practices for a Handwritten Text Recognition System","abstract":"Handwritten text recognition has been developed rapidly in the recent years, following the rise of deep learning and its applications. Though deep learning methods provide notable boost in performance concerning text recognition, non-trivial deviation in performance can be detected even when small pre-processing or architectural/optimization elements are changed. This work follows a ``best practice'' rationale; highlight simple yet effective empirical practices that can further help training and provide well-performing handwritten text recognition systems. Specifically, we considered three basic aspects of a deep HTR system and we proposed simple yet effective solutions: 1) retain the aspect ratio of the images in the preprocessing step, 2) use max-pooling for converting the 3D feature map of CNN output into a sequence of features and 3) assist the training procedure via an additional CTC loss which acts as a shortcut on the max-pooled sequential features. Using these proposed simple modifications, one can attain close to state-of-the-art results, while considering a basic convolutional-recurrent (CNN+LSTM) architecture, for both IAM and RIMES datasets. Code is available at https://github.com/georgeretsi/HTR-best-practices/.","sentences":["Handwritten text recognition has been developed rapidly in the recent years, following the rise of deep learning and its applications.","Though deep learning methods provide notable boost in performance concerning text recognition, non-trivial deviation in performance can be detected even when small pre-processing or architectural/optimization elements are changed.","This work follows a ``best practice'' rationale; highlight simple yet effective empirical practices that can further help training and provide well-performing handwritten text recognition systems.","Specifically, we considered three basic aspects of a deep HTR system and we proposed simple yet effective solutions: 1) retain the aspect ratio of the images in the preprocessing step, 2) use max-pooling for converting the 3D feature map of CNN output into a sequence of features and 3) assist the training procedure via an additional CTC loss which acts as a shortcut on the max-pooled sequential features.","Using these proposed simple modifications, one can attain close to state-of-the-art results, while considering a basic convolutional-recurrent (CNN+LSTM) architecture, for both IAM and RIMES datasets.","Code is available at https://github.com/georgeretsi/HTR-best-practices/."],"url":"http://arxiv.org/abs/2404.11339v1","category":"cs.CV"}
{"created":"2024-04-17 12:53:57","title":"Vision-based control for landing an aerial vehicle on a marine vessel","abstract":"This work addresses the landing problem of an aerial vehicle, exemplified by a simple quadrotor, on a moving platform using image-based visual servo control. First, the mathematical model of the quadrotor aircraft is introduced, followed by the design of the inner-loop control. At the second stage, the image features on the textured target plane are exploited to derive a vision-based control law. The image of the spherical centroid of a set of landmarks present in the landing target is used as a position measurement, whereas the translational optical flow is used as velocity measurement. The kinematics of the vision-based system is expressed in terms of the observable features, and the proposed control law guarantees convergence without estimating the unknown distance between the vision system and the target, which is also guaranteed to remain strictly positive, avoiding undesired collisions. The performance of the proposed control law is evaluated in MATLAB and 3-D simulation software Gazebo. Simulation results for a quadrotor UAV are provided for different velocity profiles of the moving target, showcasing the robustness of the proposed controller.","sentences":["This work addresses the landing problem of an aerial vehicle, exemplified by a simple quadrotor, on a moving platform using image-based visual servo control.","First, the mathematical model of the quadrotor aircraft is introduced, followed by the design of the inner-loop control.","At the second stage, the image features on the textured target plane are exploited to derive a vision-based control law.","The image of the spherical centroid of a set of landmarks present in the landing target is used as a position measurement, whereas the translational optical flow is used as velocity measurement.","The kinematics of the vision-based system is expressed in terms of the observable features, and the proposed control law guarantees convergence without estimating the unknown distance between the vision system and the target, which is also guaranteed to remain strictly positive, avoiding undesired collisions.","The performance of the proposed control law is evaluated in MATLAB and 3-D simulation software Gazebo.","Simulation results for a quadrotor UAV are provided for different velocity profiles of the moving target, showcasing the robustness of the proposed controller."],"url":"http://arxiv.org/abs/2404.11336v1","category":"eess.SY"}
{"created":"2024-04-17 12:39:48","title":"Following the Human Thread in Social Navigation","abstract":"The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.   We propose the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time. Tested on the novel Habitat 3.0 platform, SDA sets a novel state of the art (SoA) performance in finding and following humans.","sentences":["The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion.","Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions.","Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.   ","We propose the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics.","We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action.","Here, the trajectories are fully visible, i.e., assumed as privileged information.","In the second stage, the trained policy operates without direct access to trajectories.","Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time.","Tested on the novel Habitat 3.0 platform, SDA sets a novel state of the art (SoA) performance in finding and following humans."],"url":"http://arxiv.org/abs/2404.11327v1","category":"cs.RO"}
{"created":"2024-04-17 12:33:57","title":"Saturated RISE control for considering rotor thrust saturation of fully actuated multirotor","abstract":"This work proposes a saturated robust controller for a fully actuated multirotor that takes disturbance rejection and rotor thrust saturation into account. A disturbance rejection controller is required to prevent performance degradation in the presence of parametric uncertainty and external disturbance. Furthermore, rotor saturation should be properly addressed in a controller to avoid performance degradation or even instability due to a gap between the commanded input and the actual input during saturation. To address these issues, we present a modified saturated RISE (Robust Integral of the Sign of the Error) control method. The proposed modified saturated RISE controller is developed for expansion to a system with a non-diagonal, state-dependent input matrix. Next, we present reformulation of the system dynamics of a fully actuated multirotor, and apply the control law to the system. The proposed method is validated in simulation where the proposed controller outperforms the existing one thanks to the capability of handling the input matrix.","sentences":["This work proposes a saturated robust controller for a fully actuated multirotor that takes disturbance rejection and rotor thrust saturation into account.","A disturbance rejection controller is required to prevent performance degradation in the presence of parametric uncertainty and external disturbance.","Furthermore, rotor saturation should be properly addressed in a controller to avoid performance degradation or even instability due to a gap between the commanded input and the actual input during saturation.","To address these issues, we present a modified saturated RISE (Robust Integral of the Sign of the Error) control method.","The proposed modified saturated RISE controller is developed for expansion to a system with a non-diagonal, state-dependent input matrix.","Next, we present reformulation of the system dynamics of a fully actuated multirotor, and apply the control law to the system.","The proposed method is validated in simulation where the proposed controller outperforms the existing one thanks to the capability of handling the input matrix."],"url":"http://arxiv.org/abs/2404.11320v1","category":"cs.RO"}
{"created":"2024-04-17 12:19:33","title":"Undo and Redo Support for Replicated Registers","abstract":"Undo and redo functionality is ubiquitous in collaboration software. In single user settings, undo and redo are well understood. However, when multiple users edit a document, concurrency may arise, leading to a non-linear operation history. This renders undo and redo more complex both in terms of their semantics and implementation. We survey the undo and redo semantics of current mainstream collaboration software and derive principles for undo and redo behavior in a collaborative setting. We then apply these principles to a simple CRDT, the Multi-Valued Replicated Register, and present a novel undo and redo algorithm that implements the undo and redo semantics that we believe are most consistent with users' expectations.","sentences":["Undo and redo functionality is ubiquitous in collaboration software.","In single user settings, undo and redo are well understood.","However, when multiple users edit a document, concurrency may arise, leading to a non-linear operation history.","This renders undo and redo more complex both in terms of their semantics and implementation.","We survey the undo and redo semantics of current mainstream collaboration software and derive principles for undo and redo behavior in a collaborative setting.","We then apply these principles to a simple CRDT, the Multi-Valued Replicated Register, and present a novel undo and redo algorithm that implements the undo and redo semantics that we believe are most consistent with users' expectations."],"url":"http://arxiv.org/abs/2404.11308v1","category":"cs.DC"}
{"created":"2024-04-17 12:12:58","title":"Negatively enhanced thermopower near a Van Hove singularity in electron-doped Sr$_2$RuO$_4$","abstract":"The layered perovskite Sr$_2$RuO$_4$ serves as a model material of the two-dimensional (2D) Fermi liquid but also exhibits various emergent phenomena including the non-Fermi-liquid (NFL) behavior under external perturbations such as uniaxial pressure and chemical substitutions. Here we present the thermoelectric transport of electron-doped system Sr$_{2-y}$La$_{y}$RuO$_4$, in which a filling-induced Lifshitz transition occurs at the Van Hove singularity (VHS) point of $y\\approx 0.2$. We find that the sign of the low-temperature thermopower becomes negative only near the VHS point, where the NFL behavior has been observed in the earlier work. This observation is incompatible with either a numerical calculation within a constant relaxation-time approximation or a toy-model calculation for the 2D Lifshitz transition adopting an elastic carrier scattering. As a promising origin of the observed negatively enhanced thermopower, we propose a skewed NFL state, in which an inelastic scattering with a considerable odd-frequency term plays a crucial role to negatively enhance the thermopower.","sentences":["The layered perovskite Sr$_2$RuO$_4$ serves as a model material of the two-dimensional (2D) Fermi liquid but also exhibits various emergent phenomena including the non-Fermi-liquid (NFL) behavior under external perturbations such as uniaxial pressure and chemical substitutions.","Here we present the thermoelectric transport of electron-doped system Sr$_{2-y}$La$_{y}$RuO$_4$, in which a filling-induced Lifshitz transition occurs at the Van Hove singularity (VHS) point of $y\\approx 0.2$. We find that the sign of the low-temperature thermopower becomes negative only near the VHS point, where the NFL behavior has been observed in the earlier work.","This observation is incompatible with either a numerical calculation within a constant relaxation-time approximation or a toy-model calculation for the 2D Lifshitz transition adopting an elastic carrier scattering.","As a promising origin of the observed negatively enhanced thermopower, we propose a skewed NFL state, in which an inelastic scattering with a considerable odd-frequency term plays a crucial role to negatively enhance the thermopower."],"url":"http://arxiv.org/abs/2404.11300v1","category":"cond-mat.str-el"}
{"created":"2024-04-17 12:00:09","title":"LogSD: Detecting Anomalies from System Logs through Self-supervised Learning and Frequency-based Masking","abstract":"Log analysis is one of the main techniques that engineers use for troubleshooting large-scale software systems. Over the years, many supervised, semi-supervised, and unsupervised log analysis methods have been proposed to detect system anomalies by analyzing system logs. Among these, semi-supervised methods have garnered increasing attention as they strike a balance between relaxed labeled data requirements and optimal detection performance, contrasting with their supervised and unsupervised counterparts. However, existing semi-supervised methods overlook the potential bias introduced by highly frequent log messages on the learned normal patterns, which leads to their less than satisfactory performance. In this study, we propose LogSD, a novel semi-supervised self-supervised learning approach. LogSD employs a dual-network architecture and incorporates a frequency-based masking scheme, a global-to-local reconstruction paradigm and three self-supervised learning tasks. These features enable LogSD to focus more on relatively infrequent log messages, thereby effectively learning less biased and more discriminative patterns from historical normal data. This emphasis ultimately leads to improved anomaly detection performance. Extensive experiments have been conducted on three commonly-used datasets and the results show that LogSD significantly outperforms eight state-of-the-art benchmark methods.","sentences":["Log analysis is one of the main techniques that engineers use for troubleshooting large-scale software systems.","Over the years, many supervised, semi-supervised, and unsupervised log analysis methods have been proposed to detect system anomalies by analyzing system logs.","Among these, semi-supervised methods have garnered increasing attention as they strike a balance between relaxed labeled data requirements and optimal detection performance, contrasting with their supervised and unsupervised counterparts.","However, existing semi-supervised methods overlook the potential bias introduced by highly frequent log messages on the learned normal patterns, which leads to their less than satisfactory performance.","In this study, we propose LogSD, a novel semi-supervised self-supervised learning approach.","LogSD employs a dual-network architecture and incorporates a frequency-based masking scheme, a global-to-local reconstruction paradigm and three self-supervised learning tasks.","These features enable LogSD to focus more on relatively infrequent log messages, thereby effectively learning less biased and more discriminative patterns from historical normal data.","This emphasis ultimately leads to improved anomaly detection performance.","Extensive experiments have been conducted on three commonly-used datasets and the results show that LogSD significantly outperforms eight state-of-the-art benchmark methods."],"url":"http://arxiv.org/abs/2404.11294v1","category":"cs.SE"}
{"created":"2024-04-17 11:42:53","title":"SoK: Decentralized Finance (DeFi) -- Fundamentals, Taxonomy and Risks","abstract":"Decentralized Finance (DeFi) refers to financial services that are not necessarily related to crypto-currencies. By employing blockchain for security and integrity, DeFi creates new possibilities that attract retail and institution users, including central banks. Given its novel applications and sophisticated designs, the distinction between DeFi services and understanding the risk involved is often complex. This work systematically presents the major categories of DeFi protocols that cover over 90\\% of total value locked (TVL) in DeFi. It establishes a structured methodology to differentiate between DeFi protocols based on their design and architecture. Every DeFi protocol is classified into one of three groups: liquidity pools, pegged and synthetic tokens, and aggregator protocols, followed by risk analysis. In particular, we classify stablecoins, liquid staking tokens, and bridged (wrapped) assets as pegged tokens resembling similar risks. The full risk exposure of DeFi users is derived not only from the DeFi protocol design but also from how it is used and with which tokens.","sentences":["Decentralized Finance (DeFi) refers to financial services that are not necessarily related to crypto-currencies.","By employing blockchain for security and integrity, DeFi creates new possibilities that attract retail and institution users, including central banks.","Given its novel applications and sophisticated designs, the distinction between DeFi services and understanding the risk involved is often complex.","This work systematically presents the major categories of DeFi protocols that cover over 90\\% of total value locked (TVL) in DeFi.","It establishes a structured methodology to differentiate between DeFi protocols based on their design and architecture.","Every DeFi protocol is classified into one of three groups: liquidity pools, pegged and synthetic tokens, and aggregator protocols, followed by risk analysis.","In particular, we classify stablecoins, liquid staking tokens, and bridged (wrapped) assets as pegged tokens resembling similar risks.","The full risk exposure of DeFi users is derived not only from the DeFi protocol design but also from how it is used and with which tokens."],"url":"http://arxiv.org/abs/2404.11281v1","category":"cs.CR"}
{"created":"2024-04-17 11:39:08","title":"Study on the static detection of ICF target based on muonic X-ray sphere encoded imaging","abstract":"Muon Induced X-ray Emission (MIXE) was discovered by Chinese physicist Zhang Wenyu as early as 1947, and it can conduct non-destructive elemental analysis inside samples. Research has shown that MIXE can retain the high efficiency of direct imaging while benefiting from the low noise of pinhole imaging through encoding holes. The related technology significantly improves the counting rate while maintaining imaging quality. The sphere encoding technology effectively solves the imaging blurring caused by the tilting of the encoding system, and successfully images micrometer sized X-ray sources. This paper will combine MIXE and X-ray sphere coding imaging techniques, including ball coding and zone plates, to study the method of non-destructive deep structure imaging of ICF targets and obtaining sub element distribution. This method aims to develop a new method for ICF target detection, which is particularly important for inertial confinement fusion. At the same time, this method can be used to detect and analyze materials that are difficult to penetrate or sensitive, and is expected to solve the problem of element resolution and imaging that traditional technologies cannot overcome. It will provide new methods for the future development of multiple fields such as particle physics, material science, and X-ray optics.","sentences":["Muon Induced X-ray Emission (MIXE) was discovered by Chinese physicist Zhang Wenyu as early as 1947, and it can conduct non-destructive elemental analysis inside samples.","Research has shown that MIXE can retain the high efficiency of direct imaging while benefiting from the low noise of pinhole imaging through encoding holes.","The related technology significantly improves the counting rate while maintaining imaging quality.","The sphere encoding technology effectively solves the imaging blurring caused by the tilting of the encoding system, and successfully images micrometer sized X-ray sources.","This paper will combine MIXE and X-ray sphere coding imaging techniques, including ball coding and zone plates, to study the method of non-destructive deep structure imaging of ICF targets and obtaining sub element distribution.","This method aims to develop a new method for ICF target detection, which is particularly important for inertial confinement fusion.","At the same time, this method can be used to detect and analyze materials that are difficult to penetrate or sensitive, and is expected to solve the problem of element resolution and imaging that traditional technologies cannot overcome.","It will provide new methods for the future development of multiple fields such as particle physics, material science, and X-ray optics."],"url":"http://arxiv.org/abs/2404.11278v1","category":"physics.ins-det"}
{"created":"2024-04-17 11:31:16","title":"Jointly Recognizing Speech and Singing Voices Based on Multi-Task Audio Source Separation","abstract":"In short video and live broadcasts, speech, singing voice, and background music often overlap and obscure each other. This complexity creates difficulties in structuring and recognizing the audio content, which may impair subsequent ASR and music understanding applications. This paper proposes a multi-task audio source separation (MTASS) based ASR model called JRSV, which Jointly Recognizes Speech and singing Voices. Specifically, the MTASS module separates the mixed audio into distinct speech and singing voice tracks while removing background music. The CTC/attention hybrid recognition module recognizes both tracks. Online distillation is proposed to improve the robustness of recognition further. To evaluate the proposed methods, a benchmark dataset is constructed and released. Experimental results demonstrate that JRSV can significantly improve recognition accuracy on each track of the mixed audio.","sentences":["In short video and live broadcasts, speech, singing voice, and background music often overlap and obscure each other.","This complexity creates difficulties in structuring and recognizing the audio content, which may impair subsequent ASR and music understanding applications.","This paper proposes a multi-task audio source separation (MTASS) based ASR model called JRSV, which Jointly Recognizes Speech and singing Voices.","Specifically, the MTASS module separates the mixed audio into distinct speech and singing voice tracks while removing background music.","The CTC/attention hybrid recognition module recognizes both tracks.","Online distillation is proposed to improve the robustness of recognition further.","To evaluate the proposed methods, a benchmark dataset is constructed and released.","Experimental results demonstrate that JRSV can significantly improve recognition accuracy on each track of the mixed audio."],"url":"http://arxiv.org/abs/2404.11275v1","category":"cs.SD"}
{"created":"2024-04-17 11:21:23","title":"Milling using two mechatronically coupled robots","abstract":"Industrial robots are commonly used in various industries due to their flexibility. However, their adoption for machining tasks is minimal because of the low dynamic stiffness characteristic of serial kinematic chains. To overcome this problem, we propose coupling two industrial robots at the flanges to form a parallel kinematic machining system. Although parallel kinematic chains are inherently stiffer, one possible disadvantage of the proposed system is that it is heavily overactuated. We perform a modal analysis to show that this may be an advantage, as the redundant degrees of freedom can be used to shift the natural frequencies by applying tension to the coupling module. To demonstrate the validity of our approach, we perform a milling experiment using our coupled system. An external measurement system is used to show that tensioning the coupling module causes a deformation of the system. We further show that this deformation is static over the tool path and can be compensated for.","sentences":["Industrial robots are commonly used in various industries due to their flexibility.","However, their adoption for machining tasks is minimal because of the low dynamic stiffness characteristic of serial kinematic chains.","To overcome this problem, we propose coupling two industrial robots at the flanges to form a parallel kinematic machining system.","Although parallel kinematic chains are inherently stiffer, one possible disadvantage of the proposed system is that it is heavily overactuated.","We perform a modal analysis to show that this may be an advantage, as the redundant degrees of freedom can be used to shift the natural frequencies by applying tension to the coupling module.","To demonstrate the validity of our approach, we perform a milling experiment using our coupled system.","An external measurement system is used to show that tensioning the coupling module causes a deformation of the system.","We further show that this deformation is static over the tool path and can be compensated for."],"url":"http://arxiv.org/abs/2404.11271v1","category":"cs.RO"}
{"created":"2024-04-17 11:11:26","title":"Intermolecular charge transfer enhances the performance of molecular rectifiers","abstract":"Molecular-scale diodes made from self-assembled monolayers (SAMs) could complement silicon-based technologies with smaller, cheaper, and more versatile devices. However, advancement of this emerging technology is limited by insufficient electronic performance exhibited by the molecular current rectifiers. We overcome this barrier by exploiting the charge-transfer state that results from co-assembling SAMs of molecules with strong electron donor and acceptor termini. We obtain a substantial enhancement in current rectification, which correlates with the degree of charge transfer, as confirmed by several complementary techniques. These findings provide a previously unexplored method for manipulating the properties of molecular electronic devices by exploiting donor/acceptor interactions. They also serve as a model test platform for the study of doping mechanisms in organic systems. Our devices have the potential for fast widespread adoption due to their low-cost processing and self-assembly onto silicon substrates, which could allow seamless integration with current technologies.","sentences":["Molecular-scale diodes made from self-assembled monolayers (SAMs) could complement silicon-based technologies with smaller, cheaper, and more versatile devices.","However, advancement of this emerging technology is limited by insufficient electronic performance exhibited by the molecular current rectifiers.","We overcome this barrier by exploiting the charge-transfer state that results from co-assembling SAMs of molecules with strong electron donor and acceptor termini.","We obtain a substantial enhancement in current rectification, which correlates with the degree of charge transfer, as confirmed by several complementary techniques.","These findings provide a previously unexplored method for manipulating the properties of molecular electronic devices by exploiting donor/acceptor interactions.","They also serve as a model test platform for the study of doping mechanisms in organic systems.","Our devices have the potential for fast widespread adoption due to their low-cost processing and self-assembly onto silicon substrates, which could allow seamless integration with current technologies."],"url":"http://arxiv.org/abs/2404.11261v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 11:04:48","title":"Turbulence revealed by wavelet transform: power spectrum and intermittency for the velocity field of the cosmic baryonic fluid","abstract":"We use continuous wavelet transform techniques to construct the global and environment-dependent wavelet statistics, such as energy spectrum and kurtosis, to study the fluctuation and intermittency of the turbulent motion in the cosmic fluid velocity field with the IllustrisTNG simulation data. We find that the peak scales of the energy spectrum and the spectral ratio define two characteristic scales, which can be regarded as the integral scale and the dissipation scale of turbulence, respectively, so that the energy spectrum can be divided into the energy-containing range, the inertial range and the dissipation range of turbulence. The wavelet kurtosis is an increasing function of the wavenumber $k$, first grows rapidly then slowly with $k$, indicating that the cosmic fluid becomes increasingly intermittent with $k$. In the energy-containing range, the energy spectrum increases significantly from $z = 2$ to $1$, but remains almost unchanged from $z = 1$ to $0$. We find that both the environment-dependent spectrum and kurtosis are similar to the global ones, and the magnitude of the spectrum is smallest in the lowest-density and largest in the highest-density environment, suggesting that the cosmic fluid is more turbulent in a high-density than in a low-density environment. In the inertial range, the exponent of the energy spectrum is steeper than not only the Kolmogorov but also the Burgers exponent, suggesting that there may be more complex mechanisms for energy transfer than Kolmogorov and Burgers turbulence.","sentences":["We use continuous wavelet transform techniques to construct the global and environment-dependent wavelet statistics, such as energy spectrum and kurtosis, to study the fluctuation and intermittency of the turbulent motion in the cosmic fluid velocity field with the IllustrisTNG simulation data.","We find that the peak scales of the energy spectrum and the spectral ratio define two characteristic scales, which can be regarded as the integral scale and the dissipation scale of turbulence, respectively, so that the energy spectrum can be divided into the energy-containing range, the inertial range and the dissipation range of turbulence.","The wavelet kurtosis is an increasing function of the wavenumber $k$, first grows rapidly then slowly with $k$, indicating that the cosmic fluid becomes increasingly intermittent with $k$. In the energy-containing range, the energy spectrum increases significantly from $z = 2$ to $1$, but remains almost unchanged from $z = 1$ to $0$. We find that both the environment-dependent spectrum and kurtosis are similar to the global ones, and the magnitude of the spectrum is smallest in the lowest-density and largest in the highest-density environment, suggesting that the cosmic fluid is more turbulent in a high-density than in a low-density environment.","In the inertial range, the exponent of the energy spectrum is steeper than not only the Kolmogorov but also the Burgers exponent, suggesting that there may be more complex mechanisms for energy transfer than Kolmogorov and Burgers turbulence."],"url":"http://arxiv.org/abs/2404.11255v1","category":"astro-ph.CO"}
{"created":"2024-04-17 11:00:12","title":"Bayesian Parameterized Quantum Circuit Optimization (BPQCO): A task and hardware-dependent approach","abstract":"Variational quantum algorithms (VQA) have emerged as a promising quantum alternative for solving optimization and machine learning problems using parameterized quantum circuits (PQCs). The design of these circuits influences the ability of the algorithm to efficiently explore the solution space and converge to more optimal solutions. Choosing an appropriate circuit topology, gate set, and parameterization scheme is determinant to achieve good performance. In addition, it is not only problem-dependent, but the quantum hardware used also has a significant impact on the results. Therefore, we present BPQCO, a Bayesian Optimization-based strategy to search for optimal PQCs adapted to the problem to be solved and to the characteristics and limitations of the chosen quantum hardware. To this end, we experimentally demonstrate the influence of the circuit design on the performance obtained for two classification problems (a synthetic dataset and the well-known Iris dataset), focusing on the design of the circuit ansatz. In addition, we study the degradation of the obtained circuits in the presence of noise when simulating real quantum computers. To mitigate the effect of noise, two alternative optimization strategies based on the characteristics of the quantum system are proposed. The results obtained confirm the relevance of the presented approach and allow its adoption in further work based on the use of PQCs.","sentences":["Variational quantum algorithms (VQA) have emerged as a promising quantum alternative for solving optimization and machine learning problems using parameterized quantum circuits (PQCs).","The design of these circuits influences the ability of the algorithm to efficiently explore the solution space and converge to more optimal solutions.","Choosing an appropriate circuit topology, gate set, and parameterization scheme is determinant to achieve good performance.","In addition, it is not only problem-dependent, but the quantum hardware used also has a significant impact on the results.","Therefore, we present BPQCO, a Bayesian Optimization-based strategy to search for optimal PQCs adapted to the problem to be solved and to the characteristics and limitations of the chosen quantum hardware.","To this end, we experimentally demonstrate the influence of the circuit design on the performance obtained for two classification problems (a synthetic dataset and the well-known Iris dataset), focusing on the design of the circuit ansatz.","In addition, we study the degradation of the obtained circuits in the presence of noise when simulating real quantum computers.","To mitigate the effect of noise, two alternative optimization strategies based on the characteristics of the quantum system are proposed.","The results obtained confirm the relevance of the presented approach and allow its adoption in further work based on the use of PQCs."],"url":"http://arxiv.org/abs/2404.11253v1","category":"quant-ph"}
{"created":"2024-04-17 10:56:40","title":"A first order in time wave equation modeling nonlinear acoustics","abstract":"In this paper we focus on a small amplitude approximation of a Navier-Stokes-Fourier system modeling nonlinear acoustics. Omitting all third and higher order terms with respect to certain small parameters, we obtain a first order in time system containing linear and quadratic pressure and velocity terms.   Subsequently, the well-posedness of the derived system is shown using the classical method of Galerkin approximation in combination with a fixed point argument. We first prove the well-posedness of a linearized equation using energy estimates and then the well-posedness of the nonlinear system using a Newton-Kantorovich type argument. Based on this, we also obtain global in time well-posedness for small enough data and exponential decay. This is in line with semigroup results for a linear part of the system that we provide as well.","sentences":["In this paper we focus on a small amplitude approximation of a Navier-Stokes-Fourier system modeling nonlinear acoustics.","Omitting all third and higher order terms with respect to certain small parameters, we obtain a first order in time system containing linear and quadratic pressure and velocity terms.   ","Subsequently, the well-posedness of the derived system is shown using the classical method of Galerkin approximation in combination with a fixed point argument.","We first prove the well-posedness of a linearized equation using energy estimates and then the well-posedness of the nonlinear system using a Newton-Kantorovich type argument.","Based on this, we also obtain global in time well-posedness for small enough data and exponential decay.","This is in line with semigroup results for a linear part of the system that we provide as well."],"url":"http://arxiv.org/abs/2404.11250v1","category":"math.AP"}
{"created":"2024-04-17 10:47:28","title":"Vignetting Effects: a Tool to Characterize a Fourier Ptychographic Microscope","abstract":"Fourier Ptychographic Microscopy (FPM) is a recent technique to overcome the diffraction limit of a low numerical aperture (NA) objective lens by algorithmic post-processing of several lower resolved images. It can increase the space-bandwidth product of an optical system by computationally combining images captured under different illumination conditions. Vignetting determines the spatial extent of the bright field and dark field regions in the captured images that contain information about low and high frequency image content, respectively. State-of-the-art analyses treat vignetting as a nuisance that needs to be reduced or excluded from algorithmic consideration by means of ad-hoc decision rules [1]. In contrast, this work investigates vignetting effects as a tool to infer a range of properties of the optical system. To achieve this, we characterize the individual system components of the experimental setup and compare experimental data to both, geometrical and wave optical simulations. We demonstrate that using vignetting as an analytical tool enables the modeling of the geometric and coherence properties of the optical system as evidenced by the good agreement between our simulation and experiment. Moreover, our work investigates pupil aberrations in the FPM setup and enables their partial characterization, despite not yet encompassing all aspects.","sentences":["Fourier Ptychographic Microscopy (FPM) is a recent technique to overcome the diffraction limit of a low numerical aperture (NA) objective lens by algorithmic post-processing of several lower resolved images.","It can increase the space-bandwidth product of an optical system by computationally combining images captured under different illumination conditions.","Vignetting determines the spatial extent of the bright field and dark field regions in the captured images that contain information about low and high frequency image content, respectively.","State-of-the-art analyses treat vignetting as a nuisance that needs to be reduced or excluded from algorithmic consideration by means of ad-hoc decision rules","[1].","In contrast, this work investigates vignetting effects as a tool to infer a range of properties of the optical system.","To achieve this, we characterize the individual system components of the experimental setup and compare experimental data to both, geometrical and wave optical simulations.","We demonstrate that using vignetting as an analytical tool enables the modeling of the geometric and coherence properties of the optical system as evidenced by the good agreement between our simulation and experiment.","Moreover, our work investigates pupil aberrations in the FPM setup and enables their partial characterization, despite not yet encompassing all aspects."],"url":"http://arxiv.org/abs/2404.11242v1","category":"physics.optics"}
{"created":"2024-04-17 10:34:43","title":"Topological optical Raman superlattices","abstract":"Topological phases of ultracold atoms recently have been intensively studied both in optical superlattices and Raman lattices. However, the topological features induced by the interplay between such two lattices remain largely unexplored. Here, we present an optical Raman superlattice system that incorporates an optical superlattice and a Raman superlattice. The Raman superlattice presented here supports tunable dimerized spin-orbit couplings and staggered on-site spin flips. We find that such system respects a spin-rotation symmetry and has much richer topological properties. Specifically, we show that various topological phases could emerge in the optical Raman superlattice, such as four different chiral topological insulator phases and two different quantum spin Hall insulator phases, identified by spin winding and spin Chern numbers respectively. We also demonstrate that the spin-dependent topological invariants could be directly measured by quench dynamics.","sentences":["Topological phases of ultracold atoms recently have been intensively studied both in optical superlattices and Raman lattices.","However, the topological features induced by the interplay between such two lattices remain largely unexplored.","Here, we present an optical Raman superlattice system that incorporates an optical superlattice and a Raman superlattice.","The Raman superlattice presented here supports tunable dimerized spin-orbit couplings and staggered on-site spin flips.","We find that such system respects a spin-rotation symmetry and has much richer topological properties.","Specifically, we show that various topological phases could emerge in the optical Raman superlattice, such as four different chiral topological insulator phases and two different quantum spin Hall insulator phases, identified by spin winding and spin Chern numbers respectively.","We also demonstrate that the spin-dependent topological invariants could be directly measured by quench dynamics."],"url":"http://arxiv.org/abs/2404.11233v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-17 10:22:03","title":"Mean field initialization of the Annealed Importance Sampling algorithm for an efficient evaluation of the Partition Function of Restricted Boltzmann Machines","abstract":"Probabilistic models in physics often require from the evaluation of normalized Boltzmann factors, which in turn implies the computation of the partition function Z. Getting the exact value of Z, though, becomes a forbiddingly expensive task as the system size increases. This problem is also present in probabilistic learning models such as the Restricted Boltzmann Machine (RBM), where the situation is even worse as the exact learning rules implies the computation of Z at each iteration. A possible way to tackle this problem is to use the Annealed Importance Sampling (AIS) algorithm, which provides a tool to stochastically estimate the partition function of the system. So far, the standard application of the AIS algorithm starts from the uniform probability distribution and uses a large number of Monte Carlo steps to obtain reliable estimations of Z following an annealing process. In this work we show that both the quality of the estimation and the cost of the computation can be significantly improved by using a properly selected mean-field starting probability distribution. We perform a systematic analysis of AIS in both small- and large-sized problems, and compare the results to exact values in problems where these are known. As a result of our systematic analysis, we propose two successful strategies that work well in all the problems analyzed. We conclude that these are good starting points to estimate the partition function with AIS with a relatively low computational cost.","sentences":["Probabilistic models in physics often require from the evaluation of normalized Boltzmann factors, which in turn implies the computation of the partition function Z.","Getting the exact value of Z, though, becomes a forbiddingly expensive task as the system size increases.","This problem is also present in probabilistic learning models such as the Restricted Boltzmann Machine (RBM), where the situation is even worse as the exact learning rules implies the computation of Z at each iteration.","A possible way to tackle this problem is to use the Annealed Importance Sampling (AIS) algorithm, which provides a tool to stochastically estimate the partition function of the system.","So far, the standard application of the AIS algorithm starts from the uniform probability distribution and uses a large number of Monte Carlo steps to obtain reliable estimations of Z following an annealing process.","In this work we show that both the quality of the estimation and the cost of the computation can be significantly improved by using a properly selected mean-field starting probability distribution.","We perform a systematic analysis of AIS in both small- and large-sized problems, and compare the results to exact values in problems where these are known.","As a result of our systematic analysis, we propose two successful strategies that work well in all the problems analyzed.","We conclude that these are good starting points to estimate the partition function with AIS with a relatively low computational cost."],"url":"http://arxiv.org/abs/2404.11229v1","category":"physics.comp-ph"}
{"created":"2024-04-17 10:21:25","title":"Density estimation for ordinal biological sequences and its applications","abstract":"Biological sequences do not come at random. Instead, they appear with particular frequencies that reflect properties of the associated system or phenomenon. Knowing how biological sequences are distributed in sequence space is thus a natural first step toward understanding the underlying mechanisms. Here we propose a new method for inferring the probability distribution from which a sample of biological sequences were drawn for the case where the sequences are composed of elements that admit a natural ordering. Our method is based on Bayesian field theory, a physics-based machine learning approach, and can be regarded as a nonparametric extension of the traditional maximum entropy estimate. As an example, we use it to analyze the aneuploidy data pertaining to gliomas from The Cancer Genome Atlas project. In addition, we demonstrate two follow-up analyses that can be performed with the resulting probability distribution. One of them is to investigate the associations among the sequence sites. This provides us a way to infer the governing biological grammar. The other is to study the global geometry of the probability landscape, which allows us to look at the problem from an evolutionary point of view. It can be seen that this methodology enables us to learn from a sample of sequences about how a biological system or phenomenon in the real world works.","sentences":["Biological sequences do not come at random.","Instead, they appear with particular frequencies that reflect properties of the associated system or phenomenon.","Knowing how biological sequences are distributed in sequence space is thus a natural first step toward understanding the underlying mechanisms.","Here we propose a new method for inferring the probability distribution from which a sample of biological sequences were drawn for the case where the sequences are composed of elements that admit a natural ordering.","Our method is based on Bayesian field theory, a physics-based machine learning approach, and can be regarded as a nonparametric extension of the traditional maximum entropy estimate.","As an example, we use it to analyze the aneuploidy data pertaining to gliomas from The Cancer Genome Atlas project.","In addition, we demonstrate two follow-up analyses that can be performed with the resulting probability distribution.","One of them is to investigate the associations among the sequence sites.","This provides us a way to infer the governing biological grammar.","The other is to study the global geometry of the probability landscape, which allows us to look at the problem from an evolutionary point of view.","It can be seen that this methodology enables us to learn from a sample of sequences about how a biological system or phenomenon in the real world works."],"url":"http://arxiv.org/abs/2404.11228v1","category":"physics.bio-ph"}
{"created":"2024-04-17 10:05:21","title":"Witnessing Flows in Arithmetic","abstract":"One of the elegant achievements in the history of proof theory is the characterization of the provably total recursive functions of an arithmetical theory by its proof-theoretic ordinal as a way to measure the time complexity of the functions. Unfortunately, the machinery is not sufficiently fine-grained to be applicable on the weak theories on the one hand and to capture the bounded functions with bounded definitions of strong theories, on the other. In this paper, we develop such a machinery to address the bounded theorems of both strong and weak theories of arithmetic. In the first part, we provide a refined version of ordinal analysis to capture the feasibly definable and bounded functions that are provably total in $\\mathrm{PA}+\\bigcup_{\\beta \\prec \\alpha} \\mathrm{TI}(\\prec_{\\beta})$, the extension of Peano arithmetic by transfinite induction up to the ordinals below $\\alpha$. Roughly speaking, we identify the functions as the ones that are computable by a sequence of $\\mathrm{PV}$-provable polynomial time modifications on an initial polynomial time value, where the computational steps are indexed by the ordinals below $\\alpha$, decreasing by the modifications. In the second part, and choosing $l \\leq k$, we use similar technique to capture the functions with bounded definitions in the theory $T^k_2$ (resp. $S^k_2$) as the functions computable by exponentially (resp. polynomially) long sequence of $\\mathrm{PV}_{k-l+1}$-provable reductions between $l$-turn games starting with an explicit $\\mathrm{PV}_{k-l+1}$-provable winning strategy for the first game.","sentences":["One of the elegant achievements in the history of proof theory is the characterization of the provably total recursive functions of an arithmetical theory by its proof-theoretic ordinal as a way to measure the time complexity of the functions.","Unfortunately, the machinery is not sufficiently fine-grained to be applicable on the weak theories on the one hand and to capture the bounded functions with bounded definitions of strong theories, on the other.","In this paper, we develop such a machinery to address the bounded theorems of both strong and weak theories of arithmetic.","In the first part, we provide a refined version of ordinal analysis to capture the feasibly definable and bounded functions that are provably total in $\\mathrm{PA}+\\bigcup_{\\beta \\prec \\alpha} \\mathrm{TI}(\\prec_{\\beta})$, the extension of Peano arithmetic by transfinite induction up to the ordinals below $\\alpha$. Roughly speaking, we identify the functions as the ones that are computable by a sequence of $\\mathrm{PV}$-provable polynomial time modifications on an initial polynomial time value, where the computational steps are indexed by the ordinals below $\\alpha$, decreasing by the modifications.","In the second part, and choosing $l \\leq k$, we use similar technique to capture the functions with bounded definitions in the theory $T^k_2$ (resp.","$S^k_2$) as the functions computable by exponentially (resp.","polynomially) long sequence of $\\mathrm{PV}_{k-l+1}$-provable reductions between $l$-turn games starting with an explicit $\\mathrm{PV}_{k-l+1}$-provable winning strategy for the first game."],"url":"http://arxiv.org/abs/2404.11218v1","category":"math.LO"}
{"created":"2024-04-17 09:48:55","title":"Rotational Interval Exchange Transformations","abstract":"We show the equivalence of two possible definitions of a rotational interval exchange transformation: by the first one, it is a first return map for a circle rotation onto a union of finite number of circle arcs, and by the second one, it is an interval exchange with a scheme (in the sense of interval rearrangement ensembles), whose dual is an interval exchange scheme as well.","sentences":["We show the equivalence of two possible definitions of a rotational interval exchange transformation: by the first one, it is a first return map for a circle rotation onto a union of finite number of circle arcs, and by the second one, it is an interval exchange with a scheme (in the sense of interval rearrangement ensembles), whose dual is an interval exchange scheme as well."],"url":"http://arxiv.org/abs/2404.11211v1","category":"math.DS"}
{"created":"2024-04-17 09:37:25","title":"Kathakali Hand Gesture Recognition With Minimal Data","abstract":"The Indian classical dance-drama Kathakali has a set of hand gestures called Mudras, which form the fundamental units of all its dance moves and postures. Recognizing the depicted mudra becomes one of the first steps in its digital processing. The work treats the problem as a 24-class classification task and proposes a vector-similarity-based approach using pose estimation, eliminating the need for further training or fine-tuning. This approach overcomes the challenge of data scarcity that limits the application of AI in similar domains. The method attains 92% accuracy which is a similar or better performance as other model-training-based works existing in the domain, with the added advantage that the method can still work with data sizes as small as 1 or 5 samples with a slightly reduced performance. Working with images, videos, and even real-time streams is possible. The system can work with hand-cropped or full-body images alike. We have developed and made public a dataset for the Kathakali Mudra Recognition as part of this work.","sentences":["The Indian classical dance-drama Kathakali has a set of hand gestures called Mudras, which form the fundamental units of all its dance moves and postures.","Recognizing the depicted mudra becomes one of the first steps in its digital processing.","The work treats the problem as a 24-class classification task and proposes a vector-similarity-based approach using pose estimation, eliminating the need for further training or fine-tuning.","This approach overcomes the challenge of data scarcity that limits the application of AI in similar domains.","The method attains 92% accuracy which is a similar or better performance as other model-training-based works existing in the domain, with the added advantage that the method can still work with data sizes as small as 1 or 5 samples with a slightly reduced performance.","Working with images, videos, and even real-time streams is possible.","The system can work with hand-cropped or full-body images alike.","We have developed and made public a dataset for the Kathakali Mudra Recognition as part of this work."],"url":"http://arxiv.org/abs/2404.11205v1","category":"cs.CV"}
{"created":"2024-04-17 09:33:55","title":"The puzzling orbital residuals of XTE J1710-281: is a Jovian planet orbiting around the binary system?","abstract":"XTE J1710-281 is a transient eclipsing binary system with a period close to 3.28 hours, hosting a neutron star. The average eclipse duration is 420 seconds, and eclipse arrival times reported in the literature span from 1999 to 2017. A previous analysis of the eclipse arrival times using the eclipse timing technique revealed a complex pattern of delays, indicating the presence of three orbital glitches. These glitches correspond to sudden variations in the orbital period, allowing for the identification of four distinct epochs. We have re-analyzed the 78 eclipse arrival times spanning 18 years utilizing the eclipse timing technique to derive the corresponding delays as a function of time. We find that the observed delays align well with a fitting model employing an eccentric sine function characterized by an amplitude of $6.1 \\pm 0.5$ s, eccentricity of $0.38 \\pm 0.17$, and a period of $17.1 \\pm 1.5$ years. Additionally, we identified the orbital period as 3.28106345(13) hours, with a reference epoch of $T_0=54112.83200(2)$ Modified Julian Date (MJD). We obtained an upper limit of the orbital period derivative of $3.6 \\times 10^{-13}$ s~s$^{-1}$. From the average value of the eclipse duration, we estimate that the companion star has a mass of 0.22~\\Msun for a neutron star mass of 1.4~\\Msun, and the inclination of the source is $78.1^{+1.5}_{-1.2}$ degrees. The companion star is in thermal equilibrium. The orbital period derivative is consistent with a conservative mass transfer scenario, where the angular momentum loss due to magnetic braking dominates over gravitational radiation angular momentum loss if the former is present. The eccentric modulation can be explained by a third body with a mass of 2.7 Jovian masses, orbiting with a revolution period close to 17 years and an eccentricity of 0.38. (abridged abstract)","sentences":["XTE J1710-281 is a transient eclipsing binary system with a period close to 3.28 hours, hosting a neutron star.","The average eclipse duration is 420 seconds, and eclipse arrival times reported in the literature span from 1999 to 2017.","A previous analysis of the eclipse arrival times using the eclipse timing technique revealed a complex pattern of delays, indicating the presence of three orbital glitches.","These glitches correspond to sudden variations in the orbital period, allowing for the identification of four distinct epochs.","We have re-analyzed the 78 eclipse arrival times spanning 18 years utilizing the eclipse timing technique to derive the corresponding delays as a function of time.","We find that the observed delays align well with a fitting model employing an eccentric sine function characterized by an amplitude of $6.1 \\pm 0.5$ s, eccentricity of $0.38 \\pm 0.17$, and a period of $17.1 \\pm 1.5$ years.","Additionally, we identified the orbital period as 3.28106345(13) hours, with a reference epoch of $T_0=54112.83200(2)$ Modified Julian Date (MJD).","We obtained an upper limit of the orbital period derivative of $3.6 \\times 10^{-13}$ s~s$^{-1}$. From the average value of the eclipse duration, we estimate that the companion star has a mass of 0.22~\\Msun for a neutron star mass of 1.4~\\Msun, and the inclination of the source is $78.1^{+1.5}_{-1.2}$ degrees.","The companion star is in thermal equilibrium.","The orbital period derivative is consistent with a conservative mass transfer scenario, where the angular momentum loss due to magnetic braking dominates over gravitational radiation angular momentum loss if the former is present.","The eccentric modulation can be explained by a third body with a mass of 2.7 Jovian masses, orbiting with a revolution period close to 17 years and an eccentricity of 0.38.","(abridged abstract)"],"url":"http://arxiv.org/abs/2404.11203v1","category":"astro-ph.HE"}
{"created":"2024-04-17 09:25:05","title":"Simultaneous compensation of input delay and state/input quantization for linear systems via switched predictor feedback","abstract":"We develop a switched predictor-feedback law, which achieves global asymptotic stabilization of linear systems with input delay and with the plant and actuator states available only in (almost) quantized form. The control design relies on a quantized version of the nominal predictor-feedback law for linear systems, in which quantized measurements of the plant and actuator states enter the predictor state formula. A switching strategy is constructed to dynamically adjust the tunable parameter of the quantizer (in a piecewise constant manner), in order to initially increase the range and subsequently decrease the error of the quantizers. The key element in the proof of global asymptotic stability in the supremum norm of the actuator state is derivation of solutions' estimates combining a backstepping transformation with small-gain and input-to-state stability arguments, for addressing the error due to quantization. We extend this result to the input quantization case and illustrate our theory with a numerical example.","sentences":["We develop a switched predictor-feedback law, which achieves global asymptotic stabilization of linear systems with input delay and with the plant and actuator states available only in (almost) quantized form.","The control design relies on a quantized version of the nominal predictor-feedback law for linear systems, in which quantized measurements of the plant and actuator states enter the predictor state formula.","A switching strategy is constructed to dynamically adjust the tunable parameter of the quantizer (in a piecewise constant manner), in order to initially increase the range and subsequently decrease the error of the quantizers.","The key element in the proof of global asymptotic stability in the supremum norm of the actuator state is derivation of solutions' estimates combining a backstepping transformation with small-gain and input-to-state stability arguments, for addressing the error due to quantization.","We extend this result to the input quantization case and illustrate our theory with a numerical example."],"url":"http://arxiv.org/abs/2404.11194v1","category":"math.OC"}
{"created":"2024-04-17 09:14:18","title":"Lyapunov exponents of renewal equations: numerical approximation and convergence analysis","abstract":"We propose a numerical method for computing the Lyapunov exponents of renewal equations (delay equations of Volterra type), consisting first in applying a discrete QR technique to the associated evolution family suitably posed on a Hilbert state space and second in reducing to finite dimension each evolution operator in the obtained time sequence. The reduction to finite dimension relies on Fourier projection in the state space and on pseudospectral collocation in the forward time step. A rigorous proof of convergence of both the discretized operators and the approximated exponents is provided. A MATLAB implementation is also included for completeness.","sentences":["We propose a numerical method for computing the Lyapunov exponents of renewal equations (delay equations of Volterra type), consisting first in applying a discrete QR technique to the associated evolution family suitably posed on a Hilbert state space and second in reducing to finite dimension each evolution operator in the obtained time sequence.","The reduction to finite dimension relies on Fourier projection in the state space and on pseudospectral collocation in the forward time step.","A rigorous proof of convergence of both the discretized operators and the approximated exponents is provided.","A MATLAB implementation is also included for completeness."],"url":"http://arxiv.org/abs/2404.11191v1","category":"math.NA"}
{"created":"2024-04-17 09:04:42","title":"Approximability of the Containment Problem for Zonotopes and Ellipsotopes","abstract":"The zonotope containment problem, i.e., whether one zonotope is contained in another, is a central problem in control theory to compute invariant sets, obtain fixed points of reachable sets, detect faults, and robustify controllers. Despite the inherent co-NP-hardness of this problem, an approximation algorithm developed by S. Sadraddini and R. Tedrake has gained widespread recognition for its swift execution and consistent reliability in practical scenarios. In our study, we substantiate the precision of the algorithm with a definitive proof, elucidating the empirical accuracy observed in practice. Our proof hinges on establishing a connection between the containment problem and the computation of matrix norms, thereby enabling the extension of the approximation algorithm to encompass ellipsotopes, a broader class of sets derived from zonotopes. Moreover, we explore the computational complexity of the ellipsotope containment problem, focusing on approximability. Finally, we present new methods to calculate robust control invariant sets for linear dynamical systems, demonstrating the practical relevance of approximations to the ellipsotope containment problem.","sentences":["The zonotope containment problem, i.e., whether one zonotope is contained in another, is a central problem in control theory to compute invariant sets, obtain fixed points of reachable sets, detect faults, and robustify controllers.","Despite the inherent co-NP-hardness of this problem, an approximation algorithm developed by S. Sadraddini and R. Tedrake has gained widespread recognition for its swift execution and consistent reliability in practical scenarios.","In our study, we substantiate the precision of the algorithm with a definitive proof, elucidating the empirical accuracy observed in practice.","Our proof hinges on establishing a connection between the containment problem and the computation of matrix norms, thereby enabling the extension of the approximation algorithm to encompass ellipsotopes, a broader class of sets derived from zonotopes.","Moreover, we explore the computational complexity of the ellipsotope containment problem, focusing on approximability.","Finally, we present new methods to calculate robust control invariant sets for linear dynamical systems, demonstrating the practical relevance of approximations to the ellipsotope containment problem."],"url":"http://arxiv.org/abs/2404.11185v1","category":"math.OC"}
{"created":"2024-04-17 09:01:02","title":"FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document","abstract":"Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems. Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed. But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability. In this work, we propose highly effective and interpretable factual inconsistency detection method metric Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document for abstractive summarization systems that is based on fine-grained atomic facts decomposition. Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion. These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary's factual inconsistency. Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems. We release the code at https://github.com/plm3332/FIZZ.","sentences":["Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems.","Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed.","But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability.","In this work, we propose highly effective and interpretable factual inconsistency detection method metric Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document for abstractive summarization systems that is based on fine-grained atomic facts decomposition.","Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion.","These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary's factual inconsistency.","Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems.","We release the code at https://github.com/plm3332/FIZZ."],"url":"http://arxiv.org/abs/2404.11184v1","category":"cs.CL"}
{"created":"2024-04-17 08:45:04","title":"Active quantum distillation","abstract":"Quantum distillation is a modern technology to decrease the von Neumann entropy of a subsystem by coherent system dynamics. Here we propose an active quantum distillation protocol, in which a bang-bang theme is applied to actively control the coherent dynamics of our system in order to obtain a subsystem with the von Neumann entropy as low as possible. For a bipartite Bosonic system, we derive the analytical expression of lower bound of the entropy of subsystem under any unitary transformation with conservation of particles. The lower bound is validated by numerical simulations on the Bose-Hubbard model, where the coherent evolution is controlled by tuning one interaction term of the Hamiltonian. Our protocol can be used to decrease the entropy of one subsystem lower than the total bipartite state and increase the number of Bosons or only distill out very few Bosons in the subsystem.","sentences":["Quantum distillation is a modern technology to decrease the von Neumann entropy of a subsystem by coherent system dynamics.","Here we propose an active quantum distillation protocol, in which a bang-bang theme is applied to actively control the coherent dynamics of our system in order to obtain a subsystem with the von Neumann entropy as low as possible.","For a bipartite Bosonic system, we derive the analytical expression of lower bound of the entropy of subsystem under any unitary transformation with conservation of particles.","The lower bound is validated by numerical simulations on the Bose-Hubbard model, where the coherent evolution is controlled by tuning one interaction term of the Hamiltonian.","Our protocol can be used to decrease the entropy of one subsystem lower than the total bipartite state and increase the number of Bosons or only distill out very few Bosons in the subsystem."],"url":"http://arxiv.org/abs/2404.11175v1","category":"quant-ph"}
{"created":"2024-04-17 08:38:04","title":"Mutiny! How does Kubernetes fail, and what can we do about it?","abstract":"In this paper, we i) analyze and classify real-world failures of Kubernetes (the most popular container orchestration system), ii) develop a framework to perform a fault/error injection campaign targeting the data store preserving the cluster state, and iii) compare results of our fault/error injection experiments with real-world failures, showing that our fault/error injections can recreate many real-world failure patterns. The paper aims to address the lack of studies on systematic analyses of Kubernetes failures to date.   Our results show that even a single fault/error (e.g., a bit-flip) in the data stored can propagate, causing cluster-wide failures (3% of injections), service networking issues (4%), and service under/overprovisioning (24%). Errors in the fields tracking dependencies between object caused 51% of such cluster-wide failures. We argue that controlled fault/error injection-based testing should be employed to proactively assess Kubernetes' resiliency and guide the design of failure mitigation strategies.","sentences":["In this paper, we i) analyze and classify real-world failures of Kubernetes (the most popular container orchestration system), ii) develop a framework to perform a fault/error injection campaign targeting the data store preserving the cluster state, and iii) compare results of our fault/error injection experiments with real-world failures, showing that our fault/error injections can recreate many real-world failure patterns.","The paper aims to address the lack of studies on systematic analyses of Kubernetes failures to date.   ","Our results show that even a single fault/error (e.g., a bit-flip) in the data stored can propagate, causing cluster-wide failures (3% of injections), service networking issues (4%), and service under/overprovisioning (24%).","Errors in the fields tracking dependencies between object caused 51% of such cluster-wide failures.","We argue that controlled fault/error injection-based testing should be employed to proactively assess Kubernetes' resiliency and guide the design of failure mitigation strategies."],"url":"http://arxiv.org/abs/2404.11169v1","category":"cs.DC"}
{"created":"2024-04-17 08:08:06","title":"Multiple charge-density-wave gaps in LaSbTe and CeSbTe as revealed by ultrafast spectroscopy","abstract":"Utilizing ultrafast time-resolved pump-probe spectroscopy measurements, we investigate the photoinduced quasiparticle dynamics of the topological materials LaSbTe and CeSbTe. In LaSbTe, the relaxation of quasiparticles is dominated by two different mechanisms: electron-phonon coupling, and phonon-assisted electron-hole recombination. Significantly, the amplitude of photoinduced reflectivity related to the former one shows two pronounced peaks at 156 K and 263 K, indicating the occurrence of two charge density wave (CDW) phase transitions. The ultrafast responses of CeSbTe share a lot of similarities with LaSbTe, and an additional CDW phase transition at 154 K is revealed in CeSbTe. However, the slower relaxation of CeSbTe exhibits an exotic behavior that deviates from the typical phonon-assisted electron-hole recombination process, probably due to the imbalance between the electron- and hole-type carriers. Unlike LaSbTe, the relaxation times of CeSbTe vary slightly with the pump power, inferring the possible participation of 4$f$ electron in the decay process. In addition, two oscillation modes around 1 THz and 3 THz are identified in both LaSbTe and CeSbTe, which are mostly likely to be coherent phonon modes. These findings unravel the existence of multiple CDW orders in LaSbTe and CeSbTe, offering insights into the underlying physics of these systems.","sentences":["Utilizing ultrafast time-resolved pump-probe spectroscopy measurements, we investigate the photoinduced quasiparticle dynamics of the topological materials LaSbTe and CeSbTe.","In LaSbTe, the relaxation of quasiparticles is dominated by two different mechanisms: electron-phonon coupling, and phonon-assisted electron-hole recombination.","Significantly, the amplitude of photoinduced reflectivity related to the former one shows two pronounced peaks at 156 K and 263 K, indicating the occurrence of two charge density wave (CDW) phase transitions.","The ultrafast responses of CeSbTe share a lot of similarities with LaSbTe, and an additional CDW phase transition at 154 K is revealed in CeSbTe.","However, the slower relaxation of CeSbTe exhibits an exotic behavior that deviates from the typical phonon-assisted electron-hole recombination process, probably due to the imbalance between the electron- and hole-type carriers.","Unlike LaSbTe, the relaxation times of CeSbTe vary slightly with the pump power, inferring the possible participation of 4$f$ electron in the decay process.","In addition, two oscillation modes around 1 THz and 3 THz are identified in both LaSbTe and CeSbTe, which are mostly likely to be coherent phonon modes.","These findings unravel the existence of multiple CDW orders in LaSbTe and CeSbTe, offering insights into the underlying physics of these systems."],"url":"http://arxiv.org/abs/2404.11154v1","category":"cond-mat.str-el"}
{"created":"2024-04-17 08:01:15","title":"Automated, efficient and model-free inference for randomized clinical trials via data-driven covariate adjustment","abstract":"In May 2023, the U.S. Food and Drug Administration (FDA) released guidance for industry on \"Adjustment for Covariates in Randomized Clinical Trials for Drugs and Biological Products\". Covariate adjustment is a statistical analysis method for improving precision and power in clinical trials by adjusting for pre-specified, prognostic baseline variables. Though recommended by the FDA and the European Medicines Agency (EMA), many trials do not exploit the available information in baseline variables or make use only of the baseline measurement of the outcome. This is likely (partly) due to the regulatory mandate to pre-specify baseline covariates for adjustment, leading to challenges in determining appropriate covariates and their functional forms. We will explore the potential of automated data-adaptive methods, such as machine learning algorithms, for covariate adjustment, addressing the challenge of pre-specification. Specifically, our approach allows the use of complex models or machine learning algorithms without compromising the interpretation or validity of the treatment effect estimate and its corresponding standard error, even in the presence of misspecified outcome working models. This contrasts the majority of competing works which assume correct model specification for the validity of standard errors. Our proposed estimators either necessitate ultra-sparsity in the outcome model (which can be relaxed by limiting the number of predictors in the model) or necessitate integration with sample splitting to enhance their performance. As such, we will arrive at simple estimators and standard errors for the marginal treatment effect in randomized clinical trials, which exploit data-adaptive outcome predictions based on prognostic baseline covariates, and have low (or no) bias in finite samples even when those predictions are themselves biased.","sentences":["In May 2023, the U.S. Food and Drug Administration (FDA) released guidance for industry on \"Adjustment for Covariates in Randomized Clinical Trials for Drugs and Biological Products\".","Covariate adjustment is a statistical analysis method for improving precision and power in clinical trials by adjusting for pre-specified, prognostic baseline variables.","Though recommended by the FDA and the European Medicines Agency (EMA), many trials do not exploit the available information in baseline variables or make use only of the baseline measurement of the outcome.","This is likely (partly) due to the regulatory mandate to pre-specify baseline covariates for adjustment, leading to challenges in determining appropriate covariates and their functional forms.","We will explore the potential of automated data-adaptive methods, such as machine learning algorithms, for covariate adjustment, addressing the challenge of pre-specification.","Specifically, our approach allows the use of complex models or machine learning algorithms without compromising the interpretation or validity of the treatment effect estimate and its corresponding standard error, even in the presence of misspecified outcome working models.","This contrasts the majority of competing works which assume correct model specification for the validity of standard errors.","Our proposed estimators either necessitate ultra-sparsity in the outcome model (which can be relaxed by limiting the number of predictors in the model) or necessitate integration with sample splitting to enhance their performance.","As such, we will arrive at simple estimators and standard errors for the marginal treatment effect in randomized clinical trials, which exploit data-adaptive outcome predictions based on prognostic baseline covariates, and have low (or no) bias in finite samples even when those predictions are themselves biased."],"url":"http://arxiv.org/abs/2404.11150v1","category":"stat.ME"}
{"created":"2024-04-17 07:47:00","title":"Spectrum of $[8]_{[c\\bar{s}]} \\otimes [8]_{[q \\bar{q^\\prime}]}$ systems with quantum numbers $J^{P}=0^\\pm$ and $1^\\pm$","abstract":"Inspired by the recent experimental progress on the $T_{c\\bar{s}0}^a(2900)^{0/++}$, the fully open tetraquark spectrum with the configuration of $[8]_{[c\\bar{s}]} \\otimes [8]_{[q \\bar{q^\\prime}]}$ is systematically investigated by the QCD sum rules. In this article, we concentrate on the quantum numbers $J^{P}=0^{+}/0^{-}/1^{+}/1^{-}$.   Firstly, we construct four scalar currents ($J^{P}=0^+$) in the form of $[8]_{[c\\bar{s}]} \\otimes [8]_{[u\\bar{d}]}$ type tetraquark structure and perform the analysis of the QCD sum rules, where we consider the leading order contributions up to dimension 11 in the operator product expansion and retain contributions linear in the strange quark mass $m_s$. Our results, $M_{0^{+}}^A = 2.91_{-0.20}^{+0.20}~\\text{GeV}$ and $M_{0^{+}}^C = 2.98_{-0.21}^{+0.20}~\\text{GeV}$, are consistent with the experimentally discovered $T_{c\\bar{s}0}^a(2900)^{++}$ within error margins. Thus, our calculation supports classifying $T_{c\\bar{s}0}^a(2900)^{++}$ as a tetraquark state with the $[8]_{[c\\bar{s}]} \\otimes [8]_{u \\bar{d}}$ color configuration.   Moreover, on the basis of $J^P=0^+$, we also study tetraquark states with quantum numbers $J^P=0^-$, $1^+$, and $1^-$, predicting two new hadronic states awaiting experimental verification for each quantum number. Their masses are as follows: $M_{0^{-}}^{A}=3.45_{-0.10}^{+0.12}~\\text{GeV}$, $M_{0^{-}}^{D}=3.20_{-0.19}^{+0.17}~\\text{GeV}$, $M_{1^{+}}^{A}=2.95_{-0.19}^{+0.19}~\\text{GeV}$, $M_{1^{+}}^{C}={2.95_{-0.19}^{+0.19}}~\\text{GeV}$, $M_{1^{-}}^{A}={3.23_{-0.10}^{+0.11}}~\\text{GeV}$ and $M_{1^{-}}^{D}={3.23_{-0.11}^{+0.09}}~\\text{GeV}$. With advances in experimental techniques and accumulation of new data, these predicted results are hoped to be confirmed in future experiments.","sentences":["Inspired by the recent experimental progress on the $T_{c\\bar{s}0}^a(2900)^{0/++}$, the fully open tetraquark spectrum with the configuration of $[8]_{[c\\bar{s}]} \\otimes [8]_{[q \\bar{q^\\prime}]}$ is systematically investigated by the QCD sum rules.","In this article, we concentrate on the quantum numbers $J^{P}=0^{+}/0^{-}/1^{+}/1^{-}$.   Firstly, we construct four scalar currents ($J^{P}=0^+$) in the form of $[8]_{[c\\bar{s}]} \\otimes [8]_{[u\\bar{d}]}$ type tetraquark structure and perform the analysis of the QCD sum rules, where we consider the leading order contributions up to dimension 11 in the operator product expansion and retain contributions linear in the strange quark mass $m_s$. Our results, $M_{0^{+}}^A = 2.91_{-0.20}^{+0.20}~\\text{GeV}$ and $M_{0^{+}}^C = 2.98_{-0.21}^{+0.20}~\\text{GeV}$, are consistent with the experimentally discovered $T_{c\\bar{s}0}^a(2900)^{++}$ within error margins.","Thus, our calculation supports classifying $T_{c\\bar{s}0}^a(2900)^{++}$ as a tetraquark state with the $[8]_{[c\\bar{s}]} \\otimes [8]_{u \\bar{d}}$ color configuration.   ","Moreover, on the basis of $J^P=0^+$, we also study tetraquark states with quantum numbers $J^P=0^-$, $1^+$, and $1^-$, predicting two new hadronic states awaiting experimental verification for each quantum number.","Their masses are as follows: $M_{0^{-}}^{A}=3.45_{-0.10}^{+0.12}~\\text{GeV}$, $M_{0^{-}}^{D}=3.20_{-0.19}^{+0.17}~\\text{GeV}$, $M_{1^{+}}^{A}=2.95_{-0.19}^{+0.19}~\\text{GeV}$, $M_{1^{+}}^{C}={2.95_{-0.19}^{+0.19}}~\\text{GeV}$, $M_{1^{-}}^{A}={3.23_{-0.10}^{+0.11}}~\\text{GeV}$ and $M_{1^{-}}^{D}={3.23_{-0.11}^{+0.09}}~\\text{GeV}$. With advances in experimental techniques and accumulation of new data, these predicted results are hoped to be confirmed in future experiments."],"url":"http://arxiv.org/abs/2404.11145v1","category":"hep-ph"}
{"created":"2024-04-17 07:35:06","title":"Accuracy and repeatability of a parallel robot for personalised minimally invasive surgery","abstract":"The paper presents the methodology used for accuracy and repeatability measurements of the experimental model of a parallel robot developed for surgical applications. The experimental setup uses a motion tracking system (for accuracy) and a high precision measuring arm for position (for repeatability). The accuracy was obtained by comparing the trajectory data from the experimental measurement with a baseline trajectory defined with the kinematic models of the parallel robotic system. The repeatability was experi-mentally determined by moving (repeatedly) the robot platform in predefined points.","sentences":["The paper presents the methodology used for accuracy and repeatability measurements of the experimental model of a parallel robot developed for surgical applications.","The experimental setup uses a motion tracking system (for accuracy) and a high precision measuring arm for position (for repeatability).","The accuracy was obtained by comparing the trajectory data from the experimental measurement with a baseline trajectory defined with the kinematic models of the parallel robotic system.","The repeatability was experi-mentally determined by moving (repeatedly) the robot platform in predefined points."],"url":"http://arxiv.org/abs/2404.11140v1","category":"cs.RO"}
{"created":"2024-04-17 07:21:17","title":"Learning epidemic trajectories through Kernel Operator Learning: from modelling to optimal control","abstract":"Since infectious pathogens start spreading into a susceptible population, mathematical models can provide policy makers with reliable forecasts and scenario analyses, which can be concretely implemented or solely consulted. In these complex epidemiological scenarios, machine learning architectures can play an important role, since they directly reconstruct data-driven models circumventing the specific modelling choices and the parameter calibration, typical of classical compartmental models. In this work, we discuss the efficacy of Kernel Operator Learning (KOL) to reconstruct population dynamics during epidemic outbreaks, where the transmission rate is ruled by an input strategy. In particular, we introduce two surrogate models, named KOL-m and KOL-$\\partial$, which reconstruct in two different ways the evolution of the epidemics. Moreover, we evaluate the generalization performances of the two approaches with different kernels, including the Neural Tangent Kernels, and compare them with a classical neural network model learning method. Employing synthetic but semi-realistic data, we show how the two introduced approaches are suitable for realizing fast and robust forecasts and scenario analyses, and how these approaches are competitive for determining optimal intervention strategies with respect to specific performance measures.","sentences":["Since infectious pathogens start spreading into a susceptible population, mathematical models can provide policy makers with reliable forecasts and scenario analyses, which can be concretely implemented or solely consulted.","In these complex epidemiological scenarios, machine learning architectures can play an important role, since they directly reconstruct data-driven models circumventing the specific modelling choices and the parameter calibration, typical of classical compartmental models.","In this work, we discuss the efficacy of Kernel Operator Learning (KOL) to reconstruct population dynamics during epidemic outbreaks, where the transmission rate is ruled by an input strategy.","In particular, we introduce two surrogate models, named KOL-m and KOL-$\\partial$, which reconstruct in two different ways the evolution of the epidemics.","Moreover, we evaluate the generalization performances of the two approaches with different kernels, including the Neural Tangent Kernels, and compare them with a classical neural network model learning method.","Employing synthetic but semi-realistic data, we show how the two introduced approaches are suitable for realizing fast and robust forecasts and scenario analyses, and how these approaches are competitive for determining optimal intervention strategies with respect to specific performance measures."],"url":"http://arxiv.org/abs/2404.11130v1","category":"math.NA"}
{"created":"2024-04-17 07:06:22","title":"MHLR: Moving Haar Learning Rate Scheduler for Large-scale Face Recognition Training with One GPU","abstract":"Face recognition (FR) has seen significant advancements due to the utilization of large-scale datasets. Training deep FR models on large-scale datasets with multiple GPUs is now a common practice. In fact, computing power has evolved into a foundational and indispensable resource in the area of deep learning. It is nearly impossible to train a deep FR model without holding adequate hardware resources. Recognizing this challenge, some FR approaches have started exploring ways to reduce the time complexity of the fully-connected layer in FR models. Unlike other approaches, this paper introduces a simple yet highly effective approach, Moving Haar Learning Rate (MHLR) scheduler, for scheduling the learning rate promptly and accurately in the training process. MHLR supports large-scale FR training with only one GPU, which is able to accelerate the model to 1/4 of its original training time without sacrificing more than 1% accuracy. More specifically, MHLR only needs $30$ hours to train the model ResNet100 on the dataset WebFace12M containing more than 12M face images with 0.6M identities. Extensive experiments validate the efficiency and effectiveness of MHLR.","sentences":["Face recognition (FR) has seen significant advancements due to the utilization of large-scale datasets.","Training deep FR models on large-scale datasets with multiple GPUs is now a common practice.","In fact, computing power has evolved into a foundational and indispensable resource in the area of deep learning.","It is nearly impossible to train a deep FR model without holding adequate hardware resources.","Recognizing this challenge, some FR approaches have started exploring ways to reduce the time complexity of the fully-connected layer in FR models.","Unlike other approaches, this paper introduces a simple yet highly effective approach, Moving Haar Learning Rate (MHLR) scheduler, for scheduling the learning rate promptly and accurately in the training process.","MHLR supports large-scale FR training with only one GPU, which is able to accelerate the model to 1/4 of its original training time without sacrificing more than 1% accuracy.","More specifically, MHLR only needs $30$ hours to train the model ResNet100 on the dataset WebFace12M containing more than 12M face images with 0.6M identities.","Extensive experiments validate the efficiency and effectiveness of MHLR."],"url":"http://arxiv.org/abs/2404.11118v1","category":"cs.CV"}
{"created":"2024-04-17 07:00:43","title":"Strong-coupling superconductivity and weak vortex pinning in Ta-doped CsV$_{3}$Sb$_{5}$ single crystals","abstract":"By measuring magnetizations of pristine and Ta-doped CsV$_{3}$Sb$_{5}$ single crystals, we have carried out systematic studies on the lower critical field, critical current density, and equilibrium magnetization of this kagome system. The lower critical field has been investigated in the two typical samples, and the temperature dependent lower critical field obtained in Ta-doped sample can be fitted by using the model with two $s$-wave superconducting gaps yielding the larger gap of $2\\Delta_{s1}/k_\\mathrm{B}T_\\mathrm{c}=7.9\\;(\\pm1.8)$. This indicates a strong-coupling feature of the V-based superconductors. The measured magnetization hysteresis loops allow us to calculate the critical current density, which shows a very weak bulk vortex pinning. The magnetization hysteresis loops measured in these two kinds of samples can be well described by a recently proposed generalized phenomenological model, which leads to the determination of many fundamental parameters for these superconductors. Our systematic results and detailed analysis conclude that this V-based kagome system has features of strong-coupling superconductivity, relatively large Ginzburg-Landau parameter and weak vortex coupling.","sentences":["By measuring magnetizations of pristine and Ta-doped CsV$_{3}$Sb$_{5}$ single crystals, we have carried out systematic studies on the lower critical field, critical current density, and equilibrium magnetization of this kagome system.","The lower critical field has been investigated in the two typical samples, and the temperature dependent lower critical field obtained in Ta-doped sample can be fitted by using the model with two $s$-wave superconducting gaps yielding the larger gap of $2\\Delta_{s1}/k_\\mathrm{B}T_\\mathrm{c}=7.9\\;(\\pm1.8)$. This indicates a strong-coupling feature of the V-based superconductors.","The measured magnetization hysteresis loops allow us to calculate the critical current density, which shows a very weak bulk vortex pinning.","The magnetization hysteresis loops measured in these two kinds of samples can be well described by a recently proposed generalized phenomenological model, which leads to the determination of many fundamental parameters for these superconductors.","Our systematic results and detailed analysis conclude that this V-based kagome system has features of strong-coupling superconductivity, relatively large Ginzburg-Landau parameter and weak vortex coupling."],"url":"http://arxiv.org/abs/2404.11115v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 06:58:56","title":"Internal 1000 AU-scale Structures of the R CrA Cluster-forming Cloud -- I: Filamentary Structures","abstract":"We report on ALMA ACA observations of a high-density region of the Corona Australis cloud forming a young star cluster, and the results of resolving internal structures. In addition to embedded Class 0/I protostars in continuum, a number of complex dense filamentary structures are detected in the C18O and SO lines by the 7m array. These are sub-structures of the molecular clump that are detected by the TP array as the extended emission. We identify 101 and 37 filamentary structures with a few thousand AU widths in C18O and SO, respectively, called as feathers. The typical column density of the feathers in C18O is about 10^{22} cm^{-2}, and the volume density and line mass are ~ 10^5 cm^{-3}, and a few times M_{sun} pc^{-1}, respectively. This line mass is significantly smaller than the critical line mass expected for cold and dense gas. These structures have complex velocity fields, indicating a turbulent internal property. The number of feathers associated with Class 0/I protostars is only ~ 10, indicating that most of them do not form stars but rather being transient structures. The formation of feathers can be interpreted as a result of colliding gas flow as the morphology well reproduced by MHD simulations, supported by the the presence of HI shells in the vicinity. The colliding gas flows may accumulate gas and form filaments and feathers, and trigger the active star formation of the R CrA cluster.","sentences":["We report on ALMA ACA observations of a high-density region of the Corona Australis cloud forming a young star cluster, and the results of resolving internal structures.","In addition to embedded Class 0/I protostars in continuum, a number of complex dense filamentary structures are detected in the C18O and SO lines by the 7m array.","These are sub-structures of the molecular clump that are detected by the TP array as the extended emission.","We identify 101 and 37 filamentary structures with a few thousand AU widths in C18O and SO, respectively, called as feathers.","The typical column density of the feathers in C18O is about 10^{22} cm^{-2}, and the volume density and line mass are ~ 10^5 cm^{-3}, and a few times M_{sun} pc^{-1}, respectively.","This line mass is significantly smaller than the critical line mass expected for cold and dense gas.","These structures have complex velocity fields, indicating a turbulent internal property.","The number of feathers associated with Class 0/I protostars is only ~ 10, indicating that most of them do not form stars but rather being transient structures.","The formation of feathers can be interpreted as a result of colliding gas flow as the morphology well reproduced by MHD simulations, supported by the the presence of HI shells in the vicinity.","The colliding gas flows may accumulate gas and form filaments and feathers, and trigger the active star formation of the R CrA cluster."],"url":"http://arxiv.org/abs/2404.11113v1","category":"astro-ph.GA"}
{"created":"2024-04-17 06:58:32","title":"An Adaptive Regularized Proximal Newton-Type Methods for Composite Optimization over the Stiefel Manifold","abstract":"Recently, the proximal Newton-type method and its variants have been generalized to solve composite optimization problems over the Stiefel manifold whose objective function is the summation of a smooth function and a nonsmooth function. In this paper, we propose an adaptive quadratically regularized proximal quasi-Newton method, named ARPQN, to solve this class of problems. Under some mild assumptions, the global convergence, the local linear convergence rate and the iteration complexity of ARPQN are established. Numerical experiments and comparisons with other state-of-the-art methods indicate that ARPQN is very promising. We also propose an adaptive quadratically regularized proximal Newton method, named ARPN. It is shown the ARPN method has a local superlinear convergence rate under certain reasonable assumptions, which demonstrates attractive convergence properties of regularized proximal Newton methods.","sentences":["Recently, the proximal Newton-type method and its variants have been generalized to solve composite optimization problems over the Stiefel manifold whose objective function is the summation of a smooth function and a nonsmooth function.","In this paper, we propose an adaptive quadratically regularized proximal quasi-Newton method, named ARPQN, to solve this class of problems.","Under some mild assumptions, the global convergence, the local linear convergence rate and the iteration complexity of ARPQN are established.","Numerical experiments and comparisons with other state-of-the-art methods indicate that ARPQN is very promising.","We also propose an adaptive quadratically regularized proximal Newton method, named ARPN.","It is shown the ARPN method has a local superlinear convergence rate under certain reasonable assumptions, which demonstrates attractive convergence properties of regularized proximal Newton methods."],"url":"http://arxiv.org/abs/2404.11112v1","category":"math.OC"}
{"created":"2024-04-17 06:47:17","title":"LADDER: An Efficient Framework for Video Frame Interpolation","abstract":"Video Frame Interpolation (VFI) is a crucial technique in various applications such as slow-motion generation, frame rate conversion, video frame restoration etc. This paper introduces an efficient video frame interpolation framework that aims to strike a favorable balance between efficiency and quality. Our framework follows a general paradigm consisting of a flow estimator and a refinement module, while incorporating carefully designed components. First of all, we adopt depth-wise convolution with large kernels in the flow estimator that simultaneously reduces the parameters and enhances the receptive field for encoding rich context and handling complex motion. Secondly, diverging from a common design for the refinement module with a UNet-structure (encoder-decoder structure), which we find redundant, our decoder-only refinement module directly enhances the result from coarse to fine features, offering a more efficient process. In addition, to address the challenge of handling high-definition frames, we also introduce an innovative HD-aware augmentation strategy during training, leading to consistent enhancement on HD images. Extensive experiments are conducted on diverse datasets, Vimeo90K, UCF101, Xiph and SNU-FILM. The results demonstrate that our approach achieves state-of-the-art performance with clear improvement while requiring much less FLOPs and parameters, reaching to a better spot for balancing efficiency and quality.","sentences":["Video Frame Interpolation (VFI) is a crucial technique in various applications such as slow-motion generation, frame rate conversion, video frame restoration etc.","This paper introduces an efficient video frame interpolation framework that aims to strike a favorable balance between efficiency and quality.","Our framework follows a general paradigm consisting of a flow estimator and a refinement module, while incorporating carefully designed components.","First of all, we adopt depth-wise convolution with large kernels in the flow estimator that simultaneously reduces the parameters and enhances the receptive field for encoding rich context and handling complex motion.","Secondly, diverging from a common design for the refinement module with a UNet-structure (encoder-decoder structure), which we find redundant, our decoder-only refinement module directly enhances the result from coarse to fine features, offering a more efficient process.","In addition, to address the challenge of handling high-definition frames, we also introduce an innovative HD-aware augmentation strategy during training, leading to consistent enhancement on HD images.","Extensive experiments are conducted on diverse datasets, Vimeo90K, UCF101, Xiph and SNU-FILM.","The results demonstrate that our approach achieves state-of-the-art performance with clear improvement while requiring much less FLOPs and parameters, reaching to a better spot for balancing efficiency and quality."],"url":"http://arxiv.org/abs/2404.11108v1","category":"cs.CV"}
{"created":"2024-04-17 06:45:05","title":"KernJC: Automated Vulnerable Environment Generation for Linux Kernel Vulnerabilities","abstract":"Linux kernel vulnerability reproduction is a critical task in system security. To reproduce a kernel vulnerability, the vulnerable environment and the Proof of Concept (PoC) program are needed. Most existing research focuses on the generation of PoC, while the construction of environment is overlooked. However, establishing an effective vulnerable environment to trigger a vulnerability is challenging. Firstly, it is hard to guarantee that the selected kernel version for reproduction is vulnerable, as the vulnerability version claims in online databases can occasionally be spurious. Secondly, many vulnerabilities can not be reproduced in kernels built with default configurations. Intricate non-default kernel configurations must be set to include and trigger a kernel vulnerability, but less information is available on how to recognize these configurations.   To solve these challenges, we propose a patch-based approach to identify real vulnerable kernel versions and a graph-based approach to identify necessary configs for activating a specific vulnerability. We implement these approaches in a tool, KernJC, automating the generation of vulnerable environments for kernel vulnerabilities. To evaluate the efficacy of KernJC, we build a dataset containing 66 representative real-world vulnerabilities with PoCs from kernel vulnerability research in the past five years. The evaluation shows that KernJC builds vulnerable environments for all these vulnerabilities, 48.5% of which require non-default configs, and 4 have incorrect version claims in the National Vulnerability Database (NVD). Furthermore, we conduct large-scale spurious version detection on kernel vulnerabilities and identify 128 vulnerabilities which have spurious version claims in NVD. To foster future research, we release KernJC with the dataset in the community.","sentences":["Linux kernel vulnerability reproduction is a critical task in system security.","To reproduce a kernel vulnerability, the vulnerable environment and the Proof of Concept (PoC) program are needed.","Most existing research focuses on the generation of PoC, while the construction of environment is overlooked.","However, establishing an effective vulnerable environment to trigger a vulnerability is challenging.","Firstly, it is hard to guarantee that the selected kernel version for reproduction is vulnerable, as the vulnerability version claims in online databases can occasionally be spurious.","Secondly, many vulnerabilities can not be reproduced in kernels built with default configurations.","Intricate non-default kernel configurations must be set to include and trigger a kernel vulnerability, but less information is available on how to recognize these configurations.   ","To solve these challenges, we propose a patch-based approach to identify real vulnerable kernel versions and a graph-based approach to identify necessary configs for activating a specific vulnerability.","We implement these approaches in a tool, KernJC, automating the generation of vulnerable environments for kernel vulnerabilities.","To evaluate the efficacy of KernJC, we build a dataset containing 66 representative real-world vulnerabilities with PoCs from kernel vulnerability research in the past five years.","The evaluation shows that KernJC builds vulnerable environments for all these vulnerabilities, 48.5% of which require non-default configs, and 4 have incorrect version claims in the National Vulnerability Database (NVD).","Furthermore, we conduct large-scale spurious version detection on kernel vulnerabilities and identify 128 vulnerabilities which have spurious version claims in NVD.","To foster future research, we release KernJC with the dataset in the community."],"url":"http://arxiv.org/abs/2404.11107v1","category":"cs.CR"}
{"created":"2024-04-17 06:43:02","title":"Characterizing Requirements Smells","abstract":"Context: Software specifications are usually written in natural language and may suffer from imprecision, ambiguity, and other quality issues, called thereafter, requirement smells. Requirement smells can hinder the development of a project in many aspects, such as delays, reworks, and low customer satisfaction. From an industrial perspective, we want to focus our time and effort on identifying and preventing the requirement smells that are of high interest. Aim: This paper aims to characterise 12 requirements smells in terms of frequency, severity, and effects. Method: We interviewed ten experienced practitioners from different divisions of a large international company in the safety-critical domain called MBDA Italy Spa. Results: Our interview shows that the smell types perceived as most severe are Ambiguity and Verifiability, while as most frequent are Ambiguity and Complexity. We also provide a set of six lessons learnt about requirements smells, such as that effects of smells are expected to differ across smell types. Conclusions: Our results help to increase awareness about the importance of requirement smells. Our results pave the way for future empirical investigations, ranging from a survey confirming our findings to controlled experiments measuring the effect size of specific requirement smells.","sentences":["Context: Software specifications are usually written in natural language and may suffer from imprecision, ambiguity, and other quality issues, called thereafter, requirement smells.","Requirement smells can hinder the development of a project in many aspects, such as delays, reworks, and low customer satisfaction.","From an industrial perspective, we want to focus our time and effort on identifying and preventing the requirement smells that are of high interest.","Aim: This paper aims to characterise 12 requirements smells in terms of frequency, severity, and effects.","Method: We interviewed ten experienced practitioners from different divisions of a large international company in the safety-critical domain called MBDA Italy Spa.","Results: Our interview shows that the smell types perceived as most severe are Ambiguity and Verifiability, while as most frequent are Ambiguity and Complexity.","We also provide a set of six lessons learnt about requirements smells, such as that effects of smells are expected to differ across smell types.","Conclusions: Our results help to increase awareness about the importance of requirement smells.","Our results pave the way for future empirical investigations, ranging from a survey confirming our findings to controlled experiments measuring the effect size of specific requirement smells."],"url":"http://arxiv.org/abs/2404.11106v1","category":"cs.SE"}
{"created":"2024-04-17 06:42:26","title":"XMiner: Efficient Directed Subgraph Matching with Pattern Reduction","abstract":"Graph pattern matching, one of the fundamental graph mining problems, aims to extract structural patterns of interest from an input graph. The state-of-the-art graph matching algorithms and systems are mainly designed for undirected graphs. Directed graph matching is more complex than undirected graph matching because the edge direction must be taken into account before the exploration of each directed edge. Thus, the technologies (e.g. storage, exploiting symmetry for graph matching) for undirected graph matching may not be fully applicable to directed graphs. For example, the redundancy implied in directed graph pattern can not be detected using the symmetry breaking for undirected pattern graph. Here, we present XMiner for efficient directed graph pattern matching whose core idea is 'pattern reduction'. It first analyzes the relationship between constraints implied in a pattern digraph. Then it reduces the pattern graph into a simplified form by finding a minimum constraint cover. Finally, XMiner generates an execution plan and follows it to extract matchings of the pattern graph. So, XMiner works on simplified pattern graph and avoids much data access and redundant computation throughout the matching process. Our experimental results show that XMiner outperforms state-of the-art stand-alone graph matching systems, and scales to complex graph pattern matching tasks on larger graph.","sentences":["Graph pattern matching, one of the fundamental graph mining problems, aims to extract structural patterns of interest from an input graph.","The state-of-the-art graph matching algorithms and systems are mainly designed for undirected graphs.","Directed graph matching is more complex than undirected graph matching because the edge direction must be taken into account before the exploration of each directed edge.","Thus, the technologies (e.g. storage, exploiting symmetry for graph matching) for undirected graph matching may not be fully applicable to directed graphs.","For example, the redundancy implied in directed graph pattern can not be detected using the symmetry breaking for undirected pattern graph.","Here, we present XMiner for efficient directed graph pattern matching whose core idea is 'pattern reduction'.","It first analyzes the relationship between constraints implied in a pattern digraph.","Then it reduces the pattern graph into a simplified form by finding a minimum constraint cover.","Finally, XMiner generates an execution plan and follows it to extract matchings of the pattern graph.","So, XMiner works on simplified pattern graph and avoids much data access and redundant computation throughout the matching process.","Our experimental results show that XMiner outperforms state-of the-art stand-alone graph matching systems, and scales to complex graph pattern matching tasks on larger graph."],"url":"http://arxiv.org/abs/2404.11105v1","category":"cs.DB"}
{"created":"2024-04-17 06:37:19","title":"Solutions for several systems of algebraic differential-difference equations in $\\mathbb{C}^n$","abstract":"In this article, we introduce a new method for solving general quadratic functional equations in $\\mathbb{C}^n$. By utilizing Nevanlinna theory in $\\mathbb{C}^n$, we explore the existence and form of solutions for the several systems of general quadratic difference or partial differential-difference equations of the form $af^2 + 2\\alpha fg + bg^2 + 2\\beta f + 2\\gamma g + C=0$, where $f$ and $g$ are non-constant meromorphic functions in $\\mathbb{C}^n$. The results in this paper are some improvements and generalizations of several existing results on the existence and forms of solutions for these systems. Specifically, the existing results were related to Circular-type $ f^2+g^2=1 $ and quadratic trinomial functional equations $ f^2+2\\alpha fg+g^2=1 $. We provide examples to illustrate the validity of our conclusions regarding the existence and forms of solutions for such system of equations.","sentences":["In this article, we introduce a new method for solving general quadratic functional equations in $\\mathbb{C}^n$. By utilizing Nevanlinna theory in $\\mathbb{C}^n$, we explore the existence and form of solutions for the several systems of general quadratic difference or partial differential-difference equations of the form $af^2 + 2\\alpha fg + bg^2 + 2\\beta f + 2\\gamma g + C=0$, where $f$ and $g$ are non-constant meromorphic functions in $\\mathbb{C}^n$. The results in this paper are some improvements and generalizations of several existing results on the existence and forms of solutions for these systems.","Specifically, the existing results were related to Circular-type $ f^2+g^2=1 $ and quadratic trinomial functional equations $ f^2+2\\alpha fg+g^2=1 $.","We provide examples to illustrate the validity of our conclusions regarding the existence and forms of solutions for such system of equations."],"url":"http://arxiv.org/abs/2404.11102v1","category":"math.CV"}
{"created":"2024-04-17 06:36:17","title":"Synthesizing Realistic Data for Table Recognition","abstract":"To overcome the limitations and challenges of current automatic table data annotation methods and random table data synthesis approaches, we propose a novel method for synthesizing annotation data specifically designed for table recognition. This method utilizes the structure and content of existing complex tables, facilitating the efficient creation of tables that closely replicate the authentic styles found in the target domain. By leveraging the actual structure and content of tables from Chinese financial announcements, we have developed the first extensive table annotation dataset in this domain. We used this dataset to train several recent deep learning-based end-to-end table recognition models. Additionally, we have established the inaugural benchmark for real-world complex tables in the Chinese financial announcement domain, using it to assess the performance of models trained on our synthetic data, thereby effectively validating our method's practicality and effectiveness. Furthermore, we applied our synthesis method to augment the FinTabNet dataset, extracted from English financial announcements, by increasing the proportion of tables with multiple spanning cells to introduce greater complexity. Our experiments show that models trained on this augmented dataset achieve comprehensive improvements in performance, especially in the recognition of tables with multiple spanning cells.","sentences":["To overcome the limitations and challenges of current automatic table data annotation methods and random table data synthesis approaches, we propose a novel method for synthesizing annotation data specifically designed for table recognition.","This method utilizes the structure and content of existing complex tables, facilitating the efficient creation of tables that closely replicate the authentic styles found in the target domain.","By leveraging the actual structure and content of tables from Chinese financial announcements, we have developed the first extensive table annotation dataset in this domain.","We used this dataset to train several recent deep learning-based end-to-end table recognition models.","Additionally, we have established the inaugural benchmark for real-world complex tables in the Chinese financial announcement domain, using it to assess the performance of models trained on our synthetic data, thereby effectively validating our method's practicality and effectiveness.","Furthermore, we applied our synthesis method to augment the FinTabNet dataset, extracted from English financial announcements, by increasing the proportion of tables with multiple spanning cells to introduce greater complexity.","Our experiments show that models trained on this augmented dataset achieve comprehensive improvements in performance, especially in the recognition of tables with multiple spanning cells."],"url":"http://arxiv.org/abs/2404.11100v1","category":"cs.CV"}
{"created":"2024-04-17 06:29:49","title":"Review of Automaton Learning Algorithms with Polynomial Complexity -- Completely Solved Examples","abstract":"Automaton learning is a domain in which the target system is inferred by the automaton learning algorithm in the form of an automaton, by synthesizing a finite number of inputs and their corresponding outputs. Automaton learning makes use of a Minimally Adequate Teacher (MAT). The learner learns the target system by posing membership queries to the MAT. In this chapter, I have provided completely solved examples of automaton learning algorithms. According to the best of my knowledge these are not available in any other source.","sentences":["Automaton learning is a domain in which the target system is inferred by the automaton learning algorithm in the form of an automaton, by synthesizing a finite number of inputs and their corresponding outputs.","Automaton learning makes use of a Minimally Adequate Teacher (MAT).","The learner learns the target system by posing membership queries to the MAT.","In this chapter, I have provided completely solved examples of automaton learning algorithms.","According to the best of my knowledge these are not available in any other source."],"url":"http://arxiv.org/abs/2404.11096v1","category":"cs.FL"}
{"created":"2024-04-17 06:25:31","title":"Periodic boundary points for simply connected Fatou components of transcendental maps","abstract":"Let f be a transcendental map, and let U be an attracting or parabolic basin, or a doubly parabolic Baker domain. Assume U is simply connected. Then, we prove that periodic points are dense in the boundary of U, under certain hypothesis on the postsingular set. This generalizes a result by F. Przytycki and A. Zdunik for rational maps. Our proof uses techniques from measure theory, ergodic theory, conformal analysis, and inner functions. In particular, a result on the distortion of inner functions near the unit circle is provided, which is of independent interest.","sentences":["Let f be a transcendental map, and let U be an attracting or parabolic basin, or a doubly parabolic Baker domain.","Assume U is simply connected.","Then, we prove that periodic points are dense in the boundary of U, under certain hypothesis on the postsingular set.","This generalizes a result by F. Przytycki and A. Zdunik for rational maps.","Our proof uses techniques from measure theory, ergodic theory, conformal analysis, and inner functions.","In particular, a result on the distortion of inner functions near the unit circle is provided, which is of independent interest."],"url":"http://arxiv.org/abs/2404.11094v1","category":"math.DS"}
{"created":"2024-04-17 06:13:06","title":"Global solutions for semilinear parabolic evolution problems with H\u00f6lder continuous nonlinearities","abstract":"It is shown that semilinear parabolic evolution equations $u'=A+f(t,u)$ featuring H\\\"older continuous nonlinearities $ f=f(t,u)$ with at most linear growth possess global strong solutions for a general class of initial data. The abstract results are applied to a recent model describing front propagation in bushfires and in the context of a reaction-diffusion system.","sentences":["It is shown that semilinear parabolic evolution equations $u'=A+f(t,u)$ featuring H\\\"older continuous nonlinearities $ f=f(t,u)$ with at most linear growth possess global strong solutions for a general class of initial data.","The abstract results are applied to a recent model describing front propagation in bushfires and in the context of a reaction-diffusion system."],"url":"http://arxiv.org/abs/2404.11089v1","category":"math.AP"}
{"created":"2024-04-17 06:06:36","title":"High fidelity adaptive mirror simulations with reduced order models","abstract":"In the design process of large adaptive mirrors numerical simulations represent the first step to evaluate the system design compliance in terms of performance, stability and robustness. For the next generation of Extremely Large Telescopes increased system dimensions and bandwidths lead to the need of modeling not only the deformable mirror alone, but also all the system supporting structure or even the full telescope. The capability to perform the simulations with an acceptable amount of time and computational resources is highly dependent on finding appropriate methods to reduce the size of the resulting dynamic models. In this paper we present a framework developed together with the company Microgate to create a reduced order structural model of a large adaptive mirror as a preprocessing step to the control system simulations. The reduced dynamic model is then combined with the remaining system components allowing to simulate the full adaptive mirror in a computationally efficient way. We analyze the feasibility of our reduced models for Microgate's prototype of the adaptive mirror of the Giant Magellan Telescope.","sentences":["In the design process of large adaptive mirrors numerical simulations represent the first step to evaluate the system design compliance in terms of performance, stability and robustness.","For the next generation of Extremely Large Telescopes increased system dimensions and bandwidths lead to the need of modeling not only the deformable mirror alone, but also all the system supporting structure or even the full telescope.","The capability to perform the simulations with an acceptable amount of time and computational resources is highly dependent on finding appropriate methods to reduce the size of the resulting dynamic models.","In this paper we present a framework developed together with the company Microgate to create a reduced order structural model of a large adaptive mirror as a preprocessing step to the control system simulations.","The reduced dynamic model is then combined with the remaining system components allowing to simulate the full adaptive mirror in a computationally efficient way.","We analyze the feasibility of our reduced models for Microgate's prototype of the adaptive mirror of the Giant Magellan Telescope."],"url":"http://arxiv.org/abs/2404.11088v1","category":"astro-ph.IM"}
{"created":"2024-04-17 06:00:01","title":"FrackyFrac: A Standalone UniFrac Calculator","abstract":"UniFrac is a family of distance metrics over microbial abundances, that take taxonomic relatedness into account. Current tools and libraries for calculating UniFrac have specific requirements regarding the user's technical expertise, operating system, and pre-installed software, which might exclude potential users. FrackyFrac is a native command-line tool that can run on any platform and has no requirements. It can also generate the phylogenetic trees required for the calculation. We show that FrackyFrac's performance is on par with currently existing implementations. FrackyFrac can make UniFrac accessible to researchers who may otherwise skip it due to the effort involved, and it can simplify analysis pipelines for those who already use it.","sentences":["UniFrac is a family of distance metrics over microbial abundances, that take taxonomic relatedness into account.","Current tools and libraries for calculating UniFrac have specific requirements regarding the user's technical expertise, operating system, and pre-installed software, which might exclude potential users.","FrackyFrac is a native command-line tool that can run on any platform and has no requirements.","It can also generate the phylogenetic trees required for the calculation.","We show that FrackyFrac's performance is on par with currently existing implementations.","FrackyFrac can make UniFrac accessible to researchers who may otherwise skip it due to the effort involved, and it can simplify analysis pipelines for those who already use it."],"url":"http://arxiv.org/abs/2404.11087v1","category":"q-bio.QM"}
{"created":"2024-04-17 05:56:10","title":"$A$-hypergeometric series with parameters in the core","abstract":"In this paper, we discuss conditions for applying Frobenius's method to $A$-hypergeometric systems introduced by Okuyama and Saito [3]. We prove that, when the negative support of a fake exponent $v$ with respect to a generic weight $w$ is included in the core, we can construct all A-hypergeometric series with exponent $v$ in the direction $w$ by Frobenius's method. As an example, we describe the Aomoto-Gel'fand system of type $3 \\times 3$ in details.","sentences":["In this paper, we discuss conditions for applying Frobenius's method to $A$-hypergeometric systems introduced by Okuyama and Saito","[3].","We prove that, when the negative support of a fake exponent $v$ with respect to a generic weight $w$ is included in the core, we can construct all A-hypergeometric series with exponent $v$ in the direction $w$ by Frobenius's method.","As an example, we describe the Aomoto-Gel'fand system of type $3 \\times 3$ in details."],"url":"http://arxiv.org/abs/2404.11085v1","category":"math.AG"}
{"created":"2024-04-17 05:40:08","title":"Accuracy guarantees and quantum advantage in analogue open quantum simulation with and without noise","abstract":"Many-body open quantum systems, described by Lindbladian master equations, are a rich class of physical models that display complex equilibrium and out-of-equilibrium phenomena which remain to be understood. In this paper, we theoretically analyze noisy analogue quantum simulation of geometrically local open quantum systems and provide evidence that this problem is both hard to simulate on classical computers and could be approximately solved on near-term quantum devices. First, given a noiseless quantum simulator, we show that the dynamics of local observables and the fixed-point expectation values of rapidly-mixing local observables in geometrically local Lindbladians can be obtained to a precision of $\\varepsilon$ in time that is $\\text{poly}(\\varepsilon^{-1})$ and uniform in system size. Furthermore, we establish that the quantum simulator would provide an exponential advantage, in run-time scaling with respect to the target precision and either the evolution time (when simulating dynamics) or the Lindbladian's decay rate (when simulating fixed-points) over any classical algorithm for these problems unless BQP = BPP. We then consider the presence of noise in the quantum simulator in the form of additional geometrically-local Linbdladian terms. We show that the simulation tasks considered in this paper are stable to errors, i.e. they can be solved to a noise-limited, but system-size independent, precision. Finally, we establish that there are stable geometrically local Lindbladian simulation problems such that as the noise rate on the simulator is reduced, classical algorithms must take time exponentially longer in the inverse noise rate to attain the same precision unless BQP = BPP.","sentences":["Many-body open quantum systems, described by Lindbladian master equations, are a rich class of physical models that display complex equilibrium and out-of-equilibrium phenomena which remain to be understood.","In this paper, we theoretically analyze noisy analogue quantum simulation of geometrically local open quantum systems and provide evidence that this problem is both hard to simulate on classical computers and could be approximately solved on near-term quantum devices.","First, given a noiseless quantum simulator, we show that the dynamics of local observables and the fixed-point expectation values of rapidly-mixing local observables in geometrically local Lindbladians can be obtained to a precision of $\\varepsilon$ in time that is $\\text{poly}(\\varepsilon^{-1})$ and uniform in system size.","Furthermore, we establish that the quantum simulator would provide an exponential advantage, in run-time scaling with respect to the target precision and either the evolution time (when simulating dynamics) or the Lindbladian's decay rate (when simulating fixed-points) over any classical algorithm for these problems unless BQP = BPP.","We then consider the presence of noise in the quantum simulator in the form of additional geometrically-local Linbdladian terms.","We show that the simulation tasks considered in this paper are stable to errors, i.e. they can be solved to a noise-limited, but system-size independent, precision.","Finally, we establish that there are stable geometrically local Lindbladian simulation problems such that as the noise rate on the simulator is reduced, classical algorithms must take time exponentially longer in the inverse noise rate to attain the same precision unless BQP = BPP."],"url":"http://arxiv.org/abs/2404.11081v1","category":"quant-ph"}
{"created":"2024-04-17 05:29:44","title":"Sylow theorems for supergroups","abstract":"We introduce Sylow subgroups and $0$-groups to the theory of complex algebraic supergroups, which mimic Sylow subgroups and $p$-groups in the theory of finite groups. We prove that Sylow subgroups are always $0$-groups, and show that they are unique up to conjugacy. Further, we give an explicit classification of $0$-groups which will be very useful for future applications. Finally, we prove an analogue of Sylow's third theorem on the number of Sylow subgroups of a supergroup.","sentences":["We introduce Sylow subgroups and $0$-groups to the theory of complex algebraic supergroups, which mimic Sylow subgroups and $p$-groups in the theory of finite groups.","We prove that Sylow subgroups are always $0$-groups, and show that they are unique up to conjugacy.","Further, we give an explicit classification of $0$-groups which will be very useful for future applications.","Finally, we prove an analogue of Sylow's third theorem on the number of Sylow subgroups of a supergroup."],"url":"http://arxiv.org/abs/2404.11077v1","category":"math.RT"}
{"created":"2024-04-17 05:28:56","title":"Do you need a DAO?","abstract":"Decentralized Autonomous Organizations (DAOs) have seen exponential growth and interest due to their potential to redefine organizational structure and governance. Despite this, there is a discrepancy between the ideals of autonomy and decentralization and the actual experiences of DAO stakeholders. The Information Systems (IS) literature has yet to fully explore whether DAOs are the optimal organizational choice. Addressing this gap, our research asks, \"Is a DAO suitable for your organizational needs?\" We derive a gated decision-making framework through a thematic review of the academic and grey literature on DAOs. Through five scenarios, the framework critically emphasizes the gaps between DAOs' theoretical capabilities and practical challenges. Our findings contribute to the IS discourse on blockchain technologies, with some ancillary contributions to the IS literature on organizational management and practitioner literature.","sentences":["Decentralized Autonomous Organizations (DAOs) have seen exponential growth and interest due to their potential to redefine organizational structure and governance.","Despite this, there is a discrepancy between the ideals of autonomy and decentralization and the actual experiences of DAO stakeholders.","The Information Systems (IS) literature has yet to fully explore whether DAOs are the optimal organizational choice.","Addressing this gap, our research asks, \"Is a DAO suitable for your organizational needs?\"","We derive a gated decision-making framework through a thematic review of the academic and grey literature on DAOs.","Through five scenarios, the framework critically emphasizes the gaps between DAOs' theoretical capabilities and practical challenges.","Our findings contribute to the IS discourse on blockchain technologies, with some ancillary contributions to the IS literature on organizational management and practitioner literature."],"url":"http://arxiv.org/abs/2404.11076v1","category":"cs.HC"}
{"created":"2024-04-17 05:14:19","title":"Characterisation of the TOI-421 planetary system using CHEOPS, TESS, and archival radial velocity data","abstract":"The TOI-421 planetary system contains two sub-Neptune-type planets and is a prime target to study the formation and evolution of planets and their atmospheres. The inner planet is especially interesting as the existence of a hydrogen-dominated atmosphere at its orbital separation cannot be explained by current formation models without previous orbital migration. We jointly analysed photometric data of three TESS sectors and six CHEOPS visits as well as 156 radial velocity data points to retrieve improved planetary parameters. We also searched for TTVs and modelled the interior structure of the planets. Finally, we simulated the evolution of the primordial H-He atmospheres of the planets using two different modelling frameworks. We determine the planetary radii and masses of TOI-421 b and c to be $R_{\\rm b} = 2.64 \\pm 0.08 \\, R_{\\oplus}$, $M_{\\rm b} = 6.7 \\pm 0.6 \\, M_{\\oplus}$, $R_{\\rm c} = 5.09 \\pm 0.07 \\, R_{\\oplus}$, and $M_{\\rm c} = 14.1 \\pm 1.4 \\, M_{\\oplus}$. We do not detect any statistically significant TTV signals. Assuming the presence of a hydrogen-dominated atmosphere, the interior structure modelling results in both planets having extensive envelopes. While the modelling of the atmospheric evolution predicts for TOI-421 b to have lost any primordial atmosphere that it could have accreted at its current orbital position, TOI-421 c could have started out with an initial atmospheric mass fraction somewhere between 10 and 35%. We conclude that the low observed mean density of TOI-421 b can only be explained by either a bias in the measured planetary parameters (e.g. driven by high-altitude clouds) and/or in the context of orbital migration. We also find that the results of atmospheric evolution models are strongly dependent on the employed planetary structure model.","sentences":["The TOI-421 planetary system contains two sub-Neptune-type planets and is a prime target to study the formation and evolution of planets and their atmospheres.","The inner planet is especially interesting as the existence of a hydrogen-dominated atmosphere at its orbital separation cannot be explained by current formation models without previous orbital migration.","We jointly analysed photometric data of three TESS sectors and six CHEOPS visits as well as 156 radial velocity data points to retrieve improved planetary parameters.","We also searched for TTVs and modelled the interior structure of the planets.","Finally, we simulated the evolution of the primordial H-He atmospheres of the planets using two different modelling frameworks.","We determine the planetary radii and masses of TOI-421 b and c to be $R_{\\rm b} = 2.64 \\pm 0.08 \\, R_{\\oplus}$, $M_{\\rm b} = 6.7 \\pm 0.6 \\, M_{\\oplus}$, $R_{\\rm c} = 5.09 \\pm 0.07 \\, R_{\\oplus}$, and $M_{\\rm c} = 14.1 \\pm 1.4 \\, M_{\\oplus}$.","We do not detect any statistically significant TTV signals.","Assuming the presence of a hydrogen-dominated atmosphere, the interior structure modelling results in both planets having extensive envelopes.","While the modelling of the atmospheric evolution predicts for TOI-421 b to have lost any primordial atmosphere that it could have accreted at its current orbital position, TOI-421 c could have started out with an initial atmospheric mass fraction somewhere between 10 and 35%.","We conclude that the low observed mean density of TOI-421 b can only be explained by either a bias in the measured planetary parameters (e.g. driven by high-altitude clouds)","and/or in the context of orbital migration.","We also find that the results of atmospheric evolution models are strongly dependent on the employed planetary structure model."],"url":"http://arxiv.org/abs/2404.11074v1","category":"astro-ph.EP"}
{"created":"2024-04-17 04:59:36","title":"Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based sky-segmentation in urban canyon","abstract":"Accurate, continuous, and reliable positioning is a critical component of achieving autonomous driving. However, in complex urban canyon environments, the vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused by high buildings, trees, and elevated structures seriously affect positioning results. To address these challenges, a sky-view images segmentation algorithm based on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection. Building upon this, a novel NLOS detection and mitigation algorithm (named S-NDM) is extended to the tightly coupled Global Navigation Satellite Systems (GNSS), Inertial Measurement Units (IMU), and visual feature system which is called Sky-GVIO, with the aim of achieving continuous and accurate positioning in urban canyon environments. Furthermore, the system harmonizes Single Point Positioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its operational versatility and resilience. In urban canyon environments, the positioning performance of S-NDM algorithm proposed in this paper is evaluated under different tightly coupled SPP-related and RTK-related models. The results exhibit that Sky-GVIO system achieves meter-level accuracy under SPP mode and sub-decimeter precision with RTK, surpassing the performance of GNSS/INS/Vision frameworks devoid of S-NDM. Additionally, the sky-view image dataset, inclusive of training and evaluation subsets, has been made publicly accessible for scholarly exploration at https://github.com/whuwangjr/sky-view-images .","sentences":["Accurate, continuous, and reliable positioning is a critical component of achieving autonomous driving.","However, in complex urban canyon environments, the vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused by high buildings, trees, and elevated structures seriously affect positioning results.","To address these challenges, a sky-view images segmentation algorithm based on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection.","Building upon this, a novel NLOS detection and mitigation algorithm (named S-NDM) is extended to the tightly coupled Global Navigation Satellite Systems (GNSS), Inertial Measurement Units (IMU), and visual feature system which is called Sky-GVIO, with the aim of achieving continuous and accurate positioning in urban canyon environments.","Furthermore, the system harmonizes Single Point Positioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its operational versatility and resilience.","In urban canyon environments, the positioning performance of S-NDM algorithm proposed in this paper is evaluated under different tightly coupled SPP-related and RTK-related models.","The results exhibit that Sky-GVIO system achieves meter-level accuracy under SPP mode and sub-decimeter precision with RTK, surpassing the performance of GNSS/INS/Vision frameworks devoid of S-NDM.","Additionally, the sky-view image dataset, inclusive of training and evaluation subsets, has been made publicly accessible for scholarly exploration at https://github.com/whuwangjr/sky-view-images ."],"url":"http://arxiv.org/abs/2404.11070v1","category":"cs.CV"}
{"created":"2024-04-17 04:55:25","title":"Complex hypergraph analysis of Australian MPs' professional connections, 1947-2019","abstract":"We propose a suit of methods to analyse the professional networks of MPs, showing how to analyse weak-tie connections between legislators and the connections between background charactersitic attributes. Applied to a novel dataset on the backgrounds of Australian MPs in the Australian Labor Party and the Liberal Party of Australia (1947-2019), we show that our approach can help to describe and explain the decline in working-class and trade unionist MPs from the Labor Party, the homogeneous elitism of the mid-20 century Liberal Party, and the increasing similarity of both parties' professional networks, occuring in the period of party cartellisation from the 1980s onward. Our paper's finding show that our method has clear potential for broader applications in the study of political representation, diversity, and elite political networks.","sentences":["We propose a suit of methods to analyse the professional networks of MPs, showing how to analyse weak-tie connections between legislators and the connections between background charactersitic attributes.","Applied to a novel dataset on the backgrounds of Australian MPs in the Australian Labor Party and the Liberal Party of Australia (1947-2019), we show that our approach can help to describe and explain the decline in working-class and trade unionist MPs from the Labor Party, the homogeneous elitism of the mid-20 century Liberal Party, and the increasing similarity of both parties' professional networks, occuring in the period of party cartellisation from the 1980s onward.","Our paper's finding show that our method has clear potential for broader applications in the study of political representation, diversity, and elite political networks."],"url":"http://arxiv.org/abs/2404.11067v1","category":"cs.SI"}
{"created":"2024-04-17 04:49:19","title":"PT Symmetry, induced mechanical lasing and tunable force sensing in a coupled-mode optically levitated nanoparticle","abstract":"We theoretically investigate PT symmetry, induced mechanical lasing and force sensing in an optically levitated nanoparticle with coupled oscillation modes. The coupling in the levitated system is created by the modulation of an asymmetric optical potential in the plane transverse to the beam trapping the nanoparticle. We show that such a coupling can lead to PT-symmetric mechanical behavior for experimentally realistic parameters. Further, by examining the phonon dynamics and the second-order coherence of the nanoparticle modes, we determine that induced mechanical lasing is also possible. Finally, we demonstrate that tunable ultra-sensitive force sensing can be engineered in the system. Our studies represent an advance in the fields of coherent manipulation of coupled degrees of freedom of levitated mechanical oscillators and their application for sensing.","sentences":["We theoretically investigate PT symmetry, induced mechanical lasing and force sensing in an optically levitated nanoparticle with coupled oscillation modes.","The coupling in the levitated system is created by the modulation of an asymmetric optical potential in the plane transverse to the beam trapping the nanoparticle.","We show that such a coupling can lead to PT-symmetric mechanical behavior for experimentally realistic parameters.","Further, by examining the phonon dynamics and the second-order coherence of the nanoparticle modes, we determine that induced mechanical lasing is also possible.","Finally, we demonstrate that tunable ultra-sensitive force sensing can be engineered in the system.","Our studies represent an advance in the fields of coherent manipulation of coupled degrees of freedom of levitated mechanical oscillators and their application for sensing."],"url":"http://arxiv.org/abs/2404.11065v1","category":"quant-ph"}
{"created":"2024-04-17 04:37:58","title":"Unified Examination of Entity Linking in Absence of Candidate Sets","abstract":"Despite remarkable strides made in the development of entity linking systems in recent years, a comprehensive comparative analysis of these systems using a unified framework is notably absent. This paper addresses this oversight by introducing a new black-box benchmark and conducting a comprehensive evaluation of all state-of-the-art entity linking methods. We use an ablation study to investigate the impact of candidate sets on the performance of entity linking. Our findings uncover exactly how much such entity linking systems depend on candidate sets, and how much this limits the general applicability of each system. We present an alternative approach to candidate sets, demonstrating that leveraging the entire in-domain candidate set can serve as a viable substitute for certain models. We show the trade-off between less restrictive candidate sets, increased inference time and memory footprint for some models.","sentences":["Despite remarkable strides made in the development of entity linking systems in recent years, a comprehensive comparative analysis of these systems using a unified framework is notably absent.","This paper addresses this oversight by introducing a new black-box benchmark and conducting a comprehensive evaluation of all state-of-the-art entity linking methods.","We use an ablation study to investigate the impact of candidate sets on the performance of entity linking.","Our findings uncover exactly how much such entity linking systems depend on candidate sets, and how much this limits the general applicability of each system.","We present an alternative approach to candidate sets, demonstrating that leveraging the entire in-domain candidate set can serve as a viable substitute for certain models.","We show the trade-off between less restrictive candidate sets, increased inference time and memory footprint for some models."],"url":"http://arxiv.org/abs/2404.11061v1","category":"cs.CL"}
{"created":"2024-04-17 04:23:36","title":"Multimodal Fusion of Echocardiography and Electronic Health Records for the Detection of Cardiac Amyloidosis","abstract":"Cardiac amyloidosis, a rare and highly morbid condition, presents significant challenges for detection through echocardiography. Recently, there has been a surge in proposing machine-learning algorithms to identify cardiac amyloidosis, with the majority being imaging-based deep-learning approaches that require extensive data. In this study, we introduce a novel transformer-based multimodal fusion algorithm that leverages information from both imaging and electronic health records. Specifically, our approach utilizes echocardiography videos from both the parasternal long-axis (PLAX) view and the apical 4-chamber (A4C) view along with patients' demographic data, laboratory tests, and cardiac metrics to predict the probability of cardiac amyloidosis. We evaluated our method using 5-fold cross-validation on a dataset comprising 41 patients and achieved an Area Under the Receiver Operating Characteristic curve (AUROC) of 0.94. The experimental results demonstrate that our approach can achieve competitive results with a significantly smaller dataset compared to prior imaging-based methods that required data from thousands of patients. This underscores the potential of leveraging multimodal data to enhance diagnostic accuracy in the identification of complex cardiac conditions such as cardiac amyloidosis.","sentences":["Cardiac amyloidosis, a rare and highly morbid condition, presents significant challenges for detection through echocardiography.","Recently, there has been a surge in proposing machine-learning algorithms to identify cardiac amyloidosis, with the majority being imaging-based deep-learning approaches that require extensive data.","In this study, we introduce a novel transformer-based multimodal fusion algorithm that leverages information from both imaging and electronic health records.","Specifically, our approach utilizes echocardiography videos from both the parasternal long-axis (PLAX) view and the apical 4-chamber (A4C) view along with patients' demographic data, laboratory tests, and cardiac metrics to predict the probability of cardiac amyloidosis.","We evaluated our method using 5-fold cross-validation on a dataset comprising 41 patients and achieved an Area Under the Receiver Operating Characteristic curve (AUROC) of 0.94.","The experimental results demonstrate that our approach can achieve competitive results with a significantly smaller dataset compared to prior imaging-based methods that required data from thousands of patients.","This underscores the potential of leveraging multimodal data to enhance diagnostic accuracy in the identification of complex cardiac conditions such as cardiac amyloidosis."],"url":"http://arxiv.org/abs/2404.11058v1","category":"eess.IV"}
{"created":"2024-04-17 03:54:45","title":"Exploring Supermassive Compact Dark Matter with the Millilensing Effect of Gamma-Ray Bursts","abstract":"Gravitational lensing effect is one of most significant observational probes to investigate compact dark matter/objects over a wide mass range. In this work, we first propose to derive the population information and the abundance of supermassive compact dark matter in the mass range $\\sim10^5-10^7~M_{\\odot}$ from 6 millilensed gamma-ray burst (GRB) candidates in 3000 Fermi GRB events using the hierarchical Bayesian inference method. We obtain that, for the mass range $\\sim10^5-10^7~M_{\\odot}$, the abundance of supermassive compact dark matter is $f_{\\rm CO}=10^{-1.60}$ in the log-normal mass distribution scenario. This result is in obvious tension with some other observational constraints, e.g. ultra-faint dwarfs and dynamical friction. However, it also was argued that there is only one system in these 6 candidates has been identified as lensed GRB event with fairly high confidence. In this case, the tension would be significantly alleviated. Therefore, it would be an interesting clue for both the millilensed GRB identification and the formation mechanism of supermassive compact dark matter.","sentences":["Gravitational lensing effect is one of most significant observational probes to investigate compact dark matter/objects over a wide mass range.","In this work, we first propose to derive the population information and the abundance of supermassive compact dark matter in the mass range $\\sim10^5-10^7~M_{\\odot}$ from 6 millilensed gamma-ray burst (GRB) candidates in 3000 Fermi GRB events using the hierarchical Bayesian inference method.","We obtain that, for the mass range $\\sim10^5-10^7~M_{\\odot}$, the abundance of supermassive compact dark matter is $f_{\\rm CO}=10^{-1.60}$ in the log-normal mass distribution scenario.","This result is in obvious tension with some other observational constraints, e.g. ultra-faint dwarfs and dynamical friction.","However, it also was argued that there is only one system in these 6 candidates has been identified as lensed GRB event with fairly high confidence.","In this case, the tension would be significantly alleviated.","Therefore, it would be an interesting clue for both the millilensed GRB identification and the formation mechanism of supermassive compact dark matter."],"url":"http://arxiv.org/abs/2404.11053v1","category":"astro-ph.HE"}
{"created":"2024-04-17 03:39:02","title":"Asynchronous Memory Access Unit: Exploiting Massive Parallelism for Far Memory Access","abstract":"The growing memory demands of modern applications have driven the adoption of far memory technologies in data centers to provide cost-effective, high-capacity memory solutions. However, far memory presents new performance challenges because its access latencies are significantly longer and more variable than local DRAM. For applications to achieve acceptable performance on far memory, a high degree of memory-level parallelism (MLP) is needed to tolerate the long access latency. While modern out-of-order processors are capable of exploiting a certain degree of MLP, they are constrained by resource limitations and hardware complexity. The key obstacle is the synchronous memory access semantics of traditional load/store instructions, which occupy critical hardware resources for a long time. The longer far memory latencies exacerbate this limitation.   This paper proposes a set of Asynchronous Memory Access Instructions (AMI) and its supporting function unit, Asynchronous Memory Access Unit (AMU), inside a contemporary Out-of-Order Core. AMI separates memory request issuing from response handling to reduce resource occupation. Additionally, AMU architecture supports up to several hundreds of asynchronous memory requests through re-purposing a portion of L2 Cache as scratchpad memory (SPM) to provide sufficient temporal storage. Together with a coroutine-based programming framework, this scheme can achieve significantly higher MLP for hiding far memory latencies.   Evaluation with a cycle-accurate simulation shows AMI achieves 2.42x speedup on average for memory-bound benchmarks with 1us additional far memory latency. Over 130 outstanding requests are supported with 26.86x speedup for GUPS (random access) with 5 us latency. These demonstrate how the techniques tackle far memory performance impacts through explicit MLP expression and latency adaptation.","sentences":["The growing memory demands of modern applications have driven the adoption of far memory technologies in data centers to provide cost-effective, high-capacity memory solutions.","However, far memory presents new performance challenges because its access latencies are significantly longer and more variable than local DRAM.","For applications to achieve acceptable performance on far memory, a high degree of memory-level parallelism (MLP) is needed to tolerate the long access latency.","While modern out-of-order processors are capable of exploiting a certain degree of MLP, they are constrained by resource limitations and hardware complexity.","The key obstacle is the synchronous memory access semantics of traditional load/store instructions, which occupy critical hardware resources for a long time.","The longer far memory latencies exacerbate this limitation.   ","This paper proposes a set of Asynchronous Memory Access Instructions (AMI) and its supporting function unit, Asynchronous Memory Access Unit (AMU), inside a contemporary Out-of-Order Core.","AMI separates memory request issuing from response handling to reduce resource occupation.","Additionally, AMU architecture supports up to several hundreds of asynchronous memory requests through re-purposing a portion of L2 Cache as scratchpad memory (SPM) to provide sufficient temporal storage.","Together with a coroutine-based programming framework, this scheme can achieve significantly higher MLP for hiding far memory latencies.   ","Evaluation with a cycle-accurate simulation shows AMI achieves 2.42x speedup on average for memory-bound benchmarks with 1us additional far memory latency.","Over 130 outstanding requests are supported with 26.86x speedup for GUPS (random access) with 5 us latency.","These demonstrate how the techniques tackle far memory performance impacts through explicit MLP expression and latency adaptation."],"url":"http://arxiv.org/abs/2404.11044v1","category":"cs.AR"}
{"created":"2024-04-17 03:34:36","title":"Polarization phenomenon in heavy-ion collisions","abstract":"The strongly interacting system created in ultrarelativistic nuclear collisions behaves almost as an ideal fluid with rich patterns of the velocity field exhibiting strong vortical structure. Vorticity of the fluid, via spin-orbit coupling, leads to particle spin polarization. Due to the finite orbital momentum of the system, the polarization on average is not zero; it depends on the particle momenta reflecting the spatial variation of the local vorticity. In the last few years, this field experienced a rapid growth due to experimental discoveries of the global and local polarizations. Recent measurements triggered further development of the theoretical description of the spin dynamics and suggestions of several new mechanisms for particle polarization.   In this review, we focus mostly on the experimental results. We compare the measurements with the existing theoretical calculations but try to keep the discussion of possible underlying physics at the qualitative level. Future measurements and how they can help to answer open theoretical questions are also discussed. We pay a special attention to the employed experimental methods, as well as to the detector effects and associated corrections to the measurements.","sentences":["The strongly interacting system created in ultrarelativistic nuclear collisions behaves almost as an ideal fluid with rich patterns of the velocity field exhibiting strong vortical structure.","Vorticity of the fluid, via spin-orbit coupling, leads to particle spin polarization.","Due to the finite orbital momentum of the system, the polarization on average is not zero; it depends on the particle momenta reflecting the spatial variation of the local vorticity.","In the last few years, this field experienced a rapid growth due to experimental discoveries of the global and local polarizations.","Recent measurements triggered further development of the theoretical description of the spin dynamics and suggestions of several new mechanisms for particle polarization.   ","In this review, we focus mostly on the experimental results.","We compare the measurements with the existing theoretical calculations but try to keep the discussion of possible underlying physics at the qualitative level.","Future measurements and how they can help to answer open theoretical questions are also discussed.","We pay a special attention to the employed experimental methods, as well as to the detector effects and associated corrections to the measurements."],"url":"http://arxiv.org/abs/2404.11042v1","category":"nucl-ex"}
{"created":"2024-04-17 03:23:52","title":"Approximate Wireless Communication for Lossy Gradient Updates in IoT Federated Learning","abstract":"Federated learning (FL) has emerged as a distributed machine learning (ML) technique that can protect local data privacy for participating clients and improve system efficiency. Instead of sharing raw data, FL exchanges intermediate learning parameters, such as gradients, among clients. This article presents an efficient wireless communication approach tailored for FL parameter transmission, especially for Internet of Things (IoT) devices, to facilitate model aggregation. Our study considers practical wireless channels that can lead to random bit errors, which can substantially affect FL performance. Motivated by empirical gradient value distribution, we introduce a novel received bit masking method that confines received gradient values within prescribed limits. Moreover, given the intrinsic error resilience of ML gradients, our approach enables the delivery of approximate gradient values with errors without resorting to extensive error correction coding or retransmission. This strategy reduces computational overhead at both the transmitter and the receiver and minimizes communication latency. Consequently, our scheme is particularly well-suited for resource-constrained IoT devices. Additionally, we explore the inherent protection of the most significant bits (MSBs) through gray coding in high-order modulation. Our simulations demonstrate that our proposed scheme can effectively mitigate random bit errors in FL performance, achieving similar learning objectives, but with the 50% air time required by existing methods involving error correction and retransmission.","sentences":["Federated learning (FL) has emerged as a distributed machine learning (ML) technique that can protect local data privacy for participating clients and improve system efficiency.","Instead of sharing raw data, FL exchanges intermediate learning parameters, such as gradients, among clients.","This article presents an efficient wireless communication approach tailored for FL parameter transmission, especially for Internet of Things (IoT) devices, to facilitate model aggregation.","Our study considers practical wireless channels that can lead to random bit errors, which can substantially affect FL performance.","Motivated by empirical gradient value distribution, we introduce a novel received bit masking method that confines received gradient values within prescribed limits.","Moreover, given the intrinsic error resilience of ML gradients, our approach enables the delivery of approximate gradient values with errors without resorting to extensive error correction coding or retransmission.","This strategy reduces computational overhead at both the transmitter and the receiver and minimizes communication latency.","Consequently, our scheme is particularly well-suited for resource-constrained IoT devices.","Additionally, we explore the inherent protection of the most significant bits (MSBs) through gray coding in high-order modulation.","Our simulations demonstrate that our proposed scheme can effectively mitigate random bit errors in FL performance, achieving similar learning objectives, but with the 50% air time required by existing methods involving error correction and retransmission."],"url":"http://arxiv.org/abs/2404.11035v1","category":"cs.IT"}
{"created":"2024-04-17 03:12:25","title":"Lieb-Schultz-Mattis constraints for the insulating phases of the one-dimensional SU($N$) Kondo lattice model","abstract":"The nature of the insulating phases of the SU($N$)-generalization of the one-dimensional Kondo lattice model is investigated by means of non-perturbative approaches. By extending the Lieb-Schultz-Mattis (LSM) argument to multi-component fermion systems with translation and global SU($N$) symmetries, we derive two indices which depend on the filling and the ``SU($N$)-spin'' (representation) of the local moments. These indices strongly constrain possible insulating phases; for instance, when the local moments transform in the $N$-dimensional (defining) representation of SU($N$), a featureless Kondo insulator is possible only at filling $f= 1-1/N$. To obtain further insight into the insulating phases suggested by the LSM argument, we derive low-energy effective theories by adding an antiferromagnetic Heisenberg exchange interaction among the local moments [the SU($N$) Kondo-Heisenberg model]. A conjectured global phase diagram of the SU($N$) Kondo lattice model as a function of the filling and the Kondo coupling is then obtained by a combination of different analytical approaches.","sentences":["The nature of the insulating phases of the SU($N$)-generalization of the one-dimensional Kondo lattice model is investigated by means of non-perturbative approaches.","By extending the Lieb-Schultz-Mattis (LSM) argument to multi-component fermion systems with translation and global SU($N$) symmetries, we derive two indices which depend on the filling and the ``SU($N$)-spin'' (representation) of the local moments.","These indices strongly constrain possible insulating phases; for instance, when the local moments transform in the $N$-dimensional (defining) representation of SU($N$), a featureless Kondo insulator is possible only at filling $f= 1-1/N$. To obtain further insight into the insulating phases suggested by the LSM argument, we derive low-energy effective theories by adding an antiferromagnetic Heisenberg exchange interaction among the local moments [the SU($N$) Kondo-Heisenberg model].","A conjectured global phase diagram of the SU($N$) Kondo lattice model as a function of the filling and the Kondo coupling is then obtained by a combination of different analytical approaches."],"url":"http://arxiv.org/abs/2404.11030v1","category":"cond-mat.str-el"}
{"created":"2024-04-17 17:59:38","title":"Sublinear transport in Kagome metals: Interplay of Dirac cones and Van Hove singularities","abstract":"Kagome metals are known to host Dirac fermions and saddle point Van Hove singularities near Fermi level. With the minimal two-pocket model (Dirac cone + Van Hove singularity), we propose a semiclassical theory to explain the experimentally observed sublinear resistivity in Ni$_3$In and other Kagome metals. We derive the full semiclassical description of kinetic phenomena using Boltzmann equation, and demonstrate that internode electron-electron interaction leads to sublinear in $T$ scaling for both electrical and thermal transport at low temperatures. At higher temperatures above the Dirac node chemical potential, thermal and electric current dissipate through distinct scattering channels, making a ground for Wiedemann-Franz law violation.","sentences":["Kagome metals are known to host Dirac fermions and saddle point Van Hove singularities near Fermi level.","With the minimal two-pocket model (Dirac cone + Van Hove singularity), we propose a semiclassical theory to explain the experimentally observed sublinear resistivity in Ni$_3$In and other Kagome metals.","We derive the full semiclassical description of kinetic phenomena using Boltzmann equation, and demonstrate that internode electron-electron interaction leads to sublinear in $T$ scaling for both electrical and thermal transport at low temperatures.","At higher temperatures above the Dirac node chemical potential, thermal and electric current dissipate through distinct scattering channels, making a ground for Wiedemann-Franz law violation."],"url":"http://arxiv.org/abs/2404.11612v1","category":"cond-mat.str-el"}
{"created":"2024-04-17 17:43:23","title":"Real Time Evolvable Hardware for Optimal Reconfiguration of Cusp-Like Pulse Shapers","abstract":"The design of a cusp-like digital pulse shaper for particle energy measurements requires the definition of four parameters whose values are defined based on the nature of the shaper input signal (timing, noise, ...) provided by a sensor. However, after high doses of radiation, sensors degenerate and their output signals do not meet the original characteristics, which may lead to erroneous measurements of the particle energies. We present in this paper an evolvable cusp-like digital shaper, which is able to auto-recalibrate the original hardware implementation into a new design that match the original specifications under the new sensor features.","sentences":["The design of a cusp-like digital pulse shaper for particle energy measurements requires the definition of four parameters whose values are defined based on the nature of the shaper input signal (timing, noise, ...) provided by a sensor.","However, after high doses of radiation, sensors degenerate and their output signals do not meet the original characteristics, which may lead to erroneous measurements of the particle energies.","We present in this paper an evolvable cusp-like digital shaper, which is able to auto-recalibrate the original hardware implementation into a new design that match the original specifications under the new sensor features."],"url":"http://arxiv.org/abs/2404.11592v1","category":"cs.AR"}
{"created":"2024-04-17 17:39:59","title":"A Subspace-Constrained Tyler's Estimator and its Applications to Structure from Motion","abstract":"We present the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers. STE is a fusion of the Tyler's M-estimator (TME) and a variant of the fast median subspace. Our theoretical analysis suggests that, under a common inlier-outlier model, STE can effectively recover the underlying subspace, even when it contains a smaller fraction of inliers relative to other methods in the field of robust subspace recovery. We apply STE in the context of Structure from Motion (SfM) in two ways: for robust estimation of the fundamental matrix and for the removal of outlying cameras, enhancing the robustness of the SfM pipeline. Numerical experiments confirm the state-of-the-art performance of our method in these applications. This research makes significant contributions to the field of robust subspace recovery, particularly in the context of computer vision and 3D reconstruction.","sentences":["We present the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers.","STE is a fusion of the Tyler's M-estimator (TME) and a variant of the fast median subspace.","Our theoretical analysis suggests that, under a common inlier-outlier model, STE can effectively recover the underlying subspace, even when it contains a smaller fraction of inliers relative to other methods in the field of robust subspace recovery.","We apply STE in the context of Structure from Motion (SfM) in two ways: for robust estimation of the fundamental matrix and for the removal of outlying cameras, enhancing the robustness of the SfM pipeline.","Numerical experiments confirm the state-of-the-art performance of our method in these applications.","This research makes significant contributions to the field of robust subspace recovery, particularly in the context of computer vision and 3D reconstruction."],"url":"http://arxiv.org/abs/2404.11590v1","category":"cs.CV"}
{"created":"2024-04-17 17:11:47","title":"Simple Image Signal Processing using Global Context Guidance","abstract":"In modern smartphone cameras, the Image Signal Processor (ISP) is the core element that converts the RAW readings from the sensor into perceptually pleasant RGB images for the end users. The ISP is typically proprietary and handcrafted and consists of several blocks such as white balance, color correction, and tone mapping. Deep learning-based ISPs aim to transform RAW images into DSLR-like RGB images using deep neural networks. However, most learned ISPs are trained using patches (small regions) due to computational limitations. Such methods lack global context, which limits their efficacy on full-resolution images and harms their ability to capture global properties such as color constancy or illumination. First, we propose a novel module that can be integrated into any neural ISP to capture the global context information from the full RAW images. Second, we propose an efficient and simple neural ISP that utilizes our proposed module. Our model achieves state-of-the-art results on different benchmarks using diverse and real smartphone images.","sentences":["In modern smartphone cameras, the Image Signal Processor (ISP) is the core element that converts the RAW readings from the sensor into perceptually pleasant RGB images for the end users.","The ISP is typically proprietary and handcrafted and consists of several blocks such as white balance, color correction, and tone mapping.","Deep learning-based ISPs aim to transform RAW images into DSLR-like RGB images using deep neural networks.","However, most learned ISPs are trained using patches (small regions) due to computational limitations.","Such methods lack global context, which limits their efficacy on full-resolution images and harms their ability to capture global properties such as color constancy or illumination.","First, we propose a novel module that can be integrated into any neural ISP to capture the global context information from the full RAW images.","Second, we propose an efficient and simple neural ISP that utilizes our proposed module.","Our model achieves state-of-the-art results on different benchmarks using diverse and real smartphone images."],"url":"http://arxiv.org/abs/2404.11569v1","category":"cs.CV"}
{"created":"2024-04-17 16:50:20","title":"A Bayesian level-set inversion method for simultaneous reconstruction of absorption and diffusion coefficients in diffuse optical tomography","abstract":"In this article, we propose a non-parametric Bayesian level-set method for simultaneous reconstruction of piecewise constant diffusion and absorption coefficients in diffuse optical tomography (DOT). We show that the Bayesian formulation of the corresponding inverse problem is well-posed and the posterior measure as a solution to the inverse problem satisfies a Lipschitz estimate with respect to the measured data in terms of Hellinger distance. We reduce the problem to a shape-reconstruction problem and use level-set priors for the parameters of interest. We demonstrate the efficacy of the proposed method using numerical simulations by performing reconstructions of the original phantom using two reconstruction methods. Posing the inverse problem in a Bayesian paradigm allows us to do statistical inference for the parameters of interest whereby we are able to quantify the uncertainty in the reconstructions for both the methods. This illustrates a key advantage of Bayesian methods over traditional algorithms.","sentences":["In this article, we propose a non-parametric Bayesian level-set method for simultaneous reconstruction of piecewise constant diffusion and absorption coefficients in diffuse optical tomography (DOT).","We show that the Bayesian formulation of the corresponding inverse problem is well-posed and the posterior measure as a solution to the inverse problem satisfies a Lipschitz estimate with respect to the measured data in terms of Hellinger distance.","We reduce the problem to a shape-reconstruction problem and use level-set priors for the parameters of interest.","We demonstrate the efficacy of the proposed method using numerical simulations by performing reconstructions of the original phantom using two reconstruction methods.","Posing the inverse problem in a Bayesian paradigm allows us to do statistical inference for the parameters of interest whereby we are able to quantify the uncertainty in the reconstructions for both the methods.","This illustrates a key advantage of Bayesian methods over traditional algorithms."],"url":"http://arxiv.org/abs/2404.11552v1","category":"stat.AP"}
{"created":"2024-04-17 16:25:19","title":"Select and Reorder: A Novel Approach for Neural Sign Language Production","abstract":"Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.","sentences":["Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets.","This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR).","Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment.","Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds.","Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation.","This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings."],"url":"http://arxiv.org/abs/2404.11532v1","category":"cs.CL"}
{"created":"2024-04-17 16:05:03","title":"Improved bounds and inference on optimal regimes","abstract":"Point identification of causal effects requires strong assumptions that are unreasonable in many practical settings. However, informative bounds on these effects can often be derived under plausible assumptions. Even when these bounds are wide or cover null effects, they can guide practical decisions based on formal decision theoretic criteria. Here we derive new results on optimal treatment regimes in settings where the effect of interest is bounded. These results are driven by consideration of superoptimal regimes; we define regimes that leverage an individual's natural treatment value, which is typically ignored in the existing literature. We obtain (sharp) bounds for the value function of superoptimal regimes, and provide performance guarantees relative to conventional optimal regimes. As a case study, we consider a commonly studied Marginal Sensitivity Model and illustrate that the superoptimal regime can be identified when conventional optimal regimes are not. We similarly illustrate this property in an instrumental variable setting. Finally, we derive efficient estimators for upper and lower bounds on the superoptimal value in instrumental variable settings, building on recent results on covariate adjusted Balke-Pearl bounds. These estimators are applied to study the effect of prompt ICU admission on survival.","sentences":["Point identification of causal effects requires strong assumptions that are unreasonable in many practical settings.","However, informative bounds on these effects can often be derived under plausible assumptions.","Even when these bounds are wide or cover null effects, they can guide practical decisions based on formal decision theoretic criteria.","Here we derive new results on optimal treatment regimes in settings where the effect of interest is bounded.","These results are driven by consideration of superoptimal regimes; we define regimes that leverage an individual's natural treatment value, which is typically ignored in the existing literature.","We obtain (sharp) bounds for the value function of superoptimal regimes, and provide performance guarantees relative to conventional optimal regimes.","As a case study, we consider a commonly studied Marginal Sensitivity Model and illustrate that the superoptimal regime can be identified when conventional optimal regimes are not.","We similarly illustrate this property in an instrumental variable setting.","Finally, we derive efficient estimators for upper and lower bounds on the superoptimal value in instrumental variable settings, building on recent results on covariate adjusted Balke-Pearl bounds.","These estimators are applied to study the effect of prompt ICU admission on survival."],"url":"http://arxiv.org/abs/2404.11510v1","category":"stat.ME"}
{"created":"2024-04-17 15:59:56","title":"Sky localization of Massive Black Hole Binaries in the foreground of Galactic white dwarf binaries","abstract":"Quickly sky localizing of massive black hole binaries (MBHBs) from the foreground of double white dwarf (DWD) is essential for space-based gravitational wave (GW) detection. In an orbit period of the space crafts, there is an optimal orbital position of the GW detectors to observe GW sources, where the signal intensity is at its peak. From the model Q3-d, five MBHB sources are selected based on the optimal observation orbital positions of the GW detectors, which are associated with the orientation of the MBHB perpendicular to the detection arms. For two MBHB sources of lower intensity, luminosity distance uncertainties, $\\Delta D_L$/$D_L$ at the 95$\\%$ confidence level from the overlapping GW signals of MBHB and DWD sources, when employing wavelet decomposition and reconstruction methods, are improved by $\\sim$ 2 times and $\\sim$ 10 times. Besides, the angular resolutions $\\Delta \\Omega_s$ are also improved by a factor of $\\sim$ 35 and $\\sim$ 8. These results imply that we can obtain relatively high accuracy of quickly localizing MBHB from the overlapped GW signals with DWDs at the best observation orbit position. The luminosity distance uncertainties at the 95$\\%$ confidence level for MBHB sources with the higher sign-noise ratio, have constraints on the precision of the Hubble constant.","sentences":["Quickly sky localizing of massive black hole binaries (MBHBs) from the foreground of double white dwarf (DWD) is essential for space-based gravitational wave (GW) detection.","In an orbit period of the space crafts, there is an optimal orbital position of the GW detectors to observe GW sources, where the signal intensity is at its peak.","From the model Q3-d, five MBHB sources are selected based on the optimal observation orbital positions of the GW detectors, which are associated with the orientation of the MBHB perpendicular to the detection arms.","For two MBHB sources of lower intensity, luminosity distance uncertainties, $\\Delta D_L$/$D_L$ at the 95$\\%$ confidence level from the overlapping GW signals of MBHB and DWD sources, when employing wavelet decomposition and reconstruction methods, are improved by $\\sim$ 2 times and $\\sim$ 10 times.","Besides, the angular resolutions $\\Delta \\Omega_s$ are also improved by a factor of $\\sim$ 35 and $\\sim$ 8.","These results imply that we can obtain relatively high accuracy of quickly localizing MBHB from the overlapped GW signals with DWDs at the best observation orbit position.","The luminosity distance uncertainties at the 95$\\%$ confidence level for MBHB sources with the higher sign-noise ratio, have constraints on the precision of the Hubble constant."],"url":"http://arxiv.org/abs/2404.11505v1","category":"astro-ph.HE"}
{"created":"2024-04-17 15:15:38","title":"Rates of convergence and normal approximations for estimators of local dependence random graph models","abstract":"Local dependence random graph models are a class of block models for network data which allow for dependence among edges under a local dependence assumption defined around the block structure of the network. Since being introduced by Schweinberger and Handcock (2015), research in the statistical network analysis and network science literatures have demonstrated the potential and utility of this class of models. In this work, we provide the first statistical disclaimers which provide conditions under which estimation and inference procedures can be expected to provide accurate and valid inferences. This is accomplished by deriving convergence rates of inference procedures for local dependence random graph models based on a single observation of the graph, allowing both the number of model parameters and the sizes of blocks to tend to infinity. First, we derive the first non-asymptotic bounds on the $\\ell_2$-error of maximum likelihood estimators, along with convergence rates. Second, and more importantly, we derive the first non-asymptotic bounds on the error of the multivariate normal approximation. In so doing, we introduce the first principled approach to providing statistical disclaimers through quantifying the uncertainty about statistical conclusions based on data.","sentences":["Local dependence random graph models are a class of block models for network data which allow for dependence among edges under a local dependence assumption defined around the block structure of the network.","Since being introduced by Schweinberger and Handcock (2015), research in the statistical network analysis and network science literatures have demonstrated the potential and utility of this class of models.","In this work, we provide the first statistical disclaimers which provide conditions under which estimation and inference procedures can be expected to provide accurate and valid inferences.","This is accomplished by deriving convergence rates of inference procedures for local dependence random graph models based on a single observation of the graph, allowing both the number of model parameters and the sizes of blocks to tend to infinity.","First, we derive the first non-asymptotic bounds on the $\\ell_2$-error of maximum likelihood estimators, along with convergence rates.","Second, and more importantly, we derive the first non-asymptotic bounds on the error of the multivariate normal approximation.","In so doing, we introduce the first principled approach to providing statistical disclaimers through quantifying the uncertainty about statistical conclusions based on data."],"url":"http://arxiv.org/abs/2404.11464v1","category":"math.ST"}
{"created":"2024-04-17 15:08:25","title":"Self-affinity of discs under glass-cut dissections","abstract":"A topological disc is called $n$-self-affine if it has a dissection into $n$ affine images of itself. It is called $n$-gc-self-affine if the dissection is obtained by successive glass-cuts, which are cuts along segments splitting one disc into two. For every $n \\ge 2$, we characterize all $n$-gc-self-affine discs. All such discs turn out to be either triangles or convex quadrangles. All triangles and trapezoids are $n$-gc-self-affine for every $n$. Non-trapezoidal quadrangles are not $n$-gc-self-affine for even $n$. They are $n$-gc-self-affine for every odd $n \\ge 7$, and they are $n$-gc-self-affine for $n=5$ if they aren't affine kites. Only four one-parameter families of quadrangles turn out to be $3$-gc-self-affine.   In addition, we show that every convex quadrangle is $n$-self-affine for all $n \\ge 5$.","sentences":["A topological disc is called $n$-self-affine if it has a dissection into $n$ affine images of itself.","It is called $n$-gc-self-affine if the dissection is obtained by successive glass-cuts, which are cuts along segments splitting one disc into two.","For every $n \\ge 2$, we characterize all $n$-gc-self-affine discs.","All such discs turn out to be either triangles or convex quadrangles.","All triangles and trapezoids are $n$-gc-self-affine for every $n$. Non-trapezoidal quadrangles are not $n$-gc-self-affine for even $n$. They are $n$-gc-self-affine for every odd $n \\ge 7$, and they are $n$-gc-self-affine for $n=5$ if they aren't affine kites.","Only four one-parameter families of quadrangles turn out to be $3$-gc-self-affine.   ","In addition, we show that every convex quadrangle is $n$-self-affine for all $n \\ge 5$."],"url":"http://arxiv.org/abs/2404.11460v1","category":"math.CO"}
{"created":"2024-04-17 14:50:24","title":"Two-loop Electroweak corrections to the Higgs boson rare decay process $H\\to Z\u03b3$","abstract":"Recently, ATLAS and CMS collaborations jointly announced the first evidence of the rare Higgs boson decay channel $H\\to Z\\gamma$, with a ratio of $2.2\\pm 0.7$ times overshooting the leading order standard model (SM) prediction. In order to face this challenge, it is urgent to proceed an even accurate calculation within the SM. To this end, we calculate in this paper the next-to-leading order (NLO) electroweak (EW) corrections to $H\\to Z\\gamma$ process, in which the NLO quantum chromodynamics (QCD) corrections were found tiny. Our calculation finds that the inclusion of NLO EW corrections may greatly enhance the prediction reliability. To tame the theoretical uncertainty, we adopt five different renormalization schemes. Combining our result with previous NLO QCD corrections and the signal-background interference, we conclude that the excess in $H\\to Z\\gamma$ cannot be explained within the SM. In fact, the incompatibility between the SM prediction and the LHC measurement of the concerned process is exacerbated upon considering the higher order EW corrections, which implies something beyond the SM could be involved.","sentences":["Recently, ATLAS and CMS collaborations jointly announced the first evidence of the rare Higgs boson decay channel $H\\to Z\\gamma$, with a ratio of $2.2\\pm 0.7$ times overshooting the leading order standard model (SM) prediction.","In order to face this challenge, it is urgent to proceed an even accurate calculation within the SM.","To this end, we calculate in this paper the next-to-leading order (NLO) electroweak (EW) corrections to $H\\to Z\\gamma$ process, in which the NLO quantum chromodynamics (QCD) corrections were found tiny.","Our calculation finds that the inclusion of NLO EW corrections may greatly enhance the prediction reliability.","To tame the theoretical uncertainty, we adopt five different renormalization schemes.","Combining our result with previous NLO QCD corrections and the signal-background interference, we conclude that the excess in $H\\to Z\\gamma$ cannot be explained within the SM.","In fact, the incompatibility between the SM prediction and the LHC measurement of the concerned process is exacerbated upon considering the higher order EW corrections, which implies something beyond the SM could be involved."],"url":"http://arxiv.org/abs/2404.11441v1","category":"hep-ph"}
{"created":"2024-04-17 14:48:19","title":"Consistency of empirical distributions of sequences of graph statistics in networks with dependent edges","abstract":"One of the first steps in applications of statistical network analysis is frequently to produce summary charts of important features of the network. Many of these features take the form of sequences of graph statistics counting the number of realized events in the network, examples of which include the degree distribution, as well as the edgewise shared partner distribution, and more. We provide conditions under which the empirical distributions of sequences of graph statistics are consistent in the $\\ell_{\\infty}$-norm in settings where edges in the network are dependent. We accomplish this by elaborating a weak dependence condition which ensures that we can obtain exponential inequalities which bound probabilities of deviations of graph statistics from the expected value. We apply this concentration inequality to empirical distributions of sequences of graph statistics and derive non-asymptotic bounds on the $\\ell_{\\infty}$-error which hold with high probability. Our non-asymptotic results are then extended to demonstrate uniform convergence almost surely in selected examples. We illustrate theoretical results through examples, simulation studies, and an application.","sentences":["One of the first steps in applications of statistical network analysis is frequently to produce summary charts of important features of the network.","Many of these features take the form of sequences of graph statistics counting the number of realized events in the network, examples of which include the degree distribution, as well as the edgewise shared partner distribution, and more.","We provide conditions under which the empirical distributions of sequences of graph statistics are consistent in the $\\ell_{\\infty}$-norm in settings where edges in the network are dependent.","We accomplish this by elaborating a weak dependence condition which ensures that we can obtain exponential inequalities which bound probabilities of deviations of graph statistics from the expected value.","We apply this concentration inequality to empirical distributions of sequences of graph statistics and derive non-asymptotic bounds on the $\\ell_{\\infty}$-error which hold with high probability.","Our non-asymptotic results are then extended to demonstrate uniform convergence almost surely in selected examples.","We illustrate theoretical results through examples, simulation studies, and an application."],"url":"http://arxiv.org/abs/2404.11438v1","category":"math.ST"}
{"created":"2024-04-17 14:09:39","title":"Pharmacokinetic Measurements in Dose Finding Model Guided by Escalation with Overdose Control","abstract":"Oncology drug development starts with a dose escalation phase to find the maximal tolerable dose (MTD). Dose limiting toxicity (DLT) is the primary endpoint for dose escalation phase. Traditionally, model-based dose escalation trial designs recommend a dose for escalation based on an assumed dose-DLT relationship. Pharmacokinetic (PK) data are often available but are currently only used by clinical teams in a subjective manner to aid decision making. Formal incorporation of PK data in dose-escalation models can make the decision process more efficient and lead to an increase in precision. In this talk we present a Bayesian joint modeling framework for incorporating PK data in Oncology dose escalation trials. This framework explores the dose-PK and PK-DLT relationships jointly for better model informed dose escalation decisions. Utility of the proposed model is demonstrated through a real-life case study along with simulation.","sentences":["Oncology drug development starts with a dose escalation phase to find the maximal tolerable dose (MTD).","Dose limiting toxicity (DLT) is the primary endpoint for dose escalation phase.","Traditionally, model-based dose escalation trial designs recommend a dose for escalation based on an assumed dose-DLT relationship.","Pharmacokinetic (PK) data are often available but are currently only used by clinical teams in a subjective manner to aid decision making.","Formal incorporation of PK data in dose-escalation models can make the decision process more efficient and lead to an increase in precision.","In this talk we present a Bayesian joint modeling framework for incorporating PK data in Oncology dose escalation trials.","This framework explores the dose-PK and PK-DLT relationships jointly for better model informed dose escalation decisions.","Utility of the proposed model is demonstrated through a real-life case study along with simulation."],"url":"http://arxiv.org/abs/2404.11406v1","category":"stat.AP"}
{"created":"2024-04-17 14:08:51","title":"Wave-front tracking for a quasi-linear scalar conservation law with hysteresis","abstract":"In this article we deal with the Cauchy problem for the quasi-linear scalar conservation law \\[u_t+ {\\cal F}(u)_t+u_x=0,\\] where ${\\cal F}$ is a specific hysteresis operator, namely the Play operator. Hysteresis models a rate-independent memory relationship between the input $u$ and its output. Its presence in the partial differential equation gives a particular non-local feature to the latter allowing us to capture phenomena that may arise in some application fields. Riemann problems and the interactions between shock lines are studied and via the so-called Wave-Front Tracking method a solution to the Cauchy problem with bounded variation initial data is constructed. The solution found satisfies an entropy-like condition, making it the unique solution in the class of entropy admissible ones.","sentences":["In this article we deal with the Cauchy problem for the quasi-linear scalar conservation law \\[u_t+ {\\cal F}(u)_t+u_x=0,\\] where ${\\cal F}$ is a specific hysteresis operator, namely the Play operator.","Hysteresis models a rate-independent memory relationship between the input $u$ and its output.","Its presence in the partial differential equation gives a particular non-local feature to the latter allowing us to capture phenomena that may arise in some application fields.","Riemann problems and the interactions between shock lines are studied and via the so-called Wave-Front Tracking method a solution to the Cauchy problem with bounded variation initial data is constructed.","The solution found satisfies an entropy-like condition, making it the unique solution in the class of entropy admissible ones."],"url":"http://arxiv.org/abs/2404.11405v1","category":"math.AP"}
{"created":"2024-04-17 13:52:10","title":"Quasinormal Modes of Bardeen Black Hole in 5-dimensional Gauss-Bonnet Gravity","abstract":"This study addressed the scalar field quasinormal ringing behavior of black holes. We investigated scalar field perturbations in Bardeen black hole spacetime in 5-dimensional Einstein-Gauss-Bonnet (EGB) gravity. Using the 3rd-order WKB approximation and the finite-difference method, we computed the frequency of quasinormal modes (QNMs) in the spacetime background. The calculations demonstrated that the real part of the QNMs $\\omega$ increased, whereas the imaginary part decreased with increase in the magnetic charge parameter $Q$ of the Bardeen black hole for a fixed Gauss-Bonnet parameter $\\alpha$. This was also valid when $Q$ was fixed and $\\alpha$ increased; where in the real part of the QNMs increased and the absolute value of the imaginary part decreased. However, the change in the latter case was more significant than that in the former; thus, the frequency of eigenvibration of this black hole background under the scalar field perturbation increased and the decay of eigenvibration decreased with increase in $\\alpha$ or $Q^2$. Moreover, this result shows that the effect of $\\alpha$ on the intrinsic vibration of this black hole was greater than that of $Q $.Finally we found an interesting phenomenon by comparing black holes in 4 and 5 dimensions. With higher dimensions, the real part of the QNMs changes more obviously, but the imaginary part of the QNMs is almost unchanged. This phenomenon also indicates that the frequency of gravitational waves released by higher dimensional black holes becomes larger, but the decay rate is almost constant.","sentences":["This study addressed the scalar field quasinormal ringing behavior of black holes.","We investigated scalar field perturbations in Bardeen black hole spacetime in 5-dimensional Einstein-Gauss-Bonnet (EGB) gravity.","Using the 3rd-order WKB approximation and the finite-difference method, we computed the frequency of quasinormal modes (QNMs) in the spacetime background.","The calculations demonstrated that the real part of the QNMs $\\omega$ increased, whereas the imaginary part decreased with increase in the magnetic charge parameter $Q$ of the Bardeen black hole for a fixed Gauss-Bonnet parameter $\\alpha$. This was also valid when $Q$ was fixed and $\\alpha$ increased; where in the real part of the QNMs increased and the absolute value of the imaginary part decreased.","However, the change in the latter case was more significant than that in the former; thus, the frequency of eigenvibration of this black hole background under the scalar field perturbation increased and the decay of eigenvibration decreased with increase in $\\alpha$ or $Q^2$.","Moreover, this result shows that the effect of $\\alpha$ on the intrinsic vibration of this black hole was greater than that of $Q $.Finally","we found an interesting phenomenon by comparing black holes in 4 and 5 dimensions.","With higher dimensions, the real part of the QNMs changes more obviously, but the imaginary part of the QNMs is almost unchanged.","This phenomenon also indicates that the frequency of gravitational waves released by higher dimensional black holes becomes larger, but the decay rate is almost constant."],"url":"http://arxiv.org/abs/2404.11392v1","category":"hep-th"}
{"created":"2024-04-17 13:39:51","title":"Convergence of Policy Gradient for Stochastic Linear-Quadratic Control Problem in Infinite Horizon","abstract":"With the outstanding performance of policy gradient (PG) method in the reinforcement learning field, the convergence theory of it has aroused more and more interest recently. Meanwhile, the significant importance and abundant theoretical researches make the stochastic linear quadratic (SLQ) control problem a starting point for studying PG in model-based learning setting. In this paper, we study the PG method for the SLQ problem in infinite horizon and take a step towards providing rigorous guarantees for gradient methods. Although the cost functional of linear-quadratic problem is typically nonconvex, we still overcome the difficulty based on gradient domination condition and L-smoothness property, and prove exponential/linear convergence of gradient flow/descent algorithms.","sentences":["With the outstanding performance of policy gradient (PG) method in the reinforcement learning field, the convergence theory of it has aroused more and more interest recently.","Meanwhile, the significant importance and abundant theoretical researches make the stochastic linear quadratic (SLQ) control problem a starting point for studying PG in model-based learning setting.","In this paper, we study the PG method for the SLQ problem in infinite horizon and take a step towards providing rigorous guarantees for gradient methods.","Although the cost functional of linear-quadratic problem is typically nonconvex, we still overcome the difficulty based on gradient domination condition and L-smoothness property, and prove exponential/linear convergence of gradient flow/descent algorithms."],"url":"http://arxiv.org/abs/2404.11382v1","category":"math.OC"}
{"created":"2024-04-17 13:28:32","title":"Pressure-driven dome-shaped superconductivity in bilayer nickelate La$_3$Ni$_2$O$_7$","abstract":"Here we report a comprehensive study of the crystal structure, resistivity, and ac magnetic susceptibility in single crystals of La$_3$Ni$_2$O$_7$, with a hydrostatic pressure up to 104 GPa. X-ray diffraction measurements reveal a bilayer orthorhombic structure (space group $Amam$) at ambient pressure and a transformation into a tetragonal phase (space group $I4/mmm$) at a critical pressure of $\\sim$14 GPa. The transport measurements reveal a dome-shaped superconducting region with a maximum $T_c$ of 83 K at 18.0 GPa. The superconductivity is gradually suppressed when applying a higher pressure, but it can persist up to 90 GPa. Importantly, from our high-pressure $ac$ magnetic susceptibility measurements, the estimated maximum superconducting volume fraction is 48$\\%$ at 19.4 GPa. Thus, we demonstrate the bulk nature of superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ under high pressure. The intimate connection among the superconductivity, the oxygen content, and the tetragonal structure are discussed.","sentences":["Here we report a comprehensive study of the crystal structure, resistivity, and ac magnetic susceptibility in single crystals of La$_3$Ni$_2$O$_7$, with a hydrostatic pressure up to 104 GPa.","X-ray diffraction measurements reveal a bilayer orthorhombic structure (space group $Amam$) at ambient pressure and a transformation into a tetragonal phase (space group $I4/mmm$) at a critical pressure of $\\sim$14 GPa.","The transport measurements reveal a dome-shaped superconducting region with a maximum $T_c$ of 83 K at 18.0 GPa.","The superconductivity is gradually suppressed when applying a higher pressure, but it can persist up to 90 GPa.","Importantly, from our high-pressure $ac$ magnetic susceptibility measurements, the estimated maximum superconducting volume fraction is 48$\\%$ at 19.4 GPa.","Thus, we demonstrate the bulk nature of superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ under high pressure.","The intimate connection among the superconductivity, the oxygen content, and the tetragonal structure are discussed."],"url":"http://arxiv.org/abs/2404.11369v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 13:23:50","title":"Phonon Directionality Determines the Polarization of the Band-Edge Exciton Emission in Two-Dimensional Metal Halide Perovskites","abstract":"Two-dimensional metal-halide perovskites are highly versatile for light-driven applications due to their exceptional variety in material composition, which can be exploited for tunability of mechanical and optoelectronic properties. The band edge emission is governed by the exciton fine structure that is defined by structure and composition of both organic and inorganic layers. Moreover, electronic and elastic properties are intricately connected in these materials. Electron-phonon coupling plays a crucial role in the recombination dynamics. However, the nature of the electron-phonon coupling, as well as which kind of phonons are involved, is still under debate. Here we investigate the emission and phonon response from single two-dimensional lead-iodide microcrystals with angle-resolved polarized spectroscopy. We find an intricate dependence of the emission polarization with the vibrational directionality in the materials, which clearly reveals that several bands of the low-frequency phonons of the inorganic lead-iodide perovskite lattice play the key role in the band edge emission. Our findings demonstrate how the emission spectrum and polarization of two-dimensional layered perovskites can be designed by their material composition, which is essential for optoelectronic applications, where fine control on the spectral and structural properties of the light is desired.","sentences":["Two-dimensional metal-halide perovskites are highly versatile for light-driven applications due to their exceptional variety in material composition, which can be exploited for tunability of mechanical and optoelectronic properties.","The band edge emission is governed by the exciton fine structure that is defined by structure and composition of both organic and inorganic layers.","Moreover, electronic and elastic properties are intricately connected in these materials.","Electron-phonon coupling plays a crucial role in the recombination dynamics.","However, the nature of the electron-phonon coupling, as well as which kind of phonons are involved, is still under debate.","Here we investigate the emission and phonon response from single two-dimensional lead-iodide microcrystals with angle-resolved polarized spectroscopy.","We find an intricate dependence of the emission polarization with the vibrational directionality in the materials, which clearly reveals that several bands of the low-frequency phonons of the inorganic lead-iodide perovskite lattice play the key role in the band edge emission.","Our findings demonstrate how the emission spectrum and polarization of two-dimensional layered perovskites can be designed by their material composition, which is essential for optoelectronic applications, where fine control on the spectral and structural properties of the light is desired."],"url":"http://arxiv.org/abs/2404.11367v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 13:22:26","title":"Stress analysis of functionally graded hyperelastic variable thickness rotating annular thin disk: A semi-analytic approach","abstract":"Functionally graded materials (FGMs) represent a promising class of advanced materials designed with tailored microstructures to achieve optimized mechanical, thermal, and functional properties across varying gradients. The strategic integration of distinct materials within functionally graded materials offers engineers unprecedented control over properties such as strength, thermal conductivity, and corrosion resistance, enabling innovative solutions for demanding applications in aerospace, automotive, and biomedical industries. This study investigates a rotating annular thin disk with variable thickness composed of incompressible hyperelastic material, made up of functionally graded properties under large deformations. To elucidate these phenomena, a power relation is employed to delineate the changes in cross-sectional geometry m, the material property n, and the angular velocity w of hyperelastic material. Constants used for hyperelastic material are obtained from the experimental data. Equations are solved semi-analytically for different values of m, n, and w, and the values of radial stresses, tangential stresses, and elongation are calculated and compared for different conditions. Results show that thickness and FG properties have a significant impact on the behavior of disk, so that the expected behavior of the disk can be obtained by an optimal selection of the disks geometry and material properties. By selecting the optimum values for these variables, the location of maximum stress can be controlled in large deformations, thereby furnishing significance advantages in structural design and material selection.","sentences":["Functionally graded materials (FGMs) represent a promising class of advanced materials designed with tailored microstructures to achieve optimized mechanical, thermal, and functional properties across varying gradients.","The strategic integration of distinct materials within functionally graded materials offers engineers unprecedented control over properties such as strength, thermal conductivity, and corrosion resistance, enabling innovative solutions for demanding applications in aerospace, automotive, and biomedical industries.","This study investigates a rotating annular thin disk with variable thickness composed of incompressible hyperelastic material, made up of functionally graded properties under large deformations.","To elucidate these phenomena, a power relation is employed to delineate the changes in cross-sectional geometry m, the material property n, and the angular velocity w of hyperelastic material.","Constants used for hyperelastic material are obtained from the experimental data.","Equations are solved semi-analytically for different values of m, n, and w, and the values of radial stresses, tangential stresses, and elongation are calculated and compared for different conditions.","Results show that thickness and FG properties have a significant impact on the behavior of disk, so that the expected behavior of the disk can be obtained by an optimal selection of the disks geometry and material properties.","By selecting the optimum values for these variables, the location of maximum stress can be controlled in large deformations, thereby furnishing significance advantages in structural design and material selection."],"url":"http://arxiv.org/abs/2404.11365v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 13:20:19","title":"A methodology of quantifying membrane permeability based on returning probability theory and molecular dynamics simulation","abstract":"We propose a theoretical approach to estimate the permeability coefficient of substrates (permeants) for crossing membranes from donor (D) phase to acceptor (A) phase by means of molecular dynamics (MD) simulation. A fundamental aspect of our approach involves reformulating the returning probability (RP) theory, a rigorous bimolecular reaction theory, to describe permeation phenomena. This reformulation relies on the parallelism between permeation and bimolecular reaction processes. In the present method, the permeability coefficient is represented in terms of the thermodynamic and kinetic quantities for the reactive (R) phase that exists within the inner region of membranes. One can evaluate these quantities using multiple MD trajectories starting from phase R. We apply the RP theory to the permeation of ethanol and methylamine at different concentrations (infinitely dilute and 1 mol% conditions of permeants). Under the 1 mol% condition, the present method yields a larger permeability coefficient for ethanol ($0.12 \\pm 0.01 ~\\mathrm{cm~s^{-1}}$) than for methylamine ($0.069\\pm 0.006~\\mathrm{cm~s^{-1}}$), while the values of the permeability coefficient are satisfactorily close to those obtained from the brute-force MD simulations [$0.18\\pm 0.03 ~\\mathrm{cm~s^{-1}}$ and $0.052 \\pm 0.005 ~\\mathrm{cm~s^{-1}}$ for ethanol and methylamine, respectively]. Moreover, upon analyzing the thermodynamic and kinetic contributions to the permeability, we clarify that a higher concentration dependency of permeability for ethanol, as compared to methylamine, arises from the sensitive nature of ethanol's free-energy barrier within the inner region of the membrane against ethanol concentration.","sentences":["We propose a theoretical approach to estimate the permeability coefficient of substrates (permeants) for crossing membranes from donor (D) phase to acceptor (A) phase by means of molecular dynamics (MD) simulation.","A fundamental aspect of our approach involves reformulating the returning probability (RP) theory, a rigorous bimolecular reaction theory, to describe permeation phenomena.","This reformulation relies on the parallelism between permeation and bimolecular reaction processes.","In the present method, the permeability coefficient is represented in terms of the thermodynamic and kinetic quantities for the reactive (R) phase that exists within the inner region of membranes.","One can evaluate these quantities using multiple MD trajectories starting from phase R. We apply the RP theory to the permeation of ethanol and methylamine at different concentrations (infinitely dilute and 1 mol% conditions of permeants).","Under the 1 mol% condition, the present method yields a larger permeability coefficient for ethanol ($0.12 \\pm 0.01 ~\\mathrm{cm~s^{-1}}$) than for methylamine ($0.069\\pm 0.006~\\mathrm{cm~s^{-1}}$), while the values of the permeability coefficient are satisfactorily close to those obtained from the brute-force MD simulations [$0.18\\pm 0.03 ~\\mathrm{cm~s^{-1}}$ and $0.052 \\pm 0.005 ~\\mathrm{cm~s^{-1}}$ for ethanol and methylamine, respectively].","Moreover, upon analyzing the thermodynamic and kinetic contributions to the permeability, we clarify that a higher concentration dependency of permeability for ethanol, as compared to methylamine, arises from the sensitive nature of ethanol's free-energy barrier within the inner region of the membrane against ethanol concentration."],"url":"http://arxiv.org/abs/2404.11363v1","category":"cond-mat.soft"}
{"created":"2024-04-17 13:04:05","title":"Jacobi Prior: An Alternate Bayesian Method for Supervised Learning","abstract":"This paper introduces the `Jacobi prior,' an alternative Bayesian method, that aims to address the computational challenges inherent in traditional techniques. It demonstrates that the Jacobi prior performs better than well-known methods like Lasso, Ridge, Elastic Net, and MCMC-based Horse-Shoe Prior, especially in predicting accurately. Additionally, We also show that the Jacobi prior is more than a hundred times faster than these methods while maintaining similar predictive accuracy. The method is implemented for Generalised Linear Models, Gaussian process regression, and classification, making it suitable for longitudinal/panel data analysis. The Jacobi prior shows it can handle partitioned data across servers worldwide, making it useful for distributed computing environments. As the method runs faster while still predicting accurately, it's good for organizations wanting to reduce their environmental impact and meet ESG standards. To show how well the Jacobi prior works, we did a detailed simulation study with four experiments, looking at statistical consistency, accuracy, and speed. Additionally, we present two empirical studies. First, we thoroughly evaluate Credit Risk by studying default probability using data from the U.S. Small Business Administration (SBA). Also, we use the Jacobi prior to classifying stars, quasars, and galaxies in a 3-class problem using multinational logit regression on Sloan Digital Sky Survey data. We use different filters as features. All codes and datasets for this paper are available in the following GitHub repository: https://github.com/sourish-cmi/Jacobi-Prior/","sentences":["This paper introduces the `Jacobi prior,' an alternative Bayesian method, that aims to address the computational challenges inherent in traditional techniques.","It demonstrates that the Jacobi prior performs better than well-known methods like Lasso, Ridge, Elastic Net, and MCMC-based Horse-Shoe Prior, especially in predicting accurately.","Additionally, We also show that the Jacobi prior is more than a hundred times faster than these methods while maintaining similar predictive accuracy.","The method is implemented for Generalised Linear Models, Gaussian process regression, and classification, making it suitable for longitudinal/panel data analysis.","The Jacobi prior shows it can handle partitioned data across servers worldwide, making it useful for distributed computing environments.","As the method runs faster while still predicting accurately, it's good for organizations wanting to reduce their environmental impact and meet ESG standards.","To show how well the Jacobi prior works, we did a detailed simulation study with four experiments, looking at statistical consistency, accuracy, and speed.","Additionally, we present two empirical studies.","First, we thoroughly evaluate Credit Risk by studying default probability using data from the U.S. Small Business Administration (SBA).","Also, we use the Jacobi prior to classifying stars, quasars, and galaxies in a 3-class problem using multinational logit regression on Sloan Digital Sky Survey data.","We use different filters as features.","All codes and datasets for this paper are available in the following GitHub repository: https://github.com/sourish-cmi/Jacobi-Prior/"],"url":"http://arxiv.org/abs/2404.11345v1","category":"stat.ME"}
{"created":"2024-04-17 12:45:59","title":"Toward Understanding the Disagreement Problem in Neural Network Feature Attribution","abstract":"In recent years, neural networks have demonstrated their remarkable ability to discern intricate patterns and relationships from raw data. However, understanding the inner workings of these black box models remains challenging, yet crucial for high-stake decisions. Among the prominent approaches for explaining these black boxes are feature attribution methods, which assign relevance or contribution scores to each input variable for a model prediction. Despite the plethora of proposed techniques, ranging from gradient-based to backpropagation-based methods, a significant debate persists about which method to use. Various evaluation metrics have been proposed to assess the trustworthiness or robustness of their results. However, current research highlights disagreement among state-of-the-art methods in their explanations. Our work addresses this confusion by investigating the explanations' fundamental and distributional behavior. Additionally, through a comprehensive simulation study, we illustrate the impact of common scaling and encoding techniques on the explanation quality, assess their efficacy across different effect sizes, and demonstrate the origin of inconsistency in rank-based evaluation metrics.","sentences":["In recent years, neural networks have demonstrated their remarkable ability to discern intricate patterns and relationships from raw data.","However, understanding the inner workings of these black box models remains challenging, yet crucial for high-stake decisions.","Among the prominent approaches for explaining these black boxes are feature attribution methods, which assign relevance or contribution scores to each input variable for a model prediction.","Despite the plethora of proposed techniques, ranging from gradient-based to backpropagation-based methods, a significant debate persists about which method to use.","Various evaluation metrics have been proposed to assess the trustworthiness or robustness of their results.","However, current research highlights disagreement among state-of-the-art methods in their explanations.","Our work addresses this confusion by investigating the explanations' fundamental and distributional behavior.","Additionally, through a comprehensive simulation study, we illustrate the impact of common scaling and encoding techniques on the explanation quality, assess their efficacy across different effect sizes, and demonstrate the origin of inconsistency in rank-based evaluation metrics."],"url":"http://arxiv.org/abs/2404.11330v1","category":"stat.ML"}
{"created":"2024-04-17 12:21:57","title":"Achieving Rotation Invariance in Convolution Operations: Shifting from Data-Driven to Mechanism-Assured","abstract":"Achieving rotation invariance in deep neural networks without relying on data has always been a hot research topic. Intrinsic rotation invariance can enhance the model's feature representation capability, enabling better performance in tasks such as multi-orientation object recognition and detection. Based on various types of non-learnable operators, including gradient, sort, local binary pattern, maximum, etc., this paper designs a set of new convolution operations that are natually invariant to arbitrary rotations. Unlike most previous studies, these rotation-invariant convolutions (RIConvs) have the same number of learnable parameters and a similar computational process as conventional convolution operations, allowing them to be interchangeable. Using the MNIST-Rot dataset, we first verify the invariance of these RIConvs under various rotation angles and compare their performance with previous rotation-invariant convolutional neural networks (RI-CNNs). Two types of RIConvs based on gradient operators achieve state-of-the-art results. Subsequently, we combine RIConvs with different types and depths of classic CNN backbones. Using the OuTex_00012, MTARSI, and NWPU-RESISC-45 datasets, we test their performance on texture recognition, aircraft type recognition, and remote sensing image classification tasks. The results show that RIConvs significantly improve the accuracy of these CNN backbones, especially when the training data is limited. Furthermore, we find that even with data augmentation, RIConvs can further enhance model performance.","sentences":["Achieving rotation invariance in deep neural networks without relying on data has always been a hot research topic.","Intrinsic rotation invariance can enhance the model's feature representation capability, enabling better performance in tasks such as multi-orientation object recognition and detection.","Based on various types of non-learnable operators, including gradient, sort, local binary pattern, maximum, etc., this paper designs a set of new convolution operations that are natually invariant to arbitrary rotations.","Unlike most previous studies, these rotation-invariant convolutions (RIConvs) have the same number of learnable parameters and a similar computational process as conventional convolution operations, allowing them to be interchangeable.","Using the MNIST-Rot dataset, we first verify the invariance of these RIConvs under various rotation angles and compare their performance with previous rotation-invariant convolutional neural networks (RI-CNNs).","Two types of RIConvs based on gradient operators achieve state-of-the-art results.","Subsequently, we combine RIConvs with different types and depths of classic CNN backbones.","Using the OuTex_00012, MTARSI, and NWPU-RESISC-45 datasets, we test their performance on texture recognition, aircraft type recognition, and remote sensing image classification tasks.","The results show that RIConvs significantly improve the accuracy of these CNN backbones, especially when the training data is limited.","Furthermore, we find that even with data augmentation, RIConvs can further enhance model performance."],"url":"http://arxiv.org/abs/2404.11309v1","category":"cs.CV"}
{"created":"2024-04-17 12:12:48","title":"Learning from Unlabelled Data with Transformers: Domain Adaptation for Semantic Segmentation of High Resolution Aerial Images","abstract":"Data from satellites or aerial vehicles are most of the times unlabelled. Annotating such data accurately is difficult, requires expertise, and is costly in terms of time. Even if Earth Observation (EO) data were correctly labelled, labels might change over time. Learning from unlabelled data within a semi-supervised learning framework for segmentation of aerial images is challenging. In this paper, we develop a new model for semantic segmentation of unlabelled images, the Non-annotated Earth Observation Semantic Segmentation (NEOS) model. NEOS performs domain adaptation as the target domain does not have ground truth semantic segmentation masks. The distribution inconsistencies between the target and source domains are due to differences in acquisition scenes, environment conditions, sensors, and times. Our model aligns the learned representations of the different domains to make them coincide. The evaluation results show that NEOS is successful and outperforms other models for semantic segmentation of unlabelled data.","sentences":["Data from satellites or aerial vehicles are most of the times unlabelled.","Annotating such data accurately is difficult, requires expertise, and is costly in terms of time.","Even if Earth Observation (EO) data were correctly labelled, labels might change over time.","Learning from unlabelled data within a semi-supervised learning framework for segmentation of aerial images is challenging.","In this paper, we develop a new model for semantic segmentation of unlabelled images, the Non-annotated Earth Observation Semantic Segmentation (NEOS) model.","NEOS performs domain adaptation as the target domain does not have ground truth semantic segmentation masks.","The distribution inconsistencies between the target and source domains are due to differences in acquisition scenes, environment conditions, sensors, and times.","Our model aligns the learned representations of the different domains to make them coincide.","The evaluation results show that NEOS is successful and outperforms other models for semantic segmentation of unlabelled data."],"url":"http://arxiv.org/abs/2404.11299v1","category":"cs.CV"}
{"created":"2024-04-17 11:52:47","title":"A Preference-driven Paradigm for Enhanced Translation with Large Language Models","abstract":"Recent research has shown that large language models (LLMs) can achieve remarkable translation performance through supervised fine-tuning (SFT) using only a small amount of parallel data. However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references. Hence, the assistance from SFT often reaches a plateau once the LLMs have achieved a certain level of translation capability, and further increasing the size of parallel data does not provide additional benefits. To overcome this plateau associated with imitation-based SFT, we propose a preference-based approach built upon the Plackett-Luce model. The objective is to steer LLMs towards a more nuanced understanding of translation preferences from a holistic view, while also being more resilient in the absence of gold translations. We further build a dataset named MAPLE to verify the effectiveness of our approach, which includes multiple translations of varying quality for each source sentence. Extensive experiments demonstrate the superiority of our approach in \"breaking the plateau\" across diverse LLMs and test settings. Our in-depth analysis underscores the pivotal role of diverse translations and accurate preference scores in the success of our approach.","sentences":["Recent research has shown that large language models (LLMs) can achieve remarkable translation performance through supervised fine-tuning (SFT) using only a small amount of parallel data.","However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references.","Hence, the assistance from SFT often reaches a plateau once the LLMs have achieved a certain level of translation capability, and further increasing the size of parallel data does not provide additional benefits.","To overcome this plateau associated with imitation-based SFT, we propose a preference-based approach built upon the Plackett-Luce model.","The objective is to steer LLMs towards a more nuanced understanding of translation preferences from a holistic view, while also being more resilient in the absence of gold translations.","We further build a dataset named MAPLE to verify the effectiveness of our approach, which includes multiple translations of varying quality for each source sentence.","Extensive experiments demonstrate the superiority of our approach in \"breaking the plateau\" across diverse LLMs and test settings.","Our in-depth analysis underscores the pivotal role of diverse translations and accurate preference scores in the success of our approach."],"url":"http://arxiv.org/abs/2404.11288v1","category":"cs.CL"}
{"created":"2024-04-17 11:20:20","title":"Light propagation in time-periodic bi-isotropic media","abstract":"Photonic structures and time-crystals, wherein time is incorporated as an additional degree of freedom for light manipulation, have necessitated the development of analytical and semi-analytical tools. However, such tools are currently limited to specific configurations, leaving several unexplored physical phenomena akin to photonic time-crystals elusive. In this communication, using a coupled-wave theory approach, we unveil the occurring light propagation phenomena in a time-periodic bi-isotropic medium whose permittivity, permeability, and chirality parameter are periodic functions of time. Contrary to their static counterparts, we demonstrate that the considered dynamic medium couples only co-handed counter-propagating waves. In cases of non-constant impedance, we prove that two first-order momentum gaps are formed in the Brillouin diagram, resulting in parametric amplification with different amplification factors and corresponding momenta for the right- and left-handed modes, respectively. The presence of chirality plays a major role in manipulating lightwave signals by controlling the center of resonance, the corresponding bandwidth, and the amplification factor in a distinct fashion for each mode. For a finite ``time-slab'' of the medium, we analytically derive the scattering coefficients as functions of time and momentum, discussing how extreme values of optical rotation grant access to the temporal analog of the chirality-induced negative refraction regime. Finally, we demonstrate the mechanism under which elliptical polarizations may change field orientation whilst the electric field propagates in a momentum gap, thus simultaneously showcasing parametric amplification.","sentences":["Photonic structures and time-crystals, wherein time is incorporated as an additional degree of freedom for light manipulation, have necessitated the development of analytical and semi-analytical tools.","However, such tools are currently limited to specific configurations, leaving several unexplored physical phenomena akin to photonic time-crystals elusive.","In this communication, using a coupled-wave theory approach, we unveil the occurring light propagation phenomena in a time-periodic bi-isotropic medium whose permittivity, permeability, and chirality parameter are periodic functions of time.","Contrary to their static counterparts, we demonstrate that the considered dynamic medium couples only co-handed counter-propagating waves.","In cases of non-constant impedance, we prove that two first-order momentum gaps are formed in the Brillouin diagram, resulting in parametric amplification with different amplification factors and corresponding momenta for the right- and left-handed modes, respectively.","The presence of chirality plays a major role in manipulating lightwave signals by controlling the center of resonance, the corresponding bandwidth, and the amplification factor in a distinct fashion for each mode.","For a finite ``time-slab'' of the medium, we analytically derive the scattering coefficients as functions of time and momentum, discussing how extreme values of optical rotation grant access to the temporal analog of the chirality-induced negative refraction regime.","Finally, we demonstrate the mechanism under which elliptical polarizations may change field orientation whilst the electric field propagates in a momentum gap, thus simultaneously showcasing parametric amplification."],"url":"http://arxiv.org/abs/2404.11270v1","category":"physics.optics"}
{"created":"2024-04-17 11:19:04","title":"Towards Human Awareness in Robot Task Planning with Large Language Models","abstract":"The recent breakthroughs in the research on Large Language Models (LLMs) have triggered a transformation across several research domains. Notably, the integration of LLMs has greatly enhanced performance in robot Task And Motion Planning (TAMP). However, previous approaches often neglect the consideration of dynamic environments, i.e., the presence of dynamic objects such as humans. In this paper, we propose a novel approach to address this gap by incorporating human awareness into LLM-based robot task planning. To obtain an effective representation of the dynamic environment, our approach integrates humans' information into a hierarchical scene graph. To ensure the plan's executability, we leverage LLMs to ground the environmental topology and actionable knowledge into formal planning language. Most importantly, we use LLMs to predict future human activities and plan tasks for the robot considering the predictions. Our contribution facilitates the development of integrating human awareness into LLM-driven robot task planning, and paves the way for proactive robot decision-making in dynamic environments.","sentences":["The recent breakthroughs in the research on Large Language Models (LLMs) have triggered a transformation across several research domains.","Notably, the integration of LLMs has greatly enhanced performance in robot Task And Motion Planning (TAMP).","However, previous approaches often neglect the consideration of dynamic environments, i.e., the presence of dynamic objects such as humans.","In this paper, we propose a novel approach to address this gap by incorporating human awareness into LLM-based robot task planning.","To obtain an effective representation of the dynamic environment, our approach integrates humans' information into a hierarchical scene graph.","To ensure the plan's executability, we leverage LLMs to ground the environmental topology and actionable knowledge into formal planning language.","Most importantly, we use LLMs to predict future human activities and plan tasks for the robot considering the predictions.","Our contribution facilitates the development of integrating human awareness into LLM-driven robot task planning, and paves the way for proactive robot decision-making in dynamic environments."],"url":"http://arxiv.org/abs/2404.11267v1","category":"cs.RO"}
{"created":"2024-04-17 11:17:12","title":"Criteria for Uncertainty-based Corner Cases Detection in Instance Segmentation","abstract":"The operating environment of a highly automated vehicle is subject to change, e.g., weather, illumination, or the scenario containing different objects and other participants in which the highly automated vehicle has to navigate its passengers safely. These situations must be considered when developing and validating highly automated driving functions. This already poses a problem for training and evaluating deep learning models because without the costly labeling of thousands of recordings, not knowing whether the data contains relevant, interesting data for further model training, it is a guess under which conditions and situations the model performs poorly. For this purpose, we present corner case criteria based on the predictive uncertainty. With our corner case criteria, we are able to detect uncertainty-based corner cases of an object instance segmentation model without relying on ground truth (GT) data. We evaluated each corner case criterion using the COCO and the NuImages dataset to analyze the potential of our approach. We also provide a corner case decision function that allows us to distinguish each object into True Positive (TP), localization and/or classification corner case, or False Positive (FP). We also present our first results of an iterative training cycle that outperforms the baseline and where the data added to the training dataset is selected based on the corner case decision function.","sentences":["The operating environment of a highly automated vehicle is subject to change, e.g., weather, illumination, or the scenario containing different objects and other participants in which the highly automated vehicle has to navigate its passengers safely.","These situations must be considered when developing and validating highly automated driving functions.","This already poses a problem for training and evaluating deep learning models because without the costly labeling of thousands of recordings, not knowing whether the data contains relevant, interesting data for further model training, it is a guess under which conditions and situations the model performs poorly.","For this purpose, we present corner case criteria based on the predictive uncertainty.","With our corner case criteria, we are able to detect uncertainty-based corner cases of an object instance segmentation model without relying on ground truth (GT) data.","We evaluated each corner case criterion using the COCO and the NuImages dataset to analyze the potential of our approach.","We also provide a corner case decision function that allows us to distinguish each object into True Positive (TP), localization and/or classification corner case, or False Positive (FP).","We also present our first results of an iterative training cycle that outperforms the baseline and where the data added to the training dataset is selected based on the corner case decision function."],"url":"http://arxiv.org/abs/2404.11266v1","category":"cs.CV"}
{"created":"2024-04-17 11:13:59","title":"On an Internal Dependence of Simultaneous Measurements","abstract":"In this paper we show that there exists an internal dependence of the simultaneous measurements made by the two pairs of linear polarizers operated in each leg of the apparatus in Aspect's version of Einstein-Podolsky-Rosen-Bohm Gedankenexperiment. The corresponding Shannon-Kolmogorov's information flow linking a polarizer from one leg to a polarizer from the other leg is proportional to the absolute value of this function of dependence. It turns out that if Bell's inequality is violated, then this information flow is strictly positive, that is, the experiment performed at one leg is informationally dependent on the experiment at the other leg. By throwing out the sign of absolute value, we define the signed information flow linking a polarizer from one leg to a polarizer from the other leg which, in turn, reproduces the probabilities of the four outcomes of the simultaneous measurements, predicted by quantum mechanics. We make an attempt to illustrate the seeming random relation between the total information flow, the total signed information flow, and the violation of Bell's inequality in terms of a kind of uncertainty principle.","sentences":["In this paper we show that there exists an internal dependence of the simultaneous measurements made by the two pairs of linear polarizers operated in each leg of the apparatus in Aspect's version of Einstein-Podolsky-Rosen-Bohm Gedankenexperiment.","The corresponding Shannon-Kolmogorov's information flow linking a polarizer from one leg to a polarizer from the other leg is proportional to the absolute value of this function of dependence.","It turns out that if Bell's inequality is violated, then this information flow is strictly positive, that is, the experiment performed at one leg is informationally dependent on the experiment at the other leg.","By throwing out the sign of absolute value, we define the signed information flow linking a polarizer from one leg to a polarizer from the other leg which, in turn, reproduces the probabilities of the four outcomes of the simultaneous measurements, predicted by quantum mechanics.","We make an attempt to illustrate the seeming random relation between the total information flow, the total signed information flow, and the violation of Bell's inequality in terms of a kind of uncertainty principle."],"url":"http://arxiv.org/abs/2404.11263v1","category":"math-ph"}
{"created":"2024-04-17 11:11:23","title":"WISDOM Project -- XXIV. Cross-checking supermassive black hole mass estimates from ALMA CO gas kinematics and SINFONI stellar kinematics in the galaxy NGC 4751","abstract":"Supermassive black hole (SMBH) masses can be measured by observing the impacts of the SMBHs on dynamical tracers around them. We present high angular resolution ($0.19$ arcsec or $\\approx24$ pc) Atacama Large Millimeter/submillimeter Array observations of the $^{12}$CO(3-2) line emission of the early-type galaxy NGC 4751, which reveal a highly-inclined regularly-rotating molecular gas disc with clear central Keplerian motions. Using a Hubble Space Telescope image to constrain the stellar mass distribution, we forward model the molecular gas kinematics and data cube in a Bayesian framework using the Kinematic Molecular Simulation code. Assuming a constant mass-to-light ratio ($M/L$), we infer a SMBH mass $M_\\text{BH}=3.43^{+0.45}_{-0.44}\\times10^9$ $\\text{M}_\\odot$ and a F160W filter stellar $M/L$ $M/L_\\text{F160W}=(2.68\\pm0.11)$ $\\text{M}_\\odot/\\text{L}_{\\odot,\\text{F160W}}$ (all quoted uncertainties are at $3\\sigma$ confidence). Assuming a linearly spatially-varying $M/L$, we infer $M_\\text{BH}=2.79_{-0.57}^{+0.75}\\times10^9$ $\\text{M}_\\odot$ and $\\left(M/L_\\text{F160W}\\right)/\\left(\\text{M}_\\odot/\\text{L}_{\\odot,\\text{F160W}}\\right)=3.07^{+0.27}_{-0.35}-0.09^{+0.08}_{-0.06}\\,\\left(R/\\text{arcsec}\\right)$, where $R$ is the galactocentric radius. We also present alternative SMBH mass estimates using the Jeans Anisotropic Modelling (JAM) method and SINFONI stellar kinematics. Assuming a cylindrically-aligned velocity ellipsoid (JAM$_\\text{cyl}$) we infer $M_\\text{BH}=(2.52\\pm 0.36)\\times10^9$ $\\text{M}_\\odot$, while assuming a spherically-aligned velocity ellipsoid (JAM$_\\text{sph}$) we infer $M_\\text{BH}=(3.24\\pm0.87)\\times10^9$ $\\text{M}_\\odot$. Our derived masses are all consistent with one another, but they are larger than (and inconsistent with) one previous stellar dynamical measurement using Schwarzschil's method and the same SINFONI kinematics.","sentences":["Supermassive black hole (SMBH) masses can be measured by observing the impacts of the SMBHs on dynamical tracers around them.","We present high angular resolution ($0.19$ arcsec or $\\approx24$ pc) Atacama Large Millimeter/submillimeter Array observations of the $^{12}$CO(3-2) line emission of the early-type galaxy NGC 4751, which reveal a highly-inclined regularly-rotating molecular gas disc with clear central Keplerian motions.","Using a Hubble Space Telescope image to constrain the stellar mass distribution, we forward model the molecular gas kinematics and data cube in a Bayesian framework using the Kinematic Molecular Simulation code.","Assuming a constant mass-to-light ratio ($M/L$), we infer a SMBH mass $M_\\text{BH}=3.43^{+0.45}_{-0.44}\\times10^9$ $\\text{M}_\\odot$ and a F160W filter stellar $M/L$ $M/L_\\text{F160W}=(2.68\\pm0.11)$ $\\text{M}_\\odot/\\text{L}_{\\odot,\\text{F160W}}$ (all quoted uncertainties are at $3\\sigma$ confidence).","Assuming a linearly spatially-varying $M/L$, we infer $M_\\text{BH}=2.79_{-0.57}^{+0.75}\\times10^9$ $\\text{M}_\\odot$ and $\\left(M/L_\\text{F160W}\\right)/\\left(\\text{M}_\\odot/\\text{L}_{\\odot,\\text{F160W}}\\right)=3.07^{+0.27}_{-0.35}-0.09^{+0.08}_{-0.06}\\,\\left(R/\\text{arcsec}\\right)$, where $R$ is the galactocentric radius.","We also present alternative SMBH mass estimates using the Jeans Anisotropic Modelling (JAM) method and SINFONI stellar kinematics.","Assuming a cylindrically-aligned velocity ellipsoid (JAM$_\\text{cyl}$) we infer $M_\\text{BH}=(2.52\\pm 0.36)\\times10^9$ $\\text{M}_\\odot$, while assuming a spherically-aligned velocity ellipsoid (JAM$_\\text{sph}$) we infer $M_\\text{BH}=(3.24\\pm0.87)\\times10^9$ $\\text{M}_\\odot$. Our derived masses are all consistent with one another, but they are larger than (and inconsistent with) one previous stellar dynamical measurement using Schwarzschil's method and the same SINFONI kinematics."],"url":"http://arxiv.org/abs/2404.11260v1","category":"astro-ph.GA"}
{"created":"2024-04-17 11:06:42","title":"MMCBE: Multi-modality Dataset for Crop Biomass Estimation and Beyond","abstract":"Crop biomass, a critical indicator of plant growth, health, and productivity, is invaluable for crop breeding programs and agronomic research. However, the accurate and scalable quantification of crop biomass remains inaccessible due to limitations in existing measurement methods. One of the obstacles impeding the advancement of current crop biomass prediction methodologies is the scarcity of publicly available datasets. Addressing this gap, we introduce a new dataset in this domain, i.e. Multi-modality dataset for crop biomass estimation (MMCBE). Comprising 216 sets of multi-view drone images, coupled with LiDAR point clouds, and hand-labelled ground truth, MMCBE represents the first multi-modality one in the field. This dataset aims to establish benchmark methods for crop biomass quantification and foster the development of vision-based approaches. We have rigorously evaluated state-of-the-art crop biomass estimation methods using MMCBE and ventured into additional potential applications, such as 3D crop reconstruction from drone imagery and novel-view rendering. With this publication, we are making our comprehensive dataset available to the broader community.","sentences":["Crop biomass, a critical indicator of plant growth, health, and productivity, is invaluable for crop breeding programs and agronomic research.","However, the accurate and scalable quantification of crop biomass remains inaccessible due to limitations in existing measurement methods.","One of the obstacles impeding the advancement of current crop biomass prediction methodologies is the scarcity of publicly available datasets.","Addressing this gap, we introduce a new dataset in this domain, i.e. Multi-modality dataset for crop biomass estimation (MMCBE).","Comprising 216 sets of multi-view drone images, coupled with LiDAR point clouds, and hand-labelled ground truth, MMCBE represents the first multi-modality one in the field.","This dataset aims to establish benchmark methods for crop biomass quantification and foster the development of vision-based approaches.","We have rigorously evaluated state-of-the-art crop biomass estimation methods using MMCBE and ventured into additional potential applications, such as 3D crop reconstruction from drone imagery and novel-view rendering.","With this publication, we are making our comprehensive dataset available to the broader community."],"url":"http://arxiv.org/abs/2404.11256v1","category":"cs.CV"}
{"created":"2024-04-17 10:16:20","title":"Analytical results for uncertainty propagation through trained machine learning regression models","abstract":"Machine learning (ML) models are increasingly being used in metrology applications. However, for ML models to be credible in a metrology context they should be accompanied by principled uncertainty quantification. This paper addresses the challenge of uncertainty propagation through trained/fixed machine learning (ML) regression models. Analytical expressions for the mean and variance of the model output are obtained/presented for certain input data distributions and for a variety of ML models. Our results cover several popular ML models including linear regression, penalised linear regression, kernel ridge regression, Gaussian Processes (GPs), support vector machines (SVMs) and relevance vector machines (RVMs). We present numerical experiments in which we validate our methods and compare them with a Monte Carlo approach from a computational efficiency point of view. We also illustrate our methods in the context of a metrology application, namely modelling the state-of-health of lithium-ion cells based upon Electrical Impedance Spectroscopy (EIS) data","sentences":["Machine learning (ML) models are increasingly being used in metrology applications.","However, for ML models to be credible in a metrology context they should be accompanied by principled uncertainty quantification.","This paper addresses the challenge of uncertainty propagation through trained/fixed machine learning (ML) regression models.","Analytical expressions for the mean and variance of the model output are obtained/presented for certain input data distributions and for a variety of ML models.","Our results cover several popular ML models including linear regression, penalised linear regression, kernel ridge regression, Gaussian Processes (GPs), support vector machines (SVMs) and relevance vector machines (RVMs).","We present numerical experiments in which we validate our methods and compare them with a Monte Carlo approach from a computational efficiency point of view.","We also illustrate our methods in the context of a metrology application, namely modelling the state-of-health of lithium-ion cells based upon Electrical Impedance Spectroscopy (EIS) data"],"url":"http://arxiv.org/abs/2404.11224v1","category":"cs.LG"}
{"created":"2024-04-17 10:10:09","title":"VLST: Virtual Lung Screening Trial for Lung Cancer Detection Using Virtual Imaging Trial","abstract":"Importance: The efficacy of lung cancer screening can be significantly impacted by the imaging modality used. This Virtual Lung Screening Trial (VLST) addresses the critical need for precision in lung cancer diagnostics and the potential for reducing unnecessary radiation exposure in clinical settings.   Objectives: To establish a virtual imaging trial (VIT) platform that accurately simulates real-world lung screening trials (LSTs) to assess the diagnostic accuracy of CT and CXR modalities.   Design, Setting, and Participants: Utilizing computational models and machine learning algorithms, we created a diverse virtual patient population. The cohort, designed to mirror real-world demographics, was assessed using virtual imaging techniques that reflect historical imaging technologies.   Main Outcomes and Measures: The primary outcome was the difference in the Area Under the Curve (AUC) for CT and CXR modalities across lesion types and sizes.   Results: The study analyzed 298 CT and 313 CXR simulated images from 313 virtual patients, with a lesion-level AUC of 0.81 (95% CI: 0.78-0.84) for CT and 0.55 (95% CI: 0.53-0.56) for CXR. At the patient level, CT demonstrated an AUC of 0.85 (95% CI: 0.80-0.89), compared to 0.53 (95% CI: 0.47-0.60) for CXR. Subgroup analyses indicated CT's superior performance in detecting homogeneous lesions (AUC of 0.97 for lesion-level) and heterogeneous lesions (AUC of 0.71 for lesion-level) as well as in identifying larger nodules (AUC of 0.98 for nodules > 8 mm).   Conclusion and Relevance: The VIT platform validated the superior diagnostic accuracy of CT over CXR, especially for smaller nodules, underscoring its potential to replicate real clinical imaging trials. These findings advocate for the integration of virtual trials in the evaluation and improvement of imaging-based diagnostic tools.","sentences":["Importance: The efficacy of lung cancer screening can be significantly impacted by the imaging modality used.","This Virtual Lung Screening Trial (VLST) addresses the critical need for precision in lung cancer diagnostics and the potential for reducing unnecessary radiation exposure in clinical settings.   ","Objectives: To establish a virtual imaging trial (VIT) platform that accurately simulates real-world lung screening trials (LSTs) to assess the diagnostic accuracy of CT and CXR modalities.   ","Design, Setting, and Participants:","Utilizing computational models and machine learning algorithms, we created a diverse virtual patient population.","The cohort, designed to mirror real-world demographics, was assessed using virtual imaging techniques that reflect historical imaging technologies.   ","Main Outcomes and Measures: The primary outcome was the difference in the Area Under the Curve (AUC) for CT and CXR modalities across lesion types and sizes.   ","Results:","The study analyzed 298 CT and 313 CXR simulated images from 313 virtual patients, with a lesion-level AUC of 0.81 (95% CI: 0.78-0.84) for CT and 0.55 (95% CI: 0.53-0.56) for CXR.","At the patient level, CT demonstrated an AUC of 0.85 (95% CI: 0.80-0.89), compared to 0.53 (95% CI: 0.47-0.60) for CXR.","Subgroup analyses indicated CT's superior performance in detecting homogeneous lesions (AUC of 0.97 for lesion-level) and heterogeneous lesions (AUC of 0.71 for lesion-level) as well as in identifying larger nodules (AUC of 0.98 for nodules > 8 mm).   ","Conclusion and Relevance: The VIT platform validated the superior diagnostic accuracy of CT over CXR, especially for smaller nodules, underscoring its potential to replicate real clinical imaging trials.","These findings advocate for the integration of virtual trials in the evaluation and improvement of imaging-based diagnostic tools."],"url":"http://arxiv.org/abs/2404.11221v1","category":"eess.IV"}
{"created":"2024-04-17 09:36:29","title":"Two Positive Normalized Solutions on Star-shaped Bounded Domains to the Br\u00e9zis-Nirenberg Problem, I: Existence","abstract":"We develop a new framework to prove the existence of two positive solutions with prescribed mass on star-shaped bounded domains: one is the normalized ground state and another is of M-P type. We merely address the Sobolev critical cases since the Sobolev subcritical ones can be addressed by following similar arguments and are easier. Our framework is based on some important observations, that, to the best of our knowledge, have not appeared in previous literatures. Using these observations, we firstly establish the existence of a normalized ground state solution, whose existence is unknown so for. Then we use some novel ideas to obtain the second positive normalized solution, which is of M-P type. It seems to be the first time in the literatures to get two positive solutions under our settings, even in the Sobolev subcritical cases. We further remark that our framework is applicable to many other equations.","sentences":["We develop a new framework to prove the existence of two positive solutions with prescribed mass on star-shaped bounded domains: one is the normalized ground state and another is of M-P type.","We merely address the Sobolev critical cases since the Sobolev subcritical ones can be addressed by following similar arguments and are easier.","Our framework is based on some important observations, that, to the best of our knowledge, have not appeared in previous literatures.","Using these observations, we firstly establish the existence of a normalized ground state solution, whose existence is unknown so for.","Then we use some novel ideas to obtain the second positive normalized solution, which is of M-P type.","It seems to be the first time in the literatures to get two positive solutions under our settings, even in the Sobolev subcritical cases.","We further remark that our framework is applicable to many other equations."],"url":"http://arxiv.org/abs/2404.11204v1","category":"math.AP"}
{"created":"2024-04-17 09:33:19","title":"Neuron Specialization: Leveraging intrinsic task modularity for multilingual machine translation","abstract":"Training a unified multilingual model promotes knowledge transfer but inevitably introduces negative interference. Language-specific modeling methods show promise in reducing interference. However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules. In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation. We show that neurons in the feed-forward layers tend to be activated in a language-specific manner. Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers. Based on these findings, we propose Neuron Specialization, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks. Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer.","sentences":["Training a unified multilingual model promotes knowledge transfer but inevitably introduces negative interference.","Language-specific modeling methods show promise in reducing interference.","However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules.","In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation.","We show that neurons in the feed-forward layers tend to be activated in a language-specific manner.","Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers.","Based on these findings, we propose Neuron Specialization, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks.","Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer."],"url":"http://arxiv.org/abs/2404.11201v1","category":"cs.CL"}
{"created":"2024-04-17 09:30:41","title":"Forecasting with panel data: Estimation uncertainty versus parameter heterogeneity","abstract":"We provide a comprehensive examination of the predictive accuracy of panel forecasting methods based on individual, pooling, fixed effects, and Bayesian estimation, and propose optimal weights for forecast combination schemes. We consider linear panel data models, allowing for weakly exogenous regressors and correlated heterogeneity. We quantify the gains from exploiting panel data and demonstrate how forecasting performance depends on the degree of parameter heterogeneity, whether such heterogeneity is correlated with the regressors, the goodness of fit of the model, and the cross-sectional ($N$) and time ($T$) dimensions. Monte Carlo simulations and empirical applications to house prices and CPI inflation show that forecast combination and Bayesian forecasting methods perform best overall and rarely produce the least accurate forecasts for individual series.","sentences":["We provide a comprehensive examination of the predictive accuracy of panel forecasting methods based on individual, pooling, fixed effects, and Bayesian estimation, and propose optimal weights for forecast combination schemes.","We consider linear panel data models, allowing for weakly exogenous regressors and correlated heterogeneity.","We quantify the gains from exploiting panel data and demonstrate how forecasting performance depends on the degree of parameter heterogeneity, whether such heterogeneity is correlated with the regressors, the goodness of fit of the model, and the cross-sectional ($N$) and time ($T$) dimensions.","Monte Carlo simulations and empirical applications to house prices and CPI inflation show that forecast combination and Bayesian forecasting methods perform best overall and rarely produce the least accurate forecasts for individual series."],"url":"http://arxiv.org/abs/2404.11198v1","category":"econ.EM"}
{"created":"2024-04-17 09:15:40","title":"A beam-driven proton irradiation setup for precision radiation damage tests of silicon detectors","abstract":"A proton irradiation site for silicon detectors has been developed and commissioned at the Bonn Isochronous Cyclotron. The accelerator provides 14 MeV proton beams of up to 1 $\\mu$A at beam widths of a few mm to the setup. Devices Under Test (DUTs) are irradiated inside a cooled, thermally-insulated box at $\\le$-20{\\deg}C, while being moved through the beam in a row-based scan pattern to achieve uniform fluence distributions. Custom-made diagnostics allow for beam-based, on- and offline dosimetry, enabling a beam-driven irradiation routine which produces uniform fluence distributions with standard deviations $ \\ll 1 \\% $. Dedicated irradiations of thin titanium foils are performed to compare the commonly-used dosimetry via metallic foil activation to the beam-based approach. Within the error margins, both methods are in agreement, whereas the beam-based technique yields lower uncertainties of typically $ \\le 2 \\% $. Simulations indicate a reduction of the initial proton energy to 12.28(6) MeV on the DUT. Characterization of six, 150 $\\mu$m-thin, passive LFoundry sensors before and after irradiation yield a proton hardness factor of $\\kappa_\\text{p}=3.71(11)$, which is in agreement with expectations, allowing to irradiate up to $10^{16} \\text{n}_{eq} / \\text{cm}^2$ within a few hours.","sentences":["A proton irradiation site for silicon detectors has been developed and commissioned at the Bonn Isochronous Cyclotron.","The accelerator provides 14 MeV proton beams of up to 1 $\\mu$A at beam widths of a few mm to the setup.","Devices Under Test (DUTs) are irradiated inside a cooled, thermally-insulated box at $\\le$-20{\\deg}C, while being moved through the beam in a row-based scan pattern to achieve uniform fluence distributions.","Custom-made diagnostics allow for beam-based, on- and offline dosimetry, enabling a beam-driven irradiation routine which produces uniform fluence distributions with standard deviations $ \\ll 1 \\% $.","Dedicated irradiations of thin titanium foils are performed to compare the commonly-used dosimetry via metallic foil activation to the beam-based approach.","Within the error margins, both methods are in agreement, whereas the beam-based technique yields lower uncertainties of typically $ \\le 2 \\% $.","Simulations indicate a reduction of the initial proton energy to 12.28(6) MeV on the DUT.","Characterization of six, 150 $\\mu$m-thin, passive LFoundry sensors before and after irradiation yield a proton hardness factor of $\\kappa_\\text{p}=3.71(11)$, which is in agreement with expectations, allowing to irradiate up to $10^{16} \\text{n}_{eq} / \\text{cm}^2$ within a few hours."],"url":"http://arxiv.org/abs/2404.11192v1","category":"physics.ins-det"}
{"created":"2024-04-17 09:07:09","title":"Representations of $SL_2(F)$","abstract":"Let $p$ be a prime number, $F $ a non-archimedean local field with residue characteristic $p$, and $R$ an algebraically closed field of characteristic different from $ p$. We thoroughly investigate the irreducible smooth $R$-representations of $SL_2(F)$. The components of an irreducible smooth $R$-representation $\\Pi$ of $GL_2(F)$ restricted to $SL_2(F)$ form an $L$-packet $L(\\Pi)$. We use the classification of such $\\Pi$ to determine the cardinality of $L(\\Pi)$, which is $1,2$ or $4$. When $p=2$ we have to use the Langlands correspondence for $GL_2(F)$. When $\\ell$ is a prime number distinct from $p$ and $R=\\mathbb Q_\\ell^{ac}$, we establish the behaviour of an integral $L$-packet under reduction modulo $\\ell$. We prove a Langlands correspondence for $SL_2(F)$, and even an enhanced one when the characteristic of $R$ is not $2$. Finally, pursuing a theme of \\cite{HV23}, which studied the case of inner forms of $GL_n(F)$, we show that near identity an irreducible smooth R-representation of $SL_2(F)$ is, up to a finite dimensional representation, isomorphic to a sum of $1,2$ or $4$ representations in an $L$-packet of size $4$ (when $p$ is odd there is only one such $L$-packet).","sentences":["Let $p$ be a prime number, $F $ a non-archimedean local field with residue characteristic $p$, and $R$ an algebraically closed field of characteristic different from $ p$. We thoroughly investigate the irreducible smooth $R$-representations of $SL_2(F)$. The components of an irreducible smooth $R$-representation $\\Pi$ of $GL_2(F)$ restricted to $SL_2(F)$ form an $L$-packet $L(\\Pi)$. We use the classification of such $\\Pi$ to determine the cardinality of $L(\\Pi)$, which is $1,2$ or $4$. When $p=2$ we have to use the Langlands correspondence for $GL_2(F)$. When $\\ell$ is a prime number distinct from $p$ and $R=\\mathbb Q_\\ell^{ac}$, we establish the behaviour of an integral $L$-packet under reduction modulo $\\ell$. We prove a Langlands correspondence for $SL_2(F)$, and even an enhanced one when the characteristic of $R$ is not $2$. Finally, pursuing a theme of \\cite{HV23}, which studied the case of inner forms of $GL_n(F)$, we show that near identity an irreducible smooth R-representation of $SL_2(F)$ is, up to a finite dimensional representation, isomorphic to a sum of $1,2$ or $4$ representations in an $L$-packet of size $4$ (when $p$ is odd there is only one such $L$-packet)."],"url":"http://arxiv.org/abs/2404.11188v1","category":"math.RT"}
{"created":"2024-04-17 08:55:15","title":"How robust are the parameter constraints extending the $\u039b$CDM model?","abstract":"We present model-marginalized limits on the six standard $\\Lambda$CDM cosmological parameters ($\\Omega_{\\rm c} h^2$, $\\Omega_{\\rm b} h^2$, $\\theta_{\\rm MC}$, $\\tau_{\\rm reio}$, $n_s$ and $A_s$), as well as on selected derived quantities ($H_0$, $\\Omega_{\\rm m}$, $\\sigma_8$, $S_8$ and $r_{\\rm drag}$), obtained by considering three independent Cosmic Microwave Background (CMB) experiments: the Planck satellite, the Atacama Cosmology Telescope, and South Pole Telescope. We also consider low redshift observations in the form of Baryon Acoustic Oscillation (BAO) data from the SDSS-IV eBOSS survey and Supernovae (SN) distance moduli measurements from the Pantheon-Plus catalog. The marginalized errors are stable against the different fiducial cosmologies explored in this study. The largest impact on the parameter accuracy is produced by varying the effective number of relativistic degrees of freedom ($N_{\\rm eff}$) or the lensing amplitude ($A_{\\rm lens}$). Nevertheless the marginalized errors on some derived parameters such as $H_0$ or $\\Omega_{\\rm m}$ can be up to two orders of magnitude larger than in the canonical $\\Lambda$CDM scenario when considering only CMB data. In these cases, low redshift measurements are crucial for restoring the stability of the marginalized cosmological errors computed here. Overall, our results underscore remarkable stability in the mean values and precision of the main cosmological parameters, making irrelevant the choice of different possible cosmological scenarios once both high and low redshift probes are fully accounted for. The very same results should be understood as a tool to test exotic cosmological scenarios, as the marginalized values should be used in numerical analyses due to their robustness and slightly larger errors, providing a more realistic and conservative approach.","sentences":["We present model-marginalized limits on the six standard $\\Lambda$CDM cosmological parameters ($\\Omega_{\\rm c} h^2$, $\\Omega_{\\rm b} h^2$, $\\theta_{\\rm MC}$, $\\tau_{\\rm reio}$, $n_s$ and $A_s$), as well as on selected derived quantities ($H_0$, $\\Omega_{\\rm m}$, $\\sigma_8$, $S_8$ and $r_{\\rm drag}$), obtained by considering three independent Cosmic Microwave Background (CMB) experiments: the Planck satellite, the Atacama Cosmology Telescope, and South Pole Telescope.","We also consider low redshift observations in the form of Baryon Acoustic Oscillation (BAO) data from the SDSS-IV eBOSS survey and Supernovae (SN) distance moduli measurements from the Pantheon-Plus catalog.","The marginalized errors are stable against the different fiducial cosmologies explored in this study.","The largest impact on the parameter accuracy is produced by varying the effective number of relativistic degrees of freedom ($N_{\\rm eff}$) or the lensing amplitude ($A_{\\rm lens}$).","Nevertheless the marginalized errors on some derived parameters such as $H_0$ or $\\Omega_{\\rm m}$ can be up to two orders of magnitude larger than in the canonical $\\Lambda$CDM scenario when considering only CMB data.","In these cases, low redshift measurements are crucial for restoring the stability of the marginalized cosmological errors computed here.","Overall, our results underscore remarkable stability in the mean values and precision of the main cosmological parameters, making irrelevant the choice of different possible cosmological scenarios once both high and low redshift probes are fully accounted for.","The very same results should be understood as a tool to test exotic cosmological scenarios, as the marginalized values should be used in numerical analyses due to their robustness and slightly larger errors, providing a more realistic and conservative approach."],"url":"http://arxiv.org/abs/2404.11182v1","category":"astro-ph.CO"}
{"created":"2024-04-17 08:09:25","title":"Learning SO(3)-Invariant Semantic Correspondence via Local Shape Transform","abstract":"Establishing accurate 3D correspondences between shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised Rotation-Invariant 3D correspondence learner with Local Shape Transform, dubbed RIST, that learns to establish dense correspondences between shapes even under challenging intra-class variations and arbitrary orientations. Specifically, RIST learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shapes to be mapped to similar local shape descriptors, enabling RIST to establish dense point-wise correspondences. RIST demonstrates state-of-the-art performances on 3D part label transfer and semantic keypoint transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.","sentences":["Establishing accurate 3D correspondences between shapes stands as a pivotal challenge with profound implications for computer vision and robotics.","However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability.","In this work, we introduce a novel self-supervised Rotation-Invariant 3D correspondence learner with Local Shape Transform, dubbed RIST, that learns to establish dense correspondences between shapes even under challenging intra-class variations and arbitrary orientations.","Specifically, RIST learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor.","These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction.","Our proposed self-supervised training pipeline encourages semantically corresponding points from different shapes to be mapped to similar local shape descriptors, enabling RIST to establish dense point-wise correspondences.","RIST demonstrates state-of-the-art performances on 3D part label transfer and semantic keypoint transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins."],"url":"http://arxiv.org/abs/2404.11156v1","category":"cs.CV"}
{"created":"2024-04-17 08:08:34","title":"HybriMap: Hybrid Clues Utilization for Effective Vectorized HD Map Construction","abstract":"Constructing vectorized high-definition maps from surround-view cameras has garnered significant attention in recent years. However, the commonly employed multi-stage sequential workflow in prevailing approaches often leads to the loss of early-stage information, particularly in perspective-view features. Usually, such loss is observed as an instance missing or shape mismatching in the final birds-eye-view predictions. To address this concern, we propose a novel approach, namely \\textbf{HybriMap}, which effectively exploits clues from hybrid features to ensure the delivery of valuable information. Specifically, we design the Dual Enhancement Module, to enable both explicit integration and implicit modification under the guidance of hybrid features. Additionally, the perspective keypoints are utilized as supervision, further directing the feature enhancement process. Extensive experiments conducted on existing benchmarks have demonstrated the state-of-the-art performance of our proposed approach.","sentences":["Constructing vectorized high-definition maps from surround-view cameras has garnered significant attention in recent years.","However, the commonly employed multi-stage sequential workflow in prevailing approaches often leads to the loss of early-stage information, particularly in perspective-view features.","Usually, such loss is observed as an instance missing or shape mismatching in the final birds-eye-view predictions.","To address this concern, we propose a novel approach, namely \\textbf{HybriMap}, which effectively exploits clues from hybrid features to ensure the delivery of valuable information.","Specifically, we design the Dual Enhancement Module, to enable both explicit integration and implicit modification under the guidance of hybrid features.","Additionally, the perspective keypoints are utilized as supervision, further directing the feature enhancement process.","Extensive experiments conducted on existing benchmarks have demonstrated the state-of-the-art performance of our proposed approach."],"url":"http://arxiv.org/abs/2404.11155v1","category":"cs.CV"}
{"created":"2024-04-17 07:51:56","title":"The spatial average of solutions to SPDEs is asymptotically independent of the solution","abstract":"Let $\\left(u(t,x), t\\geq 0, x\\in \\mathbb{R}^d\\right)$ be the solution to the stochastic heat or wave equation driven by a Gaussian noise which is white in time and white or correlated with respect to the spatial variable. We consider the spatial average of the solution $F_{R}(t)= \\frac{1}{\\sigma_R}\\int_{\\vert x\\vert \\leq R} \\left( u(t,x)-1\\right) dx$, where $\\sigma^2_R= \\mathbf{E} \\left(\\int_{\\vert x\\vert \\leq R} \\left( u(t,x)-1\\right) dx\\right)^2$. It is known that, when $R$ goes to infinity, $F_R(t)$ converges in law to a standard Gaussian random variable $Z$. We show that the spatial average $F_R(t)$ is actually asymptotic independent by the solution itself, at any time and at any point in space, meaning that the random vector $(F_R(t), u(t, x_0))$ converges in distribution, as $R\\to \\infty$, to $(Z, u(t, x_0))$, where $Z$ is a standard normal random variable independent of $u(t, x_0)$. By using the Stein-Malliavin calculus, we also obtain the rate of convergence, under the Wasserstein distance, for this limit theorem.","sentences":["Let $\\left(u(t,x), t\\geq 0, x\\in \\mathbb{R}^d\\right)$ be the solution to the stochastic heat or wave equation driven by a Gaussian noise which is white in time and white or correlated with respect to the spatial variable.","We consider the spatial average of the solution $F_{R}(t)= \\frac{1}{\\sigma_R}\\int_{\\vert","x\\vert \\leq R} \\left( u(t,x)-1\\right) dx$, where $\\sigma^2_R= \\mathbf{E} \\left(\\int_{\\vert","x\\vert \\leq R} \\left( u(t,x)-1\\right) dx\\right)^2$. It is known that, when $R$ goes to infinity, $F_R(t)$ converges in law to a standard Gaussian random variable $Z$. We show that the spatial average $F_R(t)$ is actually asymptotic independent by the solution itself, at any time and at any point in space, meaning that the random vector $(F_R(t), u(t, x_0))$ converges in distribution, as $R\\to \\infty$, to $(Z, u(t, x_0))$, where $Z$ is a standard normal random variable independent of $u(t, x_0)$.","By using the Stein-Malliavin calculus, we also obtain the rate of convergence, under the Wasserstein distance, for this limit theorem."],"url":"http://arxiv.org/abs/2404.11147v1","category":"math.PR"}
{"created":"2024-04-17 07:28:41","title":"Improved analysis of the decay width of $t\\to Wb$ up to N$^{3}$LO QCD corrections","abstract":"In this paper, we analyze the top-quark decay $t\\to Wb$ up to next-to-next-to-next-to-leading order (N$^{3}$LO) QCD corrections. For the purpose, we first adopt the principle of maximum conformality (PMC) to deal with the initial pQCD series, and then we adopt the Bayesian analysis approach, which quantifies the unknown higher-order (UHO) terms' contributions in terms of a probability distribution, to estimate the possible magnitude of the N$^{4}$LO QCD corrections. In our calculation, an effective strong coupling constant $\\alpha_{s}(Q_{*})$ is determined by using all non-conformal $\\{\\beta_{i}\\}$ terms associated with the renormalization group equation. This results in the next-to-leading-log PMC scale $Q_{*}^{(\\rm NLL)}=10.3048$ GeV, which could be regarded as the correct momentum flow of the process. Consequently, we obtain an improved scale invariant pQCD prediction for the top-quark decay width $\\Gamma_{t}^{\\rm tot} = 1.3172 \\pm 0.0038$ GeV, where the error is the squared average of the uncertainties from the decay width of $W$-boson $\\Delta \\Gamma_{W} = \\pm 0.042$ GeV, the coupling constant $\\Delta \\alpha_{s}(m_{Z}) = \\pm 0.0009$ and the predicted N$^{4}$LO-level UHO-terms. The magnitude of the top-quark pole mass greatly affects the total decay width. By further taking the PDG top-quark pole mass error from cross-section measurements $\\Delta m_{t} = \\pm 0.7$ GeV into consideration, we obtain $\\Gamma_{t}^{\\rm tot} = 1.3172 ^{+0.0194}_{-0.0192}$ GeV.","sentences":["In this paper, we analyze the top-quark decay $t\\to Wb$ up to next-to-next-to-next-to-leading order (N$^{3}$LO) QCD corrections.","For the purpose, we first adopt the principle of maximum conformality (PMC) to deal with the initial pQCD series, and then we adopt the Bayesian analysis approach, which quantifies the unknown higher-order (UHO) terms' contributions in terms of a probability distribution, to estimate the possible magnitude of the N$^{4}$LO QCD corrections.","In our calculation, an effective strong coupling constant $\\alpha_{s}(Q_{*})$ is determined by using all non-conformal $\\{\\beta_{i}\\}$ terms associated with the renormalization group equation.","This results in the next-to-leading-log PMC scale $Q_{*}^{(\\rm NLL)}=10.3048$ GeV, which could be regarded as the correct momentum flow of the process.","Consequently, we obtain an improved scale invariant pQCD prediction for the top-quark decay width $\\Gamma_{t}^{\\rm tot} = 1.3172 \\pm 0.0038$ GeV, where the error is the squared average of the uncertainties from the decay width of $W$-boson $\\Delta \\Gamma_{W} = \\pm 0.042$ GeV, the coupling constant $\\Delta \\alpha_{s}(m_{Z})","= \\pm 0.0009$ and the predicted N$^{4}$LO-level UHO-terms.","The magnitude of the top-quark pole mass greatly affects the total decay width.","By further taking the PDG top-quark pole mass error from cross-section measurements $\\Delta m_{t} = \\pm 0.7$ GeV into consideration, we obtain $\\Gamma_{t}^{\\rm tot} = 1.3172 ^{+0.0194}_{-0.0192}$ GeV."],"url":"http://arxiv.org/abs/2404.11133v1","category":"hep-ph"}
{"created":"2024-04-17 07:19:20","title":"Enhancement of hole mobility in high-rate reactively sputtered Cu2O thin films induced by laser thermal annealing","abstract":"In presented work, a reactive high-power impulse magnetron sputtering (r-HiPIMS) was used for high-rate deposition ( 170 nm/min) of Cu2O films. Films were deposited on a standard soda-lime glass (SLG) substrate at a temperature of 190C. As-deposited films exhibit poor hole mobility in the orders of 1 cm2/Vs. We have systematically studied the effect of laser thermal annealing (LTA) procedure performed using high-power infrared laser under different laser parameters (number of pulses, length of the pulse). We have found, LTA procedure could significantly enhance the hole mobility (up to 24 cm2/Vs in our case). We have also fitted the results of a temperature-dependent Hall measurement to clarify the mechanism of the reported increase in hole mobility. Moreover, we have discussed the effect of the LTA procedure on microstructure (crystallinity, surface morphology) and on the value of optical band gap.","sentences":["In presented work, a reactive high-power impulse magnetron sputtering (r-HiPIMS) was used for high-rate deposition ( 170 nm/min) of Cu2O films.","Films were deposited on a standard soda-lime glass (SLG) substrate at a temperature of 190C. As-deposited films exhibit poor hole mobility in the orders of 1 cm2/Vs.","We have systematically studied the effect of laser thermal annealing (LTA) procedure performed using high-power infrared laser under different laser parameters (number of pulses, length of the pulse).","We have found, LTA procedure could significantly enhance the hole mobility (up to 24 cm2/Vs in our case).","We have also fitted the results of a temperature-dependent Hall measurement to clarify the mechanism of the reported increase in hole mobility.","Moreover, we have discussed the effect of the LTA procedure on microstructure (crystallinity, surface morphology) and on the value of optical band gap."],"url":"http://arxiv.org/abs/2404.11128v1","category":"physics.app-ph"}
{"created":"2024-04-17 07:16:41","title":"Interval-censored linear quantile regression","abstract":"Censored quantile regression has emerged as a prominent alternative to classical Cox's proportional hazards model or accelerated failure time model in both theoretical and applied statistics. While quantile regression has been extensively studied for right-censored survival data, methodologies for analyzing interval-censored data remain limited in the survival analysis literature. This paper introduces a novel local weighting approach for estimating linear censored quantile regression, specifically tailored to handle diverse forms of interval-censored survival data. The estimation equation and the corresponding convex objective function for the regression parameter can be constructed as a weighted average of quantile loss contributions at two interval endpoints. The weighting components are nonparametrically estimated using local kernel smoothing or ensemble machine learning techniques. To estimate the nonparametric distribution mass for interval-censored data, a modified EM algorithm for nonparametric maximum likelihood estimation is employed by introducing subject-specific latent Poisson variables. The proposed method's empirical performance is demonstrated through extensive simulation studies and real data analyses of two HIV/AIDS datasets.","sentences":["Censored quantile regression has emerged as a prominent alternative to classical Cox's proportional hazards model or accelerated failure time model in both theoretical and applied statistics.","While quantile regression has been extensively studied for right-censored survival data, methodologies for analyzing interval-censored data remain limited in the survival analysis literature.","This paper introduces a novel local weighting approach for estimating linear censored quantile regression, specifically tailored to handle diverse forms of interval-censored survival data.","The estimation equation and the corresponding convex objective function for the regression parameter can be constructed as a weighted average of quantile loss contributions at two interval endpoints.","The weighting components are nonparametrically estimated using local kernel smoothing or ensemble machine learning techniques.","To estimate the nonparametric distribution mass for interval-censored data, a modified EM algorithm for nonparametric maximum likelihood estimation is employed by introducing subject-specific latent Poisson variables.","The proposed method's empirical performance is demonstrated through extensive simulation studies and real data analyses of two HIV/AIDS datasets."],"url":"http://arxiv.org/abs/2404.11125v1","category":"stat.ME"}
{"created":"2024-04-17 06:49:14","title":"Consistency Training by Synthetic Question Generation for Conversational Question Answering","abstract":"Efficiently modeling historical information is a critical component in addressing user queries within a conversational question-answering (QA) context, as historical context plays a vital role in clarifying the user's questions. However, irrelevant history induces noise in the reasoning process, especially for those questions with a considerable historical context. In our novel model-agnostic approach, referred to as CoTaH (Consistency-Trained augmented History), we augment the historical information with synthetic questions and subsequently employ consistency training to train a model that utilizes both real and augmented historical data to implicitly make the reasoning robust to irrelevant history. To the best of our knowledge, this is the first instance of research using question generation as a form of data augmentation to model conversational QA settings. By citing a common modeling error prevalent in previous research, we introduce a new baseline model and compare our model's performance against it, demonstrating an improvement in results, particularly when dealing with questions that include a substantial amount of historical context. The source code can be found on our GitHub page.","sentences":["Efficiently modeling historical information is a critical component in addressing user queries within a conversational question-answering (QA) context, as historical context plays a vital role in clarifying the user's questions.","However, irrelevant history induces noise in the reasoning process, especially for those questions with a considerable historical context.","In our novel model-agnostic approach, referred to as CoTaH (Consistency-Trained augmented History), we augment the historical information with synthetic questions and subsequently employ consistency training to train a model that utilizes both real and augmented historical data to implicitly make the reasoning robust to irrelevant history.","To the best of our knowledge, this is the first instance of research using question generation as a form of data augmentation to model conversational QA settings.","By citing a common modeling error prevalent in previous research, we introduce a new baseline model and compare our model's performance against it, demonstrating an improvement in results, particularly when dealing with questions that include a substantial amount of historical context.","The source code can be found on our GitHub page."],"url":"http://arxiv.org/abs/2404.11109v1","category":"cs.CL"}
{"created":"2024-04-17 06:40:47","title":"Object Remover Performance Evaluation Methods using Class-wise Object Removal Images","abstract":"Object removal refers to the process of erasing designated objects from an image while preserving the overall appearance, and it is one area where image inpainting is widely used in real-world applications. The performance of an object remover is quantitatively evaluated by measuring the quality of object removal results, similar to how the performance of an image inpainter is gauged. Current works reporting quantitative performance evaluations utilize original images as references. In this letter, to validate the current evaluation methods cannot properly evaluate the performance of an object remover, we create a dataset with object removal ground truth and compare the evaluations made by the current methods using original images to those utilizing object removal ground truth images. The disparities between two evaluation sets validate that the current methods are not suitable for measuring the performance of an object remover. Additionally, we propose new evaluation methods tailored to gauge the performance of an object remover. The proposed methods evaluate the performance through class-wise object removal results and utilize images without the target class objects as a comparison set. We confirm that the proposed methods can make judgments consistent with human evaluators in the COCO dataset, and that they can produce measurements aligning with those using object removal ground truth in the self-acquired dataset.","sentences":["Object removal refers to the process of erasing designated objects from an image while preserving the overall appearance, and it is one area where image inpainting is widely used in real-world applications.","The performance of an object remover is quantitatively evaluated by measuring the quality of object removal results, similar to how the performance of an image inpainter is gauged.","Current works reporting quantitative performance evaluations utilize original images as references.","In this letter, to validate the current evaluation methods cannot properly evaluate the performance of an object remover, we create a dataset with object removal ground truth and compare the evaluations made by the current methods using original images to those utilizing object removal ground truth images.","The disparities between two evaluation sets validate that the current methods are not suitable for measuring the performance of an object remover.","Additionally, we propose new evaluation methods tailored to gauge the performance of an object remover.","The proposed methods evaluate the performance through class-wise object removal results and utilize images without the target class objects as a comparison set.","We confirm that the proposed methods can make judgments consistent with human evaluators in the COCO dataset, and that they can produce measurements aligning with those using object removal ground truth in the self-acquired dataset."],"url":"http://arxiv.org/abs/2404.11104v1","category":"cs.CV"}
{"created":"2024-04-17 06:39:03","title":"Distribution-Free Testing of Decision Lists with a Sublinear Number of Queries","abstract":"We give a distribution-free testing algorithm for decision lists with $\\tilde{O}(n^{11/12}/\\varepsilon^3)$ queries. This is the first sublinear algorithm for this problem, which shows that, unlike halfspaces, testing is strictly easier than learning for decision lists. Complementing the algorithm, we show that any distribution-free tester for decision lists must make $\\tilde{\\Omega}(\\sqrt{n})$ queries, or draw $\\tilde{\\Omega}(n)$ samples when the algorithm is sample-based.","sentences":["We give a distribution-free testing algorithm for decision lists with $\\tilde{O}(n^{11/12}/\\varepsilon^3)$ queries.","This is the first sublinear algorithm for this problem, which shows that, unlike halfspaces, testing is strictly easier than learning for decision lists.","Complementing the algorithm, we show that any distribution-free tester for decision lists must make $\\tilde{\\Omega}(\\sqrt{n})$ queries, or draw $\\tilde{\\Omega}(n)$ samples when the algorithm is sample-based."],"url":"http://arxiv.org/abs/2404.11103v1","category":"cs.DS"}
{"created":"2024-04-17 06:30:49","title":"Optimum Achievable Rates in Two Random Number Generation Problems with $f$-Divergences Using Smooth R\u00e9nyi Entropy","abstract":"Two typical fixed-length random number generation problems in information theory are considered for general sources. One is the source resolvability problem and the other is the intrinsic randomness problem. In each of these problems, the optimum achievable rate with respect to the given approximation measure is one of our main concerns and has been characterized using two different information quantities: the information spectrum and the smooth R\\'enyi entropy. Recently, optimum achievable rates with respect to $f$-divergences have been characterized using the information spectrum quantity. The $f$-divergence is a general non-negative measure between two probability distributions on the basis of a convex function $f$. The class of f-divergences includes several important measures such as the variational distance, the KL divergence, the Hellinger distance and so on. Hence, it is meaningful to consider the random number generation problems with respect to $f$-divergences. However, optimum achievable rates with respect to $f$-divergences using the smooth R\\'enyi entropy have not been clarified yet in both of two problems. In this paper we try to analyze the optimum achievable rates using the smooth R\\'enyi entropy and to extend the class of $f$-divergence. To do so, we first derive general formulas of the first-order optimum achievable rates with respect to $f$-divergences in both problems under the same conditions as imposed by previous studies. Next, we relax the conditions on $f$-divergence and generalize the obtained general formulas. Then, we particularize our general formulas to several specified functions $f$. As a result, we reveal that it is easy to derive optimum achievable rates for several important measures from our general formulas. Furthermore, a kind of duality between the resolvability and the intrinsic randomness is revealed in terms of the smooth R\\'enyi entropy.","sentences":["Two typical fixed-length random number generation problems in information theory are considered for general sources.","One is the source resolvability problem and the other is the intrinsic randomness problem.","In each of these problems, the optimum achievable rate with respect to the given approximation measure is one of our main concerns and has been characterized using two different information quantities: the information spectrum and the smooth R\\'enyi entropy.","Recently, optimum achievable rates with respect to $f$-divergences have been characterized using the information spectrum quantity.","The $f$-divergence is a general non-negative measure between two probability distributions on the basis of a convex function $f$.","The class of f-divergences includes several important measures such as the variational distance, the KL divergence, the Hellinger distance and so on.","Hence, it is meaningful to consider the random number generation problems with respect to $f$-divergences.","However, optimum achievable rates with respect to $f$-divergences using the smooth R\\'enyi entropy have not been clarified yet in both of two problems.","In this paper we try to analyze the optimum achievable rates using the smooth R\\'enyi entropy and to extend the class of $f$-divergence.","To do so, we first derive general formulas of the first-order optimum achievable rates with respect to $f$-divergences in both problems under the same conditions as imposed by previous studies.","Next, we relax the conditions on $f$-divergence and generalize the obtained general formulas.","Then, we particularize our general formulas to several specified functions $f$. As a result, we reveal that it is easy to derive optimum achievable rates for several important measures from our general formulas.","Furthermore, a kind of duality between the resolvability and the intrinsic randomness is revealed in terms of the smooth R\\'enyi entropy."],"url":"http://arxiv.org/abs/2404.11097v1","category":"cs.IT"}
{"created":"2024-04-17 06:16:31","title":"Estimation for conditional moment models based on martingale difference divergence","abstract":"We provide a new estimation method for conditional moment models via the martingale difference divergence (MDD).Our MDD-based estimation method is formed in the framework of a continuum of unconditional moment restrictions. Unlike the existing estimation methods in this framework, the MDD-based estimation method adopts a non-integrable weighting function, which could grab more information from unconditional moment restrictions than the integrable weighting function to enhance the estimation efficiency. Due to the nature of shift-invariance in MDD, our MDD-based estimation method can not identify the intercept parameters. To overcome this identification issue, we further provide a two-step estimation procedure for the model with intercept parameters. Under regularity conditions, we establish the asymptotics of the proposed estimators, which are not only easy-to-implement with analytic asymptotic variances, but also applicable to time series data with an unspecified form of conditional heteroskedasticity. Finally, we illustrate the usefulness of the proposed estimators by simulations and two real examples.","sentences":["We provide a new estimation method for conditional moment models via the martingale difference divergence (MDD).Our MDD-based estimation method is formed in the framework of a continuum of unconditional moment restrictions.","Unlike the existing estimation methods in this framework, the MDD-based estimation method adopts a non-integrable weighting function, which could grab more information from unconditional moment restrictions than the integrable weighting function to enhance the estimation efficiency.","Due to the nature of shift-invariance in MDD, our MDD-based estimation method can not identify the intercept parameters.","To overcome this identification issue, we further provide a two-step estimation procedure for the model with intercept parameters.","Under regularity conditions, we establish the asymptotics of the proposed estimators, which are not only easy-to-implement with analytic asymptotic variances, but also applicable to time series data with an unspecified form of conditional heteroskedasticity.","Finally, we illustrate the usefulness of the proposed estimators by simulations and two real examples."],"url":"http://arxiv.org/abs/2404.11092v1","category":"econ.EM"}
{"created":"2024-04-17 06:16:00","title":"Some nonlinear problems for the superposition of fractional operators with Neumann boundary conditions","abstract":"We discuss the existence theory of a nonlinear problem of nonlocal type subject to Neumann boundary conditions. Differently from the existing literature, the elliptic operator under consideration is obtained as a superposition of operators of mixed order.   The setting that we introduce is very general and comprises, for instance, the sum of two fractional Laplacians, or of a fractional Laplacian and a Laplacian, as particular cases (the situation in which there are infinitely many operators, and even a continuous distribution of operators, can be considered as well).   New bits of functional analysis are introduced to deal with this problem. An eigenvalue analysis divides the existence theory into two streams, one related to a Mountain Pass method, the other to a Linking technique.","sentences":["We discuss the existence theory of a nonlinear problem of nonlocal type subject to Neumann boundary conditions.","Differently from the existing literature, the elliptic operator under consideration is obtained as a superposition of operators of mixed order.   ","The setting that we introduce is very general and comprises, for instance, the sum of two fractional Laplacians, or of a fractional Laplacian and a Laplacian, as particular cases (the situation in which there are infinitely many operators, and even a continuous distribution of operators, can be considered as well).   ","New bits of functional analysis are introduced to deal with this problem.","An eigenvalue analysis divides the existence theory into two streams, one related to a Mountain Pass method, the other to a Linking technique."],"url":"http://arxiv.org/abs/2404.11091v1","category":"math.AP"}
{"created":"2024-04-17 06:14:51","title":"Quantification of locked mode instability triggered by a change in confinement","abstract":"This work presents the first analysis of the disruptive locked mode (LM) triggered by the dynamics of a confinement change. It shows that, under certain conditions, the LM threshold during the transient is significantly lower than expected from steady states. We investigate the sensitivity to a controlled $n = 1$ error field (EF) activated prior to the L-H transition in the COMPASS tokamak, at $q_{95} \\approx 3$, $\\beta_N \\approx 1$, and using EF coils on the high-field side of the vessel. A threshold for EF penetration subsequent to the L-H transition is identified, which shows no significant trend with density or applied torque, and is an apparent consequence of the reduced intrinsic rotation of the 2/1 mode during this transient phase. This finding challenges the assumption made in theoretical and empirical works that natural mode rotation can be predicted by global plasma parameters and urges against using any parametric EF penetration scaling derived from steady-state experiments to define the error field correction strategy in the entire discharge. Furthermore, even at EFs below the identified penetration threshold, disruptive locking of sawtooth-seeded 2/1 tearing modes is observed after about 30% of L-H transitions without external torque.","sentences":["This work presents the first analysis of the disruptive locked mode (LM) triggered by the dynamics of a confinement change.","It shows that, under certain conditions, the LM threshold during the transient is significantly lower than expected from steady states.","We investigate the sensitivity to a controlled $n = 1$ error field (EF) activated prior to the L-H transition in the COMPASS tokamak, at $q_{95} \\approx 3$, $\\beta_N \\approx 1$, and using EF coils on the high-field side of the vessel.","A threshold for EF penetration subsequent to the L-H transition is identified, which shows no significant trend with density or applied torque, and is an apparent consequence of the reduced intrinsic rotation of the 2/1 mode during this transient phase.","This finding challenges the assumption made in theoretical and empirical works that natural mode rotation can be predicted by global plasma parameters and urges against using any parametric EF penetration scaling derived from steady-state experiments to define the error field correction strategy in the entire discharge.","Furthermore, even at EFs below the identified penetration threshold, disruptive locking of sawtooth-seeded 2/1 tearing modes is observed after about 30% of L-H transitions without external torque."],"url":"http://arxiv.org/abs/2404.11090v1","category":"physics.plasm-ph"}
{"created":"2024-04-17 05:54:26","title":"Observation of Young's double-slit phenomenon in anti-PT-symmetric electrical circuits","abstract":"In the last few decades, interference has been extensively studied in both the quantum and classical fields, which reveals light volatility and is widely used for high-precision measurements. We have put forward the phenomenon in which the discrete diffraction and interference phenomena, presented by the time-varying voltage of a Su-Schrieffer-Heeger (SSH) circuit model with an anti-PT symmetry (APT) symmetry. To demonstrate Young's double-slit phenomenon in an APT circuit, we initially explore the coupled mode theory (CMT) of voltage in the broken phase, observe discrete diffraction under single excitation and interference under double excitations. Furthermore, we design a phase-shifting circuit to observe the effects of phase difference and distance on discrete interference. Our work combines the effects in optics with condensed matter physics, show the Young's double-slit phenomenon in electrical circuits theoretically and experimentally.","sentences":["In the last few decades, interference has been extensively studied in both the quantum and classical fields, which reveals light volatility and is widely used for high-precision measurements.","We have put forward the phenomenon in which the discrete diffraction and interference phenomena, presented by the time-varying voltage of a Su-Schrieffer-Heeger (SSH) circuit model with an anti-PT symmetry (APT) symmetry.","To demonstrate Young's double-slit phenomenon in an APT circuit, we initially explore the coupled mode theory (CMT) of voltage in the broken phase, observe discrete diffraction under single excitation and interference under double excitations.","Furthermore, we design a phase-shifting circuit to observe the effects of phase difference and distance on discrete interference.","Our work combines the effects in optics with condensed matter physics, show the Young's double-slit phenomenon in electrical circuits theoretically and experimentally."],"url":"http://arxiv.org/abs/2404.11084v1","category":"cond-mat.other"}
{"created":"2024-04-17 05:50:11","title":"Estimating conditional hazard functions and densities with the highly-adaptive lasso","abstract":"We consider estimation of conditional hazard functions and densities over the class of multivariate c\\`adl\\`ag functions with uniformly bounded sectional variation norm when data are either fully observed or subject to right-censoring. We demonstrate that the empirical risk minimizer is either not well-defined or not consistent for estimation of conditional hazard functions and densities. Under a smoothness assumption about the data-generating distribution, a highly-adaptive lasso estimator based on a particular data-adaptive sieve achieves the same convergence rate as has been shown to hold for the empirical risk minimizer in settings where the latter is well-defined. We use this result to study a highly-adaptive lasso estimator of a conditional hazard function based on right-censored data. We also propose a new conditional density estimator and derive its convergence rate. Finally, we show that the result is of interest also for settings where the empirical risk minimizer is well-defined, because the highly-adaptive lasso depends on a much smaller number of basis function than the empirical risk minimizer.","sentences":["We consider estimation of conditional hazard functions and densities over the class of multivariate c\\`adl\\`ag functions with uniformly bounded sectional variation norm when data are either fully observed or subject to right-censoring.","We demonstrate that the empirical risk minimizer is either not well-defined or not consistent for estimation of conditional hazard functions and densities.","Under a smoothness assumption about the data-generating distribution, a highly-adaptive lasso estimator based on a particular data-adaptive sieve achieves the same convergence rate as has been shown to hold for the empirical risk minimizer in settings where the latter is well-defined.","We use this result to study a highly-adaptive lasso estimator of a conditional hazard function based on right-censored data.","We also propose a new conditional density estimator and derive its convergence rate.","Finally, we show that the result is of interest also for settings where the empirical risk minimizer is well-defined, because the highly-adaptive lasso depends on a much smaller number of basis function than the empirical risk minimizer."],"url":"http://arxiv.org/abs/2404.11083v1","category":"math.ST"}
{"created":"2024-04-17 04:21:58","title":"Partial Identification of Heteroskedastic Structural VARs: Theory and Bayesian Inference","abstract":"We consider structural vector autoregressions identified through stochastic volatility. Our focus is on whether a particular structural shock is identified by heteroskedasticity without the need to impose any sign or exclusion restrictions. Three contributions emerge from our exercise: (i) a set of conditions under which the matrix containing structural parameters is partially or globally unique; (ii) a statistical procedure to assess the validity of the conditions mentioned above; and (iii) a shrinkage prior distribution for conditional variances centred on a hypothesis of homoskedasticity. Such a prior ensures that the evidence for identifying a structural shock comes only from the data and is not favoured by the prior. We illustrate our new methods using a U.S. fiscal structural model.","sentences":["We consider structural vector autoregressions identified through stochastic volatility.","Our focus is on whether a particular structural shock is identified by heteroskedasticity without the need to impose any sign or exclusion restrictions.","Three contributions emerge from our exercise: (i) a set of conditions under which the matrix containing structural parameters is partially or globally unique; (ii) a statistical procedure to assess the validity of the conditions mentioned above; and (iii) a shrinkage prior distribution for conditional variances centred on a hypothesis of homoskedasticity.","Such a prior ensures that the evidence for identifying a structural shock comes only from the data and is not favoured by the prior.","We illustrate our new methods using a U.S. fiscal structural model."],"url":"http://arxiv.org/abs/2404.11057v1","category":"econ.EM"}
{"created":"2024-04-17 03:51:55","title":"Supervised Contrastive Vision Transformer for Breast Histopathological Image Classification","abstract":"Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer. Breast tissue histopathological examination is critical in diagnosing and classifying breast cancer. Although existing methods have shown promising results, there is still room for improvement in the classification accuracy and generalization of IDC using histopathology images. We present a novel approach, Supervised Contrastive Vision Transformer (SupCon-ViT), for improving the classification of invasive ductal carcinoma in terms of accuracy and generalization by leveraging the inherent strengths and advantages of both transfer learning, i.e., pre-trained vision transformer, and supervised contrastive learning. Our results on a benchmark breast cancer dataset demonstrate that SupCon-Vit achieves state-of-the-art performance in IDC classification, with an F1-score of 0.8188, precision of 0.7692, and specificity of 0.8971, outperforming existing methods. In addition, the proposed model demonstrates resilience in scenarios with minimal labeled data, making it highly efficient in real-world clinical settings where labelled data is limited. Our findings suggest that supervised contrastive learning in conjunction with pre-trained vision transformers appears to be a viable strategy for an accurate classification of IDC, thus paving the way for a more efficient and reliable diagnosis of breast cancer through histopathological image analysis.","sentences":["Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer.","Breast tissue histopathological examination is critical in diagnosing and classifying breast cancer.","Although existing methods have shown promising results, there is still room for improvement in the classification accuracy and generalization of IDC using histopathology images.","We present a novel approach, Supervised Contrastive Vision Transformer (SupCon-ViT), for improving the classification of invasive ductal carcinoma in terms of accuracy and generalization by leveraging the inherent strengths and advantages of both transfer learning, i.e., pre-trained vision transformer, and supervised contrastive learning.","Our results on a benchmark breast cancer dataset demonstrate that SupCon-Vit achieves state-of-the-art performance in IDC classification, with an F1-score of 0.8188, precision of 0.7692, and specificity of 0.8971, outperforming existing methods.","In addition, the proposed model demonstrates resilience in scenarios with minimal labeled data, making it highly efficient in real-world clinical settings where labelled data is limited.","Our findings suggest that supervised contrastive learning in conjunction with pre-trained vision transformers appears to be a viable strategy for an accurate classification of IDC, thus paving the way for a more efficient and reliable diagnosis of breast cancer through histopathological image analysis."],"url":"http://arxiv.org/abs/2404.11052v1","category":"cs.CV"}
{"created":"2024-04-17 03:39:51","title":"Offset Unlearning for Large Language Models","abstract":"Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.","sentences":["Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns.","In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data.","However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction.","We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs.","Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models.","Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks.","$\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs."],"url":"http://arxiv.org/abs/2404.11045v1","category":"cs.CL"}
{"created":"2024-04-17 03:34:13","title":"The Effect of Defect (Re) Prediction on Software Testing","abstract":"Background: Cross-project defect prediction (CPDP) aims to use data from external projects as historical data may not be available from the same project. In CPDP, deciding on a particular historical project to build a training model can be difficult. To help with this decision, a Bandit Algorithm (BA) based approach has been proposed in prior research to select the most suitable learning project. However, this BA method could lead to the selection of unsuitable data during the early iteration of BA (i.e., early stage of software testing). Selecting an unsuitable model can reduce the prediction accuracy, leading to potential defect overlooking. This study aims to improve the BA method to reduce defects overlooking, especially during the early stage of testing. Once all modules have been tested, modules tested in the early stage are re-predicted, and some modules are retested based on the re-prediction. To assess the impact of re-prediction and retesting, we applied five kinds of BA methods, using 8, 16, and 32 OSS projects as learning data. The results show that the newly proposed approach steadily reduced the probability of defect overlooking on 86.7% of the BA methods and projects combinations.","sentences":["Background: Cross-project defect prediction (CPDP) aims to use data from external projects as historical data may not be available from the same project.","In CPDP, deciding on a particular historical project to build a training model can be difficult.","To help with this decision, a Bandit Algorithm (BA) based approach has been proposed in prior research to select the most suitable learning project.","However, this BA method could lead to the selection of unsuitable data during the early iteration of BA (i.e., early stage of software testing).","Selecting an unsuitable model can reduce the prediction accuracy, leading to potential defect overlooking.","This study aims to improve the BA method to reduce defects overlooking, especially during the early stage of testing.","Once all modules have been tested, modules tested in the early stage are re-predicted, and some modules are retested based on the re-prediction.","To assess the impact of re-prediction and retesting, we applied five kinds of BA methods, using 8, 16, and 32 OSS projects as learning data.","The results show that the newly proposed approach steadily reduced the probability of defect overlooking on 86.7% of the BA methods and projects combinations."],"url":"http://arxiv.org/abs/2404.11040v1","category":"cs.SE"}
{"created":"2024-04-17 03:28:15","title":"Nilpotent symplectic alternating algebras II","abstract":"In this paper and its sequel we continue our study of nilpotent symplectic alternating algebras. In particular we give a full classification of such algebras of dimension $10$ over any field. It is known that symplectic alternating algebras over $\\mbox{GF}(3)$ correspond to a special rich class $\\mathcal{C}$ of $2$-Engel $3$-groups of exponent $27$ and under this correspondence we will see that the nilpotent algebras correspond to a subclass of $\\mathcal{C}$ that are those groups in $\\mathcal{C}$ that have an extra group theoretical property that we refer to as being powerfully nilpotent and can be described also in the context of $p$-groups where $p$ is an arbitrary prime.","sentences":["In this paper and its sequel we continue our study of nilpotent symplectic alternating algebras.","In particular we give a full classification of such algebras of dimension $10$ over any field.","It is known that symplectic alternating algebras over $\\mbox{GF}(3)$ correspond to a special rich class $\\mathcal{C}$ of $2$-Engel $3$-groups of exponent $27$ and under this correspondence we will see that the nilpotent algebras correspond to a subclass of $\\mathcal{C}$ that are those groups in $\\mathcal{C}$ that have an extra group theoretical property that we refer to as being powerfully nilpotent and can be described also in the context of $p$-groups where $p$ is an arbitrary prime."],"url":"http://arxiv.org/abs/2404.11038v1","category":"math.RA"}
{"created":"2024-04-17 03:20:46","title":"Building Defect Prediction Models by Online Learning Considering Defect Overlooking","abstract":"Building defect prediction models based on online learning can enhance prediction accuracy. It continuously rebuilds a new prediction model, when a new data point is added. However, a module predicted as \"non-defective\" can result in fewer test cases for such modules. Thus, a defective module can be overlooked during testing. The erroneous test results are used as learning data by online learning, which could negatively affect prediction accuracy. To suppress the negative influence, we propose to apply a method that fixes the prediction as positive during the initial stage of online learning. Additionally, we improved the method to consider the probability of the overlooking. In our experiment, we demonstrate this negative influence on prediction accuracy, and the effectiveness of our approach. The results show that our approach did not negatively affect AUC but significantly improved recall.","sentences":["Building defect prediction models based on online learning can enhance prediction accuracy.","It continuously rebuilds a new prediction model, when a new data point is added.","However, a module predicted as \"non-defective\" can result in fewer test cases for such modules.","Thus, a defective module can be overlooked during testing.","The erroneous test results are used as learning data by online learning, which could negatively affect prediction accuracy.","To suppress the negative influence, we propose to apply a method that fixes the prediction as positive during the initial stage of online learning.","Additionally, we improved the method to consider the probability of the overlooking.","In our experiment, we demonstrate this negative influence on prediction accuracy, and the effectiveness of our approach.","The results show that our approach did not negatively affect AUC but significantly improved recall."],"url":"http://arxiv.org/abs/2404.11033v1","category":"cs.SE"}
{"created":"2024-04-17 03:04:40","title":"Non-Hermitian zero-energy pinning of Andreev and Majorana bound states in superconductor-semiconductor systems","abstract":"The emergence of Majorana bound states in finite length superconductor-semiconductor hybrid systems has been predicted to occur in the form of oscillatory energy levels with parity crossings around zero energy. Each zero-energy crossing is expected to produce a quantized zero-bias conductance peak but several studies have reported conductance peaks pinned at zero energy over a range of Zeeman fields, whose origin, however, is not clear. In this work we consider superconducting systems with spin-orbit coupling under a Zeeman field and demonstrate that non-Hermitian effects, due to coupling to ferromagnet leads, induce zero-energy pinning of Majorana and trivial Andreev bound states. We find that this zero-energy pinning effect occurs due to the formation of non-Hermitian spectral degeneracies known as exceptional points, whose emergence can be controlled by the interplay of non-Hermiticity, the applied Zeeman field, and chemical potentials. Moreover, depending on the non-Hermitian spatial profile, we find that non-Hermiticity changes the single point Hermitian topological phase transition into a flattened zero energy line bounded by exceptional points from multiple low energy levels. This seemingly innocent change notably enables a gap closing well below the Hermitian topological phase transition, which can be in principle simpler to achieve. Furthermore, we reveal that the energy gap separating Majorana and trivial Andreev bound states from the quasicontinuum remains robust for the values that give rise to the zero-energy pinning effect. While reasonable values of non-Hermiticity can be indeed beneficial, very strong non-Hermitian effects can be detrimental as it might destroy superconductivity. Our findings can be therefore useful for understanding the zero-energy pinning of trivial and topological states in Majorana devices.","sentences":["The emergence of Majorana bound states in finite length superconductor-semiconductor hybrid systems has been predicted to occur in the form of oscillatory energy levels with parity crossings around zero energy.","Each zero-energy crossing is expected to produce a quantized zero-bias conductance peak but several studies have reported conductance peaks pinned at zero energy over a range of Zeeman fields, whose origin, however, is not clear.","In this work we consider superconducting systems with spin-orbit coupling under a Zeeman field and demonstrate that non-Hermitian effects, due to coupling to ferromagnet leads, induce zero-energy pinning of Majorana and trivial Andreev bound states.","We find that this zero-energy pinning effect occurs due to the formation of non-Hermitian spectral degeneracies known as exceptional points, whose emergence can be controlled by the interplay of non-Hermiticity, the applied Zeeman field, and chemical potentials.","Moreover, depending on the non-Hermitian spatial profile, we find that non-Hermiticity changes the single point Hermitian topological phase transition into a flattened zero energy line bounded by exceptional points from multiple low energy levels.","This seemingly innocent change notably enables a gap closing well below the Hermitian topological phase transition, which can be in principle simpler to achieve.","Furthermore, we reveal that the energy gap separating Majorana and trivial Andreev bound states from the quasicontinuum remains robust for the values that give rise to the zero-energy pinning effect.","While reasonable values of non-Hermiticity can be indeed beneficial, very strong non-Hermitian effects can be detrimental as it might destroy superconductivity.","Our findings can be therefore useful for understanding the zero-energy pinning of trivial and topological states in Majorana devices."],"url":"http://arxiv.org/abs/2404.11026v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-17 02:41:35","title":"Optical Vortex Ladder via Sisyphus Pumping of Pseudospin","abstract":"Robust higher-order optical vortices are much in demand for applications in optical manipulation, optical communications, quantum entanglement and quantum computing. However, in numerous experimental settings, a controlled generation of optical vortices with arbitrary orbital angular momentum (OAM) remains a substantial challenge. Here, we present a concept of \"optical vortex ladder\" for stepwise generation of optical vortices through Sisyphus pumping of pseudospin modes in photonic graphene. Instead of conical diffraction and incomplete pseudospin conversion under traditional Gaussian beam excitations, the vortices produced in the ladder arise from non-trivial topology and feature diffraction-free Bessel profiles, thanks to the refined excitation of the ring spectrum around the Dirac cones. By employing a periodic \"kick\" to the photonic graphene, effectively inducing the Sisyphus pumping, the ladder enables tunable generation of optical vortices of any order even when the initial excitation does not involve any OAM. The optical vortex ladder stands out as an intriguing non-Hermitian dynamical system, and, among other possibilities, opens up a pathway for applications of topological singularities in beam shaping and wavefront engineering.","sentences":["Robust higher-order optical vortices are much in demand for applications in optical manipulation, optical communications, quantum entanglement and quantum computing.","However, in numerous experimental settings, a controlled generation of optical vortices with arbitrary orbital angular momentum (OAM) remains a substantial challenge.","Here, we present a concept of \"optical vortex ladder\" for stepwise generation of optical vortices through Sisyphus pumping of pseudospin modes in photonic graphene.","Instead of conical diffraction and incomplete pseudospin conversion under traditional Gaussian beam excitations, the vortices produced in the ladder arise from non-trivial topology and feature diffraction-free Bessel profiles, thanks to the refined excitation of the ring spectrum around the Dirac cones.","By employing a periodic \"kick\" to the photonic graphene, effectively inducing the Sisyphus pumping, the ladder enables tunable generation of optical vortices of any order even when the initial excitation does not involve any OAM.","The optical vortex ladder stands out as an intriguing non-Hermitian dynamical system, and, among other possibilities, opens up a pathway for applications of topological singularities in beam shaping and wavefront engineering."],"url":"http://arxiv.org/abs/2404.11011v1","category":"physics.optics"}
{"created":"2024-04-17 02:36:44","title":"Imprints of the Local Bubble and Dust Complexity on Polarized Dust Emission","abstract":"Using 3D dust maps and Planck polarized dust emission data, we investigate the influence of the 3D geometry of the nearby interstellar medium (ISM) on the statistics of the dust polarization on large ($80'$) scales. We test the idea that the magnetic field in the nearby dust is preferentially tangential to the Local Bubble wall, but we do not find an imprint of the Local Bubble geometry on the dust polarization fraction. We also test the hypothesis that the complexity of the 3D dust distribution drives some of the measured variation of the dust polarization fraction. We compare sightlines with similar total column densities and find evidence that, on average, the dust polarization fraction decreases when the dust column is substantially distributed among multiple components at different distances. Conversely, the polarization fraction is higher for sightlines where the dust is more concentrated in 3D space. This finding is statistically significant for the dust within 1.25 kpc, but the effect disappears if we only consider dust within 270 pc. The extended 3D dust distribution, rather than solely the dust associated with the Local Bubble, plays a role in determining the observed dust polarization fraction on these scales. This conclusion is consistent with a simple analytical prediction and remains robust under various modifications to the analysis. These results illuminate the relationship between the 3D geometry of the ISM and tracers of the interstellar magnetic field. We also discuss implications for our understanding of the polarized dust foreground to the cosmic microwave background.","sentences":["Using 3D dust maps and Planck polarized dust emission data, we investigate the influence of the 3D geometry of the nearby interstellar medium (ISM) on the statistics of the dust polarization on large ($80'$) scales.","We test the idea that the magnetic field in the nearby dust is preferentially tangential to the Local Bubble wall, but we do not find an imprint of the Local Bubble geometry on the dust polarization fraction.","We also test the hypothesis that the complexity of the 3D dust distribution drives some of the measured variation of the dust polarization fraction.","We compare sightlines with similar total column densities and find evidence that, on average, the dust polarization fraction decreases when the dust column is substantially distributed among multiple components at different distances.","Conversely, the polarization fraction is higher for sightlines where the dust is more concentrated in 3D space.","This finding is statistically significant for the dust within 1.25 kpc, but the effect disappears if we only consider dust within 270 pc.","The extended 3D dust distribution, rather than solely the dust associated with the Local Bubble, plays a role in determining the observed dust polarization fraction on these scales.","This conclusion is consistent with a simple analytical prediction and remains robust under various modifications to the analysis.","These results illuminate the relationship between the 3D geometry of the ISM and tracers of the interstellar magnetic field.","We also discuss implications for our understanding of the polarized dust foreground to the cosmic microwave background."],"url":"http://arxiv.org/abs/2404.11009v1","category":"astro-ph.GA"}
{"created":"2024-04-17 02:32:10","title":"Periodicity in New York State COVID-19 Hospitalizations Leveraged from the Variable Bandpass Periodic Block Bootstrap","abstract":"The outbreak of the SARS-CoV-2 virus, which led to an unprecedented global pandemic, has underscored the critical importance of understanding seasonal patterns. This knowledge is fundamental for decision-making in healthcare and public health domains. Investigating the presence, intensity, and precise nature of seasonal trends, as well as these temporal patterns, is essential for forecasting future occurrences, planning interventions, and making informed decisions based on the evolution of events over time. This study employs the Variable Bandpass Periodic Block Bootstrap (VBPBB) to separate and analyze different periodic components by frequency in time series data, focusing on annually correlated (PC) principal components. Bootstrapping, a method used to estimate statistical sampling distributions through random sampling with replacement, is particularly useful in this context. Specifically, block bootstrapping, a model-independent resampling method suitable for time series data, is utilized. Its extensions are aimed at preserving the correlation structures inherent in PC processes. The VBPBB applies a bandpass filter to isolate the relevant PC frequency, thereby minimizing contamination from extraneous frequencies and noise. This approach significantly narrows the confidence intervals, enhancing the precision of estimated sampling distributions for the investigated periodic characteristics. Furthermore, we compared the outcomes of block bootstrapping for periodically correlated time series with VBPBB against those from more traditional bootstrapping methods. Our analysis shows VBPBB provides strong evidence of the existence of an annual seasonal PC pattern in hospitalization rates not detectible by other methods, providing timing and confidence intervals for their impact.","sentences":["The outbreak of the SARS-CoV-2 virus, which led to an unprecedented global pandemic, has underscored the critical importance of understanding seasonal patterns.","This knowledge is fundamental for decision-making in healthcare and public health domains.","Investigating the presence, intensity, and precise nature of seasonal trends, as well as these temporal patterns, is essential for forecasting future occurrences, planning interventions, and making informed decisions based on the evolution of events over time.","This study employs the Variable Bandpass Periodic Block Bootstrap (VBPBB) to separate and analyze different periodic components by frequency in time series data, focusing on annually correlated (PC) principal components.","Bootstrapping, a method used to estimate statistical sampling distributions through random sampling with replacement, is particularly useful in this context.","Specifically, block bootstrapping, a model-independent resampling method suitable for time series data, is utilized.","Its extensions are aimed at preserving the correlation structures inherent in PC processes.","The VBPBB applies a bandpass filter to isolate the relevant PC frequency, thereby minimizing contamination from extraneous frequencies and noise.","This approach significantly narrows the confidence intervals, enhancing the precision of estimated sampling distributions for the investigated periodic characteristics.","Furthermore, we compared the outcomes of block bootstrapping for periodically correlated time series with VBPBB against those from more traditional bootstrapping methods.","Our analysis shows VBPBB provides strong evidence of the existence of an annual seasonal PC pattern in hospitalization rates not detectible by other methods, providing timing and confidence intervals for their impact."],"url":"http://arxiv.org/abs/2404.11006v1","category":"stat.AP"}
{"created":"2024-04-17 02:31:27","title":"Robust and tractable multidimensional exponential analysis","abstract":"Motivated by a number of applications in signal processing, we study the following question. Given samples of a multidimensional signal of the form \\begin{align*} f(\\bs\\ell)=\\sum_{k=1}^K a_k\\exp(-i\\langle \\bs\\ell, \\w_k\\rangle), \\\\ \\w_1,\\cdots,\\w_k\\in\\mathbb{R}^q, \\ \\bs\\ell\\in \\ZZ^q, \\ |\\bs\\ell| <n, \\end{align*} determine the values of the number $K$ of components, and the parameters $a_k$ and $\\w_k$'s. We develop an algorithm to recuperate these quantities accurately using only a subsample of size $\\O(qn)$ of this data. For this purpose, we use a novel localized kernel method to identify the parameters, including the number $K$ of signals. Our method is easy to implement, and is shown to be stable under a very low SNR range. We demonstrate the effectiveness of our resulting algorithm using 2 and 3 dimensional examples from the literature, and show substantial improvements over state-of-the-art techniques including Prony based, MUSIC and ESPRIT approaches.","sentences":["Motivated by a number of applications in signal processing, we study the following question.","Given samples of a multidimensional signal of the form \\begin{align*} f(\\bs\\ell)=\\sum_{k=1}^K a_k\\exp(-i\\langle \\bs\\ell, \\w_k\\rangle), \\\\ \\w_1,\\cdots,\\w_k\\in\\mathbb{R}^q, \\ \\bs\\ell\\in \\ZZ^q, \\ |\\bs\\ell| <n, \\end{align*} determine the values of the number $K$ of components, and the parameters $a_k$ and $\\w_k$'s.","We develop an algorithm to recuperate these quantities accurately using only a subsample of size $\\O(qn)$ of this data.","For this purpose, we use a novel localized kernel method to identify the parameters, including the number $K$ of signals.","Our method is easy to implement, and is shown to be stable under a very low SNR range.","We demonstrate the effectiveness of our resulting algorithm using 2 and 3 dimensional examples from the literature, and show substantial improvements over state-of-the-art techniques including Prony based, MUSIC and ESPRIT approaches."],"url":"http://arxiv.org/abs/2404.11004v1","category":"eess.SP"}
{"created":"2024-04-17 02:24:53","title":"AdS Ellis wormholes with scalar field","abstract":"In this paper, we study the spherically symmetric traversable wormholes with a scalar field supported by a phantom field in the anti-de Sitter (AdS) asymptotic spacetime. Despite coupling the scalar matter field, these wormholes remain massless and symmetric for reflection of the radial coordinate $r \\rightarrow -r$. The solution possesses a finite Noether charge $Q$, which varies as a function of frequency $\\omega$ with changes in the cosmological constant $\\Lambda$ and the throat size $r_0$. Under specific conditions, an approximate ``event horizon'' will appear at the throat.","sentences":["In this paper, we study the spherically symmetric traversable wormholes with a scalar field supported by a phantom field in the anti-de Sitter (AdS) asymptotic spacetime.","Despite coupling the scalar matter field, these wormholes remain massless and symmetric for reflection of the radial coordinate $r \\rightarrow -r$.","The solution possesses a finite Noether charge $Q$, which varies as a function of frequency $\\omega$ with changes in the cosmological constant $\\Lambda$ and the throat size $r_0$. Under specific conditions, an approximate ``event horizon'' will appear at the throat."],"url":"http://arxiv.org/abs/2404.11002v1","category":"gr-qc"}
{"created":"2024-04-17 02:18:14","title":"Towards unveiling the Cosmic Reionization: the ionizing photon production efficiency ($\u03be_{ion}$) of Low-mass H$\u03b1$ emitters at $z \\sim 2.3$","abstract":"We investigate the galaxy properties of $\\sim$400 low-mass ($<10^9\\,M_{\\odot}$) H$\\alpha$ emitters (HAEs) at z $\\sim$ 2.3 in the ZFOURGE survey. The selection of these HAEs is based on the excess in the observed $K_s$ broad-band flux compared to the stellar continuum estimated from the best-fit SED. These low-mass HAEs have elevated SFR(H$\\alpha$) above the star formation main sequence (SFMS), making them potential analogs of the galaxies that reionized the universe during the epoch of reionization. The ionizing photon production efficiencies ($\\xi_{ion}$) of the low-mass HAEs have a median value of $\\mathrm{log}(\\xi_{ion}/erg^{-1} Hz)=25.24^{+0.10}_{-0.13}\\ (25.35^{+0.12}_{-0.15})$, assuming the Calzetti (SMC) curve for the stellar continuum dust correction. This value is higher than that of main sequence galaxies by $\\sim$0.2 dex at similar redshift, indicating that the low-mass HAEs are more efficient in producing ionizing photons. Our results also consolidate the trend of increasing $\\xi_{ion}$ with redshift, but reveal a \"downsizing\" relationship between $\\xi_{ion}$ and stellar mass ($M_{\\odot}$) with increasing redshift. We further explore the dependence of $\\xi_{ion}$ on other galaxy properties, such as the UV spectral slope ($\\beta_{\\mathrm{UV}}$), the UV magnitude ($M_{\\mathrm{UV}}$), the equivalent widths ($EWs$) of H$\\alpha$ and [O{\\sc iii}] emission lines. Galaxies with the bluer UV slopes, fainter UV luminosities and higher equivalent widths exhibit elevated $\\xi_{ion}$ by a factor of $\\sim$2 compared to the median $\\xi_{ion}$ of our sample. JWST data will provide an opportunity to extend our method and further investigate the properties of low-mass galaxies at high redshifts.","sentences":["We investigate the galaxy properties of $\\sim$400 low-mass ($<10^9\\,M_{\\odot}$) H$\\alpha$ emitters (HAEs) at z $\\sim$ 2.3 in the ZFOURGE survey.","The selection of these HAEs is based on the excess in the observed $K_s$ broad-band flux compared to the stellar continuum estimated from the best-fit SED.","These low-mass HAEs have elevated SFR(H$\\alpha$) above the star formation main sequence (SFMS), making them potential analogs of the galaxies that reionized the universe during the epoch of reionization.","The ionizing photon production efficiencies ($\\xi_{ion}$) of the low-mass HAEs have a median value of $\\mathrm{log}(\\xi_{ion}/erg^{-1} Hz)=25.24^{+0.10}_{-0.13}\\ (25.35^{+0.12}_{-0.15})$, assuming the Calzetti (SMC) curve for the stellar continuum dust correction.","This value is higher than that of main sequence galaxies by $\\sim$0.2 dex at similar redshift, indicating that the low-mass HAEs are more efficient in producing ionizing photons.","Our results also consolidate the trend of increasing $\\xi_{ion}$ with redshift, but reveal a \"downsizing\" relationship between $\\xi_{ion}$ and stellar mass ($M_{\\odot}$) with increasing redshift.","We further explore the dependence of $\\xi_{ion}$ on other galaxy properties, such as the UV spectral slope ($\\beta_{\\mathrm{UV}}$), the UV magnitude ($M_{\\mathrm{UV}}$), the equivalent widths ($EWs$) of H$\\alpha$ and [O{\\sc iii}] emission lines.","Galaxies with the bluer UV slopes, fainter UV luminosities and higher equivalent widths exhibit elevated $\\xi_{ion}$ by a factor of $\\sim$2 compared to the median $\\xi_{ion}$ of our sample.","JWST data will provide an opportunity to extend our method and further investigate the properties of low-mass galaxies at high redshifts."],"url":"http://arxiv.org/abs/2404.10998v1","category":"astro-ph.GA"}
{"created":"2024-04-17 02:17:23","title":"Online Algorithms with Limited Data Retention","abstract":"We introduce a model of online algorithms subject to strict constraints on data retention. An online learning algorithm encounters a stream of data points, one per round, generated by some stationary process. Crucially, each data point can request that it be removed from memory $m$ rounds after it arrives. To model the impact of removal, we do not allow the algorithm to store any information or calculations between rounds other than a subset of the data points (subject to the retention constraints). At the conclusion of the stream, the algorithm answers a statistical query about the full dataset. We ask: what level of performance can be guaranteed as a function of $m$?   We illustrate this framework for multidimensional mean estimation and linear regression problems. We show it is possible to obtain an exponential improvement over a baseline algorithm that retains all data as long as possible. Specifically, we show that $m = \\textsc{Poly}(d, \\log(1/\\epsilon))$ retention suffices to achieve mean squared error $\\epsilon$ after observing $O(1/\\epsilon)$ $d$-dimensional data points. This matches the error bound of the optimal, yet infeasible, algorithm that retains all data forever. We also show a nearly matching lower bound on the retention required to guarantee error $\\epsilon$. One implication of our results is that data retention laws are insufficient to guarantee the right to be forgotten even in a non-adversarial world in which firms merely strive to (approximately) optimize the performance of their algorithms.   Our approach makes use of recent developments in the multidimensional random subset sum problem to simulate the progression of stochastic gradient descent under a model of adversarial noise, which may be of independent interest.","sentences":["We introduce a model of online algorithms subject to strict constraints on data retention.","An online learning algorithm encounters a stream of data points, one per round, generated by some stationary process.","Crucially, each data point can request that it be removed from memory $m$ rounds after it arrives.","To model the impact of removal, we do not allow the algorithm to store any information or calculations between rounds other than a subset of the data points (subject to the retention constraints).","At the conclusion of the stream, the algorithm answers a statistical query about the full dataset.","We ask: what level of performance can be guaranteed as a function of $m$?   ","We illustrate this framework for multidimensional mean estimation and linear regression problems.","We show it is possible to obtain an exponential improvement over a baseline algorithm that retains all data as long as possible.","Specifically, we show that $m = \\textsc{Poly}(d, \\log(1/\\epsilon))$ retention suffices to achieve mean squared error $\\epsilon$ after observing $O(1/\\epsilon)$ $d$-dimensional data points.","This matches the error bound of the optimal, yet infeasible, algorithm that retains all data forever.","We also show a nearly matching lower bound on the retention required to guarantee error $\\epsilon$. One implication of our results is that data retention laws are insufficient to guarantee the right to be forgotten even in a non-adversarial world in which firms merely strive to (approximately) optimize the performance of their algorithms.   ","Our approach makes use of recent developments in the multidimensional random subset sum problem to simulate the progression of stochastic gradient descent under a model of adversarial noise, which may be of independent interest."],"url":"http://arxiv.org/abs/2404.10997v1","category":"cs.LG"}
{"created":"2024-04-17 02:07:07","title":"A Proximal Gradient Method with an Explicit Line search for Multiobjective Optimization","abstract":"We present a proximal gradient method for solving convex multiobjective optimization problems, where each objective function is the sum of two convex functions, with one assumed to be continuously differentiable. The algorithm incorporates a backtracking line search procedure that requires solving only one proximal subproblem per iteration, and is exclusively applied to the differentiable part of the objective functions. Under mild assumptions, we show that the sequence generated by the method convergences to a weakly Pareto optimal point of the problem. Additionally, we establish an iteration complexity bound by showing that the method finds an $\\varepsilon$-approximate weakly Pareto point in at most ${\\cal O}(1/\\varepsilon)$ iterations. Numerical experiments illustrating the practical behavior of the method is presented.","sentences":["We present a proximal gradient method for solving convex multiobjective optimization problems, where each objective function is the sum of two convex functions, with one assumed to be continuously differentiable.","The algorithm incorporates a backtracking line search procedure that requires solving only one proximal subproblem per iteration, and is exclusively applied to the differentiable part of the objective functions.","Under mild assumptions, we show that the sequence generated by the method convergences to a weakly Pareto optimal point of the problem.","Additionally, we establish an iteration complexity bound by showing that the method finds an $\\varepsilon$-approximate weakly Pareto point in at most ${\\cal O}(1/\\varepsilon)$ iterations.","Numerical experiments illustrating the practical behavior of the method is presented."],"url":"http://arxiv.org/abs/2404.10993v1","category":"math.OC"}
{"created":"2024-04-17 01:44:51","title":"Melnikov Method for Perturbed Completely Integrable Systems","abstract":"We consider a completely integrable system of differential equations in arbitrary dimensions whose phase space contains an open set foliated by periodic orbits. This research analyzes the persistence and stability of the periodic orbits under a nonlinear periodic perturbation. For this purpose, we use the Melnikov method and Floquet theory to establish conditions for the existence and stability of periodic orbits. Our approach considers periods of the unperturbed orbits depending on the integrals and constant periods. In the applications, we deal with both cases. Precisely, we study the existence of periodic orbits in a perturbed generalized Euler system. In the degenerate case, we analyze the existence and stability of periodic orbits for a perturbed harmonic oscillator.","sentences":["We consider a completely integrable system of differential equations in arbitrary dimensions whose phase space contains an open set foliated by periodic orbits.","This research analyzes the persistence and stability of the periodic orbits under a nonlinear periodic perturbation.","For this purpose, we use the Melnikov method and Floquet theory to establish conditions for the existence and stability of periodic orbits.","Our approach considers periods of the unperturbed orbits depending on the integrals and constant periods.","In the applications, we deal with both cases.","Precisely, we study the existence of periodic orbits in a perturbed generalized Euler system.","In the degenerate case, we analyze the existence and stability of periodic orbits for a perturbed harmonic oscillator."],"url":"http://arxiv.org/abs/2404.10986v1","category":"math.DS"}
{"created":"2024-04-17 01:31:00","title":"Graph Continual Learning with Debiased Lossless Memory Replay","abstract":"Real-life graph data often expands continually, rendering the learning of graph neural networks (GNNs) on static graph data impractical. Graph continual learning (GCL) tackles this problem by continually adapting GNNs to the expanded graph of the current task while maintaining the performance over the graph of previous tasks. Memory replay-based methods, which aim to replay data of previous tasks when learning new tasks, have been explored as one principled approach to mitigate the forgetting of the knowledge learned from the previous tasks. In this paper we extend this methodology with a novel framework, called Debiased Lossless Memory replay (DeLoMe). Unlike existing methods that sample nodes/edges of previous graphs to construct the memory, DeLoMe learns small lossless synthetic node representations as the memory. The learned memory can not only preserve the graph data privacy but also capture the holistic graph information, for which the sampling-based methods are not viable. Further, prior methods suffer from bias toward the current task due to the data imbalance between the classes in the memory data and the current data. A debiased GCL loss function is devised in DeLoMe to effectively alleviate this bias. Extensive experiments on four graph datasets show the effectiveness of DeLoMe under both class- and task-incremental learning settings.","sentences":["Real-life graph data often expands continually, rendering the learning of graph neural networks (GNNs) on static graph data impractical.","Graph continual learning (GCL) tackles this problem by continually adapting GNNs to the expanded graph of the current task while maintaining the performance over the graph of previous tasks.","Memory replay-based methods, which aim to replay data of previous tasks when learning new tasks, have been explored as one principled approach to mitigate the forgetting of the knowledge learned from the previous tasks.","In this paper we extend this methodology with a novel framework, called Debiased Lossless Memory replay (DeLoMe).","Unlike existing methods that sample nodes/edges of previous graphs to construct the memory, DeLoMe learns small lossless synthetic node representations as the memory.","The learned memory can not only preserve the graph data privacy but also capture the holistic graph information, for which the sampling-based methods are not viable.","Further, prior methods suffer from bias toward the current task due to the data imbalance between the classes in the memory data and the current data.","A debiased GCL loss function is devised in DeLoMe to effectively alleviate this bias.","Extensive experiments on four graph datasets show the effectiveness of DeLoMe under both class- and task-incremental learning settings."],"url":"http://arxiv.org/abs/2404.10984v1","category":"cs.LG"}
{"created":"2024-04-17 01:29:20","title":"Remote Cross-resonance Gate between Superconducting Fixed-frequency Qubits","abstract":"High-fidelity quantum state transfer and remote entanglement between superconducting fixed-frequency qubits have not yet been realized. In this study, we propose an alternative remote cross-resonance gate. Considering multiple modes of a superconducting coaxial cable connecting qubits, we must find conditions under which the cross-resonance gate operates with a certain accuracy even in the presence of qubit frequency shifts due to manufacturing errors. For 0.25- and 0.5-m cables, remote cross-resonance gates with a concurrence of $>99.9\\%$ in entanglement generation are obtained even with $\\pm$10-MHz frequency shifts. For a 1-m cable with a narrow mode spacing, a concurrence of 99.5\\% is achieved by reducing the coupling between the qubits and cable. The optimized echoed raised-cosine pulse duration is 150--400 ns, which is similar to the operation time of cross-resonance gates between neighboring qubits on a chip. The dissipation through the cable modes does not considerably affect the obtained results. Such high-precision quantum interconnects pave the way not only for scaling up quantum computer systems but also for nonlocal connections on a chip.","sentences":["High-fidelity quantum state transfer and remote entanglement between superconducting fixed-frequency qubits have not yet been realized.","In this study, we propose an alternative remote cross-resonance gate.","Considering multiple modes of a superconducting coaxial cable connecting qubits, we must find conditions under which the cross-resonance gate operates with a certain accuracy even in the presence of qubit frequency shifts due to manufacturing errors.","For 0.25- and 0.5-m cables, remote cross-resonance gates with a concurrence of $>99.9\\%$ in entanglement generation are obtained even with $\\pm$10-MHz frequency shifts.","For a 1-m cable with a narrow mode spacing, a concurrence of 99.5\\% is achieved by reducing the coupling between the qubits and cable.","The optimized echoed raised-cosine pulse duration is 150--400 ns, which is similar to the operation time of cross-resonance gates between neighboring qubits on a chip.","The dissipation through the cable modes does not considerably affect the obtained results.","Such high-precision quantum interconnects pave the way not only for scaling up quantum computer systems but also for nonlocal connections on a chip."],"url":"http://arxiv.org/abs/2404.10983v1","category":"quant-ph"}
{"created":"2024-04-17 00:06:32","title":"Eulerian $k$-dominating reconfiguration graphs","abstract":"For a graph $G$, the vertices of the $k$-dominating graph, denoted $\\mathcal{D}_k(G)$, correspond to the dominating sets of $G$ with cardinality at most $k$. Two vertices of $\\mathcal{D}_k(G)$ are adjacent if and only if the corresponding dominating sets in $G$ can be obtained from one other by adding or removing a single vertex of $G$. Since $\\mathcal{D}_k(G)$ is not necessarily connected when $k < |V(G)|$, much research has focused on conditions under which $\\mathcal{D}_k(G)$ is connected and recent work has explored the existence of Hamilton paths in the $k$-dominating graph. We consider the complementary problem of determining the conditions under which the $k$-dominating graph is Eulerian. In the case where $k = |V(G)|$, we characterize those graphs $G$ for which $\\mathcal{D}_k(G)$ is Eulerian. In the case where $k$ is restricted, we determine for a number of graph classes, the conditions under which the $k$-dominating graph is Eulerian.","sentences":["For a graph $G$, the vertices of the $k$-dominating graph, denoted $\\mathcal{D}_k(G)$, correspond to the dominating sets of $G$ with cardinality at most $k$. Two vertices of $\\mathcal{D}_k(G)$ are adjacent if and only if the corresponding dominating sets in $G$ can be obtained from one other by adding or removing a single vertex of $G$. Since $\\mathcal{D}_k(G)$ is not necessarily connected when $k < |V(G)|$, much research has focused on conditions under which $\\mathcal{D}_k(G)$ is connected and recent work has explored the existence of Hamilton paths in the $k$-dominating graph.","We consider the complementary problem of determining the conditions under which the $k$-dominating graph is Eulerian.","In the case where $k = |V(G)|$, we characterize those graphs $G$ for which $\\mathcal{D}_k(G)$ is Eulerian.","In the case where $k$ is restricted, we determine for a number of graph classes, the conditions under which the $k$-dominating graph is Eulerian."],"url":"http://arxiv.org/abs/2404.10962v1","category":"math.CO"}
{"created":"2024-04-17 00:06:02","title":"Chernoff Bounds and Reverse Hypercontractivity on HDX","abstract":"We prove optimal concentration of measure for lifted functions on high dimensional expanders (HDX). Let $X$ be a $k$-dimensional HDX. We show for any $i\\leq k$ and $f:X(i)\\to [0,1]$: \\[\\Pr_{s\\in X(k)}\\left[\\left|\\underset{{t\\subseteq s}}{\\mathbb{E}}[f(t)]-\\mu\\right|\\geq\\varepsilon\\right]\\leq exp\\left(-\\varepsilon^2\\frac{k}{i}\\right).\\] Using this fact, we prove that high dimensional expanders are reverse hypercontractive, a powerful functional inequality from discrete analysis implying that for any sets $A,B \\subset X(k)$, the probability a $\\rho$-correlated pair passes between them is at least \\[\\Pr_{s,s' \\sim T_\\rho}[s \\in A, s' \\in B] \\geq \\Pr[A]^{O(1)} \\Pr[B]^{O(1)}.\\] Our results hold under weak spectral assumptions on $X$. Namely we prove exponential concentration of measure for any complex below the `Trickling-Down Threshold' (beyond which concentration may be arbitrarily poor), and optimal concentration for $\\sqrt{k}$-skeletons of such complexes. We also show optimal bounds for the top dimension of stronger HDX among other settings. We leverage our inequalities to prove several new agreement testing theorems on high dimensional expanders, including a new 99%-regime test for subsets, and a variant of the `Z-test' achieving inverse exponential soundness under the stronger assumption of $\\ell_\\infty$-expansion. The latter gives rise to the first optimal testers beyond the complete complex and products, a stepping stone toward the use of HDX in strong soundness PCPs. We also give applications within expansion, analysis, combinatorics, and coding theory, including a proof that two-sided HDX have optimal geometric overlap (giving the first explicit bounded-degree construction), near-optimal double samplers, new super-exponential degree lower bounds for certain HDX, distance-amplified list-decodable and locally testable codes, a Frankl-R\\\"odl Theorem and more.","sentences":["We prove optimal concentration of measure for lifted functions on high dimensional expanders (HDX).","Let $X$ be a $k$-dimensional HDX.","We show for any $i\\leq k$ and $f:X(i)\\to","[0,1]$: \\[\\Pr_{s\\in X(k)}\\left[\\left|\\underset{{t\\subseteq s}}{\\mathbb{E}}[f(t)]-\\mu\\right|\\geq\\varepsilon\\right]\\leq exp\\left(-\\varepsilon^2\\frac{k}{i}\\right).\\] Using this fact, we prove that high dimensional expanders are reverse hypercontractive, a powerful functional inequality from discrete analysis implying that for any sets $A,B \\subset X(k)$, the probability a $\\rho$-correlated pair passes between them is at least \\[\\Pr_{s,s' \\sim T_\\rho}[s \\in A, s' \\in B] \\geq \\Pr[A]^{O(1)} \\Pr[B]^{O(1)}.\\] Our results hold under weak spectral assumptions on $X$. Namely we prove exponential concentration of measure for any complex below the `Trickling-Down Threshold' (beyond which concentration may be arbitrarily poor), and optimal concentration for $\\sqrt{k}$-skeletons of such complexes.","We also show optimal bounds for the top dimension of stronger HDX among other settings.","We leverage our inequalities to prove several new agreement testing theorems on high dimensional expanders, including a new 99%-regime test for subsets, and a variant of the `Z-test' achieving inverse exponential soundness under the stronger assumption of $\\ell_\\infty$-expansion.","The latter gives rise to the first optimal testers beyond the complete complex and products, a stepping stone toward the use of HDX in strong soundness PCPs.","We also give applications within expansion, analysis, combinatorics, and coding theory, including a proof that two-sided HDX have optimal geometric overlap (giving the first explicit bounded-degree construction), near-optimal double samplers, new super-exponential degree lower bounds for certain HDX, distance-amplified list-decodable and locally testable codes, a Frankl-R\\\"odl Theorem and more."],"url":"http://arxiv.org/abs/2404.10961v1","category":"cs.CC"}
{"created":"2024-04-16 23:30:57","title":"A computational account of the development and evolution of psychotic symptoms","abstract":"The mechanisms of psychotic symptoms like hallucinations and delusions are often investigated in fully-formed illness, well after symptoms emerge. These investigations have yielded key insights, but are not well-positioned to reveal the dynamic forces underlying symptom formation itself. Understanding symptom development over time would allow us to identify steps in the pathophysiological process leading to psychosis, shifting the focus of psychiatric intervention from symptom alleviation to prevention. We propose a model for understanding the emergence of psychotic symptoms within the context of an adaptive, developing neural system. We will make the case for a pathophysiological process that begins with cortical hyperexcitability and bottom-up noise transmission, which engenders inappropriate belief formation via aberrant prediction error signaling. We will argue that this bottom-up noise drives learning about the (im)precision of new incoming sensory information because of diminished signal-to-noise ratio, causing an adaptive relative over-reliance on prior beliefs. This over-reliance on priors predisposes to hallucinations and covaries with hallucination severity. An over-reliance on priors may also lead to increased conviction in the beliefs generated by bottom-up noise and drive movement toward conversion to psychosis. We will identify predictions of our model at each stage, examine evidence to support or refute those predictions, and propose experiments that could falsify or help select between alternative elements of the overall model. Nesting computational abnormalities within longitudinal development allows us to account for hidden dynamics among the mechanisms driving symptom formation and to view established symptomatology as a point of equilibrium among competing biological forces.","sentences":["The mechanisms of psychotic symptoms like hallucinations and delusions are often investigated in fully-formed illness, well after symptoms emerge.","These investigations have yielded key insights, but are not well-positioned to reveal the dynamic forces underlying symptom formation itself.","Understanding symptom development over time would allow us to identify steps in the pathophysiological process leading to psychosis, shifting the focus of psychiatric intervention from symptom alleviation to prevention.","We propose a model for understanding the emergence of psychotic symptoms within the context of an adaptive, developing neural system.","We will make the case for a pathophysiological process that begins with cortical hyperexcitability and bottom-up noise transmission, which engenders inappropriate belief formation via aberrant prediction error signaling.","We will argue that this bottom-up noise drives learning about the (im)precision of new incoming sensory information because of diminished signal-to-noise ratio, causing an adaptive relative over-reliance on prior beliefs.","This over-reliance on priors predisposes to hallucinations and covaries with hallucination severity.","An over-reliance on priors may also lead to increased conviction in the beliefs generated by bottom-up noise and drive movement toward conversion to psychosis.","We will identify predictions of our model at each stage, examine evidence to support or refute those predictions, and propose experiments that could falsify or help select between alternative elements of the overall model.","Nesting computational abnormalities within longitudinal development allows us to account for hidden dynamics among the mechanisms driving symptom formation and to view established symptomatology as a point of equilibrium among competing biological forces."],"url":"http://arxiv.org/abs/2404.10954v1","category":"q-bio.NC"}
{"created":"2024-04-16 23:17:04","title":"Human-Algorithm Collaborative Bayesian Optimization for Engineering Systems","abstract":"Bayesian optimization has been successfully applied throughout Chemical Engineering for the optimization of functions that are expensive-to-evaluate, or where gradients are not easily obtainable. However, domain experts often possess valuable physical insights that are overlooked in fully automated decision-making approaches, necessitating the inclusion of human input. In this article we re-introduce the human back into the data-driven decision making loop by outlining an approach for collaborative Bayesian optimization. Our methodology exploits the hypothesis that humans are more efficient at making discrete choices rather than continuous ones and enables experts to influence critical early decisions. We apply high-throughput (batch) Bayesian optimization alongside discrete decision theory to enable domain experts to influence the selection of experiments. At every iteration we apply a multi-objective approach that results in a set of alternate solutions that have both high utility and are reasonably distinct. The expert then selects the desired solution for evaluation from this set, allowing for the inclusion of expert knowledge and improving accountability, whilst maintaining the advantages of Bayesian optimization. We demonstrate our approach across a number of applied and numerical case studies including bioprocess optimization and reactor geometry design, demonstrating that even in the case of an uninformed practitioner our algorithm recovers the regret of standard Bayesian optimization. Through the inclusion of continuous expert opinion, our approach enables faster convergence, and improved accountability for Bayesian optimization in engineering systems.","sentences":["Bayesian optimization has been successfully applied throughout Chemical Engineering for the optimization of functions that are expensive-to-evaluate, or where gradients are not easily obtainable.","However, domain experts often possess valuable physical insights that are overlooked in fully automated decision-making approaches, necessitating the inclusion of human input.","In this article we re-introduce the human back into the data-driven decision making loop by outlining an approach for collaborative Bayesian optimization.","Our methodology exploits the hypothesis that humans are more efficient at making discrete choices rather than continuous ones and enables experts to influence critical early decisions.","We apply high-throughput (batch) Bayesian optimization alongside discrete decision theory to enable domain experts to influence the selection of experiments.","At every iteration we apply a multi-objective approach that results in a set of alternate solutions that have both high utility and are reasonably distinct.","The expert then selects the desired solution for evaluation from this set, allowing for the inclusion of expert knowledge and improving accountability, whilst maintaining the advantages of Bayesian optimization.","We demonstrate our approach across a number of applied and numerical case studies including bioprocess optimization and reactor geometry design, demonstrating that even in the case of an uninformed practitioner our algorithm recovers the regret of standard Bayesian optimization.","Through the inclusion of continuous expert opinion, our approach enables faster convergence, and improved accountability for Bayesian optimization in engineering systems."],"url":"http://arxiv.org/abs/2404.10949v1","category":"cs.HC"}
{"created":"2024-04-16 22:47:37","title":"Viscous shock waves of Burgers equation with fast diffusion and singularity","abstract":"In this paper, we study the asymptotic stability of viscous shock waves for Burgers' equation with fast diffusion $u_t+f(u)_x=\\mu (u^m)_{xx}$ on $\\mathbb{R} \\times (0, +\\infty)$ when $0<m<1$. For the proposed constant states $u_->u_+=0$, the equation with fast diffusion $(u^m)_{xx}=m\\left(\\frac{u_x}{u^{1-m}}\\right)_x$ processes a strong singularity at $u_+=0$, which causes the stability study to be challenging. We observe that, there exist two different types of viscous shocks, one is the non-degenerate shock satisfying Lax's entropy condition with fast algebraic decay to the singular state $u_+=0$, which causes much strong singularity to the system in the form of $m\\left(\\frac{u_x}{u^{1-m}}\\right)_x$, and the other is the degenerate viscous shocks with slow algebraic decay to $u_+=0$, which makes less strong singularity to the system. In order to overcome the singularity at $u_+=0$, we technically use the weighted energy method and develop a new strategy where the weights related to the shock waves are carefully selected, while the chosen weights for the non-degenerate case are stronger than the degenerate case. Numerical simulations are also carried out in different cases to illustrate and validate our theoretical results. In particular, we numerically approximate the solution for different value of $0<m<1$, and find that the shapes of shock waves become steeper when the singularity $\\left(\\frac{u_x}{u^{1-m}}\\right)_x$ is stronger as $m\\rightarrow 0$, which indicates that the effect of singular fast diffusion on the solution is essential.","sentences":["In this paper, we study the asymptotic stability of viscous shock waves for Burgers' equation with fast diffusion $u_t+f(u)_x=\\mu (u^m)_{xx}$ on $\\mathbb{R} \\times (0, +\\infty)$ when $0<m<1$. For the proposed constant states $u_->u_+=0$, the equation with fast diffusion $(u^m)_{xx}=m\\left(\\frac{u_x}{u^{1-m}}\\right)_x$ processes a strong singularity at $u_+=0$, which causes the stability study to be challenging.","We observe that, there exist two different types of viscous shocks, one is the non-degenerate shock satisfying Lax's entropy condition with fast algebraic decay to the singular state $u_+=0$, which causes much strong singularity to the system in the form of $m\\left(\\frac{u_x}{u^{1-m}}\\right)_x$, and the other is the degenerate viscous shocks with slow algebraic decay to $u_+=0$, which makes less strong singularity to the system.","In order to overcome the singularity at $u_+=0$, we technically use the weighted energy method and develop a new strategy where the weights related to the shock waves are carefully selected, while the chosen weights for the non-degenerate case are stronger than the degenerate case.","Numerical simulations are also carried out in different cases to illustrate and validate our theoretical results.","In particular, we numerically approximate the solution for different value of $0<m<1$, and find that the shapes of shock waves become steeper when the singularity $\\left(\\frac{u_x}{u^{1-m}}\\right)_x$ is stronger as $m\\rightarrow 0$, which indicates that the effect of singular fast diffusion on the solution is essential."],"url":"http://arxiv.org/abs/2404.10941v1","category":"math.AP"}
{"created":"2024-04-16 22:35:02","title":"A Universal Property of the Spectrum of a Ring and the Semiring of its Ideals","abstract":"Arithmetic valuations are intimately connected with the structure of the ideals of a commutative ring. We show how the generalized idempotent semiring valuations of Jeffrey and Noah Giansiracusa can be used to make this connection explicit. Through this generalized valuation theory sufficiently complete positive valuations give rise to Galois correspondences with the lattice of ideals of a commutative ring. Up to isomorphism the semiring of ideals of a commutative ring can be defined as a universal factoring semiring for positive valuations. We then further show that this valuation theory can formally connect Joyal's \\textit{notion de z\\'eros} and universal property of the spectrum of a ring to arithmetic valuation theory. We show that up to isomorphism as a coframe, the closed Zariski topology on a spectrum of a commutative ring can be defined as the universal factoring semiring of additively and multiplicatively idempotent valuations.","sentences":["Arithmetic valuations are intimately connected with the structure of the ideals of a commutative ring.","We show how the generalized idempotent semiring valuations of Jeffrey and Noah Giansiracusa can be used to make this connection explicit.","Through this generalized valuation theory sufficiently complete positive valuations give rise to Galois correspondences with the lattice of ideals of a commutative ring.","Up to isomorphism the semiring of ideals of a commutative ring can be defined as a universal factoring semiring for positive valuations.","We then further show that this valuation theory can formally connect Joyal's \\textit{notion de z\\'eros} and universal property of the spectrum of a ring to arithmetic valuation theory.","We show that up to isomorphism as a coframe, the closed Zariski topology on a spectrum of a commutative ring can be defined as the universal factoring semiring of additively and multiplicatively idempotent valuations."],"url":"http://arxiv.org/abs/2404.10937v1","category":"math.AC"}
{"created":"2024-04-16 22:01:39","title":"Decreasing Wages in Gig Economy: A Game Theoretic Explanation Using Mathematical Program Networks","abstract":"Gig economy consists of two market groups connected via an intermediary. Popular examples are rideshares where passengers and drivers are mediated via platforms such as Uber and Lyft. In a duopoly market, the platforms must compete to attract not only the passengers by providing a lower rate but also the drivers by providing better wages. While this should indicate better driver payout, as platforms compete to attract the driver pool, real world statistics does not indicate such. This goes completely against the intuition that the worker side of a gig economy, given their importance, should always earn better. We attempt to answer the low wages of drivers in the gig economy by modeling the ridesharing game between duopoly platforms, drivers, and passengers using Mathematical Program Networks. Our model is parsimonious, expressive, models the same-side and cross-side externalities of the economy, and has interpretations under both single-homing and multi-homing regimes. We derive the conditions for the existence of a profitable duopoly and show that it can only happen if the platforms collude together to pay the bare minimum to the drivers. This not only answers why drivers are paid less but also provides strong managerial insights to any interested policy maker.","sentences":["Gig economy consists of two market groups connected via an intermediary.","Popular examples are rideshares where passengers and drivers are mediated via platforms such as Uber and Lyft.","In a duopoly market, the platforms must compete to attract not only the passengers by providing a lower rate but also the drivers by providing better wages.","While this should indicate better driver payout, as platforms compete to attract the driver pool, real world statistics does not indicate such.","This goes completely against the intuition that the worker side of a gig economy, given their importance, should always earn better.","We attempt to answer the low wages of drivers in the gig economy by modeling the ridesharing game between duopoly platforms, drivers, and passengers using Mathematical Program Networks.","Our model is parsimonious, expressive, models the same-side and cross-side externalities of the economy, and has interpretations under both single-homing and multi-homing regimes.","We derive the conditions for the existence of a profitable duopoly and show that it can only happen if the platforms collude together to pay the bare minimum to the drivers.","This not only answers why drivers are paid less but also provides strong managerial insights to any interested policy maker."],"url":"http://arxiv.org/abs/2404.10929v1","category":"cs.GT"}
{"created":"2024-04-16 21:55:37","title":"Ultra-light Dark Matter Limits from Astrophysical Neutrino Flavor","abstract":"Ultra-light dark matter is a class of dark matter models where the mass of the dark matter particle is very small and the dark matter behaves as a classical field pervading our galaxy. If astrophysical neutrinos interact with ultra-light dark matter, these interactions would produce a matter potential in our galaxy which may cause anomalous flavor conversions. Recently, IceCube high-energy starting event flavor measurements are used to set stringent limits on isotropic Lorentz violating fields under the Standard-Model Extension framework. We apply the IceCube Lorentz violation limits to set limits on neutrino - ultra-light dark matter couplings. We assume the dark matter field undergoes fast oscillations in our galaxy, yielding neutrino interactions with dark matter that broaden and smear the observed flavor structure of astrophysical neutrinos at IceCube. The constraints we obtain are an order of magnitude tighter than current and future terrestrial neutrino experimental limits. The sensitivity of ultra-light dark matter can be further improved in the near future by new particle identification algorithms in IceCube and the emergence of next-generation neutrino telescopes.","sentences":["Ultra-light dark matter is a class of dark matter models where the mass of the dark matter particle is very small and the dark matter behaves as a classical field pervading our galaxy.","If astrophysical neutrinos interact with ultra-light dark matter, these interactions would produce a matter potential in our galaxy which may cause anomalous flavor conversions.","Recently, IceCube high-energy starting event flavor measurements are used to set stringent limits on isotropic Lorentz violating fields under the Standard-Model Extension framework.","We apply the IceCube Lorentz violation limits to set limits on neutrino - ultra-light dark matter couplings.","We assume the dark matter field undergoes fast oscillations in our galaxy, yielding neutrino interactions with dark matter that broaden and smear the observed flavor structure of astrophysical neutrinos at IceCube.","The constraints we obtain are an order of magnitude tighter than current and future terrestrial neutrino experimental limits.","The sensitivity of ultra-light dark matter can be further improved in the near future by new particle identification algorithms in IceCube and the emergence of next-generation neutrino telescopes."],"url":"http://arxiv.org/abs/2404.10926v1","category":"hep-ph"}
{"created":"2024-04-16 21:43:34","title":"Subjective Equilibria under Beliefs of Exogenous Uncertainty: Linear Quadratic Case","abstract":"We consider a stochastic dynamic game where players have their own linear state dynamics and quadratic cost functions. Players are coupled through some environment variables, generated by another linear system driven by the states and decisions of all players. Each player observes his own states realized up to the current time as well as the past realizations of his own decisions and the environment variables. Each player (incorrectly) believes that the environment variables are generated by an independent exogenous stochastic process. In this setup, we study the notion of ``subjective equilibrium under beliefs of exogenous uncertainty (SEBEU)'' introduced in our recent work arXiv:2005.01640. At an SEBEU, each player's strategy is optimal with respect to his subjective belief; moreover, the objective probability distribution of the environment variables is consistent with players' subjective beliefs. We construct an SEBEU in pure strategies, where each player strategy is an affine function of his own state and his estimate of the system state.","sentences":["We consider a stochastic dynamic game where players have their own linear state dynamics and quadratic cost functions.","Players are coupled through some environment variables, generated by another linear system driven by the states and decisions of all players.","Each player observes his own states realized up to the current time as well as the past realizations of his own decisions and the environment variables.","Each player (incorrectly) believes that the environment variables are generated by an independent exogenous stochastic process.","In this setup, we study the notion of ``subjective equilibrium under beliefs of exogenous uncertainty (SEBEU)'' introduced in our recent work arXiv:2005.01640.","At an SEBEU, each player's strategy is optimal with respect to his subjective belief; moreover, the objective probability distribution of the environment variables is consistent with players' subjective beliefs.","We construct an SEBEU in pure strategies, where each player strategy is an affine function of his own state and his estimate of the system state."],"url":"http://arxiv.org/abs/2404.10920v1","category":"math.OC"}
{"created":"2024-04-16 21:33:05","title":"Which questions should I answer? Salience Prediction of Inquisitive Questions","abstract":"Inquisitive questions -- open-ended, curiosity-driven questions people ask as they read -- are an integral part of discourse processing (Kehler and Rohde, 2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has taken advantage of question generation capabilities of LLMs to enhance a wide range of applications. But the space of inquisitive questions is vast: many questions can be evoked from a given context. So which of those should be prioritized to find answers? Linguistic theories, unfortunately, have not yet provided an answer to this question. This paper presents QSALIENCE, a salience predictor of inquisitive questions. QSALIENCE is instruction-tuned over our dataset of linguist-annotated salience scores of 1,766 (context, question) pairs. A question scores high on salience if answering it would greatly enhance the understanding of the text (Van Rooy, 2003). We show that highly salient questions are empirically more likely to be answered in the same article, bridging potential questions (Onea, 2016) with Questions Under Discussion (Roberts, 2012). We further validate our findings by showing that answering salient questions is an indicator of summarization quality in news.","sentences":["Inquisitive questions -- open-ended, curiosity-driven questions people ask as they read -- are an integral part of discourse processing (Kehler and Rohde, 2017; Onea, 2016) and comprehension (Prince, 2004).","Recent work in NLP has taken advantage of question generation capabilities of LLMs to enhance a wide range of applications.","But the space of inquisitive questions is vast: many questions can be evoked from a given context.","So which of those should be prioritized to find answers?","Linguistic theories, unfortunately, have not yet provided an answer to this question.","This paper presents QSALIENCE, a salience predictor of inquisitive questions.","QSALIENCE is instruction-tuned over our dataset of linguist-annotated salience scores of 1,766 (context, question) pairs.","A question scores high on salience if answering it would greatly enhance the understanding of the text (Van Rooy, 2003).","We show that highly salient questions are empirically more likely to be answered in the same article, bridging potential questions (Onea, 2016) with Questions Under Discussion (Roberts, 2012).","We further validate our findings by showing that answering salient questions is an indicator of summarization quality in news."],"url":"http://arxiv.org/abs/2404.10917v1","category":"cs.CL"}
{"created":"2024-04-16 21:12:54","title":"Efficient Batch and Recursive Least Squares for Matrix Parameter Estimation with Application to Adaptive MPC","abstract":"Traditionally, batch least squares (BLS) and recursive least squares (RLS) are used for identification of a vector of parameters that form a linear model. In some situations, however, it is of interest to identify parameters in a matrix structure. In this case, a common approach is to transform the problem into standard vector form using the vectorization (vec) operator and the Kronecker product, known as vec-permutation. However, the use of the Kronecker product introduces extraneous zero terms in the regressor, resulting in unnecessary additional computational and space requirements. This work derives matrix BLS and RLS formulations which, under mild assumptions, minimize the same cost as the vec-permutation approach. This new approach requires less computational complexity and space complexity than vec-permutation in both BLS and RLS identification. It is also shown that persistent excitation guarantees convergence to the true matrix parameters. This method can used to improve computation time in the online identification of multiple-input, multiple-output systems for indirect adaptive model predictive control.","sentences":["Traditionally, batch least squares (BLS) and recursive least squares (RLS) are used for identification of a vector of parameters that form a linear model.","In some situations, however, it is of interest to identify parameters in a matrix structure.","In this case, a common approach is to transform the problem into standard vector form using the vectorization (vec) operator and the Kronecker product, known as vec-permutation.","However, the use of the Kronecker product introduces extraneous zero terms in the regressor, resulting in unnecessary additional computational and space requirements.","This work derives matrix BLS and RLS formulations which, under mild assumptions, minimize the same cost as the vec-permutation approach.","This new approach requires less computational complexity and space complexity than vec-permutation in both BLS and RLS identification.","It is also shown that persistent excitation guarantees convergence to the true matrix parameters.","This method can used to improve computation time in the online identification of multiple-input, multiple-output systems for indirect adaptive model predictive control."],"url":"http://arxiv.org/abs/2404.10911v1","category":"eess.SP"}
{"created":"2024-04-16 21:10:05","title":"Discovering Factorization Surface of Quantum Spin Chains with Machine Learning","abstract":"Entanglement in quantum many-body systems is required for a variety of quantum information tasks, making it crucial to identify the parameter space in which the ground state is fully separable, known as the factorization surface (FS). Nonetheless, the tuning parameters indicating FS for several quantum spin models remain unknown. We employ symbolic regression (SR), a supervised learning technique, to determine a closed-form expression in the parameter regime corresponding to FS of quantum many-body Hamiltonians. We verify the effectiveness of this method by examining the analytically tractable models, namely a nearest-neighbor (NN) quantum transverse XY model with additional Kaplan-Shekhtman-Entin-Aharony interactions, for which the FS is well-known. We construct an accurate expression for the FS of the XYZ model by providing the parameter set through the SR algorithm in which the ground state is derived by matrix product state formalism. With a satisfactory level of accuracy, we estimate the FS for the long-range XY model, and the NN XY model with Dzyaloshinskii-Moriya type asymmetric interaction for which the factorization surface is not known.","sentences":["Entanglement in quantum many-body systems is required for a variety of quantum information tasks, making it crucial to identify the parameter space in which the ground state is fully separable, known as the factorization surface (FS).","Nonetheless, the tuning parameters indicating FS for several quantum spin models remain unknown.","We employ symbolic regression (SR), a supervised learning technique, to determine a closed-form expression in the parameter regime corresponding to FS of quantum many-body Hamiltonians.","We verify the effectiveness of this method by examining the analytically tractable models, namely a nearest-neighbor (NN) quantum transverse XY model with additional Kaplan-Shekhtman-Entin-Aharony interactions, for which the FS is well-known.","We construct an accurate expression for the FS of the XYZ model by providing the parameter set through the SR algorithm in which the ground state is derived by matrix product state formalism.","With a satisfactory level of accuracy, we estimate the FS for the long-range XY model, and the NN XY model with Dzyaloshinskii-Moriya type asymmetric interaction for which the factorization surface is not known."],"url":"http://arxiv.org/abs/2404.10910v1","category":"quant-ph"}
{"created":"2024-04-16 21:09:33","title":"Strain-dependent Insulating State and Kondo Effect in Epitaxial SrIrO$_{3}$ Films","abstract":"The large spin-orbit coupling in iridium oxides plays a significant role in driving novel physical behaviors, including emergent phenomena in the films and heterostructures of perovskite and Ruddlesden-Popper iridates. In this work, we study the role of epitaxial strain on the electronic behavior of thin SrIrO$_3$ films. We find that compressive epitaxial strain leads to metallic transport behavior, but a slight tensile strain shows gapped behavior. Temperature-dependent resistivity measurements are used to examine different behaviors in films as a function of strain. We find Kondo contributions to the resistivity, with stronger effects in films that are thinner and under less compressive epitaxial strain. These results show the potential to tune SrIrO$_3$ into Kondo insulating states and open possibilities for a quantum critical point that can be controlled with strain in epitaxial films.","sentences":["The large spin-orbit coupling in iridium oxides plays a significant role in driving novel physical behaviors, including emergent phenomena in the films and heterostructures of perovskite and Ruddlesden-Popper iridates.","In this work, we study the role of epitaxial strain on the electronic behavior of thin SrIrO$_3$ films.","We find that compressive epitaxial strain leads to metallic transport behavior, but a slight tensile strain shows gapped behavior.","Temperature-dependent resistivity measurements are used to examine different behaviors in films as a function of strain.","We find Kondo contributions to the resistivity, with stronger effects in films that are thinner and under less compressive epitaxial strain.","These results show the potential to tune SrIrO$_3$ into Kondo insulating states and open possibilities for a quantum critical point that can be controlled with strain in epitaxial films."],"url":"http://arxiv.org/abs/2404.10909v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 20:40:32","title":"Allocation Mechanisms in Decentralized Exchange Markets with Frictions","abstract":"The classical theory of efficient allocations of an aggregate endowment in a pure-exchange economy has hitherto primarily focused on the Pareto-efficiency of allocations, under the implicit assumption that transfers between agents are frictionless, and hence costless to the economy. In this paper, we argue that certain transfers cause frictions that result in costs to the economy. We show that these frictional costs are tantamount to a form of subadditivity of the cost of transferring endowments between agents. We suggest an axiomatic study of allocation mechanisms, that is, the mechanisms that transform feasible allocations into other feasible allocations, in the presence of such transfer costs. Among other results, we provide an axiomatic characterization of those allocation mechanisms that admit representations as robust (worst-case) linear allocation mechanisms, as well as those mechanisms that admit representations as worst-case conditional expectations. We call the latter Robust Conditional Mean Allocation mechanisms, and we relate our results to the literature on (decentralized) risk sharing within a pool of agents.","sentences":["The classical theory of efficient allocations of an aggregate endowment in a pure-exchange economy has hitherto primarily focused on the Pareto-efficiency of allocations, under the implicit assumption that transfers between agents are frictionless, and hence costless to the economy.","In this paper, we argue that certain transfers cause frictions that result in costs to the economy.","We show that these frictional costs are tantamount to a form of subadditivity of the cost of transferring endowments between agents.","We suggest an axiomatic study of allocation mechanisms, that is, the mechanisms that transform feasible allocations into other feasible allocations, in the presence of such transfer costs.","Among other results, we provide an axiomatic characterization of those allocation mechanisms that admit representations as robust (worst-case) linear allocation mechanisms, as well as those mechanisms that admit representations as worst-case conditional expectations.","We call the latter Robust Conditional Mean Allocation mechanisms, and we relate our results to the literature on (decentralized) risk sharing within a pool of agents."],"url":"http://arxiv.org/abs/2404.10900v1","category":"cs.GT"}
{"created":"2024-04-16 20:40:15","title":"A variational neural Bayes framework for inference on intractable posterior distributions","abstract":"Classic Bayesian methods with complex models are frequently infeasible due to an intractable likelihood. Simulation-based inference methods, such as Approximate Bayesian Computing (ABC), calculate posteriors without accessing a likelihood function by leveraging the fact that data can be quickly simulated from the model, but converge slowly and/or poorly in high-dimensional settings. In this paper, we propose a framework for Bayesian posterior estimation by mapping data to posteriors of parameters using a neural network trained on data simulated from the complex model. Posterior distributions of model parameters are efficiently obtained by feeding observed data into the trained neural network. We show theoretically that our posteriors converge to the true posteriors in Kullback-Leibler divergence. Our approach yields computationally efficient and theoretically justified uncertainty quantification, which is lacking in existing simulation-based neural network approaches. Comprehensive simulation studies highlight our method's robustness and accuracy.","sentences":["Classic Bayesian methods with complex models are frequently infeasible due to an intractable likelihood.","Simulation-based inference methods, such as Approximate Bayesian Computing (ABC), calculate posteriors without accessing a likelihood function by leveraging the fact that data can be quickly simulated from the model, but converge slowly and/or poorly in high-dimensional settings.","In this paper, we propose a framework for Bayesian posterior estimation by mapping data to posteriors of parameters using a neural network trained on data simulated from the complex model.","Posterior distributions of model parameters are efficiently obtained by feeding observed data into the trained neural network.","We show theoretically that our posteriors converge to the true posteriors in Kullback-Leibler divergence.","Our approach yields computationally efficient and theoretically justified uncertainty quantification, which is lacking in existing simulation-based neural network approaches.","Comprehensive simulation studies highlight our method's robustness and accuracy."],"url":"http://arxiv.org/abs/2404.10899v1","category":"stat.CO"}
{"created":"2024-04-16 20:15:17","title":"Geometric Optimization of the First Robin Eigenvalue in Exterior Domains","abstract":"This paper addresses the geometric optimization problem of the first Robin eigenvalue in exterior domains, specifically the lowest point of the spectrum of the Laplace operator under Robin boundary conditions in the complement of a bounded domain. In contrast to the Laplace operator on bounded domains, the spectrum of this operator is not purely discrete. The discrete nature of the first eigenvalue depends on the parameter of the Robin boundary condition. In two dimensions, D. Krejcirik and V. Lotoreichik show that the ball maximizes the first Robin eigenvalue among all smooth, bounded, simply connected sets with given perimeter or given area, provided the eigenvalue is discrete.   We extend these findings to higher dimensions. The discrete spectrum of the Laplace operator under Robin boundary conditions can be characterized through the Steklov eigenvalue problem in exterior domains, a topic studied by G. Auchmuty and Q. Han. Assuming that the lowest point of the spectrum is a discrete eigenvalue, we show that the ball is a local maximizer among nearly spherical domains with prescribed measure. However, in general, the ball does not emerge as the global maximizer for the first Robin eigenvalue under either prescribed measure or prescribed perimeter.","sentences":["This paper addresses the geometric optimization problem of the first Robin eigenvalue in exterior domains, specifically the lowest point of the spectrum of the Laplace operator under Robin boundary conditions in the complement of a bounded domain.","In contrast to the Laplace operator on bounded domains, the spectrum of this operator is not purely discrete.","The discrete nature of the first eigenvalue depends on the parameter of the Robin boundary condition.","In two dimensions, D. Krejcirik and V. Lotoreichik show that the ball maximizes the first Robin eigenvalue among all smooth, bounded, simply connected sets with given perimeter or given area, provided the eigenvalue is discrete.   ","We extend these findings to higher dimensions.","The discrete spectrum of the Laplace operator under Robin boundary conditions can be characterized through the Steklov eigenvalue problem in exterior domains, a topic studied by G. Auchmuty and Q. Han.","Assuming that the lowest point of the spectrum is a discrete eigenvalue, we show that the ball is a local maximizer among nearly spherical domains with prescribed measure.","However, in general, the ball does not emerge as the global maximizer for the first Robin eigenvalue under either prescribed measure or prescribed perimeter."],"url":"http://arxiv.org/abs/2404.10886v1","category":"math.AP"}
{"created":"2024-04-16 20:12:43","title":"On retract varieties of algebras","abstract":"A retract variety is defined as a class of algebras closed under isomorphisms, retracts and products. Let a principal retract variety be generated by one algebra and a set-principal retract variety be generated by some set of algebras. It is shown that (a) not each set-principal retract variety is principal, and (b) not each retract variety is set-principal. A class of connected monounary algebras $\\mathcal{S}$ such that every retract variety of monounary algebras is generated by algebras that have all connected components from $\\mathcal{S}$ and at most two connected components are isomorphic is defined, this generating class is constructively described. All set-principal retract varieties of monounary algebras are characterized via degree function of monounary algebras.","sentences":["A retract variety is defined as a class of algebras closed under isomorphisms, retracts and products.","Let a principal retract variety be generated by one algebra and a set-principal retract variety be generated by some set of algebras.","It is shown that (a) not each set-principal retract variety is principal, and (b) not each retract variety is set-principal.","A class of connected monounary algebras $\\mathcal{S}$ such that every retract variety of monounary algebras is generated by algebras that have all connected components from $\\mathcal{S}$ and at most two connected components are isomorphic is defined, this generating class is constructively described.","All set-principal retract varieties of monounary algebras are characterized via degree function of monounary algebras."],"url":"http://arxiv.org/abs/2404.10885v1","category":"math.RA"}
{"created":"2024-04-16 20:01:10","title":"Differentially Private Optimization with Sparse Gradients","abstract":"Motivated by applications of large embedding models, we study differentially private (DP) optimization problems under sparsity of individual gradients. We start with new near-optimal bounds for the classic mean estimation problem but with sparse data, improving upon existing algorithms particularly for the high-dimensional regime. Building on this, we obtain pure- and approximate-DP algorithms with almost optimal rates for stochastic convex optimization with sparse gradients; the former represents the first nearly dimension-independent rates for this problem. Finally, we study the approximation of stationary points for the empirical loss in approximate-DP optimization and obtain rates that depend on sparsity instead of dimension, modulo polylogarithmic factors.","sentences":["Motivated by applications of large embedding models, we study differentially private (DP)","optimization problems under sparsity of individual gradients.","We start with new near-optimal bounds for the classic mean estimation problem but with sparse data, improving upon existing algorithms particularly for the high-dimensional regime.","Building on this, we obtain pure- and approximate-DP algorithms with almost optimal rates for stochastic convex optimization with sparse gradients; the former represents the first nearly dimension-independent rates for this problem.","Finally, we study the approximation of stationary points for the empirical loss in approximate-DP optimization and obtain rates that depend on sparsity instead of dimension, modulo polylogarithmic factors."],"url":"http://arxiv.org/abs/2404.10881v1","category":"cs.LG"}
{"created":"2024-04-16 19:52:57","title":"Course Recommender Systems Need to Consider the Job Market","abstract":"Current course recommender systems primarily leverage learner-course interactions, course content, learner preferences, and supplementary course details like instructor, institution, ratings, and reviews, to make their recommendation. However, these systems often overlook a critical aspect: the evolving skill demand of the job market. This paper focuses on the perspective of academic researchers, working in collaboration with the industry, aiming to develop a course recommender system that incorporates job market skill demands. In light of the job market's rapid changes and the current state of research in course recommender systems, we outline essential properties for course recommender systems to address these demands effectively, including explainable, sequential, unsupervised, and aligned with the job market and user's goals. Our discussion extends to the challenges and research questions this objective entails, including unsupervised skill extraction from job listings, course descriptions, and resumes, as well as predicting recommendations that align with learner objectives and the job market and designing metrics to evaluate this alignment. Furthermore, we introduce an initial system that addresses some existing limitations of course recommender systems using large Language Models (LLMs) for skill extraction and Reinforcement Learning (RL) for alignment with the job market. We provide empirical results using open-source data to demonstrate its effectiveness.","sentences":["Current course recommender systems primarily leverage learner-course interactions, course content, learner preferences, and supplementary course details like instructor, institution, ratings, and reviews, to make their recommendation.","However, these systems often overlook a critical aspect: the evolving skill demand of the job market.","This paper focuses on the perspective of academic researchers, working in collaboration with the industry, aiming to develop a course recommender system that incorporates job market skill demands.","In light of the job market's rapid changes and the current state of research in course recommender systems, we outline essential properties for course recommender systems to address these demands effectively, including explainable, sequential, unsupervised, and aligned with the job market and user's goals.","Our discussion extends to the challenges and research questions this objective entails, including unsupervised skill extraction from job listings, course descriptions, and resumes, as well as predicting recommendations that align with learner objectives and the job market and designing metrics to evaluate this alignment.","Furthermore, we introduce an initial system that addresses some existing limitations of course recommender systems using large Language Models (LLMs) for skill extraction and Reinforcement Learning (RL) for alignment with the job market.","We provide empirical results using open-source data to demonstrate its effectiveness."],"url":"http://arxiv.org/abs/2404.10876v1","category":"cs.IR"}
{"created":"2024-04-16 19:44:45","title":"Spectral Independence","abstract":"We prove the spectral gap property for random walks on the product of two non-locally isomorphic analytic real or p-adic compact groups with simple Lie algebras, under the necessary condition that the marginals posses a spectral gap. Furthermore, we give additional control on the spectral gap depending on certain specific properties of the given groups and marginals; in particular, we prove some new cases of the super-approximation conjecture. One ingredient of the proof is a local Ulam stability result which is introduced and proved in this paper. This result characterizes partially defined almost homomorphisms between two analytic compact groups with simple Lie algebras.","sentences":["We prove the spectral gap property for random walks on the product of two non-locally isomorphic analytic real or p-adic compact groups with simple Lie algebras, under the necessary condition that the marginals posses a spectral gap.","Furthermore, we give additional control on the spectral gap depending on certain specific properties of the given groups and marginals; in particular, we prove some new cases of the super-approximation conjecture.","One ingredient of the proof is a local Ulam stability result which is introduced and proved in this paper.","This result characterizes partially defined almost homomorphisms between two analytic compact groups with simple Lie algebras."],"url":"http://arxiv.org/abs/2404.10873v1","category":"math.GR"}
{"created":"2024-04-16 19:41:21","title":"Magnetic Monopole Phenomenology at Future Hadron Colliders","abstract":"In the grand tapestry of Physics, the magnetic monopole is a holy grail. Therefore, numerous efforts are underway in search of this hypothetical particle at CMS, ATLAS and MoEDAL experiments of LHC by employing different production mechanisms. The cornerstone of our comprehension of monopoles lies in Dirac's theory which outlines their characteristics and dynamics. Within this theoretical framework, an effective $U(1)$ gauge field theory, derived from conventional models, delineates the interaction between spin magnetically-charged fields and ordinary photons under electric-magnetic dualization.   The focus of this paper is the production of magnetic monopoles from Drell-Yan and the Photon-Fusion mechanisms to produce velocity-dependent scalar, fermionic, and vector monopoles of spin angular momentum $0,\\frac{1}{2},1$ respectively at LHC. A computational work is performed to compare the monopole pair-production cross-sections for both methods at different center-of-mass energies ($\\sqrt{s}$) with various magnetic dipole moments. The comparison of kinematic distributions of monopoles at Parton and reconstructed level are demonstrated including both DY and PF mechanisms.   Extracted results showcase how modern machine-learning techniques can be used to study the production of magnetic monopoles at the Future proton-proton Particle Colliders at 100 TeV. We demonstrate the observability of magnetic monopoles against the most relevant Standard Model background using multivariate methods such as Boosted Decision Trees (BDT), Likelihood, and Multilayer Perceptron (MLP). This study compares the performance of these classifiers with traditional cut-based and counting approaches, proving the superiority of our methods.","sentences":["In the grand tapestry of Physics, the magnetic monopole is a holy grail.","Therefore, numerous efforts are underway in search of this hypothetical particle at CMS, ATLAS and MoEDAL experiments of LHC by employing different production mechanisms.","The cornerstone of our comprehension of monopoles lies in Dirac's theory which outlines their characteristics and dynamics.","Within this theoretical framework, an effective $U(1)$ gauge field theory, derived from conventional models, delineates the interaction between spin magnetically-charged fields and ordinary photons under electric-magnetic dualization.   ","The focus of this paper is the production of magnetic monopoles from Drell-Yan and the Photon-Fusion mechanisms to produce velocity-dependent scalar, fermionic, and vector monopoles of spin angular momentum $0,\\frac{1}{2},1$ respectively at LHC.","A computational work is performed to compare the monopole pair-production cross-sections for both methods at different center-of-mass energies ($\\sqrt{s}$) with various magnetic dipole moments.","The comparison of kinematic distributions of monopoles at Parton and reconstructed level are demonstrated including both DY and PF mechanisms.   ","Extracted results showcase how modern machine-learning techniques can be used to study the production of magnetic monopoles at the Future proton-proton Particle Colliders at 100 TeV. We demonstrate the observability of magnetic monopoles against the most relevant Standard Model background using multivariate methods such as Boosted Decision Trees (BDT), Likelihood, and Multilayer Perceptron (MLP).","This study compares the performance of these classifiers with traditional cut-based and counting approaches, proving the superiority of our methods."],"url":"http://arxiv.org/abs/2404.10871v1","category":"hep-ph"}
{"created":"2024-04-17 16:28:16","title":"Constructing heat kernels on infinite graphs","abstract":"Let $G$ be an infinite, edge- and vertex-weighted graph with certain reasonable restrictions. We construct the heat kernel of the associated Laplacian using an adaptation of the parametrix approach due to Minakshisundaram-Pleijel in the setting of Riemannian geometry. This is partly motivated by the wish to relate the heat kernels of a graph and a subgraph, or of a domain and a discretization of it. As an application, assuming that the graph is locally finite, we express the heat kernel $H_G(x,y;t)$ as a Taylor series with the lead term being $a(x,y)t^r$, where $r$ is the combinatorial distance between $x$ and $y$ and $a(x,y)$ depends (explicitly) upon edge and vertex weights. In the case $G$ is the regular $(q+1)$-tree with $q\\geq 1$, our construction reproves different explicit formulas due to Chung-Yau and to Chinta-Jorgenson-Karlsson. Assuming uniform boundedness of the combinatorial vertex degree, we show that a dilated Gaussian depending on any distance metric on $G$, which is uniformly bounded from below can be taken as a parametrix in our construction. Our work extends in part the recent articles [LNY21, CJKS23] in that the graphs are infinite and weighted.","sentences":["Let $G$ be an infinite, edge- and vertex-weighted graph with certain reasonable restrictions.","We construct the heat kernel of the associated Laplacian using an adaptation of the parametrix approach due to Minakshisundaram-Pleijel in the setting of Riemannian geometry.","This is partly motivated by the wish to relate the heat kernels of a graph and a subgraph, or of a domain and a discretization of it.","As an application, assuming that the graph is locally finite, we express the heat kernel $H_G(x,y;t)$ as a Taylor series with the lead term being $a(x,y)t^r$, where $r$ is the combinatorial distance between $x$ and $y$ and $a(x,y)$ depends (explicitly) upon edge and vertex weights.","In the case $G$ is the regular $(q+1)$-tree with $q\\geq 1$, our construction reproves different explicit formulas due to Chung-Yau and to Chinta-Jorgenson-Karlsson.","Assuming uniform boundedness of the combinatorial vertex degree, we show that a dilated Gaussian depending on any distance metric on $G$, which is uniformly bounded from below can be taken as a parametrix in our construction.","Our work extends in part the recent articles","[LNY21, CJKS23] in that the graphs are infinite and weighted."],"url":"http://arxiv.org/abs/2404.11535v1","category":"math.AP"}
{"created":"2024-04-17 16:07:25","title":"Frameworking for a Community-led Feminist Ethics","abstract":"This paper introduces a relational perspective on ethics within the context of Feminist Digital Civics and community-led design. Ethics work in HCI has primarily focused on prescriptive machine ethics and bioethics principles rather than people. In response, we advocate for a community-led, processual approach to ethics, acknowledging power dynamics and local contexts. We thus propose a multidimensional adaptive model for ethics in HCI design, integrating an intersectional feminist ethical lens. This framework embraces feminist epistemologies, methods, and methodologies, fostering a reflexive practice. By weaving together situated knowledges, standpoint theory, intersectionality, participatory methods, and care ethics, our approach offers a holistic foundation for ethics in HCI, aiming to advance community-led practices and enrich the discourse surrounding ethics within this field.","sentences":["This paper introduces a relational perspective on ethics within the context of Feminist Digital Civics and community-led design.","Ethics work in HCI has primarily focused on prescriptive machine ethics and bioethics principles rather than people.","In response, we advocate for a community-led, processual approach to ethics, acknowledging power dynamics and local contexts.","We thus propose a multidimensional adaptive model for ethics in HCI design, integrating an intersectional feminist ethical lens.","This framework embraces feminist epistemologies, methods, and methodologies, fostering a reflexive practice.","By weaving together situated knowledges, standpoint theory, intersectionality, participatory methods, and care ethics, our approach offers a holistic foundation for ethics in HCI, aiming to advance community-led practices and enrich the discourse surrounding ethics within this field."],"url":"http://arxiv.org/abs/2404.11514v1","category":"cs.CY"}
{"created":"2024-04-17 15:26:55","title":"Assessing The Effectiveness Of Current Cybersecurity Regulations And Policies In The US","abstract":"This article assesses the effectiveness of current cybersecurity regulations and policies in the United States amidst the escalating frequency and sophistication of cyber threats. The focus is on the comprehensive framework established by the U.S. government, with a spotlight on the National Institute of Standards and Technology (NIST) Cybersecurity Framework and key regulations such as HIPAA, GLBA, FISMA, CISA, CCPA, and the DOD Cybersecurity Maturity Model Certification. The study evaluates the impact of these regulations on different sectors and analyzes trends in cybercrime data from 2000 to 2022. The findings highlight the challenges, successes, and the need for continuous adaptation in the face of evolving cyber threats","sentences":["This article assesses the effectiveness of current cybersecurity regulations and policies in the United States amidst the escalating frequency and sophistication of cyber threats.","The focus is on the comprehensive framework established by the U.S. government, with a spotlight on the National Institute of Standards and Technology (NIST) Cybersecurity Framework and key regulations such as HIPAA, GLBA, FISMA, CISA, CCPA, and the DOD Cybersecurity Maturity Model Certification.","The study evaluates the impact of these regulations on different sectors and analyzes trends in cybercrime data from 2000 to 2022.","The findings highlight the challenges, successes, and the need for continuous adaptation in the face of evolving cyber threats"],"url":"http://arxiv.org/abs/2404.11473v1","category":"cs.CR"}
{"created":"2024-04-17 14:43:05","title":"An adaptive linearized alternating direction multiplier method for solving convex optimization problems","abstract":"This thesis proposes an adaptive linearized alternating direction multiplier method to improve the convergence rate of the algorithm by using adaptive techniques to dynamically select the regular term coefficients. The innovation of this method is to utilize the information of the current iteration point to adaptively select the appropriate parameters, thus expanding the selection of the subproblem step size and improving the convergence rate of the algorithm while ensuring convergence.The advantage of this method is that it can improve the convergence rate of the algorithm as much as possible without compromising the convergence. This is very beneficial for the solution of optimization problems because the traditional linearized alternating direction multiplier method has a trade-off in the selection of the regular term coefficients: larger coefficients ensure convergence but tend to lead to small step sizes, while smaller coefficients allow for an increase in the iterative step size but tend to lead to the algorithm's non-convergence. This balance can be better handled by adaptively selecting the parameters, thus improving the efficiency of the algorithm.Overall, the method proposed in this thesis is of great importance in the field of matrix optimization and has a positive effect on improving the convergence speed and efficiency of the algorithm. It is hoped that this adaptive idea can bring new inspiration to the development of the field of matrix optimization and promote the research and application in related fields.","sentences":["This thesis proposes an adaptive linearized alternating direction multiplier method to improve the convergence rate of the algorithm by using adaptive techniques to dynamically select the regular term coefficients.","The innovation of this method is to utilize the information of the current iteration point to adaptively select the appropriate parameters, thus expanding the selection of the subproblem step size and improving the convergence rate of the algorithm while ensuring convergence.","The advantage of this method is that it can improve the convergence rate of the algorithm as much as possible without compromising the convergence.","This is very beneficial for the solution of optimization problems because the traditional linearized alternating direction multiplier method has a trade-off in the selection of the regular term coefficients: larger coefficients ensure convergence but tend to lead to small step sizes, while smaller coefficients allow for an increase in the iterative step size but tend to lead to the algorithm's non-convergence.","This balance can be better handled by adaptively selecting the parameters, thus improving the efficiency of the algorithm.","Overall, the method proposed in this thesis is of great importance in the field of matrix optimization and has a positive effect on improving the convergence speed and efficiency of the algorithm.","It is hoped that this adaptive idea can bring new inspiration to the development of the field of matrix optimization and promote the research and application in related fields."],"url":"http://arxiv.org/abs/2404.11435v1","category":"math.OC"}
{"created":"2024-04-17 14:20:59","title":"Directional Filter Design and Simulation for Superconducting On-chip Filter-banks","abstract":"Many superconducting on-chip filter-banks suffer from poor coupling to the detectors behind each filter. This is a problem intrinsic to the commonly used half wavelength filter, which has a maximum theoretical coupling of 50 %. In this paper we introduce a phase coherent filter, called a directional filter, which has a theoretical coupling of 100 %. In order to to study and compare different types of filter-banks, we first analyze the measured filter frequency scatter, losses, and spectral resolution of a DESHIMA 2.0 filter-bank chip. Based on measured fabrication tolerances and losses, we adapt the input parameters for our circuit simulations, quantitatively reproducing the measurements. We find that the frequency scatter is caused by nanometer-scale line-width variations and that variances in the spectral resolution is caused by losses in the dielectric only. Finally, we include these realistic parameters in a full filter-bank model and simulate a wide range of spectral resolutions and oversampling values. For all cases the directional filter-bank has significantly higher coupling to the detectors than the half-wave resonator filter-bank. The directional filter eliminates the need to use oversampling as a method to improve the total efficiency, instead capturing nearly all the power remaining after dielectric losses.","sentences":["Many superconducting on-chip filter-banks suffer from poor coupling to the detectors behind each filter.","This is a problem intrinsic to the commonly used half wavelength filter, which has a maximum theoretical coupling of 50 %.","In this paper we introduce a phase coherent filter, called a directional filter, which has a theoretical coupling of 100 %.","In order to to study and compare different types of filter-banks, we first analyze the measured filter frequency scatter, losses, and spectral resolution of a DESHIMA 2.0 filter-bank chip.","Based on measured fabrication tolerances and losses, we adapt the input parameters for our circuit simulations, quantitatively reproducing the measurements.","We find that the frequency scatter is caused by nanometer-scale line-width variations and that variances in the spectral resolution is caused by losses in the dielectric only.","Finally, we include these realistic parameters in a full filter-bank model and simulate a wide range of spectral resolutions and oversampling values.","For all cases the directional filter-bank has significantly higher coupling to the detectors than the half-wave resonator filter-bank.","The directional filter eliminates the need to use oversampling as a method to improve the total efficiency, instead capturing nearly all the power remaining after dielectric losses."],"url":"http://arxiv.org/abs/2404.11417v1","category":"astro-ph.IM"}
{"created":"2024-04-17 13:18:39","title":"Boosting Medical Image Segmentation Performance with Adaptive Convolution Layer","abstract":"Medical image segmentation plays a vital role in various clinical applications, enabling accurate delineation and analysis of anatomical structures or pathological regions. Traditional CNNs have achieved remarkable success in this field. However, they often rely on fixed kernel sizes, which can limit their performance and adaptability in medical images where features exhibit diverse scales and configurations due to variability in equipment, target sizes, and expert interpretations.   In this paper, we propose an adaptive layer placed ahead of leading deep-learning models such as UCTransNet, which dynamically adjusts the kernel size based on the local context of the input image.   By adaptively capturing and fusing features at multiple scales, our approach enhances the network's ability to handle diverse anatomical structures and subtle image details, even for recently performing architectures that internally implement intra-scale modules, such as UCTransnet.   Extensive experiments are conducted on   benchmark medical image datasets to evaluate the effectiveness of our proposal. It consistently outperforms traditional \\glspl{CNN} with fixed kernel sizes with a similar number of parameters, achieving superior segmentation Accuracy, Dice, and IoU in popular datasets such as SegPC2021 and ISIC2018. The model and data are published in the open-source repository, ensuring transparency and reproducibility of our promising results.","sentences":["Medical image segmentation plays a vital role in various clinical applications, enabling accurate delineation and analysis of anatomical structures or pathological regions.","Traditional CNNs have achieved remarkable success in this field.","However, they often rely on fixed kernel sizes, which can limit their performance and adaptability in medical images where features exhibit diverse scales and configurations due to variability in equipment, target sizes, and expert interpretations.   ","In this paper, we propose an adaptive layer placed ahead of leading deep-learning models such as UCTransNet, which dynamically adjusts the kernel size based on the local context of the input image.   ","By adaptively capturing and fusing features at multiple scales, our approach enhances the network's ability to handle diverse anatomical structures and subtle image details, even for recently performing architectures that internally implement intra-scale modules, such as UCTransnet.   ","Extensive experiments are conducted on   benchmark medical image datasets to evaluate the effectiveness of our proposal.","It consistently outperforms traditional \\glspl{CNN} with fixed kernel sizes with a similar number of parameters, achieving superior segmentation Accuracy, Dice, and IoU in popular datasets such as SegPC2021 and ISIC2018.","The model and data are published in the open-source repository, ensuring transparency and reproducibility of our promising results."],"url":"http://arxiv.org/abs/2404.11361v1","category":"eess.IV"}
{"created":"2024-04-17 13:07:04","title":"Isotropic maps and moment map flow","abstract":"We consider the moduli space of isotropic maps from a closed surface $\\Sigma$ to a symplectic affine space and construct a K\\\"ahler moment map geometry, on a space of differential forms on $\\Sigma$, such that the isotropic maps correspond to certain zeroes of the moment map. The moment map geometry induces a modified moment map flow, whose fixed point set correspond to isotropic maps. This construction can be adapted to the polyhedral setting. In particular, we prove that the polyhedral modified moment map flow induces a strong deformation retraction from the space of polyhedral maps onto the space of polyhedral isotropic maps.","sentences":["We consider the moduli space of isotropic maps from a closed surface $\\Sigma$ to a symplectic affine space and construct a K\\\"ahler moment map geometry, on a space of differential forms on $\\Sigma$, such that the isotropic maps correspond to certain zeroes of the moment map.","The moment map geometry induces a modified moment map flow, whose fixed point set correspond to isotropic maps.","This construction can be adapted to the polyhedral setting.","In particular, we prove that the polyhedral modified moment map flow induces a strong deformation retraction from the space of polyhedral maps onto the space of polyhedral isotropic maps."],"url":"http://arxiv.org/abs/2404.11347v1","category":"math.DG"}
{"created":"2024-04-17 13:01:39","title":"A numerical approach to levitated superconductors and its application to a superconducting cylinder in a quadrupole field","abstract":"Magnetically levitated superconductors in the Meissner state can be utilized as micro-mechanical oscillators with large mass, high quality factors and long coherence times. In previous works analytical solutions for the magnetic field distribution around a superconducting sphere in a quadrupole field have been found and used to derive the trap parameters, while non-spherical geometries have only been investigated in a few idealized cases. However, superconductors of almost arbitrary shape can be used as levitators in a magnetic trap and, as the trap's properties depend strongly on the superconductors shape, allow for a wider parameter regime to be accessed. Finite element models are suitable to obtain the field distribution around arbitrarily shaped superconductors in arbitrary fields, but have not yet been used widely in the context of levitated superconductors. Here we present a simple numerical model for this purpose and use it to calculate the field distribution around cylindrical superconductors in a quadrupole field and to evaluate the trap parameters. We find that the cylindrical shape, compared to spherical levitators, allows for substantially higher trap frequencies and coupling strengths. This in turn reduces the demands on vibration isolation and significantly eases the requirements for feedback cooling to the ground state. The numerical model is provided as supplemental material and can easily be adapted to various geometries and trap fields.","sentences":["Magnetically levitated superconductors in the Meissner state can be utilized as micro-mechanical oscillators with large mass, high quality factors and long coherence times.","In previous works analytical solutions for the magnetic field distribution around a superconducting sphere in a quadrupole field have been found and used to derive the trap parameters, while non-spherical geometries have only been investigated in a few idealized cases.","However, superconductors of almost arbitrary shape can be used as levitators in a magnetic trap and, as the trap's properties depend strongly on the superconductors shape, allow for a wider parameter regime to be accessed.","Finite element models are suitable to obtain the field distribution around arbitrarily shaped superconductors in arbitrary fields, but have not yet been used widely in the context of levitated superconductors.","Here we present a simple numerical model for this purpose and use it to calculate the field distribution around cylindrical superconductors in a quadrupole field and to evaluate the trap parameters.","We find that the cylindrical shape, compared to spherical levitators, allows for substantially higher trap frequencies and coupling strengths.","This in turn reduces the demands on vibration isolation and significantly eases the requirements for feedback cooling to the ground state.","The numerical model is provided as supplemental material and can easily be adapted to various geometries and trap fields."],"url":"http://arxiv.org/abs/2404.11342v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 11:55:45","title":"Closely Interactive Human Reconstruction with Proxemics and Physics-Guided Adaption","abstract":"Existing multi-person human reconstruction approaches mainly focus on recovering accurate poses or avoiding penetration, but overlook the modeling of close interactions. In this work, we tackle the task of reconstructing closely interactive humans from a monocular video. The main challenge of this task comes from insufficient visual information caused by depth ambiguity and severe inter-person occlusion. In view of this, we propose to leverage knowledge from proxemic behavior and physics to compensate the lack of visual information. This is based on the observation that human interaction has specific patterns following the social proxemics. Specifically, we first design a latent representation based on Vector Quantised-Variational AutoEncoder (VQ-VAE) to model human interaction. A proxemics and physics guided diffusion model is then introduced to denoise the initial distribution. We design the diffusion model as dual branch with each branch representing one individual such that the interaction can be modeled via cross attention. With the learned priors of VQ-VAE and physical constraint as the additional information, our proposed approach is capable of estimating accurate poses that are also proxemics and physics plausible. Experimental results on Hi4D, 3DPW, and CHI3D demonstrate that our method outperforms existing approaches. The code is available at \\url{https://github.com/boycehbz/HumanInteraction}.","sentences":["Existing multi-person human reconstruction approaches mainly focus on recovering accurate poses or avoiding penetration, but overlook the modeling of close interactions.","In this work, we tackle the task of reconstructing closely interactive humans from a monocular video.","The main challenge of this task comes from insufficient visual information caused by depth ambiguity and severe inter-person occlusion.","In view of this, we propose to leverage knowledge from proxemic behavior and physics to compensate the lack of visual information.","This is based on the observation that human interaction has specific patterns following the social proxemics.","Specifically, we first design a latent representation based on Vector Quantised-Variational AutoEncoder (VQ-VAE) to model human interaction.","A proxemics and physics guided diffusion model is then introduced to denoise the initial distribution.","We design the diffusion model as dual branch with each branch representing one individual such that the interaction can be modeled via cross attention.","With the learned priors of VQ-VAE and physical constraint as the additional information, our proposed approach is capable of estimating accurate poses that are also proxemics and physics plausible.","Experimental results on Hi4D, 3DPW, and CHI3D demonstrate that our method outperforms existing approaches.","The code is available at \\url{https://github.com/boycehbz/HumanInteraction}."],"url":"http://arxiv.org/abs/2404.11291v1","category":"cs.CV"}
{"created":"2024-04-17 07:39:14","title":"[DC] bRight XR: How to train designers to keep on the bright side?","abstract":"This research project aims to promote ethical principles among designers engaged in adaptive-XR by providing tools for self-assessment. We introduce a Design-Based Research (DBR) methodology to build bRight-XR, a framework including a heuristic evaluation matrix and based on learning theory.","sentences":["This research project aims to promote ethical principles among designers engaged in adaptive-XR by providing tools for self-assessment.","We introduce a Design-Based Research (DBR) methodology to build bRight-XR, a framework including a heuristic evaluation matrix and based on learning theory."],"url":"http://arxiv.org/abs/2404.11142v1","category":"cs.ET"}
{"created":"2024-04-17 07:13:49","title":"Rational curves on complete intersections and the circle method","abstract":"We study the geometry of the space of rational curves on smooth complete intersections of low degree, which pass through a given set of points on the variety. The argument uses spreading out to a finite field, together with an adaptation to function fields of positive characteristic of work by Rydin Myerson on the circle method. Our work also allows us to handle weak approximation for such varieties.","sentences":["We study the geometry of the space of rational curves on smooth complete intersections of low degree, which pass through a given set of points on the variety.","The argument uses spreading out to a finite field, together with an adaptation to function fields of positive characteristic of work by Rydin Myerson on the circle method.","Our work also allows us to handle weak approximation for such varieties."],"url":"http://arxiv.org/abs/2404.11123v1","category":"math.AG"}
{"created":"2024-04-17 06:57:57","title":"CorrNet+: Sign Language Recognition and Translation via Spatial-Temporal Correlation","abstract":"In sign language, the conveyance of human body trajectories predominantly relies upon the coordinated movements of hands and facial expressions across successive frames. Despite the recent advancements of sign language understanding methods, they often solely focus on individual frames, inevitably overlooking the inter-frame correlations that are essential for effectively modeling human body trajectories. To address this limitation, this paper introduces a spatial-temporal correlation network, denoted as CorrNet+, which explicitly identifies body trajectories across multiple frames. In specific, CorrNet+ employs a correlation module and an identification module to build human body trajectories. Afterwards, a temporal attention module is followed to adaptively evaluate the contributions of different frames. The resultant features offer a holistic perspective on human body movements, facilitating a deeper understanding of sign language. As a unified model, CorrNet+ achieves new state-of-the-art performance on two extensive sign language understanding tasks, including continuous sign language recognition (CSLR) and sign language translation (SLT). Especially, CorrNet+ surpasses previous methods equipped with resource-intensive pose-estimation networks or pre-extracted heatmaps for hand and facial feature extraction. Compared with CorrNet, CorrNet+ achieves a significant performance boost across all benchmarks while halving the computational overhead. A comprehensive comparison with previous spatial-temporal reasoning methods verifies the superiority of CorrNet+. Code is available at https://github.com/hulianyuyy/CorrNet_Plus.","sentences":["In sign language, the conveyance of human body trajectories predominantly relies upon the coordinated movements of hands and facial expressions across successive frames.","Despite the recent advancements of sign language understanding methods, they often solely focus on individual frames, inevitably overlooking the inter-frame correlations that are essential for effectively modeling human body trajectories.","To address this limitation, this paper introduces a spatial-temporal correlation network, denoted as CorrNet+, which explicitly identifies body trajectories across multiple frames.","In specific, CorrNet+ employs a correlation module and an identification module to build human body trajectories.","Afterwards, a temporal attention module is followed to adaptively evaluate the contributions of different frames.","The resultant features offer a holistic perspective on human body movements, facilitating a deeper understanding of sign language.","As a unified model, CorrNet+ achieves new state-of-the-art performance on two extensive sign language understanding tasks, including continuous sign language recognition (CSLR) and sign language translation (SLT).","Especially, CorrNet+ surpasses previous methods equipped with resource-intensive pose-estimation networks or pre-extracted heatmaps for hand and facial feature extraction.","Compared with CorrNet, CorrNet+ achieves a significant performance boost across all benchmarks while halving the computational overhead.","A comprehensive comparison with previous spatial-temporal reasoning methods verifies the superiority of CorrNet+.","Code is available at https://github.com/hulianyuyy/CorrNet_Plus."],"url":"http://arxiv.org/abs/2404.11111v1","category":"cs.CV"}
{"created":"2024-04-17 03:25:54","title":"Cross-Platform Hate Speech Detection with Weakly Supervised Causal Disentanglement","abstract":"Content moderation faces a challenging task as social media's ability to spread hate speech contrasts with its role in promoting global connectivity. With rapidly evolving slang and hate speech, the adaptability of conventional deep learning to the fluid landscape of online dialogue remains limited. In response, causality inspired disentanglement has shown promise by segregating platform specific peculiarities from universal hate indicators. However, its dependency on available ground truth target labels for discerning these nuances faces practical hurdles with the incessant evolution of platforms and the mutable nature of hate speech. Using confidence based reweighting and contrastive regularization, this study presents HATE WATCH, a novel framework of weakly supervised causal disentanglement that circumvents the need for explicit target labeling and effectively disentangles input features into invariant representations of hate. Empirical validation across platforms two with target labels and two without positions HATE WATCH as a novel method in cross platform hate speech detection with superior performance. HATE WATCH advances scalable content moderation techniques towards developing safer online communities.","sentences":["Content moderation faces a challenging task as social media's ability to spread hate speech contrasts with its role in promoting global connectivity.","With rapidly evolving slang and hate speech, the adaptability of conventional deep learning to the fluid landscape of online dialogue remains limited.","In response, causality inspired disentanglement has shown promise by segregating platform specific peculiarities from universal hate indicators.","However, its dependency on available ground truth target labels for discerning these nuances faces practical hurdles with the incessant evolution of platforms and the mutable nature of hate speech.","Using confidence based reweighting and contrastive regularization, this study presents HATE WATCH, a novel framework of weakly supervised causal disentanglement that circumvents the need for explicit target labeling and effectively disentangles input features into invariant representations of hate.","Empirical validation across platforms two with target labels and two without positions HATE WATCH as a novel method in cross platform hate speech detection with superior performance.","HATE WATCH advances scalable content moderation techniques towards developing safer online communities."],"url":"http://arxiv.org/abs/2404.11036v1","category":"cs.LG"}
{"created":"2024-04-17 02:57:31","title":"The Radius Distribution of M dwarf-hosted Planets and its Evolution","abstract":"M dwarf stars are not only the most promising hosts for detection and characterization of small and potentially habitable planets, they provide leverage relative to solar-type stars to test models of planet formation and evolution. Using Gaia astrometry, adaptive optics imaging, and calibrated gyrochronologic relations to estimate stellar properties, filter binaries, and assign ages, we refined the radii of 179 transiting planets orbiting 119 single late K- and early M-type stars detected by the Kepler mission, and assigned stellar rotation-based ages ) to 115 of these. We constructed the radius distribution of <4R$_{\\oplus}$ planets and assessed its evolution with time. As for solar-type stars, the inferred distribution contains distinct populations of \"super-Earths\" (at ~1.3R$_{\\oplus}$) and \"sub-Neptunes\" (at ~2.2Rearth) separated by a gap or \"valley\" at $\\approx$1.7R$_{\\oplus}$ that has a period dependence that is significantly weaker (power law index of -0.026$^{+0.026}_{-0.017}$) than for solar-type stars. Sub-Neptunes are largely absent at short periods ($<$2 days) and high irradiance, a feature analogous to the \"Neptune desert\" observed around solar-type stars. The relative number of sub-Neptunes to super-Earths declines between the younger and older halves of the sample (median age 3.8 Gyr), although the formal significance is low ($p = 0.06$) because of the small sample size. The decline in sub-Neptunes appears to be more pronounced at long orbital periods vs. short periods; this is not due to detection bias and could indicate that these objects are inflated by a mechanism that operates at elevated irradiance, e.g. a runaway water greenhouse augmented by H/He.","sentences":["M dwarf stars are not only the most promising hosts for detection and characterization of small and potentially habitable planets, they provide leverage relative to solar-type stars to test models of planet formation and evolution.","Using Gaia astrometry, adaptive optics imaging, and calibrated gyrochronologic relations to estimate stellar properties, filter binaries, and assign ages, we refined the radii of 179 transiting planets orbiting 119 single late K- and early M-type stars detected by the Kepler mission, and assigned stellar rotation-based ages ) to 115 of these.","We constructed the radius distribution of <4R$_{\\oplus}$ planets and assessed its evolution with time.","As for solar-type stars, the inferred distribution contains distinct populations of \"super-Earths\" (at ~1.3R$_{\\oplus}$) and \"sub-Neptunes\" (at ~2.2Rearth) separated by a gap or \"valley\" at $\\approx$1.7R$_{\\oplus}$ that has a period dependence that is significantly weaker (power law index of -0.026$^{+0.026}_{-0.017}$) than for solar-type stars.","Sub-Neptunes are largely absent at short periods ($<$2 days) and high irradiance, a feature analogous to the \"Neptune desert\" observed around solar-type stars.","The relative number of sub-Neptunes to super-Earths declines between the younger and older halves of the sample (median age 3.8 Gyr), although the formal significance is low ($p = 0.06$) because of the small sample size.","The decline in sub-Neptunes appears to be more pronounced at long orbital periods vs. short periods; this is not due to detection bias and could indicate that these objects are inflated by a mechanism that operates at elevated irradiance, e.g. a runaway water greenhouse augmented by H/He."],"url":"http://arxiv.org/abs/2404.11022v1","category":"astro-ph.EP"}
{"created":"2024-04-17 02:44:25","title":"Control Theoretic Approach to Fine-Tuning and Transfer Learning","abstract":"Given a training set in the form of a paired $(\\mathcal{X},\\mathcal{Y})$, we say that the control system $\\dot{x} = f(x,u)$ has learned the paired set via the control $u^*$ if the system steers each point of $\\mathcal{X}$ to its corresponding target in $\\mathcal{Y}$. Most existing methods for finding a control function $u^*$ require learning of a new control function if the training set is updated. To overcome this limitation, we introduce the concept of $\\textit{tuning without forgetting}$. We develop $\\textit{an iterative algorithm}$ to tune the control function $u^*$ when the training set expands, whereby points already in the paired set are still matched, and new training samples are learned. More specifically, at each update of our method, the control $u^*$ is projected onto the kernel of the end-point mapping generated by the controlled dynamics at the learned samples. It ensures keeping the end points for the previously learned samples constant while iteratively learning additional samples. Our work contributes to the scalability of control methods, offering a novel approach to adaptively handle training set expansions.","sentences":["Given a training set in the form of a paired $(\\mathcal{X},\\mathcal{Y})$, we say that the control system $\\dot{x} = f(x,u)$ has learned the paired set via the control $u^*$ if the system steers each point of $\\mathcal{X}$ to its corresponding target in $\\mathcal{Y}$. Most existing methods for finding a control function $u^*$ require learning of a new control function if the training set is updated.","To overcome this limitation, we introduce the concept of $\\textit{tuning without forgetting}$. We develop $\\textit{an iterative algorithm}$ to tune the control function $u^*$ when the training set expands, whereby points already in the paired set are still matched, and new training samples are learned.","More specifically, at each update of our method, the control $u^*$ is projected onto the kernel of the end-point mapping generated by the controlled dynamics at the learned samples.","It ensures keeping the end points for the previously learned samples constant while iteratively learning additional samples.","Our work contributes to the scalability of control methods, offering a novel approach to adaptively handle training set expansions."],"url":"http://arxiv.org/abs/2404.11013v1","category":"cs.LG"}
{"created":"2024-04-17 00:21:36","title":"Domain-Specific Block Selection and Paired-View Pseudo-Labeling for Online Test-Time Adaptation","abstract":"Test-time adaptation (TTA) aims to adapt a pre-trained model to a new test domain without access to source data after deployment. Existing approaches typically rely on self-training with pseudo-labels since ground-truth cannot be obtained from test data. Although the quality of pseudo labels is important for stable and accurate long-term adaptation, it has not been previously addressed. In this work, we propose DPLOT, a simple yet effective TTA framework that consists of two components: (1) domain-specific block selection and (2) pseudo-label generation using paired-view images. Specifically, we select blocks that involve domain-specific feature extraction and train these blocks by entropy minimization. After blocks are adjusted for current test domain, we generate pseudo-labels by averaging given test images and corresponding flipped counterparts. By simply using flip augmentation, we prevent a decrease in the quality of the pseudo-labels, which can be caused by the domain gap resulting from strong augmentation. Our experimental results demonstrate that DPLOT outperforms previous TTA methods in CIFAR10-C, CIFAR100-C, and ImageNet-C benchmarks, reducing error by up to 5.4%, 9.1%, and 2.9%, respectively. Also, we provide an extensive analysis to demonstrate effectiveness of our framework. Code is available at https://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA.","sentences":["Test-time adaptation (TTA) aims to adapt a pre-trained model to a new test domain without access to source data after deployment.","Existing approaches typically rely on self-training with pseudo-labels since ground-truth cannot be obtained from test data.","Although the quality of pseudo labels is important for stable and accurate long-term adaptation, it has not been previously addressed.","In this work, we propose DPLOT, a simple yet effective TTA framework that consists of two components: (1) domain-specific block selection and (2) pseudo-label generation using paired-view images.","Specifically, we select blocks that involve domain-specific feature extraction and train these blocks by entropy minimization.","After blocks are adjusted for current test domain, we generate pseudo-labels by averaging given test images and corresponding flipped counterparts.","By simply using flip augmentation, we prevent a decrease in the quality of the pseudo-labels, which can be caused by the domain gap resulting from strong augmentation.","Our experimental results demonstrate that DPLOT outperforms previous TTA methods in CIFAR10-C, CIFAR100-C, and ImageNet-C benchmarks, reducing error by up to 5.4%, 9.1%, and 2.9%, respectively.","Also, we provide an extensive analysis to demonstrate effectiveness of our framework.","Code is available at https://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA."],"url":"http://arxiv.org/abs/2404.10966v1","category":"cs.CV"}
{"created":"2024-04-17 00:18:48","title":"IMIL: Interactive Medical Image Learning Framework","abstract":"Data augmentations are widely used in training medical image deep learning models to increase the diversity and size of sparse datasets. However, commonly used augmentation techniques can result in loss of clinically relevant information from medical images, leading to incorrect predictions at inference time. We propose the Interactive Medical Image Learning (IMIL) framework, a novel approach for improving the training of medical image analysis algorithms that enables clinician-guided intermediate training data augmentations on misprediction outliers, focusing the algorithm on relevant visual information. To prevent the model from using irrelevant features during training, IMIL will 'blackout' clinician-designated irrelevant regions and replace the original images with the augmented samples. This ensures that for originally mispredicted samples, the algorithm subsequently attends only to relevant regions and correctly correlates them with the respective diagnosis. We validate the efficacy of IMIL using radiology residents and compare its performance to state-of-the-art data augmentations. A 4.2% improvement in accuracy over ResNet-50 was observed when using IMIL on only 4% of the training set. Our study demonstrates the utility of clinician-guided interactive training to achieve meaningful data augmentations for medical image analysis algorithms.","sentences":["Data augmentations are widely used in training medical image deep learning models to increase the diversity and size of sparse datasets.","However, commonly used augmentation techniques can result in loss of clinically relevant information from medical images, leading to incorrect predictions at inference time.","We propose the Interactive Medical Image Learning (IMIL) framework, a novel approach for improving the training of medical image analysis algorithms that enables clinician-guided intermediate training data augmentations on misprediction outliers, focusing the algorithm on relevant visual information.","To prevent the model from using irrelevant features during training, IMIL will 'blackout' clinician-designated irrelevant regions and replace the original images with the augmented samples.","This ensures that for originally mispredicted samples, the algorithm subsequently attends only to relevant regions and correctly correlates them with the respective diagnosis.","We validate the efficacy of IMIL using radiology residents and compare its performance to state-of-the-art data augmentations.","A 4.2% improvement in accuracy over ResNet-50 was observed when using IMIL on only 4% of the training set.","Our study demonstrates the utility of clinician-guided interactive training to achieve meaningful data augmentations for medical image analysis algorithms."],"url":"http://arxiv.org/abs/2404.10965v1","category":"eess.IV"}
{"created":"2024-04-16 23:29:23","title":"Limit points of $A_\u03b1$-matrices of graphs","abstract":"We study limit points of the spectral radii of $A_{\\alpha}$-matrices of graphs. Adapting a method used by J. B. Shearer in 1989, we prove a density property of $A_{\\alpha}$-limit points of caterpillars for $\\alpha$ close to zero. Precisely, we show that for $\\alpha \\in [0, 1/2)$ there exists a positive number $\\tau_2(\\alpha)>2$ such that any value $\\lambda> \\tau_2(\\alpha)$ is an $A_{\\alpha}$-limit point. We also determine the existence of other intervals for which all its points are $A_{\\alpha}$-limit points.","sentences":["We study limit points of the spectral radii of $A_{\\alpha}$-matrices of graphs.","Adapting a method used by J. B. Shearer in 1989, we prove a density property of $A_{\\alpha}$-limit points of caterpillars for $\\alpha$ close to zero.","Precisely, we show that for $\\alpha \\in [0, 1/2)$ there exists a positive number $\\tau_2(\\alpha)>2$ such that any value $\\lambda> \\tau_2(\\alpha)$ is an $A_{\\alpha}$-limit point.","We also determine the existence of other intervals for which all its points are $A_{\\alpha}$-limit points."],"url":"http://arxiv.org/abs/2404.10953v1","category":"math.CO"}
{"created":"2024-04-16 21:20:15","title":"Adaptive Kalman Filtering Developed from Recursive Least Squares Forgetting Algorithms","abstract":"Recursive least squares (RLS) is derived as the recursive minimizer of the least-squares cost function. Moreover, it is well known that RLS is a special case of the Kalman filter. This work presents the Kalman filter least squares (KFLS) cost function, whose recursive minimizer gives the Kalman filter. KFLS is an extension of generalized forgetting recursive least squares (GF-RLS), a general framework which contains various extensions of RLS from the literature as special cases. This then implies that extensions of RLS are also special cases of the Kalman filter. Motivated by this connection, we propose an algorithm that combines extensions of RLS with the Kalman filter, resulting in a new class of adaptive Kalman filters. A numerical example shows that one such adaptive Kalman filter provides improved state estimation for a mass-spring-damper with intermittent, unmodeled collisions. This example suggests that such adaptive Kalman filtering may provide potential benefits for systems with non-classical disturbances.","sentences":["Recursive least squares (RLS) is derived as the recursive minimizer of the least-squares cost function.","Moreover, it is well known that RLS is a special case of the Kalman filter.","This work presents the Kalman filter least squares (KFLS) cost function, whose recursive minimizer gives the Kalman filter.","KFLS is an extension of generalized forgetting recursive least squares (GF-RLS), a general framework which contains various extensions of RLS from the literature as special cases.","This then implies that extensions of RLS are also special cases of the Kalman filter.","Motivated by this connection, we propose an algorithm that combines extensions of RLS with the Kalman filter, resulting in a new class of adaptive Kalman filters.","A numerical example shows that one such adaptive Kalman filter provides improved state estimation for a mass-spring-damper with intermittent, unmodeled collisions.","This example suggests that such adaptive Kalman filtering may provide potential benefits for systems with non-classical disturbances."],"url":"http://arxiv.org/abs/2404.10914v1","category":"eess.SY"}
{"created":"2024-04-16 21:17:59","title":"Constructing $\\mathrm{NP}^{\\mathord{\\#}\\mathrm P}$-complete problems and ${\\mathord{\\#}\\mathrm P}$-hardness of circuit extraction in phase-free ZH","abstract":"The ZH calculus is a graphical language for quantum computation reasoning. The phase-free variant offers a simple set of generators that guarantee universality. ZH calculus is effective in MBQC and analysis of quantum circuits constructed with the universal gate set Toffoli+H. While circuits naturally translate to ZH diagrams, finding an ancilla-free circuit equivalent to a given diagram is hard. Here, we show that circuit extraction for phase-free ZH calculus is ${\\mathord{\\#}\\mathrm P}$-hard, extending the existing result for ZX calculus. Another problem believed to be hard is comparing whether two diagrams represent the same process. We show that two closely related problems are $\\mathrm{NP}^{\\mathord{\\#}\\mathrm P}$-complete. The first problem is: given two processes represented as diagrams, determine the existence of a computational basis state on which they equalize. The second problem is checking whether the matrix representation of a given diagram contains an entry equal to a given number. Our proof adapts the proof of Cook-Levin theorem to a reduction from a non-deterministic Turing Machine with access to ${\\mathord{\\#}\\mathrm P}$ oracle.","sentences":["The ZH calculus is a graphical language for quantum computation reasoning.","The phase-free variant offers a simple set of generators that guarantee universality.","ZH calculus is effective in MBQC and analysis of quantum circuits constructed with the universal gate set Toffoli+H. While circuits naturally translate to ZH diagrams, finding an ancilla-free circuit equivalent to a given diagram is hard.","Here, we show that circuit extraction for phase-free ZH calculus is ${\\mathord{\\#}\\mathrm P}$-hard, extending the existing result for ZX calculus.","Another problem believed to be hard is comparing whether two diagrams represent the same process.","We show that two closely related problems are $\\mathrm{NP}^{\\mathord{\\#}\\mathrm P}$-complete.","The first problem is: given two processes represented as diagrams, determine the existence of a computational basis state on which they equalize.","The second problem is checking whether the matrix representation of a given diagram contains an entry equal to a given number.","Our proof adapts the proof of Cook-Levin theorem to a reduction from a non-deterministic Turing Machine with access to ${\\mathord{\\#}\\mathrm P}$ oracle."],"url":"http://arxiv.org/abs/2404.10913v1","category":"quant-ph"}
{"created":"2024-04-16 20:37:14","title":"Semantics-Aware Attention Guidance for Diagnosing Whole Slide Images","abstract":"Accurate cancer diagnosis remains a critical challenge in digital pathology, largely due to the gigapixel size and complex spatial relationships present in whole slide images. Traditional multiple instance learning (MIL) methods often struggle with these intricacies, especially in preserving the necessary context for accurate diagnosis. In response, we introduce a novel framework named Semantics-Aware Attention Guidance (SAG), which includes 1) a technique for converting diagnostically relevant entities into attention signals, and 2) a flexible attention loss that efficiently integrates various semantically significant information, such as tissue anatomy and cancerous regions. Our experiments on two distinct cancer datasets demonstrate consistent improvements in accuracy, precision, and recall with two state-of-the-art baseline models. Qualitative analysis further reveals that the incorporation of heuristic guidance enables the model to focus on regions critical for diagnosis. SAG is not only effective for the models discussed here, but its adaptability extends to any attention-based diagnostic model. This opens up exciting possibilities for further improving the accuracy and efficiency of cancer diagnostics.","sentences":["Accurate cancer diagnosis remains a critical challenge in digital pathology, largely due to the gigapixel size and complex spatial relationships present in whole slide images.","Traditional multiple instance learning (MIL) methods often struggle with these intricacies, especially in preserving the necessary context for accurate diagnosis.","In response, we introduce a novel framework named Semantics-Aware Attention Guidance (SAG), which includes 1) a technique for converting diagnostically relevant entities into attention signals, and 2) a flexible attention loss that efficiently integrates various semantically significant information, such as tissue anatomy and cancerous regions.","Our experiments on two distinct cancer datasets demonstrate consistent improvements in accuracy, precision, and recall with two state-of-the-art baseline models.","Qualitative analysis further reveals that the incorporation of heuristic guidance enables the model to focus on regions critical for diagnosis.","SAG is not only effective for the models discussed here, but its adaptability extends to any attention-based diagnostic model.","This opens up exciting possibilities for further improving the accuracy and efficiency of cancer diagnostics."],"url":"http://arxiv.org/abs/2404.10894v1","category":"cs.CV"}
{"created":"2024-04-16 19:24:37","title":"Numerical methods and improvements for simulating quasi-static elastoplastic materials","abstract":"Hypo-elastoplasticity is a framework suitable for modeling the mechanics of many hard materials that have small elastic deformation and large plastic deformation. In most laboratory tests for these materials the Cauchy stress is in quasi-static equilibrium. Rycroft et al. discovered a mathematical correspondence between this physical system and the incompressible Navier-Stokes equations, and developed a projection method similar to Chorin's projection method (1968) for incompressible Newtonian fluids. Here, we improve the original projection method to simulate quasi-static hypo-elastoplasticity, by making three improvements. First, drawing inspiration from the second-order projection method for incompressible Newtonian fluids, we formulate a second-order in time numerical scheme for quasi-static hypo-elastoplasticity. Second, we implement a finite element method for solving the elliptic equations in the projection step, which provides both numerical benefits and flexibility. Third, we develop an adaptive global time-stepping scheme, which can compute accurate solutions in fewer timesteps. Our numerical tests use an example physical model of a bulk metallic glass based on the shear transformation zone theory, but the numerical methods can be applied to any elastoplastic material.","sentences":["Hypo-elastoplasticity is a framework suitable for modeling the mechanics of many hard materials that have small elastic deformation and large plastic deformation.","In most laboratory tests for these materials the Cauchy stress is in quasi-static equilibrium.","Rycroft et al. discovered a mathematical correspondence between this physical system and the incompressible Navier-Stokes equations, and developed a projection method similar to Chorin's projection method (1968) for incompressible Newtonian fluids.","Here, we improve the original projection method to simulate quasi-static hypo-elastoplasticity, by making three improvements.","First, drawing inspiration from the second-order projection method for incompressible Newtonian fluids, we formulate a second-order in time numerical scheme for quasi-static hypo-elastoplasticity.","Second, we implement a finite element method for solving the elliptic equations in the projection step, which provides both numerical benefits and flexibility.","Third, we develop an adaptive global time-stepping scheme, which can compute accurate solutions in fewer timesteps.","Our numerical tests use an example physical model of a bulk metallic glass based on the shear transformation zone theory, but the numerical methods can be applied to any elastoplastic material."],"url":"http://arxiv.org/abs/2404.10863v1","category":"physics.comp-ph"}
{"created":"2024-04-16 19:24:14","title":"Trackable Agent-based Evolution Models at Wafer Scale","abstract":"Continuing improvements in computing hardware are poised to transform capabilities for in silico modeling of cross-scale phenomena underlying major open questions in evolutionary biology and artificial life, such as transitions in individuality, eco-evolutionary dynamics, and rare evolutionary events. Emerging ML/AI-oriented hardware accelerators, like the 850,000 processor Cerebras Wafer Scale Engine (WSE), hold particular promise. However, practical challenges remain in conducting informative evolution experiments that efficiently utilize these platforms' large processor counts. Here, we focus on the problem of extracting phylogenetic information from agent-based evolution on the WSE platform. This goal drove significant refinements to decentralized in silico phylogenetic tracking, reported here. These improvements yield order-of-magnitude performance improvements. We also present an asynchronous island-based genetic algorithm (GA) framework for WSE hardware. Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million agents. We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions. In particular, we demonstrate extraction, from wafer-scale simulation, of clear phylometric signals that differentiate runs with adaptive dynamics enabled versus disabled. Together, these benchmark and validation trials reflect strong potential for highly scalable agent-based evolution simulation that is both efficient and observable. Developed capabilities will bring entirely new classes of previously intractable research questions within reach, benefiting further explorations within the evolutionary biology and artificial life communities across a variety of emerging high-performance computing platforms.","sentences":["Continuing improvements in computing hardware are poised to transform capabilities for in silico modeling of cross-scale phenomena underlying major open questions in evolutionary biology and artificial life, such as transitions in individuality, eco-evolutionary dynamics, and rare evolutionary events.","Emerging ML/AI-oriented hardware accelerators, like the 850,000 processor Cerebras Wafer Scale Engine (WSE), hold particular promise.","However, practical challenges remain in conducting informative evolution experiments that efficiently utilize these platforms' large processor counts.","Here, we focus on the problem of extracting phylogenetic information from agent-based evolution on the WSE platform.","This goal drove significant refinements to decentralized in silico phylogenetic tracking, reported here.","These improvements yield order-of-magnitude performance improvements.","We also present an asynchronous island-based genetic algorithm (GA) framework for WSE hardware.","Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million agents.","We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions.","In particular, we demonstrate extraction, from wafer-scale simulation, of clear phylometric signals that differentiate runs with adaptive dynamics enabled versus disabled.","Together, these benchmark and validation trials reflect strong potential for highly scalable agent-based evolution simulation that is both efficient and observable.","Developed capabilities will bring entirely new classes of previously intractable research questions within reach, benefiting further explorations within the evolutionary biology and artificial life communities across a variety of emerging high-performance computing platforms."],"url":"http://arxiv.org/abs/2404.10861v1","category":"cs.NE"}
{"created":"2024-04-16 19:04:03","title":"Methods to Estimate Cryptic Sequence Complexity","abstract":"Complexity is a signature quality of interest in artificial life systems. Alongside other dimensions of assessment, it is common to quantify genome sites that contribute to fitness as a complexity measure. However, limitations to the sensitivity of fitness assays in models with implicit replication criteria involving rich biotic interactions introduce the possibility of difficult-to-detect ``cryptic'' adaptive sites, which contribute small fitness effects below the threshold of individual detectability or involve epistatic redundancies. Here, we propose three knockout-based assay procedures designed to quantify cryptic adaptive sites within digital genomes. We report initial tests of these methods on a simple genome model with explicitly configured site fitness effects. In these limited tests, estimation results reflect ground truth cryptic sequence complexities well. Presented work provides initial steps toward development of new methods and software tools that improve the resolution, rigor, and tractability of complexity analyses across alife systems, particularly those requiring expensive in situ assessments of organism fitness.","sentences":["Complexity is a signature quality of interest in artificial life systems.","Alongside other dimensions of assessment, it is common to quantify genome sites that contribute to fitness as a complexity measure.","However, limitations to the sensitivity of fitness assays in models with implicit replication criteria involving rich biotic interactions introduce the possibility of difficult-to-detect ``cryptic'' adaptive sites, which contribute small fitness effects below the threshold of individual detectability or involve epistatic redundancies.","Here, we propose three knockout-based assay procedures designed to quantify cryptic adaptive sites within digital genomes.","We report initial tests of these methods on a simple genome model with explicitly configured site fitness effects.","In these limited tests, estimation results reflect ground truth cryptic sequence complexities well.","Presented work provides initial steps toward development of new methods and software tools that improve the resolution, rigor, and tractability of complexity analyses across alife systems, particularly those requiring expensive in situ assessments of organism fitness."],"url":"http://arxiv.org/abs/2404.10854v1","category":"q-bio.PE"}
{"created":"2024-04-16 18:47:07","title":"Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of Micro-UAVs","abstract":"In communication-deprived disaster scenarios, this paper introduces a Micro-Unmanned Aerial Vehicle (UAV)- enhanced content management system. In the absence of cellular infrastructure, this system deploys a hybrid network of stationary and mobile UAVs to offer vital content access to isolated communities. Static anchor UAVs equipped with both vertical and lateral links cater to local users, while agile micro-ferrying UAVs, equipped with lateral links and greater mobility, reach users in various communities. The primary goal is to devise an adaptive content dissemination system that dynamically learns caching policies to maximize content accessibility. The paper proposes a decentralized Top-k Multi-Armed Bandit (Top-k MAB) learning approach for UAV caching decisions, accommodating geotemporal disparities in content popularity and diverse content demands. The proposed mechanism involves a Selective Caching Algorithm that algorithmically reduces redundant copies of the contents by leveraging the shared information between the UAVs. It is demonstrated that Top-k MAB learning, along with selective caching algorithm, can improve system performance while making the learning process adaptive. The paper does functional verification and performance evaluation of the proposed caching framework under a wide range of network size, swarm of micro-ferrying UAVs, and heterogeneous popularity distributions.","sentences":["In communication-deprived disaster scenarios, this paper introduces a Micro-Unmanned Aerial Vehicle (UAV)- enhanced content management system.","In the absence of cellular infrastructure, this system deploys a hybrid network of stationary and mobile UAVs to offer vital content access to isolated communities.","Static anchor UAVs equipped with both vertical and lateral links cater to local users, while agile micro-ferrying UAVs, equipped with lateral links and greater mobility, reach users in various communities.","The primary goal is to devise an adaptive content dissemination system that dynamically learns caching policies to maximize content accessibility.","The paper proposes a decentralized Top-k Multi-Armed Bandit (Top-k MAB) learning approach for UAV caching decisions, accommodating geotemporal disparities in content popularity and diverse content demands.","The proposed mechanism involves a Selective Caching Algorithm that algorithmically reduces redundant copies of the contents by leveraging the shared information between the UAVs.","It is demonstrated that Top-k MAB learning, along with selective caching algorithm, can improve system performance while making the learning process adaptive.","The paper does functional verification and performance evaluation of the proposed caching framework under a wide range of network size, swarm of micro-ferrying UAVs, and heterogeneous popularity distributions."],"url":"http://arxiv.org/abs/2404.10845v1","category":"cs.LG"}
{"created":"2024-04-16 18:30:40","title":"Uncertainty Quantification of Super-Resolution Flow Mapping in Liquid Metals using Ultrasound Localization Microscopy","abstract":"Convection of liquid metals drives large natural processes and is important in technical processes. Model experiments are conducted for research purposes where simulations are expensive and the clarification of open questions requires novel flow mapping methods with an increased spatial resolution. In this work, the method of Ultrasound Localization Microscopy (ULM) is investigated for this purpose. Known from microvasculature imaging, this method provides an increased spatial resolution beyond the diffraction limit. Its applicability in liquid metal flows is promising, however the realization and reliability is challenging, as artificial scattering particles or microbubbles cannot be utilized. To solve this issue an approach using nonlinear adaptive beamforming is proposed. This allowed the reliable tracking of particles of which super-resolved flow maps can be deduced. Furthermore, the application in fluid physics requires quantified results. Therefore, an uncertainty quantification model based on the spatial resolution, velocity gradient and measurement parameters is proposed, which allows to estimate the flow maps validity under experimental conditions. The proposed method is demonstrated in magnetohydrodynamic convection experiments. In some occasions, ULM was able to measure velocity vectors within the boundary layer of the flow, which will help for future in-depth flow studies. Furthermore, the proposed uncertainty model of ULM is of generic use in other applications.","sentences":["Convection of liquid metals drives large natural processes and is important in technical processes.","Model experiments are conducted for research purposes where simulations are expensive and the clarification of open questions requires novel flow mapping methods with an increased spatial resolution.","In this work, the method of Ultrasound Localization Microscopy (ULM) is investigated for this purpose.","Known from microvasculature imaging, this method provides an increased spatial resolution beyond the diffraction limit.","Its applicability in liquid metal flows is promising, however the realization and reliability is challenging, as artificial scattering particles or microbubbles cannot be utilized.","To solve this issue an approach using nonlinear adaptive beamforming is proposed.","This allowed the reliable tracking of particles of which super-resolved flow maps can be deduced.","Furthermore, the application in fluid physics requires quantified results.","Therefore, an uncertainty quantification model based on the spatial resolution, velocity gradient and measurement parameters is proposed, which allows to estimate the flow maps validity under experimental conditions.","The proposed method is demonstrated in magnetohydrodynamic convection experiments.","In some occasions, ULM was able to measure velocity vectors within the boundary layer of the flow, which will help for future in-depth flow studies.","Furthermore, the proposed uncertainty model of ULM is of generic use in other applications."],"url":"http://arxiv.org/abs/2404.10840v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 18:22:49","title":"Dynamic Self-adaptive Multiscale Distillation from Pre-trained Multimodal Large Model for Efficient Cross-modal Representation Learning","abstract":"In recent years, pre-trained multimodal large models have attracted widespread attention due to their outstanding performance in various multimodal applications. Nonetheless, the extensive computational resources and vast datasets required for their training present significant hurdles for deployment in environments with limited computational resources. To address this challenge, we propose a novel dynamic self-adaptive multiscale distillation from pre-trained multimodal large model for efficient cross-modal representation learning for the first time. Unlike existing distillation methods, our strategy employs a multiscale perspective, enabling the extraction structural knowledge across from the pre-trained multimodal large model. Ensuring that the student model inherits a comprehensive and nuanced understanding of the teacher knowledge. To optimize each distillation loss in a balanced and efficient manner, we propose a dynamic self-adaptive distillation loss balancer, a novel component eliminating the need for manual loss weight adjustments and dynamically balances each loss item during the distillation process. Our methodology streamlines pre-trained multimodal large models using only their output features and original image-level information, requiring minimal computational resources. This efficient approach is suited for various applications and allows the deployment of advanced multimodal technologies even in resource-limited settings. Extensive experiments has demonstrated that our method maintains high performance while significantly reducing model complexity and training costs. Moreover, our distilled student model utilizes only image-level information to achieve state-of-the-art performance on cross-modal retrieval tasks, surpassing previous methods that relied on region-level information.","sentences":["In recent years, pre-trained multimodal large models have attracted widespread attention due to their outstanding performance in various multimodal applications.","Nonetheless, the extensive computational resources and vast datasets required for their training present significant hurdles for deployment in environments with limited computational resources.","To address this challenge, we propose a novel dynamic self-adaptive multiscale distillation from pre-trained multimodal large model for efficient cross-modal representation learning for the first time.","Unlike existing distillation methods, our strategy employs a multiscale perspective, enabling the extraction structural knowledge across from the pre-trained multimodal large model.","Ensuring that the student model inherits a comprehensive and nuanced understanding of the teacher knowledge.","To optimize each distillation loss in a balanced and efficient manner, we propose a dynamic self-adaptive distillation loss balancer, a novel component eliminating the need for manual loss weight adjustments and dynamically balances each loss item during the distillation process.","Our methodology streamlines pre-trained multimodal large models using only their output features and original image-level information, requiring minimal computational resources.","This efficient approach is suited for various applications and allows the deployment of advanced multimodal technologies even in resource-limited settings.","Extensive experiments has demonstrated that our method maintains high performance while significantly reducing model complexity and training costs.","Moreover, our distilled student model utilizes only image-level information to achieve state-of-the-art performance on cross-modal retrieval tasks, surpassing previous methods that relied on region-level information."],"url":"http://arxiv.org/abs/2404.10838v1","category":"cs.CV"}
{"created":"2024-04-16 18:08:53","title":"Implantation of asteroids from the terrestrial planet region: The effect of the timing of the giant planet instability","abstract":"The dynamical architecture and compositional diversity of the asteroid belt strongly constrain planet formation models. Recent Solar System formation models have shown that the asteroid belt may have been born empty and later filled with objects from the inner ($<$2~au) and outer regions (>5 au) of the solar system. In this work, we focus on the implantation of inner solar system planetesimals into the asteroid belt - envisioned to represent S and/or E- type asteroids - during the late-stage accretion of the terrestrial planets. It is widely accepted that the solar system's giant planets formed in a more compact orbital configuration and evolved to their current dynamical state due to a planetary dynamical instability. In this work, we explore how the implantation efficiency of asteroids from the terrestrial region correlates with the timing of the giant planet instability, which has proven challenging to constrain. We carried out a suite of numerical simulations of the accretion of terrestrial planets considering different initial distributions of planetesimals in the terrestrial region and dynamical instability times. Our simulations show that a giant planet dynamical instability occurring at $t\\gtrapprox5$ Myr -- relative to the time of the sun's natal disk dispersal -- is broadly consistent with the current asteroid belt, allowing the total mass carried out by S-complex type asteroids to be implanted into the belt from the terrestrial region. Finally, we conclude that an instability that occurs coincident with the gas disk dispersal is either inconsistent with the empty asteroid belt scenario, or may require that the gas disk in the inner solar system have dissipated at least a few Myr earlier than the gas in the outer disk (beyond Jupiter's orbit).","sentences":["The dynamical architecture and compositional diversity of the asteroid belt strongly constrain planet formation models.","Recent Solar System formation models have shown that the asteroid belt may have been born empty and later filled with objects from the inner ($<$2~au) and outer regions (>5 au) of the solar system.","In this work, we focus on the implantation of inner solar system planetesimals into the asteroid belt - envisioned to represent S and/or E- type asteroids - during the late-stage accretion of the terrestrial planets.","It is widely accepted that the solar system's giant planets formed in a more compact orbital configuration and evolved to their current dynamical state due to a planetary dynamical instability.","In this work, we explore how the implantation efficiency of asteroids from the terrestrial region correlates with the timing of the giant planet instability, which has proven challenging to constrain.","We carried out a suite of numerical simulations of the accretion of terrestrial planets considering different initial distributions of planetesimals in the terrestrial region and dynamical instability times.","Our simulations show that a giant planet dynamical instability occurring at $t\\gtrapprox5$ Myr -- relative to the time of the sun's natal disk dispersal -- is broadly consistent with the current asteroid belt, allowing the total mass carried out by S-complex type asteroids to be implanted into the belt from the terrestrial region.","Finally, we conclude that an instability that occurs coincident with the gas disk dispersal is either inconsistent with the empty asteroid belt scenario, or may require that the gas disk in the inner solar system have dissipated at least a few Myr earlier than the gas in the outer disk (beyond Jupiter's orbit)."],"url":"http://arxiv.org/abs/2404.10831v1","category":"astro-ph.EP"}
{"created":"2024-04-16 17:57:19","title":"Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in Unbounded Scenes","abstract":"Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed.","sentences":["Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time.","However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians.","In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes.","Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work.","We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry.","Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity.","Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis.","Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed."],"url":"http://arxiv.org/abs/2404.10772v1","category":"cs.CV"}
{"created":"2024-04-16 17:50:02","title":"RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting","abstract":"Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge. In this work, we propose an approach for 3D scene inpainting -- the task of coherently replacing parts of the reconstructed scene with desired content. Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content. A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control. Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal. Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view. The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details. Our framework achieves state-of-the-art results for object removal while maintaining high controllability. We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction.","sentences":["Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge.","In this work, we propose an approach for 3D scene inpainting -- the task of coherently replacing parts of the reconstructed scene with desired content.","Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content.","A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control.","Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal.","Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view.","The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details.","Our framework achieves state-of-the-art results for object removal while maintaining high controllability.","We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction."],"url":"http://arxiv.org/abs/2404.10765v1","category":"cs.CV"}
{"created":"2024-04-16 16:51:27","title":"GazeHTA: End-to-end Gaze Target Detection with Head-Target Association","abstract":"We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets.","sentences":["We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at.","Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets.","In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image.","GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets.","Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets."],"url":"http://arxiv.org/abs/2404.10718v1","category":"cs.CV"}
{"created":"2024-04-16 15:58:49","title":"StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text Reference via Progressive Optimization","abstract":"Creating large-scale virtual urban scenes with variant styles is inherently challenging. To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity. Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background. To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally. During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content. We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views. Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization. Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process. The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process. Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences.","sentences":["Creating large-scale virtual urban scenes with variant styles is inherently challenging.","To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity.","Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background.","To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally.","During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content.","We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views.","Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization.","Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process.","The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process.","Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences."],"url":"http://arxiv.org/abs/2404.10681v1","category":"cs.CV"}
{"created":"2024-04-16 15:35:08","title":"Partial Differential Equations on Low-Dimensional Structures","abstract":"This thesis pertains to the study of elliptic and parabolic partial differential equations on \"thin\" structures. The first main objective is to establish the strong and weak low-dimensional counterparts of the parabolic Neumann problem. The main technical result is proving the closedness of the low-dimensional second-order operator. To construct a semigroup, a variant of Magyar of the Hille-Yosida Theorem for non-invertible operators is adapted. An alternative direction of study is presented to extend the class of accessible initial data. Weak-type parabolic problems are defined, and the existence of solutions is obtained by the application of the Lions version of the Lax-Milgram Lemma. The second aspect of the thesis is to examine the higher regularity of weak solutions to low-dimensional elliptic problems. We prove that for any weak low-dimensional elliptic solution, some Cosserat vector field exists as a witness of the membership of the solution to the domain of the second-order operator.","sentences":["This thesis pertains to the study of elliptic and parabolic partial differential equations on \"thin\" structures.","The first main objective is to establish the strong and weak low-dimensional counterparts of the parabolic Neumann problem.","The main technical result is proving the closedness of the low-dimensional second-order operator.","To construct a semigroup, a variant of Magyar of the Hille-Yosida Theorem for non-invertible operators is adapted.","An alternative direction of study is presented to extend the class of accessible initial data.","Weak-type parabolic problems are defined, and the existence of solutions is obtained by the application of the Lions version of the Lax-Milgram Lemma.","The second aspect of the thesis is to examine the higher regularity of weak solutions to low-dimensional elliptic problems.","We prove that for any weak low-dimensional elliptic solution, some Cosserat vector field exists as a witness of the membership of the solution to the domain of the second-order operator."],"url":"http://arxiv.org/abs/2404.10657v1","category":"math.AP"}
{"created":"2024-04-16 15:31:25","title":"Quantum Simulation of Open Quantum Dynamics via Non-Markovian Quantum State Diffusion","abstract":"Quantum simulation of non-Markovian open quantum dynamics is essential but challenging for standard quantum computers due to their non-Hermitian nature, leading to non-unitary evolution, and the limitations of available quantum resources. Here we introduce a hybrid quantum-classical algorithm designed for simulating dissipative dynamics in system with non-Markovian environment. Our approach includes formulating a non-Markovian Stochastic Schr\\\"odinger equation with complex frequency modes (cNMSSE) where the non-Markovianity is characterized by the mode excitation. Following this, we utilize variational quantum simulation to capture the non-unitary evolution within the cNMSSE framework, leading to a substantial reduction in qubit requirements. To demonstrate our approach, we investigated the spin-boson model and dynamic quantum phase transitions (DQPT) within transverse field Ising model (TFIM). Significantly, our findings reveal the enhanced DQPT in TFIM due to non-Markovian behavior.","sentences":["Quantum simulation of non-Markovian open quantum dynamics is essential but challenging for standard quantum computers due to their non-Hermitian nature, leading to non-unitary evolution, and the limitations of available quantum resources.","Here we introduce a hybrid quantum-classical algorithm designed for simulating dissipative dynamics in system with non-Markovian environment.","Our approach includes formulating a non-Markovian Stochastic Schr\\\"odinger equation with complex frequency modes (cNMSSE) where the non-Markovianity is characterized by the mode excitation.","Following this, we utilize variational quantum simulation to capture the non-unitary evolution within the cNMSSE framework, leading to a substantial reduction in qubit requirements.","To demonstrate our approach, we investigated the spin-boson model and dynamic quantum phase transitions (DQPT) within transverse field Ising model (TFIM).","Significantly, our findings reveal the enhanced DQPT in TFIM due to non-Markovian behavior."],"url":"http://arxiv.org/abs/2404.10655v2","category":"quant-ph"}
{"created":"2024-04-16 15:22:29","title":"Navigating the Serious Game Design Landscape: A Comprehensive Reference Document","abstract":"Within the evolving field of digital intervention, serious games emerge as promising tools for evidence-based interventions. Research indicates that gamified therapy, whether employed independently or in conjunction with online psychoeducation or traditional programs, proves more efficacious in delivering care to patients. As we navigate the intricate realm of serious game design, bridging the gap between therapeutic approaches and creative design proves complex. Professionals in clinical and research roles demonstrate innovative thinking yet face challenges in executing engaging therapeutic serious games due to the lack of specialized design skills and knowledge. Thus, a larger question remains: How might we aid and educate professionals in clinical and research roles the importance of game design to support their innovative therapeutic approaches? This study examines potential solutions aimed at facilitating the integration of gamification design principles into clinical study protocols, a pivotal aspect for aligning therapeutic practices with captivating narratives in the pursuit of innovative interventions. We propose two solutions, a flow chart framework for serious games or a comprehensive reference document encompassing gamification design principles and guidelines for best design practices. Through an examination of literature reviews, it was observed that selected design decisions varied across studies. Thus, we propose that the second solution, a comprehensive reference design guide, is more versatile and adaptable.","sentences":["Within the evolving field of digital intervention, serious games emerge as promising tools for evidence-based interventions.","Research indicates that gamified therapy, whether employed independently or in conjunction with online psychoeducation or traditional programs, proves more efficacious in delivering care to patients.","As we navigate the intricate realm of serious game design, bridging the gap between therapeutic approaches and creative design proves complex.","Professionals in clinical and research roles demonstrate innovative thinking yet face challenges in executing engaging therapeutic serious games due to the lack of specialized design skills and knowledge.","Thus, a larger question remains: How might we aid and educate professionals in clinical and research roles the importance of game design to support their innovative therapeutic approaches?","This study examines potential solutions aimed at facilitating the integration of gamification design principles into clinical study protocols, a pivotal aspect for aligning therapeutic practices with captivating narratives in the pursuit of innovative interventions.","We propose two solutions, a flow chart framework for serious games or a comprehensive reference document encompassing gamification design principles and guidelines for best design practices.","Through an examination of literature reviews, it was observed that selected design decisions varied across studies.","Thus, we propose that the second solution, a comprehensive reference design guide, is more versatile and adaptable."],"url":"http://arxiv.org/abs/2404.10649v1","category":"cs.HC"}
{"created":"2024-04-16 15:14:45","title":"Adapting SAM for Surgical Instrument Tracking and Segmentation in Endoscopic Submucosal Dissection Videos","abstract":"The precise tracking and segmentation of surgical instruments have led to a remarkable enhancement in the efficiency of surgical procedures. However, the challenge lies in achieving accurate segmentation of surgical instruments while minimizing the need for manual annotation and reducing the time required for the segmentation process. To tackle this, we propose a novel framework for surgical instrument segmentation and tracking. Specifically, with a tiny subset of frames for segmentation, we ensure accurate segmentation across the entire surgical video. Our method adopts a two-stage approach to efficiently segment videos. Initially, we utilize the Segment-Anything (SAM) model, which has been fine-tuned using the Low-Rank Adaptation (LoRA) on the EndoVis17 Dataset. The fine-tuned SAM model is applied to segment the initial frames of the video accurately. Subsequently, we deploy the XMem++ tracking algorithm to follow the annotated frames, thereby facilitating the segmentation of the entire video sequence. This workflow enables us to precisely segment and track objects within the video. Through extensive evaluation of the in-distribution dataset (EndoVis17) and the out-of-distribution datasets (EndoVis18 \\& the endoscopic submucosal dissection surgery (ESD) dataset), our framework demonstrates exceptional accuracy and robustness, thus showcasing its potential to advance the automated robotic-assisted surgery.","sentences":["The precise tracking and segmentation of surgical instruments have led to a remarkable enhancement in the efficiency of surgical procedures.","However, the challenge lies in achieving accurate segmentation of surgical instruments while minimizing the need for manual annotation and reducing the time required for the segmentation process.","To tackle this, we propose a novel framework for surgical instrument segmentation and tracking.","Specifically, with a tiny subset of frames for segmentation, we ensure accurate segmentation across the entire surgical video.","Our method adopts a two-stage approach to efficiently segment videos.","Initially, we utilize the Segment-Anything (SAM) model, which has been fine-tuned using the Low-Rank Adaptation (LoRA) on the EndoVis17 Dataset.","The fine-tuned SAM model is applied to segment the initial frames of the video accurately.","Subsequently, we deploy the XMem++ tracking algorithm to follow the annotated frames, thereby facilitating the segmentation of the entire video sequence.","This workflow enables us to precisely segment and track objects within the video.","Through extensive evaluation of the in-distribution dataset (EndoVis17) and the out-of-distribution datasets (EndoVis18 \\& the endoscopic submucosal dissection surgery (ESD) dataset), our framework demonstrates exceptional accuracy and robustness, thus showcasing its potential to advance the automated robotic-assisted surgery."],"url":"http://arxiv.org/abs/2404.10640v1","category":"eess.IV"}
{"created":"2024-04-16 15:10:36","title":"On Homomorphism Indistinguishability and Hypertree Depth","abstract":"$GC^k$ is a logic introduced by Scheidt and Schweikardt (2023) to express properties of hypergraphs. It is similar to first-order logic with counting quantifiers ($C$) adapted to the hypergraph setting. It has distinct sets of variables for vertices and for hyperedges and requires vertex variables to be guarded by hyperedge variables on every quantification.   We prove that two hypergraphs $G$, $H$ satisfy the same sentences in the logic $GC^k$ with guard depth at most $k$ if, and only if, they are homomorphism indistinguishable over the class of hypergraphs of strict hypertree depth at most $k$. This lifts the analogous result for tree depth $\\leq k$ and sentences of first-order logic with counting quantifiers of quantifier rank at most $k$ due to Grohe (2020) from graphs to hypergraphs. The guard depth of a formula is the quantifier rank with respect to hyperedge variables, and strict hypertree depth is a restriction of hypertree depth as defined by Adler, Gaven\\v{c}iak and Klimo\\v{s}ov\\'a (2012). To justify this restriction, we show that for every $H$, the strict hypertree depth of $H$ is at most 1 larger than its hypertree depth, and we give additional evidence that strict hypertree depth can be viewed as a reasonable generalisation of tree depth for hypergraphs.","sentences":["$GC^k$ is a logic introduced by Scheidt and Schweikardt (2023) to express properties of hypergraphs.","It is similar to first-order logic with counting quantifiers ($C$) adapted to the hypergraph setting.","It has distinct sets of variables for vertices and for hyperedges and requires vertex variables to be guarded by hyperedge variables on every quantification.   ","We prove that two hypergraphs $G$, $H$ satisfy the same sentences in the logic $GC^k$ with guard depth at most $k$ if, and only if, they are homomorphism indistinguishable over the class of hypergraphs of strict hypertree depth at most $k$.","This lifts the analogous result for tree depth $\\leq k$ and sentences of first-order logic with counting quantifiers of quantifier rank at most $k$ due to Grohe (2020) from graphs to hypergraphs.","The guard depth of a formula is the quantifier rank with respect to hyperedge variables, and strict hypertree depth is a restriction of hypertree depth as defined by Adler, Gaven\\v{c}iak and Klimo\\v{s}ov\\'a (2012).","To justify this restriction, we show that for every $H$, the strict hypertree depth of $H$ is at most 1 larger than its hypertree depth, and we give additional evidence that strict hypertree depth can be viewed as a reasonable generalisation of tree depth for hypergraphs."],"url":"http://arxiv.org/abs/2404.10637v1","category":"cs.LO"}
{"created":"2024-04-16 14:52:15","title":"Exploring selective image matching methods for zero-shot and few-sample unsupervised domain adaptation of urban canopy prediction","abstract":"We explore simple methods for adapting a trained multi-task UNet which predicts canopy cover and height to a new geographic setting using remotely sensed data without the need of training a domain-adaptive classifier and extensive fine-tuning. Extending previous research, we followed a selective alignment process to identify similar images in the two geographical domains and then tested an array of data-based unsupervised domain adaptation approaches in a zero-shot setting as well as with a small amount of fine-tuning. We find that the selective aligned data-based image matching methods produce promising results in a zero-shot setting, and even more so with a small amount of fine-tuning. These methods outperform both an untransformed baseline and a popular data-based image-to-image translation model. The best performing methods were pixel distribution adaptation and fourier domain adaptation on the canopy cover and height tasks respectively.","sentences":["We explore simple methods for adapting a trained multi-task UNet which predicts canopy cover and height to a new geographic setting using remotely sensed data without the need of training a domain-adaptive classifier and extensive fine-tuning.","Extending previous research, we followed a selective alignment process to identify similar images in the two geographical domains and then tested an array of data-based unsupervised domain adaptation approaches in a zero-shot setting as well as with a small amount of fine-tuning.","We find that the selective aligned data-based image matching methods produce promising results in a zero-shot setting, and even more so with a small amount of fine-tuning.","These methods outperform both an untransformed baseline and a popular data-based image-to-image translation model.","The best performing methods were pixel distribution adaptation and fourier domain adaptation on the canopy cover and height tasks respectively."],"url":"http://arxiv.org/abs/2404.10626v1","category":"cs.CV"}
{"created":"2024-04-16 14:29:05","title":"Stability of planar rarefaction waves in the vanishing dissipation limit of the Navier-Stokes-Fourier system","abstract":"We consider the vanishing dissipation limit of the compressible Navier-Stokes-Fourier system, where the initial data approach a profile generating a planar rarefaction wave for the limit Euler system. We show that the associated weak solutions converge unconditionally to the planar rarefaction wave strongly in the energy norm.","sentences":["We consider the vanishing dissipation limit of the compressible Navier-Stokes-Fourier system, where the initial data approach a profile generating a planar rarefaction wave for the limit Euler system.","We show that the associated weak solutions converge unconditionally to the planar rarefaction wave strongly in the energy norm."],"url":"http://arxiv.org/abs/2404.10604v1","category":"math.AP"}
{"created":"2024-04-16 14:06:40","title":"Extended Automatic Repeat Request For Integrated Sensing And Communication Networks","abstract":"6G wireless networks will integrate communication, computing, localization, and sensing capabilities while meeting the needs of high reliability and trustworthiness. In this paper, we develop similar techniques as those used by communication modules of previous generations for the sensing functionality of 6G networks. Specifically, this paper introduces the concept of extended automatic repeat request (e-ARQ) for integrated sensing and communications (ISAC) networks. We focus on multi-static sensing schemes, in which the nodes receiving the reflected sensing signals provide the transmitting nodes with configurable levels of feedback about the sensing result. This technique improves the sensing quality via retransmissions using adaptive parameters. We show that our proposed e-ARQ boosts the sensing quality in terms of detection accuracy and provides a sense of adaptability for applications supported by ISAC networks.","sentences":["6G wireless networks will integrate communication, computing, localization, and sensing capabilities while meeting the needs of high reliability and trustworthiness.","In this paper, we develop similar techniques as those used by communication modules of previous generations for the sensing functionality of 6G networks.","Specifically, this paper introduces the concept of extended automatic repeat request (e-ARQ) for integrated sensing and communications (ISAC) networks.","We focus on multi-static sensing schemes, in which the nodes receiving the reflected sensing signals provide the transmitting nodes with configurable levels of feedback about the sensing result.","This technique improves the sensing quality via retransmissions using adaptive parameters.","We show that our proposed e-ARQ boosts the sensing quality in terms of detection accuracy and provides a sense of adaptability for applications supported by ISAC networks."],"url":"http://arxiv.org/abs/2404.10583v1","category":"eess.SP"}
{"created":"2024-04-16 13:11:02","title":"Characterizing Polkadot's Transactions Ecosystem: methodology, tools, and insights","abstract":"The growth potential of a crypto(currency) project can be measured by the use cases spurred by the underlying technology. However, these projects are usually distributed, with a weak feedback schemes. Hence, a metric that is widely used as a proxy for their healthiness is the number of transactions and related volumes. Nevertheless, such a metric can be subject to manipulation (the crypto market being an unregulated one magnifies such a risk). To address the cited gap we design a comprehensive methodology to process large cryptocurrency transaction graphs that, after clustering user addresses of interest, derives a compact representation of the network that highlights clusters interactions.   To show the viability of our solution, we bring forward a use case centered on Polkadot, which has gained significant attention in the digital currency landscape due to its pioneering approach to interoperability and scalability. However, little is known about how many and to what extent its wide range of enabled use cases have been adopted by end-users so far. The answer to this type of question means mapping Polkadot (or any analyzed crypto project) on a palette that ranges from a thriving ecosystem to a speculative coin without compelling use cases.   Our findings demonstrate that crypto exchanges exert considerable influence on the Polkadot network, owning nearly 40% of all addresses in the ledger and absorbing at least 80% of all transactions. In addition, the high volume of inter-exchange transactions (> 20%) underscores the strong interconnections among just a couple of prominent exchanges, prompting further investigations into the behavior of these actors to uncover potential unethical activities, such as wash trading. These results, while characterized by a high level of scalability and adaptability, are at the same time immune from the drawbacks of currently used metrics.","sentences":["The growth potential of a crypto(currency) project can be measured by the use cases spurred by the underlying technology.","However, these projects are usually distributed, with a weak feedback schemes.","Hence, a metric that is widely used as a proxy for their healthiness is the number of transactions and related volumes.","Nevertheless, such a metric can be subject to manipulation (the crypto market being an unregulated one magnifies such a risk).","To address the cited gap we design a comprehensive methodology to process large cryptocurrency transaction graphs that, after clustering user addresses of interest, derives a compact representation of the network that highlights clusters interactions.   ","To show the viability of our solution, we bring forward a use case centered on Polkadot, which has gained significant attention in the digital currency landscape due to its pioneering approach to interoperability and scalability.","However, little is known about how many and to what extent its wide range of enabled use cases have been adopted by end-users so far.","The answer to this type of question means mapping Polkadot (or any analyzed crypto project) on a palette that ranges from a thriving ecosystem to a speculative coin without compelling use cases.   ","Our findings demonstrate that crypto exchanges exert considerable influence on the Polkadot network, owning nearly 40% of all addresses in the ledger and absorbing at least 80% of all transactions.","In addition, the high volume of inter-exchange transactions (> 20%) underscores the strong interconnections among just a couple of prominent exchanges, prompting further investigations into the behavior of these actors to uncover potential unethical activities, such as wash trading.","These results, while characterized by a high level of scalability and adaptability, are at the same time immune from the drawbacks of currently used metrics."],"url":"http://arxiv.org/abs/2404.10543v1","category":"cs.CR"}
{"created":"2024-04-16 12:12:06","title":"LAECIPS: Large Vision Model Assisted Adaptive Edge-Cloud Collaboration for IoT-based Perception System","abstract":"Recent large vision models (e.g., SAM) enjoy great potential to facilitate intelligent perception with high accuracy. Yet, the resource constraints in the IoT environment tend to limit such large vision models to be locally deployed, incurring considerable inference latency thereby making it difficult to support real-time applications, such as autonomous driving and robotics. Edge-cloud collaboration with large-small model co-inference offers a promising approach to achieving high inference accuracy and low latency. However, existing edge-cloud collaboration methods are tightly coupled with the model architecture and cannot adapt to the dynamic data drifts in heterogeneous IoT environments. To address the issues, we propose LAECIPS, a new edge-cloud collaboration framework. In LAECIPS, both the large vision model on the cloud and the lightweight model on the edge are plug-and-play. We design an edge-cloud collaboration strategy based on hard input mining, optimized for both high accuracy and low latency. We propose to update the edge model and its collaboration strategy with the cloud under the supervision of the large vision model, so as to adapt to the dynamic IoT data streams. Theoretical analysis of LAECIPS proves its feasibility. Experiments conducted in a robotic semantic segmentation system using real-world datasets show that LAECIPS outperforms its state-of-the-art competitors in accuracy, latency, and communication overhead while having better adaptability to dynamic environments.","sentences":["Recent large vision models (e.g., SAM) enjoy great potential to facilitate intelligent perception with high accuracy.","Yet, the resource constraints in the IoT environment tend to limit such large vision models to be locally deployed, incurring considerable inference latency thereby making it difficult to support real-time applications, such as autonomous driving and robotics.","Edge-cloud collaboration with large-small model co-inference offers a promising approach to achieving high inference accuracy and low latency.","However, existing edge-cloud collaboration methods are tightly coupled with the model architecture and cannot adapt to the dynamic data drifts in heterogeneous IoT environments.","To address the issues, we propose LAECIPS, a new edge-cloud collaboration framework.","In LAECIPS, both the large vision model on the cloud and the lightweight model on the edge are plug-and-play.","We design an edge-cloud collaboration strategy based on hard input mining, optimized for both high accuracy and low latency.","We propose to update the edge model and its collaboration strategy with the cloud under the supervision of the large vision model, so as to adapt to the dynamic IoT data streams.","Theoretical analysis of LAECIPS proves its feasibility.","Experiments conducted in a robotic semantic segmentation system using real-world datasets show that LAECIPS outperforms its state-of-the-art competitors in accuracy, latency, and communication overhead while having better adaptability to dynamic environments."],"url":"http://arxiv.org/abs/2404.10498v1","category":"cs.AI"}
{"created":"2024-04-16 12:04:04","title":"Assumption-Lean Quantile Regression","abstract":"Quantile regression is a powerful tool for detecting exposure-outcome associations given covariates across different parts of the outcome's distribution, but has two major limitations when the aim is to infer the effect of an exposure. Firstly, the exposure coefficient estimator may not converge to a meaningful quantity when the model is misspecified, and secondly, variable selection methods may induce bias and excess uncertainty, rendering inferences biased and overly optimistic. In this paper, we address these issues via partially linear quantile regression models which parametrize the conditional association of interest, but do not restrict the association with other covariates in the model. We propose consistent estimators for the unknown model parameter by mapping it onto a nonparametric main effect estimand that captures the (conditional) association of interest even when the quantile model is misspecified. This estimand is estimated using the efficient influence function under the nonparametric model, allowing for the incorporation of data-adaptive procedures such as variable selection and machine learning. Our approach provides a flexible and reliable method for detecting associations that is robust to model misspecification and excess uncertainty induced by variable selection methods. The proposal is illustrated using simulation studies and data on annual health care costs associated with excess body weight.","sentences":["Quantile regression is a powerful tool for detecting exposure-outcome associations given covariates across different parts of the outcome's distribution, but has two major limitations when the aim is to infer the effect of an exposure.","Firstly, the exposure coefficient estimator may not converge to a meaningful quantity when the model is misspecified, and secondly, variable selection methods may induce bias and excess uncertainty, rendering inferences biased and overly optimistic.","In this paper, we address these issues via partially linear quantile regression models which parametrize the conditional association of interest, but do not restrict the association with other covariates in the model.","We propose consistent estimators for the unknown model parameter by mapping it onto a nonparametric main effect estimand that captures the (conditional) association of interest even when the quantile model is misspecified.","This estimand is estimated using the efficient influence function under the nonparametric model, allowing for the incorporation of data-adaptive procedures such as variable selection and machine learning.","Our approach provides a flexible and reliable method for detecting associations that is robust to model misspecification and excess uncertainty induced by variable selection methods.","The proposal is illustrated using simulation studies and data on annual health care costs associated with excess body weight."],"url":"http://arxiv.org/abs/2404.10495v2","category":"stat.ME"}
{"created":"2024-04-16 12:03:38","title":"BDAN: Mitigating Temporal Difference Across Electrodes in Cross-Subject Motor Imagery Classification via Generative Bridging Domain","abstract":"Because of \"the non-repeatability of the experiment settings and conditions\" and \"the variability of brain patterns among subjects\", the data distributions across sessions and electrodes are different in cross-subject motor imagery (MI) studies, eventually reducing the performance of the classification model. Systematically summarised based on the existing studies, a novel temporal-electrode data distribution problem is investigated under both intra-subject and inter-subject scenarios in this paper. Based on the presented issue, a novel bridging domain adaptation network (BDAN) is proposed, aiming to minimise the data distribution difference across sessions in the aspect of the electrode, thus improving and enhancing model performance. In the proposed BDAN, deep features of all the EEG data are extracted via a specially designed spatial feature extractor. With the obtained spatio-temporal features, a special generative bridging domain is established, bridging the data from all the subjects across sessions. The difference across sessions and electrodes is then minimized using the customized bridging loss functions, and the known knowledge is automatically transferred through the constructed bridging domain. To show the effectiveness of the proposed BDAN, comparison experiments and ablation studies are conducted on a public EEG dataset. The overall comparison results demonstrate the superior performance of the proposed BDAN compared with the other advanced deep learning and domain adaptation methods.","sentences":["Because of \"the non-repeatability of the experiment settings and conditions\" and \"the variability of brain patterns among subjects\", the data distributions across sessions and electrodes are different in cross-subject motor imagery (MI) studies, eventually reducing the performance of the classification model.","Systematically summarised based on the existing studies, a novel temporal-electrode data distribution problem is investigated under both intra-subject and inter-subject scenarios in this paper.","Based on the presented issue, a novel bridging domain adaptation network (BDAN) is proposed, aiming to minimise the data distribution difference across sessions in the aspect of the electrode, thus improving and enhancing model performance.","In the proposed BDAN, deep features of all the EEG data are extracted via a specially designed spatial feature extractor.","With the obtained spatio-temporal features, a special generative bridging domain is established, bridging the data from all the subjects across sessions.","The difference across sessions and electrodes is then minimized using the customized bridging loss functions, and the known knowledge is automatically transferred through the constructed bridging domain.","To show the effectiveness of the proposed BDAN, comparison experiments and ablation studies are conducted on a public EEG dataset.","The overall comparison results demonstrate the superior performance of the proposed BDAN compared with the other advanced deep learning and domain adaptation methods."],"url":"http://arxiv.org/abs/2404.10494v1","category":"cs.HC"}
{"created":"2024-04-16 11:44:12","title":"AbsGS: Recovering Fine Details for 3D Gaussian Splatting","abstract":"3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance. However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images. The underlying reason for the flaw has still been under-explored. In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting. To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification. Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting. We evaluate our proposed method on various challenging datasets. The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption. Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods. We will open source our codes upon formal publication. Our project page is available at: https://ty424.github.io/AbsGS.github.io/","sentences":["3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance.","However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images.","The underlying reason for the flaw has still been under-explored.","In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting.","To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification.","Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting.","We evaluate our proposed method on various challenging datasets.","The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption.","Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods.","We will open source our codes upon formal publication.","Our project page is available at: https://ty424.github.io/AbsGS.github.io/"],"url":"http://arxiv.org/abs/2404.10484v1","category":"cs.CV"}
{"created":"2024-04-16 11:08:20","title":"Forward lateral photovoltage scanning problem: Perturbation approach and existence-uniqueness analysis","abstract":"In this paper, we present analytical results for the so-called forward lateral photovoltage scanning (LPS) problem. The (inverse) LPS model predicts doping variations in crystal by measuring the current leaving the crystal generated by a laser at various positions. The forward model consists of a set of nonlinear elliptic equations coupled with a measuring device modeled by a resistance. Standard methods to ensure the existence and uniqueness of the forward model cannot be used in a straightforward manner due to the presence of an additional generation term modeling the effect of the laser on the crystal. Hence, we scale the original forward LPS problem and employ a perturbation approach to derive the leading order system and the correction up to the second order in an appropriate small parameter. While these simplifications pose no issues from a physical standpoint, they enable us to demonstrate the analytic existence and uniqueness of solutions for the simplified system using standard arguments from elliptic theory adapted to the coupling with the measuring device.","sentences":["In this paper, we present analytical results for the so-called forward lateral photovoltage scanning (LPS) problem.","The (inverse) LPS model predicts doping variations in crystal by measuring the current leaving the crystal generated by a laser at various positions.","The forward model consists of a set of nonlinear elliptic equations coupled with a measuring device modeled by a resistance.","Standard methods to ensure the existence and uniqueness of the forward model cannot be used in a straightforward manner due to the presence of an additional generation term modeling the effect of the laser on the crystal.","Hence, we scale the original forward LPS problem and employ a perturbation approach to derive the leading order system and the correction up to the second order in an appropriate small parameter.","While these simplifications pose no issues from a physical standpoint, they enable us to demonstrate the analytic existence and uniqueness of solutions for the simplified system using standard arguments from elliptic theory adapted to the coupling with the measuring device."],"url":"http://arxiv.org/abs/2404.10466v1","category":"math-ph"}
{"created":"2024-04-16 10:10:19","title":"Language Proficiency and F0 Entrainment: A Study of L2 English Imitation in Italian, French, and Slovak Speakers","abstract":"This study explores F0 entrainment in second language (L2) English speech imitation during an Alternating Reading Task (ART). Participants with Italian, French, and Slovak native languages imitated English utterances, and their F0 entrainment was quantified using the Dynamic Time Warping (DTW) distance between the parameterized F0 contours of the imitated utterances and those of the model utterances. Results indicate a nuanced relationship between L2 English proficiency and entrainment: speakers with higher proficiency generally exhibit less entrainment in pitch variation and declination. However, within dyads, the more proficient speakers demonstrate a greater ability to mimic pitch range, leading to increased entrainment. This suggests that proficiency influences entrainment differently at individual and dyadic levels, highlighting the complex interplay between language skill and prosodic adaptation.","sentences":["This study explores F0 entrainment in second language (L2) English speech imitation during an Alternating Reading Task (ART).","Participants with Italian, French, and Slovak native languages imitated English utterances, and their F0 entrainment was quantified using the Dynamic Time Warping (DTW) distance between the parameterized F0 contours of the imitated utterances and those of the model utterances.","Results indicate a nuanced relationship between L2 English proficiency and entrainment: speakers with higher proficiency generally exhibit less entrainment in pitch variation and declination.","However, within dyads, the more proficient speakers demonstrate a greater ability to mimic pitch range, leading to increased entrainment.","This suggests that proficiency influences entrainment differently at individual and dyadic levels, highlighting the complex interplay between language skill and prosodic adaptation."],"url":"http://arxiv.org/abs/2404.10440v1","category":"cs.CL"}
{"created":"2024-04-16 09:37:41","title":"AudioProtoPNet: An interpretable deep learning model for bird sound classification","abstract":"Recently, scientists have proposed several deep learning models to monitor the diversity of bird species. These models can detect bird species with high accuracy by analyzing acoustic signals. However, traditional deep learning algorithms are black-box models that provide no insight into their decision-making process. For domain experts, such as ornithologists, it is crucial that these models are not only efficient, but also interpretable in order to be used as assistive tools. In this study, we present an adaption of the Prototypical Part Network (ProtoPNet) for audio classification that provides inherent interpretability through its model architecture. Our approach is based on a ConvNeXt backbone architecture for feature extraction and learns prototypical patterns for each bird species using spectrograms of the training data. Classification of new data is done by comparison with these prototypes in latent space, which simultaneously serve as easily understandable explanations for the model's decisions.","sentences":["Recently, scientists have proposed several deep learning models to monitor the diversity of bird species.","These models can detect bird species with high accuracy by analyzing acoustic signals.","However, traditional deep learning algorithms are black-box models that provide no insight into their decision-making process.","For domain experts, such as ornithologists, it is crucial that these models are not only efficient, but also interpretable in order to be used as assistive tools.","In this study, we present an adaption of the Prototypical Part Network (ProtoPNet) for audio classification that provides inherent interpretability through its model architecture.","Our approach is based on a ConvNeXt backbone architecture for feature extraction and learns prototypical patterns for each bird species using spectrograms of the training data.","Classification of new data is done by comparison with these prototypes in latent space, which simultaneously serve as easily understandable explanations for the model's decisions."],"url":"http://arxiv.org/abs/2404.10420v1","category":"cs.LG"}
{"created":"2024-04-16 07:59:53","title":"Robust Performance Over Changing Intersymbol Interference Channels by Spatial Coupling","abstract":"We show that spatially coupled low-density parity- check (LDPC) codes yield robust performance over changing intersymbol interfere (ISI) channels with optimal and suboptimal detectors. We compare the performance with classical LDPC code design which involves optimizing the degree distribution for a given (known) channel. We demonstrate that these classical schemes, despite working very good when designed for a given channel, can perform poorly if the channel is exchanged. With spatially coupled LDPC codes, however, we get performances close to the symmetric information rates with just a single code, without the need to know the channel and adapt to it at the transmitter. We also investigate threshold saturation with the linear minimum mean square error (LMMSE) detector and show that with spatial coupling its performance can get remarkably close to that of an optimal detector for regular LDPC codes.","sentences":["We show that spatially coupled low-density parity- check (LDPC) codes yield robust performance over changing intersymbol interfere (ISI) channels with optimal and suboptimal detectors.","We compare the performance with classical LDPC code design which involves optimizing the degree distribution for a given (known) channel.","We demonstrate that these classical schemes, despite working very good when designed for a given channel, can perform poorly if the channel is exchanged.","With spatially coupled LDPC codes, however, we get performances close to the symmetric information rates with just a single code, without the need to know the channel and adapt to it at the transmitter.","We also investigate threshold saturation with the linear minimum mean square error (LMMSE) detector and show that with spatial coupling its performance can get remarkably close to that of an optimal detector for regular LDPC codes."],"url":"http://arxiv.org/abs/2404.10367v1","category":"cs.IT"}
{"created":"2024-04-16 07:50:28","title":"Numerical study of the Gross-Pitaevskii equation on a two-dimensional ring and vortex nucleation","abstract":"We consider the Gross-Pitaevskii equation with a confining ring potential with a Gaussian profile. By introducing a rotating sinusoidal perturbation, we numerically highlight the nucleation of quantum vortices in a particular regime throughout the dynamics. Numerical computations are made via a Strang splitting time integration and a two-point flux approximation Finite Volume scheme based on a particular admissible triangulation. We also develop numerical algorithms for vortex tracking adapted to our finite volume framework.","sentences":["We consider the Gross-Pitaevskii equation with a confining ring potential with a Gaussian profile.","By introducing a rotating sinusoidal perturbation, we numerically highlight the nucleation of quantum vortices in a particular regime throughout the dynamics.","Numerical computations are made via a Strang splitting time integration and a two-point flux approximation Finite Volume scheme based on a particular admissible triangulation.","We also develop numerical algorithms for vortex tracking adapted to our finite volume framework."],"url":"http://arxiv.org/abs/2404.10360v1","category":"math.NA"}
{"created":"2024-04-16 07:44:52","title":"Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models","abstract":"Vision-Language Models (VLMs), such as CLIP, play a foundational role in various cross-modal applications. To fully leverage VLMs' potential in adapting to downstream tasks, context optimization methods like Prompt Tuning are essential. However, one key limitation is the lack of diversity in prompt templates, whether they are hand-crafted or learned through additional modules. This limitation restricts the capabilities of pretrained VLMs and can result in incorrect predictions in downstream tasks. To address this challenge, we propose Context Optimization with Multi-Knowledge Representation (CoKnow), a framework that enhances Prompt Learning for VLMs with rich contextual knowledge. To facilitate CoKnow during inference, we trained lightweight semantic knowledge mappers, which are capable of generating Multi-Knowledge Representation for an input image without requiring additional priors. Experimentally, We conducted extensive experiments on 11 publicly available datasets, demonstrating that CoKnow outperforms a series of previous methods. We will make all resources open-source: https://github.com/EMZucas/CoKnow.","sentences":["Vision-Language Models (VLMs), such as CLIP, play a foundational role in various cross-modal applications.","To fully leverage VLMs' potential in adapting to downstream tasks, context optimization methods like Prompt Tuning are essential.","However, one key limitation is the lack of diversity in prompt templates, whether they are hand-crafted or learned through additional modules.","This limitation restricts the capabilities of pretrained VLMs and can result in incorrect predictions in downstream tasks.","To address this challenge, we propose Context Optimization with Multi-Knowledge Representation (CoKnow), a framework that enhances Prompt Learning for VLMs with rich contextual knowledge.","To facilitate CoKnow during inference, we trained lightweight semantic knowledge mappers, which are capable of generating Multi-Knowledge Representation for an input image without requiring additional priors.","Experimentally, We conducted extensive experiments on 11 publicly available datasets, demonstrating that CoKnow outperforms a series of previous methods.","We will make all resources open-source: https://github.com/EMZucas/CoKnow."],"url":"http://arxiv.org/abs/2404.10357v2","category":"cs.CV"}
{"created":"2024-04-16 07:43:01","title":"AERO: Adaptive Erase Operation for Improving Lifetime and Performance of Modern NAND Flash-Based SSDs","abstract":"This work investigates a new erase scheme in NAND flash memory to improve the lifetime and performance of modern solid-state drives (SSDs). In NAND flash memory, an erase operation applies a high voltage (e.g., > 20 V) to flash cells for a long time (e.g., > 3.5 ms), which degrades cell endurance and potentially delays user I/O requests. While a large body of prior work has proposed various techniques to mitigate the negative impact of erase operations, no work has yet investigated how erase latency should be set to fully exploit the potential of NAND flash memory; most existing techniques use a fixed latency for every erase operation which is set to cover the worst-case operating conditions. To address this, we propose AERO (Adaptive ERase Operation), a new erase scheme that dynamically adjusts erase latency to be just long enough for reliably erasing target cells, depending on the cells' current erase characteristics. AERO accurately predicts such near-optimal erase latency based on the number of fail bits during an erase operation. To maximize its benefits, we further optimize AERO in two aspects. First, at the beginning of an erase operation, AERO attempts to erase the cells for a short time (e.g., 1 ms), which enables AERO to always obtain the number of fail bits necessary to accurately predict the near-optimal erase latency. Second, AERO aggressively yet safely reduces erase latency by leveraging a large reliability margin present in modern SSDs. We demonstrate the feasibility and reliability of AERO using 160 real 3D NAND flash chips, showing that it enhances SSD lifetime over the conventional erase scheme by 43% without change to existing NAND flash chips. Our system-level evaluation using eleven real-world workloads shows that an AERO-enabled SSD reduces read tail latency by 34% on average over a state-of-the-art technique.","sentences":["This work investigates a new erase scheme in NAND flash memory to improve the lifetime and performance of modern solid-state drives (SSDs).","In NAND flash memory, an erase operation applies a high voltage (e.g., > 20 V) to flash cells for a long time (e.g., > 3.5 ms), which degrades cell endurance and potentially delays user I/O requests.","While a large body of prior work has proposed various techniques to mitigate the negative impact of erase operations, no work has yet investigated how erase latency should be set to fully exploit the potential of NAND flash memory; most existing techniques use a fixed latency for every erase operation which is set to cover the worst-case operating conditions.","To address this, we propose AERO (Adaptive ERase Operation), a new erase scheme that dynamically adjusts erase latency to be just long enough for reliably erasing target cells, depending on the cells' current erase characteristics.","AERO accurately predicts such near-optimal erase latency based on the number of fail bits during an erase operation.","To maximize its benefits, we further optimize AERO in two aspects.","First, at the beginning of an erase operation, AERO attempts to erase the cells for a short time (e.g., 1 ms), which enables AERO to always obtain the number of fail bits necessary to accurately predict the near-optimal erase latency.","Second, AERO aggressively yet safely reduces erase latency by leveraging a large reliability margin present in modern SSDs.","We demonstrate the feasibility and reliability of AERO using 160 real 3D NAND flash chips, showing that it enhances SSD lifetime over the conventional erase scheme by 43% without change to existing NAND flash chips.","Our system-level evaluation using eleven real-world workloads shows that an AERO-enabled SSD reduces read tail latency by 34% on average over a state-of-the-art technique."],"url":"http://arxiv.org/abs/2404.10355v1","category":"cs.AR"}
{"created":"2024-04-16 07:19:52","title":"Efficiently Adversarial Examples Generation for Visual-Language Models under Targeted Transfer Scenarios using Diffusion Models","abstract":"Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs). However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts. Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure. To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability. Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area. Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples. Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods. Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner.","sentences":["Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs).","However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts.","Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure.","To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples.","Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability.","Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area.","Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples.","Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods.","Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner."],"url":"http://arxiv.org/abs/2404.10335v1","category":"cs.CV"}
{"created":"2024-04-16 07:11:48","title":"Exact and Efficient Unlearning for Large Language Model-based Recommendation","abstract":"The evolving paradigm of Large Language Model-based Recom- mendation (LLMRec) customizes Large Language Models (LLMs) through parameter-efficient fine-tuning (PEFT) using recommenda- tion data. The inclusion of user data in LLMs raises privacy concerns. To protect users, the unlearning process in LLMRec, specifically removing unusable data (e.g., historical behaviors) from established LLMRec models, becomes crucial. However, existing unlearning methods are insufficient for the unique characteristics of LLM- Rec, mainly due to high computational costs or incomplete data erasure. In this study, we introduce the Adapter Partition and Ag- gregation (APA) framework for exact and efficient unlearning while maintaining recommendation performance. APA achieves this by establishing distinct adapters for partitioned training data shards and retraining only the adapters impacted by unusable data for un- learning. To preserve recommendation performance and mitigate considerable inference costs, APA employs parameter-level adapter aggregation with sample-adaptive attention for individual testing samples. Extensive experiments substantiate the effectiveness and efficiency of our proposed framework","sentences":["The evolving paradigm of Large Language Model-based Recom- mendation (LLMRec) customizes Large Language Models (LLMs) through parameter-efficient fine-tuning (PEFT) using recommenda- tion data.","The inclusion of user data in LLMs raises privacy concerns.","To protect users, the unlearning process in LLMRec, specifically removing unusable data (e.g., historical behaviors) from established LLMRec models, becomes crucial.","However, existing unlearning methods are insufficient for the unique characteristics of LLM- Rec, mainly due to high computational costs or incomplete data erasure.","In this study, we introduce the Adapter Partition and Ag- gregation (APA) framework for exact and efficient unlearning while maintaining recommendation performance.","APA achieves this by establishing distinct adapters for partitioned training data shards and retraining only the adapters impacted by unusable data for un- learning.","To preserve recommendation performance and mitigate considerable inference costs, APA employs parameter-level adapter aggregation with sample-adaptive attention for individual testing samples.","Extensive experiments substantiate the effectiveness and efficiency of our proposed framework"],"url":"http://arxiv.org/abs/2404.10327v1","category":"cs.IR"}
{"created":"2024-04-16 07:07:41","title":"Investigations on Projection-Based Reduced Order Model Development for Rotating Detonation Engine","abstract":"The current study aims to evaluate and investigate the development of projection-based reduced-order models (ROMs) for efficient and accurate RDE simulations. Specifically, we focus on assessing the projection-based ROM construction utilizing three different approaches: the linear static basis, nonlinear quadratic basis, and an adaptive model order reduction (MOR) formulation. First, an ~\\textit{a priori} analysis is performed to evaluate the effectiveness of the linear static and nonlinear quadratic bases in representing the detonation-wave dynamics. The~\\textit{a priori} analysis reveals that compared to the linear basis, the nonlinear quadratic basis provides significantly improved representation of detonation-wave dynamics within the training regime. However, it exhibits limited capabilities in representing the dynamics beyond the training regime, either in the future state or under a different operating parameter (i.e., inlet velocity). Second, the investigations proceed to the adaptive MOR formulation, which constructs an \\textit{online} adaptive ROM with a small amount of offline training data. It is demonstrated that the adaptive ROM can provide significantly enhanced predictive capabilities in modeling the RDE dynamics in the future state, and subject to parametric variations. More importantly, the adaptive ROM is shown to be capable of capturing the initial transience in establishing the detonation wave.","sentences":["The current study aims to evaluate and investigate the development of projection-based reduced-order models (ROMs) for efficient and accurate RDE simulations.","Specifically, we focus on assessing the projection-based ROM construction utilizing three different approaches: the linear static basis, nonlinear quadratic basis, and an adaptive model order reduction (MOR) formulation.","First, an ~\\textit{a priori} analysis is performed to evaluate the effectiveness of the linear static and nonlinear quadratic bases in representing the detonation-wave dynamics.","The~\\textit{a","priori} analysis reveals that compared to the linear basis, the nonlinear quadratic basis provides significantly improved representation of detonation-wave dynamics within the training regime.","However, it exhibits limited capabilities in representing the dynamics beyond the training regime, either in the future state or under a different operating parameter (i.e., inlet velocity).","Second, the investigations proceed to the adaptive MOR formulation, which constructs an \\textit{online} adaptive ROM with a small amount of offline training data.","It is demonstrated that the adaptive ROM can provide significantly enhanced predictive capabilities in modeling the RDE dynamics in the future state, and subject to parametric variations.","More importantly, the adaptive ROM is shown to be capable of capturing the initial transience in establishing the detonation wave."],"url":"http://arxiv.org/abs/2404.10323v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 07:07:40","title":"Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation","abstract":"Few-shot semantic segmentation (FSS) has achieved great success on segmenting objects of novel classes, supported by only a few annotated samples. However, existing FSS methods often underperform in the presence of domain shifts, especially when encountering new domain styles that are unseen during training. It is suboptimal to directly adapt or generalize the entire model to new domains in the few-shot scenario. Instead, our key idea is to adapt a small adapter for rectifying diverse target domain styles to the source domain. Consequently, the rectified target domain features can fittingly benefit from the well-optimized source domain segmentation model, which is intently trained on sufficient source domain data. Training domain-rectifying adapter requires sufficiently diverse target domains. We thus propose a novel local-global style perturbation method to simulate diverse potential target domains by perturbating the feature channel statistics of the individual images and collective statistics of the entire source domain, respectively. Additionally, we propose a cyclic domain alignment module to facilitate the adapter effectively rectifying domains using a reverse domain rectification supervision. The adapter is trained to rectify the image features from diverse synthesized target domains to align with the source domain. During testing on target domains, we start by rectifying the image features and then conduct few-shot segmentation on the domain-rectified features. Extensive experiments demonstrate the effectiveness of our method, achieving promising results on cross-domain few-shot semantic segmentation tasks. Our code is available at https://github.com/Matt-Su/DR-Adapter.","sentences":["Few-shot semantic segmentation (FSS) has achieved great success on segmenting objects of novel classes, supported by only a few annotated samples.","However, existing FSS methods often underperform in the presence of domain shifts, especially when encountering new domain styles that are unseen during training.","It is suboptimal to directly adapt or generalize the entire model to new domains in the few-shot scenario.","Instead, our key idea is to adapt a small adapter for rectifying diverse target domain styles to the source domain.","Consequently, the rectified target domain features can fittingly benefit from the well-optimized source domain segmentation model, which is intently trained on sufficient source domain data.","Training domain-rectifying adapter requires sufficiently diverse target domains.","We thus propose a novel local-global style perturbation method to simulate diverse potential target domains by perturbating the feature channel statistics of the individual images and collective statistics of the entire source domain, respectively.","Additionally, we propose a cyclic domain alignment module to facilitate the adapter effectively rectifying domains using a reverse domain rectification supervision.","The adapter is trained to rectify the image features from diverse synthesized target domains to align with the source domain.","During testing on target domains, we start by rectifying the image features and then conduct few-shot segmentation on the domain-rectified features.","Extensive experiments demonstrate the effectiveness of our method, achieving promising results on cross-domain few-shot semantic segmentation tasks.","Our code is available at https://github.com/Matt-Su/DR-Adapter."],"url":"http://arxiv.org/abs/2404.10322v1","category":"cs.CV"}
{"created":"2024-04-16 05:40:30","title":"Engineering software 2.0 by interpolating neural networks: unifying training, solving, and calibration","abstract":"The evolution of artificial intelligence (AI) and neural network theories has revolutionized the way software is programmed, shifting from a hard-coded series of codes to a vast neural network. However, this transition in engineering software has faced challenges such as data scarcity, multi-modality of data, low model accuracy, and slow inference. Here, we propose a new network based on interpolation theories and tensor decomposition, the interpolating neural network (INN). Instead of interpolating training data, a common notion in computer science, INN interpolates interpolation points in the physical space whose coordinates and values are trainable. It can also extrapolate if the interpolation points reside outside of the range of training data and the interpolation functions have a larger support domain. INN features orders of magnitude fewer trainable parameters, faster training, a smaller memory footprint, and higher model accuracy compared to feed-forward neural networks (FFNN) or physics-informed neural networks (PINN). INN is poised to usher in Engineering Software 2.0, a unified neural network that spans various domains of space, time, parameters, and initial/boundary conditions. This has previously been computationally prohibitive due to the exponentially growing number of trainable parameters, easily exceeding the parameter size of ChatGPT, which is over 1 trillion. INN addresses this challenge by leveraging tensor decomposition and tensor product, with adaptable network architecture.","sentences":["The evolution of artificial intelligence (AI) and neural network theories has revolutionized the way software is programmed, shifting from a hard-coded series of codes to a vast neural network.","However, this transition in engineering software has faced challenges such as data scarcity, multi-modality of data, low model accuracy, and slow inference.","Here, we propose a new network based on interpolation theories and tensor decomposition, the interpolating neural network (INN).","Instead of interpolating training data, a common notion in computer science, INN interpolates interpolation points in the physical space whose coordinates and values are trainable.","It can also extrapolate if the interpolation points reside outside of the range of training data and the interpolation functions have a larger support domain.","INN features orders of magnitude fewer trainable parameters, faster training, a smaller memory footprint, and higher model accuracy compared to feed-forward neural networks (FFNN) or physics-informed neural networks (PINN).","INN is poised to usher in Engineering Software 2.0, a unified neural network that spans various domains of space, time, parameters, and initial/boundary conditions.","This has previously been computationally prohibitive due to the exponentially growing number of trainable parameters, easily exceeding the parameter size of ChatGPT, which is over 1 trillion.","INN addresses this challenge by leveraging tensor decomposition and tensor product, with adaptable network architecture."],"url":"http://arxiv.org/abs/2404.10296v1","category":"cs.LG"}
{"created":"2024-04-16 05:29:14","title":"From Data Deluge to Data Curation: A Filtering-WoRA Paradigm for Efficient Text-based Person Search","abstract":"In text-based person search endeavors, data generation has emerged as a prevailing practice, addressing concerns over privacy preservation and the arduous task of manual annotation. Although the number of synthesized data can be infinite in theory, the scientific conundrum persists that how much generated data optimally fuels subsequent model training. We observe that only a subset of the data in these constructed datasets plays a decisive role. Therefore, we introduce a new Filtering-WoRA paradigm, which contains a filtering algorithm to identify this crucial data subset and WoRA (Weighted Low-Rank Adaptation) learning strategy for light fine-tuning. The filtering algorithm is based on the cross-modality relevance to remove the lots of coarse matching synthesis pairs. As the number of data decreases, we do not need to fine-tune the entire model. Therefore, we propose a WoRA learning strategy to efficiently update a minimal portion of model parameters. WoRA streamlines the learning process, enabling heightened efficiency in extracting knowledge from fewer, yet potent, data instances. Extensive experimentation validates the efficacy of pretraining, where our model achieves advanced and efficient retrieval performance on challenging real-world benchmarks. Notably, on the CUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing model training time by 19.82%.","sentences":["In text-based person search endeavors, data generation has emerged as a prevailing practice, addressing concerns over privacy preservation and the arduous task of manual annotation.","Although the number of synthesized data can be infinite in theory, the scientific conundrum persists that how much generated data optimally fuels subsequent model training.","We observe that only a subset of the data in these constructed datasets plays a decisive role.","Therefore, we introduce a new Filtering-WoRA paradigm, which contains a filtering algorithm to identify this crucial data subset and WoRA (Weighted Low-Rank Adaptation) learning strategy for light fine-tuning.","The filtering algorithm is based on the cross-modality relevance to remove the lots of coarse matching synthesis pairs.","As the number of data decreases, we do not need to fine-tune the entire model.","Therefore, we propose a WoRA learning strategy to efficiently update a minimal portion of model parameters.","WoRA streamlines the learning process, enabling heightened efficiency in extracting knowledge from fewer, yet potent, data instances.","Extensive experimentation validates the efficacy of pretraining, where our model achieves advanced and efficient retrieval performance on challenging real-world benchmarks.","Notably, on the CUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing model training time by 19.82%."],"url":"http://arxiv.org/abs/2404.10292v1","category":"cs.CV"}
{"created":"2024-04-16 05:20:53","title":"Inducing spectral gaps for the cohomological Laplacians of $\\operatorname{SL}_n(\\mathbb{Z})$ and $\\operatorname{SAut}(F_n)$","abstract":"The technique of inducing spectral gaps for cohomological Laplacians in degree zero was used by Kaluba, Kielak and Nowak to prove property (T) for $\\operatorname{SAut}(F_n)$ and $\\operatorname{SL}_n(\\mathbb{Z})$. In this paper, we adapt this technique to Laplacians in degree one. This allows to provide a lower bound for the cohomological Laplacian in degree one for $\\operatorname{SL}_n(\\mathbb{Z})$ for every unitary representation. In particular, one gets in that way an alternative proof of property (T) for $\\operatorname{SL}_n(\\mathbb{Z})$ whenever $n\\geq 3$.","sentences":["The technique of inducing spectral gaps for cohomological Laplacians in degree zero was used by Kaluba, Kielak and Nowak to prove property (T) for $\\operatorname{SAut}(F_n)$ and $\\operatorname{SL}_n(\\mathbb{Z})$. In this paper, we adapt this technique to Laplacians in degree one.","This allows to provide a lower bound for the cohomological Laplacian in degree one for $\\operatorname{SL}_n(\\mathbb{Z})$ for every unitary representation.","In particular, one gets in that way an alternative proof of property (T) for $\\operatorname{SL}_n(\\mathbb{Z})$ whenever $n\\geq 3$."],"url":"http://arxiv.org/abs/2404.10287v2","category":"math.GR"}
{"created":"2024-04-16 05:20:30","title":"A novel scheme for modelling dissipation or thermalization in open quantum systems","abstract":"In this letter, we introduce a novel method for investigating dissipation or thermalization in an open quantum system. In this method, the quantum system is coupled linearly with a copy of itself or with another system described by a finite number of bosonic operators. The time-dependent coupling functions play a fundamental role in this scheme. To demonstrate the efficacy and significance of this method, we apply it to examine several important and ubiquitous open quantum systems. Firstly, we investigate a quantum oscillator in the presence of a thermal bath at the inverse temperature $\\beta$, obtaining the reduced density matrix, the Husimi distribution function, and the quantum heat distribution function accurately. The results are consistent with existing literature by appropriate choices for the time-dependent coupling function. To illustrate the generalizability of this method to systems interacting with multiple thermal baths, we study the interaction of a quantum oscillator with two thermal baths at different temperatures and obtain compatible results. Subsequently, we analyze a two-level atom with energy or phase dissipation and derive the spontaneous emission and the pure dephasing processes consistently using the new method. Finally, we investigate Markovian and non-Markovian processes in a dissipative two-level atom and observe that these processes depend on the coupling strength $g_0$, and the non-Markovian property increases with an increase in $g_0$.","sentences":["In this letter, we introduce a novel method for investigating dissipation or thermalization in an open quantum system.","In this method, the quantum system is coupled linearly with a copy of itself or with another system described by a finite number of bosonic operators.","The time-dependent coupling functions play a fundamental role in this scheme.","To demonstrate the efficacy and significance of this method, we apply it to examine several important and ubiquitous open quantum systems.","Firstly, we investigate a quantum oscillator in the presence of a thermal bath at the inverse temperature $\\beta$, obtaining the reduced density matrix, the Husimi distribution function, and the quantum heat distribution function accurately.","The results are consistent with existing literature by appropriate choices for the time-dependent coupling function.","To illustrate the generalizability of this method to systems interacting with multiple thermal baths, we study the interaction of a quantum oscillator with two thermal baths at different temperatures and obtain compatible results.","Subsequently, we analyze a two-level atom with energy or phase dissipation and derive the spontaneous emission and the pure dephasing processes consistently using the new method.","Finally, we investigate Markovian and non-Markovian processes in a dissipative two-level atom and observe that these processes depend on the coupling strength $g_0$, and the non-Markovian property increases with an increase in $g_0$."],"url":"http://arxiv.org/abs/2404.10286v1","category":"quant-ph"}
{"created":"2024-04-16 04:52:41","title":"Tripod: Three Complementary Inductive Biases for Disentangled Representation Learning","abstract":"Inductive biases are crucial in disentangled representation learning for narrowing down an underspecified solution set. In this work, we consider endowing a neural network autoencoder with three select inductive biases from the literature: data compression into a grid-like latent space via quantization, collective independence amongst latents, and minimal functional influence of any latent on how other latents determine data generation. In principle, these inductive biases are deeply complementary: they most directly specify properties of the latent space, encoder, and decoder, respectively. In practice, however, naively combining existing techniques instantiating these inductive biases fails to yield significant benefits. To address this, we propose adaptations to the three techniques that simplify the learning problem, equip key regularization terms with stabilizing invariances, and quash degenerate incentives. The resulting model, Tripod, achieves state-of-the-art results on a suite of four image disentanglement benchmarks. We also verify that Tripod significantly improves upon its naive incarnation and that all three of its \"legs\" are necessary for best performance.","sentences":["Inductive biases are crucial in disentangled representation learning for narrowing down an underspecified solution set.","In this work, we consider endowing a neural network autoencoder with three select inductive biases from the literature: data compression into a grid-like latent space via quantization, collective independence amongst latents, and minimal functional influence of any latent on how other latents determine data generation.","In principle, these inductive biases are deeply complementary: they most directly specify properties of the latent space, encoder, and decoder, respectively.","In practice, however, naively combining existing techniques instantiating these inductive biases fails to yield significant benefits.","To address this, we propose adaptations to the three techniques that simplify the learning problem, equip key regularization terms with stabilizing invariances, and quash degenerate incentives.","The resulting model, Tripod, achieves state-of-the-art results on a suite of four image disentanglement benchmarks.","We also verify that Tripod significantly improves upon its naive incarnation and that all three of its \"legs\" are necessary for best performance."],"url":"http://arxiv.org/abs/2404.10282v1","category":"cs.LG"}
{"created":"2024-04-16 03:34:35","title":"PreGSU-A Generalized Traffic Scene Understanding Model for Autonomous Driving based on Pre-trained Graph Attention Network","abstract":"Scene understanding, defined as learning, extraction, and representation of interactions among traffic elements, is one of the critical challenges toward high-level autonomous driving (AD). Current scene understanding methods mainly focus on one concrete single task, such as trajectory prediction and risk level evaluation. Although they perform well on specific metrics, the generalization ability is insufficient to adapt to the real traffic complexity and downstream demand diversity. In this study, we propose PreGSU, a generalized pre-trained scene understanding model based on graph attention network to learn the universal interaction and reasoning of traffic scenes to support various downstream tasks. After the feature engineering and sub-graph module, all elements are embedded as nodes to form a dynamic weighted graph. Then, four graph attention layers are applied to learn the relationships among agents and lanes. In the pre-train phase, the understanding model is trained on two self-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road Modeling (MRM). Based on the artificial potential field theory, VIF modeling enables PreGSU to capture the agent-to-agent interactions while MRM extracts agent-to-road connections. In the fine-tuning process, the pre-trained parameters are loaded to derive detailed understanding outputs. We conduct validation experiments on two downstream tasks, i.e., trajectory prediction in urban scenario, and intention recognition in highway scenario, to verify the generalized ability and understanding ability. Results show that compared with the baselines, PreGSU achieves better accuracy on both tasks, indicating the potential to be generalized to various scenes and targets. Ablation study shows the effectiveness of pre-train task design.","sentences":["Scene understanding, defined as learning, extraction, and representation of interactions among traffic elements, is one of the critical challenges toward high-level autonomous driving (AD).","Current scene understanding methods mainly focus on one concrete single task, such as trajectory prediction and risk level evaluation.","Although they perform well on specific metrics, the generalization ability is insufficient to adapt to the real traffic complexity and downstream demand diversity.","In this study, we propose PreGSU, a generalized pre-trained scene understanding model based on graph attention network to learn the universal interaction and reasoning of traffic scenes to support various downstream tasks.","After the feature engineering and sub-graph module, all elements are embedded as nodes to form a dynamic weighted graph.","Then, four graph attention layers are applied to learn the relationships among agents and lanes.","In the pre-train phase, the understanding model is trained on two self-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road Modeling (MRM).","Based on the artificial potential field theory, VIF modeling enables PreGSU to capture the agent-to-agent interactions while MRM extracts agent-to-road connections.","In the fine-tuning process, the pre-trained parameters are loaded to derive detailed understanding outputs.","We conduct validation experiments on two downstream tasks, i.e., trajectory prediction in urban scenario, and intention recognition in highway scenario, to verify the generalized ability and understanding ability.","Results show that compared with the baselines, PreGSU achieves better accuracy on both tasks, indicating the potential to be generalized to various scenes and targets.","Ablation study shows the effectiveness of pre-train task design."],"url":"http://arxiv.org/abs/2404.10263v1","category":"cs.CV"}
{"created":"2024-04-16 03:31:28","title":"Lighter, Better, Faster Multi-Source Domain Adaptation with Gaussian Mixture Models and Optimal Transport","abstract":"In this paper, we tackle Multi-Source Domain Adaptation (MSDA), a task in transfer learning where one adapts multiple heterogeneous, labeled source probability measures towards a different, unlabeled target measure. We propose a novel framework for MSDA, based on Optimal Transport (OT) and Gaussian Mixture Models (GMMs). Our framework has two key advantages. First, OT between GMMs can be solved efficiently via linear programming. Second, it provides a convenient model for supervised learning, especially classification, as components in the GMM can be associated with existing classes. Based on the GMM-OT problem, we propose a novel technique for calculating barycenters of GMMs. Based on this novel algorithm, we propose two new strategies for MSDA: GMM-WBT and GMM-DaDiL. We empirically evaluate our proposed methods on four benchmarks in image classification and fault diagnosis, showing that we improve over the prior art while being faster and involving fewer parameters.","sentences":["In this paper, we tackle Multi-Source Domain Adaptation (MSDA), a task in transfer learning where one adapts multiple heterogeneous, labeled source probability measures towards a different, unlabeled target measure.","We propose a novel framework for MSDA, based on Optimal Transport (OT) and Gaussian Mixture Models (GMMs).","Our framework has two key advantages.","First, OT between GMMs can be solved efficiently via linear programming.","Second, it provides a convenient model for supervised learning, especially classification, as components in the GMM can be associated with existing classes.","Based on the GMM-OT problem, we propose a novel technique for calculating barycenters of GMMs.","Based on this novel algorithm, we propose two new strategies for MSDA: GMM-WBT and GMM-DaDiL. We empirically evaluate our proposed methods on four benchmarks in image classification and fault diagnosis, showing that we improve over the prior art while being faster and involving fewer parameters."],"url":"http://arxiv.org/abs/2404.10261v1","category":"stat.ML"}
{"created":"2024-04-16 03:26:43","title":"Uncovering Latent Arguments in Social Media Messaging by Employing LLMs-in-the-Loop Strategy","abstract":"The widespread use of social media has led to a surge in popularity for automated methods of analyzing public opinion. Supervised methods are adept at text categorization, yet the dynamic nature of social media discussions poses a continual challenge for these techniques due to the constant shifting of the focus. On the other hand, traditional unsupervised methods for extracting themes from public discourse, such as topic modeling, often reveal overarching patterns that might not capture specific nuances. Consequently, a significant portion of research into social media discourse still depends on labor-intensive manual coding techniques and a human-in-the-loop approach, which are both time-consuming and costly. In this work, we study the problem of discovering arguments associated with a specific theme. We propose a generic LLMs-in-the-Loop strategy that leverages the advanced capabilities of Large Language Models (LLMs) to extract latent arguments from social media messaging. To demonstrate our approach, we apply our framework to contentious topics. We use two publicly available datasets: (1) the climate campaigns dataset of 14k Facebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads with 14 themes. Furthermore, we analyze demographic targeting and the adaptation of messaging based on real-world events.","sentences":["The widespread use of social media has led to a surge in popularity for automated methods of analyzing public opinion.","Supervised methods are adept at text categorization, yet the dynamic nature of social media discussions poses a continual challenge for these techniques due to the constant shifting of the focus.","On the other hand, traditional unsupervised methods for extracting themes from public discourse, such as topic modeling, often reveal overarching patterns that might not capture specific nuances.","Consequently, a significant portion of research into social media discourse still depends on labor-intensive manual coding techniques and a human-in-the-loop approach, which are both time-consuming and costly.","In this work, we study the problem of discovering arguments associated with a specific theme.","We propose a generic LLMs-in-the-Loop strategy that leverages the advanced capabilities of Large Language Models (LLMs) to extract latent arguments from social media messaging.","To demonstrate our approach, we apply our framework to contentious topics.","We use two publicly available datasets: (1) the climate campaigns dataset of 14k Facebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads with 14 themes.","Furthermore, we analyze demographic targeting and the adaptation of messaging based on real-world events."],"url":"http://arxiv.org/abs/2404.10259v1","category":"cs.CL"}
{"created":"2024-04-16 03:09:40","title":"Kilometer-Level Coupled Modeling Using 40 Million Cores: An Eight-Year Journey of Model Development","abstract":"With current and future leading systems adopting heterogeneous architectures, adapting existing models for heterogeneous supercomputers is of urgent need for improving model resolution and reducing modeling uncertainty. This paper presents our three-week effort on porting a complex earth system model, CESM 2.2, to a 40-million-core Sunway supercomputer. Taking a non-intrusive approach that tries to minimizes manual code modifications, our project tries to achieve both improvement of performance and consistency of the model code. By using a hierarchical grid system and an OpenMP-based offloading toolkit, our porting and parallelization effort covers over 80% of the code, and achieves a simulation speed of 340 SDPD (simulated days per day) for 5-km atmosphere, 265 SDPD for 3-km ocean, and 222 SDPD for a coupled model, thus making multi-year or even multi-decadal experiments at such high resolution possible.","sentences":["With current and future leading systems adopting heterogeneous architectures, adapting existing models for heterogeneous supercomputers is of urgent need for improving model resolution and reducing modeling uncertainty.","This paper presents our three-week effort on porting a complex earth system model, CESM 2.2, to a 40-million-core Sunway supercomputer.","Taking a non-intrusive approach that tries to minimizes manual code modifications, our project tries to achieve both improvement of performance and consistency of the model code.","By using a hierarchical grid system and an OpenMP-based offloading toolkit, our porting and parallelization effort covers over 80% of the code, and achieves a simulation speed of 340 SDPD (simulated days per day) for 5-km atmosphere, 265 SDPD for 3-km ocean, and 222 SDPD for a coupled model, thus making multi-year or even multi-decadal experiments at such high resolution possible."],"url":"http://arxiv.org/abs/2404.10253v1","category":"cs.DC"}
{"created":"2024-04-16 03:08:02","title":"Learning from Offline and Online Experiences: A Hybrid Adaptive Operator Selection Framework","abstract":"In many practical applications, usually, similar optimisation problems or scenarios repeatedly appear. Learning from previous problem-solving experiences can help adjust algorithm components of meta-heuristics, e.g., adaptively selecting promising search operators, to achieve better optimisation performance. However, those experiences obtained from previously solved problems, namely offline experiences, may sometimes provide misleading perceptions when solving a new problem, if the characteristics of previous problems and the new one are relatively different. Learning from online experiences obtained during the ongoing problem-solving process is more instructive but highly restricted by limited computational resources. This paper focuses on the effective combination of offline and online experiences. A novel hybrid framework that learns to dynamically and adaptively select promising search operators is proposed. Two adaptive operator selection modules with complementary paradigms cooperate in the framework to learn from offline and online experiences and make decisions. An adaptive decision policy is maintained to balance the use of those two modules in an online manner. Extensive experiments on 170 widely studied real-value benchmark optimisation problems and a benchmark set with 34 instances for combinatorial optimisation show that the proposed hybrid framework outperforms the state-of-the-art methods. Ablation study verifies the effectiveness of each component of the framework.","sentences":["In many practical applications, usually, similar optimisation problems or scenarios repeatedly appear.","Learning from previous problem-solving experiences can help adjust algorithm components of meta-heuristics, e.g., adaptively selecting promising search operators, to achieve better optimisation performance.","However, those experiences obtained from previously solved problems, namely offline experiences, may sometimes provide misleading perceptions when solving a new problem, if the characteristics of previous problems and the new one are relatively different.","Learning from online experiences obtained during the ongoing problem-solving process is more instructive but highly restricted by limited computational resources.","This paper focuses on the effective combination of offline and online experiences.","A novel hybrid framework that learns to dynamically and adaptively select promising search operators is proposed.","Two adaptive operator selection modules with complementary paradigms cooperate in the framework to learn from offline and online experiences and make decisions.","An adaptive decision policy is maintained to balance the use of those two modules in an online manner.","Extensive experiments on 170 widely studied real-value benchmark optimisation problems and a benchmark set with 34 instances for combinatorial optimisation show that the proposed hybrid framework outperforms the state-of-the-art methods.","Ablation study verifies the effectiveness of each component of the framework."],"url":"http://arxiv.org/abs/2404.10252v1","category":"cs.NE"}
{"created":"2024-04-16 02:46:07","title":"An adaptive Euler-Maruyama scheme for SDDEs","abstract":"This paper proposes an adaptive numerical method for stochastic delay differential equations (SDDEs) with a non-global Lipschitz drift term and a non-constant delay, building upon the work of Wei Fang and others. The method adapts the step size based on the growth of the drift term. Differing slightly from the conventional Euler-Maruyama format, this paper addresses the estimation of the delay term by substituting it with the numerically obtained solution closest to the left endpoint.This approach overcomes the challenge of numerical nodes not falling within the nodes after subtracting the delay. The paper proves the convergence of the numerical method for a class of non-global Lipschitz continuous SDDEs under the assumption that the step size function satisfies certain conditions.","sentences":["This paper proposes an adaptive numerical method for stochastic delay differential equations (SDDEs) with a non-global Lipschitz drift term and a non-constant delay, building upon the work of Wei Fang and others.","The method adapts the step size based on the growth of the drift term.","Differing slightly from the conventional Euler-Maruyama format, this paper addresses the estimation of the delay term by substituting it with the numerically obtained solution closest to the left endpoint.","This approach overcomes the challenge of numerical nodes not falling within the nodes after subtracting the delay.","The paper proves the convergence of the numerical method for a class of non-global Lipschitz continuous SDDEs under the assumption that the step size function satisfies certain conditions."],"url":"http://arxiv.org/abs/2404.10244v1","category":"math.NA"}
{"created":"2024-04-16 02:29:00","title":"Compressible and Searchable: AI-native Multi-Modal Retrieval System with Learned Image Compression","abstract":"The burgeoning volume of digital content across diverse modalities necessitates efficient storage and retrieval methods. Conventional approaches struggle to cope with the escalating complexity and scale of multimedia data. In this paper, we proposed framework addresses this challenge by fusing AI-native multi-modal search capabilities with neural image compression. First we analyze the intricate relationship between compressibility and searchability, recognizing the pivotal role each plays in the efficiency of storage and retrieval systems. Through the usage of simple adapter is to bridge the feature of Learned Image Compression(LIC) and Contrastive Language-Image Pretraining(CLIP) while retaining semantic fidelity and retrieval of multi-modal data. Experimental evaluations on Kodak datasets demonstrate the efficacy of our approach, showcasing significant enhancements in compression efficiency and search accuracy compared to existing methodologies. Our work marks a significant advancement towards scalable and efficient multi-modal search systems in the era of big data.","sentences":["The burgeoning volume of digital content across diverse modalities necessitates efficient storage and retrieval methods.","Conventional approaches struggle to cope with the escalating complexity and scale of multimedia data.","In this paper, we proposed framework addresses this challenge by fusing AI-native multi-modal search capabilities with neural image compression.","First we analyze the intricate relationship between compressibility and searchability, recognizing the pivotal role each plays in the efficiency of storage and retrieval systems.","Through the usage of simple adapter is to bridge the feature of Learned Image Compression(LIC) and Contrastive Language-Image Pretraining(CLIP) while retaining semantic fidelity and retrieval of multi-modal data.","Experimental evaluations on Kodak datasets demonstrate the efficacy of our approach, showcasing significant enhancements in compression efficiency and search accuracy compared to existing methodologies.","Our work marks a significant advancement towards scalable and efficient multi-modal search systems in the era of big data."],"url":"http://arxiv.org/abs/2404.10234v1","category":"cs.AI"}
{"created":"2024-04-16 02:25:12","title":"Urban Water Sprinkler Routing: A Multi-Depot Mixed Capacitated Arc Routing Problem Incorporating Real-Time Demands","abstract":"Fugitive road dust (FRD), as one of the major pollutants in the city, poses great harm to the environment and the physical health of citizens. A common countermeasure adopted by government agencies is employing on-road water trucks (sprinklers) to spray water (sprinkle) on urban streets to reduce the FRD. Currently, the traveling routes of sprinklers are usually planned based on drivers' experience, which may lead low operation efficiency and could not respond to the real-time sprinkling demands. To address these issues, this study formulates the routes planning of sprinklers as a multi-depot mixed capacitated arc routing problem with real-time demands with the aim of minimizing the sprinklers' travel distance. We develop an improved adaptive large neighborhood search (ALNS) algorithm that incorporates a tabu-list and a perturbation mechanism to solve this problem. Furthermore, a problem-specific acceleration mechanism is designed to reduce unnecessary search domains to improve the efficiency of the algorithm. Empirical experiments are conducted based on various scenarios and the results demonstrate that the proposed algorithm generates solutions that are superior or at least comparable to the solutions generated by the traditional ALNS algorithm but with significantly lower computation time. Sensitivity analysis is conducted to explore the effects of relevant parameters on the results. This study is the first to incorporate real-time FRD pollution information, gathered through multiple data sources via IoT technology, into urban sprinkling operations, extending the traditional CARP from a tactical planning to a real-time operational environment. A real-world implementation case is also presented.","sentences":["Fugitive road dust (FRD), as one of the major pollutants in the city, poses great harm to the environment and the physical health of citizens.","A common countermeasure adopted by government agencies is employing on-road water trucks (sprinklers) to spray water (sprinkle) on urban streets to reduce the FRD.","Currently, the traveling routes of sprinklers are usually planned based on drivers' experience, which may lead low operation efficiency and could not respond to the real-time sprinkling demands.","To address these issues, this study formulates the routes planning of sprinklers as a multi-depot mixed capacitated arc routing problem with real-time demands with the aim of minimizing the sprinklers' travel distance.","We develop an improved adaptive large neighborhood search (ALNS) algorithm that incorporates a tabu-list and a perturbation mechanism to solve this problem.","Furthermore, a problem-specific acceleration mechanism is designed to reduce unnecessary search domains to improve the efficiency of the algorithm.","Empirical experiments are conducted based on various scenarios and the results demonstrate that the proposed algorithm generates solutions that are superior or at least comparable to the solutions generated by the traditional ALNS algorithm but with significantly lower computation time.","Sensitivity analysis is conducted to explore the effects of relevant parameters on the results.","This study is the first to incorporate real-time FRD pollution information, gathered through multiple data sources via IoT technology, into urban sprinkling operations, extending the traditional CARP from a tactical planning to a real-time operational environment.","A real-world implementation case is also presented."],"url":"http://arxiv.org/abs/2404.10230v1","category":"math.OC"}
{"created":"2024-04-16 02:01:56","title":"Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V","abstract":"Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. We present COME-robot, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. We meticulously construct a library of action primitives for robot exploration, navigation, and manipulation, serving as callable execution modules for GPT-4V in task planning. On top of these modules, GPT-4V serves as the brain that can accomplish multimodal reasoning, generate action policy with code, verify the task progress, and provide feedback for replanning. Such design enables COME-robot to (i) actively perceive the environments, (ii) perform situated reasoning, and (iii) recover from failures. Through comprehensive experiments involving 8 challenging real-world tabletop and manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~25%) compared to state-of-the-art baseline methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.","sentences":["Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback.","We present COME-robot, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios.","We meticulously construct a library of action primitives for robot exploration, navigation, and manipulation, serving as callable execution modules for GPT-4V in task planning.","On top of these modules, GPT-4V serves as the brain that can accomplish multimodal reasoning, generate action policy with code, verify the task progress, and provide feedback for replanning.","Such design enables COME-robot to (i) actively perceive the environments, (ii) perform situated reasoning, and (iii) recover from failures.","Through comprehensive experiments involving 8 challenging real-world tabletop and manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~25%) compared to state-of-the-art baseline methods.","We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning."],"url":"http://arxiv.org/abs/2404.10220v1","category":"cs.RO"}
{"created":"2024-04-16 01:59:03","title":"Autonomous Implicit Indoor Scene Reconstruction with Frontier Exploration","abstract":"Implicit neural representations have demonstrated significant promise for 3D scene reconstruction. Recent works have extended their applications to autonomous implicit reconstruction through the Next Best View (NBV) based method. However, the NBV method cannot guarantee complete scene coverage and often necessitates extensive viewpoint sampling, particularly in complex scenes. In the paper, we propose to 1) incorporate frontier-based exploration tasks for global coverage with implicit surface uncertainty-based reconstruction tasks to achieve high-quality reconstruction. and 2) introduce a method to achieve implicit surface uncertainty using color uncertainty, which reduces the time needed for view selection. Further with these two tasks, we propose an adaptive strategy for switching modes in view path planning, to reduce time and maintain superior reconstruction quality. Our method exhibits the highest reconstruction quality among all planning methods and superior planning efficiency in methods involving reconstruction tasks. We deploy our method on a UAV and the results show that our method can plan multi-task views and reconstruct a scene with high quality.","sentences":["Implicit neural representations have demonstrated significant promise for 3D scene reconstruction.","Recent works have extended their applications to autonomous implicit reconstruction through the Next Best View (NBV) based method.","However, the NBV method cannot guarantee complete scene coverage and often necessitates extensive viewpoint sampling, particularly in complex scenes.","In the paper, we propose to 1) incorporate frontier-based exploration tasks for global coverage with implicit surface uncertainty-based reconstruction tasks to achieve high-quality reconstruction.","and 2) introduce a method to achieve implicit surface uncertainty using color uncertainty, which reduces the time needed for view selection.","Further with these two tasks, we propose an adaptive strategy for switching modes in view path planning, to reduce time and maintain superior reconstruction quality.","Our method exhibits the highest reconstruction quality among all planning methods and superior planning efficiency in methods involving reconstruction tasks.","We deploy our method on a UAV and the results show that our method can plan multi-task views and reconstruct a scene with high quality."],"url":"http://arxiv.org/abs/2404.10218v1","category":"cs.RO"}
{"created":"2024-04-16 01:45:18","title":"Anomaly Correction of Business Processes Using Transformer Autoencoder","abstract":"Event log records all events that occur during the execution of business processes, so detecting and correcting anomalies in event log can provide reliable guarantee for subsequent process analysis. The previous works mainly include next event prediction based methods and autoencoder-based methods. These methods cannot accurately and efficiently detect anomalies and correct anomalies at the same time, and they all rely on the set threshold to detect anomalies. To solve these problems, we propose a business process anomaly correction method based on Transformer autoencoder. By using self-attention mechanism and autoencoder structure, it can efficiently process event sequences of arbitrary length, and can directly output corrected business process instances, so that it can adapt to various scenarios. At the same time, the anomaly detection is transformed into a classification problem by means of selfsupervised learning, so that there is no need to set a specific threshold in anomaly detection. The experimental results on several real-life event logs show that the proposed method is superior to the previous methods in terms of anomaly detection accuracy and anomaly correction results while ensuring high running efficiency.","sentences":["Event log records all events that occur during the execution of business processes, so detecting and correcting anomalies in event log can provide reliable guarantee for subsequent process analysis.","The previous works mainly include next event prediction based methods and autoencoder-based methods.","These methods cannot accurately and efficiently detect anomalies and correct anomalies at the same time, and they all rely on the set threshold to detect anomalies.","To solve these problems, we propose a business process anomaly correction method based on Transformer autoencoder.","By using self-attention mechanism and autoencoder structure, it can efficiently process event sequences of arbitrary length, and can directly output corrected business process instances, so that it can adapt to various scenarios.","At the same time, the anomaly detection is transformed into a classification problem by means of selfsupervised learning, so that there is no need to set a specific threshold in anomaly detection.","The experimental results on several real-life event logs show that the proposed method is superior to the previous methods in terms of anomaly detection accuracy and anomaly correction results while ensuring high running efficiency."],"url":"http://arxiv.org/abs/2404.10211v1","category":"cs.LG"}
{"created":"2024-04-16 00:29:04","title":"Strong Markov dissipation in driven-dissipative quantum systems","abstract":"The Lindblad equation, which describes Markovian quantum dynamics under dissipation, is usually derived under the weak system-bath coupling assumption. Strong system-bath coupling often leads to non-Markov evolution. The singular-coupling limit is known as an exception: it yields a Lindblad equation with an arbitrary strength of dissipation. However, the singular-coupling limit requires high-temperature limit of the bath, and hence the system ends up in a trivial infinite-temperature state, which is not desirable in the context of quantum control. In this work, it is shown that we can derive a Markovian Lindblad equation for an arbitrary strength of the system-bath coupling by considering a new scaling limit that is called the singular-driving limit, which combines the singular-coupling limit and fast periodic driving. In contrast to the standard singular-coupling limit, an interplay between dissipation and periodic driving results in a nontrivial steady state.","sentences":["The Lindblad equation, which describes Markovian quantum dynamics under dissipation, is usually derived under the weak system-bath coupling assumption.","Strong system-bath coupling often leads to non-Markov evolution.","The singular-coupling limit is known as an exception: it yields a Lindblad equation with an arbitrary strength of dissipation.","However, the singular-coupling limit requires high-temperature limit of the bath, and hence the system ends up in a trivial infinite-temperature state, which is not desirable in the context of quantum control.","In this work, it is shown that we can derive a Markovian Lindblad equation for an arbitrary strength of the system-bath coupling by considering a new scaling limit that is called the singular-driving limit, which combines the singular-coupling limit and fast periodic driving.","In contrast to the standard singular-coupling limit, an interplay between dissipation and periodic driving results in a nontrivial steady state."],"url":"http://arxiv.org/abs/2404.10195v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-16 00:15:33","title":"Holographic Renormalization Group and Stress Tensor Operators","abstract":"The holographic duality conjectures a relation between strongly coupled quantum systems and quantum gravity in higher-dimensional spacetimes. Gravitational theories in two and three dimensions are meaningful examples for classical and quantum exploration due to their unique characteristics, notably the absence of propagating bulk degrees of freedom and the presence of only boundary degrees of freedom, distinguishing them from higher-dimensional counterparts. These gravitational theories exhibit complex interactions when the bulk spacetime has a finite size, regulated by Zamolodchikov's double-trace irrelevant $T\\overline{T}$ operator.   This thesis aims to gain a holographic understanding of $\\mathrm{AdS}_3$ and JT gravity under the influence of the $T\\overline{T}$ deformation. Under a finite radial cutoff, these theories exhibit perturbative behavior that implies the emergence of the Nambu-Goto action for the corresponding boundary graviton action. We also conducted semi-classical calculations of observables related to finite-cutoff gravity and its dual $T\\overline{T}$-deformed CFT description, including correlation functions involving stress tensors and gravitational Wilson lines, along with an analysis of their supersymmetric extensions. Additionally, we explored the implications of general stress tensor deformations within field-theoretic and holographic settings.   This thesis integrates previously adapted publications while also pioneering new ground, notably exploring the definition of a quantum $T\\overline{T}$ operator beyond two dimensions with $\\frac{1}{N}$ corrections, investigating quantum-corrected higher point correlators for a planar boundary, and offering insights into a two-dimensional spherical boundary at a finite cutoff. Furthermore, throughout the thesis, we show more details in calculations at various points.","sentences":["The holographic duality conjectures a relation between strongly coupled quantum systems and quantum gravity in higher-dimensional spacetimes.","Gravitational theories in two and three dimensions are meaningful examples for classical and quantum exploration due to their unique characteristics, notably the absence of propagating bulk degrees of freedom and the presence of only boundary degrees of freedom, distinguishing them from higher-dimensional counterparts.","These gravitational theories exhibit complex interactions when the bulk spacetime has a finite size, regulated by Zamolodchikov's double-trace irrelevant $T\\overline{T}$ operator.   ","This thesis aims to gain a holographic understanding of $\\mathrm{AdS}_3$ and JT gravity under the influence of the $T\\overline{T}$ deformation.","Under a finite radial cutoff, these theories exhibit perturbative behavior that implies the emergence of the Nambu-Goto action for the corresponding boundary graviton action.","We also conducted semi-classical calculations of observables related to finite-cutoff gravity and its dual $T\\overline{T}$-deformed CFT description, including correlation functions involving stress tensors and gravitational Wilson lines, along with an analysis of their supersymmetric extensions.","Additionally, we explored the implications of general stress tensor deformations within field-theoretic and holographic settings.   ","This thesis integrates previously adapted publications while also pioneering new ground, notably exploring the definition of a quantum $T\\overline{T}$ operator beyond two dimensions with $\\frac{1}{N}$ corrections, investigating quantum-corrected higher point correlators for a planar boundary, and offering insights into a two-dimensional spherical boundary at a finite cutoff.","Furthermore, throughout the thesis, we show more details in calculations at various points."],"url":"http://arxiv.org/abs/2404.10190v1","category":"hep-th"}
{"created":"2024-04-15 23:54:56","title":"General theory for longitudinal nonreciprocal charge transport","abstract":"The longitudinal nonreciprocal charge transport (NCT) in crystalline materials is a highly non-trivial phenomenon, motivating the design of next generation two-terminal rectification devices (e.g., semiconductor diodes beyond PN junctions). The practical application of such devices is built upon crystalline materials whose longitudinal NCT occurs at room temperature and under low magnetic field. However, materials of this type are rather rare and elusive, and theory guiding the discovery of these materials is lacking. Here, we develop such a theory within the framework of semiclassical Boltzmann transport theory. By symmetry analysis, we classify the complete 122 magnetic point groups with respect to the longitudinal NCT phenomenon. The symmetry-adapted Hamiltonian analysis further uncovers a previously overlooked mechanism for this phenomenon. Our theory guides the first-principles prediction of longitudinal NCT in multiferroic \\epsilon-Fe2O3 semiconductor that possibly occurs at room temperature, without the application of external magnetic field. These findings advance our fundamental understandings of longitudinal NCT in crystalline materials, and aid the corresponding materials discoveries.","sentences":["The longitudinal nonreciprocal charge transport (NCT) in crystalline materials is a highly non-trivial phenomenon, motivating the design of next generation two-terminal rectification devices (e.g., semiconductor diodes beyond PN junctions).","The practical application of such devices is built upon crystalline materials whose longitudinal NCT occurs at room temperature and under low magnetic field.","However, materials of this type are rather rare and elusive, and theory guiding the discovery of these materials is lacking.","Here, we develop such a theory within the framework of semiclassical Boltzmann transport theory.","By symmetry analysis, we classify the complete 122 magnetic point groups with respect to the longitudinal NCT phenomenon.","The symmetry-adapted Hamiltonian analysis further uncovers a previously overlooked mechanism for this phenomenon.","Our theory guides the first-principles prediction of longitudinal NCT in multiferroic \\epsilon-Fe2O3 semiconductor that possibly occurs at room temperature, without the application of external magnetic field.","These findings advance our fundamental understandings of longitudinal NCT in crystalline materials, and aid the corresponding materials discoveries."],"url":"http://arxiv.org/abs/2404.10186v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 22:13:35","title":"Salient Object-Aware Background Generation using Text-Guided Diffusion Models","abstract":"Generating background scenes for salient objects plays a crucial role across various domains including creative design and e-commerce, as it enhances the presentation and context of subjects by integrating them into tailored environments. Background generation can be framed as a task of text-conditioned outpainting, where the goal is to extend image content beyond a salient object's boundaries on a blank background. Although popular diffusion models for text-guided inpainting can also be used for outpainting by mask inversion, they are trained to fill in missing parts of an image rather than to place an object into a scene. Consequently, when used for background creation, inpainting models frequently extend the salient object's boundaries and thereby change the object's identity, which is a phenomenon we call \"object expansion.\" This paper introduces a model for adapting inpainting diffusion models to the salient object outpainting task using Stable Diffusion and ControlNet architectures. We present a series of qualitative and quantitative results across models and datasets, including a newly proposed metric to measure object expansion that does not require any human labeling. Compared to Stable Diffusion 2.0 Inpainting, our proposed approach reduces object expansion by 3.6x on average with no degradation in standard visual metrics across multiple datasets.","sentences":["Generating background scenes for salient objects plays a crucial role across various domains including creative design and e-commerce, as it enhances the presentation and context of subjects by integrating them into tailored environments.","Background generation can be framed as a task of text-conditioned outpainting, where the goal is to extend image content beyond a salient object's boundaries on a blank background.","Although popular diffusion models for text-guided inpainting can also be used for outpainting by mask inversion, they are trained to fill in missing parts of an image rather than to place an object into a scene.","Consequently, when used for background creation, inpainting models frequently extend the salient object's boundaries and thereby change the object's identity, which is a phenomenon we call \"object expansion.\"","This paper introduces a model for adapting inpainting diffusion models to the salient object outpainting task using Stable Diffusion and ControlNet architectures.","We present a series of qualitative and quantitative results across models and datasets, including a newly proposed metric to measure object expansion that does not require any human labeling.","Compared to Stable Diffusion 2.0 Inpainting, our proposed approach reduces object expansion by 3.6x on average with no degradation in standard visual metrics across multiple datasets."],"url":"http://arxiv.org/abs/2404.10157v1","category":"cs.CV"}
{"created":"2024-04-15 21:30:50","title":"Cross-Modal Self-Training: Aligning Images and Pointclouds to Learn Classification without Labels","abstract":"Large-scale vision 2D vision language models, such as CLIP can be aligned with a 3D encoder to learn generalizable (open-vocabulary) 3D vision models. However, current methods require supervised pre-training for such alignment, and the performance of such 3D zero-shot models remains sub-optimal for real-world adaptation. In this work, we propose an optimization framework: Cross-MoST: Cross-Modal Self-Training, to improve the label-free classification performance of a zero-shot 3D vision model by simply leveraging unlabeled 3D data and their accompanying 2D views. We propose a student-teacher framework to simultaneously process 2D views and 3D point clouds and generate joint pseudo labels to train a classifier and guide cross-model feature alignment. Thereby we demonstrate that 2D vision language models such as CLIP can be used to complement 3D representation learning to improve classification performance without the need for expensive class annotations. Using synthetic and real-world 3D datasets, we further demonstrate that Cross-MoST enables efficient cross-modal knowledge exchange resulting in both image and point cloud modalities learning from each other's rich representations.","sentences":["Large-scale vision 2D vision language models, such as CLIP can be aligned with a 3D encoder to learn generalizable (open-vocabulary) 3D vision models.","However, current methods require supervised pre-training for such alignment, and the performance of such 3D zero-shot models remains sub-optimal for real-world adaptation.","In this work, we propose an optimization framework: Cross-MoST: Cross-Modal Self-Training, to improve the label-free classification performance of a zero-shot 3D vision model by simply leveraging unlabeled 3D data and their accompanying 2D views.","We propose a student-teacher framework to simultaneously process 2D views and 3D point clouds and generate joint pseudo labels to train a classifier and guide cross-model feature alignment.","Thereby we demonstrate that 2D vision language models such as CLIP can be used to complement 3D representation learning to improve classification performance without the need for expensive class annotations.","Using synthetic and real-world 3D datasets, we further demonstrate that Cross-MoST enables efficient cross-modal knowledge exchange resulting in both image and point cloud modalities learning from each other's rich representations."],"url":"http://arxiv.org/abs/2404.10146v1","category":"cs.CV"}
{"created":"2024-04-15 21:19:10","title":"ANCHOR: LLM-driven News Subject Conditioning for Text-to-Image Synthesis","abstract":"Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing synthesized image quality, but current datasets evaluate model performance only on descriptive, instruction-based prompts. Real-world news image captions take a more pragmatic approach, providing high-level situational and Named-Entity (NE) information and limited physical object descriptions, making them abstractive. To evaluate the ability of T2I models to capture intended subjects from news captions, we introduce the Abstractive News Captions with High-level cOntext Representation (ANCHOR) dataset, containing 70K+ samples sourced from 5 different news media organizations. With Large Language Models (LLM) achieving success in language and commonsense reasoning tasks, we explore the ability of different LLMs to identify and understand key subjects from abstractive captions. Our proposed method Subject-Aware Finetuning (SAFE), selects and enhances the representation of key subjects in synthesized images by leveraging LLM-generated subject weights. It also adapts to the domain distribution of news images and captions through custom Domain Fine-tuning, outperforming current T2I baselines on ANCHOR. By launching the ANCHOR dataset, we hope to motivate research in furthering the Natural Language Understanding (NLU) capabilities of T2I models.","sentences":["Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing synthesized image quality, but current datasets evaluate model performance only on descriptive, instruction-based prompts.","Real-world news image captions take a more pragmatic approach, providing high-level situational and Named-Entity (NE) information and limited physical object descriptions, making them abstractive.","To evaluate the ability of T2I models to capture intended subjects from news captions, we introduce the Abstractive News Captions with High-level cOntext Representation (ANCHOR) dataset, containing 70K+ samples sourced from 5 different news media organizations.","With Large Language Models (LLM) achieving success in language and commonsense reasoning tasks, we explore the ability of different LLMs to identify and understand key subjects from abstractive captions.","Our proposed method Subject-Aware Finetuning (SAFE), selects and enhances the representation of key subjects in synthesized images by leveraging LLM-generated subject weights.","It also adapts to the domain distribution of news images and captions through custom Domain Fine-tuning, outperforming current T2I baselines on ANCHOR.","By launching the ANCHOR dataset, we hope to motivate research in furthering the Natural Language Understanding (NLU) capabilities of T2I models."],"url":"http://arxiv.org/abs/2404.10141v1","category":"cs.CV"}
{"created":"2024-04-15 20:19:18","title":"Online Estimation via Offline Estimation: An Information-Theoretic Framework","abstract":"$ $The classical theory of statistical estimation aims to estimate a parameter of interest under data generated from a fixed design (\"offline estimation\"), while the contemporary theory of online learning provides algorithms for estimation under adaptively chosen covariates (\"online estimation\"). Motivated by connections between estimation and interactive decision making, we ask: is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion? We investigate this question from an information-theoretic perspective by introducing a new framework, Oracle-Efficient Online Estimation (OEOE), where the learner can only interact with the data stream indirectly through a sequence of offline estimators produced by a black-box algorithm operating on the stream. Our main results settle the statistical and computational complexity of online estimation in this framework.   $\\bullet$ Statistical complexity. We show that information-theoretically, there exist algorithms that achieve near-optimal online estimation error via black-box offline estimation oracles, and give a nearly-tight characterization for minimax rates in the OEOE framework.   $\\bullet$ Computational complexity. We show that the guarantees above cannot be achieved in a computationally efficient fashion in general, but give a refined characterization for the special case of conditional density estimation: computationally efficient online estimation via black-box offline estimation is possible whenever it is possible via unrestricted algorithms.   Finally, we apply our results to give offline oracle-efficient algorithms for interactive decision making.","sentences":["$ $The classical theory of statistical estimation aims to estimate a parameter of interest under data generated from a fixed design (\"offline estimation\"), while the contemporary theory of online learning provides algorithms for estimation under adaptively chosen covariates (\"online estimation\").","Motivated by connections between estimation and interactive decision making, we ask: is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion?","We investigate this question from an information-theoretic perspective by introducing a new framework, Oracle-Efficient Online Estimation (OEOE), where the learner can only interact with the data stream indirectly through a sequence of offline estimators produced by a black-box algorithm operating on the stream.","Our main results settle the statistical and computational complexity of online estimation in this framework.   ","$\\bullet$ Statistical complexity.","We show that information-theoretically, there exist algorithms that achieve near-optimal online estimation error via black-box offline estimation oracles, and give a nearly-tight characterization for minimax rates in the OEOE framework.   ","$\\bullet$ Computational complexity.","We show that the guarantees above cannot be achieved in a computationally efficient fashion in general, but give a refined characterization for the special case of conditional density estimation: computationally efficient online estimation via black-box offline estimation is possible whenever it is possible via unrestricted algorithms.   ","Finally, we apply our results to give offline oracle-efficient algorithms for interactive decision making."],"url":"http://arxiv.org/abs/2404.10122v1","category":"stat.ML"}
{"created":"2024-04-15 20:13:42","title":"Stochastic Thermodynamics at the Quantum-Classical Boundary: A Self-Consistent Framework Based on Adiabatic-Response Theory","abstract":"Microscopic thermal machines promise to play an important role in future quantum technologies. Making such devices widely applicable will require effective strategies to channel their output into easily accessible storage systems like classical degrees of freedom. Here, we develop a self-consistent theoretical framework that makes it possible to model such quantum-classical hybrid devices in a thermodynamically consistent manner. Our approach is based on the assumption that the quantum part of the device is subject to strong decoherence and dissipation induced by a thermal reservoir. Due to the ensuing separation of time scales between slowly evolving classical and fast relaxing quantum degrees of freedom, the dynamics of the hybrid system can be described by means of adiabatic-response theory. We show that, upon including fluctuations in a minimally consistent way, the resulting equations of motion can be equipped with a first and second law, both on the ensemble level and on the level of individual trajectories of the classical part of the system, where thermodynamic quantities like heat and work become stochastic variables. As an application of our theory, we work out a physically transparent model of a quantum-classical hybrid engine, whose working system consists of a chain of Rydberg atoms, which is confined in an optical cavity and driven by periodic temperature variations. We show by means of numerical simulations that the engine can sustain periodic oscillations of a movable mirror, which acts as a classical load, against external friction and extract the full distributions of input heat and output work. By making the statistics of thermodynamic processes in quantum-classical hybrid systems accessible without the need to further specify a measurement protocol, our work contributes towards bridging the long-standing gap between classical and quantum stochastic thermodynamics.","sentences":["Microscopic thermal machines promise to play an important role in future quantum technologies.","Making such devices widely applicable will require effective strategies to channel their output into easily accessible storage systems like classical degrees of freedom.","Here, we develop a self-consistent theoretical framework that makes it possible to model such quantum-classical hybrid devices in a thermodynamically consistent manner.","Our approach is based on the assumption that the quantum part of the device is subject to strong decoherence and dissipation induced by a thermal reservoir.","Due to the ensuing separation of time scales between slowly evolving classical and fast relaxing quantum degrees of freedom, the dynamics of the hybrid system can be described by means of adiabatic-response theory.","We show that, upon including fluctuations in a minimally consistent way, the resulting equations of motion can be equipped with a first and second law, both on the ensemble level and on the level of individual trajectories of the classical part of the system, where thermodynamic quantities like heat and work become stochastic variables.","As an application of our theory, we work out a physically transparent model of a quantum-classical hybrid engine, whose working system consists of a chain of Rydberg atoms, which is confined in an optical cavity and driven by periodic temperature variations.","We show by means of numerical simulations that the engine can sustain periodic oscillations of a movable mirror, which acts as a classical load, against external friction and extract the full distributions of input heat and output work.","By making the statistics of thermodynamic processes in quantum-classical hybrid systems accessible without the need to further specify a measurement protocol, our work contributes towards bridging the long-standing gap between classical and quantum stochastic thermodynamics."],"url":"http://arxiv.org/abs/2404.10118v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-15 20:03:58","title":"PRODIS - a speech database and a phoneme-based language model for the study of predictability effects in Polish","abstract":"We present a speech database and a phoneme-level language model of Polish. The database and model are designed for the analysis of prosodic and discourse factors and their impact on acoustic parameters in interaction with predictability effects. The database is also the first large, publicly available Polish speech corpus of excellent acoustic quality that can be used for phonetic analysis and training of multi-speaker speech technology systems. The speech in the database is processed in a pipeline that achieves a 90% degree of automation. It incorporates state-of-the-art, freely available tools enabling database expansion or adaptation to additional languages.","sentences":["We present a speech database and a phoneme-level language model of Polish.","The database and model are designed for the analysis of prosodic and discourse factors and their impact on acoustic parameters in interaction with predictability effects.","The database is also the first large, publicly available Polish speech corpus of excellent acoustic quality that can be used for phonetic analysis and training of multi-speaker speech technology systems.","The speech in the database is processed in a pipeline that achieves a 90% degree of automation.","It incorporates state-of-the-art, freely available tools enabling database expansion or adaptation to additional languages."],"url":"http://arxiv.org/abs/2404.10112v1","category":"cs.CL"}
{"created":"2024-04-15 19:45:07","title":"Communication-Efficient Hybrid Federated Learning for E-health with Horizontal and Vertical Data Partitioning","abstract":"E-health allows smart devices and medical institutions to collaboratively collect patients' data, which is trained by Artificial Intelligence (AI) technologies to help doctors make diagnosis. By allowing multiple devices to train models collaboratively, federated learning is a promising solution to address the communication and privacy issues in e-health. However, applying federated learning in e-health faces many challenges. First, medical data is both horizontally and vertically partitioned. Since single Horizontal Federated Learning (HFL) or Vertical Federated Learning (VFL) techniques cannot deal with both types of data partitioning, directly applying them may consume excessive communication cost due to transmitting a part of raw data when requiring high modeling accuracy. Second, a naive combination of HFL and VFL has limitations including low training efficiency, unsound convergence analysis, and lack of parameter tuning strategies. In this paper, we provide a thorough study on an effective integration of HFL and VFL, to achieve communication efficiency and overcome the above limitations when data is both horizontally and vertically partitioned. Specifically, we propose a hybrid federated learning framework with one intermediate result exchange and two aggregation phases. Based on this framework, we develop a Hybrid Stochastic Gradient Descent (HSGD) algorithm to train models. Then, we theoretically analyze the convergence upper bound of the proposed algorithm. Using the convergence results, we design adaptive strategies to adjust the training parameters and shrink the size of transmitted data. Experimental results validate that the proposed HSGD algorithm can achieve the desired accuracy while reducing communication cost, and they also verify the effectiveness of the adaptive strategies.","sentences":["E-health allows smart devices and medical institutions to collaboratively collect patients' data, which is trained by Artificial Intelligence (AI) technologies to help doctors make diagnosis.","By allowing multiple devices to train models collaboratively, federated learning is a promising solution to address the communication and privacy issues in e-health.","However, applying federated learning in e-health faces many challenges.","First, medical data is both horizontally and vertically partitioned.","Since single Horizontal Federated Learning (HFL) or Vertical Federated Learning (VFL) techniques cannot deal with both types of data partitioning, directly applying them may consume excessive communication cost due to transmitting a part of raw data when requiring high modeling accuracy.","Second, a naive combination of HFL and VFL has limitations including low training efficiency, unsound convergence analysis, and lack of parameter tuning strategies.","In this paper, we provide a thorough study on an effective integration of HFL and VFL, to achieve communication efficiency and overcome the above limitations when data is both horizontally and vertically partitioned.","Specifically, we propose a hybrid federated learning framework with one intermediate result exchange and two aggregation phases.","Based on this framework, we develop a Hybrid Stochastic Gradient Descent (HSGD) algorithm to train models.","Then, we theoretically analyze the convergence upper bound of the proposed algorithm.","Using the convergence results, we design adaptive strategies to adjust the training parameters and shrink the size of transmitted data.","Experimental results validate that the proposed HSGD algorithm can achieve the desired accuracy while reducing communication cost, and they also verify the effectiveness of the adaptive strategies."],"url":"http://arxiv.org/abs/2404.10110v1","category":"cs.LG"}
{"created":"2024-04-15 19:43:16","title":"GeoAI Reproducibility and Replicability: a computational and spatial perspective","abstract":"GeoAI has emerged as an exciting interdisciplinary research area that combines spatial theories and data with cutting-edge AI models to address geospatial problems in a novel, data-driven manner. While GeoAI research has flourished in the GIScience literature, its reproducibility and replicability (R&R), fundamental principles that determine the reusability, reliability, and scientific rigor of research findings, have rarely been discussed. This paper aims to provide an in-depth analysis of this topic from both computational and spatial perspectives. We first categorize the major goals for reproducing GeoAI research, namely, validation (repeatability), learning and adapting the method for solving a similar or new problem (reproducibility), and examining the generalizability of the research findings (replicability). Each of these goals requires different levels of understanding of GeoAI, as well as different methods to ensure its success. We then discuss the factors that may cause the lack of R&R in GeoAI research, with an emphasis on (1) the selection and use of training data; (2) the uncertainty that resides in the GeoAI model design, training, deployment, and inference processes; and more importantly (3) the inherent spatial heterogeneity of geospatial data and processes. We use a deep learning-based image analysis task as an example to demonstrate the results' uncertainty and spatial variance caused by different factors. The findings reiterate the importance of knowledge sharing, as well as the generation of a \"replicability map\" that incorporates spatial autocorrelation and spatial heterogeneity into consideration in quantifying the spatial replicability of GeoAI research.","sentences":["GeoAI has emerged as an exciting interdisciplinary research area that combines spatial theories and data with cutting-edge AI models to address geospatial problems in a novel, data-driven manner.","While GeoAI research has flourished in the GIScience literature, its reproducibility and replicability (R&R), fundamental principles that determine the reusability, reliability, and scientific rigor of research findings, have rarely been discussed.","This paper aims to provide an in-depth analysis of this topic from both computational and spatial perspectives.","We first categorize the major goals for reproducing GeoAI research, namely, validation (repeatability), learning and adapting the method for solving a similar or new problem (reproducibility), and examining the generalizability of the research findings (replicability).","Each of these goals requires different levels of understanding of GeoAI, as well as different methods to ensure its success.","We then discuss the factors that may cause the lack of R&R in GeoAI research, with an emphasis on (1) the selection and use of training data; (2) the uncertainty that resides in the GeoAI model design, training, deployment, and inference processes; and more importantly (3) the inherent spatial heterogeneity of geospatial data and processes.","We use a deep learning-based image analysis task as an example to demonstrate the results' uncertainty and spatial variance caused by different factors.","The findings reiterate the importance of knowledge sharing, as well as the generation of a \"replicability map\" that incorporates spatial autocorrelation and spatial heterogeneity into consideration in quantifying the spatial replicability of GeoAI research."],"url":"http://arxiv.org/abs/2404.10108v1","category":"cs.CV"}
{"created":"2024-04-15 18:45:49","title":"Onsager's \"Ideal Turbulence\" Theory","abstract":"Lars Onsager in 1945-1949 made an exact analysis of the high Reynolds-number limit for individual turbulent flow realizations modeled by incompressible Navier-Stokes equations, motivated by experimental observations that dissipation of kinetic energy does not vanish. I review here developments spurred by his key idea, that such flows are well-described by distributional or \"weak\" solutions of ideal Euler equations. 1/3 H\\\"older singularities of the velocity field were predicted by Onsager and since observed. His theory describes turbulent energy cascade without probabilistic assumptions and yields a local, deterministic version of the Kolmogorov 4/5th law. The approach is closely related to renormalization group methods in physics and envisages \"conservation-law anomalies\", as discovered later in quantum field theory. There are also deep connections with Large-Eddy Simulation modeling. More recently, dissipative Euler solutions of the type conjectured by Onsager have been constructed and his 1/3 H\\\"older singularity proved to be the sharp threshold for anomalous dissipation. This progress has been achieved by an unexpected connection with work of John Nash on isometric embeddings of low regularity or \"convex integration\" techniques. The dissipative Euler solutions yielded by this method are wildly non-unique for fixed initial data, suggesting \"spontaneously stochastic\" behavior of high-Reynolds number solutions. I focus in particular on applications to wall-bounded turbulence, leading to novel concepts of spatial cascades of momentum, energy and vorticity to or from the wall as deterministic, space-time local phenomena. This theory thus makes testable predictions and offers new perspectives on Large-Eddy Simulation in presence of solid walls.","sentences":["Lars Onsager in 1945-1949 made an exact analysis of the high Reynolds-number limit for individual turbulent flow realizations modeled by incompressible Navier-Stokes equations, motivated by experimental observations that dissipation of kinetic energy does not vanish.","I review here developments spurred by his key idea, that such flows are well-described by distributional or \"weak\" solutions of ideal Euler equations.","1/3 H\\\"older singularities of the velocity field were predicted by Onsager and since observed.","His theory describes turbulent energy cascade without probabilistic assumptions and yields a local, deterministic version of the Kolmogorov 4/5th law.","The approach is closely related to renormalization group methods in physics and envisages \"conservation-law anomalies\", as discovered later in quantum field theory.","There are also deep connections with Large-Eddy Simulation modeling.","More recently, dissipative Euler solutions of the type conjectured by Onsager have been constructed and his 1/3 H\\\"older singularity proved to be the sharp threshold for anomalous dissipation.","This progress has been achieved by an unexpected connection with work of John Nash on isometric embeddings of low regularity or \"convex integration\" techniques.","The dissipative Euler solutions yielded by this method are wildly non-unique for fixed initial data, suggesting \"spontaneously stochastic\" behavior of high-Reynolds number solutions.","I focus in particular on applications to wall-bounded turbulence, leading to novel concepts of spatial cascades of momentum, energy and vorticity to or from the wall as deterministic, space-time local phenomena.","This theory thus makes testable predictions and offers new perspectives on Large-Eddy Simulation in presence of solid walls."],"url":"http://arxiv.org/abs/2404.10084v1","category":"physics.flu-dyn"}
{"created":"2024-04-15 18:26:07","title":"Nano-welding of quantum spin-$1/2$ chains at minimal dissipation","abstract":"We consider the optimal control of switching on a coupling term between two quantum many-body systems. Specifically, we (i) quantify the energetic cost of establishing a weak junction between two quantum spin-$1/2$ chains in finite time $\\tau$ and (ii) identify the energetically optimal protocol to realize it. For linear driving protocols, we find that for long times the excess (irreversible) work scales as $\\tau^{-\\eta}$, where $\\eta=1, 2$ or a nonuniversal number depending on the phase of the chains. Interestingly, increasing a $J_z$ anisotropy in the chains suppresses the excess work thus promoting quasi-adiabaticity. The general optimal control problem is solved, employing a Chebyshev ansatz. We find that the optimal control protocol is intimately sensitive to the chain phases.","sentences":["We consider the optimal control of switching on a coupling term between two quantum many-body systems.","Specifically, we (i) quantify the energetic cost of establishing a weak junction between two quantum spin-$1/2$ chains in finite time $\\tau$ and (ii) identify the energetically optimal protocol to realize it.","For linear driving protocols, we find that for long times the excess (irreversible) work scales as $\\tau^{-\\eta}$, where $\\eta=1, 2$ or a nonuniversal number depending on the phase of the chains.","Interestingly, increasing a $J_z$ anisotropy in the chains suppresses the excess work thus promoting quasi-adiabaticity.","The general optimal control problem is solved, employing a Chebyshev ansatz.","We find that the optimal control protocol is intimately sensitive to the chain phases."],"url":"http://arxiv.org/abs/2404.10074v1","category":"cond-mat.str-el"}
{"created":"2024-04-15 18:24:20","title":"Low-Complexity Block-Based Decoding Algorithms for Short Block Channels","abstract":"This paper presents low-complexity block-based encoding and decoding algorithms for short block length channels. In terms of the precise use-case, we are primarily concerned with the baseline 3GPP Short block transmissions in which payloads are encoded by Reed-Muller codes and paired with orthogonal DMRS. In contemporary communication systems, the short block decoding often employs the utilization of DMRS-based least squares channel estimation, followed by maximum likelihood decoding. However, this methodology can incur substantial computational complexity when processing long bit length codes. We propose an innovative approach to tackle this challenge by introducing the principle of block/segment encoding using First-Order RM Codes which is amenable to low-cost decoding through block-based fast Hadamard transforms. The Block-based FHT has demonstrated to be cost-efficient with regards to decoding time, as it evolves from quadric to quasi-linear complexity with a manageable decline in performance. Additionally, by incorporating an adaptive DMRS/data power adjustment technique, we can bridge/reduce the performance gap and attain high sensitivity, leading to a good trade-off between performance and complexity to efficiently handle small payloads.","sentences":["This paper presents low-complexity block-based encoding and decoding algorithms for short block length channels.","In terms of the precise use-case, we are primarily concerned with the baseline 3GPP Short block transmissions in which payloads are encoded by Reed-Muller codes and paired with orthogonal DMRS.","In contemporary communication systems, the short block decoding often employs the utilization of DMRS-based least squares channel estimation, followed by maximum likelihood decoding.","However, this methodology can incur substantial computational complexity when processing long bit length codes.","We propose an innovative approach to tackle this challenge by introducing the principle of block/segment encoding using First-Order RM Codes which is amenable to low-cost decoding through block-based fast Hadamard transforms.","The Block-based FHT has demonstrated to be cost-efficient with regards to decoding time, as it evolves from quadric to quasi-linear complexity with a manageable decline in performance.","Additionally, by incorporating an adaptive DMRS/data power adjustment technique, we can bridge/reduce the performance gap and attain high sensitivity, leading to a good trade-off between performance and complexity to efficiently handle small payloads."],"url":"http://arxiv.org/abs/2404.10798v1","category":"cs.IT"}
{"created":"2024-04-15 18:11:30","title":"Enhanced Low-Complexity Receiver Design for Short Block Transmission Systems","abstract":"This paper presents a comprehensive analysis and performance enhancement of short block length channel detection incorporating training information. The current communication systems' short block length channel detection typically consists of least squares channel estimation followed by quasi-coherent detection. By investigating the receiver structure, specifically the estimator-correlator, we show that the non-coherent term, often disregarded in conventional detection metrics, results in significant losses in performance and sensitivity in typical operating regimes of 5G and 6G systems. A comparison with the fully non-coherent receiver in multi-antenna configurations reveals substantial losses in low spectral efficiency operating areas. Additionally, we demonstrate that by employing an adaptive DMRS-data power adjustment, it is possible to reduce the performance loss gap, which is amenable to a more sensitive quasi-coherent receiver. However, both of the aforementioned ML detection strategies can result in substantial computational complexity when processing long bit-length codes. We propose an approach to tackle this challenge by introducing the principle of block or segment coding using First-Order RM Codes, which is amenable to low-cost decoding through block-based fast Hadamard transforms. The Block-based FHT has demonstrated to be cost-efficient with regards to decoding time, as it evolves from quadric to quasi-linear complexity with a manageable decline in performance. Additionally, by incorporating an adaptive DMRS-data power adjustment technique, we are able to bridge/reduce the performance gap with respect to the conventional maximum likelihood receiver and attain high sensitivity, leading to a good trade-off between performance and complexity to efficiently handle small payloads.","sentences":["This paper presents a comprehensive analysis and performance enhancement of short block length channel detection incorporating training information.","The current communication systems' short block length channel detection typically consists of least squares channel estimation followed by quasi-coherent detection.","By investigating the receiver structure, specifically the estimator-correlator, we show that the non-coherent term, often disregarded in conventional detection metrics, results in significant losses in performance and sensitivity in typical operating regimes of 5G and 6G systems.","A comparison with the fully non-coherent receiver in multi-antenna configurations reveals substantial losses in low spectral efficiency operating areas.","Additionally, we demonstrate that by employing an adaptive DMRS-data power adjustment, it is possible to reduce the performance loss gap, which is amenable to a more sensitive quasi-coherent receiver.","However, both of the aforementioned ML detection strategies can result in substantial computational complexity when processing long bit-length codes.","We propose an approach to tackle this challenge by introducing the principle of block or segment coding using First-Order RM Codes, which is amenable to low-cost decoding through block-based fast Hadamard transforms.","The Block-based FHT has demonstrated to be cost-efficient with regards to decoding time, as it evolves from quadric to quasi-linear complexity with a manageable decline in performance.","Additionally, by incorporating an adaptive DMRS-data power adjustment technique, we are able to bridge/reduce the performance gap with respect to the conventional maximum likelihood receiver and attain high sensitivity, leading to a good trade-off between performance and complexity to efficiently handle small payloads."],"url":"http://arxiv.org/abs/2404.10065v1","category":"cs.IT"}
{"created":"2024-04-15 18:00:18","title":"Landau-Zener without a Qubit: Unveiling Multiphoton Interference, Synthetic Floquet Dimensions, and Dissipative Quantum Chaos","abstract":"Landau-Zener-St\\\"uckelberg-Majorana (LZSM) interference emerges when the parameters of a $\\textit{qubit}$ are periodically modulated across an avoided level crossing. Here, we investigate the occurrence of the LZSM phenomenon in nonlinear multilevel bosonic systems, where the interference pattern is determined by multiple energy levels and cannot be described by a level crossing between only two states. We fabricate two superconducting resonators made of flux-tunable Josephson junction arrays. The first device is very weakly nonlinear (the nonlinearity is smaller than the photon-loss rate) and, when a weak driving field is applied, it behaves as a linear resonator, yet shows the same LZSM interference as in a two-level system. Notably, here the interference originates from multiple avoided level crossings of the harmonic ladder. When subjected to a stronger drive, nonlinear effects start playing a role, and the interference pattern departs from the one observed in two-level systems. We demonstrate that, when two or more LZSM interference peaks merge, dissipative quantum chaos emerges. In the second device, where the nonlinearity surpasses the photon-loss rate, we observe additional LZSM interference peaks due to Kerr multiphoton resonances. When described under the light of the Floquet theory, these resonances can be interpreted as synthetic modes of an array of coupled cavities. We derive a simple effective model highlighting the essential features of the entirety of these phenomena. As the control of LZSM in qubit systems led to the implementation of fast protocols for characterization and state preparation, our findings pave the way to better control of nonlinear resonators, with implications for diverse quantum technological platforms.","sentences":["Landau-Zener-St\\\"uckelberg-Majorana (LZSM) interference emerges when the parameters of a $\\textit{qubit}$ are periodically modulated across an avoided level crossing.","Here, we investigate the occurrence of the LZSM phenomenon in nonlinear multilevel bosonic systems, where the interference pattern is determined by multiple energy levels and cannot be described by a level crossing between only two states.","We fabricate two superconducting resonators made of flux-tunable Josephson junction arrays.","The first device is very weakly nonlinear (the nonlinearity is smaller than the photon-loss rate) and, when a weak driving field is applied, it behaves as a linear resonator, yet shows the same LZSM interference as in a two-level system.","Notably, here the interference originates from multiple avoided level crossings of the harmonic ladder.","When subjected to a stronger drive, nonlinear effects start playing a role, and the interference pattern departs from the one observed in two-level systems.","We demonstrate that, when two or more LZSM interference peaks merge, dissipative quantum chaos emerges.","In the second device, where the nonlinearity surpasses the photon-loss rate, we observe additional LZSM interference peaks due to Kerr multiphoton resonances.","When described under the light of the Floquet theory, these resonances can be interpreted as synthetic modes of an array of coupled cavities.","We derive a simple effective model highlighting the essential features of the entirety of these phenomena.","As the control of LZSM in qubit systems led to the implementation of fast protocols for characterization and state preparation, our findings pave the way to better control of nonlinear resonators, with implications for diverse quantum technological platforms."],"url":"http://arxiv.org/abs/2404.10051v1","category":"quant-ph"}
{"created":"2024-04-15 17:57:30","title":"Memory Sharing for Large Language Model based Agents","abstract":"In the realm of artificial intelligence, the adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning for fixed-answer tasks such as common sense questions and yes/no queries. However, the application of In-context Learning to open-ended challenges, such as poetry creation, reveals substantial limitations due to the comprehensiveness of the provided examples and agent's ability to understand the content expressed in the problem, leading to outputs that often diverge significantly from expected results. Addressing this gap, our study introduces the Memory-Sharing (MS) framework for LLM multi-agents, which utilizes a real-time memory storage and retrieval system to enhance the In-context Learning process. Each \"memory\" within this system captures both the posed query and the corresponding real-time response from an LLM-based agent, aggregating these memories from a broad spectrum of similar agents to enrich the memory pool shared by all agents. This framework not only aids agents in identifying the most relevant examples for specific tasks but also evaluates the potential utility of their memories for future applications by other agents. Empirical validation across three distinct domains involving specialized functions of agents demonstrates that the MS framework significantly improve the agent's performance regrading the open-ended questions. Furthermore, we also discuss what type of memory pool and what retrieval strategy in MS can better help agents, offering a future develop direction of MS. The code and data are available at: https://github.com/GHupppp/MemorySharingLLM","sentences":["In the realm of artificial intelligence, the adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning for fixed-answer tasks such as common sense questions and yes/no queries.","However, the application of In-context Learning to open-ended challenges, such as poetry creation, reveals substantial limitations due to the comprehensiveness of the provided examples and agent's ability to understand the content expressed in the problem, leading to outputs that often diverge significantly from expected results.","Addressing this gap, our study introduces the Memory-Sharing (MS) framework for LLM multi-agents, which utilizes a real-time memory storage and retrieval system to enhance the In-context Learning process.","Each \"memory\" within this system captures both the posed query and the corresponding real-time response from an LLM-based agent, aggregating these memories from a broad spectrum of similar agents to enrich the memory pool shared by all agents.","This framework not only aids agents in identifying the most relevant examples for specific tasks but also evaluates the potential utility of their memories for future applications by other agents.","Empirical validation across three distinct domains involving specialized functions of agents demonstrates that the MS framework significantly improve the agent's performance regrading the open-ended questions.","Furthermore, we also discuss what type of memory pool and what retrieval strategy in MS can better help agents, offering a future develop direction of MS.","The code and data are available at: https://github.com/GHupppp/MemorySharingLLM"],"url":"http://arxiv.org/abs/2404.09982v1","category":"cs.CL"}
{"created":"2024-04-15 17:55:43","title":"Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers","abstract":"Recently, diffusion transformers have gained wide attention with its excellent performance in text-to-image and text-to-vidoe models, emphasizing the need for transformers as backbone for diffusion models. Transformer-based models have shown better generalization capability compared to CNN-based models for general vision tasks. However, much less has been explored in the existing literature regarding the capabilities of transformer-based diffusion backbones and expanding their generative prowess to other datasets. This paper focuses on enabling a single pre-trained diffusion transformer model to scale across multiple datasets swiftly, allowing for the completion of diverse generative tasks using just one model. To this end, we propose DiffScaler, an efficient scaling strategy for diffusion models where we train a minimal amount of parameters to adapt to different tasks. In particular, we learn task-specific transformations at each layer by incorporating the ability to utilize the learned subspaces of the pre-trained model, as well as the ability to learn additional task-specific subspaces, which may be absent in the pre-training dataset. As these parameters are independent, a single diffusion model with these task-specific parameters can be used to perform multiple tasks simultaneously. Moreover, we find that transformer-based diffusion models significantly outperform CNN-based diffusion models methods while performing fine-tuning over smaller datasets. We perform experiments on four unconditional image generation datasets. We show that using our proposed method, a single pre-trained model can scale up to perform these conditional and unconditional tasks, respectively, with minimal parameter tuning while performing as close as fine-tuning an entire diffusion model for that particular task.","sentences":["Recently, diffusion transformers have gained wide attention with its excellent performance in text-to-image and text-to-vidoe models, emphasizing the need for transformers as backbone for diffusion models.","Transformer-based models have shown better generalization capability compared to CNN-based models for general vision tasks.","However, much less has been explored in the existing literature regarding the capabilities of transformer-based diffusion backbones and expanding their generative prowess to other datasets.","This paper focuses on enabling a single pre-trained diffusion transformer model to scale across multiple datasets swiftly, allowing for the completion of diverse generative tasks using just one model.","To this end, we propose DiffScaler, an efficient scaling strategy for diffusion models where we train a minimal amount of parameters to adapt to different tasks.","In particular, we learn task-specific transformations at each layer by incorporating the ability to utilize the learned subspaces of the pre-trained model, as well as the ability to learn additional task-specific subspaces, which may be absent in the pre-training dataset.","As these parameters are independent, a single diffusion model with these task-specific parameters can be used to perform multiple tasks simultaneously.","Moreover, we find that transformer-based diffusion models significantly outperform CNN-based diffusion models methods while performing fine-tuning over smaller datasets.","We perform experiments on four unconditional image generation datasets.","We show that using our proposed method, a single pre-trained model can scale up to perform these conditional and unconditional tasks, respectively, with minimal parameter tuning while performing as close as fine-tuning an entire diffusion model for that particular task."],"url":"http://arxiv.org/abs/2404.09976v1","category":"cs.CV"}
{"created":"2024-04-15 17:45:36","title":"Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model","abstract":"ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation. First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden. Second, ControlNet features for different frames might not effectively handle the temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos). Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen. Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs. With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours).","sentences":["ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses.","However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation.","First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden.","Second, ControlNet features for different frames might not effectively handle the temporal consistency.","To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos).","Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing.","In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen.","Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos.","We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control.","Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs.","With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours)."],"url":"http://arxiv.org/abs/2404.09967v1","category":"cs.CV"}
{"created":"2024-04-15 17:39:44","title":"Invariant Subspace Decomposition","abstract":"We consider the task of predicting a response Y from a set of covariates X in settings where the conditional distribution of Y given X changes over time. For this to be feasible, assumptions on how the conditional distribution changes over time are required. Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible. In this work, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component. As we show, this decomposition can be utilized both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict Y at, respectively. We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization. Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure.","sentences":["We consider the task of predicting a response Y from a set of covariates X in settings where the conditional distribution of Y given X changes over time.","For this to be feasible, assumptions on how the conditional distribution changes over time are required.","Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible.","In this work, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component.","As we show, this decomposition can be utilized both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict Y at, respectively.","We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization.","Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure."],"url":"http://arxiv.org/abs/2404.09962v1","category":"stat.ML"}
{"created":"2024-04-15 17:27:00","title":"Classification Tree-based Active Learning: A Wrapper Approach","abstract":"Supervised machine learning often requires large training sets to train accurate models, yet obtaining large amounts of labeled data is not always feasible. Hence, it becomes crucial to explore active learning methods for reducing the size of training sets while maintaining high accuracy. The aim is to select the optimal subset of data for labeling from an initial unlabeled set, ensuring precise prediction of outcomes. However, conventional active learning approaches are comparable to classical random sampling. This paper proposes a wrapper active learning method for classification, organizing the sampling process into a tree structure, that improves state-of-the-art algorithms. A classification tree constructed on an initial set of labeled samples is considered to decompose the space into low-entropy regions. Input-space based criteria are used thereafter to sub-sample from these regions, the total number of points to be labeled being decomposed into each region. This adaptation proves to be a significant enhancement over existing active learning methods. Through experiments conducted on various benchmark data sets, the paper demonstrates the efficacy of the proposed framework by being effective in constructing accurate classification models, even when provided with a severely restricted labeled data set.","sentences":["Supervised machine learning often requires large training sets to train accurate models, yet obtaining large amounts of labeled data is not always feasible.","Hence, it becomes crucial to explore active learning methods for reducing the size of training sets while maintaining high accuracy.","The aim is to select the optimal subset of data for labeling from an initial unlabeled set, ensuring precise prediction of outcomes.","However, conventional active learning approaches are comparable to classical random sampling.","This paper proposes a wrapper active learning method for classification, organizing the sampling process into a tree structure, that improves state-of-the-art algorithms.","A classification tree constructed on an initial set of labeled samples is considered to decompose the space into low-entropy regions.","Input-space based criteria are used thereafter to sub-sample from these regions, the total number of points to be labeled being decomposed into each region.","This adaptation proves to be a significant enhancement over existing active learning methods.","Through experiments conducted on various benchmark data sets, the paper demonstrates the efficacy of the proposed framework by being effective in constructing accurate classification models, even when provided with a severely restricted labeled data set."],"url":"http://arxiv.org/abs/2404.09953v1","category":"cs.LG"}
{"created":"2024-04-15 17:24:57","title":"Unifying Global and Local Scene Entities Modelling for Precise Action Spotting","abstract":"Sports videos pose complex challenges, including cluttered backgrounds, camera angle changes, small action-representing objects, and imbalanced action class distribution. Existing methods for detecting actions in sports videos heavily rely on global features, utilizing a backbone network as a black box that encompasses the entire spatial frame. However, these approaches tend to overlook the nuances of the scene and struggle with detecting actions that occupy a small portion of the frame. In particular, they face difficulties when dealing with action classes involving small objects, such as balls or yellow/red cards in soccer, which only occupy a fraction of the screen space. To address these challenges, we introduce a novel approach that analyzes and models scene entities using an adaptive attention mechanism. Particularly, our model disentangles the scene content into the global environment feature and local relevant scene entities feature. To efficiently extract environmental features while considering temporal information with less computational cost, we propose the use of a 2D backbone network with a time-shift mechanism. To accurately capture relevant scene entities, we employ a Vision-Language model in conjunction with the adaptive attention mechanism. Our model has demonstrated outstanding performance, securing the 1st place in the SoccerNet-v2 Action Spotting, FineDiving, and FineGym challenge with a substantial performance improvement of 1.6, 2.0, and 1.3 points in avg-mAP compared to the runner-up methods. Furthermore, our approach offers interpretability capabilities in contrast to other deep learning models, which are often designed as black boxes. Our code and models are released at: https://github.com/Fsoft-AIC/unifying-global-local-feature.","sentences":["Sports videos pose complex challenges, including cluttered backgrounds, camera angle changes, small action-representing objects, and imbalanced action class distribution.","Existing methods for detecting actions in sports videos heavily rely on global features, utilizing a backbone network as a black box that encompasses the entire spatial frame.","However, these approaches tend to overlook the nuances of the scene and struggle with detecting actions that occupy a small portion of the frame.","In particular, they face difficulties when dealing with action classes involving small objects, such as balls or yellow/red cards in soccer, which only occupy a fraction of the screen space.","To address these challenges, we introduce a novel approach that analyzes and models scene entities using an adaptive attention mechanism.","Particularly, our model disentangles the scene content into the global environment feature and local relevant scene entities feature.","To efficiently extract environmental features while considering temporal information with less computational cost, we propose the use of a 2D backbone network with a time-shift mechanism.","To accurately capture relevant scene entities, we employ a Vision-Language model in conjunction with the adaptive attention mechanism.","Our model has demonstrated outstanding performance, securing the 1st place in the SoccerNet-v2 Action Spotting, FineDiving, and FineGym challenge with a substantial performance improvement of 1.6, 2.0, and 1.3 points in avg-mAP compared to the runner-up methods.","Furthermore, our approach offers interpretability capabilities in contrast to other deep learning models, which are often designed as black boxes.","Our code and models are released at: https://github.com/Fsoft-AIC/unifying-global-local-feature."],"url":"http://arxiv.org/abs/2404.09951v1","category":"cs.CV"}
{"created":"2024-04-15 17:12:55","title":"Novel Joint Estimation and Decoding Metrics for Short-Block length Transmission Systems","abstract":"This paper presents Bit-Interleaved Coded Modulation metrics for joint estimation detection using training or reference signal transmission strategies for short to long block length channels. We show that it is possible to enhance the performance and sensitivity through joint detection-estimation compared to standard receivers, especially when the channel state information is unknown and the density of the training dimensions is low. The performance analysis makes use of a full 5G transmitter and receiver chains for both Polar and LDPC coded transmissions paired with BPSK/QPSK modulation schemes. We consider transmissions where reference signals are interleaved with data and both are transmitted over a small number of OFDM symbols so that near-perfect channel estimation cannot be achieved. This is particularly adapted to mini-slot transmissions for ultra-reliable, low-latency communications (URLLC) or for short packet random access use cases. We characterize the performance for up to eight receiving antennas in order to determine the performance gain offered by the proposed BICM detection in realistic base station receiver scenarios. Our findings demonstrate that when the detection windows used in the metric units is on the order of four modulated symbols the proposed BICM metrics can be used to achieve detection performance that is close to that of a coherent receiver with perfect channel state information for both polar and LDPC coded configurations. Furthermore, we show that for transmissions with low DMRS density, a good trade-off can be achieved in terms of additional coding gain and improved channel estimation quality by adaptive DMRS power adjustment.","sentences":["This paper presents Bit-Interleaved Coded Modulation metrics for joint estimation detection using training or reference signal transmission strategies for short to long block length channels.","We show that it is possible to enhance the performance and sensitivity through joint detection-estimation compared to standard receivers, especially when the channel state information is unknown and the density of the training dimensions is low.","The performance analysis makes use of a full 5G transmitter and receiver chains for both Polar and LDPC coded transmissions paired with BPSK/QPSK modulation schemes.","We consider transmissions where reference signals are interleaved with data and both are transmitted over a small number of OFDM symbols so that near-perfect channel estimation cannot be achieved.","This is particularly adapted to mini-slot transmissions for ultra-reliable, low-latency communications (URLLC) or for short packet random access use cases.","We characterize the performance for up to eight receiving antennas in order to determine the performance gain offered by the proposed BICM detection in realistic base station receiver scenarios.","Our findings demonstrate that when the detection windows used in the metric units is on the order of four modulated symbols the proposed BICM metrics can be used to achieve detection performance that is close to that of a coherent receiver with perfect channel state information for both polar and LDPC coded configurations.","Furthermore, we show that for transmissions with low DMRS density, a good trade-off can be achieved in terms of additional coding gain and improved channel estimation quality by adaptive DMRS power adjustment."],"url":"http://arxiv.org/abs/2404.09943v1","category":"cs.IT"}
{"created":"2024-04-15 16:29:56","title":"An adaptive hierarchical ensemble Kalman filter with reduced basis models","abstract":"The use of model order reduction techniques in combination with ensemble-based methods for estimating the state of systems described by nonlinear partial differential equations has been of great interest in recent years in the data assimilation community. Methods such as the multi-fidelity ensemble Kalman filter (MF-EnKF) and the multi-level ensemble Kalman filter (ML-EnKF) are recognized as state-of-the-art techniques. However, in many cases, the construction of low-fidelity models in an offline stage, before solving the data assimilation problem, prevents them from being both accurate and computationally efficient. In our work, we investigate the use of {\\it{adaptive}} reduced basis techniques in which the approximation space is modified online based on the information that is extracted from a limited number of full order solutions and that is carried by the past models. This allows to simultaneously ensure good accuracy and low cost for the employed models and thus improve the performance of the multi-fidelity and multi-level methods.","sentences":["The use of model order reduction techniques in combination with ensemble-based methods for estimating the state of systems described by nonlinear partial differential equations has been of great interest in recent years in the data assimilation community.","Methods such as the multi-fidelity ensemble Kalman filter (MF-EnKF) and the multi-level ensemble Kalman filter (ML-EnKF) are recognized as state-of-the-art techniques.","However, in many cases, the construction of low-fidelity models in an offline stage, before solving the data assimilation problem, prevents them from being both accurate and computationally efficient.","In our work, we investigate the use of {\\it{adaptive}} reduced basis techniques in which the approximation space is modified online based on the information that is extracted from a limited number of full order solutions and that is carried by the past models.","This allows to simultaneously ensure good accuracy and low cost for the employed models and thus improve the performance of the multi-fidelity and multi-level methods."],"url":"http://arxiv.org/abs/2404.09907v1","category":"math.NA"}
{"created":"2024-04-15 16:28:51","title":"Quality of Experience Oriented Cross-layer Optimization for Real-time XR Video Transmission","abstract":"Extended reality (XR) is one of the most important applications of beyond 5G and 6G networks. Real-time XR video transmission presents challenges in terms of data rate and delay. In particular, the frame-by-frame transmission mode of XR video makes real-time XR video very sensitive to dynamic network environments. To improve the users' quality of experience (QoE), we design a cross-layer transmission framework for real-time XR video. The proposed framework allows the simple information exchange between the base station (BS) and the XR server, which assists in adaptive bitrate and wireless resource scheduling. We utilize the cross-layer information to formulate the problem of maximizing user QoE by finding the optimal scheduling and bitrate adjustment strategies. To address the issue of mismatched time scales between two strategies, we decouple the original problem and solve them individually using a multi-agent-based approach. Specifically, we propose the multi-step Deep Q-network (MS-DQN) algorithm to obtain a frame-priority-based wireless resource scheduling strategy and then propose the Transformer-based Proximal Policy Optimization (TPPO) algorithm for video bitrate adaptation. The experimental results show that the TPPO+MS-DQN algorithm proposed in this study can improve the QoE by 3.6% to 37.8%. More specifically, the proposed MS-DQN algorithm enhances the transmission quality by 49.9%-80.2%.","sentences":["Extended reality (XR) is one of the most important applications of beyond 5G and 6G networks.","Real-time XR video transmission presents challenges in terms of data rate and delay.","In particular, the frame-by-frame transmission mode of XR video makes real-time XR video very sensitive to dynamic network environments.","To improve the users' quality of experience (QoE), we design a cross-layer transmission framework for real-time XR video.","The proposed framework allows the simple information exchange between the base station (BS) and the XR server, which assists in adaptive bitrate and wireless resource scheduling.","We utilize the cross-layer information to formulate the problem of maximizing user QoE by finding the optimal scheduling and bitrate adjustment strategies.","To address the issue of mismatched time scales between two strategies, we decouple the original problem and solve them individually using a multi-agent-based approach.","Specifically, we propose the multi-step Deep Q-network (MS-DQN) algorithm to obtain a frame-priority-based wireless resource scheduling strategy and then propose the Transformer-based Proximal Policy Optimization (TPPO) algorithm for video bitrate adaptation.","The experimental results show that the TPPO+MS-DQN algorithm proposed in this study can improve the QoE by 3.6% to 37.8%.","More specifically, the proposed MS-DQN algorithm enhances the transmission quality by 49.9%-80.2%."],"url":"http://arxiv.org/abs/2404.09905v1","category":"cs.NI"}
{"created":"2024-04-17 16:49:02","title":"Euclid view of dusty star forming galaxies at z>~1.5 detected in wide area submillimetre surveys","abstract":"We investigate the constraints provided by the Euclid space observatory on the physical properties of dusty star forming galaxies (DSFGs) at z>~1.5 detected in wide area sub millimetre surveys with Herschel. We adopt a physical model for the high z progenitors of spheroidal galaxies, which form the bulk of the DSFGs at z>~1.5. We improve the model by combining the output of the equations of the model with a formalism for the spectral energy distribution(SED). After optimising the SED parameters to reproduce the measured infrared luminosity function and the number counts of DSFGs, we simulated a sample of DSFGs over 100 sq deg and then applied a 5 sigma detection limit of 37 mJy at 250 microns. We estimated the redshifts from the Euclid data and then fitted the Euclid and Herschel photometry with the code CIGALE to extract the physicsl parameters. We found that 100 % of the Herschel galaxies are detected in all 4 Euclid bands above 3 sigma. For 87% of the sources the accuracy on 1+z is better than 15%. The sample comprises mostly massive log(Mstar/Msun)~10.5-12.9, highly star forming, log(SFR/Msun/yr)~1.5-4, dusty, log(Mdust/Msun)~7.5-9.9, galaxies. The measured stellar mass have a dispersion of 0.19 dex around the true value, thus showing that Euclid will provide reliable stellar mass estimates for the majority of the bright DSFGs at z>~1.5 detected by Herschel. We also explored the effect of complementing the Euclid photometry with that from Vera C. Rubin Observatory/LSST.","sentences":["We investigate the constraints provided by the Euclid space observatory on the physical properties of dusty star forming galaxies (DSFGs) at z>~1.5 detected in wide area sub millimetre surveys with Herschel.","We adopt a physical model for the high z progenitors of spheroidal galaxies, which form the bulk of the DSFGs at z>~1.5.","We improve the model by combining the output of the equations of the model with a formalism for the spectral energy distribution(SED).","After optimising the SED parameters to reproduce the measured infrared luminosity function and the number counts of DSFGs, we simulated a sample of DSFGs over 100 sq deg and then applied a 5 sigma detection limit of 37 mJy at 250 microns.","We estimated the redshifts from the Euclid data and then fitted the Euclid and Herschel photometry with the code CIGALE to extract the physicsl parameters.","We found that 100 % of the Herschel galaxies are detected in all 4 Euclid bands above 3 sigma.","For 87% of the sources the accuracy on 1+z is better than 15%.","The sample comprises mostly massive log(Mstar/Msun)~10.5-12.9, highly star forming, log(SFR/Msun/yr)~1.5-4, dusty, log(Mdust/Msun)~7.5-9.9, galaxies.","The measured stellar mass have a dispersion of 0.19 dex around the true value, thus showing that Euclid will provide reliable stellar mass estimates for the majority of the bright DSFGs at z>~1.5 detected by Herschel.","We also explored the effect of complementing the Euclid photometry with that from Vera C. Rubin Observatory/LSST."],"url":"http://arxiv.org/abs/2404.11551v1","category":"astro-ph.GA"}
{"created":"2024-04-17 16:19:30","title":"Efficient anisotropic Migdal-Eliashberg calculations with the Intermediate Representation basis and Wannier interpolation","abstract":"In this study, we combine the ab initio Migdal-Eliashberg approach with the intermediate representation for the Green's function, enabling accurate and efficient calculations of the momentum-dependent superconducting gap function while fully considering the effect of the Coulomb retardation. Unlike the conventional scheme that relies on a uniform sampling across Matsubara frequencies - demanding hundreds to thousands of points - the intermediate representation works with fewer than 100 sampled Matsubara Green's functions. The developed methodology is applied to investigate the superconducting properties of three representative low-temperature elemental metals: aluminum (Al), lead (Pb), and niobium (Nb). The results demonstrate the power and reliability of our computational technique to accurately solve the ab initio anisotropic Migdal-Eliashberg equations even at extremely low temperatures, below 1 Kelvin.","sentences":["In this study, we combine the ab initio Migdal-Eliashberg approach with the intermediate representation for the Green's function, enabling accurate and efficient calculations of the momentum-dependent superconducting gap function while fully considering the effect of the Coulomb retardation.","Unlike the conventional scheme that relies on a uniform sampling across Matsubara frequencies - demanding hundreds to thousands of points - the intermediate representation works with fewer than 100 sampled Matsubara Green's functions.","The developed methodology is applied to investigate the superconducting properties of three representative low-temperature elemental metals: aluminum (Al), lead (Pb), and niobium (Nb).","The results demonstrate the power and reliability of our computational technique to accurately solve the ab initio anisotropic Migdal-Eliashberg equations even at extremely low temperatures, below 1 Kelvin."],"url":"http://arxiv.org/abs/2404.11528v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 15:45:30","title":"On the non-local problem for Boussinesq type fractional equation","abstract":"In recent years, the Boussinesq type fractional partial differential equation has attracted much attentions of researchers for its practical importance. In this paper we study a non-local problem for the Boussinesq type equation $D_t^\\alpha u(t)+A D_t^\\alpha u(t)+\\nu^2A u(t)=0,\\,\\, 0< t< T,\\,\\, 1<\\alpha<2,$ where $D_t^\\alpha$ is the Caputo fractional derivative and $A$ is abstract operator. In the classical case, i.e. at $\\alpha=2$, this problem was studied earlier and an interesting effect was discovered: the well-posedness of the problem significantly depends on the length of the time interval and the parameter $\\nu$. This note shows that for the case of a fractional equation there is no such effect: the problem is well-posed for any $T$ and $\\nu$.","sentences":["In recent years, the Boussinesq type fractional partial differential equation has attracted much attentions of researchers for its practical importance.","In this paper we study a non-local problem for the Boussinesq type equation $D_t^\\alpha u(t)+A D_t^\\alpha u(t)+\\nu^2A u(t)=0,\\,\\, 0< t<","T,\\,\\, 1<\\alpha<2,$ where $D_t^\\alpha$ is the Caputo fractional derivative and $A$ is abstract operator.","In the classical case, i.e. at $\\alpha=2$, this problem was studied earlier and an interesting effect was discovered: the well-posedness of the problem significantly depends on the length of the time interval and the parameter $\\nu$. This note shows that for the case of a fractional equation there is no such effect: the problem is well-posed for any $T$ and $\\nu$."],"url":"http://arxiv.org/abs/2404.11486v1","category":"math.AP"}
{"created":"2024-04-17 15:33:45","title":"Impact of lensing of gravitational waves on the observed distribution of neutron star masses","abstract":"The distribution of masses of neutron stars, particularly the maximum mass value, is considered a probe of their formation, evolution and internal physics (i.e., equation of state). This mass distribution could in principle be inferred from the detection of gravitational waves from binary neutron star mergers. Using mock catalogues of $10^5$ dark sirens events, expected to be detected by Einstein Telescope over an operational period of $\\sim1\\, \\rm year$ , we show how the biased luminosity distance measurement induced by gravitational lensing affects the inferred redshift and mass of the merger. This results in higher observed masses than expected. Up to $2\\%$ of the events are predicted to fall above the maximum allowed neutron star mass depending on the intrinsic mass distribution and signal-to-noise ratio threshold adopted. The underlying true mass distribution and maximum mass could still be approximately recovered in the case of bright standard sirens.","sentences":["The distribution of masses of neutron stars, particularly the maximum mass value, is considered a probe of their formation, evolution and internal physics (i.e., equation of state).","This mass distribution could in principle be inferred from the detection of gravitational waves from binary neutron star mergers.","Using mock catalogues of $10^5$ dark sirens events, expected to be detected by Einstein Telescope over an operational period of $\\sim1\\, \\rm year$ , we show how the biased luminosity distance measurement induced by gravitational lensing affects the inferred redshift and mass of the merger.","This results in higher observed masses than expected.","Up to $2\\%$ of the events are predicted to fall above the maximum allowed neutron star mass depending on the intrinsic mass distribution and signal-to-noise ratio threshold adopted.","The underlying true mass distribution and maximum mass could still be approximately recovered in the case of bright standard sirens."],"url":"http://arxiv.org/abs/2404.11480v1","category":"astro-ph.CO"}
{"created":"2024-04-17 14:55:11","title":"An accelerated Levin-Clenshaw-Curtis method for the evaluation of highly oscillatory integrals","abstract":"The efficient approximation of highly oscillatory integrals plays an important role in a wide range of applications. Whilst traditional quadrature becomes prohibitively expensive in the high-frequency regime, Levin methods provide a way to approximate these integrals in many settings at uniform cost. In this work, we present an accelerated version of Levin methods that can be applied to a wide range of physically important oscillatory integrals, by exploiting the banded action of certain differential operators on a Chebyshev polynomial basis. Our proposed version of the Levin method can be computed essentially in just $\\mathcal{O}(\\nu\\log\\nu)$ operations, where $\\nu$ is the number of quadrature points and the dependence of the cost on a number of additional parameters is made explicit in the manuscript. This presents a significant speed-up over the direct computation of the Levin method in current state-of-the-art. We outline the construction of this accelerated method for a fairly broad class of integrals and support our theoretical description with a number of illustrative numerical examples.","sentences":["The efficient approximation of highly oscillatory integrals plays an important role in a wide range of applications.","Whilst traditional quadrature becomes prohibitively expensive in the high-frequency regime, Levin methods provide a way to approximate these integrals in many settings at uniform cost.","In this work, we present an accelerated version of Levin methods that can be applied to a wide range of physically important oscillatory integrals, by exploiting the banded action of certain differential operators on a Chebyshev polynomial basis.","Our proposed version of the Levin method can be computed essentially in just $\\mathcal{O}(\\nu\\log\\nu)$ operations, where $\\nu$ is the number of quadrature points and the dependence of the cost on a number of additional parameters is made explicit in the manuscript.","This presents a significant speed-up over the direct computation of the Levin method in current state-of-the-art.","We outline the construction of this accelerated method for a fairly broad class of integrals and support our theoretical description with a number of illustrative numerical examples."],"url":"http://arxiv.org/abs/2404.11448v1","category":"math.NA"}
{"created":"2024-04-17 14:39:14","title":"Runtime Analyses of NSGA-III on Many-Objective Problems","abstract":"NSGA-II and NSGA-III are two of the most popular evolutionary multi-objective algorithms used in practice. While NSGA-II is used for few objectives such as 2 and 3, NSGA-III is designed to deal with a larger number of objectives. In a recent breakthrough, Wietheger and Doerr (IJCAI 2023) gave the first runtime analysis for NSGA-III on the 3-objective OneMinMax problem, showing that this state-of-the-art algorithm can be analyzed rigorously. We advance this new line of research by presenting the first runtime analyses of NSGA-III on the popular many-objective benchmark problems mLOTZ, mOMM, and mCOCZ, for an arbitrary constant number $m$ of objectives. Our analysis provides ways to set the important parameters of the algorithm: the number of reference points and the population size, so that a good performance can be guaranteed. We show how these parameters should be scaled with the problem dimension, the number of objectives and the fitness range. To our knowledge, these are the first runtime analyses for NSGA-III for more than 3 objectives.","sentences":["NSGA-II and NSGA-III are two of the most popular evolutionary multi-objective algorithms used in practice.","While NSGA-II is used for few objectives such as 2 and 3, NSGA-III is designed to deal with a larger number of objectives.","In a recent breakthrough, Wietheger and Doerr (IJCAI 2023) gave the first runtime analysis for NSGA-III on the 3-objective OneMinMax problem, showing that this state-of-the-art algorithm can be analyzed rigorously.","We advance this new line of research by presenting the first runtime analyses of NSGA-III on the popular many-objective benchmark problems mLOTZ, mOMM, and mCOCZ, for an arbitrary constant number $m$ of objectives.","Our analysis provides ways to set the important parameters of the algorithm: the number of reference points and the population size, so that a good performance can be guaranteed.","We show how these parameters should be scaled with the problem dimension, the number of objectives and the fitness range.","To our knowledge, these are the first runtime analyses for NSGA-III for more than 3 objectives."],"url":"http://arxiv.org/abs/2404.11433v1","category":"cs.NE"}
{"created":"2024-04-17 14:28:57","title":"Correlation Function Of Thin-Shell Operators","abstract":"In this study, we explore the correlation functions of thin-shell operators, represented semiclassically by a homogeneous, thin interface of dust particles. Employing the monodromy method, we successfully compute the contribution from the Virasoro vacuum block and present the monodromy equation in a closed form without assuming the probe limit. Although an analytical solution to the monodromy equation remains difficult, we demonstrate that it is perturbatively solvable within specific limits, including the probe, the heavy-shell, and the early-time limits. Moreover, we compare our results with gravitational calculations and find precise agreement. We strengthen our findings by proving that the thermal correlation functions in gravity, after an inverse Laplace transformation, satisfy the field theory's monodromy equation. Additionally, we identify an infinite series of unphysical solutions to the monodromy equation and discuss their potential geometrical duals.","sentences":["In this study, we explore the correlation functions of thin-shell operators, represented semiclassically by a homogeneous, thin interface of dust particles.","Employing the monodromy method, we successfully compute the contribution from the Virasoro vacuum block and present the monodromy equation in a closed form without assuming the probe limit.","Although an analytical solution to the monodromy equation remains difficult, we demonstrate that it is perturbatively solvable within specific limits, including the probe, the heavy-shell, and the early-time limits.","Moreover, we compare our results with gravitational calculations and find precise agreement.","We strengthen our findings by proving that the thermal correlation functions in gravity, after an inverse Laplace transformation, satisfy the field theory's monodromy equation.","Additionally, we identify an infinite series of unphysical solutions to the monodromy equation and discuss their potential geometrical duals."],"url":"http://arxiv.org/abs/2404.11423v1","category":"hep-th"}
{"created":"2024-04-17 14:22:57","title":"On an inhomogeneous coagulation model with a differential sedimentation kernel","abstract":"We study an inhomogeneous coagulation equation that contains a transport term in the spatial variable modeling the sedimentation of clusters. We prove local existence of mass conserving solutions for a class of coagulation kernels for which in the space homogeneous case instantaneous gelation (i.e., instantaneous loss of mass) occurs. Our result holds true in particular for sum-type kernels of homogeneity greater than one, for which solutions do not exist at all in the spatially homogeneous case. Moreover, our result covers kernels that in addition vanish on the diagonal, which have been used to describe the onset of rain and the behavior of air bubbles in water.","sentences":["We study an inhomogeneous coagulation equation that contains a transport term in the spatial variable modeling the sedimentation of clusters.","We prove local existence of mass conserving solutions for a class of coagulation kernels for which in the space homogeneous case instantaneous gelation (i.e., instantaneous loss of mass) occurs.","Our result holds true in particular for sum-type kernels of homogeneity greater than one, for which solutions do not exist at all in the spatially homogeneous case.","Moreover, our result covers kernels that in addition vanish on the diagonal, which have been used to describe the onset of rain and the behavior of air bubbles in water."],"url":"http://arxiv.org/abs/2404.11418v1","category":"math.AP"}
{"created":"2024-04-17 14:17:05","title":"Neural Shr\u00f6dinger Bridge Matching for Pansharpening","abstract":"Recent diffusion probabilistic models (DPM) in the field of pansharpening have been gradually gaining attention and have achieved state-of-the-art (SOTA) performance. In this paper, we identify shortcomings in directly applying DPMs to the task of pansharpening as an inverse problem: 1) initiating sampling directly from Gaussian noise neglects the low-resolution multispectral image (LRMS) as a prior; 2) low sampling efficiency often necessitates a higher number of sampling steps. We first reformulate pansharpening into the stochastic differential equation (SDE) form of an inverse problem. Building upon this, we propose a Schr\\\"odinger bridge matching method that addresses both issues.   We design an efficient deep neural network architecture tailored for the proposed SB matching.   In comparison to the well-established DL-regressive-based framework and the recent DPM framework, our method demonstrates SOTA performance with fewer sampling steps. Moreover, we discuss the relationship between SB matching and other methods based on SDEs and ordinary differential equations (ODEs), as well as its connection with optimal transport.   Code will be available.","sentences":["Recent diffusion probabilistic models (DPM) in the field of pansharpening have been gradually gaining attention and have achieved state-of-the-art (SOTA) performance.","In this paper, we identify shortcomings in directly applying DPMs to the task of pansharpening as an inverse problem: 1) initiating sampling directly from Gaussian noise neglects the low-resolution multispectral image (LRMS) as a prior; 2) low sampling efficiency often necessitates a higher number of sampling steps.","We first reformulate pansharpening into the stochastic differential equation (SDE) form of an inverse problem.","Building upon this, we propose a Schr\\\"odinger bridge matching method that addresses both issues.   ","We design an efficient deep neural network architecture tailored for the proposed SB matching.   ","In comparison to the well-established DL-regressive-based framework and the recent DPM framework, our method demonstrates SOTA performance with fewer sampling steps.","Moreover, we discuss the relationship between SB matching and other methods based on SDEs and ordinary differential equations (ODEs), as well as its connection with optimal transport.   ","Code will be available."],"url":"http://arxiv.org/abs/2404.11416v1","category":"cs.CV"}
{"created":"2024-04-17 13:43:12","title":"Lower Limb Movements Recognition Based on Feature Recursive Elimination and Backpropagation Neural Network","abstract":"Surface electromyographic (sEMG) signal serve as a signal source commonly used for lower limb movement recognition, reflecting the intent of human movement. However, it has been a challenge to improve the movements recognition rate while using fewer features in this area of research area. In this paper, a method for lower limb movements recognition based on recursive feature elimination and backpropagation neural network of support vector machine is proposed. First, the sEMG signal of five subjects performing eight different lower limb movements was recorded using a BIOPAC collector. The optimal feature subset consists of 25 feature vectors, determined using a Recursive Feature Elimination based on Support Vector Machine (SVM-RFE). Finally, this study used five supervised classification algorithms to recognize these eight different lower limb movements. The results of the experimental study show that the combination of the BPNN classifier and the SVM-RFE feature selection algorithm is able to achieve an excellent action recognition accuracy of 95\\%, which provides sufficient support for the feasibility of this approach.","sentences":["Surface electromyographic (sEMG) signal serve as a signal source commonly used for lower limb movement recognition, reflecting the intent of human movement.","However, it has been a challenge to improve the movements recognition rate while using fewer features in this area of research area.","In this paper, a method for lower limb movements recognition based on recursive feature elimination and backpropagation neural network of support vector machine is proposed.","First, the sEMG signal of five subjects performing eight different lower limb movements was recorded using a BIOPAC collector.","The optimal feature subset consists of 25 feature vectors, determined using a Recursive Feature Elimination based on Support Vector Machine (SVM-RFE).","Finally, this study used five supervised classification algorithms to recognize these eight different lower limb movements.","The results of the experimental study show that the combination of the BPNN classifier and the SVM-RFE feature selection algorithm is able to achieve an excellent action recognition accuracy of 95\\%, which provides sufficient support for the feasibility of this approach."],"url":"http://arxiv.org/abs/2404.11383v1","category":"eess.SP"}
{"created":"2024-04-17 13:18:57","title":"A New Approach to Solving Singularly Perturbed NLS at Local Potential Maxima","abstract":"This paper presents a new approach for addressing the singularly perturbed nonlinear Schr\\\"odinger (NLS) equation:   \\begin{equation}   -\\varepsilon^2\\Delta v + V(x) v =f(v),\\ v>0,\\ \\lim_{|x|\\to \\infty} v(x)=0,   \\end{equation} where $V$ possesses a local maximum point and $f$ satisfies the Berestycki-Lions conditions.The key to our approach is the derivation of a refined lower bound on the gradient norm.","sentences":["This paper presents a new approach for addressing the singularly perturbed nonlinear Schr\\\"odinger (NLS) equation:   \\begin{equation}   -\\varepsilon^2\\Delta v + V(x) v =f(v),\\ v>0,\\ \\lim_{|x|\\to \\infty} v(x)=0,   \\end{equation} where $V$ possesses a local maximum point and $f$ satisfies the Berestycki-Lions conditions.","The key to our approach is the derivation of a refined lower bound on the gradient norm."],"url":"http://arxiv.org/abs/2404.11362v1","category":"math.AP"}
{"created":"2024-04-17 13:11:49","title":"A Comparative Experimental and Theoretical Study on Doubly Differential Electron-Impact Ionization Cross Sections of Pyrimidine","abstract":"To provide a comprehensive data set for track structure-based simulations of radiation damage in DNA, doubly differential electron-impact ionization cross sections of pyrimidine, a building block of the nucleobases cytosine and thymine, were measured for primary electron energies between 30 eV and 1 keV as a function of emission angle and secondary electron energy. The measurements were performed for secondary electron energies from 4 eV to about half of the primary electron energy and for emission angles between 25{\\deg} and 135{\\deg}. Based on the experimental doubly differential ionization cross sections, singly differential and total ionization cross sections of pyrimidine were determined and compared to calculations using the BEB model. In addition to the measurements, a theoretical approach for calculating triply and doubly differential ionization cross section was developed, which is based on the distorted wave Born approximation, a single center expansion of molecular orbitals and an averaging of the T-matrix over different molecular orientations. The calculated doubly differential ionization cross sections of pyrimidine show a qualitatively good agreement with the experimental results.","sentences":["To provide a comprehensive data set for track structure-based simulations of radiation damage in DNA, doubly differential electron-impact ionization cross sections of pyrimidine, a building block of the nucleobases cytosine and thymine, were measured for primary electron energies between 30 eV and 1 keV as a function of emission angle and secondary electron energy.","The measurements were performed for secondary electron energies from 4 eV to about half of the primary electron energy and for emission angles between 25{\\deg} and 135{\\deg}.","Based on the experimental doubly differential ionization cross sections, singly differential and total ionization cross sections of pyrimidine were determined and compared to calculations using the BEB model.","In addition to the measurements, a theoretical approach for calculating triply and doubly differential ionization cross section was developed, which is based on the distorted wave Born approximation, a single center expansion of molecular orbitals and an averaging of the T-matrix over different molecular orientations.","The calculated doubly differential ionization cross sections of pyrimidine show a qualitatively good agreement with the experimental results."],"url":"http://arxiv.org/abs/2404.11356v1","category":"physics.chem-ph"}
{"created":"2024-04-17 12:22:54","title":"Use of Parallel Explanatory Models to Enhance Transparency of Neural Network Configurations for Cell Degradation Detection","abstract":"In a previous paper, we have shown that a recurrent neural network (RNN) can be used to detect cellular network radio signal degradations accurately. We unexpectedly found, though, that accuracy gains diminished as we added layers to the RNN. To investigate this, in this paper, we build a parallel model to illuminate and understand the internal operation of neural networks, such as the RNN, which store their internal state in order to process sequential inputs. This model is widely applicable in that it can be used with any input domain where the inputs can be represented by a Gaussian mixture. By looking at the RNN processing from a probability density function perspective, we are able to show how each layer of the RNN transforms the input distributions to increase detection accuracy. At the same time we also discover a side effect acting to limit the improvement in accuracy. To demonstrate the fidelity of the model we validate it against each stage of RNN processing as well as the output predictions. As a result, we have been able to explain the reasons for the RNN performance limits with useful insights for future designs for RNNs and similar types of neural network.","sentences":["In a previous paper, we have shown that a recurrent neural network (RNN) can be used to detect cellular network radio signal degradations accurately.","We unexpectedly found, though, that accuracy gains diminished as we added layers to the RNN.","To investigate this, in this paper, we build a parallel model to illuminate and understand the internal operation of neural networks, such as the RNN, which store their internal state in order to process sequential inputs.","This model is widely applicable in that it can be used with any input domain where the inputs can be represented by a Gaussian mixture.","By looking at the RNN processing from a probability density function perspective, we are able to show how each layer of the RNN transforms the input distributions to increase detection accuracy.","At the same time we also discover a side effect acting to limit the improvement in accuracy.","To demonstrate the fidelity of the model we validate it against each stage of RNN processing as well as the output predictions.","As a result, we have been able to explain the reasons for the RNN performance limits with useful insights for future designs for RNNs and similar types of neural network."],"url":"http://arxiv.org/abs/2404.11311v1","category":"cs.LG"}
{"created":"2024-04-17 12:12:21","title":"Nonlinear stability and transition threshold for the planar helical flow","abstract":"In this paper, we study the nonlinear stability for the planar helical flow at high Reynolds number $Re$. We prove that if the initial velocity satisfies $$ \\left\\|U_0-(\\delta^2\\sin(m_0 y),\\delta^2\\cos(m_0 y),0)\\right\\|_{X_0}\\leq c_0 Re^{-7/4} $$ for some $c_0>0$ independent of $Re$, then the solution of 3-D incompressible Navier-Stokes equation is global in time and does not transit away from the planar helical flow. Here $\\delta>1, m_0=\\delta^{-1}$ and the norm $\\|\\cdot\\|_{X_0}$ is defined in \\eqref{eq:1.8}. This is a nonlinear stability result for 3-D non-shear flow and the transition threshold is less than $7/4$.","sentences":["In this paper, we study the nonlinear stability for the planar helical flow at high Reynolds number $Re$.","We prove that if the initial velocity satisfies $$ \\left\\|U_0-(\\delta^2\\sin(m_0 y),\\delta^2\\cos(m_0 y),0)\\right\\|_{X_0}\\leq c_0 Re^{-7/4} $$ for some $c_0>0$ independent of $Re$, then the solution of 3-D incompressible Navier-Stokes equation is global in time and does not transit away from the planar helical flow.","Here $\\delta>1, m_0=\\delta^{-1}$ and the norm $\\|\\cdot\\|_{X_0}$ is defined in \\eqref{eq:1.8}.","This is a nonlinear stability result for 3-D non-shear flow and the transition threshold is less than $7/4$."],"url":"http://arxiv.org/abs/2404.11298v1","category":"math.AP"}
{"created":"2024-04-17 12:01:08","title":"Local clustering of relic neutrinos: Comparison of kinetic field theory and the Vlasov equation","abstract":"Gravitational clustering in our cosmic vicinity is expected to lead to an enhancement of the local density of relic neutrinos. We derive expressions for the neutrino density, using a perturbative approach to kinetic field theory and perturbative solutions of the Vlasov equation up to second order. Our work reveals that both formalisms give exactly the same results and can thus be considered equivalent. Numerical evaluation of the local relic neutrino density at first and second order provides some fundamental insights into the frequently applied approach of linear response to neutrino clustering (also known as the Gilbert equation). Against the naive expectation, including the second-order contribution does not lead to an improvement of the prediction for the local relic neutrino density but to a dramatic overestimation. This is because perturbation theory breaks down in a momentum-dependent fashion and in particular for densities well below unity.","sentences":["Gravitational clustering in our cosmic vicinity is expected to lead to an enhancement of the local density of relic neutrinos.","We derive expressions for the neutrino density, using a perturbative approach to kinetic field theory and perturbative solutions of the Vlasov equation up to second order.","Our work reveals that both formalisms give exactly the same results and can thus be considered equivalent.","Numerical evaluation of the local relic neutrino density at first and second order provides some fundamental insights into the frequently applied approach of linear response to neutrino clustering (also known as the Gilbert equation).","Against the naive expectation, including the second-order contribution does not lead to an improvement of the prediction for the local relic neutrino density but to a dramatic overestimation.","This is because perturbation theory breaks down in a momentum-dependent fashion and in particular for densities well below unity."],"url":"http://arxiv.org/abs/2404.11295v1","category":"hep-ph"}
{"created":"2024-04-17 11:45:04","title":"Classification of differentially non-degenerate left-symmetric algebras in dimension 3","abstract":"In terms of Nijenhuis geometry, left-symmetric algebras are the same as Nijenhuis operators whose entries are linear in coordinates. Here we consider Nijenhuis operators for which the coefficients of its characteristic polynomial are algebraically independent. We give a classification of this type of operators and hence of the corresponding LSAs (which we will call differentially non-degenerate LSA) in dimension 3.","sentences":["In terms of Nijenhuis geometry, left-symmetric algebras are the same as Nijenhuis operators whose entries are linear in coordinates.","Here we consider Nijenhuis operators for which the coefficients of its characteristic polynomial are algebraically independent.","We give a classification of this type of operators and hence of the corresponding LSAs (which we will call differentially non-degenerate LSA) in dimension 3."],"url":"http://arxiv.org/abs/2404.11282v1","category":"math.DG"}
{"created":"2024-04-17 11:15:58","title":"The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data","abstract":"Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs). The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present. However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios. In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy. This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&B), which exploits a poisoned model to train a clean model without extra benign samples. Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples. Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection. Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance. Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&B framework. Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB.","sentences":["Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs).","The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present.","However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios.","In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy.","This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&B), which exploits a poisoned model to train a clean model without extra benign samples.","Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples.","Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection.","Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance.","Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&B framework.","Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples.","Our code is available at https://github.com/Zixuan-Zhu/VaB."],"url":"http://arxiv.org/abs/2404.11265v1","category":"cs.CV"}
{"created":"2024-04-17 11:08:29","title":"The rigidity of Doyle circle packings on the infinite hexagonal triangulation","abstract":"Peter Doyle conjectured that locally univalent circle packings on the hexagonal lattice only consist of regular hexagonal packings and Doyle spirals, which is called the Doyle conjecture. In this paper, we prove a rigidity theorem for Doyle spirals in the class of infinite circle packings on the hexagonal lattice whose radii ratios of adjacent circles have a uniform bound. This gives a partial answer to the Doyle conjecture. Based on a new observation that the logarithmic of the radii ratio of adjacent circles is a weighted discrete harmonic function, we prove the result via the Liouville theorem of discrete harmonic functions.","sentences":["Peter Doyle conjectured that locally univalent circle packings on the hexagonal lattice only consist of regular hexagonal packings and Doyle spirals, which is called the Doyle conjecture.","In this paper, we prove a rigidity theorem for Doyle spirals in the class of infinite circle packings on the hexagonal lattice whose radii ratios of adjacent circles have a uniform bound.","This gives a partial answer to the Doyle conjecture.","Based on a new observation that the logarithmic of the radii ratio of adjacent circles is a weighted discrete harmonic function, we prove the result via the Liouville theorem of discrete harmonic functions."],"url":"http://arxiv.org/abs/2404.11258v1","category":"math.GT"}
{"created":"2024-04-17 11:07:17","title":"Deep Joint Learning valuation of Bermudan Swaptions","abstract":"This paper addresses the problem of pricing involved financial derivatives by means of advanced of deep learning techniques. More precisely, we smartly combine several sophisticated neural network-based concepts like differential machine learning, Monte Carlo simulation-like training samples and joint learning to come up with an efficient numerical solution. The application of the latter development represents a novelty in the context of computational finance. We also propose a novel design of interdependent neural networks to price early-exercise products, in this case, Bermudan swaptions. The improvements in efficiency and accuracy provided by the here proposed approach is widely illustrated throughout a range of numerical experiments. Moreover, this novel methodology can be extended to the pricing of other financial derivatives.","sentences":["This paper addresses the problem of pricing involved financial derivatives by means of advanced of deep learning techniques.","More precisely, we smartly combine several sophisticated neural network-based concepts like differential machine learning, Monte Carlo simulation-like training samples and joint learning to come up with an efficient numerical solution.","The application of the latter development represents a novelty in the context of computational finance.","We also propose a novel design of interdependent neural networks to price early-exercise products, in this case, Bermudan swaptions.","The improvements in efficiency and accuracy provided by the here proposed approach is widely illustrated throughout a range of numerical experiments.","Moreover, this novel methodology can be extended to the pricing of other financial derivatives."],"url":"http://arxiv.org/abs/2404.11257v1","category":"q-fin.CP"}
{"created":"2024-04-17 10:57:09","title":"Travelling waves in a minimal go-or-grow model of cell invasion","abstract":"We consider a minimal go-or-grow model of cell invasion, whereby cells can either proliferate, following logistic growth, or move, via linear diffusion, and phenotypic switching between these two states is density-dependent. Formal analysis in the fast switching regime shows that the total cell density in the two-population go-or-grow model can be described in terms of a single reaction-diffusion equation with density-dependent diffusion and proliferation. Using the connection to single-population models, we study travelling wave solutions, showing that the wave speed in the go-or-grow model is always bounded by the wave speed corresponding to the well-known Fisher-KPP equation.","sentences":["We consider a minimal go-or-grow model of cell invasion, whereby cells can either proliferate, following logistic growth, or move, via linear diffusion, and phenotypic switching between these two states is density-dependent.","Formal analysis in the fast switching regime shows that the total cell density in the two-population go-or-grow model can be described in terms of a single reaction-diffusion equation with density-dependent diffusion and proliferation.","Using the connection to single-population models, we study travelling wave solutions, showing that the wave speed in the go-or-grow model is always bounded by the wave speed corresponding to the well-known Fisher-KPP equation."],"url":"http://arxiv.org/abs/2404.11251v1","category":"math.AP"}
{"created":"2024-04-17 10:31:42","title":"Post-Poisson algebras and Poisson Yang-Baxter equation via bimodule algebra deformations","abstract":"A fundamental construction of Poisson algebras is as the quasiclassical limits (QCLs) of associative algebra deformations of commutative associative algebras. This paper extends this construction to the relative context with the notion of (bi)module algebras over another algebra for a given algebraic structure. In this language, a module Poisson algebras can be realized as the QCLs of bimodule associative deformations of module commutative associative algebras. Moreover, the notion of the scalar deformation of an $\\mathcal O$-operator is introduced so that the process of bimodule algebras deformations to QCLs is endowed with $\\mathcal O$-operators in a consistent manner. As an explicit illustration of this process, post-Poisson algebras are realized as the QCLs of bimodule associative deformations of module commutative associative algebras with the identity maps as $\\mathcal O$-operators, recovering the known fact that post-Poisson algebras are the QCLs of tridendriform algebra deformations of commutative tridendriform algebras. Furthermore,   the notion of scalar deformations of solutions of the associative Yang-Baxter equation (AYBE) is applied to realize solutions of the Poisson Yang-Baxter equation (PYBE) in Poisson algebras as solutions of the AYBE in commutative associative algebras, giving a YBE version of Poisson algebras as the QCLs of associative deformations of the commutative associative algebras. Finally, concrete solutions of the PYBE are obtained from the aforementioned tridendriform deformation-to-QCLs process.","sentences":["A fundamental construction of Poisson algebras is as the quasiclassical limits (QCLs) of associative algebra deformations of commutative associative algebras.","This paper extends this construction to the relative context with the notion of (bi)module algebras over another algebra for a given algebraic structure.","In this language, a module Poisson algebras can be realized as the QCLs of bimodule associative deformations of module commutative associative algebras.","Moreover, the notion of the scalar deformation of an $\\mathcal O$-operator is introduced so that the process of bimodule algebras deformations to QCLs is endowed with $\\mathcal O$-operators in a consistent manner.","As an explicit illustration of this process, post-Poisson algebras are realized as the QCLs of bimodule associative deformations of module commutative associative algebras with the identity maps as $\\mathcal O$-operators, recovering the known fact that post-Poisson algebras are the QCLs of tridendriform algebra deformations of commutative tridendriform algebras.","Furthermore,   the notion of scalar deformations of solutions of the associative Yang-Baxter equation (AYBE) is applied to realize solutions of the Poisson Yang-Baxter equation (PYBE) in Poisson algebras as solutions of the AYBE in commutative associative algebras, giving a YBE version of Poisson algebras as the QCLs of associative deformations of the commutative associative algebras.","Finally, concrete solutions of the PYBE are obtained from the aforementioned tridendriform deformation-to-QCLs process."],"url":"http://arxiv.org/abs/2404.11232v1","category":"math.QA"}
{"created":"2024-04-17 10:07:58","title":"Identifying coronal sources of L1 solar wind disturbances using the Fisk heliospheric magnetic field and potential field extrapolations during three solar minima","abstract":"The solar minima between solar cycles 22-23, 23-24 and 24-25 are the best observed minima on record. In situ solar wind and interplanetary magnetic field measurements by the WIND and ACE spacecraft at L1 with one-hour cadence are explored using wavelet analyses for the most quiescent year during each minimum. Times of local peaks in periodicities are identified in the solar wind velocity, magnetic field components, and proton number densities. The measured radial velocities at these times are used to trace magnetic field lines to the photosphere using two models. The first is the Fisk heliospheric magnetic field that traces field lines from L1 to the photosphere. They connect exclusively to solar poles and in 88% instances to locations of polar coronal holes. The second model uses the Parker spiral to trace from L1 to the solar source surface and potential field extrapolations from the source surface to the photosphere. These field lines terminate at equatorial and mid-latitude coordinates of which some are located close to coronal holes. This study connects for the first time coronal hole signatures in the ecliptic plane at L1 with polar coronal holes using the Fisk field. It shows how sources from both the solar equator and poles influence the solar wind at L1 and how the two models compliment each other to identify these sources.","sentences":["The solar minima between solar cycles 22-23, 23-24 and 24-25 are the best observed minima on record.","In situ solar wind and interplanetary magnetic field measurements by the WIND and ACE spacecraft at L1 with one-hour cadence are explored using wavelet analyses for the most quiescent year during each minimum.","Times of local peaks in periodicities are identified in the solar wind velocity, magnetic field components, and proton number densities.","The measured radial velocities at these times are used to trace magnetic field lines to the photosphere using two models.","The first is the Fisk heliospheric magnetic field that traces field lines from L1 to the photosphere.","They connect exclusively to solar poles and in 88% instances to locations of polar coronal holes.","The second model uses the Parker spiral to trace from L1 to the solar source surface and potential field extrapolations from the source surface to the photosphere.","These field lines terminate at equatorial and mid-latitude coordinates of which some are located close to coronal holes.","This study connects for the first time coronal hole signatures in the ecliptic plane at L1 with polar coronal holes using the Fisk field.","It shows how sources from both the solar equator and poles influence the solar wind at L1 and how the two models compliment each other to identify these sources."],"url":"http://arxiv.org/abs/2404.11219v1","category":"astro-ph.SR"}
{"created":"2024-04-17 09:33:31","title":"GhostNetV3: Exploring the Training Strategies for Compact Models","abstract":"Compact neural networks are specially designed for applications on edge devices with faster inference speed yet modest performance. However, training strategies of compact models are borrowed from that of conventional models at present, which ignores their difference in model capacity and thus may impede the performance of compact models. In this paper, by systematically investigating the impact of different training ingredients, we introduce a strong training strategy for compact models. We find that the appropriate designs of re-parameterization and knowledge distillation are crucial for training high-performance compact models, while some commonly used data augmentations for training conventional models, such as Mixup and CutMix, lead to worse performance. Our experiments on ImageNet-1K dataset demonstrate that our specialized training strategy for compact models is applicable to various architectures, including GhostNetV2, MobileNetV2 and ShuffleNetV2. Specifically, equipped with our strategy, GhostNetV3 1.3$\\times$ achieves a top-1 accuracy of 79.1% with only 269M FLOPs and a latency of 14.46ms on mobile devices, surpassing its ordinarily trained counterpart by a large margin. Moreover, our observation can also be extended to object detection scenarios. PyTorch code and checkpoints can be found at https://github.com/huawei-noah/Efficient-AI-Backbones/tree/master/ghostnetv3_pytorch.","sentences":["Compact neural networks are specially designed for applications on edge devices with faster inference speed yet modest performance.","However, training strategies of compact models are borrowed from that of conventional models at present, which ignores their difference in model capacity and thus may impede the performance of compact models.","In this paper, by systematically investigating the impact of different training ingredients, we introduce a strong training strategy for compact models.","We find that the appropriate designs of re-parameterization and knowledge distillation are crucial for training high-performance compact models, while some commonly used data augmentations for training conventional models, such as Mixup and CutMix, lead to worse performance.","Our experiments on ImageNet-1K dataset demonstrate that our specialized training strategy for compact models is applicable to various architectures, including GhostNetV2, MobileNetV2 and ShuffleNetV2.","Specifically, equipped with our strategy, GhostNetV3 1.3$\\times$ achieves a top-1 accuracy of 79.1% with only 269M FLOPs and a latency of 14.46ms on mobile devices, surpassing its ordinarily trained counterpart by a large margin.","Moreover, our observation can also be extended to object detection scenarios.","PyTorch code and checkpoints can be found at https://github.com/huawei-noah/Efficient-AI-Backbones/tree/master/ghostnetv3_pytorch."],"url":"http://arxiv.org/abs/2404.11202v1","category":"cs.CV"}
{"created":"2024-04-17 08:29:54","title":"Determining the time before or after a galaxy merger event","abstract":"Aims. This work aims to reproduce the time before or after a merger event of merging galaxies from the IllustrisTNG cosmological simulation using machine learning.   Methods. Images of merging galaxies were created in the u, g, r, and i bands from IllustrisTNG. The merger times were determined using the time difference between the last simulation snapshot where the merging galaxies were tracked as two galaxies and the first snapshot where the merging galaxies were tracked as a single galaxy. This time was then further refined using simple gravity simulations. These data were then used to train a residual network (ResNet50), a Swin Transformer (Swin), a convolutional neural network (CNN), and an autoencoder (using a single latent neuron) to reproduce the merger time. The full latent space of the autoencoder was also studied to see if it reproduces the merger time better than the other methods. This was done by reducing the latent space dimensions using Isomap, linear discriminant analysis (LDA), neighbourhood components analysis, sparse random projection, truncated singular value decomposition and uniform manifold approximation and projection.   Results. The CNN is the best of all the neural networks. The performance of the autoencoder was close to the CNN, with Swin close behind the autoencoder. ResNet50 performed the worst. The LDA dimensionality reduction performed the best of the six methods used. The exploration of the full latent space produced worse results than the single latent neuron of the autoencoder. For the test data set, we found a median error of 190 Myr, comparable to the time separation between snapshots in IllustrisTNG. Galaxies more than $\\approx$ 625 Myr before a merger have poorly recovered merger times, as well as galaxies more than $\\approx$ 125 Myr after a merger event.","sentences":["Aims.","This work aims to reproduce the time before or after a merger event of merging galaxies from the IllustrisTNG cosmological simulation using machine learning.   Methods.","Images of merging galaxies were created in the u, g, r, and i bands from IllustrisTNG.","The merger times were determined using the time difference between the last simulation snapshot where the merging galaxies were tracked as two galaxies and the first snapshot where the merging galaxies were tracked as a single galaxy.","This time was then further refined using simple gravity simulations.","These data were then used to train a residual network (ResNet50), a Swin Transformer (Swin), a convolutional neural network (CNN), and an autoencoder (using a single latent neuron) to reproduce the merger time.","The full latent space of the autoencoder was also studied to see if it reproduces the merger time better than the other methods.","This was done by reducing the latent space dimensions using Isomap, linear discriminant analysis (LDA), neighbourhood components analysis, sparse random projection, truncated singular value decomposition and uniform manifold approximation and projection.   ","Results.","The CNN is the best of all the neural networks.","The performance of the autoencoder was close to the CNN, with Swin close behind the autoencoder.","ResNet50 performed the worst.","The LDA dimensionality reduction performed the best of the six methods used.","The exploration of the full latent space produced worse results than the single latent neuron of the autoencoder.","For the test data set, we found a median error of 190 Myr, comparable to the time separation between snapshots in IllustrisTNG.","Galaxies more than $\\approx$ 625 Myr before a merger have poorly recovered merger times, as well as galaxies more than $\\approx$ 125 Myr after a merger event."],"url":"http://arxiv.org/abs/2404.11166v1","category":"astro-ph.GA"}
{"created":"2024-04-17 07:29:25","title":"Co-existence of Type II blow-ups with multiple blow-up rates for five-dimensional heat equation with critical nonlinear boundary conditions","abstract":"We consider the following five-dimensional heat equation with critical boundary condition \\begin{equation*}   \\partial_t u=\\Delta u   \\mbox{ \\ in \\ } \\mathbb{R}_+^5\\times (0,T) ,   \\quad   -\\partial_{x_5}u =|u|^\\frac{2}{3}u \\mbox{ \\ on \\ } \\pp \\mathbb{R}^5_+ \\times (0,T) . \\end{equation*} Given $\\mathfrak{o}$ distinct boundary points $q^{[i]} \\in \\partial \\mathbb{R}_+^5$, and $\\mathfrak{o}$ integers $l_i\\in \\mathbb{N}$ (possibly duplicated), $i=1,2,\\dots, \\mathfrak{o}$, for $T>0$ sufficiently small, we construct a finite-time blow-up solution $u$ with a type II blow-up rate $(T-t)^{-3l_i -3}$ for $x$ near $q^{[i]}$. This seems to be the first result of the co-existence of type II blowups with different blow-up rates. To accommodate highly unstable blowups with different blowup rates, we first develop a unified linear theory for the inner problem with more time decay in the blow-up scheme through restriction on the spatial growth of the right-hand side, and then use vanishing adjustment functions for deriving multiple rates at distinct points. This paper is inspired by [25, 52, 60].","sentences":["We consider the following five-dimensional heat equation with critical boundary condition \\begin{equation*}   \\partial_t u=\\Delta u   \\mbox{ \\ in \\ } \\mathbb{R}_+^5\\times (0,T) ,   \\quad   -\\partial_{x_5}u =|u|^\\frac{2}{3}u \\mbox{ \\ on \\ } \\pp \\mathbb{R}^5_+ \\times (0,T) .","\\end{equation*} Given $\\mathfrak{o}$ distinct boundary points $q^{[i]} \\in \\partial \\mathbb{R}_+^5$, and $\\mathfrak{o}$ integers $l_i\\in \\mathbb{N}$ (possibly duplicated), $i=1,2,\\dots, \\mathfrak{o}$, for $T>0$ sufficiently small, we construct a finite-time blow-up solution $u$ with a type II blow-up rate $(T-t)^{-3l_i -3}$ for $x$ near $q^{[i]}$. This seems to be the first result of the co-existence of type II blowups with different blow-up rates.","To accommodate highly unstable blowups with different blowup rates, we first develop a unified linear theory for the inner problem with more time decay in the blow-up scheme through restriction on the spatial growth of the right-hand side, and then use vanishing adjustment functions for deriving multiple rates at distinct points.","This paper is inspired by [25, 52, 60]."],"url":"http://arxiv.org/abs/2404.11134v1","category":"math.AP"}
{"created":"2024-04-17 06:36:20","title":"Non-existence of free boundary minimal M\u00f6bius bands in the unit three-ball","abstract":"We prove the impossibility of constructing free boundary minimal M\\\"obius bands in the Euclidean ball $\\mathbb{B}^3$. This answers in the negative a question proposed by I. Fern\\'andez, L. Hauswirth and P. Mira.","sentences":["We prove the impossibility of constructing free boundary minimal M\\\"obius bands in the Euclidean ball $\\mathbb{B}^3$. This answers in the negative a question proposed by I. Fern\\'andez, L. Hauswirth and P. Mira."],"url":"http://arxiv.org/abs/2404.11101v1","category":"math.DG"}
{"created":"2024-04-17 05:32:50","title":"Prospects for detecting the hidden-strange pentaquarklike state $N^{*}(2080)$ in the $\u03c0^{-} p\\rightarrow\u03c6n$ reaction","abstract":"In this work, the production of the hidden-strange pentaquarklike state $N^{*}(2080)$ via the $\\pi^{-} p$ scattering process is studied by the effective Lagrangian approach. Concretely, we consider the $\\rho$ meson exchange of $t$-channel and the nucleon exchange of $u$-channel, which are treated as the background terms, and take into account the contribution of the $N^{*}(2080)$ via the $s$-channel as a signal term. By global fitting of the total and differential cross sections of the $\\pi^{-} p\\rightarrow\\phi n$ process, it is shown that the $N^{*}(2080)$ contributes an obvious peak at the threshold energy point of the differential cross section at the forward angle ($\\cos\\theta = 1$), which provides clues to the detection of the $N^{*}(2080)$ by the $\\pi^{-} p$ scattering process. However, due to the limited accuracy of the experimental data, it is an objective limitation for us to determine the properties of the $N^{*}(2080)$ by the $\\pi^{-} p$ scattering process. Therefore, more accurate experimental measurements of the $\\pi^{-} p\\rightarrow\\phi n$ reaction are highly desirable, and we have also proposed that correlation measurements can be performed on J-PARC, AMBER, and future HIKE and HIAF meson beam experiments.","sentences":["In this work, the production of the hidden-strange pentaquarklike state $N^{*}(2080)$ via the $\\pi^{-} p$ scattering process is studied by the effective Lagrangian approach.","Concretely, we consider the $\\rho$ meson exchange of $t$-channel and the nucleon exchange of $u$-channel, which are treated as the background terms, and take into account the contribution of the $N^{*}(2080)$ via the $s$-channel as a signal term.","By global fitting of the total and differential cross sections of the $\\pi^{-} p\\rightarrow\\phi n$ process, it is shown that the $N^{*}(2080)$ contributes an obvious peak at the threshold energy point of the differential cross section at the forward angle ($\\cos\\theta = 1$), which provides clues to the detection of the $N^{*}(2080)$ by the $\\pi^{-} p$ scattering process.","However, due to the limited accuracy of the experimental data, it is an objective limitation for us to determine the properties of the $N^{*}(2080)$ by the $\\pi^{-} p$ scattering process.","Therefore, more accurate experimental measurements of the $\\pi^{-} p\\rightarrow\\phi n$ reaction are highly desirable, and we have also proposed that correlation measurements can be performed on J-PARC, AMBER, and future HIKE and HIAF meson beam experiments."],"url":"http://arxiv.org/abs/2404.11078v1","category":"hep-ph"}
{"created":"2024-04-17 04:41:19","title":"Attitudinal Loyalty Manifestation in Banking CSR: Cross-Buying Behavior and Customer Advocacy","abstract":"This study in the banking industry examines the influence of attitudinal loyalty on customer advocacy and cross buying behavior, alongside the moderating roles of Quality of Life and Corporate Social Responsibility support in the CSR fit and loyalty relationship. Employing Structural Equation Modeling, it reveals that higher attitudinal loyalty significantly boosts customer advocacy and propensity for cross buying. The findings highlight the importance of nurturing customer loyalty through valuable and relevant offerings, as CSR fit alone does not define the loyalty of the banking customer. Banks are advised to target customers with a high Quality of Life and engage with those who support CSR initiatives aligning with the banks objectives, to enhance loyalty and deepen customer relationships.","sentences":["This study in the banking industry examines the influence of attitudinal loyalty on customer advocacy and cross buying behavior, alongside the moderating roles of Quality of Life and Corporate Social Responsibility support in the CSR fit and loyalty relationship.","Employing Structural Equation Modeling, it reveals that higher attitudinal loyalty significantly boosts customer advocacy and propensity for cross buying.","The findings highlight the importance of nurturing customer loyalty through valuable and relevant offerings, as CSR fit alone does not define the loyalty of the banking customer.","Banks are advised to target customers with a high Quality of Life and engage with those who support CSR initiatives aligning with the banks objectives, to enhance loyalty and deepen customer relationships."],"url":"http://arxiv.org/abs/2404.11063v1","category":"econ.GN"}
{"created":"2024-04-17 03:01:47","title":"Spatial-Aware Image Retrieval: A Hyperdimensional Computing Approach for Efficient Similarity Hashing","abstract":"In the face of burgeoning image data, efficiently retrieving similar images poses a formidable challenge. Past research has focused on refining hash functions to distill images into compact indicators of resemblance. Initial attempts used shallow models, evolving to attention mechanism-based architectures from Convolutional Neural Networks (CNNs) to advanced models. Recognizing limitations in gradient-based models for spatial information embedding, we propose an innovative image hashing method, NeuroHash leveraging Hyperdimensional Computing (HDC). HDC symbolically encodes spatial information into high-dimensional vectors, reshaping image representation. Our approach combines pre-trained large vision models with HDC operations, enabling spatially encoded feature representations. Hashing with locality-sensitive hashing (LSH) ensures swift and efficient image retrieval. Notably, our framework allows dynamic hash manipulation for conditional image retrieval. Our work introduces a transformative image hashing framework enabling spatial-aware conditional retrieval. By seamlessly combining DNN-based neural and HDC-based symbolic models, our methodology breaks from traditional training, offering flexible and conditional image retrieval. Performance evaluations signify a paradigm shift in image-hashing methodologies, demonstrating enhanced retrieval accuracy.","sentences":["In the face of burgeoning image data, efficiently retrieving similar images poses a formidable challenge.","Past research has focused on refining hash functions to distill images into compact indicators of resemblance.","Initial attempts used shallow models, evolving to attention mechanism-based architectures from Convolutional Neural Networks (CNNs) to advanced models.","Recognizing limitations in gradient-based models for spatial information embedding, we propose an innovative image hashing method, NeuroHash leveraging Hyperdimensional Computing (HDC).","HDC symbolically encodes spatial information into high-dimensional vectors, reshaping image representation.","Our approach combines pre-trained large vision models with HDC operations, enabling spatially encoded feature representations.","Hashing with locality-sensitive hashing (LSH) ensures swift and efficient image retrieval.","Notably, our framework allows dynamic hash manipulation for conditional image retrieval.","Our work introduces a transformative image hashing framework enabling spatial-aware conditional retrieval.","By seamlessly combining DNN-based neural and HDC-based symbolic models, our methodology breaks from traditional training, offering flexible and conditional image retrieval.","Performance evaluations signify a paradigm shift in image-hashing methodologies, demonstrating enhanced retrieval accuracy."],"url":"http://arxiv.org/abs/2404.11025v1","category":"cs.CV"}
{"created":"2024-04-17 02:52:39","title":"Non-Hermitian butterfly spectra in a family of quasiperiodic lattices","abstract":"We propose a family of exactly solvable quasiperiodic lattice models with analytical complex mobility edges, which can incorporate mosaic modulations as a straightforward generalization. By sweeping a potential tuning parameter $\\delta$, we demonstrate a kind of interesting butterfly-like spectra in complex energy plane, which depicts energy-dependent extended-localized transitions sharing a common exact non-Hermitian mobility edge. Applying Avila's global theory, we are able to analytically calculate the Lyapunov exponents and determine the mobility edges exactly. For the minimal model without mosaic modulation, a compactly analytic formula for the complex mobility edges is obtained, which, together with analytical estimation of the range of complex energy spectrum, gives the true mobility edge. The non-Hermitian mobility edge in complex energy plane is further verified by numerical calculations of fractal dimension and spatial distribution of wave functions. Tuning parameters of non-Hermitian potentials, we also investigate the variations of the non-Hermitian mobility edges and the corresponding butterfly spectra, which exhibit richness of spectrum structures.","sentences":["We propose a family of exactly solvable quasiperiodic lattice models with analytical complex mobility edges, which can incorporate mosaic modulations as a straightforward generalization.","By sweeping a potential tuning parameter $\\delta$, we demonstrate a kind of interesting butterfly-like spectra in complex energy plane, which depicts energy-dependent extended-localized transitions sharing a common exact non-Hermitian mobility edge.","Applying Avila's global theory, we are able to analytically calculate the Lyapunov exponents and determine the mobility edges exactly.","For the minimal model without mosaic modulation, a compactly analytic formula for the complex mobility edges is obtained, which, together with analytical estimation of the range of complex energy spectrum, gives the true mobility edge.","The non-Hermitian mobility edge in complex energy plane is further verified by numerical calculations of fractal dimension and spatial distribution of wave functions.","Tuning parameters of non-Hermitian potentials, we also investigate the variations of the non-Hermitian mobility edges and the corresponding butterfly spectra, which exhibit richness of spectrum structures."],"url":"http://arxiv.org/abs/2404.11020v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-17 02:52:11","title":"You do not have to train Graph Neural Networks at all on text-attributed graphs","abstract":"Graph structured data, specifically text-attributed graphs (TAG), effectively represent relationships among varied entities. Such graphs are essential for semi-supervised node classification tasks. Graph Neural Networks (GNNs) have emerged as a powerful tool for handling this graph-structured data. Although gradient descent is commonly utilized for training GNNs for node classification, this study ventures into alternative methods, eliminating the iterative optimization processes. We introduce TrainlessGNN, a linear GNN model capitalizing on the observation that text encodings from the same class often cluster together in a linear subspace. This model constructs a weight matrix to represent each class's node attribute subspace, offering an efficient approach to semi-supervised node classification on TAG. Extensive experiments reveal that our trainless models can either match or even surpass their conventionally trained counterparts, demonstrating the possibility of refraining from gradient descent in certain configurations.","sentences":["Graph structured data, specifically text-attributed graphs (TAG), effectively represent relationships among varied entities.","Such graphs are essential for semi-supervised node classification tasks.","Graph Neural Networks (GNNs) have emerged as a powerful tool for handling this graph-structured data.","Although gradient descent is commonly utilized for training GNNs for node classification, this study ventures into alternative methods, eliminating the iterative optimization processes.","We introduce TrainlessGNN, a linear GNN model capitalizing on the observation that text encodings from the same class often cluster together in a linear subspace.","This model constructs a weight matrix to represent each class's node attribute subspace, offering an efficient approach to semi-supervised node classification on TAG.","Extensive experiments reveal that our trainless models can either match or even surpass their conventionally trained counterparts, demonstrating the possibility of refraining from gradient descent in certain configurations."],"url":"http://arxiv.org/abs/2404.11019v1","category":"cs.LG"}
{"created":"2024-04-17 02:37:21","title":"It\u014d and It\u014d-Wentzell chain rule for flows of conditional laws of continuous semimartingales: an easy approach","abstract":"We provide a general It\\=o\\,-Wentzell formula for a random field of maps on the Wasserstein space of probability measures, defined by continuous semimartingales, and evaluated along the flow of conditional distributions of another continuous semimartingale. Our method follows standard arguments of It\\=o calculus, and thus bypasses the approximation by empirical measures commonly used in the existing literature. As an application, we derive the dynamic programming equation for a mean field stochastic control problem with common noise.","sentences":["We provide a general It\\=o\\,-Wentzell formula for a random field of maps on the Wasserstein space of probability measures, defined by continuous semimartingales, and evaluated along the flow of conditional distributions of another continuous semimartingale.","Our method follows standard arguments of It\\=o calculus, and thus bypasses the approximation by empirical measures commonly used in the existing literature.","As an application, we derive the dynamic programming equation for a mean field stochastic control problem with common noise."],"url":"http://arxiv.org/abs/2404.11010v1","category":"math.PR"}
{"created":"2024-04-17 02:29:44","title":"InfoMatch: Entropy Neural Estimation for Semi-Supervised Image Classification","abstract":"Semi-supervised image classification, leveraging pseudo supervision and consistency regularization, has demonstrated remarkable success. However, the ongoing challenge lies in fully exploiting the potential of unlabeled data. To address this, we employ information entropy neural estimation to harness the potential of unlabeled samples. Inspired by contrastive learning, the entropy is estimated by maximizing a lower bound on mutual information across different augmented views. Moreover, we theoretically analyze that the information entropy of the posterior of an image classifier is approximated by maximizing the likelihood function of the softmax predictions. Guided by these insights, we optimize our model from both perspectives to ensure that the predicted probability distribution closely aligns with the ground-truth distribution. Given the theoretical connection to information entropy, we name our method \\textit{InfoMatch}. Through extensive experiments, we show its superior performance.","sentences":["Semi-supervised image classification, leveraging pseudo supervision and consistency regularization, has demonstrated remarkable success.","However, the ongoing challenge lies in fully exploiting the potential of unlabeled data.","To address this, we employ information entropy neural estimation to harness the potential of unlabeled samples.","Inspired by contrastive learning, the entropy is estimated by maximizing a lower bound on mutual information across different augmented views.","Moreover, we theoretically analyze that the information entropy of the posterior of an image classifier is approximated by maximizing the likelihood function of the softmax predictions.","Guided by these insights, we optimize our model from both perspectives to ensure that the predicted probability distribution closely aligns with the ground-truth distribution.","Given the theoretical connection to information entropy, we name our method \\textit{InfoMatch}.","Through extensive experiments, we show its superior performance."],"url":"http://arxiv.org/abs/2404.11003v1","category":"cs.CV"}
{"created":"2024-04-17 01:50:38","title":"A Relative Inexact Proximal Gradient Method With an Explicit Linesearch","abstract":"This paper presents and investigates an inexact proximal gradient method for solving composite convex optimization problems characterized by an objective function composed of a sum of a full domain differentiable convex function and a non-differentiable convex function. We introduce an explicit linesearch strategy that requires only a relative inexact solution of the proximal subproblem per iteration. We prove the convergence of the sequence generated by our scheme and establish its iteration complexity, considering both the functional values and a residual associated with first-order stationary solutions. Additionally, we provide numerical experiments to illustrate the practical efficacy of our method.","sentences":["This paper presents and investigates an inexact proximal gradient method for solving composite convex optimization problems characterized by an objective function composed of a sum of a full domain differentiable convex function and a non-differentiable convex function.","We introduce an explicit linesearch strategy that requires only a relative inexact solution of the proximal subproblem per iteration.","We prove the convergence of the sequence generated by our scheme and establish its iteration complexity, considering both the functional values and a residual associated with first-order stationary solutions.","Additionally, we provide numerical experiments to illustrate the practical efficacy of our method."],"url":"http://arxiv.org/abs/2404.10987v1","category":"math.OC"}
{"created":"2024-04-17 00:59:31","title":"Well-posedness for viscosity solutions of the one-phase Muskat problem in all dimensions","abstract":"In this article, we apply the viscosity solutions theory for integro-differential equations to the \\emph{one-phase} Muskat equation (also known as the Hele-Shaw problem with gravity). We prove global well-posedness for the corresponding Hamilton-Jacobi-Bellmann equation with bounded, uniformly continuous initial data, in all dimensions.","sentences":["In this article, we apply the viscosity solutions theory for integro-differential equations to the \\emph{one-phase} Muskat equation (also known as the Hele-Shaw problem with gravity).","We prove global well-posedness for the corresponding Hamilton-Jacobi-Bellmann equation with bounded, uniformly continuous initial data, in all dimensions."],"url":"http://arxiv.org/abs/2404.10972v1","category":"math.AP"}
{"created":"2024-04-17 00:54:59","title":"Strategies for Machine Learning Applied to Noisy HEP Datasets: Modular Solid State Detectors from SuperCDMS","abstract":"Background reduction in the SuperCDMS dark matter experiment depends on removing surface events within individual detectors by identifying the location of each incident particle interaction. Position reconstruction is achieved by combining pulse shape information over multiple phonon channels, a task well-suited to machine learning techniques. Data from an Am-241 scan of a SuperCDMS SNOLAB detector was used to study a selection of statistical approaches, including linear regression, artificial neural networks, and symbolic regression. Our results showed that simpler linear regression models were better able than artificial neural networks to generalize on such a noisy and minimal data set, but there are indications that certain architectures and training configurations can counter overfitting tendencies. This study will be repeated on a more complete SuperCDMS data set (in progress) to explore the interplay between data quality and the application of neural networks.","sentences":["Background reduction in the SuperCDMS dark matter experiment depends on removing surface events within individual detectors by identifying the location of each incident particle interaction.","Position reconstruction is achieved by combining pulse shape information over multiple phonon channels, a task well-suited to machine learning techniques.","Data from an Am-241 scan of a SuperCDMS SNOLAB detector was used to study a selection of statistical approaches, including linear regression, artificial neural networks, and symbolic regression.","Our results showed that simpler linear regression models were better able than artificial neural networks to generalize on such a noisy and minimal data set, but there are indications that certain architectures and training configurations can counter overfitting tendencies.","This study will be repeated on a more complete SuperCDMS data set (in progress) to explore the interplay between data quality and the application of neural networks."],"url":"http://arxiv.org/abs/2404.10971v1","category":"hep-ex"}
{"created":"2024-04-16 22:49:27","title":"Unsupervised machine learning for the detection of exotic phases in skyrmion phase diagrams","abstract":"Undoubtedly, machine learning techniques are being increasingly applied to a wide range of situations in the field of condensed matter. Amongst these techniques, unsupervised techniques are especially attractive, since they imply the possibility of extracting information from the data without previous labeling. In this work, we resort to the technique known as anomaly detection to explore potential exotic phases in skyrmion phase diagrams, using two different algorithms: Principal Component Analysis and Convolutional Autoencoder (CAE). First, we train these algorithms with an artificial dataset of skyrmion lattices constructed from an analytical parametrization, for different magnetizations, skyrmion lattice orientations, and skyrmion radii. We apply the trained algorithms to a set of snapshots obtained from Monte Carlo simulations for three ferromagnetic skyrmion models: two with in-plane Dzyaloshinskii-Moriya (DMI) in the triangular and kagome lattices, and one with an additional out of plane DMI in the kagome lattice. Then, we compare the root mean square error (RMSE) and the binary cross entropy between the input and output snapshots as a function of the external magnetic field and temperature. We find that the RMSE and its variance in the CAE case may be useful to not only detect exotic low temperature phases, but also to differentiate between the characteristic low temperature orderings of a skyrmion phase diagram (helical, skyrmions and ferromagnetic). Moreover, we apply the skyrmion trained CAE to two antiferromagnetic models in the triangular lattice, one that gives rise to antiferromagnetic skyrmions, and the pure exchange antiferromagnetic case. Despite the predictably larger RMSE, we find that, even in these cases, RMSE is also an indicator of different orderings and the emergence of particular features, such as the well-known pseudo-plateau in the pure exchange case.","sentences":["Undoubtedly, machine learning techniques are being increasingly applied to a wide range of situations in the field of condensed matter.","Amongst these techniques, unsupervised techniques are especially attractive, since they imply the possibility of extracting information from the data without previous labeling.","In this work, we resort to the technique known as anomaly detection to explore potential exotic phases in skyrmion phase diagrams, using two different algorithms: Principal Component Analysis and Convolutional Autoencoder (CAE).","First, we train these algorithms with an artificial dataset of skyrmion lattices constructed from an analytical parametrization, for different magnetizations, skyrmion lattice orientations, and skyrmion radii.","We apply the trained algorithms to a set of snapshots obtained from Monte Carlo simulations for three ferromagnetic skyrmion models: two with in-plane Dzyaloshinskii-Moriya (DMI) in the triangular and kagome lattices, and one with an additional out of plane DMI in the kagome lattice.","Then, we compare the root mean square error (RMSE) and the binary cross entropy between the input and output snapshots as a function of the external magnetic field and temperature.","We find that the RMSE and its variance in the CAE case may be useful to not only detect exotic low temperature phases, but also to differentiate between the characteristic low temperature orderings of a skyrmion phase diagram (helical, skyrmions and ferromagnetic).","Moreover, we apply the skyrmion trained CAE to two antiferromagnetic models in the triangular lattice, one that gives rise to antiferromagnetic skyrmions, and the pure exchange antiferromagnetic case.","Despite the predictably larger RMSE, we find that, even in these cases, RMSE is also an indicator of different orderings and the emergence of particular features, such as the well-known pseudo-plateau in the pure exchange case."],"url":"http://arxiv.org/abs/2404.10943v1","category":"cond-mat.str-el"}
{"created":"2024-04-16 22:44:29","title":"Neuromorphic Vision-based Motion Segmentation with Graph Transformer Neural Network","abstract":"Moving object segmentation is critical to interpret scene dynamics for robotic navigation systems in challenging environments. Neuromorphic vision sensors are tailored for motion perception due to their asynchronous nature, high temporal resolution, and reduced power consumption. However, their unconventional output requires novel perception paradigms to leverage their spatially sparse and temporally dense nature. In this work, we propose a novel event-based motion segmentation algorithm using a Graph Transformer Neural Network, dubbed GTNN. Our proposed algorithm processes event streams as 3D graphs by a series of nonlinear transformations to unveil local and global spatiotemporal correlations between events. Based on these correlations, events belonging to moving objects are segmented from the background without prior knowledge of the dynamic scene geometry. The algorithm is trained on publicly available datasets including MOD, EV-IMO, and \\textcolor{black}{EV-IMO2} using the proposed training scheme to facilitate efficient training on extensive datasets. Moreover, we introduce the Dynamic Object Mask-aware Event Labeling (DOMEL) approach for generating approximate ground-truth labels for event-based motion segmentation datasets. We use DOMEL to label our own recorded Event dataset for Motion Segmentation (EMS-DOMEL), which we release to the public for further research and benchmarking. Rigorous experiments are conducted on several unseen publicly-available datasets where the results revealed that GTNN outperforms state-of-the-art methods in the presence of dynamic background variations, motion patterns, and multiple dynamic objects with varying sizes and velocities. GTNN achieves significant performance gains with an average increase of 9.4% and 4.5% in terms of motion segmentation accuracy (IoU%) and detection rate (DR%), respectively.","sentences":["Moving object segmentation is critical to interpret scene dynamics for robotic navigation systems in challenging environments.","Neuromorphic vision sensors are tailored for motion perception due to their asynchronous nature, high temporal resolution, and reduced power consumption.","However, their unconventional output requires novel perception paradigms to leverage their spatially sparse and temporally dense nature.","In this work, we propose a novel event-based motion segmentation algorithm using a Graph Transformer Neural Network, dubbed GTNN.","Our proposed algorithm processes event streams as 3D graphs by a series of nonlinear transformations to unveil local and global spatiotemporal correlations between events.","Based on these correlations, events belonging to moving objects are segmented from the background without prior knowledge of the dynamic scene geometry.","The algorithm is trained on publicly available datasets including MOD, EV-IMO, and \\textcolor{black}{EV-IMO2} using the proposed training scheme to facilitate efficient training on extensive datasets.","Moreover, we introduce the Dynamic Object Mask-aware Event Labeling (DOMEL) approach for generating approximate ground-truth labels for event-based motion segmentation datasets.","We use DOMEL to label our own recorded Event dataset for Motion Segmentation (EMS-DOMEL), which we release to the public for further research and benchmarking.","Rigorous experiments are conducted on several unseen publicly-available datasets where the results revealed that GTNN outperforms state-of-the-art methods in the presence of dynamic background variations, motion patterns, and multiple dynamic objects with varying sizes and velocities.","GTNN achieves significant performance gains with an average increase of 9.4% and 4.5% in terms of motion segmentation accuracy (IoU%) and detection rate (DR%), respectively."],"url":"http://arxiv.org/abs/2404.10940v1","category":"cs.CV"}
{"created":"2024-04-16 22:15:52","title":"Molecular relaxation by reverse diffusion with time step prediction","abstract":"Molecular relaxation, finding the equilibrium state of a non-equilibrium structure, is an essential component of computational chemistry to understand reactivity. Classical force field methods often rely on insufficient local energy minimization, while neural network force field models require large labeled datasets encompassing both equilibrium and non-equilibrium structures. As a remedy, we propose MoreRed, molecular relaxation by reverse diffusion, a conceptually novel and purely statistical approach where non-equilibrium structures are treated as noisy instances of their corresponding equilibrium states. To enable the denoising of arbitrarily noisy inputs via a generative diffusion model, we further introduce a novel diffusion time step predictor. Notably, MoreRed learns a simpler pseudo potential energy surface instead of the complex physical potential energy surface. It is trained on a significantly smaller, and thus computationally cheaper, dataset consisting of solely unlabeled equilibrium structures, avoiding the computation of non-equilibrium structures altogether. We compare MoreRed to classical force fields, equivariant neural network force fields trained on a large dataset of equilibrium and non-equilibrium data, as well as a semi-empirical tight-binding model. To assess this quantitatively, we evaluate the root-mean-square deviation between the found equilibrium structures and the reference equilibrium structures as well as their DFT energies.","sentences":["Molecular relaxation, finding the equilibrium state of a non-equilibrium structure, is an essential component of computational chemistry to understand reactivity.","Classical force field methods often rely on insufficient local energy minimization, while neural network force field models require large labeled datasets encompassing both equilibrium and non-equilibrium structures.","As a remedy, we propose MoreRed, molecular relaxation by reverse diffusion, a conceptually novel and purely statistical approach where non-equilibrium structures are treated as noisy instances of their corresponding equilibrium states.","To enable the denoising of arbitrarily noisy inputs via a generative diffusion model, we further introduce a novel diffusion time step predictor.","Notably, MoreRed learns a simpler pseudo potential energy surface instead of the complex physical potential energy surface.","It is trained on a significantly smaller, and thus computationally cheaper, dataset consisting of solely unlabeled equilibrium structures, avoiding the computation of non-equilibrium structures altogether.","We compare MoreRed to classical force fields, equivariant neural network force fields trained on a large dataset of equilibrium and non-equilibrium data, as well as a semi-empirical tight-binding model.","To assess this quantitatively, we evaluate the root-mean-square deviation between the found equilibrium structures and the reference equilibrium structures as well as their DFT energies."],"url":"http://arxiv.org/abs/2404.10935v1","category":"physics.chem-ph"}
{"created":"2024-04-16 22:03:56","title":"The Relationship between Consumer Theories with and without Utility Maximization","abstract":"To study the assumption that the utility maximization hypothesis implicitly adds to consumer theory, we consider a mathematical representation of pre-marginal revolution consumer theory based on subjective exchange ratios. We introduce two axioms on subjective exchange ratio, and show that both axioms hold if and only if consumer behavior is consistent with the utility maximization hypothesis. Moreover, we express the process for a consumer to find the transaction stopping point in terms of differential equations, and prove that the conditions for its stability are equal to the two axioms introduced in the above argument. Therefore, the consumer can find his/her transaction stopping point if and only if his/her behavior is consistent with the utility maximization hypothesis. In addition to these results, we discuss equivalence conditions for axioms to evaluate their mathematical strength, and methods for expressing the theory of subjective exchange ratios in terms of binary relations.","sentences":["To study the assumption that the utility maximization hypothesis implicitly adds to consumer theory, we consider a mathematical representation of pre-marginal revolution consumer theory based on subjective exchange ratios.","We introduce two axioms on subjective exchange ratio, and show that both axioms hold if and only if consumer behavior is consistent with the utility maximization hypothesis.","Moreover, we express the process for a consumer to find the transaction stopping point in terms of differential equations, and prove that the conditions for its stability are equal to the two axioms introduced in the above argument.","Therefore, the consumer can find his/her transaction stopping point if and only if his/her behavior is consistent with the utility maximization hypothesis.","In addition to these results, we discuss equivalence conditions for axioms to evaluate their mathematical strength, and methods for expressing the theory of subjective exchange ratios in terms of binary relations."],"url":"http://arxiv.org/abs/2404.10931v1","category":"econ.TH"}
{"created":"2024-04-16 20:39:44","title":"Nonnegative tensor train for the multicomponent Smoluchowski equation","abstract":"We propose an efficient implementation of the numerical tensor-train (TT) based algorithm solving the multicomponent coagulation equation preserving the nonnegativeness of solution. Unnatural negative elements in the constructed approximation arise due to the errors of the low-rank decomposition and discretization scheme. In this work, we propose to apply the rank-one corrections in the TT-format proportional to the minimal negative element. Such an element can be found via application of the global optimization methods that can be fully implemented within efficient operations in the tensor train format. We incorporate this trick into the time-integration scheme for the multicomponent coagulation equation and also use it for post-processing of the stationary solution for the problem with the source of particles.","sentences":["We propose an efficient implementation of the numerical tensor-train (TT) based algorithm solving the multicomponent coagulation equation preserving the nonnegativeness of solution.","Unnatural negative elements in the constructed approximation arise due to the errors of the low-rank decomposition and discretization scheme.","In this work, we propose to apply the rank-one corrections in the TT-format proportional to the minimal negative element.","Such an element can be found via application of the global optimization methods that can be fully implemented within efficient operations in the tensor train format.","We incorporate this trick into the time-integration scheme for the multicomponent coagulation equation and also use it for post-processing of the stationary solution for the problem with the source of particles."],"url":"http://arxiv.org/abs/2404.10898v1","category":"math.NA"}
{"created":"2024-04-16 20:23:16","title":"Deep Learning Methods for Colloidal Silver Nanoparticle Concentration and Size Distribution Determination from UV-Vis Extinction Spectra","abstract":"Electron microscopy, while reliable, is an expensive, slow, and inefficient technique for thorough size distribution characterization of both mono- and polydisperse colloidal nanoparticles. If rapid in-situ characterization of colloid samples is to be achieved, a different approach, based on fast, widely accessible, and inexpensive optical measurements such as UV-Vis spectroscopy in combination with spectra interpretation related to Mie scattering theory, is needed. In this article, we present a tandem deep neural network (DNN) for the size distribution and concentration prediction of close to spherical silver colloidal nanoparticle batches synthesized via the seeded growth method. The first DNN identified the dipole component of the localized surface plasmon resonance and the second one determined the size distribution from the isolated spectral component. The training data was engineered to be bias-free and generated numerically. High prediction accuracy with typical root mean square percentage error of mean size ca. 1.2% was maintained, spanning the entire prediction range up to 150 nm in radius, suggesting the possible extension limits of the effective medium theory used for simulating the spectra. The DNN-predicted nanoparticle concentrations also were very close to the ones expected based on synthesis precursor contents as well as measured by atomic absorption spectroscopy.","sentences":["Electron microscopy, while reliable, is an expensive, slow, and inefficient technique for thorough size distribution characterization of both mono- and polydisperse colloidal nanoparticles.","If rapid in-situ characterization of colloid samples is to be achieved, a different approach, based on fast, widely accessible, and inexpensive optical measurements such as UV-Vis spectroscopy in combination with spectra interpretation related to Mie scattering theory, is needed.","In this article, we present a tandem deep neural network (DNN) for the size distribution and concentration prediction of close to spherical silver colloidal nanoparticle batches synthesized via the seeded growth method.","The first DNN identified the dipole component of the localized surface plasmon resonance and the second one determined the size distribution from the isolated spectral component.","The training data was engineered to be bias-free and generated numerically.","High prediction accuracy with typical root mean square percentage error of mean size ca.","1.2% was maintained, spanning the entire prediction range up to 150 nm in radius, suggesting the possible extension limits of the effective medium theory used for simulating the spectra.","The DNN-predicted nanoparticle concentrations also were very close to the ones expected based on synthesis precursor contents as well as measured by atomic absorption spectroscopy."],"url":"http://arxiv.org/abs/2404.10891v1","category":"physics.comp-ph"}
{"created":"2024-04-16 20:02:10","title":"Differential operators on {B}ergman space on bounded symmetric domains","abstract":"We classify self-adjoint first-order differential operators on weighted Bergman spaces on the $N$-dimensional unit ball $\\mathbb{B}^N$ and $\\mathbb{D}^2$ of $2\\times2$ complex matrices satisfying $I-ZZ^*>0$.Our main tools are the discrete series representations of $\\mathrm{SU}(N,1)$ and $\\mathrm{SU}(2,2)$. We believe that our observations extend to general bounded symmetric domains.","sentences":["We classify self-adjoint first-order differential operators on weighted Bergman spaces on the $N$-dimensional unit ball $\\mathbb{B}^N$ and $\\mathbb{D}^2$ of $2\\times2$ complex matrices satisfying $I-ZZ^*>0$.Our main tools are the discrete series representations of $\\mathrm{SU}(N,1)$ and $\\mathrm{SU}(2,2)$. We believe that our observations extend to general bounded symmetric domains."],"url":"http://arxiv.org/abs/2404.10882v1","category":"math.CV"}
{"created":"2024-04-16 19:09:23","title":"Warp Factory: A Numerical Toolkit for the Analysis and Optimization of Warp Drive Geometries","abstract":"The last few decades of warp drive research have focused on analytic methods to explore warp solutions to Einstein's field equations. These analytic solutions tend to favor simple metric forms which are easier to analyze but limit the space of exploration. In addition, all solutions to date have involved unphysical qualities, such as negative energy, violation of energy conditions, or enormous energy requirements. In an effort to explore the space of physically meaningful warp drives, the Advanced Propulsion Laboratory (APL) at Applied Physics has developed Warp Factory, a toolkit written in MATLAB for numerically analyzing and optimizing warp drive geometries. Warp Factory consists of a series of three primary modules: the solver, the analyzer, and the optimizer. Together, these modules allow users to solve the Einstein field equations, compute energy conditions and scalars, and perturbatively optimize general metrics. Finally, the toolkit offers insightful 2D and 3D visualizations of general metrics and stress-energy tensors. The methods used in Warp Factory, along with their application in evaluating and optimizing common metrics, are discussed. With Warp Factory, APL hopes to accelerate warp research and bring us one step closer to physical and realizable warp drives.","sentences":["The last few decades of warp drive research have focused on analytic methods to explore warp solutions to Einstein's field equations.","These analytic solutions tend to favor simple metric forms which are easier to analyze but limit the space of exploration.","In addition, all solutions to date have involved unphysical qualities, such as negative energy, violation of energy conditions, or enormous energy requirements.","In an effort to explore the space of physically meaningful warp drives, the Advanced Propulsion Laboratory (APL) at Applied Physics has developed Warp Factory, a toolkit written in MATLAB for numerically analyzing and optimizing warp drive geometries.","Warp Factory consists of a series of three primary modules: the solver, the analyzer, and the optimizer.","Together, these modules allow users to solve the Einstein field equations, compute energy conditions and scalars, and perturbatively optimize general metrics.","Finally, the toolkit offers insightful 2D and 3D visualizations of general metrics and stress-energy tensors.","The methods used in Warp Factory, along with their application in evaluating and optimizing common metrics, are discussed.","With Warp Factory, APL hopes to accelerate warp research and bring us one step closer to physical and realizable warp drives."],"url":"http://arxiv.org/abs/2404.10855v1","category":"gr-qc"}
{"created":"2024-04-16 18:11:13","title":"VARX Granger Analysis: Modeling, Inference, and Applications","abstract":"Vector Autoregressive models with exogenous input (VARX) provide a powerful framework for modeling complex dynamical systems like brains, markets, or societies. Their simplicity allows us to uncover linear effects between endogenous and exogenous variables. The Granger formalism is naturally suited for VARX models, but the connection between the two is not widely understood. We aim to bridge this gap by providing both the basic equations and easy-to-use code. We first explain how the Granger formalism can be combined with a VARX model using deviance as a test statistic. We also present a bias correction for the deviance in the case of L2 regularization, a technique used to reduce model complexity. To address the challenge of modeling long responses, we propose the use of basis functions, which further reduce parameter complexity. We demonstrate that p-values are correctly estimated, even for short signals where regularization influences the results. Additionally, we analyze the model's performance under various scenarios where model assumptions are violated, such as missing variables or indirect observations of the underlying dynamics. Finally, we showcase the practical utility of our approach by applying it to real-world data from neuroscience, physiology, and sociology. To facilitate its adoption, we make Matlab, Python, and R code available here: https://github.com/lcparra/varx","sentences":["Vector Autoregressive models with exogenous input (VARX) provide a powerful framework for modeling complex dynamical systems like brains, markets, or societies.","Their simplicity allows us to uncover linear effects between endogenous and exogenous variables.","The Granger formalism is naturally suited for VARX models, but the connection between the two is not widely understood.","We aim to bridge this gap by providing both the basic equations and easy-to-use code.","We first explain how the Granger formalism can be combined with a VARX model using deviance as a test statistic.","We also present a bias correction for the deviance in the case of L2 regularization, a technique used to reduce model complexity.","To address the challenge of modeling long responses, we propose the use of basis functions, which further reduce parameter complexity.","We demonstrate that p-values are correctly estimated, even for short signals where regularization influences the results.","Additionally, we analyze the model's performance under various scenarios where model assumptions are violated, such as missing variables or indirect observations of the underlying dynamics.","Finally, we showcase the practical utility of our approach by applying it to real-world data from neuroscience, physiology, and sociology.","To facilitate its adoption, we make Matlab, Python, and R code available here: https://github.com/lcparra/varx"],"url":"http://arxiv.org/abs/2404.10834v1","category":"stat.ME"}
{"created":"2024-04-16 17:55:31","title":"TENG: Time-Evolving Natural Gradient for Solving PDEs with Deep Neural Net","abstract":"Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the $\\textit{Time-Evolving Natural Gradient (TENG)}$, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving machine precision in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation.","sentences":["Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering.","The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems.","In this paper, we introduce the $\\textit{Time-Evolving Natural Gradient (TENG)}$, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions.","Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency.","TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving machine precision in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation."],"url":"http://arxiv.org/abs/2404.10771v1","category":"cs.LG"}
{"created":"2024-04-16 17:53:59","title":"Finite-dimensional approximations of push-forwards on locally analytic functionals and truncation of least-squares polynomials","abstract":"This paper introduces a theoretical framework for investigating analytic maps from finite discrete data, elucidating mathematical machinery underlying the polynomial approximation with least-squares in multivariate situations. Our approach is to consider the push-forward on the space of locally analytic functionals, instead of directly handling the analytic map itself. We establish a methodology enabling appropriate finite-dimensional approximation of the push-forward from finite discrete data, through the theory of the Fourier--Borel transform and the Fock space. Moreover, we prove a rigorous convergence result with a convergence rate. As an application, we prove that it is not the least-squares polynomial, but the polynomial obtained by truncating its higher-degree terms, that approximates analytic functions and further allows for approximation beyond the support of the data distribution. One advantage of our theory is that it enables us to apply linear algebraic operations to the finite-dimensional approximation of the push-forward. Utilizing this, we prove the convergence of a method for approximating an analytic vector field from finite data of the flow map of an ordinary differential equation.","sentences":["This paper introduces a theoretical framework for investigating analytic maps from finite discrete data, elucidating mathematical machinery underlying the polynomial approximation with least-squares in multivariate situations.","Our approach is to consider the push-forward on the space of locally analytic functionals, instead of directly handling the analytic map itself.","We establish a methodology enabling appropriate finite-dimensional approximation of the push-forward from finite discrete data, through the theory of the Fourier--Borel transform and the Fock space.","Moreover, we prove a rigorous convergence result with a convergence rate.","As an application, we prove that it is not the least-squares polynomial, but the polynomial obtained by truncating its higher-degree terms, that approximates analytic functions and further allows for approximation beyond the support of the data distribution.","One advantage of our theory is that it enables us to apply linear algebraic operations to the finite-dimensional approximation of the push-forward.","Utilizing this, we prove the convergence of a method for approximating an analytic vector field from finite data of the flow map of an ordinary differential equation."],"url":"http://arxiv.org/abs/2404.10769v1","category":"math.NA"}
{"created":"2024-04-16 17:52:16","title":"Generalized Linear Response Theory for Pumped Systems and its Application to Transient Optical Properties within DPOA","abstract":"We derive the two-time linear response theory for out-of-equilibrium pumped systems, generic pump-probe delays and probe frequencies. Such a theory enormously simplifies the numerical calculations, for instance, of the optical conductivity with respect to the actual procedure, which requires computing the effect of the probe pulse for each time delay with respect to the pump pulse\\textcolor{red}{.} The theory is given for a generic observable and pumped Hamiltonian and then specialized for a system with a quadratic Hamiltonian and its transient optical properties, exploiting the Dynamical Projective Operatorial Approach (DPOA). The theory is complemented by a set of crucial numerical guidelines that help perform actual calculations in a computationally affordable way. The optical response (differential transient reflectivity and absorption) of a prototypical three-band (core, valence, and conduction) model in the XUV regime is analyzed in detail to illustrate the theory and its application. Using some generalizations of the density of states, we provide a systematic approach to exploring the optical properties in terms of the system band structure features and the pump parameters. Such an analysis can be extremely helpful in understanding the actual results of experimental optical measurements. Moreover, we study the effects of inter-band and intra-band transitions, the local dipole coupling, and single and multi-photon processes. The latter is further investigated by varying the central frequency of the pump pulse to have different regions of the first Brillouin zone in resonance with it. We also study the effect of varying the pump pulse intensity. Finally, we study and analyze the transient optical properties in the probe pulse regime of IR and visible.","sentences":["We derive the two-time linear response theory for out-of-equilibrium pumped systems, generic pump-probe delays and probe frequencies.","Such a theory enormously simplifies the numerical calculations, for instance, of the optical conductivity with respect to the actual procedure, which requires computing the effect of the probe pulse for each time delay with respect to the pump pulse\\textcolor{red}{.}","The theory is given for a generic observable and pumped Hamiltonian and then specialized for a system with a quadratic Hamiltonian and its transient optical properties, exploiting the Dynamical Projective Operatorial Approach (DPOA).","The theory is complemented by a set of crucial numerical guidelines that help perform actual calculations in a computationally affordable way.","The optical response (differential transient reflectivity and absorption) of a prototypical three-band (core, valence, and conduction) model in the XUV regime is analyzed in detail to illustrate the theory and its application.","Using some generalizations of the density of states, we provide a systematic approach to exploring the optical properties in terms of the system band structure features and the pump parameters.","Such an analysis can be extremely helpful in understanding the actual results of experimental optical measurements.","Moreover, we study the effects of inter-band and intra-band transitions, the local dipole coupling, and single and multi-photon processes.","The latter is further investigated by varying the central frequency of the pump pulse to have different regions of the first Brillouin zone in resonance with it.","We also study the effect of varying the pump pulse intensity.","Finally, we study and analyze the transient optical properties in the probe pulse regime of IR and visible."],"url":"http://arxiv.org/abs/2404.10768v1","category":"physics.optics"}
{"created":"2024-04-16 17:50:09","title":"RapidVol: Rapid Reconstruction of 3D Ultrasound Volumes from Sensorless 2D Scans","abstract":"Two-dimensional (2D) freehand ultrasonography is one of the most commonly used medical imaging modalities, particularly in obstetrics and gynaecology. However, it only captures 2D cross-sectional views of inherently 3D anatomies, losing valuable contextual information. As an alternative to requiring costly and complex 3D ultrasound scanners, 3D volumes can be constructed from 2D scans using machine learning. However this usually requires long computational time. Here, we propose RapidVol: a neural representation framework to speed up slice-to-volume ultrasound reconstruction. We use tensor-rank decomposition, to decompose the typical 3D volume into sets of tri-planes, and store those instead, as well as a small neural network. A set of 2D ultrasound scans, with their ground truth (or estimated) 3D position and orientation (pose) is all that is required to form a complete 3D reconstruction. Reconstructions are formed from real fetal brain scans, and then evaluated by requesting novel cross-sectional views. When compared to prior approaches based on fully implicit representation (e.g. neural radiance fields), our method is over 3x quicker, 46% more accurate, and if given inaccurate poses is more robust. Further speed-up is also possible by reconstructing from a structural prior rather than from scratch.","sentences":["Two-dimensional (2D) freehand ultrasonography is one of the most commonly used medical imaging modalities, particularly in obstetrics and gynaecology.","However, it only captures 2D cross-sectional views of inherently 3D anatomies, losing valuable contextual information.","As an alternative to requiring costly and complex 3D ultrasound scanners, 3D volumes can be constructed from 2D scans using machine learning.","However this usually requires long computational time.","Here, we propose RapidVol: a neural representation framework to speed up slice-to-volume ultrasound reconstruction.","We use tensor-rank decomposition, to decompose the typical 3D volume into sets of tri-planes, and store those instead, as well as a small neural network.","A set of 2D ultrasound scans, with their ground truth (or estimated) 3D position and orientation (pose) is all that is required to form a complete 3D reconstruction.","Reconstructions are formed from real fetal brain scans, and then evaluated by requesting novel cross-sectional views.","When compared to prior approaches based on fully implicit representation (e.g. neural radiance fields), our method is over 3x quicker, 46% more accurate, and if given inaccurate poses is more robust.","Further speed-up is also possible by reconstructing from a structural prior rather than from scratch."],"url":"http://arxiv.org/abs/2404.10766v1","category":"eess.IV"}
{"created":"2024-04-16 17:47:27","title":"Confidential Federated Computations","abstract":"Federated Learning and Analytics (FLA) have seen widespread adoption by technology platforms for processing sensitive on-device data. However, basic FLA systems have privacy limitations: they do not necessarily require anonymization mechanisms like differential privacy (DP), and provide limited protections against a potentially malicious service provider. Adding DP to a basic FLA system currently requires either adding excessive noise to each device's updates, or assuming an honest service provider that correctly implements the mechanism and only uses the privatized outputs. Secure multiparty computation (SMPC) -based oblivious aggregations can limit the service provider's access to individual user updates and improve DP tradeoffs, but the tradeoffs are still suboptimal, and they suffer from scalability challenges and susceptibility to Sybil attacks. This paper introduces a novel system architecture that leverages trusted execution environments (TEEs) and open-sourcing to both ensure confidentiality of server-side computations and provide externally verifiable privacy properties, bolstering the robustness and trustworthiness of private federated computations.","sentences":["Federated Learning and Analytics (FLA) have seen widespread adoption by technology platforms for processing sensitive on-device data.","However, basic FLA systems have privacy limitations: they do not necessarily require anonymization mechanisms like differential privacy (DP), and provide limited protections against a potentially malicious service provider.","Adding DP to a basic FLA system currently requires either adding excessive noise to each device's updates, or assuming an honest service provider that correctly implements the mechanism and only uses the privatized outputs.","Secure multiparty computation (SMPC) -based oblivious aggregations can limit the service provider's access to individual user updates and improve DP tradeoffs, but the tradeoffs are still suboptimal, and they suffer from scalability challenges and susceptibility to Sybil attacks.","This paper introduces a novel system architecture that leverages trusted execution environments (TEEs) and open-sourcing to both ensure confidentiality of server-side computations and provide externally verifiable privacy properties, bolstering the robustness and trustworthiness of private federated computations."],"url":"http://arxiv.org/abs/2404.10764v1","category":"cs.CR"}
{"created":"2024-04-16 17:34:39","title":"A High-Order Conservative Cut Finite Element Method for Problems in Time-Dependent Domains","abstract":"A mass-conservative high-order unfitted finite element method for convection-diffusion equations in evolving domains is proposed. The space-time method presented in [P. Hansbo, M. G. Larson, S. Zahedi, Comput. Methods Appl. Mech. Engrg. 307 (2016)] is extended to naturally achieve mass conservation by utilizing Reynold's transport theorem. Furthermore, by partitioning the time-dependent domain into macroelements, a more efficient stabilization procedure for the cut finite element method in time-dependent domains is presented. Numerical experiments illustrate that the method fulfills mass conservation, attains high-order convergence, and the condition number of the resulting system matrix is controlled while sparsity is increased. Problems in bulk domains as well as coupled bulk-surface problems are considered.","sentences":["A mass-conservative high-order unfitted finite element method for convection-diffusion equations in evolving domains is proposed.","The space-time method presented in [P. Hansbo, M. G. Larson, S. Zahedi, Comput.","Methods Appl.","Mech.","Engrg.","307 (2016)] is extended to naturally achieve mass conservation by utilizing Reynold's transport theorem.","Furthermore, by partitioning the time-dependent domain into macroelements, a more efficient stabilization procedure for the cut finite element method in time-dependent domains is presented.","Numerical experiments illustrate that the method fulfills mass conservation, attains high-order convergence, and the condition number of the resulting system matrix is controlled while sparsity is increased.","Problems in bulk domains as well as coupled bulk-surface problems are considered."],"url":"http://arxiv.org/abs/2404.10756v1","category":"math.NA"}
{"created":"2024-04-16 17:24:22","title":"Interpolation and differentiation of alchemical degrees of freedom in machine learning interatomic potentials","abstract":"Machine learning interatomic potentials (MLIPs) have become a workhorse of modern atomistic simulations, and recently published universal MLIPs, pre-trained on large datasets, have demonstrated remarkable accuracy and generalizability. However, the computational cost of MLIPs limits their applicability to chemically disordered systems requiring large simulation cells or to sample-intensive statistical methods. Here, we report the use of continuous and differentiable alchemical degrees of freedom in atomistic materials simulations, exploiting the fact that graph neural network MLIPs represent discrete elements as real-valued tensors. The proposed method introduces alchemical atoms with corresponding weights into the input graph, alongside modifications to the message-passing and readout mechanisms of MLIPs, and allows smooth interpolation between the compositional states of materials. The end-to-end differentiability of MLIPs enables efficient calculation of the gradient of energy with respect to the compositional weights. Leveraging these gradients, we propose methodologies for optimizing the composition of solid solutions towards target macroscopic properties and conducting alchemical free energy simulations to quantify the free energy of vacancy formation and composition changes. The approach offers an avenue for extending the capabilities of universal MLIPs in the modeling of compositional disorder and characterizing the phase stabilities of complex materials systems.","sentences":["Machine learning interatomic potentials (MLIPs) have become a workhorse of modern atomistic simulations, and recently published universal MLIPs, pre-trained on large datasets, have demonstrated remarkable accuracy and generalizability.","However, the computational cost of MLIPs limits their applicability to chemically disordered systems requiring large simulation cells or to sample-intensive statistical methods.","Here, we report the use of continuous and differentiable alchemical degrees of freedom in atomistic materials simulations, exploiting the fact that graph neural network MLIPs represent discrete elements as real-valued tensors.","The proposed method introduces alchemical atoms with corresponding weights into the input graph, alongside modifications to the message-passing and readout mechanisms of MLIPs, and allows smooth interpolation between the compositional states of materials.","The end-to-end differentiability of MLIPs enables efficient calculation of the gradient of energy with respect to the compositional weights.","Leveraging these gradients, we propose methodologies for optimizing the composition of solid solutions towards target macroscopic properties and conducting alchemical free energy simulations to quantify the free energy of vacancy formation and composition changes.","The approach offers an avenue for extending the capabilities of universal MLIPs in the modeling of compositional disorder and characterizing the phase stabilities of complex materials systems."],"url":"http://arxiv.org/abs/2404.10746v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 17:19:50","title":"Polynomial interacting particle systems and non-linear SPDEs","abstract":"Motivated by the robustness of the capital distribution curves, we study the behavior of a certain polynomial equity market model as the number of companies goes to infinity. More precisely, we extend volatility-stabilized market models introduced by Fernholz et al. by allowing for a common noise term such that the models remain polynomial. As the number of companies approaches infinity, we show that the limit of the empirical measure of the $N$-company system converges to the unique solution of a degenerate, non-linear SPDE. The obtained limit also has a representation as the conditional probability of the solution to a certain McKean-Vlasov SDE. Together with its conditional, this is again a polynomial process for which we can prove pathwise uniqueness as well as regularity properties for the marginal densities. We also provide conditional propagation of chaos results and numerical implementations of the particle system as well as its limiting equations.","sentences":["Motivated by the robustness of the capital distribution curves, we study the behavior of a certain polynomial equity market model as the number of companies goes to infinity.","More precisely, we extend volatility-stabilized market models introduced by Fernholz et al. by allowing for a common noise term such that the models remain polynomial.","As the number of companies approaches infinity, we show that the limit of the empirical measure of the $N$-company system converges to the unique solution of a degenerate, non-linear SPDE.","The obtained limit also has a representation as the conditional probability of the solution to a certain McKean-Vlasov SDE.","Together with its conditional, this is again a polynomial process for which we can prove pathwise uniqueness as well as regularity properties for the marginal densities.","We also provide conditional propagation of chaos results and numerical implementations of the particle system as well as its limiting equations."],"url":"http://arxiv.org/abs/2404.10744v1","category":"math.PR"}
{"created":"2024-04-16 17:01:27","title":"How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model","abstract":"Understanding what makes high-dimensional data learnable is a fundamental question in machine learning. On the one hand, it is believed that the success of deep learning lies in its ability to build a hierarchy of representations that become increasingly more abstract with depth, going from simple features like edges to more complex concepts. On the other hand, learning to be insensitive to invariances of the task, such as smooth transformations for image datasets, has been argued to be important for deep networks and it strongly correlates with their performance. In this work, we aim to explain this correlation and unify these two viewpoints. We show that by introducing sparsity to generative hierarchical models of data, the task acquires insensitivity to spatial transformations that are discrete versions of smooth transformations. In particular, we introduce the Sparse Random Hierarchy Model (SRHM), where we observe and rationalize that a hierarchical representation mirroring the hierarchical model is learnt precisely when such insensitivity is learnt, thereby explaining the strong correlation between the latter and performance. Moreover, we quantify how the sample complexity of CNNs learning the SRHM depends on both the sparsity and hierarchical structure of the task.","sentences":["Understanding what makes high-dimensional data learnable is a fundamental question in machine learning.","On the one hand, it is believed that the success of deep learning lies in its ability to build a hierarchy of representations that become increasingly more abstract with depth, going from simple features like edges to more complex concepts.","On the other hand, learning to be insensitive to invariances of the task, such as smooth transformations for image datasets, has been argued to be important for deep networks and it strongly correlates with their performance.","In this work, we aim to explain this correlation and unify these two viewpoints.","We show that by introducing sparsity to generative hierarchical models of data, the task acquires insensitivity to spatial transformations that are discrete versions of smooth transformations.","In particular, we introduce the Sparse Random Hierarchy Model (SRHM), where we observe and rationalize that a hierarchical representation mirroring the hierarchical model is learnt precisely when such insensitivity is learnt, thereby explaining the strong correlation between the latter and performance.","Moreover, we quantify how the sample complexity of CNNs learning the SRHM depends on both the sparsity and hierarchical structure of the task."],"url":"http://arxiv.org/abs/2404.10727v1","category":"stat.ML"}
{"created":"2024-04-16 16:54:20","title":"Evidence of Pop~III stars' chemical signature in neutral gas at $z\\sim6$. A study based on the E-XQR-30 spectroscopic sample","abstract":"This study explores the metal enrichment signatures attributed to the first generation of stars (PopIII) in the Universe, focusing on the E-XQR-30 sample. We aim to identify traces of Pop III metal enrichment by analyzing neutral gas in the interstellar medium of primordial galaxies and their satellite clumps, detected in absorption. To chase the chemical signature of PopIII stars, we studied metal absorption systems in the E-XQR-30 sample, selected through the detection of the OI absorption line at 1302A. The OI line is a reliable tracer of HI and allowed us to overcome the challenges posed by the Lyman-$\\alpha$ forest's increasing saturation at redshifts above $\\sim5$ to identify Damped Lyman-$\\alpha$ systems (DLA). We detected and analyzed 29 OI systems at $z\\geq5.4$, differentiating between proximate DLAs (PDLA) and intervening DLAs. Voigt function fits were applied to obtain ionic column densities, and relative chemical abundances were determined for 28 systems. These were then compared with the predictions of theoretical models. Our findings expand the study of OI systems at $z\\geq5.4$ fourfold. No systematic differences were observed in the average chemical abundances between PDLAs and intervening DLAs. The chemical abundances in our sample align with literature systems at $z>4.5$, suggesting a similar enrichment pattern for this class of absorption systems. A comparison between these DLA-analogues at $4.5<z<6.5$ with a sample of very metal-poor DLAs at $2<z<4.5$ shows in general similar average values for the relative abundances, with the exception of [C/O], [Si/Fe] and [Si/O] which are significantly larger for the high-$z$ sample. Furthermore, the dispersion of the measurements significantly increases in the high-redshift bin. This increase is predicted by the theoretical models and indicates a potential retention of PopIII signatures in the probed gas. (Abridged)","sentences":["This study explores the metal enrichment signatures attributed to the first generation of stars (PopIII) in the Universe, focusing on the E-XQR-30 sample.","We aim to identify traces of Pop III metal enrichment by analyzing neutral gas in the interstellar medium of primordial galaxies and their satellite clumps, detected in absorption.","To chase the chemical signature of PopIII stars, we studied metal absorption systems in the E-XQR-30 sample, selected through the detection of the OI absorption line at 1302A.","The OI line is a reliable tracer of HI and allowed us to overcome the challenges posed by the Lyman-$\\alpha$ forest's increasing saturation at redshifts above $\\sim5$ to identify Damped Lyman-$\\alpha$ systems (DLA).","We detected and analyzed 29 OI systems at $z\\geq5.4$, differentiating between proximate DLAs (PDLA) and intervening DLAs.","Voigt function fits were applied to obtain ionic column densities, and relative chemical abundances were determined for 28 systems.","These were then compared with the predictions of theoretical models.","Our findings expand the study of OI systems at $z\\geq5.4$ fourfold.","No systematic differences were observed in the average chemical abundances between PDLAs and intervening DLAs.","The chemical abundances in our sample align with literature systems at $z>4.5$, suggesting a similar enrichment pattern for this class of absorption systems.","A comparison between these DLA-analogues at $4.5<z<6.5$ with a sample of very metal-poor DLAs at $2<z<4.5$ shows in general similar average values for the relative abundances, with the exception of [C/O], [Si/Fe] and","[Si/O] which are significantly larger for the high-$z$ sample.","Furthermore, the dispersion of the measurements significantly increases in the high-redshift bin.","This increase is predicted by the theoretical models and indicates a potential retention of PopIII signatures in the probed gas.","(Abridged)"],"url":"http://arxiv.org/abs/2404.10722v1","category":"astro-ph.GA"}
{"created":"2024-04-16 16:53:15","title":"Dependence of spicule properties on magnetic field - results from Magnetohydrodynamics simulations","abstract":"Solar spicules are plasma jets observed in the interface region between the visible solar surface and the corona. At any given time, there are millions of spicules present all over the Sun. While various models attempt to elucidate their origin and characteristics, here, we consider the one driven by the magneto-convection undulations. The radiative magneto-hydrodynamical (rMHD) equations are solved using PENCIL CODE with a spatial resolution of 16 km using various magnetic field strengths. The obtained rMHD simulation data are investigated to unveil the various trends in spicular properties as function of the applied magnetic fields. The important outcome of this study is the finding of a consistent reduction in both the number density and the maximum height reached by spicules as magnetic field strength increases. We also use parabolic fitting on time-distance curves of spicules that are taller than $75^\\mathrm{th}$ percentile in the distribution, in order to find a relation between deceleration of the spicule tip and the magnetic field strength. Our results offer insights into the response of solar spicules to magnetic field strength.","sentences":["Solar spicules are plasma jets observed in the interface region between the visible solar surface and the corona.","At any given time, there are millions of spicules present all over the Sun.","While various models attempt to elucidate their origin and characteristics, here, we consider the one driven by the magneto-convection undulations.","The radiative magneto-hydrodynamical (rMHD) equations are solved using PENCIL CODE with a spatial resolution of 16 km using various magnetic field strengths.","The obtained rMHD simulation data are investigated to unveil the various trends in spicular properties as function of the applied magnetic fields.","The important outcome of this study is the finding of a consistent reduction in both the number density and the maximum height reached by spicules as magnetic field strength increases.","We also use parabolic fitting on time-distance curves of spicules that are taller than $75^\\mathrm{th}$ percentile in the distribution, in order to find a relation between deceleration of the spicule tip and the magnetic field strength.","Our results offer insights into the response of solar spicules to magnetic field strength."],"url":"http://arxiv.org/abs/2404.10720v1","category":"astro-ph.SR"}
{"created":"2024-04-16 16:16:00","title":"Drift estimation in stochastic flows using kernel integral operators","abstract":"A stochastic differential equation (SDE) describes a motion in which a particle is governed simultaneously by the direction provided by a vector field / drift, and the scattering effects of white noise. The resulting motion can only be described as a random process instead of a solution curve. Due to the non-deterministic nature of this motion, the task of determining the drift from data is quite challenging, since the data does not directly represent the directional information of the flow. This paper describes an interpretation of vector field as a conditional expectation, which makes its estimation feasible via kernel-integral methods. It presents a numerical procedure based on kernel integral operators, that computes this expectation. In addition, some techniques are presented which can overcome the challenge of dimensionality if the SDE's carry some structure enabling sparsity.","sentences":["A stochastic differential equation (SDE) describes a motion in which a particle is governed simultaneously by the direction provided by a vector field / drift, and the scattering effects of white noise.","The resulting motion can only be described as a random process instead of a solution curve.","Due to the non-deterministic nature of this motion, the task of determining the drift from data is quite challenging, since the data does not directly represent the directional information of the flow.","This paper describes an interpretation of vector field as a conditional expectation, which makes its estimation feasible via kernel-integral methods.","It presents a numerical procedure based on kernel integral operators, that computes this expectation.","In addition, some techniques are presented which can overcome the challenge of dimensionality if the SDE's carry some structure enabling sparsity."],"url":"http://arxiv.org/abs/2404.10698v1","category":"math.DS"}
{"created":"2024-04-16 16:11:56","title":"A hybrid Quantum-Classical Algorithm for Mixed-Integer Optimization in Power Systems","abstract":"Mixed Integer Linear Programming (MILP) can be considered the backbone of the modern power system optimization process, with a large application spectrum, from Unit Commitment and Optimal Transmission Switching to verifying Neural Networks for power system applications. The main issue of these formulations is the computational complexity of the solution algorithms, as they are considered NP-Hard problems. Quantum computing has been tested as a potential solution towards reducing the computational burden imposed by these problems, providing promising results, motivating the can be used to speedup the solution of MILPs. In this work, we present a general framework for solving power system optimization problems with a Quantum Computer (QC), which leverages mathematical tools and QCs' sampling ability to provide accelerated solutions. Our guiding applications are the optimal transmission switching and the verification of neural networks trained to solve a DC Optimal Power Flow. Specifically, using an accelerated version of Benders Decomposition , we split a given MILP into an Integer Master Problem and a linear Subproblem and solve it through a hybrid ``quantum-classical'' approach, getting the best of both worlds. We provide 2 use cases, and benchmark the developed framework against other classical and hybrid methodologies, to demonstrate the opportunities and challenges of hybrid quantum-classical algorithms for power system mixed integer optimization problems.","sentences":["Mixed Integer Linear Programming (MILP) can be considered the backbone of the modern power system optimization process, with a large application spectrum, from Unit Commitment and Optimal Transmission Switching to verifying Neural Networks for power system applications.","The main issue of these formulations is the computational complexity of the solution algorithms, as they are considered NP-Hard problems.","Quantum computing has been tested as a potential solution towards reducing the computational burden imposed by these problems, providing promising results, motivating the can be used to speedup the solution of MILPs.","In this work, we present a general framework for solving power system optimization problems with a Quantum Computer (QC), which leverages mathematical tools and QCs' sampling ability to provide accelerated solutions.","Our guiding applications are the optimal transmission switching and the verification of neural networks trained to solve a DC Optimal Power Flow.","Specifically, using an accelerated version of Benders Decomposition , we split a given MILP into an Integer Master Problem and a linear Subproblem and solve it through a hybrid ``quantum-classical'' approach, getting the best of both worlds.","We provide 2 use cases, and benchmark the developed framework against other classical and hybrid methodologies, to demonstrate the opportunities and challenges of hybrid quantum-classical algorithms for power system mixed integer optimization problems."],"url":"http://arxiv.org/abs/2404.10693v1","category":"quant-ph"}
{"created":"2024-04-16 16:09:38","title":"Network architecture search of X-ray based scientific applications","abstract":"X-ray and electron diffraction-based microscopy use bragg peak detection and ptychography to perform 3-D imaging at an atomic resolution. Typically, these techniques are implemented using computationally complex tasks such as a Psuedo-Voigt function or solving a complex inverse problem. Recently, the use of deep neural networks has improved the existing state-of-the-art approaches. However, the design and development of the neural network models depends on time and labor intensive tuning of the model by application experts. To that end, we propose a hyperparameter (HPS) and neural architecture search (NAS) approach to automate the design and optimization of the neural network models for model size, energy consumption and throughput. We demonstrate the improved performance of the auto-tuned models when compared to the manually tuned BraggNN and PtychoNN benchmark. We study and demonstrate the importance of the exploring the search space of tunable hyperparameters in enhancing the performance of bragg peak detection and ptychographic reconstruction. Our NAS and HPS of (1) BraggNN achieves a 31.03\\% improvement in bragg peak detection accuracy with a 87.57\\% reduction in model size, and (2) PtychoNN achieves a 16.77\\% improvement in model accuracy and a 12.82\\% reduction in model size when compared to the baseline PtychoNN model. When inferred on the Orin-AGX platform, the optimized Braggnn and Ptychonn models demonstrate a 10.51\\% and 9.47\\% reduction in inference latency and a 44.18\\% and 15.34\\% reduction in energy consumption when compared to their respective baselines, when inferred in the Orin-AGX edge platform.","sentences":["X-ray and electron diffraction-based microscopy use bragg peak detection and ptychography to perform 3-D imaging at an atomic resolution.","Typically, these techniques are implemented using computationally complex tasks such as a Psuedo-Voigt function or solving a complex inverse problem.","Recently, the use of deep neural networks has improved the existing state-of-the-art approaches.","However, the design and development of the neural network models depends on time and labor intensive tuning of the model by application experts.","To that end, we propose a hyperparameter (HPS) and neural architecture search (NAS) approach to automate the design and optimization of the neural network models for model size, energy consumption and throughput.","We demonstrate the improved performance of the auto-tuned models when compared to the manually tuned BraggNN and PtychoNN benchmark.","We study and demonstrate the importance of the exploring the search space of tunable hyperparameters in enhancing the performance of bragg peak detection and ptychographic reconstruction.","Our NAS and HPS of (1) BraggNN achieves a 31.03\\% improvement in bragg peak detection accuracy with a 87.57\\% reduction in model size, and (2) PtychoNN","achieves a 16.77\\% improvement in model accuracy and a 12.82\\% reduction in model size when compared to the baseline PtychoNN model.","When inferred on the Orin-AGX platform, the optimized Braggnn and Ptychonn models demonstrate a 10.51\\% and 9.47\\% reduction in inference latency and a 44.18\\% and 15.34\\% reduction in energy consumption when compared to their respective baselines, when inferred in the Orin-AGX edge platform."],"url":"http://arxiv.org/abs/2404.10689v1","category":"cs.LG"}
{"created":"2024-04-16 16:04:11","title":"Driver Fatigue Prediction using Randomly Activated Neural Networks for Smart Ridesharing Platforms","abstract":"Drivers in ridesharing platforms exhibit cognitive atrophy and fatigue as they accept ride offers along the day, which can have a significant impact on the overall efficiency of the ridesharing platform. In contrast to the current literature which focuses primarily on modeling and learning driver's preferences across different ride offers, this paper proposes a novel Dynamic Discounted Satisficing (DDS) heuristic to model and predict driver's sequential ride decisions during a given shift. Based on DDS heuristic, a novel stochastic neural network with random activations is proposed to model DDS heuristic and predict the final decision made by a given driver. The presence of random activations in the network necessitated the development of a novel training algorithm called Sampling-Based Back Propagation Through Time (SBPTT), where gradients are computed for independent instances of neural networks (obtained via sampling the distribution of activation threshold) and aggregated to update the network parameters. Using both simulation experiments as well as on real Chicago taxi dataset, this paper demonstrates the improved performance of the proposed approach, when compared to state-of-the-art methods.","sentences":["Drivers in ridesharing platforms exhibit cognitive atrophy and fatigue as they accept ride offers along the day, which can have a significant impact on the overall efficiency of the ridesharing platform.","In contrast to the current literature which focuses primarily on modeling and learning driver's preferences across different ride offers, this paper proposes a novel Dynamic Discounted Satisficing (DDS) heuristic to model and predict driver's sequential ride decisions during a given shift.","Based on DDS heuristic, a novel stochastic neural network with random activations is proposed to model DDS heuristic and predict the final decision made by a given driver.","The presence of random activations in the network necessitated the development of a novel training algorithm called Sampling-Based Back Propagation Through Time (SBPTT), where gradients are computed for independent instances of neural networks (obtained via sampling the distribution of activation threshold) and aggregated to update the network parameters.","Using both simulation experiments as well as on real Chicago taxi dataset, this paper demonstrates the improved performance of the proposed approach, when compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.10684v1","category":"cs.LG"}
{"created":"2024-04-16 15:49:09","title":"Climbing to the Top of the ATLAS 13 TeV data","abstract":"The large amount of data recorded with the ATLAS detector at the Large Hadron Collider, corresponding to 140 fb$^{-1}$ of $pp$ collisions at a centre-of-mass energy of $\\sqrt{s}=13$ TeV, has brought our knowledge of the top quark to the next level. The measurement of the top$-$antitop quark pair-production cross-section has reached a precision of 1.8% and the cross-section was measured differentially up to several TeV in several observables including the top-quark transverse momentum and top-quark-pair invariant mass. Single-top-quark production was studied in all production modes. Rare production processes where the top quark is associated with a vector boson, and four-top-quark production, have become accessible and precision measurements of several of these processes have reached cross-section uncertainties of around 10% or smaller. Innovative measurements of the top-quark mass and properties have also emerged, including the observation of quantum entanglement in the top-quark sector and tests of lepton-flavour universality using top-quark decays. Searches for flavour-changing neutral currents in the top-quark sector have been significantly improved, reaching branching-ratio exclusion limits ranging from $10^{-3}$ to $10^{-5}$. Many of these analyses have been used to set limits on Wilson coefficients within the effective field theory framework.","sentences":["The large amount of data recorded with the ATLAS detector at the Large Hadron Collider, corresponding to 140 fb$^{-1}$ of $pp$ collisions at a centre-of-mass energy of $\\sqrt{s}=13$ TeV, has brought our knowledge of the top quark to the next level.","The measurement of the top$-$antitop quark pair-production cross-section has reached a precision of 1.8% and the cross-section was measured differentially up to several TeV in several observables including the top-quark transverse momentum and top-quark-pair invariant mass.","Single-top-quark production was studied in all production modes.","Rare production processes where the top quark is associated with a vector boson, and four-top-quark production, have become accessible and precision measurements of several of these processes have reached cross-section uncertainties of around 10% or smaller.","Innovative measurements of the top-quark mass and properties have also emerged, including the observation of quantum entanglement in the top-quark sector and tests of lepton-flavour universality using top-quark decays.","Searches for flavour-changing neutral currents in the top-quark sector have been significantly improved, reaching branching-ratio exclusion limits ranging from $10^{-3}$ to $10^{-5}$. Many of these analyses have been used to set limits on Wilson coefficients within the effective field theory framework."],"url":"http://arxiv.org/abs/2404.10674v1","category":"hep-ex"}
{"created":"2024-04-16 15:42:20","title":"Iterated Invariant Extended Kalman Filter (IIEKF)","abstract":"In this paper, we introduce the Iterated Invariant Extended Kalman Filter (IIEKF), which is an invariant extended Kalman filter (IEKF) where the updated state in the light of the latest measurement is defined as a maximum a posteriori (MAP) estimate. Under some compatibility requirements on the output map, we prove strong mathematical guarantees which echo those of the Kalman filter in the linear case. We apply the technique to two problems: solving a system of equations on a Lie group, and a problem of engineering interest, namely ego-localization of the hook of a crane. The latter serves as a benchmarking example, where the IIEKF favorably compares to other filters.","sentences":["In this paper, we introduce the Iterated Invariant Extended Kalman Filter (IIEKF), which is an invariant extended Kalman filter (IEKF) where the updated state in the light of the latest measurement is defined as a maximum a posteriori (MAP) estimate.","Under some compatibility requirements on the output map, we prove strong mathematical guarantees which echo those of the Kalman filter in the linear case.","We apply the technique to two problems: solving a system of equations on a Lie group, and a problem of engineering interest, namely ego-localization of the hook of a crane.","The latter serves as a benchmarking example, where the IIEKF favorably compares to other filters."],"url":"http://arxiv.org/abs/2404.10665v1","category":"eess.SY"}
{"created":"2024-04-16 15:22:30","title":"Space Regularity of Evolution Equations Driven by Rough Paths","abstract":"In this paper, we consider the linear evolution equation $dy(t)=Ay(t)dt+Gy(t)dx(t)$, where $A$ is a closed operator, associated to a semigroup, with good smoothing effects in a Banach space $E$, $x$ is a nonsmooth path, which is $\\eta$-H\\\"older continuous for some $\\eta\\in (1/3,1/2]$, and $G$ is a non-smoothing linear operator on $E$. We prove that the Cauchy problem associated with the previous equation admits a unique mild solution and we also show that the solution increases the regularity of the initial datum as soon as time evolves. Then, we show that the mild solution is also an integral solution and this allows us to prove a It\\^o formula.","sentences":["In this paper, we consider the linear evolution equation $dy(t)=Ay(t)dt+Gy(t)dx(t)$, where $A$ is a closed operator, associated to a semigroup, with good smoothing effects in a Banach space $E$, $x$ is a nonsmooth path, which is $\\eta$-H\\\"older continuous for some $\\eta\\in (1/3,1/2]$, and $G$ is a non-smoothing linear operator on $E$. We prove that the Cauchy problem associated with the previous equation admits a unique mild solution and we also show that the solution increases the regularity of the initial datum as soon as time evolves.","Then, we show that the mild solution is also an integral solution and this allows us to prove a It\\^o formula."],"url":"http://arxiv.org/abs/2404.10650v1","category":"math.AP"}
{"created":"2024-04-16 15:08:14","title":"Gravitational radiation of a spherically symmetric source in $f(R)$-gravitation","abstract":"It is shown that Birkhoff's theorem for the general theory of relativity is overcome in the $f(R)$-theory of gravitation. That means, the $f(R)$-theory of gravitation, unlike Einstein's general theory of relativity, does not forbid gravitational radiation from a spherically symmetric source (whether stationary or non-stationary). As a consequence, in the $f(R)$-theory a spherically symmetric gravitational deformation (e.g., collapse/expansion or pulsation) could emit gravitational waves (of tensor- and scalar polarization modes), a phenomenon impossible in the general relativity. A test model is examined and it turns out that the gravitational radiation is strongest when the surface of the deforming object is in the vicinity of the (modified) event horizon, even suddenly flares up just outside the latter. In this letter, within the $f(R)$-theory of gravitation, a gravitational wave equation and a formula for the gravitational emission power are derived. These formulae, along with searching for signals, can be used for the experimental test of the $f(R)$-theory. In general, including the spherically symmetry case, gravitational radiation of both tensor- and scalar polarization modes are allowed, although under some circumstance the contribution of scalar modes is strongly suppressed.","sentences":["It is shown that Birkhoff's theorem for the general theory of relativity is overcome in the $f(R)$-theory of gravitation.","That means, the $f(R)$-theory of gravitation, unlike Einstein's general theory of relativity, does not forbid gravitational radiation from a spherically symmetric source (whether stationary or non-stationary).","As a consequence, in the $f(R)$-theory a spherically symmetric gravitational deformation (e.g., collapse/expansion or pulsation) could emit gravitational waves (of tensor- and scalar polarization modes), a phenomenon impossible in the general relativity.","A test model is examined and it turns out that the gravitational radiation is strongest when the surface of the deforming object is in the vicinity of the (modified) event horizon, even suddenly flares up just outside the latter.","In this letter, within the $f(R)$-theory of gravitation, a gravitational wave equation and a formula for the gravitational emission power are derived.","These formulae, along with searching for signals, can be used for the experimental test of the $f(R)$-theory.","In general, including the spherically symmetry case, gravitational radiation of both tensor- and scalar polarization modes are allowed, although under some circumstance the contribution of scalar modes is strongly suppressed."],"url":"http://arxiv.org/abs/2404.10808v1","category":"gr-qc"}
{"created":"2024-04-16 14:48:40","title":"Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks","abstract":"NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or GIRAFFE have shown very high rendering quality under large representational variety. However, rendering with Neural Radiance Fields poses challenges for 3D applications: First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games. 3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates. In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware GANs with the flexibility and computational advantages of 3DGS. By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time. Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes.","sentences":["NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or GIRAFFE have shown very high rendering quality under large representational variety.","However, rendering with Neural Radiance Fields poses challenges for 3D applications:","First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets.","Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games.","3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates.","In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware GANs with the flexibility and computational advantages of 3DGS.","By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time.","Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes."],"url":"http://arxiv.org/abs/2404.10625v1","category":"cs.CV"}
{"created":"2024-04-16 14:45:27","title":"Learning Deep Dynamical Systems using Stable Neural ODEs","abstract":"Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS). State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time. We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information. A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed. We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task.","sentences":["Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS).","State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time.","We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information.","A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed.","We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task."],"url":"http://arxiv.org/abs/2404.10622v1","category":"cs.RO"}
{"created":"2024-04-16 14:43:33","title":"PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape Reconstruction","abstract":"We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D objects from images using interpretable shape programs. In comparison to traditional CAD model retrieval methods, the use of shape programs for 3D reconstruction allows for reasoning about the semantic properties of reconstructed objects, editing, low memory footprint, etc. However, the utilization of shape programs for 3D scene understanding has been largely neglected in past works. As our main contribution, we enable gradient-based optimization by introducing a module that translates shape programs designed in Blender, for example, into efficient PyTorch code. We also provide a method that relies on PyTorchGeoNodes and is inspired by Monte Carlo Tree Search (MCTS) to jointly optimize discrete and continuous parameters of shape programs and reconstruct 3D objects for input scenes. In our experiments, we apply our algorithm to reconstruct 3D objects in the ScanNet dataset and evaluate our results against CAD model retrieval-based reconstructions. Our experiments indicate that our reconstructions match well the input scenes while enabling semantic reasoning about reconstructed objects.","sentences":["We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D objects from images using interpretable shape programs.","In comparison to traditional CAD model retrieval methods, the use of shape programs for 3D reconstruction allows for reasoning about the semantic properties of reconstructed objects, editing, low memory footprint, etc.","However, the utilization of shape programs for 3D scene understanding has been largely neglected in past works.","As our main contribution, we enable gradient-based optimization by introducing a module that translates shape programs designed in Blender, for example, into efficient PyTorch code.","We also provide a method that relies on PyTorchGeoNodes and is inspired by Monte Carlo Tree Search (MCTS) to jointly optimize discrete and continuous parameters of shape programs and reconstruct 3D objects for input scenes.","In our experiments, we apply our algorithm to reconstruct 3D objects in the ScanNet dataset and evaluate our results against CAD model retrieval-based reconstructions.","Our experiments indicate that our reconstructions match well the input scenes while enabling semantic reasoning about reconstructed objects."],"url":"http://arxiv.org/abs/2404.10620v1","category":"cs.CV"}
{"created":"2024-04-16 14:38:40","title":"Linear Stability Analysis of Relativistic Magnetized Jets: The Minimalist Approach","abstract":"A minimalist approach to the linear stability problem in fluid dynamics is developed that ensures efficiency by utilizing only the essential elements required to find the eigenvalues for given boundary conditions. It is shown that the problem is equivalent to a single first-order ordinary differential equation, and that studying the argument of the unknown complex function in the eigenvalue space is sufficient to find the dispersion relation. The method is applied to a model for relativistic magnetized astrophysical jets.","sentences":["A minimalist approach to the linear stability problem in fluid dynamics is developed that ensures efficiency by utilizing only the essential elements required to find the eigenvalues for given boundary conditions.","It is shown that the problem is equivalent to a single first-order ordinary differential equation, and that studying the argument of the unknown complex function in the eigenvalue space is sufficient to find the dispersion relation.","The method is applied to a model for relativistic magnetized astrophysical jets."],"url":"http://arxiv.org/abs/2404.10613v1","category":"astro-ph.HE"}
{"created":"2024-04-16 14:36:34","title":"Swirling due to misaligned perception-dependent motility","abstract":"A system of particles with motility variable in terms of a vision-type of perception is here investigated by a combination of Langevin dynamics simulations in two-dimensional systems and an analytical approach based on conservation law principles. Persistent swirling with predetermined direction is here induced by differentiating the self-propulsion direction and the perception cone axis. Clusters can have a fluid-like center with a rotating outer layer, or display a solid-like rotation driven by the outer layer activity. Discontinuous motility with misaligned perception might therefore serve as a powerful self-organization strategy in micro-robots.","sentences":["A system of particles with motility variable in terms of a vision-type of perception is here investigated by a combination of Langevin dynamics simulations in two-dimensional systems and an analytical approach based on conservation law principles.","Persistent swirling with predetermined direction is here induced by differentiating the self-propulsion direction and the perception cone axis.","Clusters can have a fluid-like center with a rotating outer layer, or display a solid-like rotation driven by the outer layer activity.","Discontinuous motility with misaligned perception might therefore serve as a powerful self-organization strategy in micro-robots."],"url":"http://arxiv.org/abs/2404.10608v1","category":"cond-mat.soft"}
{"created":"2024-04-16 14:36:14","title":"Transitive Lie algebroids and \\textbf{Q}-manifolds","abstract":"We introduce the notion of \\textbf{Q}-principal bundle, which is the appropriate version of principal fibre bundles in the setting of R. Barre's \\textbf{Q}-manifolds. As an application, we prove that every transitive Lie algebroid arises from the Atiyah sequence of a certain \\textbf{Q}-principal bundle, and we give the interpretation of this result in terms of groupoids.","sentences":["We introduce the notion of \\textbf{Q}-principal bundle, which is the appropriate version of principal fibre bundles in the setting of R. Barre's \\textbf{Q}-manifolds.","As an application, we prove that every transitive Lie algebroid arises from the Atiyah sequence of a certain \\textbf{Q}-principal bundle, and we give the interpretation of this result in terms of groupoids."],"url":"http://arxiv.org/abs/2404.10607v1","category":"math.DG"}
{"created":"2024-04-16 14:26:39","title":"Towards free-response paradigm: a theory on decision-making in spiking neural networks","abstract":"The energy-efficient and brain-like information processing abilities of Spiking Neural Networks (SNNs) have attracted considerable attention, establishing them as a crucial element of brain-inspired computing. One prevalent challenge encountered by SNNs is the trade-off between inference speed and accuracy, which requires sufficient time to achieve the desired level of performance. Drawing inspiration from animal behavior experiments that demonstrate a connection between decision-making reaction times, task complexity, and confidence levels, this study seeks to apply these insights to SNNs. The focus is on understanding how SNNs make inferences, with a particular emphasis on untangling the interplay between signal and noise in decision-making processes. The proposed theoretical framework introduces a new optimization objective for SNN training, highlighting the importance of not only the accuracy of decisions but also the development of predictive confidence through learning from past experiences. Experimental results demonstrate that SNNs trained according to this framework exhibit improved confidence expression, leading to better decision-making outcomes. In addition, a strategy is introduced for efficient decision-making during inference, which allows SNNs to complete tasks more quickly and can use stopping times as indicators of decision confidence. By integrating neuroscience insights with neuromorphic computing, this study opens up new possibilities to explore the capabilities of SNNs and advance their application in complex decision-making scenarios.","sentences":["The energy-efficient and brain-like information processing abilities of Spiking Neural Networks (SNNs) have attracted considerable attention, establishing them as a crucial element of brain-inspired computing.","One prevalent challenge encountered by SNNs is the trade-off between inference speed and accuracy, which requires sufficient time to achieve the desired level of performance.","Drawing inspiration from animal behavior experiments that demonstrate a connection between decision-making reaction times, task complexity, and confidence levels, this study seeks to apply these insights to SNNs.","The focus is on understanding how SNNs make inferences, with a particular emphasis on untangling the interplay between signal and noise in decision-making processes.","The proposed theoretical framework introduces a new optimization objective for SNN training, highlighting the importance of not only the accuracy of decisions but also the development of predictive confidence through learning from past experiences.","Experimental results demonstrate that SNNs trained according to this framework exhibit improved confidence expression, leading to better decision-making outcomes.","In addition, a strategy is introduced for efficient decision-making during inference, which allows SNNs to complete tasks more quickly and can use stopping times as indicators of decision confidence.","By integrating neuroscience insights with neuromorphic computing, this study opens up new possibilities to explore the capabilities of SNNs and advance their application in complex decision-making scenarios."],"url":"http://arxiv.org/abs/2404.10599v1","category":"cs.NE"}
{"created":"2024-04-16 14:21:24","title":"Formation of GW230529 from Isolated Binary Evolution and Its Electromagnetic Counterparts","abstract":"In this {\\em Letter}, we explore the formation of the mass-gap black hole-neutron star (mgBHNS) merger detected in gravitational wave (GW) event, i.e., GW230529, from the isolated binary evolution channel, and study potential signatures of its electromagnetic signals. By adopting the `delayed' supernova prescription and reasonable model realizations, our population synthesis simulation results can simultaneously match the inferred event rate densities of GW230529-like mgBHNS and total BHNS mergers, as well as the population distribution of the BH mass in BHNS mergers reported by the LIGO-Virgo-KAGRA Collaboration. Thus, we conclude that the recently-discovered mgBHNS merger, GW230529, can be explained through the isolated binary evolution channel. Considering the equation of states of AP4 and DD2, the probabilities that GW230529 can make tidal disruption are $12.8\\%$ and $63.2\\%$, respectively. If GW230529 is a disrupted event, the associated kilonova is predicted to have an apparent magnitude of $\\sim23-24\\,{\\rm{mag}}$, and hence, can be detected by the present survey projects and LSST. Since GW230529 could be an off-axis event inferred from the GW observation, its associated gamma-ray burst (GRB) might be too dim to be observed by $\\gamma$-ray detectors, interpreting the lack of GRB observations. The detection of GW230529 confirms the existence of mgBHNS mergers formed through the isolated binary evolution channel, suggesting that BHNS mergers are still likely to be multimessenger sources that emit GWs, GRBs, and kilonovae. Although mgBHNS mergers account for $\\sim60\\%$ cosmological BHNS population, we find that $\\gtrsim90\\%$ disrupted BHNS mergers are expected to originate from mgBHNS mergers.","sentences":["In this {\\em Letter}, we explore the formation of the mass-gap black hole-neutron star (mgBHNS) merger detected in gravitational wave (GW) event, i.e., GW230529, from the isolated binary evolution channel, and study potential signatures of its electromagnetic signals.","By adopting the `delayed' supernova prescription and reasonable model realizations, our population synthesis simulation results can simultaneously match the inferred event rate densities of GW230529-like mgBHNS and total BHNS mergers, as well as the population distribution of the BH mass in BHNS mergers reported by the LIGO-Virgo-KAGRA Collaboration.","Thus, we conclude that the recently-discovered mgBHNS merger, GW230529, can be explained through the isolated binary evolution channel.","Considering the equation of states of AP4 and DD2, the probabilities that GW230529 can make tidal disruption are $12.8\\%$ and $63.2\\%$, respectively.","If GW230529 is a disrupted event, the associated kilonova is predicted to have an apparent magnitude of $\\sim23-24\\,{\\rm{mag}}$, and hence, can be detected by the present survey projects and LSST.","Since GW230529 could be an off-axis event inferred from the GW observation, its associated gamma-ray burst (GRB) might be too dim to be observed by $\\gamma$-ray detectors, interpreting the lack of GRB observations.","The detection of GW230529 confirms the existence of mgBHNS mergers formed through the isolated binary evolution channel, suggesting that BHNS mergers are still likely to be multimessenger sources that emit GWs, GRBs, and kilonovae.","Although mgBHNS mergers account for $\\sim60\\%$ cosmological BHNS population, we find that $\\gtrsim90\\%$ disrupted BHNS mergers are expected to originate from mgBHNS mergers."],"url":"http://arxiv.org/abs/2404.10596v1","category":"astro-ph.HE"}
{"created":"2024-04-16 14:11:26","title":"Prediction of Nuclear Charge Density Distribution with Feedback Neural Network","abstract":"The nuclear charge density distribution plays an important role in nuclear physics and atomic physics. As one of the most frequently used models to obtain charge density distribution, the two-parameter fermi (2pF) model has been widely applied in both nuclear physics and atomic physics. Currently, the feedforward neural network has been employed to study the available 2pF model parameters for 86 nuclei, and it is found that by introducing A^{1/3} into the input parameter of the neural network, the accuracy and precision of the parameter learning effect are improved. Furthermore, the average result of multiple predictions is more reliable than the best result of a single prediction, and there is no significant difference between the average result of density value and of parameter value for the average charge density distribution. In addition, 2pF parameters of 284 (near) stable nuclei are also predicted in this work, which provides a reference for the experiment.","sentences":["The nuclear charge density distribution plays an important role in nuclear physics and atomic physics.","As one of the most frequently used models to obtain charge density distribution, the two-parameter fermi (2pF) model has been widely applied in both nuclear physics and atomic physics.","Currently, the feedforward neural network has been employed to study the available 2pF model parameters for 86 nuclei, and it is found that by introducing A^{1/3} into the input parameter of the neural network, the accuracy and precision of the parameter learning effect are improved.","Furthermore, the average result of multiple predictions is more reliable than the best result of a single prediction, and there is no significant difference between the average result of density value and of parameter value for the average charge density distribution.","In addition, 2pF parameters of 284 (near) stable nuclei are also predicted in this work, which provides a reference for the experiment."],"url":"http://arxiv.org/abs/2404.10585v1","category":"nucl-th"}
{"created":"2024-04-16 13:47:05","title":"More variables or more bins? Impact on the EFT interpretation of Drell-Yan measurements","abstract":"We generalize previous studies on constraining operators of the Standard Model Effective Field Theory using Drell-Yan (DY) measurements to include at the same time all relevant operators and uncertainties. It has been shown that fully differential measurements (triple differential for neutral and double differential for charged) are more sensitive to EFT effects. Nevertheless, due to the finite statistics, the fully differential measurements sacrifice some statistical power on the shape (less invariant mass or transverse momentum bins) in favour of more kinematic variables. We show that when the observables are particularly sensitive to the shape of the distributions, such as the invariant mass of the two leptons in neutral DY, the single differential measurement with more bins, may be as sensitive as the fully differential one, at least for specific EFT operators. This suggests to always supplement fully differential analyses with projections into the relevant distributions evaluated with finer bins.","sentences":["We generalize previous studies on constraining operators of the Standard Model Effective Field Theory using Drell-Yan (DY) measurements to include at the same time all relevant operators and uncertainties.","It has been shown that fully differential measurements (triple differential for neutral and double differential for charged) are more sensitive to EFT effects.","Nevertheless, due to the finite statistics, the fully differential measurements sacrifice some statistical power on the shape (less invariant mass or transverse momentum bins) in favour of more kinematic variables.","We show that when the observables are particularly sensitive to the shape of the distributions, such as the invariant mass of the two leptons in neutral DY, the single differential measurement with more bins, may be as sensitive as the fully differential one, at least for specific EFT operators.","This suggests to always supplement fully differential analyses with projections into the relevant distributions evaluated with finer bins."],"url":"http://arxiv.org/abs/2404.10569v1","category":"hep-ph"}
{"created":"2024-04-16 13:39:25","title":"Photonic Neuromorphic Accelerators for Event-Based Imaging Flow Cytometry","abstract":"In this work, we present experimental results of a high-speed label-free imaging cytometry system that seamlessly merges the high-capturing rate and data sparsity of an event-based CMOS camera with lightweight photonic neuromorphic processing. This combination offers high classification accuracy and a massive reduction in the number of trainable parameters of the digital machine-learning back-end. The photonic neuromorphic accelerator is based on a hardware-friendly passive optical spectrum slicing technique that is able to extract meaningful features from the generated spike-trains. The experimental scenario comprises the discrimination of artificial polymethyl methacrylate calibrated beads, having different diameters, flowing at a mean speed of 0.01m/sec. Classification accuracy, using only lightweight, digital machine-learning schemes has topped at 98.2%. On the other hand, by experimentally pre-processing the raw spike data through the proposed photonic neuromorphic spectrum slicer we achieved an accuracy of 98.6%. This performance was accompanied by a reduction in the number of trainable parameters at the classification back-end by a factor ranging from 8 to 22, depending on the configuration of the digital neural network.","sentences":["In this work, we present experimental results of a high-speed label-free imaging cytometry system that seamlessly merges the high-capturing rate and data sparsity of an event-based CMOS camera with lightweight photonic neuromorphic processing.","This combination offers high classification accuracy and a massive reduction in the number of trainable parameters of the digital machine-learning back-end.","The photonic neuromorphic accelerator is based on a hardware-friendly passive optical spectrum slicing technique that is able to extract meaningful features from the generated spike-trains.","The experimental scenario comprises the discrimination of artificial polymethyl methacrylate calibrated beads, having different diameters, flowing at a mean speed of 0.01m/sec.","Classification accuracy, using only lightweight, digital machine-learning schemes has topped at 98.2%.","On the other hand, by experimentally pre-processing the raw spike data through the proposed photonic neuromorphic spectrum slicer we achieved an accuracy of 98.6%.","This performance was accompanied by a reduction in the number of trainable parameters at the classification back-end by a factor ranging from 8 to 22, depending on the configuration of the digital neural network."],"url":"http://arxiv.org/abs/2404.10564v1","category":"physics.optics"}
{"created":"2024-04-16 13:37:11","title":"On the Lorenzoni-Magri hierarchy of hydrodynamic type","abstract":"In 2005 Lorenzoni and Magri showed that a hydrodynamic-type hierarchy determined by the powers of a type (1,1) tensor field (on a smooth manifold) with vanishing Nijenhuis torsion can be deformed to a more general hierarchy, with the help of a chain of conservation laws of the new hierarchy. We review this construction. The (1,1) tensor fields of the resulting hierarchy have non-vanishing Nijenhuis torsion, in general, but their Haantjes tensor vanishes.","sentences":["In 2005 Lorenzoni and Magri showed that a hydrodynamic-type hierarchy determined by the powers of a type (1,1) tensor field (on a smooth manifold) with vanishing Nijenhuis torsion can be deformed to a more general hierarchy, with the help of a chain of conservation laws of the new hierarchy.","We review this construction.","The (1,1) tensor fields of the resulting hierarchy have non-vanishing Nijenhuis torsion, in general, but their Haantjes tensor vanishes."],"url":"http://arxiv.org/abs/2404.10562v1","category":"math-ph"}
{"created":"2024-04-16 13:25:44","title":"The theoretical basis of reservoir pressure in arteries","abstract":"The separation of measured arterial pressure into a reservoir pressure and an excess pressure was introduced nearly 20 years ago as an heuristic hypothesis. We demonstrate that a two-time asymptotic analysis of the 1-D conservation equations in each artery coupled with the separation of the smaller arteries into inviscid and resistance arteries, based on their resistance coefficients, results, for the first time, in a formal derivation of the reservoir pressure. The key to the two-time analysis is the existence of a fast time associated with the propagation of waves through the arteries and a slow time associated with the convective velocity of the blood. The ratio between these two time scales is given by the Mach number; the ratio of a characteristic convective velocity to a characteristic wave speed. If the Mach number is small, a formal asymptotic analysis can be carried out which is accurate to the order of the square of the Mach number. The slow-time conservation equations involve a resistance coefficient that models the effect of viscosity on the convective velocity. On the basis of this resistance coefficient, we separate the arteries into the larger inviscid arteries where the coefficient is negligible and the smaller resistance arteries where it it is not negligible. The slow time pressure in the inviscid arteries is shown to be spatially uniform but varying in time. We define this pressure as the reservoir pressure. Dynamic analysis using mass conservation in the inviscid arteries shows that the reservoir pressure accounts for the storage of potential energy by the distension of the elastic inviscid arteries during early systole and its release during late systole and diastole. This analysis thus provides a formal derivation of the reservoir pressure and its physical meaning.","sentences":["The separation of measured arterial pressure into a reservoir pressure and an excess pressure was introduced nearly 20 years ago as an heuristic hypothesis.","We demonstrate that a two-time asymptotic analysis of the 1-D conservation equations in each artery coupled with the separation of the smaller arteries into inviscid and resistance arteries, based on their resistance coefficients, results, for the first time, in a formal derivation of the reservoir pressure.","The key to the two-time analysis is the existence of a fast time associated with the propagation of waves through the arteries and a slow time associated with the convective velocity of the blood.","The ratio between these two time scales is given by the Mach number; the ratio of a characteristic convective velocity to a characteristic wave speed.","If the Mach number is small, a formal asymptotic analysis can be carried out which is accurate to the order of the square of the Mach number.","The slow-time conservation equations involve a resistance coefficient that models the effect of viscosity on the convective velocity.","On the basis of this resistance coefficient, we separate the arteries into the larger inviscid arteries where the coefficient is negligible and the smaller resistance arteries where it it is not negligible.","The slow time pressure in the inviscid arteries is shown to be spatially uniform but varying in time.","We define this pressure as the reservoir pressure.","Dynamic analysis using mass conservation in the inviscid arteries shows that the reservoir pressure accounts for the storage of potential energy by the distension of the elastic inviscid arteries during early systole and its release during late systole and diastole.","This analysis thus provides a formal derivation of the reservoir pressure and its physical meaning."],"url":"http://arxiv.org/abs/2404.10806v1","category":"q-bio.QM"}
{"created":"2024-04-16 13:18:02","title":"Classification of Prostate Cancer in 3D Magnetic Resonance Imaging Data based on Convolutional Neural Networks","abstract":"Prostate cancer is a commonly diagnosed cancerous disease among men world-wide. Even with modern technology such as multi-parametric magnetic resonance tomography and guided biopsies, the process for diagnosing prostate cancer remains time consuming and requires highly trained professionals. In this paper, different convolutional neural networks (CNN) are evaluated on their abilities to reliably classify whether an MRI sequence contains malignant lesions. Implementations of a ResNet, a ConvNet and a ConvNeXt for 3D image data are trained and evaluated. The models are trained using different data augmentation techniques, learning rates, and optimizers. The data is taken from a private dataset, provided by Cantonal Hospital Aarau. The best result was achieved by a ResNet3D, yielding an average precision score of 0.4583 and AUC ROC score of 0.6214.","sentences":["Prostate cancer is a commonly diagnosed cancerous disease among men world-wide.","Even with modern technology such as multi-parametric magnetic resonance tomography and guided biopsies, the process for diagnosing prostate cancer remains time consuming and requires highly trained professionals.","In this paper, different convolutional neural networks (CNN) are evaluated on their abilities to reliably classify whether an MRI sequence contains malignant lesions.","Implementations of a ResNet, a ConvNet and a ConvNeXt for 3D image data are trained and evaluated.","The models are trained using different data augmentation techniques, learning rates, and optimizers.","The data is taken from a private dataset, provided by Cantonal Hospital Aarau.","The best result was achieved by a ResNet3D, yielding an average precision score of 0.4583 and AUC ROC score of 0.6214."],"url":"http://arxiv.org/abs/2404.10548v1","category":"eess.IV"}
{"created":"2024-04-16 13:16:19","title":"Warm-Start Variational Quantum Policy Iteration","abstract":"Reinforcement learning is a powerful framework aiming to determine optimal behavior in highly complex decision-making scenarios. This objective can be achieved using policy iteration, which requires to solve a typically large linear system of equations. We propose the variational quantum policy iteration (VarQPI) algorithm, realizing this step with a NISQ-compatible quantum-enhanced subroutine. Its scalability is supported by an analysis of the structure of generic reinforcement learning environments, laying the foundation for potential quantum advantage with utility-scale quantum computers. Furthermore, we introduce the warm-start initialization variant (WS-VarQPI) that significantly reduces resource overhead. The algorithm solves a large FrozenLake environment with an underlying 256x256-dimensional linear system, indicating its practical robustness.","sentences":["Reinforcement learning is a powerful framework aiming to determine optimal behavior in highly complex decision-making scenarios.","This objective can be achieved using policy iteration, which requires to solve a typically large linear system of equations.","We propose the variational quantum policy iteration (VarQPI) algorithm, realizing this step with a NISQ-compatible quantum-enhanced subroutine.","Its scalability is supported by an analysis of the structure of generic reinforcement learning environments, laying the foundation for potential quantum advantage with utility-scale quantum computers.","Furthermore, we introduce the warm-start initialization variant (WS-VarQPI) that significantly reduces resource overhead.","The algorithm solves a large FrozenLake environment with an underlying 256x256-dimensional linear system, indicating its practical robustness."],"url":"http://arxiv.org/abs/2404.10546v1","category":"quant-ph"}
{"created":"2024-04-16 12:59:15","title":"Macdonald identities, Weyl-Kac denominator formulas and affine Grassmannians","abstract":"We expand the affine Weyl denominator formulas as signed $q$-series of ordinary Weyl characters running over the affine Grassmannian. Here the grading in $q$ coincides with the (dual) atomic length of the root system considered as introduced by Chapelier-Laget and Gerber. Next, we give simple expressions of the atomic lengths in terms of self-conjugate core partitions. This permits in particular to rederive, from the general theory of affine root systems, some results of the second author obtained by case-by-case computations on determinants and the use of particular families of strict partitions. These families are proved to be in simple one-to-one correspondences with the previous core partition model and, through this correspondence, the atomic length on cores equates the rank of the strict partitions considered. Finally, we make explicit some interactions between the affine Grassmannian elements and the Nekrasov-Okounkov type formulas.","sentences":["We expand the affine Weyl denominator formulas as signed $q$-series of ordinary Weyl characters running over the affine Grassmannian.","Here the grading in $q$ coincides with the (dual) atomic length of the root system considered as introduced by Chapelier-Laget and Gerber.","Next, we give simple expressions of the atomic lengths in terms of self-conjugate core partitions.","This permits in particular to rederive, from the general theory of affine root systems, some results of the second author obtained by case-by-case computations on determinants and the use of particular families of strict partitions.","These families are proved to be in simple one-to-one correspondences with the previous core partition model and, through this correspondence, the atomic length on cores equates the rank of the strict partitions considered.","Finally, we make explicit some interactions between the affine Grassmannian elements and the Nekrasov-Okounkov type formulas."],"url":"http://arxiv.org/abs/2404.10532v1","category":"math.CO"}
{"created":"2024-04-16 12:49:24","title":"Scalar field dark matter with time-varying equation of state","abstract":"We propose a new model of scalar field dark matter interacting with dark energy. Adopting a fluid description of the dark matter field in the regime of rapid oscillations, we find that the equation of state for dark matter is non-zero and even becomes increasingly negative at late times during dark energy domination. Furthermore, the speed of sound of dark matter is non-vanishing at all length scales, and a non-adiabatic pressure contribution arises. The results indicate that there are still unexplored possible interactions within the dark sector that lead to novel background effects and can impact structure formation processes.","sentences":["We propose a new model of scalar field dark matter interacting with dark energy.","Adopting a fluid description of the dark matter field in the regime of rapid oscillations, we find that the equation of state for dark matter is non-zero and even becomes increasingly negative at late times during dark energy domination.","Furthermore, the speed of sound of dark matter is non-vanishing at all length scales, and a non-adiabatic pressure contribution arises.","The results indicate that there are still unexplored possible interactions within the dark sector that lead to novel background effects and can impact structure formation processes."],"url":"http://arxiv.org/abs/2404.10524v1","category":"astro-ph.CO"}
{"created":"2024-04-16 12:48:54","title":"Capturing the Macroscopic Behaviour of Molecular Dynamics with Membership Functions","abstract":"Markov processes serve as foundational models in many scientific disciplines, such as molecular dynamics, and their simulation forms a common basis for analysis. While simulations produce useful trajectories, obtaining macroscopic information directly from microstate data presents significant challenges. This paper addresses this gap by introducing the concept of membership functions being the macrostates themselves. We derive equations for the holding times of these macrostates and demonstrate their consistency with the classical definition. Furthermore, we discuss the application of the ISOKANN method for learning these quantities from simulation data. In addition, we present a novel method for extracting transition paths based on the ISOKANN results and demonstrate its efficacy by applying it to simulations of the mu-opioid receptor. With this approach we provide a new perspective on analyzing the macroscopic behaviour of Markov systems.","sentences":["Markov processes serve as foundational models in many scientific disciplines, such as molecular dynamics, and their simulation forms a common basis for analysis.","While simulations produce useful trajectories, obtaining macroscopic information directly from microstate data presents significant challenges.","This paper addresses this gap by introducing the concept of membership functions being the macrostates themselves.","We derive equations for the holding times of these macrostates and demonstrate their consistency with the classical definition.","Furthermore, we discuss the application of the ISOKANN method for learning these quantities from simulation data.","In addition, we present a novel method for extracting transition paths based on the ISOKANN results and demonstrate its efficacy by applying it to simulations of the mu-opioid receptor.","With this approach we provide a new perspective on analyzing the macroscopic behaviour of Markov systems."],"url":"http://arxiv.org/abs/2404.10523v1","category":"physics.chem-ph"}
{"created":"2024-04-16 12:48:08","title":"Finiteness of the number of irreducible $\u03bb$-quiddities over a finite commutative and unitary ring","abstract":"$\\lambda$-quiddities of size $n$ are $n$-tuples of elements of a fixed set, solutions of a matrix equation appearing in the study of Coxeter's friezes. The study of these solutions involves in particular the use of a notion of irreducibility. The main objective of this text is to show that there is a finite number of irreducible $\\lambda$-quiddities over a finite unitary commutative ring and to obtain in this case an upper bound of the maximal size of them.","sentences":["$\\lambda$-quiddities of size $n$ are $n$-tuples of elements of a fixed set, solutions of a matrix equation appearing in the study of Coxeter's friezes.","The study of these solutions involves in particular the use of a notion of irreducibility.","The main objective of this text is to show that there is a finite number of irreducible $\\lambda$-quiddities over a finite unitary commutative ring and to obtain in this case an upper bound of the maximal size of them."],"url":"http://arxiv.org/abs/2404.10521v1","category":"math.CO"}
{"created":"2024-04-16 12:43:46","title":"Optical absorption in tilted geometries as an indirect measure of longitudinal plasma waves in layered cuprates","abstract":"Electromagnetic waves propagating in a layered superconductor with arbitrary momentum with respect to the main crystallographic directions display an unavoidable mixing between longitudinal and transverse degrees of freedom. Here we show that this basic physical mechanism explains the emergence of a well-defined absorption peak in the in-plane optical conductivity for light propagating at small tilting angles with respect to the stacking direction in layered cuprates. More specifically, we show that this peak, often interpreted as a spurious leakage of the $c$-axis Josephson plasmon, is instead a signature of the true longitudinal plasma mode occurring at larger momenta. By combining a classical approach based on Maxwell's equations with a full quantum derivation of the plasma modes based on the modelling of the superconducting phase degrees of freedom, we provide an analytical expression for the absorption peak as a function of the tilting angle and light polarization. We suggest that an all-optical measurement in tilted geometry can be used as an alternative way to access plasma-wave dispersion, usually measured by means of large-momenta scattering techniques like RIXS or EELS.","sentences":["Electromagnetic waves propagating in a layered superconductor with arbitrary momentum with respect to the main crystallographic directions display an unavoidable mixing between longitudinal and transverse degrees of freedom.","Here we show that this basic physical mechanism explains the emergence of a well-defined absorption peak in the in-plane optical conductivity for light propagating at small tilting angles with respect to the stacking direction in layered cuprates.","More specifically, we show that this peak, often interpreted as a spurious leakage of the $c$-axis Josephson plasmon, is instead a signature of the true longitudinal plasma mode occurring at larger momenta.","By combining a classical approach based on Maxwell's equations with a full quantum derivation of the plasma modes based on the modelling of the superconducting phase degrees of freedom, we provide an analytical expression for the absorption peak as a function of the tilting angle and light polarization.","We suggest that an all-optical measurement in tilted geometry can be used as an alternative way to access plasma-wave dispersion, usually measured by means of large-momenta scattering techniques like RIXS or EELS."],"url":"http://arxiv.org/abs/2404.10519v1","category":"cond-mat.supr-con"}
{"created":"2024-04-16 12:41:25","title":"MobileNetV4 - Universal Models for the Mobile Ecosystem","abstract":"We present the latest generation of MobileNets, known as MobileNetV4 (MNv4), featuring universally efficient architecture designs for mobile devices. At its core, we introduce the Universal Inverted Bottleneck (UIB) search block, a unified and flexible structure that merges Inverted Bottleneck (IB), ConvNext, Feed Forward Network (FFN), and a novel Extra Depthwise (ExtraDW) variant. Alongside UIB, we present Mobile MQA, an attention block tailored for mobile accelerators, delivering a significant 39% speedup. An optimized neural architecture search (NAS) recipe is also introduced which improves MNv4 search effectiveness. The integration of UIB, Mobile MQA and the refined NAS recipe results in a new suite of MNv4 models that are mostly Pareto optimal across mobile CPUs, DSPs, GPUs, as well as specialized accelerators like Apple Neural Engine and Google Pixel EdgeTPU - a characteristic not found in any other models tested. Finally, to further boost accuracy, we introduce a novel distillation technique. Enhanced by this technique, our MNv4-Hybrid-Large model delivers 87% ImageNet-1K accuracy, with a Pixel 8 EdgeTPU runtime of just 3.8ms.","sentences":["We present the latest generation of MobileNets, known as MobileNetV4 (MNv4), featuring universally efficient architecture designs for mobile devices.","At its core, we introduce the Universal Inverted Bottleneck (UIB) search block, a unified and flexible structure that merges Inverted Bottleneck (IB), ConvNext, Feed Forward Network (FFN), and a novel Extra Depthwise (ExtraDW) variant.","Alongside UIB, we present Mobile MQA, an attention block tailored for mobile accelerators, delivering a significant 39% speedup.","An optimized neural architecture search (NAS) recipe is also introduced which improves MNv4 search effectiveness.","The integration of UIB, Mobile MQA and the refined NAS recipe results in a new suite of MNv4 models that are mostly Pareto optimal across mobile CPUs, DSPs, GPUs, as well as specialized accelerators like Apple Neural Engine and Google Pixel EdgeTPU - a characteristic not found in any other models tested.","Finally, to further boost accuracy, we introduce a novel distillation technique.","Enhanced by this technique, our MNv4-Hybrid-Large model delivers 87% ImageNet-1K accuracy, with a Pixel 8 EdgeTPU runtime of just 3.8ms."],"url":"http://arxiv.org/abs/2404.10518v1","category":"cs.CV"}
{"created":"2024-04-17 16:16:50","title":"A Comparison of Traditional and Deep Learning Methods for Parameter Estimation of the Ornstein-Uhlenbeck Process","abstract":"We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widely used in finance, physics, and biology. Parameter estimation of the OU process is a challenging problem. Thus, we review traditional tracking methods and compare them with novel applications of deep learning to estimate the parameters of the OU process. We use a multi-layer perceptron to estimate the parameters of the OU process and compare its performance with traditional parameter estimation methods, such as the Kalman filter and maximum likelihood estimation. We find that the multi-layer perceptron can accurately estimate the parameters of the OU process given a large dataset of observed trajectories; however, traditional parameter estimation methods may be more suitable for smaller datasets.","sentences":["We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widely used in finance, physics, and biology.","Parameter estimation of the OU process is a challenging problem.","Thus, we review traditional tracking methods and compare them with novel applications of deep learning to estimate the parameters of the OU process.","We use a multi-layer perceptron to estimate the parameters of the OU process and compare its performance with traditional parameter estimation methods, such as the Kalman filter and maximum likelihood estimation.","We find that the multi-layer perceptron can accurately estimate the parameters of the OU process given a large dataset of observed trajectories; however, traditional parameter estimation methods may be more suitable for smaller datasets."],"url":"http://arxiv.org/abs/2404.11526v1","category":"q-fin.CP"}
{"created":"2024-04-17 15:45:31","title":"Randomly Pivoted Partial Cholesky: Random How?","abstract":"We consider the problem of finding good low rank approximations of symmetric, positive-definite $A \\in \\mathbb{R}^{n \\times n}$. Chen-Epperly-Tropp-Webber showed, among many other things, that the randomly pivoted partial Cholesky algorithm that chooses the $i-$th row with probability proportional to the diagonal entry $A_{ii}$ leads to a universal contraction of the trace norm (the Schatten 1-norm) in expectation for each step. We show that if one chooses the $i-$th row with likelihood proportional to $A_{ii}^2$ one obtains the same result in the Frobenius norm (the Schatten 2-norm). Implications for the greedy pivoting rule and pivot selection strategies are discussed.","sentences":["We consider the problem of finding good low rank approximations of symmetric, positive-definite $A \\in \\mathbb{R}^{n \\times n}$. Chen-Epperly-Tropp-Webber showed, among many other things, that the randomly pivoted partial Cholesky algorithm that chooses the $i-$th row with probability proportional to the diagonal entry $A_{ii}$ leads to a universal contraction of the trace norm (the Schatten 1-norm) in expectation for each step.","We show that if one chooses the $i-$th row with likelihood proportional to $A_{ii}^2$ one obtains the same result in the Frobenius norm (the Schatten 2-norm).","Implications for the greedy pivoting rule and pivot selection strategies are discussed."],"url":"http://arxiv.org/abs/2404.11487v1","category":"math.NA"}
{"created":"2024-04-17 15:23:12","title":"A Federated Learning Approach to Privacy Preserving Offensive Language Identification","abstract":"The spread of various forms of offensive speech online is an important concern in social media. While platforms have been investing heavily in ways of coping with this problem, the question of privacy remains largely unaddressed. Models trained to detect offensive language on social media are trained and/or fine-tuned using large amounts of data often stored in centralized servers. Since most social media data originates from end users, we propose a privacy preserving decentralized architecture for identifying offensive language online by introducing Federated Learning (FL) in the context of offensive language identification. FL is a decentralized architecture that allows multiple models to be trained locally without the need for data sharing hence preserving users' privacy. We propose a model fusion approach to perform FL. We trained multiple deep learning models on four publicly available English benchmark datasets (AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We also present initial cross-lingual experiments in English and Spanish. We show that the proposed model fusion approach outperforms baselines in all the datasets while preserving privacy.","sentences":["The spread of various forms of offensive speech online is an important concern in social media.","While platforms have been investing heavily in ways of coping with this problem, the question of privacy remains largely unaddressed.","Models trained to detect offensive language on social media are trained and/or fine-tuned using large amounts of data often stored in centralized servers.","Since most social media data originates from end users, we propose a privacy preserving decentralized architecture for identifying offensive language online by introducing Federated Learning (FL) in the context of offensive language identification.","FL is a decentralized architecture that allows multiple models to be trained locally without the need for data sharing hence preserving users' privacy.","We propose a model fusion approach to perform FL.","We trained multiple deep learning models on four publicly available English benchmark datasets (AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail.","We also present initial cross-lingual experiments in English and Spanish.","We show that the proposed model fusion approach outperforms baselines in all the datasets while preserving privacy."],"url":"http://arxiv.org/abs/2404.11470v1","category":"cs.CL"}
{"created":"2024-04-17 15:07:06","title":"Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent","abstract":"A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions. Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging. In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications. To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters. Like GPT-4, our model can process both English and Chinese. We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi.","sentences":["A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions.","Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging.","In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications.","To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters.","Like GPT-4, our model can process both English and Chinese.","We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi."],"url":"http://arxiv.org/abs/2404.11459v1","category":"cs.CL"}
{"created":"2024-04-17 14:55:27","title":"AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large Language Models for Extracting Cognitive Pathways from Social Media Texts","abstract":"Cognitive Behavioral Therapy (CBT) is an effective technique for addressing the irrational thoughts stemming from mental illnesses, but it necessitates precise identification of cognitive pathways to be successfully implemented in patient care. In current society, individuals frequently express negative emotions on social media on specific topics, often exhibiting cognitive distortions, including suicidal behaviors in extreme cases. Yet, there is a notable absence of methodologies for analyzing cognitive pathways that could aid psychotherapists in conducting effective interventions online. In this study, we gathered data from social media and established the task of extracting cognitive pathways, annotating the data based on a cognitive theoretical framework. We initially categorized the task of extracting cognitive pathways as a hierarchical text classification with four main categories and nineteen subcategories. Following this, we structured a text summarization task to help psychotherapists quickly grasp the essential information. Our experiments evaluate the performance of deep learning and large language models (LLMs) on these tasks. The results demonstrate that our deep learning method achieved a micro-F1 score of 62.34% in the hierarchical text classification task. Meanwhile, in the text summarization task, GPT-4 attained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the experimental deep learning model's performance. However, it may suffer from an issue of hallucination. We have made all models and codes publicly available to support further research in this field.","sentences":["Cognitive Behavioral Therapy (CBT) is an effective technique for addressing the irrational thoughts stemming from mental illnesses, but it necessitates precise identification of cognitive pathways to be successfully implemented in patient care.","In current society, individuals frequently express negative emotions on social media on specific topics, often exhibiting cognitive distortions, including suicidal behaviors in extreme cases.","Yet, there is a notable absence of methodologies for analyzing cognitive pathways that could aid psychotherapists in conducting effective interventions online.","In this study, we gathered data from social media and established the task of extracting cognitive pathways, annotating the data based on a cognitive theoretical framework.","We initially categorized the task of extracting cognitive pathways as a hierarchical text classification with four main categories and nineteen subcategories.","Following this, we structured a text summarization task to help psychotherapists quickly grasp the essential information.","Our experiments evaluate the performance of deep learning and large language models (LLMs) on these tasks.","The results demonstrate that our deep learning method achieved a micro-F1 score of 62.34% in the hierarchical text classification task.","Meanwhile, in the text summarization task, GPT-4 attained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the experimental deep learning model's performance.","However, it may suffer from an issue of hallucination.","We have made all models and codes publicly available to support further research in this field."],"url":"http://arxiv.org/abs/2404.11449v1","category":"cs.CL"}
{"created":"2024-04-17 14:34:02","title":"Matern Correlation: A Panoramic Primer","abstract":"Matern correlation is of pivotal importance in spatial statistics and machine learning. This paper serves as a panoramic primer for this correlation with an emphasis on the exposition of its changing behavior and smoothness properties in response to the change of its two parameters. Such exposition is achieved through a series of simulation studies, the use of an interactive 3D visualization applet, and a practical modeling example, all tailored for a wide-ranging statistical audience. Meanwhile, the thorough understanding of these parameter-smoothness relationships, in turn, serves as a pragmatic guide for researchers in their real-world modeling endeavors, such as setting appropriate initial values for these parameters and parameter-fine-tuning in their Bayesian modeling practice or simulation studies involving the Matern correlation. Derived problems surrounding Matern, such as inconsistent parameter inference, extended forms of Matern and limitations of Matern, are also explored and surveyed to impart a panoramic view of this correlation.","sentences":["Matern correlation is of pivotal importance in spatial statistics and machine learning.","This paper serves as a panoramic primer for this correlation with an emphasis on the exposition of its changing behavior and smoothness properties in response to the change of its two parameters.","Such exposition is achieved through a series of simulation studies, the use of an interactive 3D visualization applet, and a practical modeling example, all tailored for a wide-ranging statistical audience.","Meanwhile, the thorough understanding of these parameter-smoothness relationships, in turn, serves as a pragmatic guide for researchers in their real-world modeling endeavors, such as setting appropriate initial values for these parameters and parameter-fine-tuning in their Bayesian modeling practice or simulation studies involving the Matern correlation.","Derived problems surrounding Matern, such as inconsistent parameter inference, extended forms of Matern and limitations of Matern, are also explored and surveyed to impart a panoramic view of this correlation."],"url":"http://arxiv.org/abs/2404.11427v1","category":"stat.ME"}
{"created":"2024-04-17 13:09:44","title":"Consisaug: A Consistency-based Augmentation for Polyp Detection in Endoscopy Image Analysis","abstract":"Colorectal cancer (CRC), which frequently originates from initially benign polyps, remains a significant contributor to global cancer-related mortality. Early and accurate detection of these polyps via colonoscopy is crucial for CRC prevention. However, traditional colonoscopy methods depend heavily on the operator's experience, leading to suboptimal polyp detection rates. Besides, the public database are limited in polyp size and shape diversity. To enhance the available data for polyp detection, we introduce Consisaug, an innovative and effective methodology to augment data that leverages deep learning. We utilize the constraint that when the image is flipped the class label should be equal and the bonding boxes should be consistent. We implement our Consisaug on five public polyp datasets and at three backbones, and the results show the effectiveness of our method.","sentences":["Colorectal cancer (CRC), which frequently originates from initially benign polyps, remains a significant contributor to global cancer-related mortality.","Early and accurate detection of these polyps via colonoscopy is crucial for CRC prevention.","However, traditional colonoscopy methods depend heavily on the operator's experience, leading to suboptimal polyp detection rates.","Besides, the public database are limited in polyp size and shape diversity.","To enhance the available data for polyp detection, we introduce Consisaug, an innovative and effective methodology to augment data that leverages deep learning.","We utilize the constraint that when the image is flipped the class label should be equal and the bonding boxes should be consistent.","We implement our Consisaug on five public polyp datasets and at three backbones, and the results show the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.11355v1","category":"cs.CV"}
{"created":"2024-04-17 12:36:20","title":"On Learning Parities with Dependent Noise","abstract":"In this expository note we show that the learning parities with noise (LPN) assumption is robust to weak dependencies in the noise distribution of small batches of samples. This provides a partial converse to the linearization technique of [AG11]. The material in this note is drawn from a recent work by the authors [GMR24], where the robustness guarantee was a key component in a cryptographic separation between reinforcement learning and supervised learning.","sentences":["In this expository note we show that the learning parities with noise (LPN) assumption is robust to weak dependencies in the noise distribution of small batches of samples.","This provides a partial converse to the linearization technique of [AG11].","The material in this note is drawn from a recent work by the authors [GMR24], where the robustness guarantee was a key component in a cryptographic separation between reinforcement learning and supervised learning."],"url":"http://arxiv.org/abs/2404.11325v1","category":"cs.CR"}
{"created":"2024-04-17 12:13:18","title":"A Semantic Segmentation-guided Approach for Ground-to-Aerial Image Matching","abstract":"Nowadays the accurate geo-localization of ground-view images has an important role across domains as diverse as journalism, forensics analysis, transports, and Earth Observation. This work addresses the problem of matching a query ground-view image with the corresponding satellite image without GPS data. This is done by comparing the features from a ground-view image and a satellite one, innovatively leveraging the corresponding latter's segmentation mask through a three-stream Siamese-like network. The proposed method, Semantic Align Net (SAN), focuses on limited Field-of-View (FoV) and ground panorama images (images with a FoV of 360{\\deg}). The novelty lies in the fusion of satellite images in combination with their semantic segmentation masks, aimed at ensuring that the model can extract useful features and focus on the significant parts of the images. This work shows how SAN through semantic analysis of images improves the performance on the unlabelled CVUSA dataset for all the tested FoVs.","sentences":["Nowadays the accurate geo-localization of ground-view images has an important role across domains as diverse as journalism, forensics analysis, transports, and Earth Observation.","This work addresses the problem of matching a query ground-view image with the corresponding satellite image without GPS data.","This is done by comparing the features from a ground-view image and a satellite one, innovatively leveraging the corresponding latter's segmentation mask through a three-stream Siamese-like network.","The proposed method, Semantic Align Net (SAN), focuses on limited Field-of-View (FoV) and ground panorama images (images with a FoV of 360{\\deg}).","The novelty lies in the fusion of satellite images in combination with their semantic segmentation masks, aimed at ensuring that the model can extract useful features and focus on the significant parts of the images.","This work shows how SAN through semantic analysis of images improves the performance on the unlabelled CVUSA dataset for all the tested FoVs."],"url":"http://arxiv.org/abs/2404.11302v1","category":"cs.CV"}
{"created":"2024-04-17 11:34:14","title":"Quantum-inspired Techniques in Tensor Networks for Industrial Contexts","abstract":"In this paper we present a study of the applicability and feasibility of quantum-inspired algorithms and techniques in tensor networks for industrial environments and contexts, with a compilation of the available literature and an analysis of the use cases that may be affected by such methods. In addition, we explore the limitations of such techniques in order to determine their potential scalability.","sentences":["In this paper we present a study of the applicability and feasibility of quantum-inspired algorithms and techniques in tensor networks for industrial environments and contexts, with a compilation of the available literature and an analysis of the use cases that may be affected by such methods.","In addition, we explore the limitations of such techniques in order to determine their potential scalability."],"url":"http://arxiv.org/abs/2404.11277v1","category":"quant-ph"}
{"created":"2024-04-17 09:53:03","title":"Deciphering seasonal depression variations and interplays between weather changes, physical activity, and depression severity in real-world settings: Learnings from RADAR-MDD longitudinal mobile health study","abstract":"Prior research has shown that changes in seasons and weather can have a significant impact on depression severity. However, findings are inconsistent across populations, and the interplay between weather, behavior, and depression has not been fully quantified. This study analyzed real-world data from 428 participants (a subset; 68.7% of the cohort) in the RADAR-MDD longitudinal mobile health study to investigate seasonal variations in depression (measured through a remote validated assessment - PHQ-8) and examine the potential interplay between dynamic weather changes, physical activity (monitored via wearables), and depression severity. The clustering of PHQ-8 scores identified four distinct seasonal variations in depression severity: one stable trend and three varying patterns where depression peaks in different seasons. Among these patterns, participants within the stable trend had the oldest average age (p=0.002) and the lowest baseline PHQ-8 score (p=0.003). Mediation analysis assessing the indirect effect of weather on physical activity and depression showed significant differences among participants with different affective responses to weather. These findings illustrate the heterogeneity in individuals' seasonal depression variations and responses to weather, underscoring the necessity for personalized approaches to help understand the impact of environmental factors on the real-world effectiveness of behavioral treatments.","sentences":["Prior research has shown that changes in seasons and weather can have a significant impact on depression severity.","However, findings are inconsistent across populations, and the interplay between weather, behavior, and depression has not been fully quantified.","This study analyzed real-world data from 428 participants (a subset; 68.7% of the cohort) in the RADAR-MDD longitudinal mobile health study to investigate seasonal variations in depression (measured through a remote validated assessment - PHQ-8) and examine the potential interplay between dynamic weather changes, physical activity (monitored via wearables), and depression severity.","The clustering of PHQ-8 scores identified four distinct seasonal variations in depression severity: one stable trend and three varying patterns where depression peaks in different seasons.","Among these patterns, participants within the stable trend had the oldest average age (p=0.002) and the lowest baseline PHQ-8 score (p=0.003).","Mediation analysis assessing the indirect effect of weather on physical activity and depression showed significant differences among participants with different affective responses to weather.","These findings illustrate the heterogeneity in individuals' seasonal depression variations and responses to weather, underscoring the necessity for personalized approaches to help understand the impact of environmental factors on the real-world effectiveness of behavioral treatments."],"url":"http://arxiv.org/abs/2404.11212v1","category":"stat.AP"}
{"created":"2024-04-17 08:26:34","title":"LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory","abstract":"Transformer models have been successful in various sequence processing tasks, but the self-attention mechanism's computational cost limits its practicality for long sequences. Although there are existing attention variants that improve computational efficiency, they have a limited ability to abstract global information effectively based on their hand-crafted mixing strategies. On the other hand, state-space models (SSMs) are tailored for long sequences but cannot capture complicated local information. Therefore, the combination of them as a unified token mixer is a trend in recent long-sequence models. However, the linearized attention degrades performance significantly even when equipped with SSMs. To address the issue, we propose a new method called LongVQ. LongVQ uses the vector quantization (VQ) technique to compress the global abstraction as a length-fixed codebook, enabling the linear-time computation of the attention matrix. This technique effectively maintains dynamic global and local patterns, which helps to complement the lack of long-range dependency issues. Our experiments on the Long Range Arena benchmark, autoregressive language modeling, and image and speech classification demonstrate the effectiveness of LongVQ. Our model achieves significant improvements over other sequence models, including variants of Transformers, Convolutions, and recent State Space Models.","sentences":["Transformer models have been successful in various sequence processing tasks, but the self-attention mechanism's computational cost limits its practicality for long sequences.","Although there are existing attention variants that improve computational efficiency, they have a limited ability to abstract global information effectively based on their hand-crafted mixing strategies.","On the other hand, state-space models (SSMs) are tailored for long sequences but cannot capture complicated local information.","Therefore, the combination of them as a unified token mixer is a trend in recent long-sequence models.","However, the linearized attention degrades performance significantly even when equipped with SSMs.","To address the issue, we propose a new method called LongVQ.","LongVQ uses the vector quantization (VQ) technique to compress the global abstraction as a length-fixed codebook, enabling the linear-time computation of the attention matrix.","This technique effectively maintains dynamic global and local patterns, which helps to complement the lack of long-range dependency issues.","Our experiments on the Long Range Arena benchmark, autoregressive language modeling, and image and speech classification demonstrate the effectiveness of LongVQ.","Our model achieves significant improvements over other sequence models, including variants of Transformers, Convolutions, and recent State Space Models."],"url":"http://arxiv.org/abs/2404.11163v1","category":"cs.LG"}
{"created":"2024-04-17 08:21:02","title":"Pre-processing matters: A segment search method for WSI classification","abstract":"Pre-processing for whole slide images can affect classification performance both in the training and inference stages. Our study analyzes the impact of pre-processing parameters on inference and training across single- and multiple-domain datasets. However, searching for an optimal parameter set is time-consuming. To overcome this, we propose a novel Similarity-based Simulated Annealing approach for fast parameter tuning to enhance inference performance on single-domain data. Our method demonstrates significant performance improvements in accuracy, which raise accuracy from 0.512 to 0.847 in a single domain. We further extend our insight into training performance in multi-domain data by employing a novel Bayesian optimization to search optimal pre-processing parameters, resulting in a high AUC of 0.967. We highlight that better pre-processing for WSI can contribute to further accuracy improvement in the histology area.","sentences":["Pre-processing for whole slide images can affect classification performance both in the training and inference stages.","Our study analyzes the impact of pre-processing parameters on inference and training across single- and multiple-domain datasets.","However, searching for an optimal parameter set is time-consuming.","To overcome this, we propose a novel Similarity-based Simulated Annealing approach for fast parameter tuning to enhance inference performance on single-domain data.","Our method demonstrates significant performance improvements in accuracy, which raise accuracy from 0.512 to 0.847 in a single domain.","We further extend our insight into training performance in multi-domain data by employing a novel Bayesian optimization to search optimal pre-processing parameters, resulting in a high AUC of 0.967.","We highlight that better pre-processing for WSI can contribute to further accuracy improvement in the histology area."],"url":"http://arxiv.org/abs/2404.11161v1","category":"cs.CV"}
{"created":"2024-04-17 07:36:40","title":"Context-Aware Siamese Networks for Efficient Emotion Recognition in Conversation","abstract":"The advent of deep learning models has made a considerable contribution to the achievement of Emotion Recognition in Conversation (ERC). However, this task still remains an important challenge due to the plurality and subjectivity of human emotions. Previous work on ERC provides predictive models using mostly graph-based conversation representations. In this work, we propose a way to model the conversational context that we incorporate into a metric learning training strategy, with a two-step process. This allows us to perform ERC in a flexible classification scenario and to end up with a lightweight yet efficient model. Using metric learning through a Siamese Network architecture, we achieve 57.71 in macro F1 score for emotion classification in conversation on DailyDialog dataset, which outperforms the related work. This state-of-the-art result is promising regarding the use of metric learning for emotion recognition, yet perfectible compared to the microF1 score obtained.","sentences":["The advent of deep learning models has made a considerable contribution to the achievement of Emotion Recognition in Conversation (ERC).","However, this task still remains an important challenge due to the plurality and subjectivity of human emotions.","Previous work on ERC provides predictive models using mostly graph-based conversation representations.","In this work, we propose a way to model the conversational context that we incorporate into a metric learning training strategy, with a two-step process.","This allows us to perform ERC in a flexible classification scenario and to end up with a lightweight yet efficient model.","Using metric learning through a Siamese Network architecture, we achieve 57.71 in macro F1 score for emotion classification in conversation on DailyDialog dataset, which outperforms the related work.","This state-of-the-art result is promising regarding the use of metric learning for emotion recognition, yet perfectible compared to the microF1 score obtained."],"url":"http://arxiv.org/abs/2404.11141v1","category":"cs.CL"}
{"created":"2024-04-17 07:26:23","title":"A Novel ICD Coding Framework Based on Associated and Hierarchical Code Description Distillation","abstract":"ICD(International Classification of Diseases) coding involves assigning ICD codes to patients visit based on their medical notes. ICD coding is a challenging multilabel text classification problem due to noisy medical document inputs. Recent advancements in automated ICD coding have enhanced performance by integrating additional data and knowledge bases with the encoding of medical notes and codes. However, most of them ignore the code hierarchy, leading to improper code assignments. To address these problems, we propose a novel framework based on associated and hierarchical code description distillation (AHDD) for better code representation learning and avoidance of improper code assignment.we utilize the code description and the hierarchical structure inherent to the ICD codes. Therefore, in this paper, we leverage the code description and the hierarchical structure inherent to the ICD codes. The code description is also applied to aware the attention layer and output layer. Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baselines.","sentences":["ICD(International Classification of Diseases) coding involves assigning ICD codes to patients visit based on their medical notes.","ICD coding is a challenging multilabel text classification problem due to noisy medical document inputs.","Recent advancements in automated ICD coding have enhanced performance by integrating additional data and knowledge bases with the encoding of medical notes and codes.","However, most of them ignore the code hierarchy, leading to improper code assignments.","To address these problems, we propose a novel framework based on associated and hierarchical code description distillation (AHDD) for better code representation learning and avoidance of improper code assignment.we utilize the code description and the hierarchical structure inherent to the ICD codes.","Therefore, in this paper, we leverage the code description and the hierarchical structure inherent to the ICD codes.","The code description is also applied to aware the attention layer and output layer.","Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baselines."],"url":"http://arxiv.org/abs/2404.11132v1","category":"cs.CL"}
{"created":"2024-04-17 07:20:56","title":"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales","abstract":"The remarkable performance of Multimodal Large Language Models (MLLMs) has unequivocally demonstrated their proficient understanding capabilities in handling a wide array of visual tasks. Nevertheless, the opaque nature of their black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate compositional reasoning tasks is also constrained, culminating in a stagnation of learning progression for these models. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness and precision. Subsequently, through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability. Empirical evidence from experiments demonstrates the superiority of our method across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability. Our approach also reduces hallucinations owing to its high correlation between images and text.","sentences":["The remarkable performance of Multimodal Large Language Models (MLLMs) has unequivocally demonstrated their proficient understanding capabilities in handling a wide array of visual tasks.","Nevertheless, the opaque nature of their black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination.","Their ability to execute intricate compositional reasoning tasks is also constrained, culminating in a stagnation of learning progression for these models.","In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs.","This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness and precision.","Subsequently, through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness.","Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability.","Empirical evidence from experiments demonstrates the superiority of our method across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability.","Our approach also reduces hallucinations owing to its high correlation between images and text."],"url":"http://arxiv.org/abs/2404.11129v1","category":"cs.CV"}
{"created":"2024-04-17 07:08:38","title":"TiNO-Edit: Timestep and Noise Optimization for Robust Diffusion-Based Image Editing","abstract":"Despite many attempts to leverage pre-trained text-to-image models (T2I) like Stable Diffusion (SD) for controllable image editing, producing good predictable results remains a challenge. Previous approaches have focused on either fine-tuning pre-trained T2I models on specific datasets to generate certain kinds of images (e.g., with a specific object or person), or on optimizing the weights, text prompts, and/or learning features for each input image in an attempt to coax the image generator to produce the desired result. However, these approaches all have shortcomings and fail to produce good results in a predictable and controllable manner. To address this problem, we present TiNO-Edit, an SD-based method that focuses on optimizing the noise patterns and diffusion timesteps during editing, something previously unexplored in the literature. With this simple change, we are able to generate results that both better align with the original images and reflect the desired result. Furthermore, we propose a set of new loss functions that operate in the latent domain of SD, greatly speeding up the optimization when compared to prior approaches, which operate in the pixel domain. Our method can be easily applied to variations of SD including Textual Inversion and DreamBooth that encode new concepts and incorporate them into the edited results. We present a host of image-editing capabilities enabled by our approach. Our code is publicly available at https://github.com/SherryXTChen/TiNO-Edit.","sentences":["Despite many attempts to leverage pre-trained text-to-image models (T2I) like Stable Diffusion (SD) for controllable image editing, producing good predictable results remains a challenge.","Previous approaches have focused on either fine-tuning pre-trained T2I models on specific datasets to generate certain kinds of images (e.g., with a specific object or person), or on optimizing the weights, text prompts, and/or learning features for each input image in an attempt to coax the image generator to produce the desired result.","However, these approaches all have shortcomings and fail to produce good results in a predictable and controllable manner.","To address this problem, we present TiNO-Edit, an SD-based method that focuses on optimizing the noise patterns and diffusion timesteps during editing, something previously unexplored in the literature.","With this simple change, we are able to generate results that both better align with the original images and reflect the desired result.","Furthermore, we propose a set of new loss functions that operate in the latent domain of SD, greatly speeding up the optimization when compared to prior approaches, which operate in the pixel domain.","Our method can be easily applied to variations of SD including Textual Inversion and DreamBooth that encode new concepts and incorporate them into the edited results.","We present a host of image-editing capabilities enabled by our approach.","Our code is publicly available at https://github.com/SherryXTChen/TiNO-Edit."],"url":"http://arxiv.org/abs/2404.11120v1","category":"cs.CV"}
{"created":"2024-04-17 07:07:41","title":"DRepMRec: A Dual Representation Learning Framework for Multimodal Recommendation","abstract":"Multimodal Recommendation focuses mainly on how to effectively integrate behavior and multimodal information in the recommendation task. Previous works suffer from two major issues. Firstly, the training process tightly couples the behavior module and multimodal module by jointly optimizing them using the sharing model parameters, which leads to suboptimal performance since behavior signals and modality signals often provide opposite guidance for the parameters updates. Secondly, previous approaches fail to take into account the significant distribution differences between behavior and modality when they attempt to fuse behavior and modality information. This resulted in a misalignment between the representations of behavior and modality. To address these challenges, in this paper, we propose a novel Dual Representation learning framework for Multimodal Recommendation called DRepMRec, which introduce separate dual lines for coupling problem and Behavior-Modal Alignment (BMA) for misalignment problem. Specifically, DRepMRec leverages two independent lines of representation learning to calculate behavior and modal representations. After obtaining separate behavior and modal representations, we design a Behavior-Modal Alignment Module (BMA) to align and fuse the dual representations to solve the misalignment problem. Furthermore, we integrate the BMA into other recommendation models, resulting in consistent performance improvements. To ensure dual representations maintain their semantic independence during alignment, we introduce Similarity-Supervised Signal (SSS) for representation learning. We conduct extensive experiments on three public datasets and our method achieves state-of-the-art (SOTA) results. The source code will be available upon acceptance.","sentences":["Multimodal Recommendation focuses mainly on how to effectively integrate behavior and multimodal information in the recommendation task.","Previous works suffer from two major issues.","Firstly, the training process tightly couples the behavior module and multimodal module by jointly optimizing them using the sharing model parameters, which leads to suboptimal performance since behavior signals and modality signals often provide opposite guidance for the parameters updates.","Secondly, previous approaches fail to take into account the significant distribution differences between behavior and modality when they attempt to fuse behavior and modality information.","This resulted in a misalignment between the representations of behavior and modality.","To address these challenges, in this paper, we propose a novel Dual Representation learning framework for Multimodal Recommendation called DRepMRec, which introduce separate dual lines for coupling problem and Behavior-Modal Alignment (BMA) for misalignment problem.","Specifically, DRepMRec leverages two independent lines of representation learning to calculate behavior and modal representations.","After obtaining separate behavior and modal representations, we design a Behavior-Modal Alignment Module (BMA) to align and fuse the dual representations to solve the misalignment problem.","Furthermore, we integrate the BMA into other recommendation models, resulting in consistent performance improvements.","To ensure dual representations maintain their semantic independence during alignment, we introduce Similarity-Supervised Signal (SSS) for representation learning.","We conduct extensive experiments on three public datasets and our method achieves state-of-the-art (SOTA) results.","The source code will be available upon acceptance."],"url":"http://arxiv.org/abs/2404.11119v1","category":"cs.IR"}
{"created":"2024-04-17 04:52:24","title":"Efficient Approaches for GEMM Acceleration on Leading AI-Optimized FPGAs","abstract":"FPGAs are a promising platform for accelerating Deep Learning (DL) applications, due to their high performance, low power consumption, and reconfigurability. Recently, the leading FPGA vendors have enhanced their architectures to more efficiently support the computational demands of DL workloads. However, the two most prominent AI-optimized FPGAs, i.e., AMD/Xilinx Versal ACAP and Intel Stratix 10 NX, employ significantly different architectural approaches. This paper presents novel systematic frameworks to optimize the performance of General Matrix Multiplication (GEMM), a fundamental operation in DL workloads, by exploiting the unique and distinct architectural characteristics of each FPGA. Our evaluation on GEMM workloads for int8 precision shows up to 77 and 68 TOPs (int8) throughput, with up to 0.94 and 1.35 TOPs/W energy efficiency for Versal VC1902 and Stratix 10 NX, respectively. This work provides insights and guidelines for optimizing GEMM-based applications on both platforms, while also delving into their programmability trade-offs and associated challenges.","sentences":["FPGAs are a promising platform for accelerating Deep Learning (DL) applications, due to their high performance, low power consumption, and reconfigurability.","Recently, the leading FPGA vendors have enhanced their architectures to more efficiently support the computational demands of DL workloads.","However, the two most prominent AI-optimized FPGAs, i.e., AMD/Xilinx Versal ACAP and Intel Stratix 10 NX, employ significantly different architectural approaches.","This paper presents novel systematic frameworks to optimize the performance of General Matrix Multiplication (GEMM), a fundamental operation in DL workloads, by exploiting the unique and distinct architectural characteristics of each FPGA.","Our evaluation on GEMM workloads for int8 precision shows up to 77 and 68 TOPs (int8) throughput, with up to 0.94 and 1.35 TOPs/W energy efficiency for Versal VC1902 and Stratix 10 NX, respectively.","This work provides insights and guidelines for optimizing GEMM-based applications on both platforms, while also delving into their programmability trade-offs and associated challenges."],"url":"http://arxiv.org/abs/2404.11066v1","category":"cs.AR"}
{"created":"2024-04-17 03:51:24","title":"WPS-Dataset: A benchmark for wood plate segmentation in bark removal processing","abstract":"Using deep learning methods is a promising approach to improving bark removal efficiency and enhancing the quality of wood products. However, the lack of publicly available datasets for wood plate segmentation in bark removal processing poses challenges for researchers in this field. To address this issue, a benchmark for wood plate segmentation in bark removal processing named WPS-dataset is proposed in this study, which consists of 4863 images. We designed an image acquisition device and assembled it on a bark removal equipment to capture images in real industrial settings. We evaluated the WPS-dataset using six typical segmentation models. The models effectively learn and understand the WPS-dataset characteristics during training, resulting in high performance and accuracy in wood plate segmentation tasks. We believe that our dataset can lay a solid foundation for future research in bark removal processing and contribute to advancements in this field.","sentences":["Using deep learning methods is a promising approach to improving bark removal efficiency and enhancing the quality of wood products.","However, the lack of publicly available datasets for wood plate segmentation in bark removal processing poses challenges for researchers in this field.","To address this issue, a benchmark for wood plate segmentation in bark removal processing named WPS-dataset is proposed in this study, which consists of 4863 images.","We designed an image acquisition device and assembled it on a bark removal equipment to capture images in real industrial settings.","We evaluated the WPS-dataset using six typical segmentation models.","The models effectively learn and understand the WPS-dataset characteristics during training, resulting in high performance and accuracy in wood plate segmentation tasks.","We believe that our dataset can lay a solid foundation for future research in bark removal processing and contribute to advancements in this field."],"url":"http://arxiv.org/abs/2404.11051v1","category":"cs.CV"}
{"created":"2024-04-17 03:20:42","title":"CORE: Data Augmentation for Link Prediction via Information Bottleneck","abstract":"Link prediction (LP) is a fundamental task in graph representation learning, with numerous applications in diverse domains. However, the generalizability of LP models is often compromised due to the presence of noisy or spurious information in graphs and the inherent incompleteness of graph data. To address these challenges, we draw inspiration from the Information Bottleneck principle and propose a novel data augmentation method, COmplete and REduce (CORE) to learn compact and predictive augmentations for LP models. In particular, CORE aims to recover missing edges in graphs while simultaneously removing noise from the graph structures, thereby enhancing the model's robustness and performance. Extensive experiments on multiple benchmark datasets demonstrate the applicability and superiority of CORE over state-of-the-art methods, showcasing its potential as a leading approach for robust LP in graph representation learning.","sentences":["Link prediction (LP) is a fundamental task in graph representation learning, with numerous applications in diverse domains.","However, the generalizability of LP models is often compromised due to the presence of noisy or spurious information in graphs and the inherent incompleteness of graph data.","To address these challenges, we draw inspiration from the Information Bottleneck principle and propose a novel data augmentation method, COmplete and REduce (CORE) to learn compact and predictive augmentations for LP models.","In particular, CORE aims to recover missing edges in graphs while simultaneously removing noise from the graph structures, thereby enhancing the model's robustness and performance.","Extensive experiments on multiple benchmark datasets demonstrate the applicability and superiority of CORE over state-of-the-art methods, showcasing its potential as a leading approach for robust LP in graph representation learning."],"url":"http://arxiv.org/abs/2404.11032v1","category":"cs.LG"}
{"created":"2024-04-17 03:10:29","title":"Student self-management, academic achievement: Exploring the mediating role of self-efficacy and the moderating influence of gender insights from a survey conducted in 3 universities in America","abstract":"Excellent students are not only those who master more effective and efficient learning techniques to acquire and apply information. Even in the absence of correct learning, they are able to self-motivate, evaluate, and adjust their behavior. This study aims to explore the relationship between student self-management and academic achievement, with a focus on investigating the mediating role of self-efficacy and the moderating influence of gender in this relationship. A total of 289 students from three universities in the United States participated in this research. The results of the study indicate that students' level of self-management is positively correlated with their academic achievement, with self-efficacy playing a mediating role in this relationship and gender exerting a certain moderating effect. This study provides important insights into understanding the relationship between student self-management and academic achievement and supports the crucial role of educational leaders in educational practice.","sentences":["Excellent students are not only those who master more effective and efficient learning techniques to acquire and apply information.","Even in the absence of correct learning, they are able to self-motivate, evaluate, and adjust their behavior.","This study aims to explore the relationship between student self-management and academic achievement, with a focus on investigating the mediating role of self-efficacy and the moderating influence of gender in this relationship.","A total of 289 students from three universities in the United States participated in this research.","The results of the study indicate that students' level of self-management is positively correlated with their academic achievement, with self-efficacy playing a mediating role in this relationship and gender exerting a certain moderating effect.","This study provides important insights into understanding the relationship between student self-management and academic achievement and supports the crucial role of educational leaders in educational practice."],"url":"http://arxiv.org/abs/2404.11029v1","category":"cs.CY"}
{"created":"2024-04-17 02:57:53","title":"Duality induced by an embedding structure of determinantal point process","abstract":"This paper investigates the information geometrical structure of a determinantal point process (DPP). It demonstrates that a DPP is embedded in the exponential family of log-linear models. The extent of deviation from an exponential family is analyzed using the $\\mathrm{e}$-embedding curvature tensor, which identifies partially flat parameters of a DPP. On the basis of this embedding structure, the duality related to a marginal kernel and an $L$-ensemble kernel is discovered.","sentences":["This paper investigates the information geometrical structure of a determinantal point process (DPP).","It demonstrates that a DPP is embedded in the exponential family of log-linear models.","The extent of deviation from an exponential family is analyzed using the $\\mathrm{e}$-embedding curvature tensor, which identifies partially flat parameters of a DPP.","On the basis of this embedding structure, the duality related to a marginal kernel and an $L$-ensemble kernel is discovered."],"url":"http://arxiv.org/abs/2404.11024v1","category":"math.ST"}
{"created":"2024-04-17 02:18:48","title":"Machine-Learning-Enhanced Soft Robotic System Inspired by Rectal Functions for Investigating Fecal incontinence","abstract":"Fecal incontinence, arising from a myriad of pathogenic mechanisms, has attracted considerable global attention. Despite its significance, the replication of the defecatory system for studying fecal incontinence mechanisms remains limited largely due to social stigma and taboos. Inspired by the rectum's functionalities, we have developed a soft robotic system, encompassing a power supply, pressure sensing, data acquisition systems, a flushing mechanism, a stage, and a rectal module. The innovative soft rectal module includes actuators inspired by sphincter muscles, both soft and rigid covers, and soft rectum mold. The rectal mold, fabricated from materials that closely mimic human rectal tissue, is produced using the mold replication fabrication method. Both the soft and rigid components of the mold are realized through the application of 3D-printing technology. The sphincter muscles-inspired actuators featuring double-layer pouch structures are modeled and optimized based on multilayer perceptron methods aiming to obtain high contractions ratios (100%), high generated pressure (9.8 kPa), and small recovery time (3 s). Upon assembly, this defecation robot is capable of smoothly expelling liquid faeces, performing controlled solid fecal cutting, and defecating extremely solid long faeces, thus closely replicating the human rectum and anal canal's functions. This defecation robot has the potential to assist humans in understanding the complex defecation system and contribute to the development of well-being devices related to defecation.","sentences":["Fecal incontinence, arising from a myriad of pathogenic mechanisms, has attracted considerable global attention.","Despite its significance, the replication of the defecatory system for studying fecal incontinence mechanisms remains limited largely due to social stigma and taboos.","Inspired by the rectum's functionalities, we have developed a soft robotic system, encompassing a power supply, pressure sensing, data acquisition systems, a flushing mechanism, a stage, and a rectal module.","The innovative soft rectal module includes actuators inspired by sphincter muscles, both soft and rigid covers, and soft rectum mold.","The rectal mold, fabricated from materials that closely mimic human rectal tissue, is produced using the mold replication fabrication method.","Both the soft and rigid components of the mold are realized through the application of 3D-printing technology.","The sphincter muscles-inspired actuators featuring double-layer pouch structures are modeled and optimized based on multilayer perceptron methods aiming to obtain high contractions ratios (100%), high generated pressure (9.8 kPa), and small recovery time (3 s).","Upon assembly, this defecation robot is capable of smoothly expelling liquid faeces, performing controlled solid fecal cutting, and defecating extremely solid long faeces, thus closely replicating the human rectum and anal canal's functions.","This defecation robot has the potential to assist humans in understanding the complex defecation system and contribute to the development of well-being devices related to defecation."],"url":"http://arxiv.org/abs/2404.10999v1","category":"cs.RO"}
{"created":"2024-04-17 02:17:05","title":"Clipped SGD Algorithms for Privacy Preserving Performative Prediction: Bias Amplification and Remedies","abstract":"Clipped stochastic gradient descent (SGD) algorithms are among the most popular algorithms for privacy preserving optimization that reduces the leakage of users' identity in model training. This paper studies the convergence properties of these algorithms in a performative prediction setting, where the data distribution may shift due to the deployed prediction model. For example, the latter is caused by strategical users during the training of loan policy for banks. Our contributions are two-fold. First, we show that the straightforward implementation of a projected clipped SGD (PCSGD) algorithm may converge to a biased solution compared to the performative stable solution. We quantify the lower and upper bound for the magnitude of the bias and demonstrate a bias amplification phenomenon where the bias grows with the sensitivity of the data distribution. Second, we suggest two remedies to the bias amplification effect. The first one utilizes an optimal step size design for PCSGD that takes the privacy guarantee into account. The second one uses the recently proposed DiceSGD algorithm [Zhang et al., 2024]. We show that the latter can successfully remove the bias and converge to the performative stable solution. Numerical experiments verify our analysis.","sentences":["Clipped stochastic gradient descent (SGD) algorithms are among the most popular algorithms for privacy preserving optimization that reduces the leakage of users' identity in model training.","This paper studies the convergence properties of these algorithms in a performative prediction setting, where the data distribution may shift due to the deployed prediction model.","For example, the latter is caused by strategical users during the training of loan policy for banks.","Our contributions are two-fold.","First, we show that the straightforward implementation of a projected clipped SGD (PCSGD) algorithm may converge to a biased solution compared to the performative stable solution.","We quantify the lower and upper bound for the magnitude of the bias and demonstrate a bias amplification phenomenon where the bias grows with the sensitivity of the data distribution.","Second, we suggest two remedies to the bias amplification effect.","The first one utilizes an optimal step size design for PCSGD that takes the privacy guarantee into account.","The second one uses the recently proposed DiceSGD algorithm","[Zhang et al., 2024].","We show that the latter can successfully remove the bias and converge to the performative stable solution.","Numerical experiments verify our analysis."],"url":"http://arxiv.org/abs/2404.10995v1","category":"math.OC"}
{"created":"2024-04-17 02:01:50","title":"Automating Personalized Parsons Problems with Customized Contexts and Concepts","abstract":"Parsons problems provide useful scaffolding for introductory programming students learning to write code. However, generating large numbers of high-quality Parsons problems that appeal to the diverse range of interests in a typical introductory course is a significant challenge for educators. Large language models (LLMs) may offer a solution, by allowing students to produce on-demand Parsons problems for topics covering the breadth of the introductory programming curriculum, and targeting thematic contexts that align with their personal interests. In this paper, we introduce PuzzleMakerPy, an educational tool that uses an LLM to generate unlimited contextualized drag-and-drop programming exercises in the form of Parsons Problems, which introductory programmers can use as a supplemental learning resource. We evaluated PuzzleMakerPy by deploying it in a large introductory programming course, and found that the ability to personalize the contextual framing used in problem descriptions was highly engaging for students, and being able to customize the programming topics was reported as being useful for their learning.","sentences":["Parsons problems provide useful scaffolding for introductory programming students learning to write code.","However, generating large numbers of high-quality Parsons problems that appeal to the diverse range of interests in a typical introductory course is a significant challenge for educators.","Large language models (LLMs) may offer a solution, by allowing students to produce on-demand Parsons problems for topics covering the breadth of the introductory programming curriculum, and targeting thematic contexts that align with their personal interests.","In this paper, we introduce PuzzleMakerPy, an educational tool that uses an LLM to generate unlimited contextualized drag-and-drop programming exercises in the form of Parsons Problems, which introductory programmers can use as a supplemental learning resource.","We evaluated PuzzleMakerPy by deploying it in a large introductory programming course, and found that the ability to personalize the contextual framing used in problem descriptions was highly engaging for students, and being able to customize the programming topics was reported as being useful for their learning."],"url":"http://arxiv.org/abs/2404.10990v1","category":"cs.CY"}
{"created":"2024-04-17 01:53:03","title":"FairSSD: Understanding Bias in Synthetic Speech Detectors","abstract":"Methods that can generate synthetic speech which is perceptually indistinguishable from speech recorded by a human speaker, are easily available. Several incidents report misuse of synthetic speech generated from these methods to commit fraud. To counter such misuse, many methods have been proposed to detect synthetic speech. Some of these detectors are more interpretable, can generalize to detect synthetic speech in the wild and are robust to noise. However, limited work has been done on understanding bias in these detectors. In this work, we examine bias in existing synthetic speech detectors to determine if they will unfairly target a particular gender, age and accent group. We also inspect whether these detectors will have a higher misclassification rate for bona fide speech from speech-impaired speakers w.r.t fluent speakers. Extensive experiments on 6 existing synthetic speech detectors using more than 0.9 million speech signals demonstrate that most detectors are gender, age and accent biased, and future work is needed to ensure fairness. To support future research, we release our evaluation dataset, models used in our study and source code at https://gitlab.com/viper-purdue/fairssd.","sentences":["Methods that can generate synthetic speech which is perceptually indistinguishable from speech recorded by a human speaker, are easily available.","Several incidents report misuse of synthetic speech generated from these methods to commit fraud.","To counter such misuse, many methods have been proposed to detect synthetic speech.","Some of these detectors are more interpretable, can generalize to detect synthetic speech in the wild and are robust to noise.","However, limited work has been done on understanding bias in these detectors.","In this work, we examine bias in existing synthetic speech detectors to determine if they will unfairly target a particular gender, age and accent group.","We also inspect whether these detectors will have a higher misclassification rate for bona fide speech from speech-impaired speakers w.r.t fluent speakers.","Extensive experiments on 6 existing synthetic speech detectors using more than 0.9 million speech signals demonstrate that most detectors are gender, age and accent biased, and future work is needed to ensure fairness.","To support future research, we release our evaluation dataset, models used in our study and source code at https://gitlab.com/viper-purdue/fairssd."],"url":"http://arxiv.org/abs/2404.10989v1","category":"cs.CV"}
{"created":"2024-04-17 01:52:48","title":"From Paper to Platform: Evolution of a Novel Learning Environment for Tabletop Exercises","abstract":"For undergraduate students of computing, learning to solve complex practical problems in a team is an essential skill for their future careers. This skill is needed in various fields, such as in cybersecurity and IT governance. Tabletop exercises are an innovative teaching method used in practice for training teams in incident response and evaluation of contingency plans. However, tabletop exercises are not yet widely established in university education. This paper presents data and teaching experience from a cybersecurity course that introduces tabletop exercises in classrooms using a novel technology: INJECT Exercise Platform (IXP), a web-based learning environment for delivering and evaluating the exercises. This technology substantially improves the prior practice, since tabletop exercises worldwide have usually been conducted using pen and paper. Unlike in traditional tabletop exercises, which are difficult to evaluate manually, IXP provides insights into students' behavior and learning based on automated analysis of interaction data. We demonstrate IXP's capabilities and evolution by comparing exercise sessions hosted throughout three years at different stages of the platform's readiness. The analysis of student data is supplemented by the discussion of the lessons learned from employing IXP in computing education contexts. The data analytics enabled a detailed comparison of the teams' performance and behavior. Instructors who consider innovating their classes with tabletop exercises may use IXP and benefit from the insights in this paper.","sentences":["For undergraduate students of computing, learning to solve complex practical problems in a team is an essential skill for their future careers.","This skill is needed in various fields, such as in cybersecurity and IT governance.","Tabletop exercises are an innovative teaching method used in practice for training teams in incident response and evaluation of contingency plans.","However, tabletop exercises are not yet widely established in university education.","This paper presents data and teaching experience from a cybersecurity course that introduces tabletop exercises in classrooms using a novel technology: INJECT Exercise Platform (IXP), a web-based learning environment for delivering and evaluating the exercises.","This technology substantially improves the prior practice, since tabletop exercises worldwide have usually been conducted using pen and paper.","Unlike in traditional tabletop exercises, which are difficult to evaluate manually, IXP provides insights into students' behavior and learning based on automated analysis of interaction data.","We demonstrate IXP's capabilities and evolution by comparing exercise sessions hosted throughout three years at different stages of the platform's readiness.","The analysis of student data is supplemented by the discussion of the lessons learned from employing IXP in computing education contexts.","The data analytics enabled a detailed comparison of the teams' performance and behavior.","Instructors who consider innovating their classes with tabletop exercises may use IXP and benefit from the insights in this paper."],"url":"http://arxiv.org/abs/2404.10988v1","category":"cs.CY"}
{"created":"2024-04-17 01:35:52","title":"Pixel-Wise Symbol Spotting via Progressive Points Location for Parsing CAD Images","abstract":"Parsing Computer-Aided Design (CAD) drawings is a fundamental step for CAD revision, semantic-based management, and the generation of 3D prototypes in both the architecture and engineering industries. Labeling symbols from a CAD drawing is a challenging yet notorious task from a practical point of view. In this work, we propose to label and spot symbols from CAD images that are converted from CAD drawings. The advantage of spotting symbols from CAD images lies in the low requirement of labelers and the low-cost annotation. However, pixel-wise spotting symbols from CAD images is challenging work. We propose a pixel-wise point location via Progressive Gaussian Kernels (PGK) to balance between training efficiency and location accuracy. Besides, we introduce a local offset to the heatmap-based point location method. Based on the keypoints detection, we propose a symbol grouping method to redraw the rectangle symbols in CAD images. We have released a dataset containing CAD images of equipment rooms from telecommunication industrial CAD drawings. Extensive experiments on this real-world dataset show that the proposed method has good generalization ability.","sentences":["Parsing Computer-Aided Design (CAD) drawings is a fundamental step for CAD revision, semantic-based management, and the generation of 3D prototypes in both the architecture and engineering industries.","Labeling symbols from a CAD drawing is a challenging yet notorious task from a practical point of view.","In this work, we propose to label and spot symbols from CAD images that are converted from CAD drawings.","The advantage of spotting symbols from CAD images lies in the low requirement of labelers and the low-cost annotation.","However, pixel-wise spotting symbols from CAD images is challenging work.","We propose a pixel-wise point location via Progressive Gaussian Kernels (PGK) to balance between training efficiency and location accuracy.","Besides, we introduce a local offset to the heatmap-based point location method.","Based on the keypoints detection, we propose a symbol grouping method to redraw the rectangle symbols in CAD images.","We have released a dataset containing CAD images of equipment rooms from telecommunication industrial CAD drawings.","Extensive experiments on this real-world dataset show that the proposed method has good generalization ability."],"url":"http://arxiv.org/abs/2404.10985v1","category":"cs.CV"}
{"created":"2024-04-17 00:40:26","title":"Tinker or Transfer? A Tale of Two Techniques in Teaching Visualization","abstract":"In education there exists a tension between two modes of learning: traditional lecture-based instruction and more tinkering-based creative learning. In this paper, we outline our efforts as two Ph.D. students (who are skilled in visualization but are not, importantly, professionally trained visualization experts) to implement creative learning activities in an information visualization course in our home department. We describe our motivation for doing so, and how what began out of necessity turned into an endeavor whose utility we strongly believe in. In implementing these activities, we received largely positive reviews from students, along with constructive feedback which helped us iteratively improve the activities. Finally, we also detail our future plans for turning this work into a formal design inquiry with students to build a new class centered entirely around creative learning.","sentences":["In education there exists a tension between two modes of learning: traditional lecture-based instruction and more tinkering-based creative learning.","In this paper, we outline our efforts as two Ph.D. students (who are skilled in visualization but are not, importantly, professionally trained visualization experts) to implement creative learning activities in an information visualization course in our home department.","We describe our motivation for doing so, and how what began out of necessity turned into an endeavor whose utility we strongly believe in.","In implementing these activities, we received largely positive reviews from students, along with constructive feedback which helped us iteratively improve the activities.","Finally, we also detail our future plans for turning this work into a formal design inquiry with students to build a new class centered entirely around creative learning."],"url":"http://arxiv.org/abs/2404.10967v1","category":"cs.HC"}
{"created":"2024-04-16 23:47:23","title":"Personalized Federated Learning via Stacking","abstract":"Traditional Federated Learning (FL) methods typically train a single global model collaboratively without exchanging raw data. In contrast, Personalized Federated Learning (PFL) techniques aim to create multiple models that are better tailored to individual clients' data. We present a novel personalization approach based on stacked generalization where clients directly send each other privacy-preserving models to be used as base models to train a meta-model on private data. Our approach is flexible, accommodating various privacy-preserving techniques and model types, and can be applied in horizontal, hybrid, and vertically partitioned federations. Additionally, it offers a natural mechanism for assessing each client's contribution to the federation. Through comprehensive evaluations across diverse simulated data heterogeneity scenarios, we showcase the effectiveness of our method.","sentences":["Traditional Federated Learning (FL) methods typically train a single global model collaboratively without exchanging raw data.","In contrast, Personalized Federated Learning (PFL) techniques aim to create multiple models that are better tailored to individual clients' data.","We present a novel personalization approach based on stacked generalization where clients directly send each other privacy-preserving models to be used as base models to train a meta-model on private data.","Our approach is flexible, accommodating various privacy-preserving techniques and model types, and can be applied in horizontal, hybrid, and vertically partitioned federations.","Additionally, it offers a natural mechanism for assessing each client's contribution to the federation.","Through comprehensive evaluations across diverse simulated data heterogeneity scenarios, we showcase the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.10957v1","category":"cs.LG"}
{"created":"2024-04-16 23:05:17","title":"Residual Connections Harm Self-Supervised Abstract Feature Learning","abstract":"We demonstrate that adding a weighting factor to decay the strength of identity shortcuts within residual networks substantially improves semantic feature learning in the state-of-the-art self-supervised masked autoencoding (MAE) paradigm. Our modification to the identity shortcuts within a VIT-B/16 backbone of an MAE boosts linear probing accuracy on ImageNet from 67.3% to 72.3%. This significant gap suggests that, while residual connection structure serves an essential role in facilitating gradient propagation, it may have a harmful side effect of reducing capacity for abstract learning by virtue of injecting an echo of shallower representations into deeper layers. We ameliorate this downside via a fixed formula for monotonically decreasing the contribution of identity connections as layer depth increases. Our design promotes the gradual development of feature abstractions, without impacting network trainability. Analyzing the representations learned by our modified residual networks, we find correlation between low effective feature rank and downstream task performance.","sentences":["We demonstrate that adding a weighting factor to decay the strength of identity shortcuts within residual networks substantially improves semantic feature learning in the state-of-the-art self-supervised masked autoencoding (MAE) paradigm.","Our modification to the identity shortcuts within a VIT-B/16 backbone of an MAE boosts linear probing accuracy on ImageNet from 67.3% to 72.3%.","This significant gap suggests that, while residual connection structure serves an essential role in facilitating gradient propagation, it may have a harmful side effect of reducing capacity for abstract learning by virtue of injecting an echo of shallower representations into deeper layers.","We ameliorate this downside via a fixed formula for monotonically decreasing the contribution of identity connections as layer depth increases.","Our design promotes the gradual development of feature abstractions, without impacting network trainability.","Analyzing the representations learned by our modified residual networks, we find correlation between low effective feature rank and downstream task performance."],"url":"http://arxiv.org/abs/2404.10947v1","category":"cs.CV"}
{"created":"2024-04-16 22:27:23","title":"Beam Training in mmWave Vehicular Systems: Machine Learning for Decoupling Beam Selection","abstract":"Codebook-based beam selection is one approach for configuring millimeter wave communication links. The overhead required to reconfigure the transmit and receive beam pair, though, increases in highly dynamic vehicular communication systems. Location information coupled with machine learning (ML) beam recommendation is one way to reduce the overhead of beam pair selection. In this paper, we develop ML-based location-aided approaches to decouple the beam selection between the user equipment (UE) and the base station (BS). We quantify the performance gaps due to decoupling beam selection and also disaggregating the UE's location information from the BS. Our simulation results show that decoupling beam selection with available location information at the BS performs comparable to joint beam pair selection at the BS. Moreover, decoupled beam selection without location closely approaches the performance of beam pair selection at the BS when sufficient beam pairs are swept.","sentences":["Codebook-based beam selection is one approach for configuring millimeter wave communication links.","The overhead required to reconfigure the transmit and receive beam pair, though, increases in highly dynamic vehicular communication systems.","Location information coupled with machine learning (ML) beam recommendation is one way to reduce the overhead of beam pair selection.","In this paper, we develop ML-based location-aided approaches to decouple the beam selection between the user equipment (UE) and the base station (BS).","We quantify the performance gaps due to decoupling beam selection and also disaggregating the UE's location information from the BS.","Our simulation results show that decoupling beam selection with available location information at the BS performs comparable to joint beam pair selection at the BS.","Moreover, decoupled beam selection without location closely approaches the performance of beam pair selection at the BS when sufficient beam pairs are swept."],"url":"http://arxiv.org/abs/2404.10936v1","category":"eess.SP"}
{"created":"2024-04-16 21:57:58","title":"A Concise Tiling Strategy for Preserving Spatial Context in Earth Observation Imagery","abstract":"We propose a new tiling strategy, Flip-n-Slide, which has been developed for specific use with large Earth observation satellite images when the location of objects-of-interest (OoI) is unknown and spatial context can be necessary for class disambiguation. Flip-n-Slide is a concise and minimalistic approach that allows OoI to be represented at multiple tile positions and orientations. This strategy introduces multiple views of spatio-contextual information, without introducing redundancies into the training set. By maintaining distinct transformation permutations for each tile overlap, we enhance the generalizability of the training set without misrepresenting the true data distribution. Our experiments validate the effectiveness of Flip-n-Slide in the task of semantic segmentation, a necessary data product in geophysical studies. We find that Flip-n-Slide outperforms the previous state-of-the-art augmentation routines for tiled data in all evaluation metrics. For underrepresented classes, Flip-n-Slide increases precision by as much as 15.8%.","sentences":["We propose a new tiling strategy, Flip-n-Slide, which has been developed for specific use with large Earth observation satellite images when the location of objects-of-interest (OoI) is unknown and spatial context can be necessary for class disambiguation.","Flip-n-Slide is a concise and minimalistic approach that allows OoI to be represented at multiple tile positions and orientations.","This strategy introduces multiple views of spatio-contextual information, without introducing redundancies into the training set.","By maintaining distinct transformation permutations for each tile overlap, we enhance the generalizability of the training set without misrepresenting the true data distribution.","Our experiments validate the effectiveness of Flip-n-Slide in the task of semantic segmentation, a necessary data product in geophysical studies.","We find that Flip-n-Slide outperforms the previous state-of-the-art augmentation routines for tiled data in all evaluation metrics.","For underrepresented classes, Flip-n-Slide increases precision by as much as 15.8%."],"url":"http://arxiv.org/abs/2404.10927v1","category":"cs.CV"}
{"created":"2024-04-16 21:45:59","title":"Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training","abstract":"Recent advancements in language modeling have led to the emergence of Large Language Models (LLMs) capable of various natural language processing tasks. Despite their success in text-based tasks, applying LLMs to the speech domain remains limited and challenging. This paper presents BLOOMZMMS, a novel model that integrates a multilingual LLM with a multilingual speech encoder, aiming to harness the capabilities of LLMs for speech recognition and beyond. Utilizing a multi-instructional training approach, we demonstrate the transferability of linguistic knowledge from the text to the speech modality. Our experiments, conducted on 1900 hours of transcribed data from 139 languages, establish that a multilingual speech representation can be effectively learned and aligned with a multilingual LLM. While this learned representation initially shows limitations in task generalization, we address this issue by generating synthetic targets in a multi-instructional style. Our zero-shot evaluation results confirm the robustness of our approach across multiple tasks, including speech translation and multilingual spoken language understanding, thereby opening new avenues for applying LLMs in the speech domain.","sentences":["Recent advancements in language modeling have led to the emergence of Large Language Models (LLMs) capable of various natural language processing tasks.","Despite their success in text-based tasks, applying LLMs to the speech domain remains limited and challenging.","This paper presents BLOOMZMMS, a novel model that integrates a multilingual LLM with a multilingual speech encoder, aiming to harness the capabilities of LLMs for speech recognition and beyond.","Utilizing a multi-instructional training approach, we demonstrate the transferability of linguistic knowledge from the text to the speech modality.","Our experiments, conducted on 1900 hours of transcribed data from 139 languages, establish that a multilingual speech representation can be effectively learned and aligned with a multilingual LLM.","While this learned representation initially shows limitations in task generalization, we address this issue by generating synthetic targets in a multi-instructional style.","Our zero-shot evaluation results confirm the robustness of our approach across multiple tasks, including speech translation and multilingual spoken language understanding, thereby opening new avenues for applying LLMs in the speech domain."],"url":"http://arxiv.org/abs/2404.10922v1","category":"cs.CL"}
{"created":"2024-04-16 21:45:10","title":"Tao: Re-Thinking DL-based Microarchitecture Simulation","abstract":"Microarchitecture simulators are indispensable tools for microarchitecture designers to validate, estimate, and optimize new hardware that meets specific design requirements. While the quest for a fast, accurate and detailed microarchitecture simulation has been ongoing for decades, existing simulators excel and fall short at different aspects: (i) Although execution-driven simulation is accurate and detailed, it is extremely slow and requires expert-level experience to design. (ii) Trace-driven simulation reuses the execution traces in pursuit of fast simulation but faces accuracy concerns and fails to achieve significant speedup. (iii) Emerging deep learning (DL)-based simulations are remarkably fast and have acceptable accuracy but fail to provide adequate low-level microarchitectural performance metrics crucial for microarchitectural bottleneck analysis. Additionally, they introduce substantial overheads from trace regeneration and model re-training when simulating a new microarchitecture.   Re-thinking the advantages and limitations of the aforementioned simulation paradigms, this paper introduces TAO that redesigns the DL-based simulation with three primary contributions: First, we propose a new training dataset design such that the subsequent simulation only needs functional trace as inputs, which can be rapidly generated and reused across microarchitectures. Second, we redesign the input features and the DL model using self-attention to support predicting various performance metrics. Third, we propose techniques to train a microarchitecture agnostic embedding layer that enables fast transfer learning between different microarchitectural configurations and reduces the re-training overhead of conventional DL-based simulators. Our extensive evaluation shows {\\ours} can reduce the overall training and simulation time by 18.06x over the state-of-the-art DL-based endeavors.","sentences":["Microarchitecture simulators are indispensable tools for microarchitecture designers to validate, estimate, and optimize new hardware that meets specific design requirements.","While the quest for a fast, accurate and detailed microarchitecture simulation has been ongoing for decades, existing simulators excel and fall short at different aspects: (i)","Although execution-driven simulation is accurate and detailed, it is extremely slow and requires expert-level experience to design.","(ii) Trace-driven simulation reuses the execution traces in pursuit of fast simulation but faces accuracy concerns and fails to achieve significant speedup.","(iii) Emerging deep learning (DL)-based simulations are remarkably fast and have acceptable accuracy but fail to provide adequate low-level microarchitectural performance metrics crucial for microarchitectural bottleneck analysis.","Additionally, they introduce substantial overheads from trace regeneration and model re-training when simulating a new microarchitecture.   ","Re-thinking the advantages and limitations of the aforementioned simulation paradigms, this paper introduces TAO that redesigns the DL-based simulation with three primary contributions: First, we propose a new training dataset design such that the subsequent simulation only needs functional trace as inputs, which can be rapidly generated and reused across microarchitectures.","Second, we redesign the input features and the DL model using self-attention to support predicting various performance metrics.","Third, we propose techniques to train a microarchitecture agnostic embedding layer that enables fast transfer learning between different microarchitectural configurations and reduces the re-training overhead of conventional DL-based simulators.","Our extensive evaluation shows {\\ours} can reduce the overall training and simulation time by 18.06x over the state-of-the-art DL-based endeavors."],"url":"http://arxiv.org/abs/2404.10921v1","category":"cs.AR"}
{"created":"2024-04-16 21:16:17","title":"Bridging Theory to Practice in Software Testing Teaching through Team-based Learning (TBL) and Open Source Software (OSS) Contribution","abstract":"Curricula recommendation for undergraduate Software Engineering courses underscore the importance of transcending from traditional lecture format to actively involving students in time-limited, iterative development practices. This paper presents a teaching approach for a software testing course that integrates theory and practical experience through the utilization of both TBL and active contributions to OSS projects. The paper reports on our experience implementing the pedagogical approach over four consecutive semesters of a Software Testing course within an undergraduate Software Engineering program. The experience encompassed both online and in-person classes, involving a substantial cohort of over 300 students spanning four semesters. Students' perceptions regarding the course are analyzed and compared with previous, related studies. Our results are positively aligned with the existing literature of software engineering teaching, confirming the effectiveness of combining TBL with OSS contributions. Additionally, our survey has shed light on the challenges that students encounter during their first contribution to OSS projects, highlighting the need for targeted solutions. Overall, the experience demonstrates that the proposed pedagogical structure can effectively facilitate the transition from theoretical knowledge to real-world practice in the domain of Software Testing.","sentences":["Curricula recommendation for undergraduate Software Engineering courses underscore the importance of transcending from traditional lecture format to actively involving students in time-limited, iterative development practices.","This paper presents a teaching approach for a software testing course that integrates theory and practical experience through the utilization of both TBL and active contributions to OSS projects.","The paper reports on our experience implementing the pedagogical approach over four consecutive semesters of a Software Testing course within an undergraduate Software Engineering program.","The experience encompassed both online and in-person classes, involving a substantial cohort of over 300 students spanning four semesters.","Students' perceptions regarding the course are analyzed and compared with previous, related studies.","Our results are positively aligned with the existing literature of software engineering teaching, confirming the effectiveness of combining TBL with OSS contributions.","Additionally, our survey has shed light on the challenges that students encounter during their first contribution to OSS projects, highlighting the need for targeted solutions.","Overall, the experience demonstrates that the proposed pedagogical structure can effectively facilitate the transition from theoretical knowledge to real-world practice in the domain of Software Testing."],"url":"http://arxiv.org/abs/2404.10912v1","category":"cs.SE"}
{"created":"2024-04-16 20:51:36","title":"Multi-Task Multi-Modal Self-Supervised Learning for Facial Expression Recognition","abstract":"Human communication is multi-modal; e.g., face-to-face interaction involves auditory signals (speech) and visual signals (face movements and hand gestures). Hence, it is essential to exploit multiple modalities when designing machine learning-based facial expression recognition systems. In addition, given the ever-growing quantities of video data that capture human facial expressions, such systems should utilize raw unlabeled videos without requiring expensive annotations. Therefore, in this work, we employ a multitask multi-modal self-supervised learning method for facial expression recognition from in-the-wild video data. Our model combines three self-supervised objective functions: First, a multi-modal contrastive loss, that pulls diverse data modalities of the same video together in the representation space. Second, a multi-modal clustering loss that preserves the semantic structure of input data in the representation space. Finally, a multi-modal data reconstruction loss. We conduct a comprehensive study on this multimodal multi-task self-supervised learning method on three facial expression recognition benchmarks. To that end, we examine the performance of learning through different combinations of self-supervised tasks on the facial expression recognition downstream task. Our model ConCluGen outperforms several multi-modal self-supervised and fully supervised baselines on the CMU-MOSEI dataset. Our results generally show that multi-modal self-supervision tasks offer large performance gains for challenging tasks such as facial expression recognition, while also reducing the amount of manual annotations required. We release our pre-trained models as well as source code publicly","sentences":["Human communication is multi-modal; e.g., face-to-face interaction involves auditory signals (speech) and visual signals (face movements and hand gestures).","Hence, it is essential to exploit multiple modalities when designing machine learning-based facial expression recognition systems.","In addition, given the ever-growing quantities of video data that capture human facial expressions, such systems should utilize raw unlabeled videos without requiring expensive annotations.","Therefore, in this work, we employ a multitask multi-modal self-supervised learning method for facial expression recognition from in-the-wild video data.","Our model combines three self-supervised objective functions:","First, a multi-modal contrastive loss, that pulls diverse data modalities of the same video together in the representation space.","Second, a multi-modal clustering loss that preserves the semantic structure of input data in the representation space.","Finally, a multi-modal data reconstruction loss.","We conduct a comprehensive study on this multimodal multi-task self-supervised learning method on three facial expression recognition benchmarks.","To that end, we examine the performance of learning through different combinations of self-supervised tasks on the facial expression recognition downstream task.","Our model ConCluGen outperforms several multi-modal self-supervised and fully supervised baselines on the CMU-MOSEI dataset.","Our results generally show that multi-modal self-supervision tasks offer large performance gains for challenging tasks such as facial expression recognition, while also reducing the amount of manual annotations required.","We release our pre-trained models as well as source code publicly"],"url":"http://arxiv.org/abs/2404.10904v1","category":"cs.CV"}
{"created":"2024-04-16 20:48:25","title":"Superior Polymeric Gas Separation Membrane Designed by Explainable Graph Machine Learning","abstract":"Gas separation using polymer membranes promises to dramatically drive down the energy, carbon, and water intensity of traditional thermally driven separation, but developing the membrane materials is challenging. Here, we demonstrate a novel graph machine learning (ML) strategy to guide the experimental discovery of synthesizable polymer membranes with performances simultaneously exceeding the empirical upper bounds in multiple industrially important gas separation tasks. Two predicted candidates are synthesized and experimentally validated to perform beyond the upper bounds for multiple gas pairs (O2/N2, H2/CH4, and H2/N2). Notably, the O2/N2 separation selectivity is 1.6-6.7 times higher than existing polymer membranes. The molecular origin of the high performance is revealed by combining the inherent interpretability of our ML model, experimental characterization, and molecule-level simulation. Our study presents a unique explainable ML-experiment combination to tackle challenging energy material design problems in general, and the discovered polymers are beneficial for industrial gas separation.","sentences":["Gas separation using polymer membranes promises to dramatically drive down the energy, carbon, and water intensity of traditional thermally driven separation, but developing the membrane materials is challenging.","Here, we demonstrate a novel graph machine learning (ML) strategy to guide the experimental discovery of synthesizable polymer membranes with performances simultaneously exceeding the empirical upper bounds in multiple industrially important gas separation tasks.","Two predicted candidates are synthesized and experimentally validated to perform beyond the upper bounds for multiple gas pairs (O2/N2, H2/CH4, and H2/N2).","Notably, the O2/N2 separation selectivity is 1.6-6.7 times higher than existing polymer membranes.","The molecular origin of the high performance is revealed by combining the inherent interpretability of our ML model, experimental characterization, and molecule-level simulation.","Our study presents a unique explainable ML-experiment combination to tackle challenging energy material design problems in general, and the discovered polymers are beneficial for industrial gas separation."],"url":"http://arxiv.org/abs/2404.10903v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 16:39:30","title":"Reduction of dust radial drift by turbulence in protoplanetary disks","abstract":"Dust particles in protoplanetary disks, lacking support from pressure, rotate at velocities exceeding those of the surrounding gas. Consequently, they experience a head-wind from the gas that drives them toward the central star. Radial drift occurs on timescales much shorter than those inferred from disk observations or those required for dust to aggregate and form planets. Additionally, turbulence is often assumed to amplify the radial drift of dust in planet-forming disks when modeled through an effective viscous transport. However, the local interactions between turbulent eddies and particles are known to be significantly more intricate than in a viscous fluid. Our objective is to elucidate and characterize the dynamic effects of Keplerian turbulence on the mean radial and azimuthal velocities of dust particles. We employ 2D shearing-box incompressible simulations of the gas, which is maintained in a developed turbulent state while rotating at a sub-Keplerian speed. Dust is modeled as Lagrangian particles set at a Keplerian velocity, therefore experiencing a radial force toward the star through drag. Turbulent eddies are found to reduce the radial drift, while simultaneously enhancing the azimuthal velocities of small particles. This dynamic behavior arises from the modification of dust trajectories due to turbulent eddies.","sentences":["Dust particles in protoplanetary disks, lacking support from pressure, rotate at velocities exceeding those of the surrounding gas.","Consequently, they experience a head-wind from the gas that drives them toward the central star.","Radial drift occurs on timescales much shorter than those inferred from disk observations or those required for dust to aggregate and form planets.","Additionally, turbulence is often assumed to amplify the radial drift of dust in planet-forming disks when modeled through an effective viscous transport.","However, the local interactions between turbulent eddies and particles are known to be significantly more intricate than in a viscous fluid.","Our objective is to elucidate and characterize the dynamic effects of Keplerian turbulence on the mean radial and azimuthal velocities of dust particles.","We employ 2D shearing-box incompressible simulations of the gas, which is maintained in a developed turbulent state while rotating at a sub-Keplerian speed.","Dust is modeled as Lagrangian particles set at a Keplerian velocity, therefore experiencing a radial force toward the star through drag.","Turbulent eddies are found to reduce the radial drift, while simultaneously enhancing the azimuthal velocities of small particles.","This dynamic behavior arises from the modification of dust trajectories due to turbulent eddies."],"url":"http://arxiv.org/abs/2404.11544v1","category":"astro-ph.EP"}
{"created":"2024-04-17 08:38:57","title":"Relating bubble sort to birthday problem","abstract":"Birthday problem is a well-known classic problem in probability theory widely applied in cryptography. Although bubble sort is a popular algorithm leading to some interesting theoretical problems in computer science, its relation to birthday problem has not been found yet. This paper indicates how Rayleigh distribution naturally arises in bubble sort by relating it to birthday problem, which presents a novel direction for analysing bubble sort and birthday problem. Then asymptotic distributions and statistical characteristics of bubble sort and birthday problem with very small absolute errors are presented. Moreover, this paper proves that some common optimizations of bubble sort could lead to average performance degradation.","sentences":["Birthday problem is a well-known classic problem in probability theory widely applied in cryptography.","Although bubble sort is a popular algorithm leading to some interesting theoretical problems in computer science, its relation to birthday problem has not been found yet.","This paper indicates how Rayleigh distribution naturally arises in bubble sort by relating it to birthday problem, which presents a novel direction for analysing bubble sort and birthday problem.","Then asymptotic distributions and statistical characteristics of bubble sort and birthday problem with very small absolute errors are presented.","Moreover, this paper proves that some common optimizations of bubble sort could lead to average performance degradation."],"url":"http://arxiv.org/abs/2404.11170v1","category":"math.PR"}
{"created":"2024-04-17 03:13:58","title":"TaCOS: Task-Specific Camera Optimization with Simulation","abstract":"The performance of robots in their applications heavily depends on the quality of sensory input. However, designing sensor payloads and their parameters for specific robotic tasks is an expensive process that requires well-established sensor knowledge and extensive experiments with physical hardware. With cameras playing a pivotal role in robotic perception, we introduce a novel end-to-end optimization approach for co-designing a camera with specific robotic tasks by combining derivative-free and gradient-based optimizers. The proposed method leverages recent computer graphics techniques and physical camera characteristics to prototype the camera in software, simulate operational environments and tasks for robots, and optimize the camera design based on the desired tasks in a cost-effective way. We validate the accuracy of our camera simulation by comparing it with physical cameras, and demonstrate the design of cameras with stronger performance than common off-the-shelf alternatives. Our approach supports the optimization of both continuous and discrete camera parameters, manufacturing constraints, and can be generalized to a broad range of camera design scenarios including multiple cameras and unconventional cameras. This work advances the fully automated design of cameras for specific robotics tasks.","sentences":["The performance of robots in their applications heavily depends on the quality of sensory input.","However, designing sensor payloads and their parameters for specific robotic tasks is an expensive process that requires well-established sensor knowledge and extensive experiments with physical hardware.","With cameras playing a pivotal role in robotic perception, we introduce a novel end-to-end optimization approach for co-designing a camera with specific robotic tasks by combining derivative-free and gradient-based optimizers.","The proposed method leverages recent computer graphics techniques and physical camera characteristics to prototype the camera in software, simulate operational environments and tasks for robots, and optimize the camera design based on the desired tasks in a cost-effective way.","We validate the accuracy of our camera simulation by comparing it with physical cameras, and demonstrate the design of cameras with stronger performance than common off-the-shelf alternatives.","Our approach supports the optimization of both continuous and discrete camera parameters, manufacturing constraints, and can be generalized to a broad range of camera design scenarios including multiple cameras and unconventional cameras.","This work advances the fully automated design of cameras for specific robotics tasks."],"url":"http://arxiv.org/abs/2404.11031v1","category":"cs.CV"}
{"created":"2024-04-17 00:47:41","title":"Integrated Communication, Navigation, and Remote Sensing in LEO Networks with Vehicular Applications","abstract":"Traditionally, communication, navigation, and remote sensing (CNR) satellites are separately performed, leading to resource waste, information isolation, and independent optimization for each functionality. Taking future automated driving as an example, it faces great challenges in providing high-reliable and low-latency lane-level positioning, decimeter-level transportation observation, and huge traffic sensing information downloading. To this end, this article proposes an integrated CNR (ICNR) framework based on low earth orbit (LEO) satellite mega-constellations from the perspective of vehicular applications. After introducing the main working principles of the CNR functionalities to serve as the technological basis, we characterize the potentials of the integration gain in vehicular use cases. Then, we investigate the ICNR framework in different integration levels, which sheds strong light on qualitative performance improvement by sophisticatedly sharing orbit constellation, wireless resource, and data information towards meeting the requirements of vehicular applications. We also instantiate a fundamental numerical case study to demonstrate the integration gain and highlight the main tradeoffs in managing the ICNR networks from the perspective of vehicular applications.","sentences":["Traditionally, communication, navigation, and remote sensing (CNR) satellites are separately performed, leading to resource waste, information isolation, and independent optimization for each functionality.","Taking future automated driving as an example, it faces great challenges in providing high-reliable and low-latency lane-level positioning, decimeter-level transportation observation, and huge traffic sensing information downloading.","To this end, this article proposes an integrated CNR (ICNR) framework based on low earth orbit (LEO) satellite mega-constellations from the perspective of vehicular applications.","After introducing the main working principles of the CNR functionalities to serve as the technological basis, we characterize the potentials of the integration gain in vehicular use cases.","Then, we investigate the ICNR framework in different integration levels, which sheds strong light on qualitative performance improvement by sophisticatedly sharing orbit constellation, wireless resource, and data information towards meeting the requirements of vehicular applications.","We also instantiate a fundamental numerical case study to demonstrate the integration gain and highlight the main tradeoffs in managing the ICNR networks from the perspective of vehicular applications."],"url":"http://arxiv.org/abs/2404.10969v1","category":"cs.IT"}
{"created":"2024-04-16 23:54:55","title":"On approximability of the Permanent of PSD matrices","abstract":"We study the complexity of approximating the permanent of a positive semidefinite matrix $A\\in \\mathbb{C}^{n\\times n}$.   1. We design a new approximation algorithm for $\\mathrm{per}(A)$ with approximation ratio $e^{(0.9999 + \\gamma)n}$, exponentially improving upon the current best bound of $e^{(1+\\gamma-o(1))n}$ [AGOS17,YP22]. Here, $\\gamma \\approx 0.577$ is Euler's constant.   2. We prove that it is NP-hard to approximate $\\mathrm{per}(A)$ within a factor $e^{(\\gamma-\\epsilon)n}$ for any $\\epsilon>0$. This is the first exponential hardness of approximation for this problem. Along the way, we prove optimal hardness of approximation results for the $\\|\\cdot\\|_{2\\to q}$ ``norm'' problem of a matrix for all $-1 < q < 2$.","sentences":["We study the complexity of approximating the permanent of a positive semidefinite matrix $A\\in \\mathbb{C}^{n\\times n}$.   1.","We design a new approximation algorithm for $\\mathrm{per}(A)$ with approximation ratio $e^{(0.9999 + \\gamma)n}$, exponentially improving upon the current best bound of $e^{(1+\\gamma-o(1))n}$","[AGOS17,YP22].","Here, $\\gamma \\approx 0.577$ is Euler's constant.   ","2.","We prove that it is NP-hard to approximate $\\mathrm{per}(A)$ within a factor $e^{(\\gamma-\\epsilon)n}$ for any $\\epsilon>0$. This is the first exponential hardness of approximation for this problem.","Along the way, we prove optimal hardness of approximation results for the $\\|\\cdot\\|_{2\\to q}$ ``norm'' problem of a matrix for all $-1 < q < 2$."],"url":"http://arxiv.org/abs/2404.10959v1","category":"cs.DS"}
{"created":"2024-04-16 23:18:07","title":"Algorithms for Computing the Augustin--Csisz\u00e1r Mutual Information and Lapidoth--Pfister Mutual Information","abstract":"The Augustin--Csisz{\\' a}r mutual information (MI) and Lapidoth--Pfister MI are well-known generalizations of the Shannon MI, but do not have known closed-form expressions, so they need to be calculated by solving optimization problems. In this study, we propose alternating optimization algorithms for computing these types of MI and present proofs of their global convergence properties. We also provide a novel variational characterization of the Augustin--Csisz{\\' a}r MI that is similar to that of the Sibson MI.","sentences":["The Augustin--Csisz{\\' a}r mutual information (MI) and Lapidoth--Pfister MI are well-known generalizations of the Shannon MI, but do not have known closed-form expressions, so they need to be calculated by solving optimization problems.","In this study, we propose alternating optimization algorithms for computing these types of MI and present proofs of their global convergence properties.","We also provide a novel variational characterization of the Augustin--Csisz{\\' a}r MI that is similar to that of the Sibson MI."],"url":"http://arxiv.org/abs/2404.10950v1","category":"cs.IT"}
{"created":"2024-04-16 22:02:30","title":"A preconditioner for solving linear programming problems with dense columns","abstract":"The Interior-Point Methods are a class for solving linear programming problems that rely upon the solution of linear systems. At each iteration, it becomes important to determine how to solve these linear systems when the constraint matrix of the linear programming problem includes dense columns. In this paper, we propose a preconditioner to handle linear programming problems with dense columns, and we prove theoretically that the final linear system to solve is uniformly bounded when the Interior-Point Method is converging to an optimal solution. This result is illustrated through computational experiments, which show that our proposed method is robust and competitive in terms of running time and/or number of iterations compared with existing methods.","sentences":["The Interior-Point Methods are a class for solving linear programming problems that rely upon the solution of linear systems.","At each iteration, it becomes important to determine how to solve these linear systems when the constraint matrix of the linear programming problem includes dense columns.","In this paper, we propose a preconditioner to handle linear programming problems with dense columns, and we prove theoretically that the final linear system to solve is uniformly bounded when the Interior-Point Method is converging to an optimal solution.","This result is illustrated through computational experiments, which show that our proposed method is robust and competitive in terms of running time and/or number of iterations compared with existing methods."],"url":"http://arxiv.org/abs/2404.10930v1","category":"math.OC"}
{"created":"2024-04-16 21:58:43","title":"GPU-Based Parallel Computing Methods for Medical Photoacoustic Image Reconstruction","abstract":"Recent years have witnessed a rapid advancement in GPU technology, establishing it as a formidable high-performance parallel computing technology with superior floating-point computational capabilities compared to traditional CPUs. This paper explores the application of this technology in the field of photoacoustic imaging, an emerging non-destructive testing technique in biomedical engineering characterized by its high contrast, resolution, and penetration depth. We conduct a data parallelism analysis targeting the computationally intensive image reconstruction segment of photoacoustic imaging. By parallelizing the serial code for iterative reconstruction and optimizing memory access, we achieve significant improvements in processing speed. Our experiments compare the imaging speeds of vascular images reconstructed using CPUs and GPUs, with the results visualized using Matlab. The findings demonstrate that, while maintaining data accuracy, GPU parallel computing methods can markedly accelerate photoacoustic image reconstruction. This acceleration has the potential to facilitate the broader adoption of photoacoustic imaging in applications such as hemodynamic monitoring, clinical disease diagnosis, and drug development.","sentences":["Recent years have witnessed a rapid advancement in GPU technology, establishing it as a formidable high-performance parallel computing technology with superior floating-point computational capabilities compared to traditional CPUs.","This paper explores the application of this technology in the field of photoacoustic imaging, an emerging non-destructive testing technique in biomedical engineering characterized by its high contrast, resolution, and penetration depth.","We conduct a data parallelism analysis targeting the computationally intensive image reconstruction segment of photoacoustic imaging.","By parallelizing the serial code for iterative reconstruction and optimizing memory access, we achieve significant improvements in processing speed.","Our experiments compare the imaging speeds of vascular images reconstructed using CPUs and GPUs, with the results visualized using Matlab.","The findings demonstrate that, while maintaining data accuracy, GPU parallel computing methods can markedly accelerate photoacoustic image reconstruction.","This acceleration has the potential to facilitate the broader adoption of photoacoustic imaging in applications such as hemodynamic monitoring, clinical disease diagnosis, and drug development."],"url":"http://arxiv.org/abs/2404.10928v1","category":"cs.DC"}
{"created":"2024-04-16 19:29:27","title":"OSR-ViT: A Simple and Modular Framework for Open-Set Object Detection and Discovery","abstract":"An object detector's ability to detect and flag \\textit{novel} objects during open-world deployments is critical for many real-world applications. Unfortunately, much of the work in open object detection today is disjointed and fails to adequately address applications that prioritize unknown object recall \\textit{in addition to} known-class accuracy. To close this gap, we present a new task called Open-Set Object Detection and Discovery (OSODD) and as a solution propose the Open-Set Regions with ViT features (OSR-ViT) detection framework. OSR-ViT combines a class-agnostic proposal network with a powerful ViT-based classifier. Its modular design simplifies optimization and allows users to easily swap proposal solutions and feature extractors to best suit their application. Using our multifaceted evaluation protocol, we show that OSR-ViT obtains performance levels that far exceed state-of-the-art supervised methods. Our method also excels in low-data settings, outperforming supervised baselines using a fraction of the training data.","sentences":["An object detector's ability to detect and flag \\textit{novel} objects during open-world deployments is critical for many real-world applications.","Unfortunately, much of the work in open object detection today is disjointed and fails to adequately address applications that prioritize unknown object recall \\textit{in addition to} known-class accuracy.","To close this gap, we present a new task called Open-Set Object Detection and Discovery (OSODD) and as a solution propose the Open-Set Regions with ViT features (OSR-ViT) detection framework.","OSR-ViT combines a class-agnostic proposal network with a powerful ViT-based classifier.","Its modular design simplifies optimization","and allows users to easily swap proposal solutions and feature extractors to best suit their application.","Using our multifaceted evaluation protocol, we show that OSR-ViT obtains performance levels that far exceed state-of-the-art supervised methods.","Our method also excels in low-data settings, outperforming supervised baselines using a fraction of the training data."],"url":"http://arxiv.org/abs/2404.10865v1","category":"cs.CV"}
{"created":"2024-04-16 18:54:57","title":"Sample Complexity of the Linear Quadratic Regulator: A Reinforcement Learning Lens","abstract":"We provide the first known algorithm that provably achieves $\\varepsilon$-optimality within $\\widetilde{\\mathcal{O}}(1/\\varepsilon)$ function evaluations for the discounted discrete-time LQR problem with unknown parameters, without relying on two-point gradient estimates. These estimates are known to be unrealistic in many settings, as they depend on using the exact same initialization, which is to be selected randomly, for two different policies. Our results substantially improve upon the existing literature outside the realm of two-point gradient estimates, which either leads to $\\widetilde{\\mathcal{O}}(1/\\varepsilon^2)$ rates or heavily relies on stability assumptions.","sentences":["We provide the first known algorithm that provably achieves $\\varepsilon$-optimality within $\\widetilde{\\mathcal{O}}(1/\\varepsilon)$ function evaluations for the discounted discrete-time LQR problem with unknown parameters, without relying on two-point gradient estimates.","These estimates are known to be unrealistic in many settings, as they depend on using the exact same initialization, which is to be selected randomly, for two different policies.","Our results substantially improve upon the existing literature outside the realm of two-point gradient estimates, which either leads to $\\widetilde{\\mathcal{O}}(1/\\varepsilon^2)$ rates or heavily relies on stability assumptions."],"url":"http://arxiv.org/abs/2404.10851v1","category":"eess.SY"}
{"created":"2024-04-16 18:00:06","title":"A Framework to Formulate Pathfinding Problems for Quantum Computing","abstract":"With the applications of quantum computing becoming more and more widespread, finding ways that allow end users without experience in the field to apply quantum computers to solve their individual problems is becoming a crucial task. However, current optimization algorithms require problem instances to be posed in complex formats that are challenging to formulate, even for experts. In particular, the Quadratic Unconstrained Binary Optimization (QUBO) formalism employed by many quantum optimization algorithms, such as the Quantum Approximate Optimization Algorithm (QAOA), involves the mathematical rewriting of constraints under strict conditions. To facilitate this process, we propose a framework to automatically generate QUBO formulations for pathfinding problems. This framework allows users to translate their specific problem instances into formulations that can be passed directly to quantum algorithms for optimization without requiring any expertise in the field of quantum computing. It supports three different encoding schemes that can easily be compared without requiring manual reformulation efforts. The resulting QUBO formulations are robust and efficient, reducing the previously tedious and error-prone reformulation process to a task that can be completed in a matter of seconds. In addition to an open-source Python package available on https://github.com/cda-tum/mqt-qubomaker, we also provide a graphical user interface accessible through the web (https://cda-tum.github.io/mqt-qubomaker/), which can be used to operate the framework without requiring the end user to write any code.","sentences":["With the applications of quantum computing becoming more and more widespread, finding ways that allow end users without experience in the field to apply quantum computers to solve their individual problems is becoming a crucial task.","However, current optimization algorithms require problem instances to be posed in complex formats that are challenging to formulate, even for experts.","In particular, the Quadratic Unconstrained Binary Optimization (QUBO) formalism employed by many quantum optimization algorithms, such as the Quantum Approximate Optimization Algorithm (QAOA), involves the mathematical rewriting of constraints under strict conditions.","To facilitate this process, we propose a framework to automatically generate QUBO formulations for pathfinding problems.","This framework allows users to translate their specific problem instances into formulations that can be passed directly to quantum algorithms for optimization without requiring any expertise in the field of quantum computing.","It supports three different encoding schemes that can easily be compared without requiring manual reformulation efforts.","The resulting QUBO formulations are robust and efficient, reducing the previously tedious and error-prone reformulation process to a task that can be completed in a matter of seconds.","In addition to an open-source Python package available on https://github.com/cda-tum/mqt-qubomaker, we also provide a graphical user interface accessible through the web (https://cda-tum.github.io/mqt-qubomaker/), which can be used to operate the framework without requiring the end user to write any code."],"url":"http://arxiv.org/abs/2404.10820v1","category":"quant-ph"}
{"created":"2024-04-16 17:59:55","title":"Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback","abstract":"Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction. To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm namely robust contextual dueling bandit (\\algo), which is based on uncertainty-weighted maximum likelihood estimation. Our algorithm achieves an $\\tilde O(d\\sqrt{T}+dC)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, and $ 0 \\le C \\le T$ is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback. Additionally, we conduct experiments to evaluate our proposed algorithm against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.","sentences":["Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM).","However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction.","To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary.","We propose an algorithm namely robust contextual dueling bandit (\\algo), which is based on uncertainty-weighted maximum likelihood estimation.","Our algorithm achieves an $\\tilde O(d\\sqrt{T}+dC)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, and $ 0 \\le C \\le T$ is the total number of adversarial feedback.","We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback.","Additionally, we conduct experiments to evaluate our proposed algorithm against various types of adversarial feedback.","Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback."],"url":"http://arxiv.org/abs/2404.10776v1","category":"cs.LG"}
{"created":"2024-04-16 17:35:35","title":"Watch Your Step: Optimal Retrieval for Continual Learning at Scale","abstract":"One of the most widely used approaches in continual learning is referred to as replay. Replay methods support interleaved learning by storing past experiences in a replay buffer. Although there are methods for selectively constructing the buffer and reprocessing its contents, there is limited exploration of the problem of selectively retrieving samples from the buffer. Current solutions have been tested in limited settings and, more importantly, in isolation. Existing work has also not explored the impact of duplicate replays on performance. In this work, we propose a framework for evaluating selective retrieval strategies, categorized by simple, independent class- and sample-selective primitives. We evaluated several combinations of existing strategies for selective retrieval and present their performances. Furthermore, we propose a set of strategies to prevent duplicate replays and explore whether new samples with low loss values can be learned without replay. In an effort to match our problem setting to a realistic continual learning pipeline, we restrict our experiments to a setting involving a large, pre-trained, open vocabulary object detection model, which is fully fine-tuned on a sequence of 15 datasets.","sentences":["One of the most widely used approaches in continual learning is referred to as replay.","Replay methods support interleaved learning by storing past experiences in a replay buffer.","Although there are methods for selectively constructing the buffer and reprocessing its contents, there is limited exploration of the problem of selectively retrieving samples from the buffer.","Current solutions have been tested in limited settings and, more importantly, in isolation.","Existing work has also not explored the impact of duplicate replays on performance.","In this work, we propose a framework for evaluating selective retrieval strategies, categorized by simple, independent class- and sample-selective primitives.","We evaluated several combinations of existing strategies for selective retrieval and present their performances.","Furthermore, we propose a set of strategies to prevent duplicate replays and explore whether new samples with low loss values can be learned without replay.","In an effort to match our problem setting to a realistic continual learning pipeline, we restrict our experiments to a setting involving a large, pre-trained, open vocabulary object detection model, which is fully fine-tuned on a sequence of 15 datasets."],"url":"http://arxiv.org/abs/2404.10758v1","category":"cs.CV"}
{"created":"2024-04-16 17:35:25","title":"Deep Learning and LLM-based Methods Applied to Stellar Lightcurve Classification","abstract":"Light curves serve as a valuable source of information on stellar formation and evolution. With the rapid advancement of machine learning techniques, it can be effectively processed to extract astronomical patterns and information. In this study, we present a comprehensive evaluation of deep-learning and large language model (LLM) based models for the automatic classification of variable star light curves, based on large datasets from the Kepler and K2 missions. Special emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries, examining the influence of observational cadence and phase distribution on classification precision. Employing AutoDL optimization, we achieve striking performance with the 1D-Convolution+BiLSTM architecture and the Swin Transformer, hitting accuracies of 94\\% and 99\\% correspondingly, with the latter demonstrating a notable 83\\% accuracy in discerning the elusive Type II Cepheids-comprising merely 0.02\\% of the total dataset.We unveil StarWhisper LightCurve (LC), an innovative Series comprising three LLM-based models: LLM, multimodal large language model (MLLM), and Large Audio Language Model (LALM). Each model is fine-tuned with strategic prompt engineering and customized training methods to explore the emergent abilities of these models for astronomical data. Remarkably, StarWhisper LC Series exhibit high accuracies around 90\\%, significantly reducing the need for explicit feature engineering, thereby paving the way for streamlined parallel data processing and the progression of multifaceted multimodal models in astronomical applications. The study furnishes two detailed catalogs illustrating the impacts of phase and sampling intervals on deep learning classification accuracy, showing that a substantial decrease of up to 14\\% in observation duration and 21\\% in sampling points can be realized without compromising accuracy by more than 10\\%.","sentences":["Light curves serve as a valuable source of information on stellar formation and evolution.","With the rapid advancement of machine learning techniques, it can be effectively processed to extract astronomical patterns and information.","In this study, we present a comprehensive evaluation of deep-learning and large language model (LLM) based models for the automatic classification of variable star light curves, based on large datasets from the Kepler and K2 missions.","Special emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries, examining the influence of observational cadence and phase distribution on classification precision.","Employing AutoDL optimization, we achieve striking performance with the 1D-Convolution+BiLSTM architecture and the Swin Transformer, hitting accuracies of 94\\% and 99\\% correspondingly, with the latter demonstrating a notable 83\\% accuracy in discerning the elusive Type II Cepheids-comprising merely 0.02\\% of the total dataset.","We unveil StarWhisper LightCurve (LC), an innovative Series comprising three LLM-based models: LLM, multimodal large language model (MLLM), and Large Audio Language Model (LALM).","Each model is fine-tuned with strategic prompt engineering and customized training methods to explore the emergent abilities of these models for astronomical data.","Remarkably, StarWhisper LC Series exhibit high accuracies around 90\\%, significantly reducing the need for explicit feature engineering, thereby paving the way for streamlined parallel data processing and the progression of multifaceted multimodal models in astronomical applications.","The study furnishes two detailed catalogs illustrating the impacts of phase and sampling intervals on deep learning classification accuracy, showing that a substantial decrease of up to 14\\% in observation duration and 21\\% in sampling points can be realized without compromising accuracy by more than 10\\%."],"url":"http://arxiv.org/abs/2404.10757v1","category":"astro-ph.IM"}
{"created":"2024-04-16 17:19:49","title":"An angle rounding parameter initialization technique for ma-QAOA","abstract":"The multi-angle quantum approximate optimization algorithm (ma-QAOA) is a recently introduced algorithm that gives at least the same approximation ratio as the quantum approximate optimization algorithm (QAOA) and, in most cases, gives a significantly higher approximation ratio than QAOA. One drawback to ma-QAOA is that it uses significantly more classical parameters than QAOA, so the classical optimization component more complex. In this paper, we motivate a new parameter initialization strategy in which angles are initially randomly set to multiples of $\\pi/4$ between $-2\\pi$ and $2\\pi$ and this vector is used to seed one round of BFGS. We find that the parameter initialization strategy on four-vertex and eight-vertex data sets gives average approximation ratios of 0.931 and 0.894, respectively. This is comparable to the average approximation ratios of ma-QAOA where optimal parameters are found using BFGS with 1 random starting seed, which are 0.910 and 0.901 for the four-vertex and eight-vertex data sets.","sentences":["The multi-angle quantum approximate optimization algorithm (ma-QAOA) is a recently introduced algorithm that gives at least the same approximation ratio as the quantum approximate optimization algorithm (QAOA) and, in most cases, gives a significantly higher approximation ratio than QAOA.","One drawback to ma-QAOA is that it uses significantly more classical parameters than QAOA, so the classical optimization component more complex.","In this paper, we motivate a new parameter initialization strategy in which angles are initially randomly set to multiples of $\\pi/4$ between $-2\\pi$ and $2\\pi$ and this vector is used to seed one round of BFGS.","We find that the parameter initialization strategy on four-vertex and eight-vertex data sets gives average approximation ratios of 0.931 and 0.894, respectively.","This is comparable to the average approximation ratios of ma-QAOA where optimal parameters are found using BFGS with 1 random starting seed, which are 0.910 and 0.901 for the four-vertex and eight-vertex data sets."],"url":"http://arxiv.org/abs/2404.10743v1","category":"quant-ph"}
{"created":"2024-04-16 17:11:44","title":"Quantum Teleportation Coexisting with Conventional Classical Communications in Optical Fiber","abstract":"The ability for quantum and classical networks to operate in the same optical fibers would aid the deployment of quantum network technology. However, quantum performance can be susceptible to noise photons generated by spontaneous Raman scattering of high-power coexisting classical light. Quantum teleportation is a fundamental operation in quantum networking, but has yet to be demonstrated in fibers populated with high data rate conventional optical signals. In this paper, we demonstrate a three-node quantum state teleportation system coexisting with 400-Gbps C-band classical communications in 30.2 km of fiber. To protect quantum fidelity, Raman noise rates are suppressed using optimized O-band quantum channels and filtering in multiple degrees of freedom. Fidelity is shown to be well maintained with elevated classical powers as high as 18.7 dBm, which could support multiple classical channels with many terabits/s aggregate data rates. These results show the feasibility of advanced quantum and classical network applications operating within a unified fiber infrastructure.","sentences":["The ability for quantum and classical networks to operate in the same optical fibers would aid the deployment of quantum network technology.","However, quantum performance can be susceptible to noise photons generated by spontaneous Raman scattering of high-power coexisting classical light.","Quantum teleportation is a fundamental operation in quantum networking, but has yet to be demonstrated in fibers populated with high data rate conventional optical signals.","In this paper, we demonstrate a three-node quantum state teleportation system coexisting with 400-Gbps C-band classical communications in 30.2 km of fiber.","To protect quantum fidelity, Raman noise rates are suppressed using optimized O-band quantum channels and filtering in multiple degrees of freedom.","Fidelity is shown to be well maintained with elevated classical powers as high as 18.7 dBm, which could support multiple classical channels with many terabits/s aggregate data rates.","These results show the feasibility of advanced quantum and classical network applications operating within a unified fiber infrastructure."],"url":"http://arxiv.org/abs/2404.10738v1","category":"quant-ph"}
{"created":"2024-04-16 16:59:50","title":"Automatic re-calibration of quantum devices by reinforcement learning","abstract":"During their operation, due to shifts in environmental conditions, devices undergo various forms of detuning from their optimal settings. Typically, this is addressed through control loops, which monitor variables and the device performance, to maintain settings at their optimal values. Quantum devices are particularly challenging since their functionality relies on precisely tuning their parameters. At the same time, the detailed modeling of the environmental behavior is often computationally unaffordable, while a direct measure of the parameters defining the system state is costly and introduces extra noise in the mechanism. In this study, we investigate the application of reinforcement learning techniques to develop a model-free control loop for continuous recalibration of quantum device parameters. Furthermore, we explore the advantages of incorporating minimal environmental noise models. As an example, the application to numerical simulations of a Kennedy receiver-based long-distance quantum communication protocol is presented.","sentences":["During their operation, due to shifts in environmental conditions, devices undergo various forms of detuning from their optimal settings.","Typically, this is addressed through control loops, which monitor variables and the device performance, to maintain settings at their optimal values.","Quantum devices are particularly challenging since their functionality relies on precisely tuning their parameters.","At the same time, the detailed modeling of the environmental behavior is often computationally unaffordable, while a direct measure of the parameters defining the system state is costly and introduces extra noise in the mechanism.","In this study, we investigate the application of reinforcement learning techniques to develop a model-free control loop for continuous recalibration of quantum device parameters.","Furthermore, we explore the advantages of incorporating minimal environmental noise models.","As an example, the application to numerical simulations of a Kennedy receiver-based long-distance quantum communication protocol is presented."],"url":"http://arxiv.org/abs/2404.10726v1","category":"quant-ph"}
{"created":"2024-04-16 16:32:10","title":"Keeping the photon in the dark: Enabling full quantum dot control by chirped pulses and magnetic fields","abstract":"Because dark excitons in quantum dots are not directly optically accessible, so far they have not played a significant role in using quantum dots for photon generation. They possess significantly longer lifetimes than their brighter counterparts and hence offer enormous potential for photon storage or manipulation. In this work, we demonstrate an all-optical storage and retrieval of the spin-forbidden dark exciton in a quantum dot from the ground state employing chirped pulses and an in-plane magnetic field. Our experimental findings are in excellent agreement with theoretical predictions of the dynamics calculated using state-of-the-art product tensor methods. Our scheme enables an all-optical control of dark states without relying on any preceding decays. This opens up a new dimension for optimal quantum control and time-bin entangled photon pair generation from quantum dots.","sentences":["Because dark excitons in quantum dots are not directly optically accessible, so far they have not played a significant role in using quantum dots for photon generation.","They possess significantly longer lifetimes than their brighter counterparts and hence offer enormous potential for photon storage or manipulation.","In this work, we demonstrate an all-optical storage and retrieval of the spin-forbidden dark exciton in a quantum dot from the ground state employing chirped pulses and an in-plane magnetic field.","Our experimental findings are in excellent agreement with theoretical predictions of the dynamics calculated using state-of-the-art product tensor methods.","Our scheme enables an all-optical control of dark states without relying on any preceding decays.","This opens up a new dimension for optimal quantum control and time-bin entangled photon pair generation from quantum dots."],"url":"http://arxiv.org/abs/2404.10708v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-16 16:06:44","title":"Swarm-Based Trajectory Generation and Optimization for Stress-Aligned 3D Printing","abstract":"In this study, we present a novel swarm-based approach for generating optimized stress-aligned trajectories for 3D printing applications. The method utilizes swarming dynamics to simulate the motion of virtual agents along the stress produced in a loaded part. Agent trajectories are then used as print trajectories. With this approach, the complex global trajectory generation problem is subdivided into a set of sequential and computationally efficient quadratic programs. Through comprehensive evaluations in both simulation and experiments, we compare our method with state-of-the-art approaches. Our results highlight a remarkable improvement in computational efficiency, achieving a 115x faster computation speed than existing methods. This efficiency, combined with the possibility to tune the trajectories spacing to match the deposition process constraints, makes the potential integration of our approach into existing 3D printing processes seamless. Additionally, the open-hole tensile specimen produced on a conventional fused filament fabrication set-up with our algorithm achieve a notable ~10% improvement in specific modulus compared to existing trajectory optimization methods.","sentences":["In this study, we present a novel swarm-based approach for generating optimized stress-aligned trajectories for 3D printing applications.","The method utilizes swarming dynamics to simulate the motion of virtual agents along the stress produced in a loaded part.","Agent trajectories are then used as print trajectories.","With this approach, the complex global trajectory generation problem is subdivided into a set of sequential and computationally efficient quadratic programs.","Through comprehensive evaluations in both simulation and experiments, we compare our method with state-of-the-art approaches.","Our results highlight a remarkable improvement in computational efficiency, achieving a 115x faster computation speed than existing methods.","This efficiency, combined with the possibility to tune the trajectories spacing to match the deposition process constraints, makes the potential integration of our approach into existing 3D printing processes seamless.","Additionally, the open-hole tensile specimen produced on a conventional fused filament fabrication set-up with our algorithm achieve a notable ~10% improvement in specific modulus compared to existing trajectory optimization methods."],"url":"http://arxiv.org/abs/2404.10686v1","category":"math.OC"}
{"created":"2024-04-16 15:52:22","title":"Circuit-theoretic Joint Parameter-State Estimation -- Balancing Optimality and AC Feasibility","abstract":"AC State Estimation (ACSE) is widely recognized as a practical approach for determining the grid states in steady-state conditions. It serves as a fundamental analysis to ensure grid security and is a reference for market dispatch. As grid complexity increases with rapid electrification and decarbonization, there is a growing need for more accurate knowledge of the grid operating state. However, existing ACSE algorithms have technical gaps. Critically, current ACSE algorithms are susceptible to erroneous system parameters, which are assumed to be fixed in traditional approaches. In this paper, we build a novel circuit-theoretic joint parameter-state estimation algorithm to address this limitation. The innovative algorithm builds an analogous equivalent circuit of the grid with states and certain parameters unknown. It solves a circuit-constrained optimization to estimate the most likely grid states and parameters given a set of measurements. Further, it quantifies the goodness of the estimated output by formulating tight convex envelopes around the original non-convex problem to quantify the quality of estimates. We compare the various proposed approaches on systems with up to 2869 nodes while demonstrating a tradeoff between solution optimality and model fidelity.","sentences":["AC State Estimation (ACSE) is widely recognized as a practical approach for determining the grid states in steady-state conditions.","It serves as a fundamental analysis to ensure grid security and is a reference for market dispatch.","As grid complexity increases with rapid electrification and decarbonization, there is a growing need for more accurate knowledge of the grid operating state.","However, existing ACSE algorithms have technical gaps.","Critically, current ACSE algorithms are susceptible to erroneous system parameters, which are assumed to be fixed in traditional approaches.","In this paper, we build a novel circuit-theoretic joint parameter-state estimation algorithm to address this limitation.","The innovative algorithm builds an analogous equivalent circuit of the grid with states and certain parameters unknown.","It solves a circuit-constrained optimization to estimate the most likely grid states and parameters given a set of measurements.","Further, it quantifies the goodness of the estimated output by formulating tight convex envelopes around the original non-convex problem to quantify the quality of estimates.","We compare the various proposed approaches on systems with up to 2869 nodes while demonstrating a tradeoff between solution optimality and model fidelity."],"url":"http://arxiv.org/abs/2404.10676v1","category":"eess.SY"}
{"created":"2024-04-16 15:15:59","title":"A Cloud Resources Portfolio Optimization Business Model - From Theory to Practice","abstract":"Cloud resources have become increasingly important, with many businesses using cloud solutions to supplement or outright replace their existing IT infrastructure. However, as there is a plethora of providers with varying products, services, and markets, it has become increasingly more challenging to keep track of the best solutions for each application. Cloud service intermediaries aim to alleviate this problem by offering services that help users meet their requirements.   This paper aims to lay the groundwork for developing a cloud portfolio management platform and its business model, defined via a business model canvas. Furthermore, a prototype of a platform is developed offering a cloud portfolio optimization service, using two algorithms developed in previous research to create suitable and well-utilized allocations for a customer's applications.","sentences":["Cloud resources have become increasingly important, with many businesses using cloud solutions to supplement or outright replace their existing IT infrastructure.","However, as there is a plethora of providers with varying products, services, and markets, it has become increasingly more challenging to keep track of the best solutions for each application.","Cloud service intermediaries aim to alleviate this problem by offering services that help users meet their requirements.   ","This paper aims to lay the groundwork for developing a cloud portfolio management platform and its business model, defined via a business model canvas.","Furthermore, a prototype of a platform is developed offering a cloud portfolio optimization service, using two algorithms developed in previous research to create suitable and well-utilized allocations for a customer's applications."],"url":"http://arxiv.org/abs/2404.10641v1","category":"cs.DC"}
{"created":"2024-04-16 15:11:08","title":"A Fast 3-Approximation for the Capacitated Tree Cover Problem with Edge Loads","abstract":"The capacitated tree cover problem with edge loads is a variant of the tree cover problem, where we are given facility opening costs, edge costs and loads, as well as vertex loads. We try to find a tree cover of minimum cost such that the total edge and vertex load of each tree does not exceed a given bound. We present an $\\mathcal{O}(m\\log n)$ time 3-approximation algorithm for this problem.   This is achieved by starting with a certain LP formulation. We give a combinatorial algorithm that solves the LP optimally in time $\\mathcal{O}(m\\log n)$. Then, we show that a linear time rounding and splitting technique leads to an integral solution that costs at most 3 times as much as the LP solution. Finally, we prove that the integrality gap of the LP is $3$, which shows that we can not improve the rounding step in general.","sentences":["The capacitated tree cover problem with edge loads is a variant of the tree cover problem, where we are given facility opening costs, edge costs and loads, as well as vertex loads.","We try to find a tree cover of minimum cost such that the total edge and vertex load of each tree does not exceed a given bound.","We present an $\\mathcal{O}(m\\log n)$ time 3-approximation algorithm for this problem.   ","This is achieved by starting with a certain LP formulation.","We give a combinatorial algorithm that solves the LP optimally in time $\\mathcal{O}(m\\log n)$. Then, we show that a linear time rounding and splitting technique leads to an integral solution that costs at most 3 times as much as the LP solution.","Finally, we prove that the integrality gap of the LP is $3$, which shows that we can not improve the rounding step in general."],"url":"http://arxiv.org/abs/2404.10638v1","category":"cs.DS"}
{"created":"2024-04-16 14:32:47","title":"UAV Trajectory Optimization for Sensing Exploiting Target Location Distribution Map","abstract":"In this paper, we study the trajectory optimization of a cellular-connected unmanned aerial vehicle (UAV) which aims to sense the location of a target while maintaining satisfactory communication quality with the ground base stations (GBSs). In contrast to most existing works which assumed the target's location is known, we focus on a more challenging scenario where the exact location of the target to be sensed is unknown and random, while its distribution is known a priori and stored in a novel target location distribution map. Based on this map, the probability for the UAV to successfully sense the target can be expressed as a function of the UAV's trajectory. We aim to optimize the UAV's trajectory between two pre-determined locations to maximize the overall sensing probability during its flight, subject to a GBS-UAV communication quality constraint at each time instant and a maximum mission completion time constraint. Despite the non-convexity and NP-hardness of this problem, we devise three high-quality suboptimal solutions tailored for it with polynomial complexity. Numerical results show that our proposed designs outperform various benchmark schemes.","sentences":["In this paper, we study the trajectory optimization of a cellular-connected unmanned aerial vehicle (UAV) which aims to sense the location of a target while maintaining satisfactory communication quality with the ground base stations (GBSs).","In contrast to most existing works which assumed the target's location is known, we focus on a more challenging scenario where the exact location of the target to be sensed is unknown and random, while its distribution is known a priori and stored in a novel target location distribution map.","Based on this map, the probability for the UAV to successfully sense the target can be expressed as a function of the UAV's trajectory.","We aim to optimize the UAV's trajectory between two pre-determined locations to maximize the overall sensing probability during its flight, subject to a GBS-UAV communication quality constraint at each time instant and a maximum mission completion time constraint.","Despite the non-convexity and NP-hardness of this problem, we devise three high-quality suboptimal solutions tailored for it with polynomial complexity.","Numerical results show that our proposed designs outperform various benchmark schemes."],"url":"http://arxiv.org/abs/2404.10605v1","category":"cs.IT"}
{"created":"2024-04-16 14:28:57","title":"Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences","abstract":"Leveraging multi-view diffusion models as priors for 3D optimization have alleviated the problem of 3D consistency, e.g., the Janus face problem or the content drift problem, in zero-shot text-to-3D models. However, the 3D geometric fidelity of the output remains an unresolved issue; albeit the rendered 2D views are realistic, the underlying geometry may contain errors such as unreasonable concavities. In this work, we propose CorrespondentDream, an effective method to leverage annotation-free, cross-view correspondences yielded from the diffusion U-Net to provide additional 3D prior to the NeRF optimization process. We find that these correspondences are strongly consistent with human perception, and by adopting it in our loss design, we are able to produce NeRF models with geometries that are more coherent with common sense, e.g., more smoothed object surface, yielding higher 3D fidelity. We demonstrate the efficacy of our approach through various comparative qualitative results and a solid user study.","sentences":["Leveraging multi-view diffusion models as priors for 3D optimization have alleviated the problem of 3D consistency, e.g., the Janus face problem or the content drift problem, in zero-shot text-to-3D models.","However, the 3D geometric fidelity of the output remains an unresolved issue; albeit the rendered 2D views are realistic, the underlying geometry may contain errors such as unreasonable concavities.","In this work, we propose CorrespondentDream, an effective method to leverage annotation-free, cross-view correspondences yielded from the diffusion U-Net to provide additional 3D prior to the NeRF optimization process.","We find that these correspondences are strongly consistent with human perception, and by adopting it in our loss design, we are able to produce NeRF models with geometries that are more coherent with common sense, e.g., more smoothed object surface, yielding higher 3D fidelity.","We demonstrate the efficacy of our approach through various comparative qualitative results and a solid user study."],"url":"http://arxiv.org/abs/2404.10603v1","category":"cs.CV"}
{"created":"2024-04-16 14:23:19","title":"Resilient-By-Design Framework for MIMO-OFDM Communications under Smart Jamming","abstract":"Native jamming mitigation is essential for addressing security and resilience in future 6G wireless networks. In this paper a resilient-by-design framework for effective anti-jamming in MIMO-OFDM wireless communications is introduced. A novel approach that integrates information from wireless sensing services to develop anti-jamming strategies, which do not rely on any prior information or assumptions on the adversary's concrete setup, is explored. To this end, a method that replaces conventional approaches to noise covariance estimation in anti-jamming with a surrogate covariance model is proposed, which instead incorporates sensing information on the jamming signal's directions-of-arrival (DoAs) to provide an effective approximation of the true jamming strategy. The study further focuses on integrating this novel, sensing-assisted approach into the joint optimization of beamforming, user scheduling and power allocation for a multi-user MIMO-OFDM uplink setting. Despite the NP-hard nature of this optimization problem, it can be effectively solved using an iterative water-filling approach. In order to assess the effectiveness of the proposed sensing-assisted jamming mitigation, the corresponding worst-case jamming strategy is investigated, which aims to minimize the total user sum-rate. Experimental simulations eventually affirm the robustness of our approach against both worst-case and barrage jamming, demonstrating its potential to address a wide range of jamming scenarios. Since such an integration of sensing-assisted information is directly implemented on the physical layer, resilience is incorporated preemptively by-design.","sentences":["Native jamming mitigation is essential for addressing security and resilience in future 6G wireless networks.","In this paper a resilient-by-design framework for effective anti-jamming in MIMO-OFDM wireless communications is introduced.","A novel approach that integrates information from wireless sensing services to develop anti-jamming strategies, which do not rely on any prior information or assumptions on the adversary's concrete setup, is explored.","To this end, a method that replaces conventional approaches to noise covariance estimation in anti-jamming with a surrogate covariance model is proposed, which instead incorporates sensing information on the jamming signal's directions-of-arrival (DoAs) to provide an effective approximation of the true jamming strategy.","The study further focuses on integrating this novel, sensing-assisted approach into the joint optimization of beamforming, user scheduling and power allocation for a multi-user MIMO-OFDM uplink setting.","Despite the NP-hard nature of this optimization problem, it can be effectively solved using an iterative water-filling approach.","In order to assess the effectiveness of the proposed sensing-assisted jamming mitigation, the corresponding worst-case jamming strategy is investigated, which aims to minimize the total user sum-rate.","Experimental simulations eventually affirm the robustness of our approach against both worst-case and barrage jamming, demonstrating its potential to address a wide range of jamming scenarios.","Since such an integration of sensing-assisted information is directly implemented on the physical layer, resilience is incorporated preemptively by-design."],"url":"http://arxiv.org/abs/2404.10598v1","category":"cs.IT"}
{"created":"2024-04-16 14:05:29","title":"Data-driven subgrouping of patient trajectories with chronic diseases: Evidence from low back pain","abstract":"Clinical data informs the personalization of health care with a potential for more effective disease management. In practice, this is achieved by subgrouping, whereby clusters with similar patient characteristics are identified and then receive customized treatment plans with the goal of targeting subgroup-specific disease dynamics. In this paper, we propose a novel mixture hidden Markov model for subgrouping patient trajectories from chronic diseases. Our model is probabilistic and carefully designed to capture different trajectory phases of chronic diseases (i.e., \"severe\", \"moderate\", and \"mild\") through tailored latent states. We demonstrate our subgrouping framework based on a longitudinal study across 847 patients with non-specific low back pain. Here, our subgrouping framework identifies 8 subgroups. Further, we show that our subgrouping framework outperforms common baselines in terms of cluster validity indices. Finally, we discuss the applicability of the model to other chronic and long-lasting diseases.","sentences":["Clinical data informs the personalization of health care with a potential for more effective disease management.","In practice, this is achieved by subgrouping, whereby clusters with similar patient characteristics are identified and then receive customized treatment plans with the goal of targeting subgroup-specific disease dynamics.","In this paper, we propose a novel mixture hidden Markov model for subgrouping patient trajectories from chronic diseases.","Our model is probabilistic and carefully designed to capture different trajectory phases of chronic diseases (i.e., \"severe\", \"moderate\", and \"mild\") through tailored latent states.","We demonstrate our subgrouping framework based on a longitudinal study across 847 patients with non-specific low back pain.","Here, our subgrouping framework identifies 8 subgroups.","Further, we show that our subgrouping framework outperforms common baselines in terms of cluster validity indices.","Finally, we discuss the applicability of the model to other chronic and long-lasting diseases."],"url":"http://arxiv.org/abs/2404.10580v1","category":"stat.AP"}
{"created":"2024-04-16 13:45:37","title":"Tropical toric maximum likelihood estimation","abstract":"We consider toric maximum likelihood estimation over the field of Puiseux series and study critical points of the likelihood function using tropical methods. This problem translates to finding the intersection points of a tropical affine space with a classical linear subspace. We derive new structural results on tropical affine spaces and use these to give a complete and explicit description of the tropical critical points in certain cases. In these cases, we associate tropical critical points to the simplices in a regular triangulation of the polytope giving rise to the toric model.","sentences":["We consider toric maximum likelihood estimation over the field of Puiseux series and study critical points of the likelihood function using tropical methods.","This problem translates to finding the intersection points of a tropical affine space with a classical linear subspace.","We derive new structural results on tropical affine spaces and use these to give a complete and explicit description of the tropical critical points in certain cases.","In these cases, we associate tropical critical points to the simplices in a regular triangulation of the polytope giving rise to the toric model."],"url":"http://arxiv.org/abs/2404.10567v1","category":"math.AG"}
{"created":"2024-04-16 13:33:12","title":"Nonlinear kernel-free quadratic hyper-surface support vector machine with 0-1 loss function","abstract":"For the binary classification problem, a novel nonlinear kernel-free quadratic hyper-surface support vector machine with 0-1 loss function (QSSVM$_{0/1}$) is proposed. Specifically, the task of QSSVM$_{0/1}$ is to seek a quadratic separating hyper-surface to divide the samples into two categories. And it has better interpretability than the methods using kernel functions, since each feature of the sample acts both independently and synergistically. By introducing the 0-1 loss function to construct the optimization model makes the model obtain strong sample sparsity. The proximal stationary point of the optimization problem is defined by the proximal operator of the 0-1 loss function, which figures out the problem of non-convex discontinuity of the optimization problem due to the 0-1 loss function. A new iterative algorithm based on the alternating direction method of multipliers (ADMM) framework is designed to solve the optimization problem, which relates to the working set defined by support vectors. The computational complexity and convergence of the algorithm are discussed. Numerical experiments on 4 artificial datasets and 14 benchmark datasets demonstrate that our QSSVM$_{0/1}$ achieves higher classification accuracy, fewer support vectors and less CPU time cost than other state-of-the-art methods.","sentences":["For the binary classification problem, a novel nonlinear kernel-free quadratic hyper-surface support vector machine with 0-1 loss function (QSSVM$_{0/1}$) is proposed.","Specifically, the task of QSSVM$_{0/1}$ is to seek a quadratic separating hyper-surface to divide the samples into two categories.","And it has better interpretability than the methods using kernel functions, since each feature of the sample acts both independently and synergistically.","By introducing the 0-1 loss function to construct the optimization model makes the model obtain strong sample sparsity.","The proximal stationary point of the optimization problem is defined by the proximal operator of the 0-1 loss function, which figures out the problem of non-convex discontinuity of the optimization problem due to the 0-1 loss function.","A new iterative algorithm based on the alternating direction method of multipliers (ADMM) framework is designed to solve the optimization problem, which relates to the working set defined by support vectors.","The computational complexity and convergence of the algorithm are discussed.","Numerical experiments on 4 artificial datasets and 14 benchmark datasets demonstrate that our QSSVM$_{0/1}$ achieves higher classification accuracy, fewer support vectors and less CPU time cost than other state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.10559v1","category":"math.OC"}
{"created":"2024-04-16 13:32:25","title":"Decade-Bandwidth RF-Input Pseudo-Doherty Load Modulated Balanced Amplifier using Signal-Flow-Based Phase Alignment Design","abstract":"This paper reports a first-ever decade-bandwidth pseudo-Doherty load-modulated balanced amplifier (PD-LMBA), designed for emerging 4G/5G communications and multi-band operations. By revisiting the LMBA theory using the signal-flow graph, a frequency-agnostic phase-alignment condition is found that is critical for ensuring intrinsically broadband load modulation behavior. This unique design methodology enables, for the first time, the independent optimization of broadband balanced amplifier (BA, as the peaking) and control amplifier (CA, as the carrier), thus fundamentally addressing the longstanding limits imposed on the design of wideband load-modulated power amplifiers (PAs). To prove the proposed concept, an ultra-wideband RF-input PD-LMBA is designed and developed using GaN technology covering the frequency range from 0.2 to 2 GHz. Experimental results demonstrate an efficiency of 51% to 72% for peak output power and 44% to 62% for 10-dB OBO, respectively.","sentences":["This paper reports a first-ever decade-bandwidth pseudo-Doherty load-modulated balanced amplifier (PD-LMBA), designed for emerging 4G/5G communications and multi-band operations.","By revisiting the LMBA theory using the signal-flow graph, a frequency-agnostic phase-alignment condition is found that is critical for ensuring intrinsically broadband load modulation behavior.","This unique design methodology enables, for the first time, the independent optimization of broadband balanced amplifier (BA, as the peaking) and control amplifier (CA, as the carrier), thus fundamentally addressing the longstanding limits imposed on the design of wideband load-modulated power amplifiers (PAs).","To prove the proposed concept, an ultra-wideband RF-input PD-LMBA is designed and developed using GaN technology covering the frequency range from 0.2 to 2 GHz.","Experimental results demonstrate an efficiency of 51% to 72% for peak output power and 44% to 62% for 10-dB OBO, respectively."],"url":"http://arxiv.org/abs/2404.10558v1","category":"cs.AR"}
{"created":"2024-04-16 13:09:48","title":"MPCOM: Robotic Data Gathering with Radio Mapping and Model Predictive Communication","abstract":"Robotic data gathering (RDG) is an emerging paradigm that navigates a robot to harvest data from remote sensors. However, motion planning in this paradigm needs to maximize the RDG efficiency instead of the navigation efficiency, for which the existing motion planning methods become inefficient, as they plan robot trajectories merely according to motion factors. This paper proposes radio map guided model predictive communication (MPCOM), which navigates the robot with both grid and radio maps for shape-aware collision avoidance and communication-aware trajectory generation in a dynamic environment. The proposed MPCOM is able to trade off the time spent on reaching goal, avoiding collision, and improving communication. MPCOM captures high-order signal propagation characteristics using radio maps and incorporates the map-guided communication regularizer to the motion planning block. Experiments in IRSIM and CARLA simulators show that the proposed MPCOM outperforms other benchmarks in both LOS and NLOS cases. Real-world testing based on car-like robots is also provided to demonstrate the effectiveness of MPCOM in indoor environments.","sentences":["Robotic data gathering (RDG) is an emerging paradigm that navigates a robot to harvest data from remote sensors.","However, motion planning in this paradigm needs to maximize the RDG efficiency instead of the navigation efficiency, for which the existing motion planning methods become inefficient, as they plan robot trajectories merely according to motion factors.","This paper proposes radio map guided model predictive communication (MPCOM), which navigates the robot with both grid and radio maps for shape-aware collision avoidance and communication-aware trajectory generation in a dynamic environment.","The proposed MPCOM is able to trade off the time spent on reaching goal, avoiding collision, and improving communication.","MPCOM captures high-order signal propagation characteristics using radio maps and incorporates the map-guided communication regularizer to the motion planning block.","Experiments in IRSIM and CARLA simulators show that the proposed MPCOM outperforms other benchmarks in both LOS and NLOS cases.","Real-world testing based on car-like robots is also provided to demonstrate the effectiveness of MPCOM in indoor environments."],"url":"http://arxiv.org/abs/2404.10541v1","category":"cs.RO"}
{"created":"2024-04-16 12:53:22","title":"Breaking of Time Translation Symmetry and Ergodicity, and Entropy decrease in a Continuous Time Crystal Driven by Nonreciprocal Optical Forces","abstract":"Nonreciprocal nonequilibrium process are attracting growing interest in sociology, animal behaviour, chemistry, and nanotechnology, and may have played a role in the origin of life. It is less widely recognized, however, that in open systems light can induce nonreciprocal predator-prey like forces between nanoparticles. Such forces provide access to the continuous time crystal state of matter, which has been demonstrated in a plasmonic metamaterial array of nanowires wherein light triggers a spontaneous mobilization transition to the robust oscillatory state, breaking time translation symmetry. Here, we report on the first experimental study of the transient dynamics of light-induced mobilization and demobilization in a time crystal. By analysing time resolved phase trajectories of the system of nanowires, we show that the mobilization transition is accompanied by breaking of continuous time translation symmetry and ergodicity, and a decrease in the entropy of motion. This insight into the transient dynamics of a nonreciprocity-driven time crystal is relevant to optical timetronics, an information and communications technology paradigm relying on the unique functionalities of time crystals, and applications of the interacting nanowire oscillator platform to modelling a wide range of nonreciprocal processes from many-body dynamics to the early stages of matter-to-life transitions.","sentences":["Nonreciprocal nonequilibrium process are attracting growing interest in sociology, animal behaviour, chemistry, and nanotechnology, and may have played a role in the origin of life.","It is less widely recognized, however, that in open systems light can induce nonreciprocal predator-prey like forces between nanoparticles.","Such forces provide access to the continuous time crystal state of matter, which has been demonstrated in a plasmonic metamaterial array of nanowires wherein light triggers a spontaneous mobilization transition to the robust oscillatory state, breaking time translation symmetry.","Here, we report on the first experimental study of the transient dynamics of light-induced mobilization and demobilization in a time crystal.","By analysing time resolved phase trajectories of the system of nanowires, we show that the mobilization transition is accompanied by breaking of continuous time translation symmetry and ergodicity, and a decrease in the entropy of motion.","This insight into the transient dynamics of a nonreciprocity-driven time crystal is relevant to optical timetronics, an information and communications technology paradigm relying on the unique functionalities of time crystals, and applications of the interacting nanowire oscillator platform to modelling a wide range of nonreciprocal processes from many-body dynamics to the early stages of matter-to-life transitions."],"url":"http://arxiv.org/abs/2404.10525v1","category":"physics.optics"}
{"created":"2024-04-16 12:44:02","title":"A Game-Theoretic Approach for PMU Deployment Against False Data Injection Attacks","abstract":"Phasor Measurement Units (PMUs) are used in the measurement, control and protection of power grids. However, deploying PMUs at every bus in a power system is prohibitively expensive, necessitating partial PMU placement that can ensure system observability with minimal units. One consequence of this economic approach is increased system vulnerability to False Data Injection Attacks (FDIAs). This paper proposes a zero-sum game-based approach to strategically place an additional PMU (following the initial optimal PMU deployment that ensures full observability) to bolster robustness against FDIAs by introducing redundancy in attack-susceptible areas. To compute the Nash equilibrium (NE) solution, we leverage a reinforcement learning algorithm that mitigates the need for complete knowledge of the opponent's actions. The proposed PMU deployment algorithm increases the detection rate of FDIA by 36% compared to benchmark algorithms.","sentences":["Phasor Measurement Units (PMUs) are used in the measurement, control and protection of power grids.","However, deploying PMUs at every bus in a power system is prohibitively expensive, necessitating partial PMU placement that can ensure system observability with minimal units.","One consequence of this economic approach is increased system vulnerability to False Data Injection Attacks (FDIAs).","This paper proposes a zero-sum game-based approach to strategically place an additional PMU (following the initial optimal PMU deployment that ensures full observability) to bolster robustness against FDIAs by introducing redundancy in attack-susceptible areas.","To compute the Nash equilibrium (NE) solution, we leverage a reinforcement learning algorithm that mitigates the need for complete knowledge of the opponent's actions.","The proposed PMU deployment algorithm increases the detection rate of FDIA by 36% compared to benchmark algorithms."],"url":"http://arxiv.org/abs/2404.10520v1","category":"eess.SY"}
{"created":"2024-04-16 12:38:03","title":"An Enhanced Differential Grouping Method for Large-Scale Overlapping Problems","abstract":"Large-scale overlapping problems are prevalent in practical engineering applications, and the optimization challenge is significantly amplified due to the existence of shared variables. Decomposition-based cooperative coevolution (CC) algorithms have demonstrated promising performance in addressing large-scale overlapping problems. However, current CC frameworks designed for overlapping problems rely on grouping methods for the identification of overlapping problem structures and the current grouping methods for large-scale overlapping problems fail to consider both accuracy and efficiency simultaneously. In this article, we propose a two-stage enhanced grouping method for large-scale overlapping problems, called OEDG, which achieves accurate grouping while significantly reducing computational resource consumption. In the first stage, OEDG employs a grouping method based on the finite differences principle to identify all subcomponents and shared variables. In the second stage, we propose two grouping refinement methods, called subcomponent union detection (SUD) and subcomponent detection (SD), to enhance and refine the grouping results. SUD examines the information of the subcomponents and shared variables obtained in the previous stage, and SD corrects inaccurate grouping results. To better verify the performance of the proposed OEDG, we propose a series of novel benchmarks that consider various properties of large-scale overlapping problems, including the topology structure, overlapping degree, and separability. Extensive experimental results demonstrate that OEDG is capable of accurately grouping different types of large-scale overlapping problems while consuming fewer computational resources. Finally, we empirically verify that the proposed OEDG can effectively improve the optimization performance of diverse large-scale overlapping problems.","sentences":["Large-scale overlapping problems are prevalent in practical engineering applications, and the optimization challenge is significantly amplified due to the existence of shared variables.","Decomposition-based cooperative coevolution (CC) algorithms have demonstrated promising performance in addressing large-scale overlapping problems.","However, current CC frameworks designed for overlapping problems rely on grouping methods for the identification of overlapping problem structures and the current grouping methods for large-scale overlapping problems fail to consider both accuracy and efficiency simultaneously.","In this article, we propose a two-stage enhanced grouping method for large-scale overlapping problems, called OEDG, which achieves accurate grouping while significantly reducing computational resource consumption.","In the first stage, OEDG employs a grouping method based on the finite differences principle to identify all subcomponents and shared variables.","In the second stage, we propose two grouping refinement methods, called subcomponent union detection (SUD) and subcomponent detection (SD), to enhance and refine the grouping results.","SUD examines the information of the subcomponents and shared variables obtained in the previous stage, and SD corrects inaccurate grouping results.","To better verify the performance of the proposed OEDG, we propose a series of novel benchmarks that consider various properties of large-scale overlapping problems, including the topology structure, overlapping degree, and separability.","Extensive experimental results demonstrate that OEDG is capable of accurately grouping different types of large-scale overlapping problems while consuming fewer computational resources.","Finally, we empirically verify that the proposed OEDG can effectively improve the optimization performance of diverse large-scale overlapping problems."],"url":"http://arxiv.org/abs/2404.10515v1","category":"cs.NE"}
{"created":"2024-04-16 12:19:54","title":"Self-Supervised Visual Preference Alignment","abstract":"This paper makes the first attempt towards unsupervised preference alignment in Vision-Language Models (VLMs). We generate chosen and rejected responses with regard to the original and augmented image pairs, and conduct preference alignment with direct preference optimization. It is based on a core idea: properly designed augmentation to the image input will induce VLM to generate false but hard negative responses, which helps the model to learn from and produce more robust and powerful answers. The whole pipeline no longer hinges on supervision from GPT4 or human involvement during alignment, and is highly efficient with few lines of code. With only 8k randomly sampled unsupervised data, it achieves 90\\% relative score to GPT-4 on complex reasoning in LLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex multi-modal benchmark MM-Vet. Visualizations shows its improved ability to align with user-intentions. A series of ablations are firmly conducted to reveal the latent mechanism of the approach, which also indicates its potential towards further scaling. Code will be available.","sentences":["This paper makes the first attempt towards unsupervised preference alignment in Vision-Language Models (VLMs).","We generate chosen and rejected responses with regard to the original and augmented image pairs, and conduct preference alignment with direct preference optimization.","It is based on a core idea: properly designed augmentation to the image input will induce VLM to generate false but hard negative responses, which helps the model to learn from and produce more robust and powerful answers.","The whole pipeline no longer hinges on supervision from GPT4 or human involvement during alignment, and is highly efficient with few lines of code.","With only 8k randomly sampled unsupervised data, it achieves 90\\% relative score to GPT-4 on complex reasoning in LLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex multi-modal benchmark MM-Vet.","Visualizations shows its improved ability to align with user-intentions.","A series of ablations are firmly conducted to reveal the latent mechanism of the approach, which also indicates its potential towards further scaling.","Code will be available."],"url":"http://arxiv.org/abs/2404.10501v1","category":"cs.CV"}
{"created":"2024-04-16 12:19:08","title":"When Emotional Stimuli meet Prompt Designing: An Auto-Prompt Graphical Paradigm","abstract":"With the development of Large Language Models (LLM), numerous prompts have been proposed, each with a rich set of features and their own merits. This paper summarizes the prompt words for large language models (LLMs), categorizing them into stimulating and framework types, and proposes an Auto-Prompt Graphical Paradigm(APGP) that combines both stimulating and framework prompts to enhance the problem-solving capabilities of LLMs across multiple domains, then exemplifies it with a framework that adheres to this paradigm. The framework involves automated prompt generation and consideration of emotion-stimulus factors, guiding LLMs in problem abstraction, diversified solutions generation, comprehensive optimization, and self-verification after providing answers, ensuring solution accuracy. Compared to traditional stimuli and framework prompts, this framework integrates the advantages of both by adopting automated approaches inspired by APE work, overcoming the limitations of manually designed prompts. Test results on the ruozhiba and BBH datasets demonstrate that this framework can effectively improve the efficiency and accuracy of LLMs in problem-solving, paving the way for new applications of LLMs.","sentences":["With the development of Large Language Models (LLM), numerous prompts have been proposed, each with a rich set of features and their own merits.","This paper summarizes the prompt words for large language models (LLMs), categorizing them into stimulating and framework types, and proposes an Auto-Prompt Graphical Paradigm(APGP) that combines both stimulating and framework prompts to enhance the problem-solving capabilities of LLMs across multiple domains, then exemplifies it with a framework that adheres to this paradigm.","The framework involves automated prompt generation and consideration of emotion-stimulus factors, guiding LLMs in problem abstraction, diversified solutions generation, comprehensive optimization, and self-verification after providing answers, ensuring solution accuracy.","Compared to traditional stimuli and framework prompts, this framework integrates the advantages of both by adopting automated approaches inspired by APE work, overcoming the limitations of manually designed prompts.","Test results on the ruozhiba and BBH datasets demonstrate that this framework can effectively improve the efficiency and accuracy of LLMs in problem-solving, paving the way for new applications of LLMs."],"url":"http://arxiv.org/abs/2404.10500v1","category":"cs.CL"}
{"created":"2024-04-16 11:42:06","title":"BayesJudge: Bayesian Kernel Language Modelling with Confidence Uncertainty in Legal Judgment Prediction","abstract":"Predicting legal judgments with reliable confidence is paramount for responsible legal AI applications. While transformer-based deep neural networks (DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing their prediction confidence remains crucial. We present a novel Bayesian approach called BayesJudge that harnesses the synergy between deep learning and deep Gaussian Processes to quantify uncertainty through Bayesian kernel Monte Carlo dropout. Our method leverages informative priors and flexible data modelling via kernels, surpassing existing methods in both predictive accuracy and confidence estimation as indicated through brier score. Extensive evaluations of public legal datasets showcase our model's superior performance across diverse tasks. We also introduce an optimal solution to automate the scrutiny of unreliable predictions, resulting in a significant increase in the accuracy of the model's predictions by up to 27\\%. By empowering judges and legal professionals with more reliable information, our work paves the way for trustworthy and transparent legal AI applications that facilitate informed decisions grounded in both knowledge and quantified uncertainty.","sentences":["Predicting legal judgments with reliable confidence is paramount for responsible legal AI applications.","While transformer-based deep neural networks (DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing their prediction confidence remains crucial.","We present a novel Bayesian approach called BayesJudge that harnesses the synergy between deep learning and deep Gaussian Processes to quantify uncertainty through Bayesian kernel Monte Carlo dropout.","Our method leverages informative priors and flexible data modelling via kernels, surpassing existing methods in both predictive accuracy and confidence estimation as indicated through brier score.","Extensive evaluations of public legal datasets showcase our model's superior performance across diverse tasks.","We also introduce an optimal solution to automate the scrutiny of unreliable predictions, resulting in a significant increase in the accuracy of the model's predictions by up to 27\\%.","By empowering judges and legal professionals with more reliable information, our work paves the way for trustworthy and transparent legal AI applications that facilitate informed decisions grounded in both knowledge and quantified uncertainty."],"url":"http://arxiv.org/abs/2404.10481v1","category":"cs.LG"}
{"created":"2024-04-16 11:38:44","title":"Efficient optimal dispersed Haar-like filters for face detection","abstract":"This paper introduces a new dispersed Haar-like filter for efficiently detection face. The basic idea for finding the filter is maximising between-class and minimising within-class variance. The proposed filters can be considered as an optimal configuration dispersed Haar-like filters; filters with disjoint black and white parts.","sentences":["This paper introduces a new dispersed Haar-like filter for efficiently detection face.","The basic idea for finding the filter is maximising between-class and minimising within-class variance.","The proposed filters can be considered as an optimal configuration dispersed Haar-like filters; filters with disjoint black and white parts."],"url":"http://arxiv.org/abs/2404.10476v1","category":"cs.CV"}
{"created":"2024-04-16 11:25:00","title":"Machine Learning Based Optimization Workflow for Tuning Numerical Settings of Differential Equation Solvers for Boundary Value Problems","abstract":"Several numerical differential equation solvers have been employed effectively over the years as an alternative to analytical solvers to quickly and conveniently solve differential equations. One category of these is boundary value solvers, which are used to solve real-world problems formulated as differential equations with boundary conditions. These solvers require certain numerical settings to solve the differential equations that affect their solvability and performance. A systematic fine-tuning of these settings is required to obtain the desired solution and performance. Currently, these settings are either selected by trial and error or require domain expertise. In this paper, we propose a machine learning-based optimization workflow for fine-tuning the numerical settings to reduce the time and domain expertise required in the process. In the evaluation section, we discuss the scalability, stability, and reliability of the proposed workflow. We demonstrate our workflow on a numerical boundary value problem solver.","sentences":["Several numerical differential equation solvers have been employed effectively over the years as an alternative to analytical solvers to quickly and conveniently solve differential equations.","One category of these is boundary value solvers, which are used to solve real-world problems formulated as differential equations with boundary conditions.","These solvers require certain numerical settings to solve the differential equations that affect their solvability and performance.","A systematic fine-tuning of these settings is required to obtain the desired solution and performance.","Currently, these settings are either selected by trial and error or require domain expertise.","In this paper, we propose a machine learning-based optimization workflow for fine-tuning the numerical settings to reduce the time and domain expertise required in the process.","In the evaluation section, we discuss the scalability, stability, and reliability of the proposed workflow.","We demonstrate our workflow on a numerical boundary value problem solver."],"url":"http://arxiv.org/abs/2404.10472v1","category":"math.NA"}
{"created":"2024-04-16 11:14:33","title":"How quickly can you pack short paths? Engineering a search-tree algorithm for disjoint s-t paths of bounded length","abstract":"We study the Short Path Packing problem which asks, given a graph $G$, integers $k$ and $\\ell$, and vertices $s$ and $t$, whether there exist $k$ pairwise internally vertex-disjoint $s$-$t$ paths of length at most $\\ell$. The problem has been proven to be NP-hard and fixed-parameter tractable parameterized by $k$ and $\\ell$. Most previous research on this problem has been theoretical with limited practical implemetations. We present an exact FPT-algorithm based on a search-tree approach in combination with greedy localization. While its worst case runtime complexity of $(k\\cdot \\ell^2)^{k\\cdot \\ell}\\cdot n^{O(1)}$ is larger than the state of the art, the nature of search-tree algorithms allows for a broad range of potential optimizations. We exploit this potential by presenting techniques for input preprocessing, early detection of trivial and infeasible instances, and strategic selection of promising subproblems. Those approaches were implemented and heavily tested on a large dataset of diverse graphs. The results show that our heuristic improvements are very effective and that for the majority of instances, we can achieve fast runtimes.","sentences":["We study the Short Path Packing problem which asks, given a graph $G$, integers $k$ and $\\ell$, and vertices $s$ and $t$, whether there exist $k$ pairwise internally vertex-disjoint $s$-$t$ paths of length at most $\\ell$. The problem has been proven to be NP-hard and fixed-parameter tractable parameterized by $k$ and $\\ell$. Most previous research on this problem has been theoretical with limited practical implemetations.","We present an exact FPT-algorithm based on a search-tree approach in combination with greedy localization.","While its worst case runtime complexity of $(k\\cdot \\ell^2)^{k\\cdot \\ell}\\cdot n^{O(1)}$ is larger than the state of the art, the nature of search-tree algorithms allows for a broad range of potential optimizations.","We exploit this potential by presenting techniques for input preprocessing, early detection of trivial and infeasible instances, and strategic selection of promising subproblems.","Those approaches were implemented and heavily tested on a large dataset of diverse graphs.","The results show that our heuristic improvements are very effective and that for the majority of instances, we can achieve fast runtimes."],"url":"http://arxiv.org/abs/2404.10469v1","category":"cs.DS"}
{"created":"2024-04-16 11:09:35","title":"Formulations of the continuous set-covering problem on networks: a comparative study","abstract":"We study the continuous set covering problem on networks and propose several new MILP formulations and valid inequalities. In contrast to state-of-the-art formulations, the new formulations only use edges to index installed points, and the formulation sizes are smaller. The covering conditions can be represented as multivariate piecewise linear concave constraints, which we formulate as disjunctive systems. We propose three MILP formulations based on indicator constraint, big-M, and disjunctive programming techniques for modeling the disjunctive system. Finally, we give a classification of new and old formulations, and conduct experiments to compare them computationally.","sentences":["We study the continuous set covering problem on networks and propose several new MILP formulations and valid inequalities.","In contrast to state-of-the-art formulations, the new formulations only use edges to index installed points, and the formulation sizes are smaller.","The covering conditions can be represented as multivariate piecewise linear concave constraints, which we formulate as disjunctive systems.","We propose three MILP formulations based on indicator constraint, big-M, and disjunctive programming techniques for modeling the disjunctive system.","Finally, we give a classification of new and old formulations, and conduct experiments to compare them computationally."],"url":"http://arxiv.org/abs/2404.10467v1","category":"math.OC"}
{"created":"2024-04-16 11:02:49","title":"Pulse Engineering via Projection of Response Functions","abstract":"We present an iterative optimal control method of quantum systems, aimed at an implementation of a desired operation with optimal fidelity. The update step of the method is based on the linear response of the fidelity to the control operators, and its projection onto the mode functions of the corresponding operator. Our method extends methods such as gradient ascent pulse engineering and variational quantum algorithms, by determining the fidelity gradient in a hyperparameter-free manner, and using it for a multi-parameter update, capitalizing on the multi-mode overlap of the perturbation and the mode functions. This directly reduces the number of dynamical trajectories that need to be evaluated in order to update a set of parameters. We demonstrate this approach, and compare it to the standard GRAPE algorithm, for the example of a quantum gate on two qubits, demonstrating a clear improvement in convergence and optimal fidelity of the generated protocol.","sentences":["We present an iterative optimal control method of quantum systems, aimed at an implementation of a desired operation with optimal fidelity.","The update step of the method is based on the linear response of the fidelity to the control operators, and its projection onto the mode functions of the corresponding operator.","Our method extends methods such as gradient ascent pulse engineering and variational quantum algorithms, by determining the fidelity gradient in a hyperparameter-free manner, and using it for a multi-parameter update, capitalizing on the multi-mode overlap of the perturbation and the mode functions.","This directly reduces the number of dynamical trajectories that need to be evaluated in order to update a set of parameters.","We demonstrate this approach, and compare it to the standard GRAPE algorithm, for the example of a quantum gate on two qubits, demonstrating a clear improvement in convergence and optimal fidelity of the generated protocol."],"url":"http://arxiv.org/abs/2404.10462v1","category":"quant-ph"}
{"created":"2024-04-16 11:00:46","title":"A Theory of Quantum Jumps","abstract":"Using the principles of the ETH - Approach to Quantum Mechanics we study fluorescence and the phenomenon of ``quantum jumps'' in idealized models of atoms coupled to the quantized electromagnetic field. In a limiting regime where the orbital motion of the atoms is neglected and the velocity of light tends to infinity we derive explicit non-linear stochastic differential equations describing the effective time evolution of states of individual atoms. These equations give rise to a measure on state-trajectories with quantum jumps which is a quantum-mechanical analogue of the Wiener measure of Brownian motion. Our results amount to a derivation of the fundamental randomness in the quantum-mechanical description of microscopic systems from basic principles in the context of some simple models.","sentences":["Using the principles of the ETH - Approach to Quantum Mechanics we study fluorescence and the phenomenon of ``quantum jumps'' in idealized models of atoms coupled to the quantized electromagnetic field.","In a limiting regime where the orbital motion of the atoms is neglected and the velocity of light tends to infinity we derive explicit non-linear stochastic differential equations describing the effective time evolution of states of individual atoms.","These equations give rise to a measure on state-trajectories with quantum jumps which is a quantum-mechanical analogue of the Wiener measure of Brownian motion.","Our results amount to a derivation of the fundamental randomness in the quantum-mechanical description of microscopic systems from basic principles in the context of some simple models."],"url":"http://arxiv.org/abs/2404.10460v1","category":"quant-ph"}
{"created":"2024-04-16 10:33:06","title":"On estimation of heavy-tailed stable linear regression","abstract":"We study the parameter estimation method for linear regression models with possibly skewed stable distributed errors. Our estimation procedure consists of two stages: first, for the regression coefficients, the Cauchy quasi-maximum likelihood estimator (CQMLE) is considered after taking the differences to remove the skewness of noise, and we prove its asymptotic normality and tail-probability estimate; second, as for stable-distribution parameters, we consider the moment estimators based on the symmetrized and centered residuals and prove their $\\sqrt{n}$-consistency. To derive the $\\sqrt{n}$-consistency, we essentially used the tail-probability estimate of the CQMLE. The proposed estimation procedure has a very low computational load and is much less time-consuming compared with the maximum-likelihood estimator. Further, our estimator can be effectively used as an initial value of the numerical optimization of the log-likelihood.","sentences":["We study the parameter estimation method for linear regression models with possibly skewed stable distributed errors.","Our estimation procedure consists of two stages: first, for the regression coefficients, the Cauchy quasi-maximum likelihood estimator (CQMLE) is considered after taking the differences to remove the skewness of noise, and we prove its asymptotic normality and tail-probability estimate; second, as for stable-distribution parameters, we consider the moment estimators based on the symmetrized and centered residuals and prove their $\\sqrt{n}$-consistency.","To derive the $\\sqrt{n}$-consistency, we essentially used the tail-probability estimate of the CQMLE.","The proposed estimation procedure has a very low computational load and is much less time-consuming compared with the maximum-likelihood estimator.","Further, our estimator can be effectively used as an initial value of the numerical optimization of the log-likelihood."],"url":"http://arxiv.org/abs/2404.10448v1","category":"math.ST"}
{"created":"2024-04-16 10:04:38","title":"The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose Refinement","abstract":"Pose refinement is an interesting and practically relevant research direction. Pose refinement can be used to (1) obtain a more accurate pose estimate from an initial prior (e.g., from retrieval), (2) as pre-processing, i.e., to provide a better starting point to a more expensive pose estimator, (3) as post-processing of a more accurate localizer. Existing approaches focus on learning features / scene representations for the pose refinement task. This involves training an implicit scene representation or learning features while optimizing a camera pose-based loss. A natural question is whether training specific features / representations is truly necessary or whether similar results can be already achieved with more generic features. In this work, we present a simple approach that combines pre-trained features with a particle filter and a renderable representation of the scene. Despite its simplicity, it achieves state-of-the-art results, demonstrating that one can easily build a pose refiner without the need for specific training. The code is at https://github.com/ga1i13o/mcloc_poseref","sentences":["Pose refinement is an interesting and practically relevant research direction.","Pose refinement can be used to (1) obtain a more accurate pose estimate from an initial prior (e.g., from retrieval), (2) as pre-processing, i.e., to provide a better starting point to a more expensive pose estimator, (3) as post-processing of a more accurate localizer.","Existing approaches focus on learning features / scene representations for the pose refinement task.","This involves training an implicit scene representation or learning features while optimizing a camera pose-based loss.","A natural question is whether training specific features / representations is truly necessary or whether similar results can be already achieved with more generic features.","In this work, we present a simple approach that combines pre-trained features with a particle filter and a renderable representation of the scene.","Despite its simplicity, it achieves state-of-the-art results, demonstrating that one can easily build a pose refiner without the need for specific training.","The code is at https://github.com/ga1i13o/mcloc_poseref"],"url":"http://arxiv.org/abs/2404.10438v1","category":"cs.CV"}
{"created":"2024-04-16 10:02:36","title":"Tree Bandits for Generative Bayes","abstract":"In generative models with obscured likelihood, Approximate Bayesian Computation (ABC) is often the tool of last resort for inference. However, ABC demands many prior parameter trials to keep only a small fraction that passes an acceptance test. To accelerate ABC rejection sampling, this paper develops a self-aware framework that learns from past trials and errors. We apply recursive partitioning classifiers on the ABC lookup table to sequentially refine high-likelihood regions into boxes. Each box is regarded as an arm in a binary bandit problem treating ABC acceptance as a reward. Each arm has a proclivity for being chosen for the next ABC evaluation, depending on the prior distribution and past rejections. The method places more splits in those areas where the likelihood resides, shying away from low-probability regions destined for ABC rejections. We provide two versions: (1) ABC-Tree for posterior sampling, and (2) ABC-MAP for maximum a posteriori estimation. We demonstrate accurate ABC approximability at much lower simulation cost. We justify the use of our tree-based bandit algorithms with nearly optimal regret bounds. Finally, we successfully apply our approach to the problem of masked image classification using deep generative models.","sentences":["In generative models with obscured likelihood, Approximate Bayesian Computation (ABC) is often the tool of last resort for inference.","However, ABC demands many prior parameter trials to keep only a small fraction that passes an acceptance test.","To accelerate ABC rejection sampling, this paper develops a self-aware framework that learns from past trials and errors.","We apply recursive partitioning classifiers on the ABC lookup table to sequentially refine high-likelihood regions into boxes.","Each box is regarded as an arm in a binary bandit problem treating ABC acceptance as a reward.","Each arm has a proclivity for being chosen for the next ABC evaluation, depending on the prior distribution and past rejections.","The method places more splits in those areas where the likelihood resides, shying away from low-probability regions destined for ABC rejections.","We provide two versions: (1) ABC-Tree for posterior sampling, and (2) ABC-MAP for maximum a posteriori estimation.","We demonstrate accurate ABC approximability at much lower simulation cost.","We justify the use of our tree-based bandit algorithms with nearly optimal regret bounds.","Finally, we successfully apply our approach to the problem of masked image classification using deep generative models."],"url":"http://arxiv.org/abs/2404.10436v1","category":"cs.LG"}
{"created":"2024-04-16 09:46:24","title":"Zero-Sum Games for Volterra Integral Equations and Viscosity Solutions of Path-Dependent Hamilton-Jacobi Equations","abstract":"We consider a game, in which the dynamics is described by a non-linear Volterra integral equation of Hammerstein type with a weakly-singular kernel and the goals of the first and second players are, respectively, to minimize and maximize a given cost functional. We propose a way of how the dynamic programming principle can be formalized and the theory of generalized (viscosity) solutions of path-dependent Hamilton--Jacobi equations can be developed in order to prove the existence of the game value, obtain a characterization of the value functional, and construct players' optimal feedback strategies.","sentences":["We consider a game, in which the dynamics is described by a non-linear Volterra integral equation of Hammerstein type with a weakly-singular kernel and the goals of the first and second players are, respectively, to minimize and maximize a given cost functional.","We propose a way of how the dynamic programming principle can be formalized and the theory of generalized (viscosity) solutions of path-dependent Hamilton--Jacobi equations can be developed in order to prove the existence of the game value, obtain a characterization of the value functional, and construct players' optimal feedback strategies."],"url":"http://arxiv.org/abs/2404.10428v1","category":"math.OC"}
{"created":"2024-04-16 09:43:58","title":"Optimizing BioTac Simulation for Realistic Tactile Perception","abstract":"Tactile sensing presents a promising opportunity for enhancing the interaction capabilities of today's robots. BioTac is a commonly used tactile sensor that enables robots to perceive and respond to physical tactile stimuli. However, the sensor's non-linearity poses challenges in simulating its behavior. In this paper, we first investigate a BioTac simulation that uses temperature, force, and contact point positions to predict the sensor outputs. We show that training with BioTac temperature readings does not yield accurate sensor output predictions during deployment. Consequently, we tested three alternative models, i.e., an XGBoost regressor, a neural network, and a transformer encoder. We train these models without temperature readings and provide a detailed investigation of the window size of the input vectors. We demonstrate that we achieve statistically significant improvements over the baseline network. Furthermore, our results reveal that the XGBoost regressor and transformer outperform traditional feed-forward neural networks in this task. We make all our code and results available online on https://github.com/wzaielamri/Optimizing_BioTac_Simulation.","sentences":["Tactile sensing presents a promising opportunity for enhancing the interaction capabilities of today's robots.","BioTac is a commonly used tactile sensor that enables robots to perceive and respond to physical tactile stimuli.","However, the sensor's non-linearity poses challenges in simulating its behavior.","In this paper, we first investigate a BioTac simulation that uses temperature, force, and contact point positions to predict the sensor outputs.","We show that training with BioTac temperature readings does not yield accurate sensor output predictions during deployment.","Consequently, we tested three alternative models, i.e., an XGBoost regressor, a neural network, and a transformer encoder.","We train these models without temperature readings and provide a detailed investigation of the window size of the input vectors.","We demonstrate that we achieve statistically significant improvements over the baseline network.","Furthermore, our results reveal that the XGBoost regressor and transformer outperform traditional feed-forward neural networks in this task.","We make all our code and results available online on https://github.com/wzaielamri/Optimizing_BioTac_Simulation."],"url":"http://arxiv.org/abs/2404.10425v1","category":"cs.RO"}
{"created":"2024-04-16 09:31:19","title":"VDTuner: Automated Performance Tuning for Vector Data Management Systems","abstract":"Vector data management systems (VDMSs) have become an indispensable cornerstone in large-scale information retrieval and machine learning systems like large language models. To enhance the efficiency and flexibility of similarity search, VDMS exposes many tunable index parameters and system parameters for users to specify. However, due to the inherent characteristics of VDMS, automatic performance tuning for VDMS faces several critical challenges, which cannot be well addressed by the existing auto-tuning methods. In this paper, we introduce VDTuner, a learning-based automatic performance tuning framework for VDMS, leveraging multi-objective Bayesian optimization. VDTuner overcomes the challenges associated with VDMS by efficiently exploring a complex multi-dimensional parameter space without requiring any prior knowledge. Moreover, it is able to achieve a good balance between search speed and recall rate, delivering an optimal configuration. Extensive evaluations demonstrate that VDTuner can markedly improve VDMS performance (14.12% in search speed and 186.38% in recall rate) compared with default setting, and is more efficient compared with state-of-the-art baselines (up to 3.57 times faster in terms of tuning time). In addition, VDTuner is scalable to specific user preference and cost-aware optimization objective. VDTuner is available online at https://github.com/tiannuo-yang/VDTuner.","sentences":["Vector data management systems (VDMSs) have become an indispensable cornerstone in large-scale information retrieval and machine learning systems like large language models.","To enhance the efficiency and flexibility of similarity search, VDMS exposes many tunable index parameters and system parameters for users to specify.","However, due to the inherent characteristics of VDMS, automatic performance tuning for VDMS faces several critical challenges, which cannot be well addressed by the existing auto-tuning methods.","In this paper, we introduce VDTuner, a learning-based automatic performance tuning framework for VDMS, leveraging multi-objective Bayesian optimization.","VDTuner overcomes the challenges associated with VDMS by efficiently exploring a complex multi-dimensional parameter space without requiring any prior knowledge.","Moreover, it is able to achieve a good balance between search speed and recall rate, delivering an optimal configuration.","Extensive evaluations demonstrate that VDTuner can markedly improve VDMS performance (14.12% in search speed and 186.38% in recall rate) compared with default setting, and is more efficient compared with state-of-the-art baselines (up to 3.57 times faster in terms of tuning time).","In addition, VDTuner is scalable to specific user preference and cost-aware optimization objective.","VDTuner is available online at https://github.com/tiannuo-yang/VDTuner."],"url":"http://arxiv.org/abs/2404.10413v1","category":"cs.DB"}
{"created":"2024-04-16 09:19:11","title":"Comprehensive Survey of Model Compression and Speed up for Vision Transformers","abstract":"Vision Transformers (ViT) have marked a paradigm shift in computer vision, outperforming state-of-the-art models across diverse tasks. However, their practical deployment is hampered by high computational and memory demands. This study addresses the challenge by evaluating four primary model compression techniques: quantization, low-rank approximation, knowledge distillation, and pruning. We methodically analyze and compare the efficacy of these techniques and their combinations in optimizing ViTs for resource-constrained environments. Our comprehensive experimental evaluation demonstrates that these methods facilitate a balanced compromise between model accuracy and computational efficiency, paving the way for wider application in edge computing devices.","sentences":["Vision Transformers (ViT) have marked a paradigm shift in computer vision, outperforming state-of-the-art models across diverse tasks.","However, their practical deployment is hampered by high computational and memory demands.","This study addresses the challenge by evaluating four primary model compression techniques: quantization, low-rank approximation, knowledge distillation, and pruning.","We methodically analyze and compare the efficacy of these techniques and their combinations in optimizing ViTs for resource-constrained environments.","Our comprehensive experimental evaluation demonstrates that these methods facilitate a balanced compromise between model accuracy and computational efficiency, paving the way for wider application in edge computing devices."],"url":"http://arxiv.org/abs/2404.10407v1","category":"cs.CV"}
{"created":"2024-04-16 09:12:16","title":"Integration of Self-Supervised BYOL in Semi-Supervised Medical Image Recognition","abstract":"Image recognition techniques heavily rely on abundant labeled data, particularly in medical contexts. Addressing the challenges associated with obtaining labeled data has led to the prominence of self-supervised learning and semi-supervised learning, especially in scenarios with limited annotated data. In this paper, we proposed an innovative approach by integrating self-supervised learning into semi-supervised models to enhance medical image recognition. Our methodology commences with pre-training on unlabeled data utilizing the BYOL method. Subsequently, we merge pseudo-labeled and labeled datasets to construct a neural network classifier, refining it through iterative fine-tuning. Experimental results on three different datasets demonstrate that our approach optimally leverages unlabeled data, outperforming existing methods in terms of accuracy for medical image recognition.","sentences":["Image recognition techniques heavily rely on abundant labeled data, particularly in medical contexts.","Addressing the challenges associated with obtaining labeled data has led to the prominence of self-supervised learning and semi-supervised learning, especially in scenarios with limited annotated data.","In this paper, we proposed an innovative approach by integrating self-supervised learning into semi-supervised models to enhance medical image recognition.","Our methodology commences with pre-training on unlabeled data utilizing the BYOL method.","Subsequently, we merge pseudo-labeled and labeled datasets to construct a neural network classifier, refining it through iterative fine-tuning.","Experimental results on three different datasets demonstrate that our approach optimally leverages unlabeled data, outperforming existing methods in terms of accuracy for medical image recognition."],"url":"http://arxiv.org/abs/2404.10405v1","category":"cs.CV"}
{"created":"2024-04-16 08:56:00","title":"Problem of eigenvalues of stochastic Hamiltonian systems with boundary conditions and Markov chain","abstract":"In this paper, we study the eigenvalue problem of stochastic Hamiltonian system driven by Brownian motion and Markov chain with boundary conditions and time-dependent coefficients. For any dimensional case, the existence of the first eigenvalue is proven and the corresponding eigenfunctions are constructed by virtue of dual transformation and generalized Riccati equation system. Furthermore, we have more finely characterized the existence of all eigenvalues and constructed the related eigenfunctions for one-dimensional Hamiltonian system. Moreover, the increasing order of these eigenvalues have also been given.","sentences":["In this paper, we study the eigenvalue problem of stochastic Hamiltonian system driven by Brownian motion and Markov chain with boundary conditions and time-dependent coefficients.","For any dimensional case, the existence of the first eigenvalue is proven and the corresponding eigenfunctions are constructed by virtue of dual transformation and generalized Riccati equation system.","Furthermore, we have more finely characterized the existence of all eigenvalues and constructed the related eigenfunctions for one-dimensional Hamiltonian system.","Moreover, the increasing order of these eigenvalues have also been given."],"url":"http://arxiv.org/abs/2404.10398v1","category":"math.PR"}
{"created":"2024-04-16 08:54:29","title":"Efficient evaluation of Bernstein-B\u00e9zier coefficients of B-spline basis functions over one knot span","abstract":"New differential-recurrence relations for B-spline basis functions are given. Using these relations, a recursive method for finding the Bernstein-B\\'{e}zier coefficients of B-spline basis functions over a single knot span is proposed. The algorithm works for any knot sequence which guarantees that all B-spline functions are at least $C^0$-continuous. It has good numerical behavior and has an asymptotically optimal computational complexity.","sentences":["New differential-recurrence relations for B-spline basis functions are given.","Using these relations, a recursive method for finding the Bernstein-B\\'{e}zier coefficients of B-spline basis functions over a single knot span is proposed.","The algorithm works for any knot sequence which guarantees that all B-spline functions are at least $C^0$-continuous.","It has good numerical behavior and has an asymptotically optimal computational complexity."],"url":"http://arxiv.org/abs/2404.10396v1","category":"math.NA"}
{"created":"2024-04-16 08:53:25","title":"Spline-Interpolated Model Predictive Path Integral Control with Stein Variational Inference for Reactive Navigation","abstract":"This paper presents a reactive navigation method that leverages a Model Predictive Path Integral (MPPI) control enhanced with spline interpolation for the control input sequence and Stein Variational Gradient Descent (SVGD). The MPPI framework addresses a nonlinear optimization problem by determining an optimal sequence of control inputs through a sampling-based approach. The efficacy of MPPI is significantly influenced by the sampling noise. To rapidly identify routes that circumvent large and/or newly detected obstacles, it is essential to employ high levels of sampling noise. However, such high noise levels result in jerky control input sequences, leading to non-smooth trajectories. To mitigate this issue, we propose the integration of spline interpolation within the MPPI process, enabling the generation of smooth control input sequences despite the utilization of substantial sampling noises. Nonetheless, the standard MPPI algorithm struggles in scenarios featuring multiple optimal or near-optimal solutions, such as environments with several viable obstacle avoidance paths, due to its assumption that the distribution over an optimal control input sequence can be closely approximated by a Gaussian distribution. To address this limitation, we extend our method by incorporating SVGD into the MPPI framework with spline interpolation. SVGD, rooted in the optimal transportation algorithm, possesses the unique ability to cluster samples around an optimal solution. Consequently, our approach facilitates robust reactive navigation by swiftly identifying obstacle avoidance paths while maintaining the smoothness of the control input sequences. The efficacy of our proposed method is validated on simulations with a quadrotor, demonstrating superior performance over existing baseline techniques.","sentences":["This paper presents a reactive navigation method that leverages a Model Predictive Path Integral (MPPI) control enhanced with spline interpolation for the control input sequence and Stein Variational Gradient Descent (SVGD).","The MPPI framework addresses a nonlinear optimization problem by determining an optimal sequence of control inputs through a sampling-based approach.","The efficacy of MPPI is significantly influenced by the sampling noise.","To rapidly identify routes that circumvent large and/or newly detected obstacles, it is essential to employ high levels of sampling noise.","However, such high noise levels result in jerky control input sequences, leading to non-smooth trajectories.","To mitigate this issue, we propose the integration of spline interpolation within the MPPI process, enabling the generation of smooth control input sequences despite the utilization of substantial sampling noises.","Nonetheless, the standard MPPI algorithm struggles in scenarios featuring multiple optimal or near-optimal solutions, such as environments with several viable obstacle avoidance paths, due to its assumption that the distribution over an optimal control input sequence can be closely approximated by a Gaussian distribution.","To address this limitation, we extend our method by incorporating SVGD into the MPPI framework with spline interpolation.","SVGD, rooted in the optimal transportation algorithm, possesses the unique ability to cluster samples around an optimal solution.","Consequently, our approach facilitates robust reactive navigation by swiftly identifying obstacle avoidance paths while maintaining the smoothness of the control input sequences.","The efficacy of our proposed method is validated on simulations with a quadrotor, demonstrating superior performance over existing baseline techniques."],"url":"http://arxiv.org/abs/2404.10395v1","category":"cs.RO"}
{"created":"2024-04-16 08:52:42","title":"Portrait3D: Text-Guided High-Quality 3D Portrait Generation Using Pyramid Representation and GANs Prior","abstract":"Existing neural rendering-based text-to-3D-portrait generation methods typically make use of human geometry prior and diffusion models to obtain guidance. However, relying solely on geometry information introduces issues such as the Janus problem, over-saturation, and over-smoothing. We present Portrait3D, a novel neural rendering-based framework with a novel joint geometry-appearance prior to achieve text-to-3D-portrait generation that overcomes the aforementioned issues. To accomplish this, we train a 3D portrait generator, 3DPortraitGAN-Pyramid, as a robust prior. This generator is capable of producing 360{\\deg} canonical 3D portraits, serving as a starting point for the subsequent diffusion-based generation process. To mitigate the \"grid-like\" artifact caused by the high-frequency information in the feature-map-based 3D representation commonly used by most 3D-aware GANs, we integrate a novel pyramid tri-grid 3D representation into 3DPortraitGAN-Pyramid. To generate 3D portraits from text, we first project a randomly generated image aligned with the given prompt into the pre-trained 3DPortraitGAN-Pyramid's latent space. The resulting latent code is then used to synthesize a pyramid tri-grid. Beginning with the obtained pyramid tri-grid, we use score distillation sampling to distill the diffusion model's knowledge into the pyramid tri-grid. Following that, we utilize the diffusion model to refine the rendered images of the 3D portrait and then use these refined images as training data to further optimize the pyramid tri-grid, effectively eliminating issues with unrealistic color and unnatural artifacts. Our experimental results show that Portrait3D can produce realistic, high-quality, and canonical 3D portraits that align with the prompt.","sentences":["Existing neural rendering-based text-to-3D-portrait generation methods typically make use of human geometry prior and diffusion models to obtain guidance.","However, relying solely on geometry information introduces issues such as the Janus problem, over-saturation, and over-smoothing.","We present Portrait3D, a novel neural rendering-based framework with a novel joint geometry-appearance prior to achieve text-to-3D-portrait generation that overcomes the aforementioned issues.","To accomplish this, we train a 3D portrait generator, 3DPortraitGAN-Pyramid, as a robust prior.","This generator is capable of producing 360{\\deg} canonical 3D portraits, serving as a starting point for the subsequent diffusion-based generation process.","To mitigate the \"grid-like\" artifact caused by the high-frequency information in the feature-map-based 3D representation commonly used by most 3D-aware GANs, we integrate a novel pyramid tri-grid 3D representation into 3DPortraitGAN-Pyramid.","To generate 3D portraits from text, we first project a randomly generated image aligned with the given prompt into the pre-trained 3DPortraitGAN-Pyramid's latent space.","The resulting latent code is then used to synthesize a pyramid tri-grid.","Beginning with the obtained pyramid tri-grid, we use score distillation sampling to distill the diffusion model's knowledge into the pyramid tri-grid.","Following that, we utilize the diffusion model to refine the rendered images of the 3D portrait and then use these refined images as training data to further optimize the pyramid tri-grid, effectively eliminating issues with unrealistic color and unnatural artifacts.","Our experimental results show that Portrait3D can produce realistic, high-quality, and canonical 3D portraits that align with the prompt."],"url":"http://arxiv.org/abs/2404.10394v1","category":"cs.CV"}
{"created":"2024-04-16 08:48:46","title":"Offline Trajectory Generalization for Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) aims to learn policies from static datasets of previously collected trajectories. Existing methods for offline RL either constrain the learned policy to the support of offline data or utilize model-based virtual environments to generate simulated rollouts. However, these methods suffer from (i) poor generalization to unseen states; and (ii) trivial improvement from low-qualified rollout simulation. In this paper, we propose offline trajectory generalization through world transformers for offline reinforcement learning (OTTO). Specifically, we use casual Transformers, a.k.a. World Transformers, to predict state dynamics and the immediate reward. Then we propose four strategies to use World Transformers to generate high-rewarded trajectory simulation by perturbing the offline data. Finally, we jointly use offline data with simulated data to train an offline RL algorithm. OTTO serves as a plug-in module and can be integrated with existing offline RL methods to enhance them with better generalization capability of transformers and high-rewarded data augmentation. Conducting extensive experiments on D4RL benchmark datasets, we verify that OTTO significantly outperforms state-of-the-art offline RL methods.","sentences":["Offline reinforcement learning (RL) aims to learn policies from static datasets of previously collected trajectories.","Existing methods for offline RL either constrain the learned policy to the support of offline data or utilize model-based virtual environments to generate simulated rollouts.","However, these methods suffer from (i) poor generalization to unseen states; and (ii) trivial improvement from low-qualified rollout simulation.","In this paper, we propose offline trajectory generalization through world transformers for offline reinforcement learning (OTTO).","Specifically, we use casual Transformers, a.k.a. World Transformers, to predict state dynamics and the immediate reward.","Then we propose four strategies to use World Transformers to generate high-rewarded trajectory simulation by perturbing the offline data.","Finally, we jointly use offline data with simulated data to train an offline RL algorithm.","OTTO serves as a plug-in module and can be integrated with existing offline RL methods to enhance them with better generalization capability of transformers and high-rewarded data augmentation.","Conducting extensive experiments on D4RL benchmark datasets, we verify that OTTO significantly outperforms state-of-the-art offline RL methods."],"url":"http://arxiv.org/abs/2404.10393v1","category":"cs.LG"}
{"created":"2024-04-16 08:48:10","title":"Generating 6-D Trajectories for Omnidirectional Multirotor Aerial Vehicles in Cluttered Environments","abstract":"As fully-actuated systems, omnidirectional multirotor aerial vehicles (OMAVs) have more flexible maneuverability and advantages in aggressive flight in cluttered environments than traditional underactuated MAVs. %Due to the high dimensionality of configuration space, making the designed trajectory generation algorithm efficient is challenging. This paper aims to achieve safe flight of OMAVs in cluttered environments. Considering existing static obstacles, an efficient optimization-based framework is proposed to generate 6-D $SE(3)$ trajectories for OMAVs. Given the kinodynamic constraints and the 3D collision-free region represented by a series of intersecting convex polyhedra, the proposed method finally generates a safe and dynamically feasible 6-D trajectory. First, we parameterize the vehicle's attitude into a free 3D vector using stereographic projection to eliminate the constraints inherent in the $SO(3)$ manifold, while the complete $SE(3)$ trajectory is represented as a 6-D polynomial in time without inherent constraints. The vehicle's shape is modeled as a cuboid attached to the body frame to achieve whole-body collision evaluation. Then, we formulate the origin trajectory generation problem as a constrained optimization problem. The original constrained problem is finally transformed into an unconstrained one that can be solved efficiently. To verify the proposed framework's performance, simulations and real-world experiments based on a tilt-rotor hexarotor aerial vehicle are carried out.","sentences":["As fully-actuated systems, omnidirectional multirotor aerial vehicles (OMAVs) have more flexible maneuverability and advantages in aggressive flight in cluttered environments than traditional underactuated MAVs.","%Due to the high dimensionality of configuration space, making the designed trajectory generation algorithm efficient is challenging.","This paper aims to achieve safe flight of OMAVs in cluttered environments.","Considering existing static obstacles, an efficient optimization-based framework is proposed to generate 6-D $SE(3)$ trajectories for OMAVs.","Given the kinodynamic constraints and the 3D collision-free region represented by a series of intersecting convex polyhedra, the proposed method finally generates a safe and dynamically feasible 6-D trajectory.","First, we parameterize the vehicle's attitude into a free 3D vector using stereographic projection to eliminate the constraints inherent in the $SO(3)$ manifold, while the complete $SE(3)$ trajectory is represented as a 6-D polynomial in time without inherent constraints.","The vehicle's shape is modeled as a cuboid attached to the body frame to achieve whole-body collision evaluation.","Then, we formulate the origin trajectory generation problem as a constrained optimization problem.","The original constrained problem is finally transformed into an unconstrained one that can be solved efficiently.","To verify the proposed framework's performance, simulations and real-world experiments based on a tilt-rotor hexarotor aerial vehicle are carried out."],"url":"http://arxiv.org/abs/2404.10392v1","category":"cs.RO"}
{"created":"2024-04-16 08:44:34","title":"Paving the Way to Hybrid Quantum-Classical Scientific Workflows","abstract":"The increasing growth of data volume, and the consequent explosion in demand for computational power, are affecting scientific computing, as shown by the rise of extreme data scientific workflows. As the need for computing power increases, quantum computing has been proposed as a way to deliver it. It may provide significant theoretical speedups for many scientific applications (i.e., molecular dynamics, quantum chemistry, combinatorial optimization, and machine learning). Therefore, integrating quantum computers into the computing continuum constitutes a promising way to speed up scientific computation. However, the scientific computing community still lacks the necessary tools and expertise to fully harness the power of quantum computers in the execution of complex applications such as scientific workflows. In this work, we describe the main characteristics of quantum computing and its main benefits for scientific applications, then we formalize hybrid quantum-classic workflows, explore how to identify quantum components and map them onto resources. We demonstrate concepts on a real use case and define a software architecture for a hybrid workflow management system.","sentences":["The increasing growth of data volume, and the consequent explosion in demand for computational power, are affecting scientific computing, as shown by the rise of extreme data scientific workflows.","As the need for computing power increases, quantum computing has been proposed as a way to deliver it.","It may provide significant theoretical speedups for many scientific applications (i.e., molecular dynamics, quantum chemistry, combinatorial optimization, and machine learning).","Therefore, integrating quantum computers into the computing continuum constitutes a promising way to speed up scientific computation.","However, the scientific computing community still lacks the necessary tools and expertise to fully harness the power of quantum computers in the execution of complex applications such as scientific workflows.","In this work, we describe the main characteristics of quantum computing and its main benefits for scientific applications, then we formalize hybrid quantum-classic workflows, explore how to identify quantum components and map them onto resources.","We demonstrate concepts on a real use case and define a software architecture for a hybrid workflow management system."],"url":"http://arxiv.org/abs/2404.10389v1","category":"cs.ET"}
{"created":"2024-04-16 08:43:45","title":"Worst-Case Riemannian Optimization with Uncertain Target Steering Vector for Slow-Time Transmit Sequence of Cognitive Radar","abstract":"Optimization of slow-time transmit sequence endows cognitive radar with the ability to suppress strong clutter in the range-Doppler domain. However, in practice, inaccurate target velocity information or random phase error would induce uncertainty about the actual target steering vector, which would in turn severely deteriorate the the performance of the slow-time matched filter. In order to solve this problem, we propose a new optimization method for slow-time transmit sequence design. The proposed method transforms the original non-convex optimization with an uncertain target steering vector into a two-step worst-case optimization problem. For each sub-problem, we develop a corresponding trust-region Riemannian optimization algorithm. By iteratively solving the two sub-problems, a sub-optimal solution can be reached without accurate information about the target steering vector. Furthermore, the convergence property of the proposed algorithms has been analyzed and detailed proof of the convergence is given. Unlike the traditional waveform optimization method, the proposed method is designed to work with an uncertain target steering vector and therefore, is more robust in practical radar systems. Numerical simulation results in different scenarios verify the effectiveness of the proposed method in suppressing the clutter and show its advantages in terms of the output signal-to-clutter plus noise ratio (SCNR) over traditional methods.","sentences":["Optimization of slow-time transmit sequence endows cognitive radar with the ability to suppress strong clutter in the range-Doppler domain.","However, in practice, inaccurate target velocity information or random phase error would induce uncertainty about the actual target steering vector, which would in turn severely deteriorate the the performance of the slow-time matched filter.","In order to solve this problem, we propose a new optimization method for slow-time transmit sequence design.","The proposed method transforms the original non-convex optimization with an uncertain target steering vector into a two-step worst-case optimization problem.","For each sub-problem, we develop a corresponding trust-region Riemannian optimization algorithm.","By iteratively solving the two sub-problems, a sub-optimal solution can be reached without accurate information about the target steering vector.","Furthermore, the convergence property of the proposed algorithms has been analyzed and detailed proof of the convergence is given.","Unlike the traditional waveform optimization method, the proposed method is designed to work with an uncertain target steering vector and therefore, is more robust in practical radar systems.","Numerical simulation results in different scenarios verify the effectiveness of the proposed method in suppressing the clutter and show its advantages in terms of the output signal-to-clutter plus noise ratio (SCNR) over traditional methods."],"url":"http://arxiv.org/abs/2404.10388v1","category":"eess.SP"}
{"created":"2024-04-16 08:37:36","title":"I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey","abstract":"High-Performance Computing (HPC) systems excel in managing distributed workloads, and the growing interest in Artificial Intelligence (AI) has resulted in a surge in demand for faster methods of Machine Learning (ML) model training and inference. In the past, research on HPC I/O focused on optimizing the underlying storage system for modeling and simulation applications and checkpointing the results, causing writes to be the dominant I/O operation. These applications typically access large portions of the data written by simulations or experiments. ML workloads, in contrast, perform small I/O reads spread across a large number of random files. This shift of I/O access patterns poses several challenges to HPC storage systems. In this paper, we survey I/O in ML applications on HPC systems, and target literature within a 6-year time window from 2019 to 2024. We provide an overview of the common phases of ML, review available profilers and benchmarks, examine the I/O patterns encountered during ML training, explore I/O optimizations utilized in modern ML frameworks and proposed in recent literature, and lastly, present gaps requiring further R&D. We seek to summarize the common practices used in accessing data by ML applications and expose research gaps that could spawn further R&D.","sentences":["High-Performance Computing (HPC) systems excel in managing distributed workloads, and the growing interest in Artificial Intelligence (AI) has resulted in a surge in demand for faster methods of Machine Learning (ML) model training and inference.","In the past, research on HPC I/O focused on optimizing the underlying storage system for modeling and simulation applications and checkpointing the results, causing writes to be the dominant I/O operation.","These applications typically access large portions of the data written by simulations or experiments.","ML workloads, in contrast, perform small I/O reads spread across a large number of random files.","This shift of I/O access patterns poses several challenges to HPC storage systems.","In this paper, we survey I/O in ML applications on HPC systems, and target literature within a 6-year time window from 2019 to 2024.","We provide an overview of the common phases of ML, review available profilers and benchmarks, examine the I/O patterns encountered during ML training, explore I/O optimizations utilized in modern ML frameworks and proposed in recent literature, and lastly, present gaps requiring further R&D.","We seek to summarize the common practices used in accessing data by ML applications and expose research gaps that could spawn further R&D."],"url":"http://arxiv.org/abs/2404.10386v1","category":"cs.DC"}
{"created":"2024-04-16 08:28:16","title":"Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering","abstract":"Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform surprisingly well and outperform human experts on many tasks. However, in many domain-specific evaluations, these LLMs often suffer from hallucination problems due to insufficient training of relevant corpus. Furthermore, fine-tuning large models may face problems such as the LLMs are not open source or the construction of high-quality domain instruction is difficult. Therefore, structured knowledge databases such as knowledge graph can better provide domain back- ground knowledge for LLMs and make full use of the reasoning and analysis capabilities of LLMs. In some previous works, LLM was called multiple times to determine whether the current triplet was suitable for inclusion in the subgraph when retrieving subgraphs through a question. Especially for the question that require a multi-hop reasoning path, frequent calls to LLM will consume a lot of computing power. Moreover, when choosing the reasoning path, LLM will be called once for each step, and if one of the steps is selected incorrectly, it will lead to the accumulation of errors in the following steps. In this paper, we integrated and optimized a pipeline for selecting reasoning paths from KG based on LLM, which can reduce the dependency on LLM. In addition, we propose a simple and effective subgraph retrieval method based on chain of thought (CoT) and page rank which can returns the paths most likely to contain the answer. We conduct experiments on three datasets: GenMedGPT-5k [14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using fewer LLM calls can achieve the same results as previous SOTAs models.","sentences":["Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform surprisingly well and outperform human experts on many tasks.","However, in many domain-specific evaluations, these LLMs often suffer from hallucination problems due to insufficient training of relevant corpus.","Furthermore, fine-tuning large models may face problems such as the LLMs are not open source or the construction of high-quality domain instruction is difficult.","Therefore, structured knowledge databases such as knowledge graph can better provide domain back- ground knowledge for LLMs and make full use of the reasoning and analysis capabilities of LLMs.","In some previous works, LLM was called multiple times to determine whether the current triplet was suitable for inclusion in the subgraph when retrieving subgraphs through a question.","Especially for the question that require a multi-hop reasoning path, frequent calls to LLM will consume a lot of computing power.","Moreover, when choosing the reasoning path, LLM will be called once for each step, and if one of the steps is selected incorrectly, it will lead to the accumulation of errors in the following steps.","In this paper, we integrated and optimized a pipeline for selecting reasoning paths from KG based on LLM, which can reduce the dependency on LLM.","In addition, we propose a simple and effective subgraph retrieval method based on chain of thought (CoT) and page rank which can returns the paths most likely to contain the answer.","We conduct experiments on three datasets: GenMedGPT-5k","[14], WebQuestions [2], and CMCQA","[21].","Finally, RoK can demonstrate that using fewer LLM calls can achieve the same results as previous SOTAs models."],"url":"http://arxiv.org/abs/2404.10384v1","category":"cs.CL"}
{"created":"2024-04-17 17:14:53","title":"Towards Energetic Quantum Advantage in Trapped-Ion Quantum Computation","abstract":"The question of the energetic efficiency of quantum computers has gained some attention only recently. A precise understanding of the resources required to operate a quantum computer with a targeted computational performance and how the energy requirements can impact the scalability is still missing. In this work, one implementation of the quantum Fourier transform (QFT) algorithm in a trapped ion setup was studied. The main focus was to obtain a theoretical characterization of the energetic costs of quantum computation. The energetic cost of the experiment was estimated by analyzing the components of the setup and the steps involved in a quantum computation, from the cooling and preparation of the ions to the implementation of the algorithm and readout of the result. A potential scaling of the energetic costs was argued and used to find a possible threshold for an energetic quantum advantage against state-of-the-art classical supercomputers.","sentences":["The question of the energetic efficiency of quantum computers has gained some attention only recently.","A precise understanding of the resources required to operate a quantum computer with a targeted computational performance and how the energy requirements can impact the scalability is still missing.","In this work, one implementation of the quantum Fourier transform (QFT) algorithm in a trapped ion setup was studied.","The main focus was to obtain a theoretical characterization of the energetic costs of quantum computation.","The energetic cost of the experiment was estimated by analyzing the components of the setup and the steps involved in a quantum computation, from the cooling and preparation of the ions to the implementation of the algorithm and readout of the result.","A potential scaling of the energetic costs was argued and used to find a possible threshold for an energetic quantum advantage against state-of-the-art classical supercomputers."],"url":"http://arxiv.org/abs/2404.11572v1","category":"quant-ph"}
{"created":"2024-04-17 17:00:47","title":"Conversion of twistedness from light to atoms","abstract":"We develop a simple model and propose a scheme that allows the production of twisted atoms in free space using the absorption of twisted photons by a bound electron. We show that in the inelastic collision of a photon and an atom, the twisted state of the photon is transferred to the center-of-mass state, so that the projection of the orbital momentum of the atom becomes $m_\\gamma-\\Delta m_e$. We also show that, depending on the experimental conditions, the twistedness of the photon is either transferred to the atomic center-of-mass quantum state or modifies the selection rule for the bound electron transition.","sentences":["We develop a simple model and propose a scheme that allows the production of twisted atoms in free space using the absorption of twisted photons by a bound electron.","We show that in the inelastic collision of a photon and an atom, the twisted state of the photon is transferred to the center-of-mass state, so that the projection of the orbital momentum of the atom becomes $m_\\gamma-\\Delta m_e$. We also show that, depending on the experimental conditions, the twistedness of the photon is either transferred to the atomic center-of-mass quantum state or modifies the selection rule for the bound electron transition."],"url":"http://arxiv.org/abs/2404.11558v1","category":"quant-ph"}
{"created":"2024-04-17 16:34:24","title":"Bremsstrahlung Radiation Power in Fusion Plasmas Revisited: Towards Accurate Analytical Fitting","abstract":"In fusion plasmas, where electron temperatures $T_e$ range from keV to hundreds of keV, Bremsstrahlung radiation constitutes a significant energy loss mechanism. While various thermal average fitting formulas exist in the literature, their accuracy is limited, particularly for $T_e \\leq 20$ keV with error $>10\\%$. Additionally, non-relativistic fitting formulas become invalid for $T_e \\geq 50$ keV. The accurate calculation of Bremsstrahlung radiation is important for fusion gain study, especially of advanced fuels fusion. In this work, we develop new but still simple fitting formulas that are valid for electron temperatures ranging from small ($\\lesssim0.1$ keV) to extreme relativistic range, with errors of less than 1\\% even for high atomic number ions (e.g., $Z=30$), which could be sufficient for fusion plasma applications. Both electron-ion (e-i) and electron-electron (e-e) Bremsstrahlung radiations are considered. Both polynomial fitting with truncated ($t=k_BT_e/m_ec^2\\leq10$) and one-fit-all formulas are provided. Additionally, we offer fitting formulas for e-i and e-e specifically for electron energies, which could prove useful in non-Maxwellian Bremsstrahlung radiation studies.","sentences":["In fusion plasmas, where electron temperatures $T_e$ range from keV to hundreds of keV, Bremsstrahlung radiation constitutes a significant energy loss mechanism.","While various thermal average fitting formulas exist in the literature, their accuracy is limited, particularly for $T_e \\leq 20$ keV with error $>10\\%$.","Additionally, non-relativistic fitting formulas become invalid for $T_e \\geq 50$ keV. The accurate calculation of Bremsstrahlung radiation is important for fusion gain study, especially of advanced fuels fusion.","In this work, we develop new but still simple fitting formulas that are valid for electron temperatures ranging from small ($\\lesssim0.1$ keV) to extreme relativistic range, with errors of less than 1\\% even for high atomic number ions (e.g., $Z=30$), which could be sufficient for fusion plasma applications.","Both electron-ion (e-i) and electron-electron (e-e) Bremsstrahlung radiations are considered.","Both polynomial fitting with truncated ($t=k_BT_e/m_ec^2\\leq10$) and one-fit-all formulas are provided.","Additionally, we offer fitting formulas for e-i and e-e specifically for electron energies, which could prove useful in non-Maxwellian Bremsstrahlung radiation studies."],"url":"http://arxiv.org/abs/2404.11540v1","category":"physics.plasm-ph"}
{"created":"2024-04-17 16:17:56","title":"A global analysis of $\u03c0^0$, $K_S^0$ and $\u03b7$ fragmentation functions with BESIII data","abstract":"In this research, we conduct a global QCD analysis of fragmentation functions (FFs) for neutral pions ($\\pi^0$), neutral kaons ($K_S^0$), and eta mesons ($\\eta$), utilizing world data of single inclusive hadron production in $e^+e^-$ annihilation involving the most recent BESIII data with low collision energy, to test the operational region of QCD collinear factorization for single inclusive hadron production. We found that the QCD-based analysis at next-to-next-to leading order in perturbative QCD with parameterized higher-twist effects can explain both existing high-energy world data and the BESIII new measurements, while the latter cannot be explained with existing FFs extracted with high-energy data. To investigate the higher-twist contributions to this discrepancy, a direct functional approach is employed, providing testing framework for characterizing the experimental results over a wide range of energy scales, from low to high, thus extending the classical theoretical models to the BESIII domain.","sentences":["In this research, we conduct a global QCD analysis of fragmentation functions (FFs) for neutral pions ($\\pi^0$), neutral kaons ($K_S^0$), and eta mesons ($\\eta$), utilizing world data of single inclusive hadron production in $e^+e^-$ annihilation involving the most recent BESIII data with low collision energy, to test the operational region of QCD collinear factorization for single inclusive hadron production.","We found that the QCD-based analysis at next-to-next-to leading order in perturbative QCD with parameterized higher-twist effects can explain both existing high-energy world data and the BESIII new measurements, while the latter cannot be explained with existing FFs extracted with high-energy data.","To investigate the higher-twist contributions to this discrepancy, a direct functional approach is employed, providing testing framework for characterizing the experimental results over a wide range of energy scales, from low to high, thus extending the classical theoretical models to the BESIII domain."],"url":"http://arxiv.org/abs/2404.11527v1","category":"hep-ph"}
{"created":"2024-04-17 15:15:47","title":"X-posing Free Speech: Examining the Impact of Moderation Relaxation on Online Social Networks","abstract":"We investigate the impact of free speech and the relaxation of moderation on online social media platforms using Elon Musk's takeover of Twitter as a case study. By curating a dataset of over 10 million tweets, our study employs a novel framework combining content and network analysis. Our findings reveal a significant increase in the distribution of certain forms of hate content, particularly targeting the LGBTQ+ community and liberals. Network analysis reveals the formation of cohesive hate communities facilitated by influential bridge users, with substantial growth in interactions hinting at increased hate production and diffusion. By tracking the temporal evolution of PageRank, we identify key influencers, primarily self-identified far-right supporters disseminating hate against liberals and woke culture. Ironically, embracing free speech principles appears to have enabled hate speech against the very concept of freedom of expression and free speech itself. Our findings underscore the delicate balance platforms must strike between open expression and robust moderation to curb the proliferation of hate online.","sentences":["We investigate the impact of free speech and the relaxation of moderation on online social media platforms using Elon Musk's takeover of Twitter as a case study.","By curating a dataset of over 10 million tweets, our study employs a novel framework combining content and network analysis.","Our findings reveal a significant increase in the distribution of certain forms of hate content, particularly targeting the LGBTQ+ community and liberals.","Network analysis reveals the formation of cohesive hate communities facilitated by influential bridge users, with substantial growth in interactions hinting at increased hate production and diffusion.","By tracking the temporal evolution of PageRank, we identify key influencers, primarily self-identified far-right supporters disseminating hate against liberals and woke culture.","Ironically, embracing free speech principles appears to have enabled hate speech against the very concept of freedom of expression and free speech itself.","Our findings underscore the delicate balance platforms must strike between open expression and robust moderation to curb the proliferation of hate online."],"url":"http://arxiv.org/abs/2404.11465v1","category":"cs.SI"}
{"created":"2024-04-17 14:57:25","title":"Top-quark spin properties at LHC","abstract":"Due to its high mass top quarks decay before top-flavoured hadrons can be formed. This feature yields experimental access to the top quark polarization and production asymmetries. The large top quark sample moreover enables measurements of other properties, such as the W-boson branching ratios and helicity, and fragmentation functions of the bottom quarks. In this contribution, recent measurements of top-quark spin properties at the LHC are presented, including in particular the first evidence of the charge asymmetry in $t\\bar{t}$ production (ATLAS), the charge asymmetry in a boosted regime (CMS) and the first measurement of the charge asymmetry in $t\\bar{t}+W$ (ATLAS).","sentences":["Due to its high mass top quarks decay before top-flavoured hadrons can be formed.","This feature yields experimental access to the top quark polarization and production asymmetries.","The large top quark sample moreover enables measurements of other properties, such as the W-boson branching ratios and helicity, and fragmentation functions of the bottom quarks.","In this contribution, recent measurements of top-quark spin properties at the LHC are presented, including in particular the first evidence of the charge asymmetry in $t\\bar{t}$ production (ATLAS), the charge asymmetry in a boosted regime (CMS) and the first measurement of the charge asymmetry in $t\\bar{t}+W$ (ATLAS)."],"url":"http://arxiv.org/abs/2404.11452v1","category":"hep-ex"}
{"created":"2024-04-17 14:35:46","title":"Separating diameter two properties from their weak-star counterparts in spaces of Lipschitz functions","abstract":"We address some open problems concerning Banach spaces of real-valued Lipschitz functions. Specifically, we prove that the diameter two properties differ from their weak-star counterparts in these spaces. In particular, we establish the existence of dual Banach spaces lacking the symmetric strong diameter two property but possessing its weak-star counterpart. We show that there exists an octahedral Lipschitz-free space whose bidual is not octahedral. Furthermore, we prove that the Banach space of real-valued Lipschitz functions from any infinite subset of $\\ell_1$ possesses the symmetric strong diameter two property. These results are achieved by introducing new sufficient conditions, providing new examples and clarifying the status of known ones.","sentences":["We address some open problems concerning Banach spaces of real-valued Lipschitz functions.","Specifically, we prove that the diameter two properties differ from their weak-star counterparts in these spaces.","In particular, we establish the existence of dual Banach spaces lacking the symmetric strong diameter two property but possessing its weak-star counterpart.","We show that there exists an octahedral Lipschitz-free space whose bidual is not octahedral.","Furthermore, we prove that the Banach space of real-valued Lipschitz functions from any infinite subset of $\\ell_1$ possesses the symmetric strong diameter two property.","These results are achieved by introducing new sufficient conditions, providing new examples and clarifying the status of known ones."],"url":"http://arxiv.org/abs/2404.11430v1","category":"math.FA"}
{"created":"2024-04-17 14:14:09","title":"Anisotropic Nonsaturating Magnetoresistance Observed in HoMn$_6$Ge$_6$: A Kagome Dirac Semimetal","abstract":"We report the magnetic and magnetotransport properties and electronic band structure of the kagome Dirac semimetal HoMn$_6$Ge$_6$. Temperature-dependent electrical resistivity demonstrates various magnetic-transition-driven anomalies. Notably, a crossover from negative to positive magnetoresistance (MR) is observed at around 150 K. While the linear nonsaturating positive MR in the low-temperature region is mainly driven by the linear Dirac-like band dispersions as predicted by the first-principles calculations, the negative MR observed in the high-temperature region is due to the spin-flop type magnetic transition. Consistent with anisotropic Fermi surface topology, we observe anisotropic magnetoresistance at low temperatures. A significant anomalous Hall effect has been noticed at high temperatures in addition to a switching of the dominant charge carrier from electron to hole at around 215 K.","sentences":["We report the magnetic and magnetotransport properties and electronic band structure of the kagome Dirac semimetal HoMn$_6$Ge$_6$. Temperature-dependent electrical resistivity demonstrates various magnetic-transition-driven anomalies.","Notably, a crossover from negative to positive magnetoresistance (MR) is observed at around 150 K. While the linear nonsaturating positive MR in the low-temperature region is mainly driven by the linear Dirac-like band dispersions as predicted by the first-principles calculations, the negative MR observed in the high-temperature region is due to the spin-flop type magnetic transition.","Consistent with anisotropic Fermi surface topology, we observe anisotropic magnetoresistance at low temperatures.","A significant anomalous Hall effect has been noticed at high temperatures in addition to a switching of the dominant charge carrier from electron to hole at around 215 K."],"url":"http://arxiv.org/abs/2404.11414v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 14:02:30","title":"Revealing the spatial nature of sublattice symmetry","abstract":"The sublattice symmetry on a bipartite lattice is commonly regarded as the chiral symmetry in the AIII class of the tenfold Altland--Zirnbauer classification. Here, we reveal the spatial nature of sublattice symmetry, and show that this assertion holds only if the periodicity of primitive unit cells agrees with that of the sublattice labeling. In cases where the periodicity does not agree, sublattice symmetry is represented as a glide reflection in energy--momentum space, which inverts energy and simultaneously translates some $k$ by $\\pi$, leading to substantially different physics. Particularly, it introduces novel constraints on zero modes in semimetals and completely alters the classification table of topological insulators compared to class AIII. Notably, the dimensions corresponding to trivial and nontrivial classifications are switched, and the nontrivial classification becomes $\\mathbb{Z}_2$ instead of $\\mathbb{Z}$. We have applied these results to several models, including the Hofstadter model both with and without dimerization.","sentences":["The sublattice symmetry on a bipartite lattice is commonly regarded as the chiral symmetry in the AIII class of the tenfold Altland--Zirnbauer classification.","Here, we reveal the spatial nature of sublattice symmetry, and show that this assertion holds only if the periodicity of primitive unit cells agrees with that of the sublattice labeling.","In cases where the periodicity does not agree, sublattice symmetry is represented as a glide reflection in energy--momentum space, which inverts energy and simultaneously translates some $k$ by $\\pi$, leading to substantially different physics.","Particularly, it introduces novel constraints on zero modes in semimetals and completely alters the classification table of topological insulators compared to class AIII.","Notably, the dimensions corresponding to trivial and nontrivial classifications are switched, and the nontrivial classification becomes $\\mathbb{Z}_2$ instead of $\\mathbb{Z}$. We have applied these results to several models, including the Hofstadter model both with and without dimerization."],"url":"http://arxiv.org/abs/2404.11398v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-17 13:35:51","title":"The mind-brain relationship and the perspective of meaning","abstract":"We view the mind-body problem in terms of the two interconnected problems of phenomenal consciousness and mental causation, namely, how subjective conscious experience can arise from physical neurological processes and how conscious mental states can causally act upon the physical world. In order to address these problems, I develop here a non-physicalist framework that combines two apparently antithetical views: the materialist view of the mind as a product of the brain and the metaphysical view of consciousness rooted in an underlying hidden reality. I discuss how this framework resolves the problem of mental causation while being simultaneously consistent with fundamental physical principles. I will elucidate how the framework ties in to the perspective of meaning that acts as the bridge between physical neurological processes and the conscious mind. Moreover, we will see how both our awareness of the self and our representation of the external world are connected to this perspective.","sentences":["We view the mind-body problem in terms of the two interconnected problems of phenomenal consciousness and mental causation, namely, how subjective conscious experience can arise from physical neurological processes and how conscious mental states can causally act upon the physical world.","In order to address these problems, I develop here a non-physicalist framework that combines two apparently antithetical views: the materialist view of the mind as a product of the brain and the metaphysical view of consciousness rooted in an underlying hidden reality.","I discuss how this framework resolves the problem of mental causation while being simultaneously consistent with fundamental physical principles.","I will elucidate how the framework ties in to the perspective of meaning that acts as the bridge between physical neurological processes and the conscious mind.","Moreover, we will see how both our awareness of the self and our representation of the external world are connected to this perspective."],"url":"http://arxiv.org/abs/2404.11379v1","category":"q-bio.NC"}
{"created":"2024-04-17 12:41:01","title":"Channel Estimation in TDD Cell-free Scenario using OTFS Modulation","abstract":"Channel estimation techniques for orthogonal time frequency space (OTFS) modulation scheme are investigated. The orthogonal matching pursuit algorithm is investigated with and without side channel information and an efficient data placement is proposed alongside the pilot in the multi-user scenario based on impulse pilot-based estimation. Finally, the algorithms are compared in different multi-user scenarios with numerical results.","sentences":["Channel estimation techniques for orthogonal time frequency space (OTFS) modulation scheme are investigated.","The orthogonal matching pursuit algorithm is investigated with and without side channel information and an efficient data placement is proposed alongside the pilot in the multi-user scenario based on impulse pilot-based estimation.","Finally, the algorithms are compared in different multi-user scenarios with numerical results."],"url":"http://arxiv.org/abs/2404.11328v1","category":"cs.IT"}
{"created":"2024-04-17 12:22:44","title":"Autonomous aerial perching and unperching using omnidirectional tiltrotor and switching controller","abstract":"Aerial unperching of multirotors has received little attention as opposed to perching that has been investigated to elongate operation time. This study presents a new aerial robot capable of both perching and unperching autonomously on/from a ferromagnetic surface during flight, and a switching controller to avoid rotor saturation and mitigate overshoot during transition between free-flight and perching. To enable stable perching and unperching maneuvers on/from a vertical surface, a lightweight ($\\approx$ $1$ \\si{kg}), fully actuated tiltrotor that can hover at $90^\\circ$ pitch angle is first developed. We design a perching/unperching module composed of a single servomotor and a magnet, which is then mounted on the tiltrotor. A switching controller including exclusive control modes for transitions between free-flight and perching is proposed. Lastly, we propose a simple yet effective strategy to ensure robust perching in the presence of measurement and control errors and avoid collisions with the perching site immediately after unperching. We validate the proposed framework in experiments where the tiltrotor successfully performs perching and unperching on/from a vertical surface during flight. We further show effectiveness of the proposed transition mode in the switching controller by ablation studies where large overshoot and even collision with a perching site occur. To the best of the authors' knowledge, this work presents the first autonomous aerial unperching framework using a fully actuated tiltrotor.","sentences":["Aerial unperching of multirotors has received little attention as opposed to perching that has been investigated to elongate operation time.","This study presents a new aerial robot capable of both perching and unperching autonomously on/from a ferromagnetic surface during flight, and a switching controller to avoid rotor saturation and mitigate overshoot during transition between free-flight and perching.","To enable stable perching and unperching maneuvers on/from a vertical surface, a lightweight ($\\approx$ $1$ \\si{kg}), fully actuated tiltrotor that can hover at $90^\\circ$ pitch angle is first developed.","We design a perching/unperching module composed of a single servomotor and a magnet, which is then mounted on the tiltrotor.","A switching controller including exclusive control modes for transitions between free-flight and perching is proposed.","Lastly, we propose a simple yet effective strategy to ensure robust perching in the presence of measurement and control errors and avoid collisions with the perching site immediately after unperching.","We validate the proposed framework in experiments where the tiltrotor successfully performs perching and unperching on/from a vertical surface during flight.","We further show effectiveness of the proposed transition mode in the switching controller by ablation studies where large overshoot and even collision with a perching site occur.","To the best of the authors' knowledge, this work presents the first autonomous aerial unperching framework using a fully actuated tiltrotor."],"url":"http://arxiv.org/abs/2404.11310v1","category":"cs.RO"}
{"created":"2024-04-17 11:57:22","title":"Sizes of the Nucleon","abstract":"Evidences are updated and strengthened for the two-scales picture of low-energy nucleon structure as a compact `hard' valence quark core surrounded by a `soft' cloud of quark-antiquark pairs (the meson cloud). These considerations are quantified by a spectral analysis of the mean-squared radii associated with the isoscalar and isovector electric form factors of the nucleon. Further supporting arguments come from corresponding studies of the axial and mass form factors and their inferred radii. Separating low-mass (mesonic) and high-mass (short-range) contributions in the spectral representations of each of these form factors, we conclude that a central core with an r.m.s. radius of about 1/2 fm results consistently as the common feature in all cases. Implications are discussed for baryonic matter at densities beyond that of equilibrium nuclear matter.","sentences":["Evidences are updated and strengthened for the two-scales picture of low-energy nucleon structure as a compact `hard' valence quark core surrounded by a `soft' cloud of quark-antiquark pairs (the meson cloud).","These considerations are quantified by a spectral analysis of the mean-squared radii associated with the isoscalar and isovector electric form factors of the nucleon.","Further supporting arguments come from corresponding studies of the axial and mass form factors and their inferred radii.","Separating low-mass (mesonic) and high-mass (short-range) contributions in the spectral representations of each of these form factors, we conclude that a central core with an r.m.s.","radius of about 1/2 fm results consistently as the common feature in all cases.","Implications are discussed for baryonic matter at densities beyond that of equilibrium nuclear matter."],"url":"http://arxiv.org/abs/2404.11292v1","category":"nucl-th"}
{"created":"2024-04-17 11:51:22","title":"Skyrmion crystals stabilized by $\u03c9$-mesons","abstract":"We investigate the ground state crystalline structure of nuclear matter in the $\\omega$-meson variant of the Skyrme model. After minimizing energy with respect to variations of both the Skyrme field and the period lattice, we find four distinct periodic solutions which are similar to those found in the standard Skyrme model. We use these crystals to calculate coefficients in the Bethe--Weizs\\\"acker semi-empirical mass formula and the compression modulus of infinite nuclear matter, and find a significant improvement as compared with other variants of the Skyrme model.","sentences":["We investigate the ground state crystalline structure of nuclear matter in the $\\omega$-meson variant of the Skyrme model.","After minimizing energy with respect to variations of both the Skyrme field and the period lattice, we find four distinct periodic solutions which are similar to those found in the standard Skyrme model.","We use these crystals to calculate coefficients in the Bethe--Weizs\\\"acker semi-empirical mass formula and the compression modulus of infinite nuclear matter, and find a significant improvement as compared with other variants of the Skyrme model."],"url":"http://arxiv.org/abs/2404.11287v1","category":"hep-th"}
{"created":"2024-04-17 11:48:14","title":"Amplifying Main Memory-Based Timing Covert and Side Channels using Processing-in-Memory Operations","abstract":"The adoption of processing-in-memory (PiM) architectures has been gaining momentum because they provide high performance and low energy consumption by alleviating the data movement bottleneck. Yet, the security of such architectures has not been thoroughly explored. The adoption of PiM solutions provides a new way to directly access main memory, which can be potentially exploited by malicious user applications. We show that this new way to access main memory opens opportunities for high-throughput timing attack vectors that are hard-to-mitigate without significant performance overhead.   We introduce IMPACT, a set of high-throughput main memory-based timing attacks that leverage characteristics of PiM architectures to establish covert and side channels. IMPACT enables high-throughput communication and private information leakage. To achieve this, IMPACT (i) eliminates expensive cache bypassing steps required by processor-centric main memory and cache-based timing attacks and (ii) leverages the intrinsic parallelism of PiM operations. First, we showcase two covert-channel attack variants that run on the host CPU and leverage PiM architectures to gain direct and fast access to main memory and establish high-throughput communication covert channels. Second, we showcase a side-channel attack on a DNA sequence analysis application that leaks the private characteristics of a user's sample genome by leveraging PiM operations. Our results demonstrate that (i) our covert channels achieve up to 14.16 Mb/s communication throughput, which is 6.38x faster than the state-of-the-art main memory-based covert channels, and (ii) our side-channel attack allows the attacker to determine the properties of a sample genome at a throughput of 7.5 Mb/s with 96% accuracy. We discuss and evaluate several countermeasures for IMPACT to enable secure and robust PiM architectures.","sentences":["The adoption of processing-in-memory (PiM) architectures has been gaining momentum because they provide high performance and low energy consumption by alleviating the data movement bottleneck.","Yet, the security of such architectures has not been thoroughly explored.","The adoption of PiM solutions provides a new way to directly access main memory, which can be potentially exploited by malicious user applications.","We show that this new way to access main memory opens opportunities for high-throughput timing attack vectors that are hard-to-mitigate without significant performance overhead.   ","We introduce IMPACT, a set of high-throughput main memory-based timing attacks that leverage characteristics of PiM architectures to establish covert and side channels.","IMPACT enables high-throughput communication and private information leakage.","To achieve this, IMPACT (i) eliminates expensive cache bypassing steps required by processor-centric main memory and cache-based timing attacks and (ii) leverages the intrinsic parallelism of PiM operations.","First, we showcase two covert-channel attack variants that run on the host CPU and leverage PiM architectures to gain direct and fast access to main memory and establish high-throughput communication covert channels.","Second, we showcase a side-channel attack on a DNA sequence analysis application that leaks the private characteristics of a user's sample genome by leveraging PiM operations.","Our results demonstrate that (i) our covert channels achieve up to 14.16 Mb/s communication throughput, which is 6.38x faster than the state-of-the-art main memory-based covert channels, and (ii) our side-channel attack allows the attacker to determine the properties of a sample genome at a throughput of 7.5 Mb/s with 96% accuracy.","We discuss and evaluate several countermeasures for IMPACT to enable secure and robust PiM architectures."],"url":"http://arxiv.org/abs/2404.11284v1","category":"cs.CR"}
{"created":"2024-04-17 11:25:19","title":"Training Transformer Models by Wavelet Losses Improves Quantitative and Visual Performance in Single Image Super-Resolution","abstract":"Transformer-based models have achieved remarkable results in low-level vision tasks including image super-resolution (SR). However, early Transformer-based approaches that rely on self-attention within non-overlapping windows encounter challenges in acquiring global information. To activate more input pixels globally, hybrid attention models have been proposed. Moreover, training by solely minimizing pixel-wise RGB losses, such as L1, have been found inadequate for capturing essential high-frequency details. This paper presents two contributions: i) We introduce convolutional non-local sparse attention (NLSA) blocks to extend the hybrid transformer architecture in order to further enhance its receptive field. ii) We employ wavelet losses to train Transformer models to improve quantitative and subjective performance. While wavelet losses have been explored previously, showing their power in training Transformer-based SR models is novel. Our experimental results demonstrate that the proposed model provides state-of-the-art PSNR results as well as superior visual performance across various benchmark datasets.","sentences":["Transformer-based models have achieved remarkable results in low-level vision tasks including image super-resolution (SR).","However, early Transformer-based approaches that rely on self-attention within non-overlapping windows encounter challenges in acquiring global information.","To activate more input pixels globally, hybrid attention models have been proposed.","Moreover, training by solely minimizing pixel-wise RGB losses, such as L1, have been found inadequate for capturing essential high-frequency details.","This paper presents two contributions: i) We introduce convolutional non-local sparse attention (NLSA) blocks to extend the hybrid transformer architecture in order to further enhance its receptive field.","ii)","We employ wavelet losses to train Transformer models to improve quantitative and subjective performance.","While wavelet losses have been explored previously, showing their power in training Transformer-based SR models is novel.","Our experimental results demonstrate that the proposed model provides state-of-the-art PSNR results as well as superior visual performance across various benchmark datasets."],"url":"http://arxiv.org/abs/2404.11273v1","category":"eess.IV"}
{"created":"2024-04-17 11:23:32","title":"Diagonalizing the Jaynes-Cummings Hamiltonian and Jaynes-Cummings coherent states","abstract":"We determine the form of the unitary transformation that diagonalizes the Jaynes-Cummings Hamiltonian. This leads to operators the action of which has a simple interpretation in terms of the dressed states, the energy eigenstates. This suggests a set of coherent states and spin coherent states based on the dressed states.","sentences":["We determine the form of the unitary transformation that diagonalizes the Jaynes-Cummings Hamiltonian.","This leads to operators the action of which has a simple interpretation in terms of the dressed states, the energy eigenstates.","This suggests a set of coherent states and spin coherent states based on the dressed states."],"url":"http://arxiv.org/abs/2404.11272v1","category":"quant-ph"}
{"created":"2024-04-17 11:09:12","title":"Photophysics of defect-passivated quasi-2D (PEA)2PbBr4 perovskite using an organic small-molecule","abstract":"2D Ruddlesden - Popper perovskites are promising candidates for energy harvesting applications due to their tunable optical properties and excellent ambient stability. Moreover, they are solution-processable and compatible with upscalable manufacturing via various printing techniques. Unfortunately, such methods often induce large degrees of heterogeneity due to poorly controlled crystallization. Here, we address this issue by blending the well-known 2D perovskite (PEA)2PbBr4 with an organic small-molecule, namely C8-BTBT, employed as an additive with different blending ratios. Using terahertz (THz) absorption and temperature-dependent photoluminescence (PL) spectroscopy techniques we observe that with the C8-BTBT additive the photophysical properties are altered while the perovskite structure in the film remains unaffected. More precisely, the inclusion of trace amounts of C8-BTBT in the hybrid films results in defect passivation at perovskite platelet boundaries and at the surfaces, as indicated by increased carrier lifetimes and substantially increased photoluminescence quantum yields (PLQY). This in turn improves the responsivity of photodetectors using the 2D perovskite as active layer. Our study highlights a straightforward strategy for fabricating high-quality 2D perovskites via large-area processing techniques.","sentences":["2D Ruddlesden - Popper perovskites are promising candidates for energy harvesting applications due to their tunable optical properties and excellent ambient stability.","Moreover, they are solution-processable and compatible with upscalable manufacturing via various printing techniques.","Unfortunately, such methods often induce large degrees of heterogeneity due to poorly controlled crystallization.","Here, we address this issue by blending the well-known 2D perovskite (PEA)2PbBr4 with an organic small-molecule, namely C8-BTBT, employed as an additive with different blending ratios.","Using terahertz (THz) absorption and temperature-dependent photoluminescence (PL) spectroscopy techniques we observe that with the C8-BTBT additive the photophysical properties are altered while the perovskite structure in the film remains unaffected.","More precisely, the inclusion of trace amounts of C8-BTBT in the hybrid films results in defect passivation at perovskite platelet boundaries and at the surfaces, as indicated by increased carrier lifetimes and substantially increased photoluminescence quantum yields (PLQY).","This in turn improves the responsivity of photodetectors using the 2D perovskite as active layer.","Our study highlights a straightforward strategy for fabricating high-quality 2D perovskites via large-area processing techniques."],"url":"http://arxiv.org/abs/2404.11259v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 10:59:28","title":"\"That's our game!\" : Reflections on co-designing a robotic game with neurodiverse children","abstract":"Many neurodivergent (ND) children are integrated into mainstream schools alongside their neurotypical (NT) peers. However, they often face social exclusion, which may have lifelong effects. Inclusive play activities can be a strong driver of inclusion. Unfortunately, games designed for the specific needs of neurodiverse groups, those that include neurodivergent and neurotypical individuals, are scarce. Given the potential of robots as engaging devices, we led a 6-month co-design process to build an inclusive and entertaining robotic game for neurodiverse classrooms. We first interviewed neurodivergent adults and educators to identify the barriers and facilitators for including neurodivergent children in mainstream classrooms. Then, we conducted five co-design sessions, engaging four neurodiverse classrooms with 81 children (19 neurodivergent). We present a reflection on our co-design process and the resulting robotic game through the lens of Self-Determination Theory, discussing how our methodology supported the intrinsic motivations of neurodivergent children.","sentences":["Many neurodivergent (ND) children are integrated into mainstream schools alongside their neurotypical (NT) peers.","However, they often face social exclusion, which may have lifelong effects.","Inclusive play activities can be a strong driver of inclusion.","Unfortunately, games designed for the specific needs of neurodiverse groups, those that include neurodivergent and neurotypical individuals, are scarce.","Given the potential of robots as engaging devices, we led a 6-month co-design process to build an inclusive and entertaining robotic game for neurodiverse classrooms.","We first interviewed neurodivergent adults and educators to identify the barriers and facilitators for including neurodivergent children in mainstream classrooms.","Then, we conducted five co-design sessions, engaging four neurodiverse classrooms with 81 children (19 neurodivergent).","We present a reflection on our co-design process and the resulting robotic game through the lens of Self-Determination Theory, discussing how our methodology supported the intrinsic motivations of neurodivergent children."],"url":"http://arxiv.org/abs/2404.11252v1","category":"cs.HC"}
{"created":"2024-04-17 10:53:51","title":"Dynamic Local Symmetry Fluctuations of Electron Density in Halide Perovskites","abstract":"Metal halide perovskites have emerged as an exciting class of materials for applications in solar energy harvesting, optical devices, catalysis, and other photophysical applications. Many of the exciting properties of halide perovskites are tied to their soft, dynamic, and anharmonic lattice. In particular, the precise coupling between anharmonic lattice dynamics and electronic fluctuations is not completely understood. To build an understanding of this coupling, we use ab initio molecular dynamics simulations supplemented by the calculation of maximally localized Wannier functions to carry out a dynamic group theory analysis of local electron density fluctuations and how these fluctuations are coupled to lattice fluctuations in the model inorganic halide perovskite CsSnBr3. We detail symmetry-dependent couplings between vibrational modes, including octahedral tilting. Importantly, we suggest that the large anharmonicity of some of the vibrational modes in CsSnBr3 result from electron rotation--nuclear translation coupling, in analogy to rotation--translation coupling effects in molecular plastic crystals. We also identify electronic fluctuations in the Cs cation that couple to distortions in the surrounding Sn-Br cubic coordination environment. We anticipate that our approach and resulting insights into electronic fluctuations will aid in further understanding the role of the fluctuating lattice in determining important physical properties of halide perovskites and beyond.","sentences":["Metal halide perovskites have emerged as an exciting class of materials for applications in solar energy harvesting, optical devices, catalysis, and other photophysical applications.","Many of the exciting properties of halide perovskites are tied to their soft, dynamic, and anharmonic lattice.","In particular, the precise coupling between anharmonic lattice dynamics and electronic fluctuations is not completely understood.","To build an understanding of this coupling, we use ab initio molecular dynamics simulations supplemented by the calculation of maximally localized Wannier functions to carry out a dynamic group theory analysis of local electron density fluctuations and how these fluctuations are coupled to lattice fluctuations in the model inorganic halide perovskite CsSnBr3.","We detail symmetry-dependent couplings between vibrational modes, including octahedral tilting.","Importantly, we suggest that the large anharmonicity of some of the vibrational modes in CsSnBr3 result from electron rotation--nuclear translation coupling, in analogy to rotation--translation coupling effects in molecular plastic crystals.","We also identify electronic fluctuations in the Cs cation that couple to distortions in the surrounding Sn-Br cubic coordination environment.","We anticipate that our approach and resulting insights into electronic fluctuations will aid in further understanding the role of the fluctuating lattice in determining important physical properties of halide perovskites and beyond."],"url":"http://arxiv.org/abs/2404.11247v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 09:58:59","title":"Predicting isovector charmonium-like states from X(3872) properties","abstract":"Using chiral effective field theory, we predict that there must be isovector charmonium-like $D\\bar D^*$ hadronic molecules with $J^{PC}=1^{++}$ denoted as $W_{c1}$. The inputs are the properties of the $X(3872)$, including its mass and the ratio of its branching fractions of decays into $J/\\psi\\rho^0$ and $J/\\psi\\omega$. The predicted states are virtual state poles of the scattering matrix, pointing at a molecular nature of the $X(3872)$ as well as its spin partners. They should show up as either a mild cusp or dip at the $D\\bar D^*$ thresholds, explaining why they are elusive in experiments. The so far negative observation also indicates that the $X(3872)$ is either a bound state with non-vanishing binding energy or a virtual state, only in these cases the $X(3872)$ signal dominates over that from the $W_{c1}^0$. The pole positions are $3865.3^{+4.2}_{-7.4}- i 0.15^{+0.04}_{-0.03}$ MeV for $W_{c1}^0$ and $3866.9^{+4.6}_{-7.7}- i (0.07\\pm0.01)$ MeV for $W_{c1}^\\pm$. The findings imply that the peak in the $J/\\psi\\pi^+\\pi^-$ invariant mass distribution is not purely from the $X(3872)$ but contains contributions from $W_{c1}^0$ predicted here. The states should have isovector heavy quark spin partners with $J^{PC}=0^{++}$, $2^{++}$ and $1^{+-}$, with the last one corresponding to $Z_c$. We suggest to search for the charged $0^{++}$, $1^{++}$ and $2^{++}$ states in $J/\\psi\\pi^\\pm \\pi^0$.","sentences":["Using chiral effective field theory, we predict that there must be isovector charmonium-like $D\\bar D^*$ hadronic molecules with $J^{PC}=1^{++}$ denoted as $W_{c1}$. The inputs are the properties of the $X(3872)$, including its mass and the ratio of its branching fractions of decays into $J/\\psi\\rho^0$ and $J/\\psi\\omega$. The predicted states are virtual state poles of the scattering matrix, pointing at a molecular nature of the $X(3872)$ as well as its spin partners.","They should show up as either a mild cusp or dip at the $D\\bar D^*$ thresholds, explaining why they are elusive in experiments.","The so far negative observation also indicates that the $X(3872)$ is either a bound state with non-vanishing binding energy or a virtual state, only in these cases the $X(3872)$ signal dominates over that from the $W_{c1}^0$.","The pole positions are $3865.3^{+4.2}_{-7.4}- i 0.15^{+0.04}_{-0.03}$ MeV for $W_{c1}^0$ and $3866.9^{+4.6}_{-7.7}- i (0.07\\pm0.01)$ MeV for $W_{c1}^\\pm$. The findings imply that the peak in the $J/\\psi\\pi^+\\pi^-$ invariant mass distribution is not purely from the $X(3872)$ but contains contributions from $W_{c1}^0$ predicted here.","The states should have isovector heavy quark spin partners with $J^{PC}=0^{++}$, $2^{++}$ and $1^{+-}$, with the last one corresponding to $Z_c$. We suggest to search for the charged $0^{++}$, $1^{++}$ and $2^{++}$ states in $J/\\psi\\pi^\\pm \\pi^0$."],"url":"http://arxiv.org/abs/2404.11215v1","category":"hep-ph"}
{"created":"2024-04-17 09:08:42","title":"The Writing is on the Wall: Analyzing the Boom of Inscriptions and its Impact on Rollup Performance and Cost Efficiency","abstract":"Late 2023 witnessed significant user activity on EVM chains, resulting in a surge in transaction activity and putting many rollups into the first live test. While some rollups performed well, some others experienced downtime during this period, affecting transaction finality time and gas fees. To address the lack of empirical research on rollups, we perform the first study during a heightened activity during the late 2023 transaction boom, as attributed to inscriptions - a novel technique that enables NFT and ERC-20 token creation on Bitcoin and other blockchains. We observe that minting inscription-based meme tokens on zkSync Era allows for trading at a fraction of the costs, compared to the Bitcoin or Ethereum networks. We also found that the increased transaction activity, over 99% attributed to the minting of new inscription tokens, positively affected other users of zkSync Era, resulting in lowered gas fees. Unlike L1 blockchains, ZK rollups may experience lower gas fees with increased transaction volume. Lastly, the introduction of blobs - a form of temporary data storage - decreased the gas costs of Ethereum rollups, but also raised a number of questions about the security of inscription-based tokens.","sentences":["Late 2023 witnessed significant user activity on EVM chains, resulting in a surge in transaction activity and putting many rollups into the first live test.","While some rollups performed well, some others experienced downtime during this period, affecting transaction finality time and gas fees.","To address the lack of empirical research on rollups, we perform the first study during a heightened activity during the late 2023 transaction boom, as attributed to inscriptions - a novel technique that enables NFT and ERC-20 token creation on Bitcoin and other blockchains.","We observe that minting inscription-based meme tokens on zkSync Era allows for trading at a fraction of the costs, compared to the Bitcoin or Ethereum networks.","We also found that the increased transaction activity, over 99% attributed to the minting of new inscription tokens, positively affected other users of zkSync Era, resulting in lowered gas fees.","Unlike L1 blockchains, ZK rollups may experience lower gas fees with increased transaction volume.","Lastly, the introduction of blobs - a form of temporary data storage - decreased the gas costs of Ethereum rollups, but also raised a number of questions about the security of inscription-based tokens."],"url":"http://arxiv.org/abs/2404.11189v1","category":"cs.CR"}
{"created":"2024-04-17 09:05:17","title":"Transverse Inscription of Silicon Waveguides by Picosecond Laser Pulses","abstract":"In this paper, picosecond laser inscription of segmented waveguides in crystalline silicon based on a deterministic single-pulse modification process is demonstrated.Pulses of 43 ps duration at 1.55 ${\\mu}$m wavelength are used to transversely inscribe periodic structures with a pulse-to-pulse pitch of around 2 ${\\mu}$m. Infrared shadowgraphy images and Raman spectroscopy measurements indicate that the modifications exhibit a spherical shape. Characterization of waveguide performance at 1.55 ${\\mu}$m for various pulse energies and periods is carried out. Direct comparison with numerical simulations confirms the presence of graded index waveguides, encompassing a micrometer core size and a maximum refractive index change of around $7\\times 10^{-3}$. This short-pulse inscription approach can pave the way for three-dimensional integrated photonic devices in the bulk of silicon.","sentences":["In this paper, picosecond laser inscription of segmented waveguides in crystalline silicon based on a deterministic single-pulse modification process is demonstrated.","Pulses of 43 ps duration at 1.55 ${\\mu}$m wavelength are used to transversely inscribe periodic structures with a pulse-to-pulse pitch of around 2 ${\\mu}$m.","Infrared shadowgraphy images and Raman spectroscopy measurements indicate that the modifications exhibit a spherical shape.","Characterization of waveguide performance at 1.55 ${\\mu}$m for various pulse energies and periods is carried out.","Direct comparison with numerical simulations confirms the presence of graded index waveguides, encompassing a micrometer core size and a maximum refractive index change of around $7\\times 10^{-3}$. This short-pulse inscription approach can pave the way for three-dimensional integrated photonic devices in the bulk of silicon."],"url":"http://arxiv.org/abs/2404.11187v1","category":"physics.optics"}
{"created":"2024-04-17 08:56:34","title":"Hubble Expansion Signature on Simulated Halo Density Profiles: A Path to Observing the Turnaround Radius","abstract":"Density profiles are important tools in galaxy cluster research, offering insights into clusters dynamical states and their relationship with the broader Universe. While these profiles provide valuable information about the matter content of the Universe, their utility in understanding its dark energy component has remained limited due a lack of tools allowing us to study the transition from cluster portions that are relaxed and infalling, to those that are merging with the Hubble flow. In this work we investigate signatures of this transition in stacked density profiles of simulated cluster-sized halos at different redshifts. To highlight the Hubble flow around clusters we use their turnaround radius to normalize stacked simulated density profiles and calculate their logarithmic slope. Then, we complement our analysis by modeling the outer portions of these profiles assuming Gaussian early Universe statistics and spherical collapse without shell-crossing. We find the logarithmic slope of median cluster density profiles beyond the turnaround radius - where the Hubble flow dominates - to be Universal and well described by our model. Importantly, we find the slope of the profiles to diverge from the SCM prediction from within the turnaround radius where the actual profiles exhibit caustics which give rise to the splashback feature. We suggest utilizing this divergence from the spherical collapse model as a method to identify the turnaround radius in stacked cluster density profiles, offering a new perspective on understanding cluster dynamics and their cosmological implications.","sentences":["Density profiles are important tools in galaxy cluster research, offering insights into clusters dynamical states and their relationship with the broader Universe.","While these profiles provide valuable information about the matter content of the Universe, their utility in understanding its dark energy component has remained limited due a lack of tools allowing us to study the transition from cluster portions that are relaxed and infalling, to those that are merging with the Hubble flow.","In this work we investigate signatures of this transition in stacked density profiles of simulated cluster-sized halos at different redshifts.","To highlight the Hubble flow around clusters we use their turnaround radius to normalize stacked simulated density profiles and calculate their logarithmic slope.","Then, we complement our analysis by modeling the outer portions of these profiles assuming Gaussian early Universe statistics and spherical collapse without shell-crossing.","We find the logarithmic slope of median cluster density profiles beyond the turnaround radius - where the Hubble flow dominates - to be Universal and well described by our model.","Importantly, we find the slope of the profiles to diverge from the SCM prediction from within the turnaround radius where the actual profiles exhibit caustics which give rise to the splashback feature.","We suggest utilizing this divergence from the spherical collapse model as a method to identify the turnaround radius in stacked cluster density profiles, offering a new perspective on understanding cluster dynamics and their cosmological implications."],"url":"http://arxiv.org/abs/2404.11183v1","category":"astro-ph.CO"}
{"created":"2024-04-17 08:50:29","title":"Causal Deconfounding via Confounder Disentanglement for Dual-Target Cross-Domain Recommendation","abstract":"In recent years, dual-target Cross-Domain Recommendation (CDR) has been proposed to capture comprehensive user preferences in order to ultimately enhance the recommendation accuracy in both data-richer and data-sparser domains simultaneously. However, in addition to users' true preferences, the user-item interactions might also be affected by confounders (e.g., free shipping, sales promotion). As a result, dual-target CDR has to meet two challenges: (1) how to effectively decouple observed confounders, including single-domain confounders and cross-domain confounders, and (2) how to preserve the positive effects of observed confounders on predicted interactions, while eliminating their negative effects on capturing comprehensive user preferences. To address the above two challenges, we propose a Causal Deconfounding framework via Confounder Disentanglement for dual-target Cross-Domain Recommendation, called CD2CDR. In CD2CDR, we first propose a confounder disentanglement module to effectively decouple observed single-domain and cross-domain confounders. We then propose a causal deconfounding module to preserve the positive effects of such observed confounders and eliminate their negative effects via backdoor adjustment, thereby enhancing the recommendation accuracy in each domain. Extensive experiments conducted on five real-world datasets demonstrate that CD2CDR significantly outperforms the state-of-the-art methods.","sentences":["In recent years, dual-target Cross-Domain Recommendation (CDR) has been proposed to capture comprehensive user preferences in order to ultimately enhance the recommendation accuracy in both data-richer and data-sparser domains simultaneously.","However, in addition to users' true preferences, the user-item interactions might also be affected by confounders (e.g., free shipping, sales promotion).","As a result, dual-target CDR has to meet two challenges: (1) how to effectively decouple observed confounders, including single-domain confounders and cross-domain confounders, and (2) how to preserve the positive effects of observed confounders on predicted interactions, while eliminating their negative effects on capturing comprehensive user preferences.","To address the above two challenges, we propose a Causal Deconfounding framework via Confounder Disentanglement for dual-target Cross-Domain Recommendation, called CD2CDR.","In CD2CDR, we first propose a confounder disentanglement module to effectively decouple observed single-domain and cross-domain confounders.","We then propose a causal deconfounding module to preserve the positive effects of such observed confounders and eliminate their negative effects via backdoor adjustment, thereby enhancing the recommendation accuracy in each domain.","Extensive experiments conducted on five real-world datasets demonstrate that CD2CDR significantly outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.11180v1","category":"cs.IR"}
{"created":"2024-04-17 08:13:54","title":"Extraction of nonperturbative parameters for $D^{(*)}$ mesons from lattice data","abstract":"Recent data for the masses of $D$ and $D^*$ mesons determined using methods of lattice QCD for several values of the charm quark mass different from its physical mass are analysed in Heavy Quark Effective Theory. Nonperturbative parameters are extracted that arise at order ${\\cal O}(1/m_c)$ in the heavy-quark mass expansion of a heavy-light meson mass. The determined parameters are used to establish the charm quark masses corresponding to the employed lattice sets.","sentences":["Recent data for the masses of $D$ and $D^*$ mesons determined using methods of lattice QCD for several values of the charm quark mass different from its physical mass are analysed in Heavy Quark Effective Theory.","Nonperturbative parameters are extracted that arise at order ${\\cal O}(1/m_c)$ in the heavy-quark mass expansion of a heavy-light meson mass.","The determined parameters are used to establish the charm quark masses corresponding to the employed lattice sets."],"url":"http://arxiv.org/abs/2404.11158v1","category":"hep-lat"}
{"created":"2024-04-17 07:29:37","title":"Ion acceleration from micrometric targets immersed in an intense laser field","abstract":"We report on an experimental study of proton acceleration by intense laser irradiation of micrometric bar targets, whose dimensions are transversely immersed in the laser focal volume and longitudinally smaller than half its wavelength. Using only 120 mJ of laser energy, we recorded proton energies in excess of 6 MeV, exceeding those reported with any other experimental method by a factor of 3. 3D particle-in-cell simulations revealed that the efficient energy transfer from the diffracted laser fields to electrons on both sides of the target, combined with its reduced surface area, results in a thicker electron sheath and higher accelerating electric field gradients. We demonstrated numerically how this ion acceleration technique opens up the possibility of laser ion acceleration in multiple stages, allowing manipulation of the accelerated ion spectrum by optical means.","sentences":["We report on an experimental study of proton acceleration by intense laser irradiation of micrometric bar targets, whose dimensions are transversely immersed in the laser focal volume and longitudinally smaller than half its wavelength.","Using only 120 mJ of laser energy, we recorded proton energies in excess of 6 MeV, exceeding those reported with any other experimental method by a factor of 3.","3D particle-in-cell simulations revealed that the efficient energy transfer from the diffracted laser fields to electrons on both sides of the target, combined with its reduced surface area, results in a thicker electron sheath and higher accelerating electric field gradients.","We demonstrated numerically how this ion acceleration technique opens up the possibility of laser ion acceleration in multiple stages, allowing manipulation of the accelerated ion spectrum by optical means."],"url":"http://arxiv.org/abs/2404.11135v1","category":"physics.plasm-ph"}
{"created":"2024-04-17 06:56:46","title":"Quasinormal modes and their excitation beyond general relativity","abstract":"The response of black holes to small perturbations is known to be partially described by a superposition of quasinormal modes. Despite their importance to enable strong-field tests of gravity, little to nothing is known about what overtones and quasinormal-mode amplitudes are like for black holes in extensions to general relativity. We take a first step in this direction and study what is arguably the simplest model that allows first-principle calculations to be made: a nonrotating black hole in an effective-field-theory extension of general relativity with cubic-in-curvature terms. Using a phase-amplitude scheme that uses analytical continuation and the Pr\\\"ufer transformation, we compute, for the first time, the quasinormal overtone frequencies (in this theory) and quasinormal-mode excitation factors (in any theory beyond general relativity). We find that the overtone quasinormal frequencies and their excitation factors are more sensitive than the fundamental mode to the lengthscale $l$ introduced by the higher-derivative terms in the effective field theory. We argue that a description of all overtones cannot be made within the regime of validity of the effective field theory, and we conjecture that this is a general feature of any extension to general relativity that introduces a new lengthscale. We also find that a parametrization of the modifications to the general-relativistic quasinormal frequencies in terms of the ratio between $l$ and the black hole's mass is somewhat inadequate, and we propose a better alternative. As an application, we perform a preliminary study of the implications of the breakdown, in the effective field theory, of the equivalence between the quasinormal mode spectra associated to metric perturbations of polar and axial parity of the Schwarzschild black hole in general relativity. We also present a simple justification for the loss of isospectrality.","sentences":["The response of black holes to small perturbations is known to be partially described by a superposition of quasinormal modes.","Despite their importance to enable strong-field tests of gravity, little to nothing is known about what overtones and quasinormal-mode amplitudes are like for black holes in extensions to general relativity.","We take a first step in this direction and study what is arguably the simplest model that allows first-principle calculations to be made: a nonrotating black hole in an effective-field-theory extension of general relativity with cubic-in-curvature terms.","Using a phase-amplitude scheme that uses analytical continuation and the Pr\\\"ufer transformation, we compute, for the first time, the quasinormal overtone frequencies (in this theory) and quasinormal-mode excitation factors (in any theory beyond general relativity).","We find that the overtone quasinormal frequencies and their excitation factors are more sensitive than the fundamental mode to the lengthscale $l$ introduced by the higher-derivative terms in the effective field theory.","We argue that a description of all overtones cannot be made within the regime of validity of the effective field theory, and we conjecture that this is a general feature of any extension to general relativity that introduces a new lengthscale.","We also find that a parametrization of the modifications to the general-relativistic quasinormal frequencies in terms of the ratio between $l$ and the black hole's mass is somewhat inadequate, and we propose a better alternative.","As an application, we perform a preliminary study of the implications of the breakdown, in the effective field theory, of the equivalence between the quasinormal mode spectra associated to metric perturbations of polar and axial parity of the Schwarzschild black hole in general relativity.","We also present a simple justification for the loss of isospectrality."],"url":"http://arxiv.org/abs/2404.11110v1","category":"gr-qc"}
{"created":"2024-04-17 06:33:44","title":"Interplay between magnetic and lattice excitations and emergent multiple phase transitions in MnPSe3-xSx","abstract":"The intricate interplay between spin and lattice degrees of freedom in two-dimensional magnetic materials plays a pivotal role in modifying their magnetic characteristics, engendering hybrid quasiparticles, and implementing functional devices. Herein, we present our comprehensive and in-depth investigations on magnetic and lattice excitations of MnPSe3-xSx (x = 0, 0.5, and 1.5) alloys, utilizing temperature- and polarization-dependent Raman scattering. Our experimental results reveal the occurrence of multiple phase transitions, evidenced by notable changes in phonon self-energy and the appearance or splitting of phonon modes. These emergent phases are tied to the development of long and short-range spin-spin correlations, as well as to spin reorientations or magnetic instabilities. Our analysis of two-magnon excitations as a function of temperature and composition showcases their hybridization with phonons whose degree weakens with increasing x. Moreover, the suppression of spin-dependent phonon intensity in chemically most-disordered MnPSe3-xSx (x = 1.5) suggests that chalcogen substitution offers a control knob of tuning spin and phonon dynamics by modulating concurrently superexchange pathways and a degree of trigonal distortions.","sentences":["The intricate interplay between spin and lattice degrees of freedom in two-dimensional magnetic materials plays a pivotal role in modifying their magnetic characteristics, engendering hybrid quasiparticles, and implementing functional devices.","Herein, we present our comprehensive and in-depth investigations on magnetic and lattice excitations of MnPSe3-xSx (x = 0, 0.5, and 1.5) alloys, utilizing temperature- and polarization-dependent Raman scattering.","Our experimental results reveal the occurrence of multiple phase transitions, evidenced by notable changes in phonon self-energy and the appearance or splitting of phonon modes.","These emergent phases are tied to the development of long and short-range spin-spin correlations, as well as to spin reorientations or magnetic instabilities.","Our analysis of two-magnon excitations as a function of temperature and composition showcases their hybridization with phonons whose degree weakens with increasing x.","Moreover, the suppression of spin-dependent phonon intensity in chemically most-disordered MnPSe3-xSx (x = 1.5) suggests that chalcogen substitution offers a control knob of tuning spin and phonon dynamics by modulating concurrently superexchange pathways and a degree of trigonal distortions."],"url":"http://arxiv.org/abs/2404.11099v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 03:44:13","title":"Can mid-infrared spectral energy distribution quantitatively trace protoplanetary disk evolution?","abstract":"Infrared (IR) spectral energy distribution (SED) is the major tracer of protoplanetary disks. It was recently proposed to use the mid-IR (MIR) SED slope $\\alpha$ defined between 2-24$\\mu$m as a potential quantitative tracer of disk age. We critically examine the viability of this idea and confront it with additional statistics of IR luminosities and SED shapes. We point out that, because the statistical properties of most of the complicated physical factors involved in disk evolution are still poorly understood in a quantitative sense, the only viable way is to assume them to be random so that an idealized `average disk' can be defined, which allows the $\\alpha$ histogram to trace its age. We confirm that the statistics of the zeroth order (luminosity), first order (slope $\\alpha$) and second order characteristics (concavity) of the observed MIR SED indeed carry useful information upon the evolutionary processes of the `average disk' and provide useful constraints to future disk population synthesis modeling. We also demonstrate that intrinsic diversities in MIR SED shapes and luminosities are always large at the level of individual stars so that the application of the evolutionary path of the `average disk' to individual stars must be done with care. The data of all curves in plots are provided on GitHub.","sentences":["Infrared (IR) spectral energy distribution (SED) is the major tracer of protoplanetary disks.","It was recently proposed to use the mid-IR (MIR) SED slope $\\alpha$ defined between 2-24$\\mu$m as a potential quantitative tracer of disk age.","We critically examine the viability of this idea and confront it with additional statistics of IR luminosities and SED shapes.","We point out that, because the statistical properties of most of the complicated physical factors involved in disk evolution are still poorly understood in a quantitative sense, the only viable way is to assume them to be random so that an idealized `average disk' can be defined, which allows the $\\alpha$ histogram to trace its age.","We confirm that the statistics of the zeroth order (luminosity), first order (slope $\\alpha$) and second order characteristics (concavity) of the observed MIR SED indeed carry useful information upon the evolutionary processes of the `average disk' and provide useful constraints to future disk population synthesis modeling.","We also demonstrate that intrinsic diversities in MIR SED shapes and luminosities are always large at the level of individual stars so that the application of the evolutionary path of the `average disk' to individual stars must be done with care.","The data of all curves in plots are provided on GitHub."],"url":"http://arxiv.org/abs/2404.11048v1","category":"astro-ph.SR"}
{"created":"2024-04-17 02:56:11","title":"Diagnosing Emergent Isotropy in Anisotropic Holographic Systems using Quantum Information measures","abstract":"We investigate the behavior of holographic quantum information measures including Holographic Entanglement Entropy, Entanglement Wedge Cross-section, and butterfly velocity in a holographic p-wave superconductor model. We find a novel critical temperature, $T_{II}$, where an isotropic point emerges, signaling a shift from anisotropy to isotropy within the bulk geometry. We find that these measures respond differently to anisotropy, with HEE and EWCS revealing effects at various scales, while butterfly velocity is a precise probe of horizon anisotropy. These findings deepen our understanding of anisotropy in correlated systems, highlight the sensitivity of quantum information measures to spacetime structure, and pave the way for exploring quantum correlations in anisotropic phases.","sentences":["We investigate the behavior of holographic quantum information measures including Holographic Entanglement Entropy, Entanglement Wedge Cross-section, and butterfly velocity in a holographic p-wave superconductor model.","We find a novel critical temperature, $T_{II}$, where an isotropic point emerges, signaling a shift from anisotropy to isotropy within the bulk geometry.","We find that these measures respond differently to anisotropy, with HEE and EWCS revealing effects at various scales, while butterfly velocity is a precise probe of horizon anisotropy.","These findings deepen our understanding of anisotropy in correlated systems, highlight the sensitivity of quantum information measures to spacetime structure, and pave the way for exploring quantum correlations in anisotropic phases."],"url":"http://arxiv.org/abs/2404.11021v1","category":"hep-th"}
{"created":"2024-04-17 02:42:31","title":"Search for synchrotron emission from secondary electrons of proton-proton interaction in Galactic PeVatron candidate HESS J1641$-$463","abstract":"HESS J1641-463 is an unidentified gamma-ray source with a hard TeV gamma-ray spectrum, and thus it has been proposed to be a possible candidate for cosmic ray (CR) accelerators up to PeV energies (a PeVatron candidate). The source spatially coincides with the radio supernova remnant (SNR) G338.5+0.1, but has not yet been fully explored in the X-ray band. We analyzed newly taken NuSTAR data, pointing at HESS J1641-463, with 82 ks effective exposure time. There is no apparent X-ray counterpart of HESS J1641-463, while nearby stellar cluster, Mercer 81, and stray-light X-rays are detected. Combined with the archival Chandra data, partially covering the source, we derived an upper limit of $\\sim 6\\times 10^{-13}$ erg cm$^{-2}$ s$^{-1}$ in 2-10 keV ($\\sim 3\\times 10^{-13}$ erg cm$^{-2}$ s$^{-1}$ in 10-20 keV). If the gamma-ray emission is originated from decay of $\\pi^0$ mesons produced in interactions between CR protons and ambient materials, secondary electrons in the proton-proton interactions can potentially emit synchrotron photons in the X-ray band, which can be tested by our X-ray observations. Although the obtained X-ray upper limits cannot place a constraint on the primary proton spectrum, it will be possible with a future hard X-ray mission.","sentences":["HESS J1641-463 is an unidentified gamma-ray source with a hard TeV gamma-ray spectrum, and thus it has been proposed to be a possible candidate for cosmic ray (CR) accelerators up to PeV energies (a PeVatron candidate).","The source spatially coincides with the radio supernova remnant (SNR) G338.5","+0.1, but has not yet been fully explored in the X-ray band.","We analyzed newly taken NuSTAR data, pointing at HESS J1641-463, with 82 ks effective exposure time.","There is no apparent X-ray counterpart of HESS J1641-463, while nearby stellar cluster, Mercer 81, and stray-light X-rays are detected.","Combined with the archival Chandra data, partially covering the source, we derived an upper limit of $\\sim 6\\times 10^{-13}$ erg cm$^{-2}$ s$^{-1}$ in 2-10 keV ($\\sim 3\\times 10^{-13}$ erg cm$^{-2}$ s$^{-1}$ in 10-20 keV).","If the gamma-ray emission is originated from decay of $\\pi^0$ mesons produced in interactions between CR protons and ambient materials, secondary electrons in the proton-proton interactions can potentially emit synchrotron photons in the X-ray band, which can be tested by our X-ray observations.","Although the obtained X-ray upper limits cannot place a constraint on the primary proton spectrum, it will be possible with a future hard X-ray mission."],"url":"http://arxiv.org/abs/2404.11012v1","category":"astro-ph.HE"}
{"created":"2024-04-17 02:19:57","title":"Modulation of the Octahedral Structure and Potential Superconductivity of La3Ni2O7 at Ambient Pressure by Compressive Strain","abstract":"Superconductivity at Tc = 80 K has recently been reported above 14 GPa in La3Ni2O7, which thus introduces a new family of high-temperature superconductors. Using a first-principles calculation with Coulomb repulsion, we unveil a surprising new route to obtain superconductivity in La3Ni2O7 at ambient pressure by introducing compressive strain along the [001] direction. The shape of the NiO6 octahedra affect the Ni-3dz2 density of states (DOS) at Fermi level, and it can be modulated by applying compressive strain instead of hydrostatic pressure. Notably, when the octahedral regularity parameter defined herein is R ~ 4%, La3Ni2O7 acquires a high Ni-3dz2 DOS and hole Fermi pocket. Our study thus indicates a path for obtaining superconductivity in La3Ni2O7 at ambient pressure and elucidates the relationship between structural properties and superconductivity.","sentences":["Superconductivity at Tc = 80 K has recently been reported above 14 GPa in La3Ni2O7, which thus introduces a new family of high-temperature superconductors.","Using a first-principles calculation with Coulomb repulsion, we unveil a surprising new route to obtain superconductivity in La3Ni2O7 at ambient pressure by introducing compressive strain along the [001] direction.","The shape of the NiO6 octahedra affect the Ni-3dz2 density of states (DOS) at Fermi level, and it can be modulated by applying compressive strain instead of hydrostatic pressure.","Notably, when the octahedral regularity parameter defined herein is R ~ 4%, La3Ni2O7 acquires a high Ni-3dz2 DOS and hole Fermi pocket.","Our study thus indicates a path for obtaining superconductivity in La3Ni2O7 at ambient pressure and elucidates the relationship between structural properties and superconductivity."],"url":"http://arxiv.org/abs/2404.11001v1","category":"cond-mat.supr-con"}
{"created":"2024-04-17 01:28:54","title":"The Relationship Between Simulated Sub-Millimeter and Near-Infrared Images of Sagittarius A* from a Magnetically Arrested Black Hole Accretion Flow","abstract":"Sagittarius A* (Sgr A*), the supermassive black hole at the center of the Milky Way, undergoes large-amplitude near-infrared (NIR) flares that can coincide with the continuous rotation of the NIR emission region. One promising explanation for this observed NIR behavior is a magnetic flux eruption, which occurs in three-dimensional General Relativistic Magneto-Hydrodynamic (3D GRMHD) simulations of magnetically arrested accretion flows. After running two-temperature 3D GRMHD simulations, where the electron temperature is evolved self-consistently along with the gas temperature, it is possible to calculate ray-traced images of the synchotron emission from thermal electrons in the accretion flow. Changes in the gas dominated ($\\sigma=b^2/2\\rho<1$) regions of the accretion flow during a magnetic flux eruption reproduce the NIR flaring and NIR emission region rotation of Sgr A* with durations consistent with observation. In this paper, we demonstrate that these models also predict that large (1.5x - 2x) size increases of the sub-millimeter (sub-mm) and millimeter (mm) emission region follow most NIR flares by 20 - 50 minutes. These size increases occur across a wide parameter space of black hole spin ($a=0.3,0.5,-0.5,0.9375$) and initial tilt angle between the accretion flow and black hole spin axes $\\theta_0$ ($\\theta_0=0^{\\circ}$, $16^{\\circ}$, $30^{\\circ}$). We also calculate the sub-mm polarization angle rotation and the shift of the sub-mm spectral index from zero to -0.8 during a prominent NIR flare in our high spin ($a=0.9375$) simulation. We show that, during a magnetic flux eruption, a large ($\\sim10r_g$), magnetically dominated $(\\sigma>1)$, low density, and high temperature ``bubble'' forms in the accretion flow. The drop in density inside the bubble and additional electron heating in accretion flow between 15$r_g$ - 25$r_g$ leads to a sub-mm size increase in corresponding images.","sentences":["Sagittarius A* (Sgr A*), the supermassive black hole at the center of the Milky Way, undergoes large-amplitude near-infrared (NIR) flares that can coincide with the continuous rotation of the NIR emission region.","One promising explanation for this observed NIR behavior is a magnetic flux eruption, which occurs in three-dimensional General Relativistic Magneto-Hydrodynamic (3D GRMHD) simulations of magnetically arrested accretion flows.","After running two-temperature 3D GRMHD simulations, where the electron temperature is evolved self-consistently along with the gas temperature, it is possible to calculate ray-traced images of the synchotron emission from thermal electrons in the accretion flow.","Changes in the gas dominated ($\\sigma=b^2/2\\rho<1$) regions of the accretion flow during a magnetic flux eruption reproduce the NIR flaring and NIR emission region rotation of Sgr A* with durations consistent with observation.","In this paper, we demonstrate that these models also predict that large (1.5x - 2x) size increases of the sub-millimeter (sub-mm) and millimeter (mm) emission region follow most NIR flares by 20 - 50 minutes.","These size increases occur across a wide parameter space of black hole spin ($a=0.3,0.5,-0.5,0.9375$) and initial tilt angle between the accretion flow and black hole spin axes $\\theta_0$ ($\\theta_0=0^{\\circ}$, $16^{\\circ}$, $30^{\\circ}$).","We also calculate the sub-mm polarization angle rotation and the shift of the sub-mm spectral index from zero to -0.8 during a prominent NIR flare in our high spin ($a=0.9375$) simulation.","We show that, during a magnetic flux eruption, a large ($\\sim10r_g$), magnetically dominated $(\\sigma>1)$, low density, and high temperature ``bubble'' forms in the accretion flow.","The drop in density inside the bubble and additional electron heating in accretion flow between 15$r_g$ - 25$r_g$ leads to a sub-mm size increase in corresponding images."],"url":"http://arxiv.org/abs/2404.10982v1","category":"astro-ph.HE"}
{"created":"2024-04-17 01:24:08","title":"Scales of Stability and Turbulence in the Molecular ISM","abstract":"From the archival data of the BU-FCRAO $^{13}$ CO GRS, we measure the radial profiles of column density and turbulent velocity dispersion from the centers of molecular clouds outward into the surrounding diffuse molecular ISM. Averaged across spatial scales, the profiles are consistent with turbulence that is on average in hydrostatic equilibrium. We measure the turbulent kinetic energy (KE) and the gravitational potential energy (GE) within and the pressure energy (PE) outside the clouds. The distribution of the sum 2KE-|GE|-PE has a mean near zero indicating approximate virial equilibrium. The average pressure energy is consistent with estimates of the pressure of the multiphase ISM at the Galactic mid-plane. If the dynamical time scale of the turbulence scales with its crossing time, this apparent equilibrium may result from the rapid virialization of the KE and GE with respect to changes in PE. In a snapshot, the clouds appear to be in virial equilibrium within a confining pressure. However, the duration of the equilibrium is approximately the crossing time. An analysis of Larson's scaling relationships of line width and column density finds no correlation. Tthe inference of constant column density with cloud size is a misinterpretation of this lack of correlation.","sentences":["From the archival data of the BU-FCRAO $^{13}$ CO GRS, we measure the radial profiles of column density and turbulent velocity dispersion from the centers of molecular clouds outward into the surrounding diffuse molecular ISM.","Averaged across spatial scales, the profiles are consistent with turbulence that is on average in hydrostatic equilibrium.","We measure the turbulent kinetic energy (KE) and the gravitational potential energy (GE) within and the pressure energy (PE) outside the clouds.","The distribution of the sum 2KE-|GE|-PE has a mean near zero indicating approximate virial equilibrium.","The average pressure energy is consistent with estimates of the pressure of the multiphase ISM at the Galactic mid-plane.","If the dynamical time scale of the turbulence scales with its crossing time, this apparent equilibrium may result from the rapid virialization of the KE and GE with respect to changes in PE.","In a snapshot, the clouds appear to be in virial equilibrium within a confining pressure.","However, the duration of the equilibrium is approximately the crossing time.","An analysis of Larson's scaling relationships of line width and column density finds no correlation.","Tthe inference of constant column density with cloud size is a misinterpretation of this lack of correlation."],"url":"http://arxiv.org/abs/2404.10979v1","category":"astro-ph.GA"}
{"created":"2024-04-17 01:17:54","title":"Methane Emission From a Cool Brown Dwarf","abstract":"Beyond our solar system, aurorae have been inferred from radio observations of isolated brown dwarfs (e.g. Hallinan et al. 2006; Kao et al. 2023). Within our solar system, giant planets have auroral emission with signatures across the electromagnetic spectrum including infrared emission of H3+ and methane. Isolated brown dwarfs with auroral signatures in the radio have been searched for corresponding infrared features but have only had null detections (e.g. Gibbs et al. 2022). CWISEP J193518.59-154620.3. (W1935 for short) is an isolated brown dwarf with a temperature of ~482 K. Here we report JWST observations of strong methane emission from W1935 at 3.326 microns. Atmospheric modeling leads us to conclude that a temperature inversion of ~300 K centered at 1-10 millibar replicates the feature. This represents an atmospheric temperature inversion for a Jupiter-like atmosphere without irradiation from a host star. A plausible explanation for the strong inversion is heating by auroral processes, although other internal and/or external dynamical processes cannot be ruled out. The best fit model rules out the contribution of H3+ emission which is prominent in solar system gas giants however this is consistent with rapid destruction of H3+ at the higher pressure where the W1935 emission originates (e.g. Helling et al. 2019).","sentences":["Beyond our solar system, aurorae have been inferred from radio observations of isolated brown dwarfs (e.g. Hallinan et al. 2006; Kao et al. 2023).","Within our solar system, giant planets have auroral emission with signatures across the electromagnetic spectrum including infrared emission of H3+ and methane.","Isolated brown dwarfs with auroral signatures in the radio have been searched for corresponding infrared features but have only had null detections (e.g. Gibbs et al. 2022).","CWISEP J193518.59-154620.3.","(W1935 for short) is an isolated brown dwarf with a temperature of ~482 K. Here we report JWST observations of strong methane emission from W1935 at 3.326 microns.","Atmospheric modeling leads us to conclude that a temperature inversion of ~300 K centered at 1-10 millibar replicates the feature.","This represents an atmospheric temperature inversion for a Jupiter-like atmosphere without irradiation from a host star.","A plausible explanation for the strong inversion is heating by auroral processes, although other internal and/or external dynamical processes cannot be ruled out.","The best fit model rules out the contribution of H3+ emission which is prominent in solar system gas giants however this is consistent with rapid destruction of H3+ at the higher pressure where the W1935 emission originates (e.g. Helling et al. 2019)."],"url":"http://arxiv.org/abs/2404.10977v1","category":"astro-ph.SR"}
{"created":"2024-04-17 01:09:04","title":"Compressive Bayesian non-negative matrix factorization for mutational signatures analysis","abstract":"Non-negative matrix factorization (NMF) is widely used in many applications for dimensionality reduction. Inferring an appropriate number of factors for NMF is a challenging problem, and several approaches based on information criteria or sparsity-inducing priors have been proposed. However, inference in these models is often complicated and computationally challenging. In this paper, we introduce a novel methodology for overfitted Bayesian NMF models using \"compressive hyperpriors\" that force unneeded factors down to negligible values while only imposing mild shrinkage on needed factors. The method is based on using simple semi-conjugate priors to facilitate inference, while setting the strength of the hyperprior in a data-dependent way to achieve this compressive property. We apply our method to mutational signatures analysis in cancer genomics, where we find that it outperforms state-of-the-art alternatives. In particular, we illustrate how our compressive hyperprior enables the use of biologically informed priors on the signatures, yielding significantly improved accuracy. We provide theoretical results establishing the compressive property, and we demonstrate the method in simulations and on real data from a breast cancer application.","sentences":["Non-negative matrix factorization (NMF) is widely used in many applications for dimensionality reduction.","Inferring an appropriate number of factors for NMF is a challenging problem, and several approaches based on information criteria or sparsity-inducing priors have been proposed.","However, inference in these models is often complicated and computationally challenging.","In this paper, we introduce a novel methodology for overfitted Bayesian NMF models using \"compressive hyperpriors\" that force unneeded factors down to negligible values while only imposing mild shrinkage on needed factors.","The method is based on using simple semi-conjugate priors to facilitate inference, while setting the strength of the hyperprior in a data-dependent way to achieve this compressive property.","We apply our method to mutational signatures analysis in cancer genomics, where we find that it outperforms state-of-the-art alternatives.","In particular, we illustrate how our compressive hyperprior enables the use of biologically informed priors on the signatures, yielding significantly improved accuracy.","We provide theoretical results establishing the compressive property, and we demonstrate the method in simulations and on real data from a breast cancer application."],"url":"http://arxiv.org/abs/2404.10974v1","category":"stat.ME"}
{"created":"2024-04-16 22:06:44","title":"Self-trapping of active particles induced by non-reciprocal interactions in disordered media","abstract":"Disordered media is known to modify the transport properties of active matter depending on interactions between particles and with the substrate. Here we study systems of active particles with visual-like perception that co-align with the positions of perceived conspecifics, and anti-align with positions of static obstacles. We report a novel self-trapping mechanism of particles forming closed loops that progressively shrink, surrounding one or multiple obstacles.This mechanism corresponds to a pinning behavior preventing particle diffusion. Increased co-alignment strength is found to reduce loop shrinking time, although this effect reaches a plateau at higher strengths. Loops are found to initially exhibit local polar order, but eventually they transition to nematic states as they absorb more particles. We show a phase diagram demonstrating self-trapping occurs within a specific range of aperture angles of the vision cone.","sentences":["Disordered media is known to modify the transport properties of active matter depending on interactions between particles and with the substrate.","Here we study systems of active particles with visual-like perception that co-align with the positions of perceived conspecifics, and anti-align with positions of static obstacles.","We report a novel self-trapping mechanism of particles forming closed loops that progressively shrink, surrounding one or multiple obstacles.","This mechanism corresponds to a pinning behavior preventing particle diffusion.","Increased co-alignment strength is found to reduce loop shrinking time, although this effect reaches a plateau at higher strengths.","Loops are found to initially exhibit local polar order, but eventually they transition to nematic states as they absorb more particles.","We show a phase diagram demonstrating self-trapping occurs within a specific range of aperture angles of the vision cone."],"url":"http://arxiv.org/abs/2404.10932v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-16 21:34:42","title":"Geometrisation of Fermions in Semi-Riemannian Spacetimes","abstract":"An earlier scheme [arXiv:2404.03360], where torsion plays an essential part in a flat spacetime account of fermion spin, is extended to spacetimes with non-zero Riemann curvature. It is found that further essential features of the fermion, in particular its electromagnetic field, are determined by the curvature and torsion of spacetime. A natural model for the metric, singular along the spin axis of the particle, permits integration of a stress-energy tensor that fixes a realistic electron mass in terms of the Planck mass, despite the many orders of magnitude by which these values differ.","sentences":["An earlier scheme [arXiv:2404.03360], where torsion plays an essential part in a flat spacetime account of fermion spin, is extended to spacetimes with non-zero Riemann curvature.","It is found that further essential features of the fermion, in particular its electromagnetic field, are determined by the curvature and torsion of spacetime.","A natural model for the metric, singular along the spin axis of the particle, permits integration of a stress-energy tensor that fixes a realistic electron mass in terms of the Planck mass, despite the many orders of magnitude by which these values differ."],"url":"http://arxiv.org/abs/2404.10918v1","category":"gr-qc"}
{"created":"2024-04-16 20:19:27","title":"The sandwich problem for odd-hole-free and even-hole-free graphs","abstract":"For a property $\\mathcal{P}$ of graphs, the $\\mathcal{P}$-\\textsc{Sandwich-Problem}, introduced by Golumbic and Shamir (1993), is the following: Given a pair of graphs $(G_1, G_2)$ on the same vertex set $V$, does there exist a graph $G$ such that $V(G)=V$, $E(G_{1})\\subseteq E(G) \\subseteq E(G_{2})$, and $G$ satisfies $\\mathcal{P}$? A {\\em hole} in a graph is an induced subgraph which is a cycle of length at least four. An odd (respectively even) hole is a hole of odd (respectively even) length. Given a class of graphs $\\mathcal{C}$ and a graph $G$ we say that $G$ is {\\em $\\mathcal{C}$-free} if it contains no induced subgraph isomorphic to a member of $\\mathcal{C}$. In this paper we prove that if $\\mathcal{P}$ is the property of being odd-hole-free or the property of being even-hole-free, then the $\\mathcal{P}$-\\textsc{Sandwich-Problem} is NP-hard.","sentences":["For a property $\\mathcal{P}$ of graphs, the $\\mathcal{P}$-\\textsc{Sandwich-Problem}, introduced by Golumbic and Shamir (1993), is the following: Given a pair of graphs $(G_1, G_2)$ on the same vertex set $V$, does there exist a graph $G$ such that $V(G)=V$, $E(G_{1})\\subseteq E(G)","\\subseteq E(G_{2})$, and $G$ satisfies $\\mathcal{P}$?","A {\\em hole} in a graph is an induced subgraph which is a cycle of length at least four.","An odd (respectively even) hole is a hole of odd (respectively even) length.","Given a class of graphs $\\mathcal{C}$ and a graph $G$ we say that $G$ is {\\em $\\mathcal{C}$-free} if it contains no induced subgraph isomorphic to a member of $\\mathcal{C}$. In this paper we prove that if $\\mathcal{P}$ is the property of being odd-hole-free or the property of being even-hole-free, then the $\\mathcal{P}$-\\textsc{Sandwich-Problem} is NP-hard."],"url":"http://arxiv.org/abs/2404.10888v1","category":"math.CO"}
{"created":"2024-04-16 20:09:39","title":"Modeling Interconnected Modules in Multivariate Outcomes: Evaluating the Impact of Alcohol Intake on Plasma Metabolomics","abstract":"Alcohol consumption has been shown to influence cardiovascular mechanisms in humans, leading to observable alterations in the plasma metabolomic profile. Regression models are commonly employed to investigate these effects, treating metabolomics features as the outcomes and alcohol intake as the exposure. Given the latent dependence structure among the numerous metabolomic features (e.g., co-expression networks with interconnected modules), modeling this structure is crucial for accurately identifying metabolomic features associated with alcohol intake. However, integrating dependence structures into regression models remains difficult in both estimation and inference procedures due to their large or high dimensionality. To bridge this gap, we propose an innovative multivariate regression model that accounts for correlations among outcome features by incorporating an interconnected community structure. Furthermore, we derive closed-form and likelihood-based estimators, accompanied by explicit exact and explicit asymptotic covariance matrix estimators, respectively. Simulation analysis demonstrates that our approach provides accurate estimation of both dependence and regression coefficients, and enhances sensitivity while maintaining a well-controlled discovery rate, as evidenced through benchmarking against existing regression models. Finally, we apply our approach to assess the impact of alcohol intake on $249$ metabolomic biomarkers measured using nuclear magnetic resonance spectroscopy. The results indicate that alcohol intake can elevate high-density lipoprotein levels by enhancing the transport rate of Apolipoproteins A1.","sentences":["Alcohol consumption has been shown to influence cardiovascular mechanisms in humans, leading to observable alterations in the plasma metabolomic profile.","Regression models are commonly employed to investigate these effects, treating metabolomics features as the outcomes and alcohol intake as the exposure.","Given the latent dependence structure among the numerous metabolomic features (e.g., co-expression networks with interconnected modules), modeling this structure is crucial for accurately identifying metabolomic features associated with alcohol intake.","However, integrating dependence structures into regression models remains difficult in both estimation and inference procedures due to their large or high dimensionality.","To bridge this gap, we propose an innovative multivariate regression model that accounts for correlations among outcome features by incorporating an interconnected community structure.","Furthermore, we derive closed-form and likelihood-based estimators, accompanied by explicit exact and explicit asymptotic covariance matrix estimators, respectively.","Simulation analysis demonstrates that our approach provides accurate estimation of both dependence and regression coefficients, and enhances sensitivity while maintaining a well-controlled discovery rate, as evidenced through benchmarking against existing regression models.","Finally, we apply our approach to assess the impact of alcohol intake on $249$ metabolomic biomarkers measured using nuclear magnetic resonance spectroscopy.","The results indicate that alcohol intake can elevate high-density lipoprotein levels by enhancing the transport rate of Apolipoproteins A1."],"url":"http://arxiv.org/abs/2404.10884v1","category":"stat.ME"}
