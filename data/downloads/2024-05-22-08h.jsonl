{"created":"2024-05-21 17:59:29","title":"Reducing Transformer Key-Value Cache Size with Cross-Layer Attention","abstract":"Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible","sentences":["Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs).","However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes.","Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA).","MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy.","In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA).","With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA.","In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible"],"url":"http://arxiv.org/abs/2405.12981v1","category":"cs.LG"}
{"created":"2024-05-21 17:59:22","title":"OmniGlue: Generalizable Feature Matching with Foundation Model Guidance","abstract":"The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time. Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors. We perform comprehensive experiments on a suite of $7$ datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue's novel components lead to relative gains on unseen domains of $20.9\\%$ with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by $9.5\\%$ relatively.Code and model can be found at https://hwjiang1510.github.io/OmniGlue","sentences":["The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks.","However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains.","In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle.","OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time.","Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors.","We perform comprehensive experiments on a suite of $7$ datasets with varied image domains, including scene-level, object-centric and aerial images.","OmniGlue's novel components lead to relative gains on unseen domains of $20.9\\%$ with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by $9.5\\%$ relatively.","Code and model can be found at https://hwjiang1510.github.io/OmniGlue"],"url":"http://arxiv.org/abs/2405.12979v1","category":"cs.CV"}
{"created":"2024-05-21 17:59:22","title":"A Symmetry-centric Perspective on the Geometry of the String Landscape and the Swampland","abstract":"As famously observed by Ooguri and Vafa nearly twenty years ago, scalar field moduli spaces in quantum gravity appear to exhibit various universal features. For instance, they seem to be infinite in diameter, have trivial fundamental group, and feature towers of massive particles that become light in their asymptotic limits. In this essay, we explain how these features can be reformulated in more modern language using generalized notions of global symmetries. Such symmetries are ubiquitous in non-gravitational quantum field theories, but it is widely believed that they must be either gauged or broken in quantum gravity. In what follows, we will see that the observations of Ooguri and Vafa can be understood as consequences of such gauging or breaking.","sentences":["As famously observed by Ooguri and Vafa nearly twenty years ago, scalar field moduli spaces in quantum gravity appear to exhibit various universal features.","For instance, they seem to be infinite in diameter, have trivial fundamental group, and feature towers of massive particles that become light in their asymptotic limits.","In this essay, we explain how these features can be reformulated in more modern language using generalized notions of global symmetries.","Such symmetries are ubiquitous in non-gravitational quantum field theories, but it is widely believed that they must be either gauged or broken in quantum gravity.","In what follows, we will see that the observations of Ooguri and Vafa can be understood as consequences of such gauging or breaking."],"url":"http://arxiv.org/abs/2405.12980v1","category":"hep-th"}
{"created":"2024-05-21 17:59:01","title":"Personalized Residuals for Concept-Driven Text-to-Image Generation","abstract":"We present personalized residuals and localized attention-guided sampling for efficient concept-driven generation using text-to-image diffusion models. Our method first represents concepts by freezing the weights of a pretrained text-conditioned diffusion model and learning low-rank residuals for a small subset of the model's layers. The residual-based approach then directly enables application of our proposed sampling technique, which applies the learned residuals only in areas where the concept is localized via cross-attention and applies the original diffusion weights in all other regions. Localized sampling therefore combines the learned identity of the concept with the existing generative prior of the underlying diffusion model. We show that personalized residuals effectively capture the identity of a concept in ~3 minutes on a single GPU without the use of regularization images and with fewer parameters than previous models, and localized sampling allows using the original model as strong prior for large parts of the image.","sentences":["We present personalized residuals and localized attention-guided sampling for efficient concept-driven generation using text-to-image diffusion models.","Our method first represents concepts by freezing the weights of a pretrained text-conditioned diffusion model and learning low-rank residuals for a small subset of the model's layers.","The residual-based approach then directly enables application of our proposed sampling technique, which applies the learned residuals only in areas where the concept is localized via cross-attention and applies the original diffusion weights in all other regions.","Localized sampling therefore combines the learned identity of the concept with the existing generative prior of the underlying diffusion model.","We show that personalized residuals effectively capture the identity of a concept in ~3 minutes on a single GPU without the use of regularization images and with fewer parameters than previous models, and localized sampling allows using the original model as strong prior for large parts of the image."],"url":"http://arxiv.org/abs/2405.12978v1","category":"cs.CV"}
{"created":"2024-05-21 17:58:29","title":"Implication of jet physics from MeV line emission of GRB 221009A","abstract":"Ultra-relativistic jets are believed to play important role in producing prompt emission and afterglow of Gamma-Ray Burst (GRB), but the nature of the jet is poorly known owing to the lacking of decisive features observed in the prompt emission. A series of bright, narrow and temporally evolving MeV emission line detected in the brightest-of-all-time GRB 221009A provide a chance to probe GRB jet physics. The evolution of the central energy of the line with power-law index $-1$ is naturally explained by high-latitude curvature effect. Under the assumption that the line emission is generated in the prompt emission by $e^\\pm$ pair production, cooling and annihilation in the jet, we can strictly constrain jet physics with observed line emission properties. We find the radius of the emission region is $r\\sim10^{16}$cm. The narrow line width of $10\\%$ implies that pairs cool fast down to non-relativistic state within a time of tenth of the dynamical time. This requires a magnetic-field energy density much larger than the prompt gamma-ray energy density in the jet, implying a magnetic field dominated jet. The temporal behavior of line flux suggests some angle dependence of line emission. We also discuss the difficulties of other scenarios to interpret the observed emission line.","sentences":["Ultra-relativistic jets are believed to play important role in producing prompt emission and afterglow of Gamma-Ray Burst (GRB), but the nature of the jet is poorly known owing to the lacking of decisive features observed in the prompt emission.","A series of bright, narrow and temporally evolving MeV emission line detected in the brightest-of-all-time GRB 221009A provide a chance to probe GRB jet physics.","The evolution of the central energy of the line with power-law index $-1$ is naturally explained by high-latitude curvature effect.","Under the assumption that the line emission is generated in the prompt emission by $e^\\pm$ pair production, cooling and annihilation in the jet, we can strictly constrain jet physics with observed line emission properties.","We find the radius of the emission region is $r\\sim10^{16}$cm.","The narrow line width of $10\\%$ implies that pairs cool fast down to non-relativistic state within a time of tenth of the dynamical time.","This requires a magnetic-field energy density much larger than the prompt gamma-ray energy density in the jet, implying a magnetic field dominated jet.","The temporal behavior of line flux suggests some angle dependence of line emission.","We also discuss the difficulties of other scenarios to interpret the observed emission line."],"url":"http://arxiv.org/abs/2405.12977v1","category":"astro-ph.HE"}
{"created":"2024-05-21 17:56:04","title":"$\\mathcal{K}$-Lorentzian Polynomials","abstract":"Lorentzian polynomials are a fascinating class of real polynomials with many applications. Their definition is specific to the nonnegative orthant. Following recent work, we examine Lorentzian polynomials on proper convex cones. For a self-dual cone $\\mathcal{K}$ we find a connection between $\\mathcal{K}$-Lorentzian polynomials and $\\mathcal{K}$-positive linear maps, which were studied in the context of the generalized Perron-Frobenius theorem. We find that as the cone $\\mathcal{K}$ varies, even the set of quadratic $\\mathcal{K}$-Lorentzian polynomials can be difficult to understand algorithmically. We also show that, just as in the case of the nonnegative orthant, $\\mathcal{K}$-Lorentzian and $\\mathcal{K}$-completely log-concave polynomials coincide.","sentences":["Lorentzian polynomials are a fascinating class of real polynomials with many applications.","Their definition is specific to the nonnegative orthant.","Following recent work, we examine Lorentzian polynomials on proper convex cones.","For a self-dual cone $\\mathcal{K}$ we find a connection between $\\mathcal{K}$-Lorentzian polynomials and $\\mathcal{K}$-positive linear maps, which were studied in the context of the generalized Perron-Frobenius theorem.","We find that as the cone $\\mathcal{K}$ varies, even the set of quadratic $\\mathcal{K}$-Lorentzian polynomials can be difficult to understand algorithmically.","We also show that, just as in the case of the nonnegative orthant, $\\mathcal{K}$-Lorentzian and $\\mathcal{K}$-completely log-concave polynomials coincide."],"url":"http://arxiv.org/abs/2405.12973v1","category":"math.AG"}
{"created":"2024-05-21 17:50:12","title":"Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control","abstract":"Current face reenactment and swapping methods mainly rely on GAN frameworks, but recent focus has shifted to pre-trained diffusion models for their superior generation capabilities. However, training these models is resource-intensive, and the results have not yet achieved satisfactory performance levels. To address this issue, we introduce Face-Adapter, an efficient and effective adapter designed for high-precision and high-fidelity face editing for pre-trained diffusion models. We observe that both face reenactment/swapping tasks essentially involve combinations of target structure, ID and attribute. We aim to sufficiently decouple the control of these factors to achieve both tasks in one model. Specifically, our method contains: 1) A Spatial Condition Generator that provides precise landmarks and background; 2) A Plug-and-play Identity Encoder that transfers face embeddings to the text space by a transformer decoder. 3) An Attribute Controller that integrates spatial conditions and detailed attributes. Face-Adapter achieves comparable or even superior performance in terms of motion control precision, ID retention capability, and generation quality compared to fully fine-tuned face reenactment/swapping models. Additionally, Face-Adapter seamlessly integrates with various StableDiffusion models.","sentences":["Current face reenactment and swapping methods mainly rely on GAN frameworks, but recent focus has shifted to pre-trained diffusion models for their superior generation capabilities.","However, training these models is resource-intensive, and the results have not yet achieved satisfactory performance levels.","To address this issue, we introduce Face-Adapter, an efficient and effective adapter designed for high-precision and high-fidelity face editing for pre-trained diffusion models.","We observe that both face reenactment/swapping tasks essentially involve combinations of target structure, ID and attribute.","We aim to sufficiently decouple the control of these factors to achieve both tasks in one model.","Specifically, our method contains: 1) A Spatial Condition Generator that provides precise landmarks and background; 2) A Plug-and-play Identity Encoder that transfers face embeddings to the text space by a transformer decoder.","3) An Attribute Controller that integrates spatial conditions and detailed attributes.","Face-Adapter achieves comparable or even superior performance in terms of motion control precision, ID retention capability, and generation quality compared to fully fine-tuned face reenactment/swapping models.","Additionally, Face-Adapter seamlessly integrates with various StableDiffusion models."],"url":"http://arxiv.org/abs/2405.12970v1","category":"cs.CV"}
{"created":"2024-05-21 17:49:10","title":"Can We Treat Noisy Labels as Accurate?","abstract":"Noisy labels significantly hinder the accuracy and generalization of machine learning models, particularly due to ambiguous instance features. Traditional techniques that attempt to correct noisy labels directly, such as those using transition matrices, often fail to address the inherent complexities of the problem sufficiently. In this paper, we introduce EchoAlign, a transformative paradigm shift in learning from noisy labels. Instead of focusing on label correction, EchoAlign treats noisy labels ($\\tilde{Y}$) as accurate and modifies corresponding instance features ($X$) to achieve better alignment with $\\tilde{Y}$. EchoAlign's core components are (1) EchoMod: Employing controllable generative models, EchoMod precisely modifies instances while maintaining their intrinsic characteristics and ensuring alignment with the noisy labels. (2) EchoSelect: Instance modification inevitably introduces distribution shifts between training and test sets. EchoSelect maintains a significant portion of clean original instances to mitigate these shifts. It leverages the distinct feature similarity distributions between original and modified instances as a robust tool for accurate sample selection. This integrated approach yields remarkable results. In environments with 30% instance-dependent noise, even at 99% selection accuracy, EchoSelect retains nearly twice the number of samples compared to the previous best method. Notably, on three datasets, EchoAlign surpasses previous state-of-the-art techniques with a substantial improvement.","sentences":["Noisy labels significantly hinder the accuracy and generalization of machine learning models, particularly due to ambiguous instance features.","Traditional techniques that attempt to correct noisy labels directly, such as those using transition matrices, often fail to address the inherent complexities of the problem sufficiently.","In this paper, we introduce EchoAlign, a transformative paradigm shift in learning from noisy labels.","Instead of focusing on label correction, EchoAlign treats noisy labels ($\\tilde{Y}$) as accurate and modifies corresponding instance features ($X$) to achieve better alignment with $\\tilde{Y}$. EchoAlign's core components are (1) EchoMod: Employing controllable generative models, EchoMod precisely modifies instances while maintaining their intrinsic characteristics and ensuring alignment with the noisy labels.","(2) EchoSelect: Instance modification inevitably introduces distribution shifts between training and test sets.","EchoSelect maintains a significant portion of clean original instances to mitigate these shifts.","It leverages the distinct feature similarity distributions between original and modified instances as a robust tool for accurate sample selection.","This integrated approach yields remarkable results.","In environments with 30% instance-dependent noise, even at 99% selection accuracy, EchoSelect retains nearly twice the number of samples compared to the previous best method.","Notably, on three datasets, EchoAlign surpasses previous state-of-the-art techniques with a substantial improvement."],"url":"http://arxiv.org/abs/2405.12969v1","category":"cs.LG"}
{"created":"2024-05-21 17:45:36","title":"The future of cosmological likelihood-based inference: accelerated high-dimensional parameter estimation and model comparison","abstract":"We advocate for a new paradigm of cosmological likelihood-based inference, leveraging recent developments in machine learning and its underlying technology, to accelerate Bayesian inference in high-dimensional settings. Specifically, we combine (i) emulation, where a machine learning model is trained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii) differentiable and probabilistic programming, e.g. JAX and NumPyro, respectively; (iii) scalable Markov chain Monte Carlo (MCMC) sampling techniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv) decoupled and scalable Bayesian model selection techniques that compute the Bayesian evidence purely from posterior samples, e.g. the learned harmonic mean implemented in harmonic. This paradigm allows us to carry out a complete Bayesian analysis, including both parameter estimation and model selection, in a fraction of the time of traditional approaches. First, we demonstrate the application of this paradigm on a simulated cosmic shear analysis for a Stage IV survey in 37- and 39-dimensional parameter spaces, comparing $\\Lambda$CDM and a dynamical dark energy model ($w_0w_a$CDM). We recover posterior contours and evidence estimates that are in excellent agreement with those computed by the traditional nested sampling approach while reducing the computational cost from 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a joint analysis between three simulated next-generation surveys, each performing a 3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces. Standard nested sampling techniques are simply not feasible in this high-dimensional setting, requiring a projected 12 years of compute time on 48 CPU cores; on the other hand, the proposed approach only requires 8 days of compute time on 24 GPUs. All packages used in our analyses are publicly available.","sentences":["We advocate for a new paradigm of cosmological likelihood-based inference, leveraging recent developments in machine learning and its underlying technology, to accelerate Bayesian inference in high-dimensional settings.","Specifically, we combine (i) emulation, where a machine learning model is trained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii) differentiable and probabilistic programming, e.g. JAX and NumPyro, respectively; (iii) scalable Markov chain Monte Carlo (MCMC) sampling techniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv) decoupled and scalable Bayesian model selection techniques that compute the Bayesian evidence purely from posterior samples, e.g. the learned harmonic mean implemented in harmonic.","This paradigm allows us to carry out a complete Bayesian analysis, including both parameter estimation and model selection, in a fraction of the time of traditional approaches.","First, we demonstrate the application of this paradigm on a simulated cosmic shear analysis for a Stage IV survey in 37- and 39-dimensional parameter spaces, comparing $\\Lambda$CDM and a dynamical dark energy model ($w_0w_a$CDM).","We recover posterior contours and evidence estimates that are in excellent agreement with those computed by the traditional nested sampling approach while reducing the computational cost from 8 months on 48 CPU cores to 2 days on 12 GPUs.","Second, we consider a joint analysis between three simulated next-generation surveys, each performing a 3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces.","Standard nested sampling techniques are simply not feasible in this high-dimensional setting, requiring a projected 12 years of compute time on 48 CPU cores; on the other hand, the proposed approach only requires 8 days of compute time on 24 GPUs.","All packages used in our analyses are publicly available."],"url":"http://arxiv.org/abs/2405.12965v1","category":"astro-ph.CO"}
{"created":"2024-05-21 17:35:20","title":"Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale","abstract":"Searching through chemical space is an exceptionally challenging problem because the number of possible molecules grows combinatorially with the number of atoms. Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties. This molecular search problem closely resembles the \"alignment\" problem for large language models, though for many chemical tasks we have a specific and easily evaluable reward function. Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies. We show theoretically that this algorithm is closely related to proximal policy optimization (PPO) and direct preference optimization (DPO), but has a minimizer that converges to an ideal Gibbs-Boltzmann distribution with the reward playing the role of an energy function. Furthermore, this algorithm is highly scalable, does not require reinforcement learning, and performs well relative to DPO when the number of preference observations per pairing is small. We deploy this approach to align molecular transformers to generate molecules with externally specified properties and find that it does so robustly, searching through diverse parts of chemical space. While our focus here is on chemical search, we also obtain excellent results on an AI supervised task for LLM alignment, showing that the method is scalable and general.","sentences":["Searching through chemical space is an exceptionally challenging problem because the number of possible molecules grows combinatorially with the number of atoms.","Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties.","This molecular search problem closely resembles the \"alignment\" problem for large language models, though for many chemical tasks we have a specific and easily evaluable reward function.","Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies.","We show theoretically that this algorithm is closely related to proximal policy optimization (PPO) and direct preference optimization (DPO), but has a minimizer that converges to an ideal Gibbs-Boltzmann distribution with the reward playing the role of an energy function.","Furthermore, this algorithm is highly scalable, does not require reinforcement learning, and performs well relative to DPO when the number of preference observations per pairing is small.","We deploy this approach to align molecular transformers to generate molecules with externally specified properties and find that it does so robustly, searching through diverse parts of chemical space.","While our focus here is on chemical search, we also obtain excellent results on an AI supervised task for LLM alignment, showing that the method is scalable and general."],"url":"http://arxiv.org/abs/2405.12961v1","category":"cs.LG"}
{"created":"2024-05-21 17:32:04","title":"Soft Synergies: Model Order Reduction of Hybrid Soft-Rigid Robots via Optimal Strain Parameterization","abstract":"Soft robots offer remarkable adaptability and safety advantages over rigid robots, but modeling their complex, nonlinear dynamics remains challenging. Strain-based models have recently emerged as a promising candidate to describe such systems, however, they tend to be high-dimensional and time consuming. This paper presents a novel model order reduction approach for soft and hybrid robots by combining strain-based modeling with Proper Orthogonal Decomposition (POD). The method identifies optimal coupled strain basis functions -or mechanical synergies- from simulation data, enabling the description of soft robot configurations with a minimal number of generalized coordinates. The reduced order model (ROM) achieves substantial dimensionality reduction while preserving accuracy. Rigorous testing demonstrates the interpolation and extrapolation capabilities of the ROM for soft manipulators under static and dynamic conditions. The approach is further validated on a snake-like hyper-redundant rigid manipulator and a closed-chain system with soft and rigid components, illustrating its broad applicability. Finally, the approach is leveraged for shape estimation of a real six-actuator soft manipulator using only two position markers, showcasing its practical utility. This POD-based ROM offers significant computational speed-ups, paving the way for real-time simulation and control of complex soft and hybrid robots.","sentences":["Soft robots offer remarkable adaptability and safety advantages over rigid robots, but modeling their complex, nonlinear dynamics remains challenging.","Strain-based models have recently emerged as a promising candidate to describe such systems, however, they tend to be high-dimensional and time consuming.","This paper presents a novel model order reduction approach for soft and hybrid robots by combining strain-based modeling with Proper Orthogonal Decomposition (POD).","The method identifies optimal coupled strain basis functions -or mechanical synergies- from simulation data, enabling the description of soft robot configurations with a minimal number of generalized coordinates.","The reduced order model (ROM) achieves substantial dimensionality reduction while preserving accuracy.","Rigorous testing demonstrates the interpolation and extrapolation capabilities of the ROM for soft manipulators under static and dynamic conditions.","The approach is further validated on a snake-like hyper-redundant rigid manipulator and a closed-chain system with soft and rigid components, illustrating its broad applicability.","Finally, the approach is leveraged for shape estimation of a real six-actuator soft manipulator using only two position markers, showcasing its practical utility.","This POD-based ROM offers significant computational speed-ups, paving the way for real-time simulation and control of complex soft and hybrid robots."],"url":"http://arxiv.org/abs/2405.12959v1","category":"cs.RO"}
{"created":"2024-05-21 17:28:06","title":"Truncated Variance Reduced Value Iteration","abstract":"We provide faster randomized algorithms for computing an $\\epsilon$-optimal policy in a discounted Markov decision process with $A_{\\text{tot}}$-state-action pairs, bounded rewards, and discount factor $\\gamma$. We provide an $\\tilde{O}(A_{\\text{tot}}[(1 - \\gamma)^{-3}\\epsilon^{-2} + (1 - \\gamma)^{-2}])$-time algorithm in the sampling setting, where the probability transition matrix is unknown but accessible through a generative model which can be queried in $\\tilde{O}(1)$-time, and an $\\tilde{O}(s + (1-\\gamma)^{-2})$-time algorithm in the offline setting where the probability transition matrix is known and $s$-sparse. These results improve upon the prior state-of-the-art which either ran in $\\tilde{O}(A_{\\text{tot}}[(1 - \\gamma)^{-3}\\epsilon^{-2} + (1 - \\gamma)^{-3}])$ time [Sidford, Wang, Wu, Ye 2018] in the sampling setting, $\\tilde{O}(s + A_{\\text{tot}} (1-\\gamma)^{-3})$ time [Sidford, Wang, Wu, Yang, Ye 2018] in the offline setting, or time at least quadratic in the number of states using interior point methods for linear programming. We achieve our results by building upon prior stochastic variance-reduced value iteration methods [Sidford, Wang, Wu, Yang, Ye 2018]. We provide a variant that carefully truncates the progress of its iterates to improve the variance of new variance-reduced sampling procedures that we introduce to implement the steps. Our method is essentially model-free and can be implemented in $\\tilde{O}(A_{\\text{tot}})$-space when given generative model access. Consequently, our results take a step in closing the sample-complexity gap between model-free and model-based methods.","sentences":["We provide faster randomized algorithms for computing an $\\epsilon$-optimal policy in a discounted Markov decision process with $A_{\\text{tot}}$-state-action pairs, bounded rewards, and discount factor $\\gamma$.","We provide an $\\tilde{O}(A_{\\text{tot}}[(1 - \\gamma)^{-3}\\epsilon^{-2} + (1 - \\gamma)^{-2}])$-time algorithm in the sampling setting, where the probability transition matrix is unknown but accessible through a generative model which can be queried in $\\tilde{O}(1)$-time, and an $\\tilde{O}(s + (1-\\gamma)^{-2})$-time algorithm in the offline setting where the probability transition matrix is known and $s$-sparse.","These results improve upon the prior state-of-the-art which either ran in $\\tilde{O}(A_{\\text{tot}}[(1 - \\gamma)^{-3}\\epsilon^{-2} + (1 - \\gamma)^{-3}])$ time","[Sidford, Wang, Wu, Ye 2018] in the sampling setting, $\\tilde{O}(s + A_{\\text{tot}} (1-\\gamma)^{-3})$ time [Sidford, Wang, Wu, Yang, Ye 2018] in the offline setting, or time at least quadratic in the number of states using interior point methods for linear programming.","We achieve our results by building upon prior stochastic variance-reduced value iteration methods [Sidford, Wang, Wu, Yang, Ye 2018].","We provide a variant that carefully truncates the progress of its iterates to improve the variance of new variance-reduced sampling procedures that we introduce to implement the steps.","Our method is essentially model-free and can be implemented in $\\tilde{O}(A_{\\text{tot}})$-space when given generative model access.","Consequently, our results take a step in closing the sample-complexity gap between model-free and model-based methods."],"url":"http://arxiv.org/abs/2405.12952v1","category":"cs.LG"}
{"created":"2024-05-21 17:27:00","title":"Strategic Deployment of Honeypots in Blockchain-based IoT Systems","abstract":"This paper addresses the challenge of enhancing cybersecurity in Blockchain-based Internet of Things (BIoTs) systems, which are increasingly vulnerable to sophisticated cyberattacks. It introduces an AI-powered system model for the dynamic deployment of honeypots, utilizing an Intrusion Detection System (IDS) integrated with smart contract functionalities on IoT nodes. This model enables the transformation of regular nodes into decoys in response to suspicious activities, thereby strengthening the security of BIoT networks. The paper analyses strategic interactions between potential attackers and the AI-enhanced IDS through a game-theoretic model, specifically Bayesian games. The model focuses on understanding and predicting sophisticated attacks that may initially appear normal, emphasizing strategic decision-making, optimized honeypot deployment, and adaptive strategies in response to evolving attack patterns.","sentences":["This paper addresses the challenge of enhancing cybersecurity in Blockchain-based Internet of Things (BIoTs) systems, which are increasingly vulnerable to sophisticated cyberattacks.","It introduces an AI-powered system model for the dynamic deployment of honeypots, utilizing an Intrusion Detection System (IDS) integrated with smart contract functionalities on IoT nodes.","This model enables the transformation of regular nodes into decoys in response to suspicious activities, thereby strengthening the security of BIoT networks.","The paper analyses strategic interactions between potential attackers and the AI-enhanced IDS through a game-theoretic model, specifically Bayesian games.","The model focuses on understanding and predicting sophisticated attacks that may initially appear normal, emphasizing strategic decision-making, optimized honeypot deployment, and adaptive strategies in response to evolving attack patterns."],"url":"http://arxiv.org/abs/2405.12951v1","category":"cs.CR"}
{"created":"2024-05-21 17:21:41","title":"Exact predicates, exact constructions and combinatorics for mesh CSG","abstract":"This article introduces a general mesh intersection algorithm that exactly computes the so-called Weiler model and that uses it to implement boolean operations with arbitrary multi-operand expressions, CSG (constructive solid geometry) and some mesh repair operations. From an input polygon soup, the algorithm first computes the co-refinement, with an exact representation of the intersection points. Then, the decomposition of 3D space into volumetric regions (Weiler model) is constructed, by sorting the facets around the non-manifold intersection edges (radial sort), using specialized exact predicates. Finally, based on the input boolean expression, the triangular facets that belong to the boundary of the result are classified. This is, to our knowledge, the first algorithm that computes an exact Weiler model. To implement all the involved predicates and constructions, two geometric kernels are proposed, tested and discussed (arithmetic expansions and multi-precision floating-point). As a guiding principle,the combinatorial information shared between each step is kept as simple as possible. It is made possible by treating all the particular cases in the kernel. In particular, triangles with intersections are remeshed using the (uniquely defined) Constrained Delaunay Triangulation, with symbolic perturbations to disambiguate configurations with co-cyclic points. It makes it easy to discard the duplicated triangles that appear when remeshing overlapping facets. The method is tested and compared with previous work, on the existing \"thingi10K\" dataset (to test co-refinement and mesh repair) and on a new \"thingiCSG\" dataset made publicly available (to test the full CSG pipeline) on a variety of interesting examples featuring different types of \"pathologies\"","sentences":["This article introduces a general mesh intersection algorithm that exactly computes the so-called Weiler model and that uses it to implement boolean operations with arbitrary multi-operand expressions, CSG (constructive solid geometry) and some mesh repair operations.","From an input polygon soup, the algorithm first computes the co-refinement, with an exact representation of the intersection points.","Then, the decomposition of 3D space into volumetric regions (Weiler model) is constructed, by sorting the facets around the non-manifold intersection edges (radial sort), using specialized exact predicates.","Finally, based on the input boolean expression, the triangular facets that belong to the boundary of the result are classified.","This is, to our knowledge, the first algorithm that computes an exact Weiler model.","To implement all the involved predicates and constructions, two geometric kernels are proposed, tested and discussed (arithmetic expansions and multi-precision floating-point).","As a guiding principle,the combinatorial information shared between each step is kept as simple as possible.","It is made possible by treating all the particular cases in the kernel.","In particular, triangles with intersections are remeshed using the (uniquely defined) Constrained Delaunay Triangulation, with symbolic perturbations to disambiguate configurations with co-cyclic points.","It makes it easy to discard the duplicated triangles that appear when remeshing overlapping facets.","The method is tested and compared with previous work, on the existing \"thingi10K\" dataset (to test co-refinement and mesh repair) and on a new \"thingiCSG\" dataset made publicly available (to test the full CSG pipeline) on a variety of interesting examples featuring different types of \"pathologies\""],"url":"http://arxiv.org/abs/2405.12949v1","category":"cs.CG"}
{"created":"2024-05-21 17:17:25","title":"Improved upper bounds for the Heilbronn's Problem for $k$-gons","abstract":"The Heilbronn triangle problem asks for the placement of $n$ points in a unit square that maximizes the smallest area of a triangle formed by any three of those points. In $1972$, Schmidt considered a natural generalization of this problem. He asked for the placement of $n$ points in a unit square that maximizes the smallest area of the convex hull formed by any four of those points. He showed a lower bound of $\\Omega(n^{-3/2})$, which was improved to $\\Omega(n^{-3/2}\\log{n})$ by Leffman.   A trivial upper bound of $3/n$ could be obtained, and Schmidt asked if this could be improved asymptotically. However, despite several efforts, no asymptotic improvement over the trivial upper bound was known for the last $50$ years, and the problem started to get the tag of being notoriously hard. Szemer{\\'e}di posed the question of whether one can, at least, improve the constant in this trivial upper bound. In this work, we answer this question by proving an upper bound of $2/n+o(1/n)$. We also extend our results to any convex hulls formed by $k\\geq 4$ points.","sentences":["The Heilbronn triangle problem asks for the placement of $n$ points in a unit square that maximizes the smallest area of a triangle formed by any three of those points.","In $1972$, Schmidt considered a natural generalization of this problem.","He asked for the placement of $n$ points in a unit square that maximizes the smallest area of the convex hull formed by any four of those points.","He showed a lower bound of $\\Omega(n^{-3/2})$, which was improved to $\\Omega(n^{-3/2}\\log{n})$ by Leffman.   ","A trivial upper bound of $3/n$ could be obtained, and Schmidt asked if this could be improved asymptotically.","However, despite several efforts, no asymptotic improvement over the trivial upper bound was known for the last $50$ years, and the problem started to get the tag of being notoriously hard.","Szemer{\\'e}di posed the question of whether one can, at least, improve the constant in this trivial upper bound.","In this work, we answer this question by proving an upper bound of $2/n+o(1/n)$. We also extend our results to any convex hulls formed by $k\\geq 4$ points."],"url":"http://arxiv.org/abs/2405.12945v1","category":"cs.DM"}
{"created":"2024-05-21 17:16:19","title":"The nucleon axial radius","abstract":"We present a systematic study of the relativistic axial-vector four-current distributions inside a nucleon. We show in particular that the slope of the axial form factor $G_A(Q^2)$ in the forward limit -- conventionally denoted as $R^2_A$ in the literature -- does not represent the three-dimensional mean-square axial radius in the Breit frame, but corresponds instead to a contribution to the mean-square spin radius. We derive explicit expressions for the latter in different frames and find in general additional contributions that depend on both the nucleon mass and the forward values of the axial-vector form factors $G_A(0)$ and $G_P(0)$.","sentences":["We present a systematic study of the relativistic axial-vector four-current distributions inside a nucleon.","We show in particular that the slope of the axial form factor $G_A(Q^2)$ in the forward limit -- conventionally denoted as $R^2_A$ in the literature -- does not represent the three-dimensional mean-square axial radius in the Breit frame, but corresponds instead to a contribution to the mean-square spin radius.","We derive explicit expressions for the latter in different frames and find in general additional contributions that depend on both the nucleon mass and the forward values of the axial-vector form factors $G_A(0)$ and $G_P(0)$."],"url":"http://arxiv.org/abs/2405.12943v1","category":"hep-ph"}
{"created":"2024-05-21 17:13:13","title":"Learning the Infinitesimal Generator of Stochastic Diffusion Processes","abstract":"We address data-driven learning of the infinitesimal generator of stochastic diffusion processes, essential for understanding numerical simulations of natural and physical systems. The unbounded nature of the generator poses significant challenges, rendering conventional analysis techniques for Hilbert-Schmidt operators ineffective. To overcome this, we introduce a novel framework based on the energy functional for these stochastic processes. Our approach integrates physical priors through an energy-based risk metric in both full and partial knowledge settings. We evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the partial knowledge setting. Notably, our approach provides learning bounds independent of the state space dimension and ensures non-spurious spectral estimation. Additionally, we elucidate how the distortion between the intrinsic energy-induced metric of the stochastic diffusion and the RKHS metric used for generator estimation impacts the spectral learning bounds.","sentences":["We address data-driven learning of the infinitesimal generator of stochastic diffusion processes, essential for understanding numerical simulations of natural and physical systems.","The unbounded nature of the generator poses significant challenges, rendering conventional analysis techniques for Hilbert-Schmidt operators ineffective.","To overcome this, we introduce a novel framework based on the energy functional for these stochastic processes.","Our approach integrates physical priors through an energy-based risk metric in both full and partial knowledge settings.","We evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the partial knowledge setting.","Notably, our approach provides learning bounds independent of the state space dimension and ensures non-spurious spectral estimation.","Additionally, we elucidate how the distortion between the intrinsic energy-induced metric of the stochastic diffusion and the RKHS metric used for generator estimation impacts the spectral learning bounds."],"url":"http://arxiv.org/abs/2405.12940v1","category":"stat.ML"}
{"created":"2024-05-21 17:08:44","title":"Asymptotic analysis of sum-rate under SIC","abstract":"Limitation of the cost of coordination and contention among a large number of nodes calls for grant-free approaches, exploiting physical layer techniques to solve collisions. Successive Interference Cancellation (SIC) is becoming a key building block of multiple access channel receiver, in an effort to support massive Internet of Things (IoT). In this paper, we explore the large-scale performance of SIC in a theoretical framework. A general model of a SIC receiver is stated for a shared channel with $n$ transmitters. The asymptotic sum-rate performance is characterized as $n \\rightarrow \\infty$, for a suitably scaled target Signal to Noise Interference Ratio (SNIR). The probability distribution of the number of correctly decoded packets is shown to tend to a deterministic distribution asymptotically for large values of $n$. The asymptotic analysis is carried out for any probability distribution of the wireless channel gain, assuming that the average received power level is same for all nodes, through power control.","sentences":["Limitation of the cost of coordination and contention among a large number of nodes calls for grant-free approaches, exploiting physical layer techniques to solve collisions.","Successive Interference Cancellation (SIC) is becoming a key building block of multiple access channel receiver, in an effort to support massive Internet of Things (IoT).","In this paper, we explore the large-scale performance of SIC in a theoretical framework.","A general model of a SIC receiver is stated for a shared channel with $n$ transmitters.","The asymptotic sum-rate performance is characterized as $n \\rightarrow \\infty$, for a suitably scaled target Signal to Noise Interference Ratio (SNIR).","The probability distribution of the number of correctly decoded packets is shown to tend to a deterministic distribution asymptotically for large values of $n$. The asymptotic analysis is carried out for any probability distribution of the wireless channel gain, assuming that the average received power level is same for all nodes, through power control."],"url":"http://arxiv.org/abs/2405.12937v1","category":"eess.SP"}
{"created":"2024-05-21 17:04:44","title":"Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs","abstract":"Large Language Models (LLMs) have shown remarkable capabilities in tasks such as summarization, arithmetic reasoning, and question answering. However, they encounter significant challenges in the domain of moral reasoning and ethical decision-making, especially in complex scenarios with multiple stakeholders. This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing moral reasoning in LLMs by exploring decisions' consequences from multiple stakeholder perspectives. Central to SKIG's mechanism is simulating accountability for actions, which, alongside empathy exercises and risk assessment, is pivotal to its effectiveness. We validate SKIG's performance across various moral reasoning benchmarks with proprietary and opensource LLMs, and investigate its crucial components through extensive ablation analyses.","sentences":["Large Language Models (LLMs) have shown remarkable capabilities in tasks such as summarization, arithmetic reasoning, and question answering.","However, they encounter significant challenges in the domain of moral reasoning and ethical decision-making, especially in complex scenarios with multiple stakeholders.","This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing moral reasoning in LLMs by exploring decisions' consequences from multiple stakeholder perspectives.","Central to SKIG's mechanism is simulating accountability for actions, which, alongside empathy exercises and risk assessment, is pivotal to its effectiveness.","We validate SKIG's performance across various moral reasoning benchmarks with proprietary and opensource LLMs, and investigate its crucial components through extensive ablation analyses."],"url":"http://arxiv.org/abs/2405.12933v1","category":"cs.CL"}
{"created":"2024-05-21 16:56:36","title":"Code-mixed Sentiment and Hate-speech Prediction","abstract":"Code-mixed discourse combines multiple languages in a single text. It is commonly used in informal discourse in countries with several official languages, but also in many other countries in combination with English or neighboring languages. As recently large language models have dominated most natural language processing tasks, we investigated their performance in code-mixed settings for relevant tasks. We first created four new bilingual pre-trained masked language models for English-Hindi and English-Slovene languages, specifically aimed to support informal language. Then we performed an evaluation of monolingual, bilingual, few-lingual, and massively multilingual models on several languages, using two tasks that frequently contain code-mixed text, in particular, sentiment analysis and offensive language detection in social media texts. The results show that the most successful classifiers are fine-tuned bilingual models and multilingual models, specialized for social media texts, followed by non-specialized massively multilingual and monolingual models, while huge generative models are not competitive. For our affective problems, the models mostly perform slightly better on code-mixed data compared to non-code-mixed data.","sentences":["Code-mixed discourse combines multiple languages in a single text.","It is commonly used in informal discourse in countries with several official languages, but also in many other countries in combination with English or neighboring languages.","As recently large language models have dominated most natural language processing tasks, we investigated their performance in code-mixed settings for relevant tasks.","We first created four new bilingual pre-trained masked language models for English-Hindi and English-Slovene languages, specifically aimed to support informal language.","Then we performed an evaluation of monolingual, bilingual, few-lingual, and massively multilingual models on several languages, using two tasks that frequently contain code-mixed text, in particular, sentiment analysis and offensive language detection in social media texts.","The results show that the most successful classifiers are fine-tuned bilingual models and multilingual models, specialized for social media texts, followed by non-specialized massively multilingual and monolingual models, while huge generative models are not competitive.","For our affective problems, the models mostly perform slightly better on code-mixed data compared to non-code-mixed data."],"url":"http://arxiv.org/abs/2405.12929v1","category":"cs.CL"}
{"created":"2024-05-21 16:53:07","title":"Displacement within velocity effect in gravitational wave memory","abstract":"Sandwich gravitational waves exhibit the velocity memory effect (VM) which however can become, for specific values of the wave parameters, pure displacement (DM) as suggested by Zel'dovich and Polnarev. Fixing such a \"miraculous\" value, the particle trajectory is an (approximate) standing wave characterized by a unique integer $m$, for which the particle does not absorb any energy from the passing wave. Our statements are illustrated by a simple Gaussian and by the P\\\"oschl-Teller potential as profiles.","sentences":["Sandwich gravitational waves exhibit the velocity memory effect (VM) which however can become, for specific values of the wave parameters, pure displacement (DM) as suggested by Zel'dovich and Polnarev.","Fixing such a \"miraculous\" value, the particle trajectory is an (approximate) standing wave characterized by a unique integer $m$, for which the particle does not absorb any energy from the passing wave.","Our statements are illustrated by a simple Gaussian and by the P\\\"oschl-Teller potential as profiles."],"url":"http://arxiv.org/abs/2405.12928v1","category":"gr-qc"}
{"created":"2024-05-21 16:53:03","title":"On Image Registration and Subpixel Estimation","abstract":"Image registration is a classical problem in machine vision which seeks methods to align discrete images of the same scene to subpixel accuracy in general situations. As with all estimation problems, the underlying difficulty is the partial information available about the ground truth. We consider a basic and idealized one-dimensional image registration problem motivated by questions about measurement and about quantization, and we demonstrate that the extent to which subinterval/subpixel inferences can be made in this setting depends on a type of complexity associated with the function of interest, the relationship between the function and the pixel size, and the number of distinct sampling count observations available.","sentences":["Image registration is a classical problem in machine vision which seeks methods to align discrete images of the same scene to subpixel accuracy in general situations.","As with all estimation problems, the underlying difficulty is the partial information available about the ground truth.","We consider a basic and idealized one-dimensional image registration problem motivated by questions about measurement and about quantization, and we demonstrate that the extent to which subinterval/subpixel inferences can be made in this setting depends on a type of complexity associated with the function of interest, the relationship between the function and the pixel size, and the number of distinct sampling count observations available."],"url":"http://arxiv.org/abs/2405.12927v1","category":"cs.CV"}
{"created":"2024-05-21 16:49:14","title":"Panmodal Information Interaction","abstract":"The emergence of generative artificial intelligence (GenAI) is transforming information interaction. For decades, search engines such as Google and Bing have been the primary means of locating relevant information for the general population. They have provided search results in the same standard format (the so-called \"10 blue links\"). The recent ability to chat via natural language with AI-based agents and have GenAI automatically synthesize answers in real-time (grounded in top-ranked results) is changing how people interact with and consume information at massive scale. These two information interaction modalities (traditional search and AI-powered chat) coexist in current search engines, either loosely coupled (e.g., as separate options/tabs) or tightly coupled (e.g., integrated as a chat answer embedded directly within a traditional search result page). We believe that the existence of these two different modalities, and potentially many others, is creating an opportunity to re-imagine the search experience, capitalize on the strengths of many modalities, and develop systems and strategies to support seamless flow between them. We refer to these as panmodal experiences. Unlike monomodal experiences, where only one modality is available and/or used for the task at hand, panmodal experiences make multiple modalities available to users (multimodal), directly support transitions between modalities (crossmodal), and seamlessly combine modalities to tailor task assistance (transmodal). While our focus is search and chat, with learnings from insights from a survey of over 100 individuals who have recently performed common tasks on these two modalities, we also present a more general vision for the future of information interaction using multiple modalities and the emergent capabilities of GenAI.","sentences":["The emergence of generative artificial intelligence (GenAI) is transforming information interaction.","For decades, search engines such as Google and Bing have been the primary means of locating relevant information for the general population.","They have provided search results in the same standard format (the so-called \"10 blue links\").","The recent ability to chat via natural language with AI-based agents and have GenAI automatically synthesize answers in real-time (grounded in top-ranked results) is changing how people interact with and consume information at massive scale.","These two information interaction modalities (traditional search and AI-powered chat) coexist in current search engines, either loosely coupled (e.g., as separate options/tabs) or tightly coupled (e.g., integrated as a chat answer embedded directly within a traditional search result page).","We believe that the existence of these two different modalities, and potentially many others, is creating an opportunity to re-imagine the search experience, capitalize on the strengths of many modalities, and develop systems and strategies to support seamless flow between them.","We refer to these as panmodal experiences.","Unlike monomodal experiences, where only one modality is available and/or used for the task at hand, panmodal experiences make multiple modalities available to users (multimodal), directly support transitions between modalities (crossmodal), and seamlessly combine modalities to tailor task assistance (transmodal).","While our focus is search and chat, with learnings from insights from a survey of over 100 individuals who have recently performed common tasks on these two modalities, we also present a more general vision for the future of information interaction using multiple modalities and the emergent capabilities of GenAI."],"url":"http://arxiv.org/abs/2405.12923v1","category":"cs.IR"}
{"created":"2024-05-21 16:46:47","title":"Quantum optimal control robust to $1/f^\u03b1$ noises using fractional calculus: voltage-controlled exchange in semiconductor spin qubits","abstract":"Low-frequency $1/f^\\alpha$ charge noise significantly hinders the performance of voltage-controlled spin qubits in quantum dots. Here, we utilize fractional calculus to design voltage control pulses yielding the highest average fidelities for noisy quantum gate operations. We focus specifically on the exponential voltage control of the exchange interaction generating two-spin $\\mathrm{SWAP}^k$ gates. When stationary charge noise is the dominant source of gate infidelity, we derive that the optimal exchange pulse is long and weak, with the broad shape of the symmetric beta distribution function with parameter $1-\\alpha/2$. The common practice of making exchange pulses fast and high-amplitude still remains beneficial in the case of strongly nonstationary noise dynamics, modeled as fractional Brownian motion. The proposed methods are applicable to the characterization and optimization of quantum gate operations in various voltage-controlled qubit architectures.","sentences":["Low-frequency $1/f^\\alpha$ charge noise significantly hinders the performance of voltage-controlled spin qubits in quantum dots.","Here, we utilize fractional calculus to design voltage control pulses yielding the highest average fidelities for noisy quantum gate operations.","We focus specifically on the exponential voltage control of the exchange interaction generating two-spin $\\mathrm{SWAP}^k$ gates.","When stationary charge noise is the dominant source of gate infidelity, we derive that the optimal exchange pulse is long and weak, with the broad shape of the symmetric beta distribution function with parameter $1-\\alpha/2$. The common practice of making exchange pulses fast and high-amplitude still remains beneficial in the case of strongly nonstationary noise dynamics, modeled as fractional Brownian motion.","The proposed methods are applicable to the characterization and optimization of quantum gate operations in various voltage-controlled qubit architectures."],"url":"http://arxiv.org/abs/2405.12922v1","category":"quant-ph"}
{"created":"2024-05-21 16:38:20","title":"Genuine $k$-partite correlations and entanglement in the ground state of the Dicke model for interacting qubits","abstract":"The analysis of correlations among subsystems is essential for both the understanding of critical phenomena and for performing quantum information tasks. However, the majority of correlation measures are restricted to bipartitions due to the inherent challenges associated with handling multiple partitions and subsystems. To address this, we investigate Genuine Multipartite Correlations (GMC) of the Dicke model with interacting qubits. This method allows for the precise quantification of correlations within each subpart of the system, as well as for the percentage contribution of each GMC of order $k$. Most importantly, we show that GMC signal both first- and second-order quantum phase transitions present in the model. Furthermore, we employ Quantum Fisher Information (QFI) to detect genuine multipartite entanglement, since the GMC encompass both classical and quantum correlations. Ultimately, we compare the Dicke model with interacting qubits to spin-centers in solids interacting with a quantum field of magnons to demonstrate a potential experimental realization of this generalized Dicke model.","sentences":["The analysis of correlations among subsystems is essential for both the understanding of critical phenomena and for performing quantum information tasks.","However, the majority of correlation measures are restricted to bipartitions due to the inherent challenges associated with handling multiple partitions and subsystems.","To address this, we investigate Genuine Multipartite Correlations (GMC) of the Dicke model with interacting qubits.","This method allows for the precise quantification of correlations within each subpart of the system, as well as for the percentage contribution of each GMC of order $k$. Most importantly, we show that GMC signal both first- and second-order quantum phase transitions present in the model.","Furthermore, we employ Quantum Fisher Information (QFI) to detect genuine multipartite entanglement, since the GMC encompass both classical and quantum correlations.","Ultimately, we compare the Dicke model with interacting qubits to spin-centers in solids interacting with a quantum field of magnons to demonstrate a potential experimental realization of this generalized Dicke model."],"url":"http://arxiv.org/abs/2405.12916v1","category":"quant-ph"}
{"created":"2024-05-21 16:38:13","title":"G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation","abstract":"Large Language Models (LLMs) have demonstrated remarkable abilities in general scenarios. Instruction finetuning empowers them to align with humans in various tasks. Nevertheless, the Diversity and Quality of the instruction data remain two main challenges for instruction finetuning. With regard to this, in this paper, we propose a novel gradient-based method to automatically select high-quality and diverse instruction finetuning data for machine translation. Our key innovation centers around analyzing how individual training examples influence the model during training. Specifically, we select training examples that exert beneficial influences on the model as high-quality ones by means of Influence Function plus a small high-quality seed dataset. Moreover, to enhance the diversity of the training data we maximize the variety of influences they have on the model by clustering on their gradients and resampling. Extensive experiments on WMT22 and FLORES translation tasks demonstrate the superiority of our methods, and in-depth analysis further validates their effectiveness and generalization.","sentences":["Large Language Models (LLMs) have demonstrated remarkable abilities in general scenarios.","Instruction finetuning empowers them to align with humans in various tasks.","Nevertheless, the Diversity and Quality of the instruction data remain two main challenges for instruction finetuning.","With regard to this, in this paper, we propose a novel gradient-based method to automatically select high-quality and diverse instruction finetuning data for machine translation.","Our key innovation centers around analyzing how individual training examples influence the model during training.","Specifically, we select training examples that exert beneficial influences on the model as high-quality ones by means of Influence Function plus a small high-quality seed dataset.","Moreover, to enhance the diversity of the training data we maximize the variety of influences they have on the model by clustering on their gradients and resampling.","Extensive experiments on WMT22 and FLORES translation tasks demonstrate the superiority of our methods, and in-depth analysis further validates their effectiveness and generalization."],"url":"http://arxiv.org/abs/2405.12915v1","category":"cs.CL"}
{"created":"2024-05-21 16:35:02","title":"An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation","abstract":"One critical prerequisite for faithful text-to-image generation is the accurate understanding of text inputs. Existing methods leverage the text encoder of the CLIP model to represent input prompts. However, the pre-trained CLIP model can merely encode English with a maximum token length of 77. Moreover, the model capacity of the text encoder from CLIP is relatively limited compared to Large Language Models (LLMs), which offer multilingual input, accommodate longer context, and achieve superior text representation. In this paper, we investigate LLMs as the text encoder to improve the language understanding in text-to-image generation. Unfortunately, training text-to-image generative model with LLMs from scratch demands significant computational resources and data. To this end, we introduce a three-stage training pipeline that effectively and efficiently integrates the existing text-to-image model with LLMs. Specifically, we propose a lightweight adapter that enables fast training of the text-to-image model using the textual representations from LLMs. Extensive experiments demonstrate that our model supports not only multilingual but also longer input context with superior image generation quality.","sentences":["One critical prerequisite for faithful text-to-image generation is the accurate understanding of text inputs.","Existing methods leverage the text encoder of the CLIP model to represent input prompts.","However, the pre-trained CLIP model can merely encode English with a maximum token length of 77.","Moreover, the model capacity of the text encoder from CLIP is relatively limited compared to Large Language Models (LLMs), which offer multilingual input, accommodate longer context, and achieve superior text representation.","In this paper, we investigate LLMs as the text encoder to improve the language understanding in text-to-image generation.","Unfortunately, training text-to-image generative model with LLMs from scratch demands significant computational resources and data.","To this end, we introduce a three-stage training pipeline that effectively and efficiently integrates the existing text-to-image model with LLMs.","Specifically, we propose a lightweight adapter that enables fast training of the text-to-image model using the textual representations from LLMs.","Extensive experiments demonstrate that our model supports not only multilingual but also longer input context with superior image generation quality."],"url":"http://arxiv.org/abs/2405.12914v1","category":"cs.CV"}
{"created":"2024-05-21 16:33:39","title":"A Hamiltonian, post-Born, three-dimensional, on-the-fly ray tracing algorithm for gravitational lensing","abstract":"The analyses of the next generation cosmological surveys demand an accurate, efficient, and differentiable method for simulating the universe and its observables across cosmological volumes. We present Hamiltonian ray tracing (HRT) -- the first post-Born (accounting for lens-lens coupling and without relying on the Born approximation), three-dimensional (without assuming the thin-lens approximation), and on-the-fly (applicable to any structure formation simulations) ray tracing algorithm based on the Hamiltonian formalism. HRT performs symplectic integration of the photon geodesics in a weak gravitational field, and can integrate tightly with any gravity solver, enabling co-evolution of matter particles and light rays with minimal additional computations. We implement HRT in the particle-mesh library \\texttt{pmwd}, leveraging hardware accelerators such as GPUs and automatic differentiation capabilities based on \\texttt{JAX}. When tested on a point-mass lens, HRT achieves sub-percent accuracy in deflection angles above the resolution limit across both weak and moderately strong lensing regimes. We also test HRT in cosmological simulations on the convergence maps and their power spectra.","sentences":["The analyses of the next generation cosmological surveys demand an accurate, efficient, and differentiable method for simulating the universe and its observables across cosmological volumes.","We present Hamiltonian ray tracing (HRT) -- the first post-Born (accounting for lens-lens coupling and without relying on the Born approximation), three-dimensional (without assuming the thin-lens approximation), and on-the-fly (applicable to any structure formation simulations) ray tracing algorithm based on the Hamiltonian formalism.","HRT performs symplectic integration of the photon geodesics in a weak gravitational field, and can integrate tightly with any gravity solver, enabling co-evolution of matter particles and light rays with minimal additional computations.","We implement HRT in the particle-mesh library \\texttt{pmwd}, leveraging hardware accelerators such as GPUs and automatic differentiation capabilities based on \\texttt{JAX}.","When tested on a point-mass lens, HRT achieves sub-percent accuracy in deflection angles above the resolution limit across both weak and moderately strong lensing regimes.","We also test HRT in cosmological simulations on the convergence maps and their power spectra."],"url":"http://arxiv.org/abs/2405.12913v1","category":"astro-ph.CO"}
{"created":"2024-05-21 16:31:29","title":"Stationary surfaces of height-dependent weighted area functionals in $\\mathbb{R}^3$ and $\\mathbb{L}^3$","abstract":"We describe a general correspondence between weighted minimal surfaces in $\\mathbb{R}^3$ and weighted maximal surfaces with some admissible singularities in $\\mathbb{L}^3$, for a class of functions $\\varphi$ which provides the corresponding weight. For these families of surfaces, we provide a Weierstrass representation when $\\dot{\\varphi}\\neq 0$ and analyze in detail the asymptotic behavior of both such a weighted maximal surface around its singular set and its corresponding weighted minimal immersion around the nodal set of its angle function, establishing criteria that allow us to easily determine the type of singularity and classify the associated moduli spaces.","sentences":["We describe a general correspondence between weighted minimal surfaces in $\\mathbb{R}^3$ and weighted maximal surfaces with some admissible singularities in $\\mathbb{L}^3$, for a class of functions $\\varphi$ which provides the corresponding weight.","For these families of surfaces, we provide a Weierstrass representation when $\\dot{\\varphi}\\neq 0$ and analyze in detail the asymptotic behavior of both such a weighted maximal surface around its singular set and its corresponding weighted minimal immersion around the nodal set of its angle function, establishing criteria that allow us to easily determine the type of singularity and classify the associated moduli spaces."],"url":"http://arxiv.org/abs/2405.12911v1","category":"math.DG"}
{"created":"2024-05-21 16:30:25","title":"Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment","abstract":"This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic modelling summary judgment cases in the United Kingdom. Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends. We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.10%. The analysis reveals distinct patterns in the application of summary judgments across various legal domains. As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification. Therefore, this paper provides a new and general taxonomy for UK law. The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies.","sentences":["This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic modelling summary judgment cases in the United Kingdom.","Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends.","We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.10%.","The analysis reveals distinct patterns in the application of summary judgments across various legal domains.","As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification.","Therefore, this paper provides a new and general taxonomy for UK law.","The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies."],"url":"http://arxiv.org/abs/2405.12910v1","category":"cs.CL"}
{"created":"2024-05-21 16:22:06","title":"Exponential Steepest Ascent from Valued Constraint Graphs of Pathwidth Four","abstract":"We examine the complexity of maximising fitness via local search on valued constraint satisfaction problems (VCSPs). We consider two kinds of local ascents: (1) steepest ascents, where each step changes the domain that produces a maximal increase in fitness; and (2) $\\prec$-ordered ascents, where -- of the domains with available fitness increasing changes -- each step changes the $\\prec$-minimal domain. We provide a general padding argument to simulate any ordered ascent by a steepest ascent. We construct a VCSP that is a path of binary constraints between alternating 2-state and 3-state domains with exponentially long ordered ascents. We apply our padding argument to this VCSP to obtain a Boolean VCSP that has a constraint (hyper)graph of arity 5 and pathwidth 4 with exponential steepest ascents. This is an improvement on the previous best known construction for long steepest ascents, which had arity 8 and pathwidth 7.","sentences":["We examine the complexity of maximising fitness via local search on valued constraint satisfaction problems (VCSPs).","We consider two kinds of local ascents: (1) steepest ascents, where each step changes the domain that produces a maximal increase in fitness; and (2) $\\prec$-ordered ascents, where -- of the domains with available fitness increasing changes -- each step changes the $\\prec$-minimal domain.","We provide a general padding argument to simulate any ordered ascent by a steepest ascent.","We construct a VCSP that is a path of binary constraints between alternating 2-state and 3-state domains with exponentially long ordered ascents.","We apply our padding argument to this VCSP to obtain a Boolean VCSP that has a constraint (hyper)graph of arity 5 and pathwidth 4 with exponential steepest ascents.","This is an improvement on the previous best known construction for long steepest ascents, which had arity 8 and pathwidth 7."],"url":"http://arxiv.org/abs/2405.12906v1","category":"cs.DM"}
{"created":"2024-05-21 16:18:17","title":"Memory effects in colloidal motion under confinement and driving","abstract":"The transport of individual particles in inhomogeneous environments is complex and exhibits non-Markovian responses. The latter may be quantified by a memory function within the framework of the linear generalised Langevin equation (GLE). Here, we exemplify the implications of steady driving on the memory function of a colloidal model system for Brownian motion in a corrugated potential landscape, specifically, for one-dimensional motion in a sinusoidal potential. To this end, we consider the overdamped limit of the GLE, which is facilitated by separating the memory function into a singular (Markovian) and a regular (non-Markovian) part. Relying on exact solutions for the investigated model, we show that the random force entering the GLE must display a bias far from equilibrium, which corroborates a recent general prediction. Based on data for the mean-square displacement obtained from Brownian dynamics simulations, we estimate the memory function for different driving strengths and show that already moderate driving accelerates the decay of the memory function by several orders of magnitude in time. We find that the memory may persist on much longer timescales than expected from the convergence of the mean-square displacement to its long-time asymptote. Furthermore, the functional form of the memory function changes from a monotonic decay to a non-monotonic, damped oscillatory behaviour, which can be understood from a competition of confined motion and depinning. Our analysis of the simulation data further reveals a pronounced non-Gaussianity, which questions the Gaussian approximation of the random force entering the GLE.","sentences":["The transport of individual particles in inhomogeneous environments is complex and exhibits non-Markovian responses.","The latter may be quantified by a memory function within the framework of the linear generalised Langevin equation (GLE).","Here, we exemplify the implications of steady driving on the memory function of a colloidal model system for Brownian motion in a corrugated potential landscape, specifically, for one-dimensional motion in a sinusoidal potential.","To this end, we consider the overdamped limit of the GLE, which is facilitated by separating the memory function into a singular (Markovian) and a regular (non-Markovian) part.","Relying on exact solutions for the investigated model, we show that the random force entering the GLE must display a bias far from equilibrium, which corroborates a recent general prediction.","Based on data for the mean-square displacement obtained from Brownian dynamics simulations, we estimate the memory function for different driving strengths and show that already moderate driving accelerates the decay of the memory function by several orders of magnitude in time.","We find that the memory may persist on much longer timescales than expected from the convergence of the mean-square displacement to its long-time asymptote.","Furthermore, the functional form of the memory function changes from a monotonic decay to a non-monotonic, damped oscillatory behaviour, which can be understood from a competition of confined motion and depinning.","Our analysis of the simulation data further reveals a pronounced non-Gaussianity, which questions the Gaussian approximation of the random force entering the GLE."],"url":"http://arxiv.org/abs/2405.12904v1","category":"cond-mat.soft"}
{"created":"2024-05-21 16:14:55","title":"Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents","abstract":"Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model's resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.","sentences":["Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies.","Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience.","In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO).","The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token.","We demonstrate that ADPO enhances the model's resilience against harmful conversations while minimizing performance degradation.","Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO.","To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data."],"url":"http://arxiv.org/abs/2405.12900v1","category":"cs.CL"}
{"created":"2024-05-21 16:00:04","title":"Block Encodings of Discrete Subgroups on Quantum Computer","abstract":"We introduce a block encoding method for mapping discrete subgroups to qubits on a quantum computer. This method is applicable to general discrete groups, including crystal-like subgroups such as $\\mathbb{BI}$ of $SU(2)$ and $\\mathbb{V}$ of $SU(3)$. We detail the construction of primitive gates -- the inversion gate, the group multiplication gate, the trace gate, and the group Fourier gate -- utilizing this encoding method for $\\mathbb{BT}$ and for the first time $\\mathbb{BI}$ group. We also provide resource estimations to extract the gluon viscosity. The inversion gates for $\\mathbb{BT}$ and $\\mathbb{BI}$ are benchmarked on the $\\texttt{Baiwang}$ quantum computer with estimated fidelities of $40^{+5}_{-4}\\%$ and $4^{+5}_{-3}\\%$ respectively.","sentences":["We introduce a block encoding method for mapping discrete subgroups to qubits on a quantum computer.","This method is applicable to general discrete groups, including crystal-like subgroups such as $\\mathbb{BI}$ of $SU(2)$ and $\\mathbb{V}$ of $SU(3)$. We detail the construction of primitive gates -- the inversion gate, the group multiplication gate, the trace gate, and the group Fourier gate -- utilizing this encoding method for $\\mathbb{BT}$ and for the first time $\\mathbb{BI}$ group.","We also provide resource estimations to extract the gluon viscosity.","The inversion gates for $\\mathbb{BT}$ and $\\mathbb{BI}$ are benchmarked on the $\\texttt{Baiwang}$ quantum computer with estimated fidelities of $40^{+5}_{-4}\\%$ and $4^{+5}_{-3}\\%$ respectively."],"url":"http://arxiv.org/abs/2405.12890v1","category":"hep-lat"}
{"created":"2024-05-21 15:59:55","title":"Keep the Momentum: Conservation Laws beyond Euclidean Gradient Flows","abstract":"Conservation laws are well-established in the context of Euclidean gradient flow dynamics, notably for linear or ReLU neural network training. Yet, their existence and principles for non-Euclidean geometries and momentum-based dynamics remain largely unknown. In this paper, we characterize \"all\" conservation laws in this general setting. In stark contrast to the case of gradient flows, we prove that the conservation laws for momentum-based dynamics exhibit temporal dependence. Additionally, we often observe a \"conservation loss\" when transitioning from gradient flow to momentum dynamics. Specifically, for linear networks, our framework allows us to identify all momentum conservation laws, which are less numerous than in the gradient flow case except in sufficiently over-parameterized regimes. With ReLU networks, no conservation law remains. This phenomenon also manifests in non-Euclidean metrics, used e.g. for Nonnegative Matrix Factorization (NMF): all conservation laws can be determined in the gradient flow context, yet none persists in the momentum case.","sentences":["Conservation laws are well-established in the context of Euclidean gradient flow dynamics, notably for linear or ReLU neural network training.","Yet, their existence and principles for non-Euclidean geometries and momentum-based dynamics remain largely unknown.","In this paper, we characterize \"all\" conservation laws in this general setting.","In stark contrast to the case of gradient flows, we prove that the conservation laws for momentum-based dynamics exhibit temporal dependence.","Additionally, we often observe a \"conservation loss\" when transitioning from gradient flow to momentum dynamics.","Specifically, for linear networks, our framework allows us to identify all momentum conservation laws, which are less numerous than in the gradient flow case except in sufficiently over-parameterized regimes.","With ReLU networks, no conservation law remains.","This phenomenon also manifests in non-Euclidean metrics, used e.g. for Nonnegative Matrix Factorization (NMF): all conservation laws can be determined in the gradient flow context, yet none persists in the momentum case."],"url":"http://arxiv.org/abs/2405.12888v1","category":"cs.LG"}
{"created":"2024-05-21 15:55:09","title":"Investigating Persuasion Techniques in Arabic: An Empirical Study Leveraging Large Language Models","abstract":"In the current era of digital communication and widespread use of social media, it is crucial to develop an understanding of persuasive techniques employed in written text. This knowledge is essential for effectively discerning accurate information and making informed decisions. To address this need, this paper presents a comprehensive empirical study focused on identifying persuasive techniques in Arabic social media content. To achieve this objective, we utilize Pre-trained Language Models (PLMs) and leverage the ArAlEval dataset, which encompasses two tasks: binary classification to determine the presence or absence of persuasion techniques, and multi-label classification to identify the specific types of techniques employed in the text. Our study explores three different learning approaches by harnessing the power of PLMs: feature extraction, fine-tuning, and prompt engineering techniques. Through extensive experimentation, we find that the fine-tuning approach yields the highest results on the aforementioned dataset, achieving an f1-micro score of 0.865 and an f1-weighted score of 0.861. Furthermore, our analysis sheds light on an interesting finding. While the performance of the GPT model is relatively lower compared to the other approaches, we have observed that by employing few-shot learning techniques, we can enhance its results by up to 20\\%. This offers promising directions for future research and exploration in this topic\\footnote{Upon Acceptance, the source code will be released on GitHub.}.","sentences":["In the current era of digital communication and widespread use of social media, it is crucial to develop an understanding of persuasive techniques employed in written text.","This knowledge is essential for effectively discerning accurate information and making informed decisions.","To address this need, this paper presents a comprehensive empirical study focused on identifying persuasive techniques in Arabic social media content.","To achieve this objective, we utilize Pre-trained Language Models (PLMs) and leverage the ArAlEval dataset, which encompasses two tasks: binary classification to determine the presence or absence of persuasion techniques, and multi-label classification to identify the specific types of techniques employed in the text.","Our study explores three different learning approaches by harnessing the power of PLMs: feature extraction, fine-tuning, and prompt engineering techniques.","Through extensive experimentation, we find that the fine-tuning approach yields the highest results on the aforementioned dataset, achieving an f1-micro score of 0.865 and an f1-weighted score of 0.861.","Furthermore, our analysis sheds light on an interesting finding.","While the performance of the GPT model is relatively lower compared to the other approaches, we have observed that by employing few-shot learning techniques, we can enhance its results by up to 20\\%.","This offers promising directions for future research and exploration in this topic\\footnote{Upon Acceptance, the source code will be released on GitHub.}."],"url":"http://arxiv.org/abs/2405.12884v1","category":"cs.CL"}
{"created":"2024-05-21 15:53:35","title":"Explaining Expert Search and Team Formation Systems with ExES","abstract":"Expert search and team formation systems operate on collaboration networks, with nodes representing individuals, labeled with their skills, and edges denoting collaboration relationships. Given a keyword query corresponding to the desired skills, these systems identify experts that best match the query. However, state-of-the-art solutions to this problem lack transparency. To address this issue, we propose ExES, a tool designed to explain expert search and team formation systems using factual and counterfactual methods from the field of explainable artificial intelligence (XAI). ExES uses factual explanations to highlight important skills and collaborations, and counterfactual explanations to suggest new skills and collaborations to increase the likelihood of being identified as an expert. Towards a practical deployment as an interactive explanation tool, we present and experimentally evaluate a suite of pruning strategies to speed up the explanation search. In many cases, our pruning strategies make ExES an order of magnitude faster than exhaustive search, while still producing concise and actionable explanations.","sentences":["Expert search and team formation systems operate on collaboration networks, with nodes representing individuals, labeled with their skills, and edges denoting collaboration relationships.","Given a keyword query corresponding to the desired skills, these systems identify experts that best match the query.","However, state-of-the-art solutions to this problem lack transparency.","To address this issue, we propose ExES, a tool designed to explain expert search and team formation systems using factual and counterfactual methods from the field of explainable artificial intelligence (XAI).","ExES uses factual explanations to highlight important skills and collaborations, and counterfactual explanations to suggest new skills and collaborations to increase the likelihood of being identified as an expert.","Towards a practical deployment as an interactive explanation tool, we present and experimentally evaluate a suite of pruning strategies to speed up the explanation search.","In many cases, our pruning strategies make ExES an order of magnitude faster than exhaustive search, while still producing concise and actionable explanations."],"url":"http://arxiv.org/abs/2405.12881v1","category":"cs.DB"}
{"created":"2024-05-21 15:52:18","title":"Measuring Hawking Radiation from Black Hole Morsels in Astrophysical Black Hole Mergers","abstract":"We show that it is possible to observe the Hawking radiation emitted by small black holes assumed to form in catastrophic astrophysical events such as black hole mergers. Gamma ray bursts in the TeV range are unique footprints of these asteroid-mass black hole morsels ejected during the merger. The time delay of the gamma ray bursts from the gravitational wave event is correlated to the mass distribution of the morsels. The integrated mass of the morsels allowed by the unaccounted merger mass leads to a Hawking induced radiation in photons that is above the sensitivity of atmospheric Cherenkov telescopes such as HESS, LHAASO and HAWC.","sentences":["We show that it is possible to observe the Hawking radiation emitted by small black holes assumed to form in catastrophic astrophysical events such as black hole mergers.","Gamma ray bursts in the TeV range are unique footprints of these asteroid-mass black hole morsels ejected during the merger.","The time delay of the gamma ray bursts from the gravitational wave event is correlated to the mass distribution of the morsels.","The integrated mass of the morsels allowed by the unaccounted merger mass leads to a Hawking induced radiation in photons that is above the sensitivity of atmospheric Cherenkov telescopes such as HESS, LHAASO and HAWC."],"url":"http://arxiv.org/abs/2405.12880v1","category":"astro-ph.HE"}
{"created":"2024-05-21 15:44:31","title":"Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in Remote Sensing Images","abstract":"Remote sensing image change captioning (RSICC) aims at generating human-like language to describe the semantic changes between bi-temporal remote sensing image pairs. It provides valuable insights into environmental dynamics and land management. Unlike conventional change captioning task, RSICC involves not only retrieving relevant information across different modalities and generating fluent captions, but also mitigating the impact of pixel-level differences on terrain change localization. The pixel problem due to long time span decreases the accuracy of generated caption. Inspired by the remarkable generative power of diffusion model, we propose a probabilistic diffusion model for RSICC to solve the aforementioned problems. In training process, we construct a noise predictor conditioned on cross modal features to learn the distribution from the real caption distribution to the standard Gaussian distribution under the Markov chain. Meanwhile, a cross-mode fusion and a stacking self-attention module are designed for noise predictor in the reverse process. In testing phase, the well-trained noise predictor helps to estimate the mean value of the distribution and generate change captions step by step. Extensive experiments on the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC and its individual components. The quantitative results showcase superior performance over existing methods across both traditional and newly augmented metrics. The code and materials will be available online at https://github.com/Fay-Y/Diffusion-RSCC.","sentences":["Remote sensing image change captioning (RSICC) aims at generating human-like language to describe the semantic changes between bi-temporal remote sensing image pairs.","It provides valuable insights into environmental dynamics and land management.","Unlike conventional change captioning task, RSICC involves not only retrieving relevant information across different modalities and generating fluent captions, but also mitigating the impact of pixel-level differences on terrain change localization.","The pixel problem due to long time span decreases the accuracy of generated caption.","Inspired by the remarkable generative power of diffusion model, we propose a probabilistic diffusion model for RSICC to solve the aforementioned problems.","In training process, we construct a noise predictor conditioned on cross modal features to learn the distribution from the real caption distribution to the standard Gaussian distribution under the Markov chain.","Meanwhile, a cross-mode fusion and a stacking self-attention module are designed for noise predictor in the reverse process.","In testing phase, the well-trained noise predictor helps to estimate the mean value of the distribution and generate change captions step by step.","Extensive experiments on the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC and its individual components.","The quantitative results showcase superior performance over existing methods across both traditional and newly augmented metrics.","The code and materials will be available online at https://github.com/Fay-Y/Diffusion-RSCC."],"url":"http://arxiv.org/abs/2405.12875v1","category":"cs.CV"}
{"created":"2024-05-21 15:41:34","title":"Spatial-aware Attention Generative Adversarial Network for Semi-supervised Anomaly Detection in Medical Image","abstract":"Medical anomaly detection is a critical research area aimed at recognizing abnormal images to aid in diagnosis.Most existing methods adopt synthetic anomalies and image restoration on normal samples to detect anomaly. The unlabeled data consisting of both normal and abnormal data is not well explored. We introduce a novel Spatial-aware Attention Generative Adversarial Network (SAGAN) for one-class semi-supervised generation of health images.Our core insight is the utilization of position encoding and attention to accurately focus on restoring abnormal regions and preserving normal regions. To fully utilize the unlabelled data, SAGAN relaxes the cyclic consistency requirement of the existing unpaired image-to-image conversion methods, and generates high-quality health images corresponding to unlabeled data, guided by the reconstruction of normal images and restoration of pseudo-anomaly images.Subsequently, the discrepancy between the generated healthy image and the original image is utilized as an anomaly score.Extensive experiments on three medical datasets demonstrate that the proposed SAGAN outperforms the state-of-the-art methods.","sentences":["Medical anomaly detection is a critical research area aimed at recognizing abnormal images to aid in diagnosis.","Most existing methods adopt synthetic anomalies and image restoration on normal samples to detect anomaly.","The unlabeled data consisting of both normal and abnormal data is not well explored.","We introduce a novel Spatial-aware Attention Generative Adversarial Network (SAGAN) for one-class semi-supervised generation of health images.","Our core insight is the utilization of position encoding and attention to accurately focus on restoring abnormal regions and preserving normal regions.","To fully utilize the unlabelled data, SAGAN relaxes the cyclic consistency requirement of the existing unpaired image-to-image conversion methods, and generates high-quality health images corresponding to unlabeled data, guided by the reconstruction of normal images and restoration of pseudo-anomaly images.","Subsequently, the discrepancy between the generated healthy image and the original image is utilized as an anomaly score.","Extensive experiments on three medical datasets demonstrate that the proposed SAGAN outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.12872v1","category":"eess.IV"}
{"created":"2024-05-21 15:33:21","title":"Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics","abstract":"Learning to represent and simulate the dynamics of physical systems is a crucial yet challenging task. Existing equivariant Graph Neural Network (GNN) based methods have encapsulated the symmetry of physics, \\emph{e.g.}, translations, rotations, etc, leading to better generalization ability. Nevertheless, their frame-to-frame formulation of the task overlooks the non-Markov property mainly incurred by unobserved dynamics in the environment. In this paper, we reformulate dynamics simulation as a spatio-temporal prediction task, by employing the trajectory in the past period to recover the Non-Markovian interactions. We propose Equivariant Spatio-Temporal Attentive Graph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to fulfill our purpose. At its core, we design a novel Equivariant Discrete Fourier Transform (EDFT) to extract periodic patterns from the history frames, and then construct an Equivariant Spatial Module (ESM) to accomplish spatial message passing, and an Equivariant Temporal Module (ETM) with the forward attention and equivariant pooling mechanisms to aggregate temporal message. We evaluate our model on three real datasets corresponding to the molecular-, protein- and macro-level. Experimental results verify the effectiveness of ESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.","sentences":["Learning to represent and simulate the dynamics of physical systems is a crucial yet challenging task.","Existing equivariant Graph Neural Network (GNN) based methods have encapsulated the symmetry of physics, \\emph{e.g.}, translations, rotations, etc, leading to better generalization ability.","Nevertheless, their frame-to-frame formulation of the task overlooks the non-Markov property mainly incurred by unobserved dynamics in the environment.","In this paper, we reformulate dynamics simulation as a spatio-temporal prediction task, by employing the trajectory in the past period to recover the Non-Markovian interactions.","We propose Equivariant Spatio-Temporal Attentive Graph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to fulfill our purpose.","At its core, we design a novel Equivariant Discrete Fourier Transform (EDFT) to extract periodic patterns from the history frames, and then construct an Equivariant Spatial Module (ESM) to accomplish spatial message passing, and an Equivariant Temporal Module (ETM) with the forward attention and equivariant pooling mechanisms to aggregate temporal message.","We evaluate our model on three real datasets corresponding to the molecular-, protein- and macro-level.","Experimental results verify the effectiveness of ESTAG compared to typical spatio-temporal GNNs and equivariant GNNs."],"url":"http://arxiv.org/abs/2405.12868v1","category":"cs.LG"}
{"created":"2024-05-21 15:32:16","title":"Leveraging Quantum Machine Learning Generalization to Significantly Speed-up Quantum Compilation","abstract":"Existing numerical optimizers deployed in quantum compilers use expensive $\\mathcal{O}(4^n)$ matrix-matrix operations. Inspired by recent advances in quantum machine learning (QML), QFactor-Sample replaces matrix-matrix operations with simpler $\\mathcal{O}(2^n)$ circuit simulations on a set of sample inputs. The simpler the circuit, the lower the number of required input samples. We validate QFactor-Sample on a large set of circuits and discuss its hyperparameter tuning. When incorporated in the BQSKit quantum compiler and compared against a state-of-the-art domain-specific optimizer, We demonstrate improved scalability and a reduction in compile time, achieving an average speedup factor of {\\bf 69} for circuits with more than 8 qubits. We also discuss how improved numerical optimization affects the dynamics of partitioning-based compilation schemes, which allow a trade-off between compilation speed and solution quality.","sentences":["Existing numerical optimizers deployed in quantum compilers use expensive $\\mathcal{O}(4^n)$ matrix-matrix operations.","Inspired by recent advances in quantum machine learning (QML), QFactor-Sample replaces matrix-matrix operations with simpler $\\mathcal{O}(2^n)$ circuit simulations on a set of sample inputs.","The simpler the circuit, the lower the number of required input samples.","We validate QFactor-Sample on a large set of circuits and discuss its hyperparameter tuning.","When incorporated in the BQSKit quantum compiler and compared against a state-of-the-art domain-specific optimizer, We demonstrate improved scalability and a reduction in compile time, achieving an average speedup factor of {\\bf 69} for circuits with more than 8 qubits.","We also discuss how improved numerical optimization affects the dynamics of partitioning-based compilation schemes, which allow a trade-off between compilation speed and solution quality."],"url":"http://arxiv.org/abs/2405.12866v1","category":"quant-ph"}
{"created":"2024-05-21 15:30:25","title":"Robust portfolio optimization model for electronic coupon allocation","abstract":"Currently, many e-commerce websites issue online/electronic coupons as an effective tool for promoting sales of various products and services. We focus on the problem of optimally allocating coupons to customers subject to a budget constraint on an e-commerce website. We apply a robust portfolio optimization model based on customer segmentation to the coupon allocation problem. We also validate the efficacy of our method through numerical experiments using actual data from randomly distributed coupons. Main contributions of our research are twofold. First, we handle six types of coupons, thereby making it extremely difficult to accurately estimate the difference in the effects of various coupons. Second, we demonstrate from detailed numerical results that the robust optimization model achieved larger uplifts of sales than did the commonly-used multiple-choice knapsack model and the conventional mean-variance optimization model. Our results open up great potential for robust portfolio optimization as an effective tool for practical coupon allocation.","sentences":["Currently, many e-commerce websites issue online/electronic coupons as an effective tool for promoting sales of various products and services.","We focus on the problem of optimally allocating coupons to customers subject to a budget constraint on an e-commerce website.","We apply a robust portfolio optimization model based on customer segmentation to the coupon allocation problem.","We also validate the efficacy of our method through numerical experiments using actual data from randomly distributed coupons.","Main contributions of our research are twofold.","First, we handle six types of coupons, thereby making it extremely difficult to accurately estimate the difference in the effects of various coupons.","Second, we demonstrate from detailed numerical results that the robust optimization model achieved larger uplifts of sales than did the commonly-used multiple-choice knapsack model and the conventional mean-variance optimization model.","Our results open up great potential for robust portfolio optimization as an effective tool for practical coupon allocation."],"url":"http://arxiv.org/abs/2405.12865v1","category":"cs.IR"}
{"created":"2024-05-21 15:26:06","title":"Toward Constraint Compliant Goal Formulation and Planning","abstract":"One part of complying with norms, rules, and preferences is incorporating constraints (such as knowledge of ethics) into one's goal formulation and planning processing. We explore in a simple domain how the encoding of knowledge in different ethical frameworks influences an agent's goal formulation and planning processing and demonstrate ability of an agent to satisfy and satisfice when its collection of relevant constraints includes a mix of \"hard\" and \"soft\" constraints of various types. How the agent attempts to comply with ethical constraints depends on the ethical framing and we investigate tradeoffs between deontological framing and utilitarian framing for complying with an ethical norm. Representative scenarios highlight how performing the same task with different framings of the same norm leads to different behaviors. Our explorations suggest an important role for metacognitive judgments in resolving ethical conflicts during goal formulation and planning.","sentences":["One part of complying with norms, rules, and preferences is incorporating constraints (such as knowledge of ethics) into one's goal formulation and planning processing.","We explore in a simple domain how the encoding of knowledge in different ethical frameworks influences an agent's goal formulation and planning processing and demonstrate ability of an agent to satisfy and satisfice when its collection of relevant constraints includes a mix of \"hard\" and \"soft\" constraints of various types.","How the agent attempts to comply with ethical constraints depends on the ethical framing and we investigate tradeoffs between deontological framing and utilitarian framing for complying with an ethical norm.","Representative scenarios highlight how performing the same task with different framings of the same norm leads to different behaviors.","Our explorations suggest an important role for metacognitive judgments in resolving ethical conflicts during goal formulation and planning."],"url":"http://arxiv.org/abs/2405.12862v1","category":"cs.AI"}
{"created":"2024-05-21 15:24:37","title":"Influence of Water Droplet Contamination for Transparency Segmentation","abstract":"Computer vision techniques are on the rise for industrial applications, like process supervision and autonomous agents, e.g., in the healthcare domain and dangerous environments. While the general usability of these techniques is high, there are still challenging real-world use-cases. Especially transparent structures, which can appear in the form of glass doors, protective casings or everyday objects like glasses, pose a challenge for computer vision methods. This paper evaluates the combination of transparent objects in conjunction with (naturally occurring) contamination through environmental effects like hazing. We introduce a novel publicly available dataset containing 489 images incorporating three grades of water droplet contamination on transparent structures and examine the resulting influence on transparency handling. Our findings show, that contaminated transparent objects are easier to segment and that we are able to distinguish between different severity levels of contamination with a current state-of-the art machine-learning model. This in turn opens up the possibility to enhance computer vision systems regarding resilience against, e.g., datashifts through contaminated protection casings or implement an automated cleaning alert.","sentences":["Computer vision techniques are on the rise for industrial applications, like process supervision and autonomous agents, e.g., in the healthcare domain and dangerous environments.","While the general usability of these techniques is high, there are still challenging real-world use-cases.","Especially transparent structures, which can appear in the form of glass doors, protective casings or everyday objects like glasses, pose a challenge for computer vision methods.","This paper evaluates the combination of transparent objects in conjunction with (naturally occurring) contamination through environmental effects like hazing.","We introduce a novel publicly available dataset containing 489 images incorporating three grades of water droplet contamination on transparent structures and examine the resulting influence on transparency handling.","Our findings show, that contaminated transparent objects are easier to segment and that we are able to distinguish between different severity levels of contamination with a current state-of-the art machine-learning model.","This in turn opens up the possibility to enhance computer vision systems regarding resilience against, e.g., datashifts through contaminated protection casings or implement an automated cleaning alert."],"url":"http://arxiv.org/abs/2405.12861v1","category":"cs.CV"}
{"created":"2024-05-21 15:12:10","title":"Ubiquity of the spin-orbit induced magnon nonreciprocity in ultrathin ferromagnets","abstract":"The propagation of magnons along a symmetry path may depend on the direction of propagation, similar to many other quasiparticles in nature. This phenomenon is commonly referred to as nonreciprocity. In addition to the fact that it is of great interest to understand the fundamental physical mechanism leading to this nonreciprocal propagation, the phenomenon of magnon nonreciprocity may be used to design magnon-based logic devices. Recently, it has been demonstrated that a significantly large spin-orbit coupling can lead to giant nonreciprocity of exchange-dominated terahertz magnons, when they are excited by means of spin-polarized electrons [Phys. Rev. Lett. 132, 126702 (2024)]. Here, by providing experimental results on two additional systems we demonstrate the generality of the observed phenomenon. Comparing the results of a Co/Ni bilayer on Ir(001) to those of a Co double layer on Ir(001) and W(110) we unravel the impact of the interfacial electronic hybridization on the observed phenomenon and provide further insights into the microscopic mechanism leading to this nonreciprocal magnon excitation. It was observed that the interfacial electronic hybridization, which is governing the magnetic properties of the interface atomic layer, is somewhat important but is not crucial for the magnon nonreciprocity. On the other hand the choice of the incident energy of the incoming electron beam is decisive for the observation of the effect. Our results indicate that depending on the energy of the incident electron beam and the scattering geometry the magnon nonreciprocity can be tuned and even be inverted for some ranges of the magnon momentum.","sentences":["The propagation of magnons along a symmetry path may depend on the direction of propagation, similar to many other quasiparticles in nature.","This phenomenon is commonly referred to as nonreciprocity.","In addition to the fact that it is of great interest to understand the fundamental physical mechanism leading to this nonreciprocal propagation, the phenomenon of magnon nonreciprocity may be used to design magnon-based logic devices.","Recently, it has been demonstrated that a significantly large spin-orbit coupling can lead to giant nonreciprocity of exchange-dominated terahertz magnons, when they are excited by means of spin-polarized electrons [Phys. Rev. Lett.","132, 126702 (2024)].","Here, by providing experimental results on two additional systems we demonstrate the generality of the observed phenomenon.","Comparing the results of a Co/Ni bilayer on Ir(001) to those of a Co double layer on Ir(001) and W(110) we unravel the impact of the interfacial electronic hybridization on the observed phenomenon and provide further insights into the microscopic mechanism leading to this nonreciprocal magnon excitation.","It was observed that the interfacial electronic hybridization, which is governing the magnetic properties of the interface atomic layer, is somewhat important but is not crucial for the magnon nonreciprocity.","On the other hand the choice of the incident energy of the incoming electron beam is decisive for the observation of the effect.","Our results indicate that depending on the energy of the incident electron beam and the scattering geometry the magnon nonreciprocity can be tuned and even be inverted for some ranges of the magnon momentum."],"url":"http://arxiv.org/abs/2405.12854v1","category":"cond-mat.str-el"}
{"created":"2024-05-21 15:09:33","title":"Ultrafast Broadband Strong-Field Tunnelling in Asymmetric Nanogaps for Time-Resolved Nanoscopy","abstract":"Femtosecond-fast and nanometre-size pulses of electrons are emerging as unique probes for ultrafast dynamics at the nanoscale. Presently, such pulses are achievable only in highly sophisticated ultrafast electron microscopes or equally complex setups involving few-cycle-pulsed lasers with stable carrier-envelope phase (CEP) and nanotip probes. Here, we show that the generation of femtosecond pulses of nanoscale tunnelling electrons can be achieved in any ultrafast optical laboratory, using any (deep-UV to mid-IR) femtosecond laser in combination with photosensitive asymmetric nanogap (PAN) diodes fabricated via easy-to-scale adhesion lithography. The dominant mechanism producing tunnelling electrons in PANs is strong-field emission, which is easily achievable without CEP locking or external bias voltage. We employ PANs to demonstrate ultrafast nanoscopy of metal-halide perovskite quantum dots immobilised inside a 10-nm Al/Au nanogap and to characterise laser pulses across the entire optical region (266-6700 nm). Short electron pulses in PANs open the way towards scalable on-chip femtosecond electron measurements and novel design approaches for integrated ultrafast sensing nanodevices.","sentences":["Femtosecond-fast and nanometre-size pulses of electrons are emerging as unique probes for ultrafast dynamics at the nanoscale.","Presently, such pulses are achievable only in highly sophisticated ultrafast electron microscopes or equally complex setups involving few-cycle-pulsed lasers with stable carrier-envelope phase (CEP) and nanotip probes.","Here, we show that the generation of femtosecond pulses of nanoscale tunnelling electrons can be achieved in any ultrafast optical laboratory, using any (deep-UV to mid-IR) femtosecond laser in combination with photosensitive asymmetric nanogap (PAN) diodes fabricated via easy-to-scale adhesion lithography.","The dominant mechanism producing tunnelling electrons in PANs is strong-field emission, which is easily achievable without CEP locking or external bias voltage.","We employ PANs to demonstrate ultrafast nanoscopy of metal-halide perovskite quantum dots immobilised inside a 10-nm Al/Au nanogap and to characterise laser pulses across the entire optical region (266-6700 nm).","Short electron pulses in PANs open the way towards scalable on-chip femtosecond electron measurements and novel design approaches for integrated ultrafast sensing nanodevices."],"url":"http://arxiv.org/abs/2405.12851v1","category":"physics.optics"}
{"created":"2024-05-21 14:59:39","title":"Training and inference in the ReckON RSNN architecture implemented on a MPSoC","abstract":"With the rise of artificial intelligence, biological neuron models are being used to implement neural networks that can learn certain tasks after a training phase. One type of such networks are spiking neural networks (SNNs) that rely on a simplified model for biological neurons, the Integrate and Fire neuron. Several accelerators have emerged to implement SNNs with this kind of neuron. The ReckON system is one of these that allows both the training and execution of a recurrent SNN. The ReckON architecture, implemented on a custom ASIC, can be fully described using a hardware description language. In this work, we adapt the Verilog description to implement it on a Xilinx Multiprocessor System on Chip system (MPSoC). We present the circuits required for the efficient operation of the system, and a Python framework to use it on the Pynq ZU platform. We validate the architecture and implementation in two different scenarios, and show how the simulated accuracy is preserved with a peak performance of 3.8M events processed per second.","sentences":["With the rise of artificial intelligence, biological neuron models are being used to implement neural networks that can learn certain tasks after a training phase.","One type of such networks are spiking neural networks (SNNs) that rely on a simplified model for biological neurons, the Integrate and Fire neuron.","Several accelerators have emerged to implement SNNs with this kind of neuron.","The ReckON system is one of these that allows both the training and execution of a recurrent SNN.","The ReckON architecture, implemented on a custom ASIC, can be fully described using a hardware description language.","In this work, we adapt the Verilog description to implement it on a Xilinx Multiprocessor System on Chip system (MPSoC).","We present the circuits required for the efficient operation of the system, and a Python framework to use it on the Pynq ZU platform.","We validate the architecture and implementation in two different scenarios, and show how the simulated accuracy is preserved with a peak performance of 3.8M events processed per second."],"url":"http://arxiv.org/abs/2405.12849v1","category":"cs.AR"}
{"created":"2024-05-21 14:50:20","title":"OpenCarbonEval: A Unified Carbon Emission Estimation Framework in Large-Scale AI Models","abstract":"In recent years, large-scale auto-regressive models have made significant progress in various tasks, such as text or video generation. However, the environmental impact of these models has been largely overlooked, with a lack of assessment and analysis of their carbon footprint. To address this gap, we introduce OpenCarbonEval, a unified framework for integrating large-scale models across diverse modalities to predict carbon emissions, which could provide AI service providers and users with a means to estimate emissions beforehand and help mitigate the environmental pressure associated with these models. In OpenCarbonEval, we propose a dynamic throughput modeling approach that could capture workload and hardware fluctuations in the training process for more precise emissions estimates. Our evaluation results demonstrate that OpenCarbonEval can more accurately predict training emissions than previous methods, and can be seamlessly applied to different modal tasks. Specifically, we show that OpenCarbonEval achieves superior performance in predicting carbon emissions for both visual models and language models. By promoting sustainable AI development and deployment, OpenCarbonEval can help reduce the environmental impact of large-scale models and contribute to a more environmentally responsible future for the AI community.","sentences":["In recent years, large-scale auto-regressive models have made significant progress in various tasks, such as text or video generation.","However, the environmental impact of these models has been largely overlooked, with a lack of assessment and analysis of their carbon footprint.","To address this gap, we introduce OpenCarbonEval, a unified framework for integrating large-scale models across diverse modalities to predict carbon emissions, which could provide AI service providers and users with a means to estimate emissions beforehand and help mitigate the environmental pressure associated with these models.","In OpenCarbonEval, we propose a dynamic throughput modeling approach that could capture workload and hardware fluctuations in the training process for more precise emissions estimates.","Our evaluation results demonstrate that OpenCarbonEval can more accurately predict training emissions than previous methods, and can be seamlessly applied to different modal tasks.","Specifically, we show that OpenCarbonEval achieves superior performance in predicting carbon emissions for both visual models and language models.","By promoting sustainable AI development and deployment, OpenCarbonEval can help reduce the environmental impact of large-scale models and contribute to a more environmentally responsible future for the AI community."],"url":"http://arxiv.org/abs/2405.12843v1","category":"cs.CY"}
{"created":"2024-05-21 14:49:12","title":"SmartFlow: Robotic Process Automation using LLMs","abstract":"Robotic Process Automation (RPA) systems face challenges in handling complex processes and diverse screen layouts that require advanced human-like decision-making capabilities. These systems typically rely on pixel-level encoding through drag-and-drop or automation frameworks such as Selenium to create navigation workflows, rather than visual understanding of screen elements. In this context, we present SmartFlow, an AI-based RPA system that uses pre-trained large language models (LLMs) coupled with deep-learning based image understanding. Our system can adapt to new scenarios, including changes in the user interface and variations in input data, without the need for human intervention. SmartFlow uses computer vision and natural language processing to perceive visible elements on the graphical user interface (GUI) and convert them into a textual representation. This information is then utilized by LLMs to generate a sequence of actions that are executed by a scripting engine to complete an assigned task. To assess the effectiveness of SmartFlow, we have developed a dataset that includes a set of generic enterprise applications with diverse layouts, which we are releasing for research use. Our evaluations on this dataset demonstrate that SmartFlow exhibits robustness across different layouts and applications. SmartFlow can automate a wide range of business processes such as form filling, customer service, invoice processing, and back-office operations. SmartFlow can thus assist organizations in enhancing productivity by automating an even larger fraction of screen-based workflows. The demo-video and dataset are available at https://smartflow-4c5a0a.webflow.io/.","sentences":["Robotic Process Automation (RPA) systems face challenges in handling complex processes and diverse screen layouts that require advanced human-like decision-making capabilities.","These systems typically rely on pixel-level encoding through drag-and-drop or automation frameworks such as Selenium to create navigation workflows, rather than visual understanding of screen elements.","In this context, we present SmartFlow, an AI-based RPA system that uses pre-trained large language models (LLMs) coupled with deep-learning based image understanding.","Our system can adapt to new scenarios, including changes in the user interface and variations in input data, without the need for human intervention.","SmartFlow uses computer vision and natural language processing to perceive visible elements on the graphical user interface (GUI) and convert them into a textual representation.","This information is then utilized by LLMs to generate a sequence of actions that are executed by a scripting engine to complete an assigned task.","To assess the effectiveness of SmartFlow, we have developed a dataset that includes a set of generic enterprise applications with diverse layouts, which we are releasing for research use.","Our evaluations on this dataset demonstrate that SmartFlow exhibits robustness across different layouts and applications.","SmartFlow can automate a wide range of business processes such as form filling, customer service, invoice processing, and back-office operations.","SmartFlow can thus assist organizations in enhancing productivity by automating an even larger fraction of screen-based workflows.","The demo-video and dataset are available at https://smartflow-4c5a0a.webflow.io/."],"url":"http://arxiv.org/abs/2405.12842v1","category":"cs.RO"}
{"created":"2024-05-21 14:46:55","title":"Unveiling the Power of Intermediate Representations for Static Analysis: A Survey","abstract":"Static analysis techniques enhance the security, performance, and reliability of programs by analyzing and portraiting program behaviors without the need for actual execution. In essence, static analysis takes the Intermediate Representation (IR) of a target program as input to retrieve essential program information and understand the program. However, there is a lack of systematic analysis on the benefit of IR for static analysis, besides serving as an information provider. In general, a modern static analysis framework should possess the ability to conduct diverse analyses on different languages, producing reliable results with minimal time consumption, and offering extensive customization options. In this survey, we systematically characterize these goals and review the potential solutions from the perspective of IR. It can serve as a manual for learners and practitioners in the static analysis field to better understand IR design. Meanwhile, numerous research opportunities are revealed for researchers.","sentences":["Static analysis techniques enhance the security, performance, and reliability of programs by analyzing and portraiting program behaviors without the need for actual execution.","In essence, static analysis takes the Intermediate Representation (IR) of a target program as input to retrieve essential program information and understand the program.","However, there is a lack of systematic analysis on the benefit of IR for static analysis, besides serving as an information provider.","In general, a modern static analysis framework should possess the ability to conduct diverse analyses on different languages, producing reliable results with minimal time consumption, and offering extensive customization options.","In this survey, we systematically characterize these goals and review the potential solutions from the perspective of IR.","It can serve as a manual for learners and practitioners in the static analysis field to better understand IR design.","Meanwhile, numerous research opportunities are revealed for researchers."],"url":"http://arxiv.org/abs/2405.12841v1","category":"cs.PL"}
{"created":"2024-05-21 14:42:39","title":"Quantum Non-Identical Mean Estimation: Efficient Algorithms and Fundamental Limits","abstract":"We systematically investigate quantum algorithms and lower bounds for mean estimation given query access to non-identically distributed samples. On the one hand, we give quantum mean estimators with quadratic quantum speed-up given samples from different bounded or sub-Gaussian random variables. On the other hand, we prove that, in general, it is impossible for any quantum algorithm to achieve quadratic speed-up over the number of classical samples needed to estimate the mean $\\mu$, where the samples come from different random variables with mean close to $\\mu$. Technically, our quantum algorithms reduce bounded and sub-Gaussian random variables to the Bernoulli case, and use an uncomputation trick to overcome the challenge that direct amplitude estimation does not work with non-identical query access. Our quantum query lower bounds are established by simulating non-identical oracles by parallel oracles, and also by an adversarial method with non-identical oracles. Both results pave the way for proving quantum query lower bounds with non-identical oracles in general, which may be of independent interest.","sentences":["We systematically investigate quantum algorithms and lower bounds for mean estimation given query access to non-identically distributed samples.","On the one hand, we give quantum mean estimators with quadratic quantum speed-up given samples from different bounded or sub-Gaussian random variables.","On the other hand, we prove that, in general, it is impossible for any quantum algorithm to achieve quadratic speed-up over the number of classical samples needed to estimate the mean $\\mu$, where the samples come from different random variables with mean close to $\\mu$. Technically, our quantum algorithms reduce bounded and sub-Gaussian random variables to the Bernoulli case, and use an uncomputation trick to overcome the challenge that direct amplitude estimation does not work with non-identical query access.","Our quantum query lower bounds are established by simulating non-identical oracles by parallel oracles, and also by an adversarial method with non-identical oracles.","Both results pave the way for proving quantum query lower bounds with non-identical oracles in general, which may be of independent interest."],"url":"http://arxiv.org/abs/2405.12838v1","category":"quant-ph"}
{"created":"2024-05-21 14:39:39","title":"Aircraft Conflict Resolution: A Benchmark Generator","abstract":"Aircraft conflict resolution is one of the major tasks of computer-aided air traffic management and represents a challenging optimization problem. Many models and methods have been proposed to assist trajectory regulation to avoid conflicts. However, the question of testing the different mathematical optimization approaches against each other is still open. Standard benchmarks include unrealistic scenarios in which all the flights move toward a common point or completely random generated instances. There is a lack of a common set of test instances that allows comparison of the available methods under a variety of heterogeneous and representative scenarios. We present a flight deconfliction benchmark generator that allows the user to choose between (i) different predefined scenario inspired by existing benchmarks in the literature; (ii) pseudo-random traffic meeting certain congestion measurements; (iii) and randomly generated traffic. The proposed setting can account for different levels of difficulty in the deconfliction of the aircraft and allows to explore and compare the real limitations of optimization approaches for aircraft conflict resolution.","sentences":["Aircraft conflict resolution is one of the major tasks of computer-aided air traffic management and represents a challenging optimization problem.","Many models and methods have been proposed to assist trajectory regulation to avoid conflicts.","However, the question of testing the different mathematical optimization approaches against each other is still open.","Standard benchmarks include unrealistic scenarios in which all the flights move toward a common point or completely random generated instances.","There is a lack of a common set of test instances that allows comparison of the available methods under a variety of heterogeneous and representative scenarios.","We present a flight deconfliction benchmark generator that allows the user to choose between (i) different predefined scenario inspired by existing benchmarks in the literature; (ii) pseudo-random traffic meeting certain congestion measurements; (iii) and randomly generated traffic.","The proposed setting can account for different levels of difficulty in the deconfliction of the aircraft and allows to explore and compare the real limitations of optimization approaches for aircraft conflict resolution."],"url":"http://arxiv.org/abs/2405.12836v1","category":"math.OC"}
{"created":"2024-05-21 14:37:35","title":"A Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data","abstract":"Automatic radiology report generation can alleviate the workload for physicians and minimize regional disparities in medical resources, therefore becoming an important topic in the medical image analysis field. It is a challenging task, as the computational model needs to mimic physicians to obtain information from multi-modal input data (i.e., medical images, clinical information, medical knowledge, etc.), and produce comprehensive and accurate reports. Recently, numerous works emerged to address this issue using deep learning-based methods, such as transformers, contrastive learning, and knowledge-base construction. This survey summarizes the key techniques developed in the most recent works and proposes a general workflow for deep learning-based report generation with five main components, including multi-modality data acquisition, data preparation, feature learning, feature fusion/interaction, and report generation. The state-of-the-art methods for each of these components are highlighted. Additionally, training strategies, public datasets, evaluation methods, current challenges, and future directions in this field are summarized. We have also conducted a quantitative comparison between different methods under the same experimental setting. This is the most up-to-date survey that focuses on multi-modality inputs and data fusion for radiology report generation. The aim is to provide comprehensive and rich information for researchers interested in automatic clinical report generation and medical image analysis, especially when using multimodal inputs, and assist them in developing new algorithms to advance the field.","sentences":["Automatic radiology report generation can alleviate the workload for physicians and minimize regional disparities in medical resources, therefore becoming an important topic in the medical image analysis field.","It is a challenging task, as the computational model needs to mimic physicians to obtain information from multi-modal input data (i.e., medical images, clinical information, medical knowledge, etc.), and produce comprehensive and accurate reports.","Recently, numerous works emerged to address this issue using deep learning-based methods, such as transformers, contrastive learning, and knowledge-base construction.","This survey summarizes the key techniques developed in the most recent works and proposes a general workflow for deep learning-based report generation with five main components, including multi-modality data acquisition, data preparation, feature learning, feature fusion/interaction, and report generation.","The state-of-the-art methods for each of these components are highlighted.","Additionally, training strategies, public datasets, evaluation methods, current challenges, and future directions in this field are summarized.","We have also conducted a quantitative comparison between different methods under the same experimental setting.","This is the most up-to-date survey that focuses on multi-modality inputs and data fusion for radiology report generation.","The aim is to provide comprehensive and rich information for researchers interested in automatic clinical report generation and medical image analysis, especially when using multimodal inputs, and assist them in developing new algorithms to advance the field."],"url":"http://arxiv.org/abs/2405.12833v1","category":"cs.CV"}
{"created":"2024-05-21 14:36:16","title":"Wav-KAN: Wavelet Kolmogorov-Arnold Networks","abstract":"In this paper , we introduce Wav-KAN, an innovative neural network architecture that leverages the Wavelet Kolmogorov-Arnold Networks (Wav-KAN) framework to enhance interpretability and performance. Traditional multilayer perceptrons (MLPs) and even recent advancements like Spl-KAN face challenges related to interpretability, training speed, robustness, computational efficiency, and performance. Wav-KAN addresses these limitations by incorporating wavelet functions into the Kolmogorov-Arnold network structure, enabling the network to capture both high-frequency and low-frequency components of the input data efficiently. Wavelet-based approximations employ orthogonal or semi-orthogonal basis and also maintains a balance between accurately representing the underlying data structure and avoiding overfitting to the noise. Analogous to how water conforms to the shape of its container, Wav-KAN adapts to the data structure, resulting in enhanced accuracy, faster training speeds, and increased robustness compared to Spl-KAN and MLPs. Our results highlight the potential of Wav-KAN as a powerful tool for developing interpretable and high-performance neural networks, with applications spanning various fields. This work sets the stage for further exploration and implementation of Wav-KAN in frameworks such as PyTorch, TensorFlow, and also it makes wavelet in KAN in wide-spread usage like nowadays activation functions like ReLU, sigmoid in universal approximation theory (UAT).","sentences":["In this paper , we introduce Wav-KAN, an innovative neural network architecture that leverages the Wavelet Kolmogorov-Arnold Networks (Wav-KAN) framework to enhance interpretability and performance.","Traditional multilayer perceptrons (MLPs) and even recent advancements like Spl-KAN face challenges related to interpretability, training speed, robustness, computational efficiency, and performance.","Wav-KAN addresses these limitations by incorporating wavelet functions into the Kolmogorov-Arnold network structure, enabling the network to capture both high-frequency and low-frequency components of the input data efficiently.","Wavelet-based approximations employ orthogonal or semi-orthogonal basis and also maintains a balance between accurately representing the underlying data structure and avoiding overfitting to the noise.","Analogous to how water conforms to the shape of its container, Wav-KAN adapts to the data structure, resulting in enhanced accuracy, faster training speeds, and increased robustness compared to Spl-KAN and MLPs.","Our results highlight the potential of Wav-KAN as a powerful tool for developing interpretable and high-performance neural networks, with applications spanning various fields.","This work sets the stage for further exploration and implementation of Wav-KAN in frameworks such as PyTorch, TensorFlow, and also it makes wavelet in KAN in wide-spread usage like nowadays activation functions like ReLU, sigmoid in universal approximation theory (UAT)."],"url":"http://arxiv.org/abs/2405.12832v1","category":"cs.LG"}
{"created":"2024-05-21 14:31:10","title":"Hodge-de Rham and Lichn\u00e9rowicz Laplacians on double forms and some vanishing theorems","abstract":"A $(p,q)$-double form on a Riemannian manifold $(M,g)$ can be considered simultaneously as a vector-valued differential $p$-form over $M$ or alternatively as a vector-valued $q$-form. Accordingly, the usual Hodge-de Rham Laplacian on differential forms can be extended to double forms in two ways. The differential operators obtained in this way are denoted by $\\Delta$ and $\\widetilde{\\Delta}$.\\\\ In this paper, we show that the Lichn\\'erowicz Laplacian $\\Delta_L$ once operating on double forms, is nothing but the average of the two operators mentioned above. We introduce a new product on double forms to establish index-free formulas for the curvature terms in the Weitzenb\\\"ock formulas corresponding to the Laplacians $\\Delta, \\widetilde{\\Delta}$ and $\\Delta_L$. We prove vanishing theorems for the Hodge-de Rham Laplacian $\\Delta$ on $(p,0)$ double forms and for $\\Delta_L$ and $\\Delta$ on symmetric double forms of arbitrary order. These results generalize recent results by Petersen-Wink. Our vanishing theorems reveal the impact of the role played by the rank of the eigenvectors of the curvature operator on the structure (e.g. the topology) of the manifold.","sentences":["A $(p,q)$-double form on a Riemannian manifold $(M,g)$ can be considered simultaneously as a vector-valued differential $p$-form over $M$ or alternatively as a vector-valued $q$-form.","Accordingly, the usual Hodge-de Rham Laplacian on differential forms can be extended to double forms in two ways.","The differential operators obtained in this way are denoted by $\\Delta$ and $\\widetilde{\\Delta}$.\\\\ In this paper, we show that the Lichn\\'erowicz Laplacian $\\Delta_L$ once operating on double forms, is nothing but the average of the two operators mentioned above.","We introduce a new product on double forms to establish index-free formulas for the curvature terms in the Weitzenb\\\"ock formulas corresponding to the Laplacians $\\Delta, \\widetilde{\\Delta}$ and $\\Delta_L$. We prove vanishing theorems for the Hodge-de Rham Laplacian $\\Delta$ on $(p,0)$ double forms and for $\\Delta_L$ and $\\Delta$ on symmetric double forms of arbitrary order.","These results generalize recent results by Petersen-Wink.","Our vanishing theorems reveal the impact of the role played by the rank of the eigenvectors of the curvature operator on the structure (e.g. the topology) of the manifold."],"url":"http://arxiv.org/abs/2405.12828v1","category":"math.DG"}
{"created":"2024-05-21 14:28:43","title":"Classification of translation surfaces in $\\mathbb{R}^3$ with constant sectional curvature","abstract":"In this paper, we study translation surfaces in the Euclidean space endowed with a canonical semi-symmetric non-metric connection. We completely classify the translation surfaces of constant sectional curvature with respect to this connection, proving that they are generalized cylinders. This consequence is the same as in the case of the Levi-Civita connection, but in this new setting, there are also generalized cylinders whose sectional curvature can be constant or non-constant, in contrast to the Levi-Civita connection, where the Gaussian curvature is zero.","sentences":["In this paper, we study translation surfaces in the Euclidean space endowed with a canonical semi-symmetric non-metric connection.","We completely classify the translation surfaces of constant sectional curvature with respect to this connection, proving that they are generalized cylinders.","This consequence is the same as in the case of the Levi-Civita connection, but in this new setting, there are also generalized cylinders whose sectional curvature can be constant or non-constant, in contrast to the Levi-Civita connection, where the Gaussian curvature is zero."],"url":"http://arxiv.org/abs/2405.12825v1","category":"math.DG"}
{"created":"2024-05-21 14:28:25","title":"A note on the finitely generated fixed subgroup property","abstract":"We study when a group of form $G\\times\\mathbb{Z}$ has the finitely generated fixed subgroup property of automorphisms ($FGFP_a$), and provide some partial answers and non-trivial examples.","sentences":["We study when a group of form $G\\times\\mathbb{Z}$ has the finitely generated fixed subgroup property of automorphisms ($FGFP_a$), and provide some partial answers and non-trivial examples."],"url":"http://arxiv.org/abs/2405.12824v1","category":"math.GR"}
{"created":"2024-05-21 14:26:36","title":"Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D Referring Expression Comprehension","abstract":"Embodied perception is essential for intelligent vehicles and robots, enabling more natural interaction and task execution. However, these advancements currently embrace vision level, rarely focusing on using 3D modeling sensors, which limits the full understanding of surrounding objects with multi-granular characteristics. Recently, as a promising automotive sensor with affordable cost, 4D Millimeter-Wave radar provides denser point clouds than conventional radar and perceives both semantic and physical characteristics of objects, thus enhancing the reliability of perception system. To foster the development of natural language-driven context understanding in radar scenes for 3D grounding, we construct the first dataset, Talk2Radar, which bridges these two modalities for 3D Referring Expression Comprehension. Talk2Radar contains 8,682 referring prompt samples with 20,558 referred objects. Moreover, we propose a novel model, T-RadarNet for 3D REC upon point clouds, achieving state-of-the-art performances on Talk2Radar dataset compared with counterparts, where Deformable-FPN and Gated Graph Fusion are meticulously designed for efficient point cloud feature modeling and cross-modal fusion between radar and text features, respectively. Further, comprehensive experiments are conducted to give a deep insight into radar-based 3D REC. We release our project at https://github.com/GuanRunwei/Talk2Radar.","sentences":["Embodied perception is essential for intelligent vehicles and robots, enabling more natural interaction and task execution.","However, these advancements currently embrace vision level, rarely focusing on using 3D modeling sensors, which limits the full understanding of surrounding objects with multi-granular characteristics.","Recently, as a promising automotive sensor with affordable cost, 4D Millimeter-Wave radar provides denser point clouds than conventional radar and perceives both semantic and physical characteristics of objects, thus enhancing the reliability of perception system.","To foster the development of natural language-driven context understanding in radar scenes for 3D grounding, we construct the first dataset, Talk2Radar, which bridges these two modalities for 3D Referring Expression Comprehension.","Talk2Radar contains 8,682 referring prompt samples with 20,558 referred objects.","Moreover, we propose a novel model, T-RadarNet for 3D REC upon point clouds, achieving state-of-the-art performances on Talk2Radar dataset compared with counterparts, where Deformable-FPN and Gated Graph Fusion are meticulously designed for efficient point cloud feature modeling and cross-modal fusion between radar and text features, respectively.","Further, comprehensive experiments are conducted to give a deep insight into radar-based 3D REC.","We release our project at https://github.com/GuanRunwei/Talk2Radar."],"url":"http://arxiv.org/abs/2405.12821v1","category":"cs.RO"}
{"created":"2024-05-21 14:24:01","title":"Large Language Models Meet NLP: A Survey","abstract":"While large language models (LLMs) like ChatGPT have shown impressive capabilities in Natural Language Processing (NLP) tasks, a systematic investigation of their potential in this field remains largely unexplored. This study aims to address this gap by exploring the following questions: (1) How are LLMs currently applied to NLP tasks in the literature? (2) Have traditional NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for NLP? To answer these questions, we take the first step to provide a comprehensive overview of LLMs in NLP. Specifically, we first introduce a unified taxonomy including (1) parameter-frozen application and (2) parameter-tuning application to offer a unified perspective for understanding the current progress of LLMs in NLP. Furthermore, we summarize the new frontiers and the associated challenges, aiming to inspire further groundbreaking advancements. We hope this work offers valuable insights into the {potential and limitations} of LLMs in NLP, while also serving as a practical guide for building effective LLMs in NLP.","sentences":["While large language models (LLMs) like ChatGPT have shown impressive capabilities in Natural Language Processing (NLP) tasks, a systematic investigation of their potential in this field remains largely unexplored.","This study aims to address this gap by exploring the following questions: (1) How are LLMs currently applied to NLP tasks in the literature?","(2) Have traditional NLP tasks already been solved with LLMs?","(3) What is the future of the LLMs for NLP?","To answer these questions, we take the first step to provide a comprehensive overview of LLMs in NLP.","Specifically, we first introduce a unified taxonomy including (1) parameter-frozen application and (2) parameter-tuning application to offer a unified perspective for understanding the current progress of LLMs in NLP.","Furthermore, we summarize the new frontiers and the associated challenges, aiming to inspire further groundbreaking advancements.","We hope this work offers valuable insights into the {potential and limitations} of LLMs in NLP, while also serving as a practical guide for building effective LLMs in NLP."],"url":"http://arxiv.org/abs/2405.12819v1","category":"cs.CL"}
{"created":"2024-05-21 14:16:33","title":"A Non-Parametric Box-Cox Approach to Robustifying High-Dimensional Linear Hypothesis Testing","abstract":"The mainstream theory of hypothesis testing in high-dimensional regression typically assumes the underlying true model is a low-dimensional linear regression model, yet the Box-Cox transformation is a regression technique commonly used to mitigate anomalies like non-additivity and heteroscedasticity. This paper introduces a more flexible framework, the non-parametric Box-Cox model with unspecified transformation, to address model mis-specification in high-dimensional linear hypothesis testing while preserving the interpretation of regression coefficients. Model estimation and computation in high dimensions poses challenges beyond traditional sparse penalization methods. We propose the constrained partial penalized composite probit regression method for sparse estimation and investigate its statistical properties. Additionally, we present a computationally efficient algorithm using augmented Lagrangian and coordinate majorization descent for solving regularization problems with folded concave penalization and linear constraints. For testing linear hypotheses, we propose the partial penalized composite likelihood ratio test, score test and Wald test, and show that their limiting distributions under null and local alternatives follow generalized chi-squared distributions with the same degrees of freedom and noncentral parameter. Extensive simulation studies are conducted to examine the finite sample performance of the proposed tests. Our analysis of supermarket data illustrates potential discrepancies between our testing procedures and standard high-dimensional methods, highlighting the importance of our robustified approach.","sentences":["The mainstream theory of hypothesis testing in high-dimensional regression typically assumes the underlying true model is a low-dimensional linear regression model, yet the Box-Cox transformation is a regression technique commonly used to mitigate anomalies like non-additivity and heteroscedasticity.","This paper introduces a more flexible framework, the non-parametric Box-Cox model with unspecified transformation, to address model mis-specification in high-dimensional linear hypothesis testing while preserving the interpretation of regression coefficients.","Model estimation and computation in high dimensions poses challenges beyond traditional sparse penalization methods.","We propose the constrained partial penalized composite probit regression method for sparse estimation and investigate its statistical properties.","Additionally, we present a computationally efficient algorithm using augmented Lagrangian and coordinate majorization descent for solving regularization problems with folded concave penalization and linear constraints.","For testing linear hypotheses, we propose the partial penalized composite likelihood ratio test, score test and Wald test, and show that their limiting distributions under null and local alternatives follow generalized chi-squared distributions with the same degrees of freedom and noncentral parameter.","Extensive simulation studies are conducted to examine the finite sample performance of the proposed tests.","Our analysis of supermarket data illustrates potential discrepancies between our testing procedures and standard high-dimensional methods, highlighting the importance of our robustified approach."],"url":"http://arxiv.org/abs/2405.12816v1","category":"stat.ME"}
{"created":"2024-05-21 14:00:54","title":"Engineering band structures of two-dimensional materials with remote moire ferroelectricity","abstract":"The stacking order and twist angle provide abundant opportunities for engineering band structures of two-dimensional materials, including the formation of moire bands, flat bands, and topologically nontrivial bands. The inversion symmetry breaking in rhombohedral-stacked transitional metal dichalcogenides (TMDCs) endows them with an interfacial ferroelectricity associated with an out-of-plane electric polarization. By utilizing twist angle as a knob to construct rhombohedral-stacked TMDCs, antiferroelectric domain networks with alternating out-of-plane polarization can be generated. Here, we demonstrate that such spatially periodic ferroelectric polarizations in parallel-stacked twisted WSe2 can imprint their moire potential onto a remote bilayer graphene. This remote moire potential gives rise to pronounced satellite resistance peaks besides the charge-neutrality point in graphene, which are tunable by the twist angle of WSe2. Our observations of ferroelectric hysteresis at finite displacement fields suggest the moire is delivered by a long-range electrostatic potential. The constructed superlattices by moire ferroelectricity represent a highly flexible approach, as they involve the separation of the moire construction layer from the electronic transport layer. This remote moire is identified as a weak potential and can coexist with conventional moire. Our results offer a comprehensive strategy for engineering band structures and properties of two-dimensional materials by utilizing moire ferroelectricity.","sentences":["The stacking order and twist angle provide abundant opportunities for engineering band structures of two-dimensional materials, including the formation of moire bands, flat bands, and topologically nontrivial bands.","The inversion symmetry breaking in rhombohedral-stacked transitional metal dichalcogenides (TMDCs) endows them with an interfacial ferroelectricity associated with an out-of-plane electric polarization.","By utilizing twist angle as a knob to construct rhombohedral-stacked TMDCs, antiferroelectric domain networks with alternating out-of-plane polarization can be generated.","Here, we demonstrate that such spatially periodic ferroelectric polarizations in parallel-stacked twisted WSe2 can imprint their moire potential onto a remote bilayer graphene.","This remote moire potential gives rise to pronounced satellite resistance peaks besides the charge-neutrality point in graphene, which are tunable by the twist angle of WSe2.","Our observations of ferroelectric hysteresis at finite displacement fields suggest the moire is delivered by a long-range electrostatic potential.","The constructed superlattices by moire ferroelectricity represent a highly flexible approach, as they involve the separation of the moire construction layer from the electronic transport layer.","This remote moire is identified as a weak potential and can coexist with conventional moire.","Our results offer a comprehensive strategy for engineering band structures and properties of two-dimensional materials by utilizing moire ferroelectricity."],"url":"http://arxiv.org/abs/2405.12811v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 13:58:18","title":"Influence of quantum correction on the Schwarzschild black hole polarized image","abstract":"Using a model of an accretion disk around a Schwarzschild black hole, the analytic estimates for image polarization were derived by Narayan $et~al.$. [Astrophys. J, 102, 912 (2021)]. Recently, the EHT team also obtained polarization images of the Sgr A$^{*}$ and measured both linear and circular polarization [Astrophys. J. Lett, 964, L25 (2024)]. We find that quantum correction effects can also influence polarization information. Considering the quantum corrected Schwarzschild black hole (Kazakov-Solodukhin black hole), we derive the polarization intensity of the target black hole and investigate polarization images under different parameters. It is found that a larger quantum deformation leads to an expansion of the polarization region, while the polarization intensity value decrease. Under different observation angles, magnetic fields, fluid direction angles, and fluid velocity conditions, we also derive polarization images of corrected black holes. These key indicators not only affect the intensity of polarization but also the direction of polarization. We establish the relationship between polarization intensity and quantum correction deformation parameters, revealing a gradual decline in polarization intensity with reduced radius and an anti-polarization behavior induced by the progressive increase in deformation parameters at a constant radius. Our analysis may provide observational evidence for quantum effect of general relativity.","sentences":["Using a model of an accretion disk around a Schwarzschild black hole, the analytic estimates for image polarization were derived by Narayan $et~al.$.","[Astrophys.","J, 102, 912 (2021)].","Recently, the EHT team also obtained polarization images of the Sgr A$^{*}$ and measured both linear and circular polarization","[Astrophys.","J. Lett, 964, L25 (2024)].","We find that quantum correction effects can also influence polarization information.","Considering the quantum corrected Schwarzschild black hole (Kazakov-Solodukhin black hole), we derive the polarization intensity of the target black hole and investigate polarization images under different parameters.","It is found that a larger quantum deformation leads to an expansion of the polarization region, while the polarization intensity value decrease.","Under different observation angles, magnetic fields, fluid direction angles, and fluid velocity conditions, we also derive polarization images of corrected black holes.","These key indicators not only affect the intensity of polarization but also the direction of polarization.","We establish the relationship between polarization intensity and quantum correction deformation parameters, revealing a gradual decline in polarization intensity with reduced radius and an anti-polarization behavior induced by the progressive increase in deformation parameters at a constant radius.","Our analysis may provide observational evidence for quantum effect of general relativity."],"url":"http://arxiv.org/abs/2405.12808v1","category":"gr-qc"}
{"created":"2024-05-21 13:58:17","title":"FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information","abstract":"This paper establishes a mathematical foundation for the Adam optimizer, elucidating its connection to natural gradient descent through Riemannian and information geometry. We rigorously analyze the diagonal empirical Fisher information matrix (FIM) in Adam, clarifying all detailed approximations and advocating for the use of log probability functions as loss, which should be based on discrete distributions, due to the limitations of empirical FIM. Our analysis uncovers flaws in the original Adam algorithm, leading to proposed corrections such as enhanced momentum calculations, adjusted bias corrections, and gradient clipping. We refine the weight decay term based on our theoretical framework. Our modified algorithm, Fisher Adam (FAdam), demonstrates superior performance across diverse domains including LLM, ASR, and VQ-VAE, achieving state-of-the-art results in ASR.","sentences":["This paper establishes a mathematical foundation for the Adam optimizer, elucidating its connection to natural gradient descent through Riemannian and information geometry.","We rigorously analyze the diagonal empirical Fisher information matrix (FIM) in Adam, clarifying all detailed approximations and advocating for the use of log probability functions as loss, which should be based on discrete distributions, due to the limitations of empirical FIM.","Our analysis uncovers flaws in the original Adam algorithm, leading to proposed corrections such as enhanced momentum calculations, adjusted bias corrections, and gradient clipping.","We refine the weight decay term based on our theoretical framework.","Our modified algorithm, Fisher Adam (FAdam), demonstrates superior performance across diverse domains including LLM, ASR, and VQ-VAE, achieving state-of-the-art results in ASR."],"url":"http://arxiv.org/abs/2405.12807v1","category":"cs.LG"}
{"created":"2024-05-21 13:56:22","title":"Deep LPPLS: Forecasting of temporal critical points in natural, engineering and financial systems","abstract":"The Log-Periodic Power Law Singularity (LPPLS) model offers a general framework for capturing dynamics and predicting transition points in diverse natural and social systems. In this work, we present two calibration techniques for the LPPLS model using deep learning. First, we introduce the Mono-LPPLS-NN (M-LNN) model; for any given empirical time series, a unique M-LNN model is trained and shown to outperform state-of-the-art techniques in estimating the nonlinear parameters $(t_c, m, \\omega)$ of the LPPLS model as evidenced by the comprehensive distribution of parameter errors. Second, we extend the M-LNN model to a more general model architecture, the Poly-LPPLS-NN (P-LNN), which is able to quickly estimate the nonlinear parameters of the LPPLS model for any given time-series of a fixed length, including previously unseen time-series during training. The Poly class of models train on many synthetic LPPLS time-series augmented with various noise structures in a supervised manner. Given enough training examples, the P-LNN models also outperform state-of-the-art techniques for estimating the parameters of the LPPLS model as evidenced by the comprehensive distribution of parameter errors. Additionally, this class of models is shown to substantially reduce the time to obtain parameter estimates. Finally, we present applications to the diagnostic and prediction of two financial bubble peaks (followed by their crash) and of a famous rockslide. These contributions provide a bridge between deep learning and the study of the prediction of transition times in complex time series.","sentences":["The Log-Periodic Power Law Singularity (LPPLS) model offers a general framework for capturing dynamics and predicting transition points in diverse natural and social systems.","In this work, we present two calibration techniques for the LPPLS model using deep learning.","First, we introduce the Mono-LPPLS-NN (M-LNN) model; for any given empirical time series, a unique M-LNN model is trained and shown to outperform state-of-the-art techniques in estimating the nonlinear parameters $(t_c, m, \\omega)$ of the LPPLS model as evidenced by the comprehensive distribution of parameter errors.","Second, we extend the M-LNN model to a more general model architecture, the Poly-LPPLS-NN (P-LNN), which is able to quickly estimate the nonlinear parameters of the LPPLS model for any given time-series of a fixed length, including previously unseen time-series during training.","The Poly class of models train on many synthetic LPPLS time-series augmented with various noise structures in a supervised manner.","Given enough training examples, the P-LNN models also outperform state-of-the-art techniques for estimating the parameters of the LPPLS model as evidenced by the comprehensive distribution of parameter errors.","Additionally, this class of models is shown to substantially reduce the time to obtain parameter estimates.","Finally, we present applications to the diagnostic and prediction of two financial bubble peaks (followed by their crash) and of a famous rockslide.","These contributions provide a bridge between deep learning and the study of the prediction of transition times in complex time series."],"url":"http://arxiv.org/abs/2405.12803v1","category":"cs.CE"}
{"created":"2024-05-21 13:50:02","title":"Scientific discourse on YouTube: Motivations for citing research in comments","abstract":"YouTube is a valuable source of user-generated content on a wide range of topics, and it encourages user participation through the use of a comment system. Video content is increasingly addressing scientific topics, and there is evidence that both academics and consumers use video descriptions and video comments to refer to academic research and scientific publications. Because commenting is a discursive behavior, this study will provide insights on why individuals post links to research publications in comments. For this, a qualitative content analysis and iterative coding approach were applied. Furthermore, the reasons for mentioning academic publications in comments were contrasted with the reasons for citing in scholarly works and with reasons for commenting on YouTube. We discovered that the primary motives for sharing research links were (1) providing more insights into the topic and (2) challenging information offered by other commentators.","sentences":["YouTube is a valuable source of user-generated content on a wide range of topics, and it encourages user participation through the use of a comment system.","Video content is increasingly addressing scientific topics, and there is evidence that both academics and consumers use video descriptions and video comments to refer to academic research and scientific publications.","Because commenting is a discursive behavior, this study will provide insights on why individuals post links to research publications in comments.","For this, a qualitative content analysis and iterative coding approach were applied.","Furthermore, the reasons for mentioning academic publications in comments were contrasted with the reasons for citing in scholarly works and with reasons for commenting on YouTube.","We discovered that the primary motives for sharing research links were (1) providing more insights into the topic and (2) challenging information offered by other commentators."],"url":"http://arxiv.org/abs/2405.12798v1","category":"cs.CY"}
{"created":"2024-05-21 13:44:55","title":"DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control","abstract":"Generating customized content in videos has received increasing attention recently. However, existing works primarily focus on customized text-to-video generation for single subject, suffering from subject-missing and attribute-binding problems when the video is expected to contain multiple subjects. Furthermore, existing models struggle to assign the desired actions to the corresponding subjects (action-binding problem), failing to achieve satisfactory multi-subject generation performance. To tackle the problems, in this paper, we propose DisenStudio, a novel framework that can generate text-guided videos for customized multiple subjects, given few images for each subject. Specifically, DisenStudio enhances a pretrained diffusion-based text-to-video model with our proposed spatial-disentangled cross-attention mechanism to associate each subject with the desired action. Then the model is customized for the multiple subjects with the proposed motion-preserved disentangled finetuning, which involves three tuning strategies: multi-subject co-occurrence tuning, masked single-subject tuning, and multi-subject motion-preserved tuning. The first two strategies guarantee the subject occurrence and preserve their visual attributes, and the third strategy helps the model maintain the temporal motion-generation ability when finetuning on static images. We conduct extensive experiments to demonstrate our proposed DisenStudio significantly outperforms existing methods in various metrics. Additionally, we show that DisenStudio can be used as a powerful tool for various controllable generation applications.","sentences":["Generating customized content in videos has received increasing attention recently.","However, existing works primarily focus on customized text-to-video generation for single subject, suffering from subject-missing and attribute-binding problems when the video is expected to contain multiple subjects.","Furthermore, existing models struggle to assign the desired actions to the corresponding subjects (action-binding problem), failing to achieve satisfactory multi-subject generation performance.","To tackle the problems, in this paper, we propose DisenStudio, a novel framework that can generate text-guided videos for customized multiple subjects, given few images for each subject.","Specifically, DisenStudio enhances a pretrained diffusion-based text-to-video model with our proposed spatial-disentangled cross-attention mechanism to associate each subject with the desired action.","Then the model is customized for the multiple subjects with the proposed motion-preserved disentangled finetuning, which involves three tuning strategies: multi-subject co-occurrence tuning, masked single-subject tuning, and multi-subject motion-preserved tuning.","The first two strategies guarantee the subject occurrence and preserve their visual attributes, and the third strategy helps the model maintain the temporal motion-generation ability when finetuning on static images.","We conduct extensive experiments to demonstrate our proposed DisenStudio significantly outperforms existing methods in various metrics.","Additionally, we show that DisenStudio can be used as a powerful tool for various controllable generation applications."],"url":"http://arxiv.org/abs/2405.12796v1","category":"cs.CV"}
{"created":"2024-05-21 13:44:18","title":"The Atacama Cosmology Telescope: DR6 Gravitational Lensing and SDSS BOSS cross-correlation measurement and constraints on gravity with the $E_G$ statistic","abstract":"We derive new constraints on the $E_G$ statistic as a test of gravity, combining the CMB lensing map estimated from Data Release 6 (DR6) of the Atacama Cosmology Telescope with SDSS BOSS CMASS and LOWZ galaxy data. We develop an analysis pipeline to measure the cross-correlation between CMB lensing maps and galaxy data, following a blinding policy and testing the approach through null and consistency checks. By testing the equivalence of the spatial and temporal gravitational potentials, the $E_G$ statistic can distinguish $\\Lambda$CDM from alternative models of gravity. We find $E_G= 0.31^{+0.06}_{-0.05}$ for ACT and CMASS data at 68.28\\% confidence level, and $E_G = 0.49^{+0.14}_{-0.11}$ for ACT and LOWZ. Systematic errors are estimated to be 3\\% and 4\\% respectively. Including CMB lensing information from Planck PR4 results in $E_G = 0.34^{+0.05}_{-0.05}$ with CMASS and $E_G= 0.43^{+0.11}_{-0.09}$ with LOWZ. These are consistent with predictions for the $\\Lambda$CDM model that best fits the Planck CMB anisotropy and SDSS BOSS BAO, where $E_G^{\\rm GR} (z_{\\rm eff} = 0.555) = 0.401\\pm 0.005$ for CMB lensing combined with CMASS and $E_G^{\\rm GR} (z_{\\rm eff} = 0.316) = 0.452\\pm0.005$ combined with LOWZ. We also find $E_G$ to be scale independent, with PTE $>5\\%$, as predicted by general relativity. The methods developed in this work are also applicable to improved future analyses with upcoming spectroscopic galaxy samples and CMB lensing measurements.","sentences":["We derive new constraints on the $E_G$ statistic as a test of gravity, combining the CMB lensing map estimated from Data Release 6 (DR6) of the Atacama Cosmology Telescope with SDSS BOSS CMASS and LOWZ galaxy data.","We develop an analysis pipeline to measure the cross-correlation between CMB lensing maps and galaxy data, following a blinding policy and testing the approach through null and consistency checks.","By testing the equivalence of the spatial and temporal gravitational potentials, the $E_G$ statistic can distinguish $\\Lambda$CDM from alternative models of gravity.","We find $E_G= 0.31^{+0.06}_{-0.05}$ for ACT and CMASS data at 68.28\\% confidence level, and $E_G = 0.49^{+0.14}_{-0.11}$ for ACT and LOWZ.","Systematic errors are estimated to be 3\\% and 4\\% respectively.","Including CMB lensing information from Planck PR4 results in $E_G = 0.34^{+0.05}_{-0.05}$ with CMASS and $E_G= 0.43^{+0.11}_{-0.09}$ with LOWZ.","These are consistent with predictions for the $\\Lambda$CDM model that best fits the Planck CMB anisotropy and SDSS BOSS BAO, where $E_G^{\\rm GR} (z_{\\rm eff} = 0.555)","= 0.401\\pm 0.005$ for CMB lensing combined with CMASS and $E_G^{\\rm GR} (z_{\\rm eff} = 0.316)","= 0.452\\pm0.005$ combined with LOWZ.","We also find $E_G$ to be scale independent, with PTE $>5\\%$, as predicted by general relativity.","The methods developed in this work are also applicable to improved future analyses with upcoming spectroscopic galaxy samples and CMB lensing measurements."],"url":"http://arxiv.org/abs/2405.12795v1","category":"astro-ph.CO"}
{"created":"2024-05-21 13:42:35","title":"Adaptive local boundary conditions to improve Deformable Image Registration","abstract":"Objective: In medical imaging, it is often crucial to accurately assess and correct movement during image-guided therapy. Deformable image registration (DIR) consists in estimating the required spatial transformation to align a moving image with a fixed one. However, it is acknowledged that, boundary conditions applied to the solution are critical in preventing mis-registration. Despite the extensive research on registration techniques, relatively few have addressed the issue of boundary conditions in the context of medical DIR. Our aim is a step towards customizing boundary conditions to suit the diverse registration tasks at hand.   Approach: We propose a generic, locally adaptive, Robin-type condition enabling to balance between Dirichlet and Neumann boundary conditions, depending on incoming/outgoing flow fields on the image boundaries. The proposed framework is entirely automatized through the determination of a reduced set of hyperparameters optimized via energy minimization.   Main results: The proposed approach was tested on a mono-modal CT thorax registration task and an abdominal CT to MRI registration task. For the first task, we observed a relative improvement in terms of target registration error of up to 12% (mean 4%), compared to homogeneous Dirichlet and homogeneous Neumann. For the second task, the automatic framework provides results closed to the best achievable.   Significance: This study underscores the importance of tailoring the registration problem at the image boundaries. In this research, we introduce a novel method to adapt the boundary conditions on a voxel-by-voxel basis, yielding optimized results in two distinct tasks: mono-modal CT thorax registration and abdominal CT to MRI registration. The proposed framework enables optimized boundary conditions in image registration without any a priori assumptions regarding the images or the motion.","sentences":["Objective: In medical imaging, it is often crucial to accurately assess and correct movement during image-guided therapy.","Deformable image registration (DIR) consists in estimating the required spatial transformation to align a moving image with a fixed one.","However, it is acknowledged that, boundary conditions applied to the solution are critical in preventing mis-registration.","Despite the extensive research on registration techniques, relatively few have addressed the issue of boundary conditions in the context of medical DIR.","Our aim is a step towards customizing boundary conditions to suit the diverse registration tasks at hand.   ","Approach:","We propose a generic, locally adaptive, Robin-type condition enabling to balance between Dirichlet and Neumann boundary conditions, depending on incoming/outgoing flow fields on the image boundaries.","The proposed framework is entirely automatized through the determination of a reduced set of hyperparameters optimized via energy minimization.   ","Main results: The proposed approach was tested on a mono-modal CT thorax registration task and an abdominal CT to MRI registration task.","For the first task, we observed a relative improvement in terms of target registration error of up to 12% (mean 4%), compared to homogeneous Dirichlet and homogeneous Neumann.","For the second task, the automatic framework provides results closed to the best achievable.   ","Significance: This study underscores the importance of tailoring the registration problem at the image boundaries.","In this research, we introduce a novel method to adapt the boundary conditions on a voxel-by-voxel basis, yielding optimized results in two distinct tasks: mono-modal CT thorax registration and abdominal CT to MRI registration.","The proposed framework enables optimized boundary conditions in image registration without any a priori assumptions regarding the images or the motion."],"url":"http://arxiv.org/abs/2405.12791v1","category":"cs.CV"}
{"created":"2024-05-21 13:40:54","title":"A Novel Methodology for Autonomous Planetary Exploration Using Multi-Robot Teams","abstract":"One of the fundamental limiting factors in planetary exploration is the autonomous capabilities of planetary exploration rovers. This study proposes a novel methodology for trustworthy autonomous multi-robot teams which incorporates data from multiple sources (HiRISE orbiter imaging, probability distribution maps, and on-board rover sensors) to find efficient exploration routes in Jezero crater. A map is generated, consisting of a 3D terrain model, traversability analysis, and probability distribution map of points of scientific interest. A three-stage mission planner generates an efficient route, which maximises the accumulated probability of identifying points of interest. A 4D RRT* algorithm is used to determine smooth, flat paths, and prioritised planning is used to coordinate a safe set of paths. The above methodology is shown to coordinate safe and efficient rover paths, which ensure the rovers remain within their nominal pitch and roll limits throughout operation.","sentences":["One of the fundamental limiting factors in planetary exploration is the autonomous capabilities of planetary exploration rovers.","This study proposes a novel methodology for trustworthy autonomous multi-robot teams which incorporates data from multiple sources (HiRISE orbiter imaging, probability distribution maps, and on-board rover sensors) to find efficient exploration routes in Jezero crater.","A map is generated, consisting of a 3D terrain model, traversability analysis, and probability distribution map of points of scientific interest.","A three-stage mission planner generates an efficient route, which maximises the accumulated probability of identifying points of interest.","A 4D RRT* algorithm is used to determine smooth, flat paths, and prioritised planning is used to coordinate a safe set of paths.","The above methodology is shown to coordinate safe and efficient rover paths, which ensure the rovers remain within their nominal pitch and roll limits throughout operation."],"url":"http://arxiv.org/abs/2405.12790v1","category":"cs.RO"}
{"created":"2024-05-21 13:38:15","title":"What Have We Achieved on Non-autoregressive Translation?","abstract":"Recent advances have made non-autoregressive (NAT) translation comparable to autoregressive methods (AT). However, their evaluation using BLEU has been shown to weakly correlate with human annotations. Limited research compares non-autoregressive translation and autoregressive translation comprehensively, leaving uncertainty about the true proximity of NAT to AT. To address this gap, we systematically evaluate four representative NAT methods across various dimensions, including human evaluation. Our empirical results demonstrate that despite narrowing the performance gap, state-of-the-art NAT still underperforms AT under more reliable evaluation metrics. Furthermore, we discover that explicitly modeling dependencies is crucial for generating natural language and generalizing to out-of-distribution sequences.","sentences":["Recent advances have made non-autoregressive (NAT) translation comparable to autoregressive methods (AT).","However, their evaluation using BLEU has been shown to weakly correlate with human annotations.","Limited research compares non-autoregressive translation and autoregressive translation comprehensively, leaving uncertainty about the true proximity of NAT to AT.","To address this gap, we systematically evaluate four representative NAT methods across various dimensions, including human evaluation.","Our empirical results demonstrate that despite narrowing the performance gap, state-of-the-art NAT still underperforms AT under more reliable evaluation metrics.","Furthermore, we discover that explicitly modeling dependencies is crucial for generating natural language and generalizing to out-of-distribution sequences."],"url":"http://arxiv.org/abs/2405.12788v1","category":"cs.CL"}
{"created":"2024-05-21 13:34:23","title":"Rethinking the Vulnerabilities of Face Recognition Systems:From a Practical Perspective","abstract":"Face Recognition Systems (FRS) have increasingly integrated into critical applications, including surveillance and user authentication, highlighting their pivotal role in modern security systems. Recent studies have revealed vulnerabilities in FRS to adversarial (e.g., adversarial patch attacks) and backdoor attacks (e.g., training data poisoning), raising significant concerns about their reliability and trustworthiness. Previous studies primarily focus on traditional adversarial or backdoor attacks, overlooking the resource-intensive or privileged-manipulation nature of such threats, thus limiting their practical generalization, stealthiness, universality and robustness. Correspondingly, in this paper, we delve into the inherent vulnerabilities in FRS through user studies and preliminary explorations. By exploiting these vulnerabilities, we identify a novel attack, facial identity backdoor attack dubbed FIBA, which unveils a potentially more devastating threat against FRS:an enrollment-stage backdoor attack. FIBA circumvents the limitations of traditional attacks, enabling broad-scale disruption by allowing any attacker donning a specific trigger to bypass these systems. This implies that after a single, poisoned example is inserted into the database, the corresponding trigger becomes a universal key for any attackers to spoof the FRS. This strategy essentially challenges the conventional attacks by initiating at the enrollment stage, dramatically transforming the threat landscape by poisoning the feature database rather than the training data.","sentences":["Face Recognition Systems (FRS) have increasingly integrated into critical applications, including surveillance and user authentication, highlighting their pivotal role in modern security systems.","Recent studies have revealed vulnerabilities in FRS to adversarial (e.g., adversarial patch attacks) and backdoor attacks (e.g., training data poisoning), raising significant concerns about their reliability and trustworthiness.","Previous studies primarily focus on traditional adversarial or backdoor attacks, overlooking the resource-intensive or privileged-manipulation nature of such threats, thus limiting their practical generalization, stealthiness, universality and robustness.","Correspondingly, in this paper, we delve into the inherent vulnerabilities in FRS through user studies and preliminary explorations.","By exploiting these vulnerabilities, we identify a novel attack, facial identity backdoor attack dubbed FIBA, which unveils a potentially more devastating threat against FRS:an enrollment-stage backdoor attack.","FIBA circumvents the limitations of traditional attacks, enabling broad-scale disruption by allowing any attacker donning a specific trigger to bypass these systems.","This implies that after a single, poisoned example is inserted into the database, the corresponding trigger becomes a universal key for any attackers to spoof the FRS.","This strategy essentially challenges the conventional attacks by initiating at the enrollment stage, dramatically transforming the threat landscape by poisoning the feature database rather than the training data."],"url":"http://arxiv.org/abs/2405.12786v1","category":"cs.CR"}
{"created":"2024-05-21 13:32:46","title":"Artificial Intelligence Approaches for Predictive Maintenance in the Steel Industry: A Survey","abstract":"Predictive Maintenance (PdM) emerged as one of the pillars of Industry 4.0, and became crucial for enhancing operational efficiency, allowing to minimize downtime, extend lifespan of equipment, and prevent failures. A wide range of PdM tasks can be performed using Artificial Intelligence (AI) methods, which often use data generated from industrial sensors. The steel industry, which is an important branch of the global economy, is one of the potential beneficiaries of this trend, given its large environmental footprint, the globalized nature of the market, and the demanding working conditions. This survey synthesizes the current state of knowledge in the field of AI-based PdM within the steel industry and is addressed to researchers and practitioners. We identified 219 articles related to this topic and formulated five research questions, allowing us to gain a global perspective on current trends and the main research gaps. We examined equipment and facilities subjected to PdM, determined common PdM approaches, and identified trends in the AI methods used to develop these solutions. We explored the characteristics of the data used in the surveyed articles and assessed the practical implications of the research presented there. Most of the research focuses on the blast furnace or hot rolling, using data from industrial sensors. Current trends show increasing interest in the domain, especially in the use of deep learning. The main challenges include implementing the proposed methods in a production environment, incorporating them into maintenance plans, and enhancing the accessibility and reproducibility of the research.","sentences":["Predictive Maintenance (PdM) emerged as one of the pillars of Industry 4.0, and became crucial for enhancing operational efficiency, allowing to minimize downtime, extend lifespan of equipment, and prevent failures.","A wide range of PdM tasks can be performed using Artificial Intelligence (AI) methods, which often use data generated from industrial sensors.","The steel industry, which is an important branch of the global economy, is one of the potential beneficiaries of this trend, given its large environmental footprint, the globalized nature of the market, and the demanding working conditions.","This survey synthesizes the current state of knowledge in the field of AI-based PdM within the steel industry and is addressed to researchers and practitioners.","We identified 219 articles related to this topic and formulated five research questions, allowing us to gain a global perspective on current trends and the main research gaps.","We examined equipment and facilities subjected to PdM, determined common PdM approaches, and identified trends in the AI methods used to develop these solutions.","We explored the characteristics of the data used in the surveyed articles and assessed the practical implications of the research presented there.","Most of the research focuses on the blast furnace or hot rolling, using data from industrial sensors.","Current trends show increasing interest in the domain, especially in the use of deep learning.","The main challenges include implementing the proposed methods in a production environment, incorporating them into maintenance plans, and enhancing the accessibility and reproducibility of the research."],"url":"http://arxiv.org/abs/2405.12785v1","category":"cs.AI"}
{"created":"2024-05-21 13:29:35","title":"Generalize Polyp Segmentation via Inpainting across Diverse Backgrounds and Pseudo-Mask Refinement","abstract":"Inpainting lesions within different normal backgrounds is a potential method of addressing the generalization problem, which is crucial for polyp segmentation models. However, seamlessly introducing polyps into complex endoscopic environments while simultaneously generating accurate pseudo-masks remains a challenge for current inpainting methods. To address these issues, we first leverage the pre-trained Stable Diffusion Inpaint and ControlNet, to introduce a robust generative model capable of inpainting polyps across different backgrounds. Secondly, we utilize the prior that synthetic polyps are confined to the inpainted region, to establish an inpainted region-guided pseudo-mask refinement network. We also propose a sample selection strategy that prioritizes well-aligned and hard synthetic cases for further model fine-tuning. Experiments demonstrate that our inpainting model outperformed baseline methods both qualitatively and quantitatively in inpainting quality. Moreover, our data augmentation strategy significantly enhances the performance of polyp segmentation models on external datasets, achieving or surpassing the level of fully supervised training benchmarks in that domain. Our code is available at https://github.com/497662892/PolypInpainter.","sentences":["Inpainting lesions within different normal backgrounds is a potential method of addressing the generalization problem, which is crucial for polyp segmentation models.","However, seamlessly introducing polyps into complex endoscopic environments while simultaneously generating accurate pseudo-masks remains a challenge for current inpainting methods.","To address these issues, we first leverage the pre-trained Stable Diffusion Inpaint and ControlNet, to introduce a robust generative model capable of inpainting polyps across different backgrounds.","Secondly, we utilize the prior that synthetic polyps are confined to the inpainted region, to establish an inpainted region-guided pseudo-mask refinement network.","We also propose a sample selection strategy that prioritizes well-aligned and hard synthetic cases for further model fine-tuning.","Experiments demonstrate that our inpainting model outperformed baseline methods both qualitatively and quantitatively in inpainting quality.","Moreover, our data augmentation strategy significantly enhances the performance of polyp segmentation models on external datasets, achieving or surpassing the level of fully supervised training benchmarks in that domain.","Our code is available at https://github.com/497662892/PolypInpainter."],"url":"http://arxiv.org/abs/2405.12784v1","category":"cs.CV"}
{"created":"2024-05-21 13:29:24","title":"Epanechnikov Variational Autoencoder","abstract":"In this paper, we bridge Variational Autoencoders (VAEs) [17] and kernel density estimations (KDEs) [25 ],[23] by approximating the posterior by KDEs and deriving an upper bound of the Kullback-Leibler (KL) divergence in the evidence lower bound (ELBO). The flexibility of KDEs makes the optimization of posteriors in VAEs possible, which not only addresses the limitations of Gaussian latent space in vanilla VAE but also provides a new perspective of estimating the KL-divergence in ELBO. Under appropriate conditions [ 9],[3 ], we show that the Epanechnikov kernel is the optimal choice in minimizing the derived upper bound of KL-divergence asymptotically. Compared with Gaussian kernel, Epanechnikov kernel has compact support which should make the generated sample less noisy and blurry. The implementation of Epanechnikov kernel in ELBO is straightforward as it lies in the \"location-scale\" family of distributions where the reparametrization tricks can be directly employed. A series of experiments on benchmark datasets such as MNIST, Fashion-MNIST, CIFAR-10 and CelebA further demonstrate the superiority of Epanechnikov Variational Autoenocoder (EVAE) over vanilla VAE in the quality of reconstructed images, as measured by the FID score and Sharpness[27].","sentences":["In this paper, we bridge Variational Autoencoders (VAEs)","[17] and kernel density estimations (KDEs)","[25 ],[23] by approximating the posterior by KDEs and deriving an upper bound of the Kullback-Leibler (KL) divergence in the evidence lower bound (ELBO).","The flexibility of KDEs makes the optimization of posteriors in VAEs possible, which not only addresses the limitations of Gaussian latent space in vanilla VAE but also provides a new perspective of estimating the KL-divergence in ELBO.","Under appropriate conditions [ 9],[3 ], we show that the Epanechnikov kernel is the optimal choice in minimizing the derived upper bound of KL-divergence asymptotically.","Compared with Gaussian kernel, Epanechnikov kernel has compact support which should make the generated sample less noisy and blurry.","The implementation of Epanechnikov kernel in ELBO is straightforward as it lies in the \"location-scale\" family of distributions where the reparametrization tricks can be directly employed.","A series of experiments on benchmark datasets such as MNIST, Fashion-MNIST, CIFAR-10 and CelebA further demonstrate the superiority of Epanechnikov Variational Autoenocoder (EVAE) over vanilla VAE in the quality of reconstructed images, as measured by the FID score and Sharpness[27]."],"url":"http://arxiv.org/abs/2405.12783v1","category":"stat.ML"}
{"created":"2024-05-21 13:26:27","title":"Transformer in Touch: A Survey","abstract":"The Transformer model, initially achieving significant success in the field of natural language processing, has recently shown great potential in the application of tactile perception. This review aims to comprehensively outline the application and development of Transformers in tactile technology. We first introduce the two fundamental concepts behind the success of the Transformer: the self-attention mechanism and large-scale pre-training. Then, we delve into the application of Transformers in various tactile tasks, including but not limited to object recognition, cross-modal generation, and object manipulation, offering a concise summary of the core methodologies, performance benchmarks, and design highlights. Finally, we suggest potential areas for further research and future work, aiming to generate more interest within the community, tackle existing challenges, and encourage the use of Transformer models in the tactile field.","sentences":["The Transformer model, initially achieving significant success in the field of natural language processing, has recently shown great potential in the application of tactile perception.","This review aims to comprehensively outline the application and development of Transformers in tactile technology.","We first introduce the two fundamental concepts behind the success of the Transformer: the self-attention mechanism and large-scale pre-training.","Then, we delve into the application of Transformers in various tactile tasks, including but not limited to object recognition, cross-modal generation, and object manipulation, offering a concise summary of the core methodologies, performance benchmarks, and design highlights.","Finally, we suggest potential areas for further research and future work, aiming to generate more interest within the community, tackle existing challenges, and encourage the use of Transformer models in the tactile field."],"url":"http://arxiv.org/abs/2405.12779v1","category":"cs.LG"}
{"created":"2024-05-21 13:26:21","title":"A conclusive non-detection of magnetic field in the Am star o Peg with high-precision near-infrared spectroscopy","abstract":"The A-type metallic-line (Am) stars are typically considered to be non-magnetic or possessing very weak sub-G magnetic fields. This view has been repeatedly challenged in the literature, most commonly for the bright hot Am star o Peg. Several studies claimed to detect 1-2 kG field of unknown topology in this object, possibly indicating a new process of magnetic field generation in intermediate-mass stars. In this study, we revisit the evidence of a strong magnetic field in o Peg using new high-resolution spectropolarimetric observations and advanced spectral fitting techniques. The mean magnetic field strength in o Peg is estimated from the high-precision CRIRES+ measurement of near-infrared sulphur lines. This observation is modelled with a polarised radiative transfer code, including treatment of the departures from local thermodynamic equilibrium. In addition, the least-squares deconvolution multi-line technique is employed to derive longitudinal field measurements from archival optical spectropolarimetric observations of this star. Our analysis of the near-infrared S I lines reveals no evidence of Zeeman broadening, ruling out magnetic field with a strength exceeding 260 G. This null result is compatible with the relative intensification of Fe II lines in the optical spectrum taking into account blending and uncertain atomic parameters of the relevant diagnostic transitions. Longitudinal field measurements at three different nights also yield null results with a precision of 2 G. This study refutes the claims of kG-strength dipolar or tangled magnetic field in o Peg. This star is effectively non-magnetic, with the surface magnetic field characteristics no different from those of other Am stars.","sentences":["The A-type metallic-line (Am) stars are typically considered to be non-magnetic or possessing very weak sub-G magnetic fields.","This view has been repeatedly challenged in the literature, most commonly for the bright hot Am star o Peg.","Several studies claimed to detect 1-2 kG field of unknown topology in this object, possibly indicating a new process of magnetic field generation in intermediate-mass stars.","In this study, we revisit the evidence of a strong magnetic field in o Peg using new high-resolution spectropolarimetric observations and advanced spectral fitting techniques.","The mean magnetic field strength in o Peg is estimated from the high-precision CRIRES+ measurement of near-infrared sulphur lines.","This observation is modelled with a polarised radiative transfer code, including treatment of the departures from local thermodynamic equilibrium.","In addition, the least-squares deconvolution multi-line technique is employed to derive longitudinal field measurements from archival optical spectropolarimetric observations of this star.","Our analysis of the near-infrared S I lines reveals no evidence of Zeeman broadening, ruling out magnetic field with a strength exceeding 260 G.","This null result is compatible with the relative intensification of Fe II lines in the optical spectrum taking into account blending and uncertain atomic parameters of the relevant diagnostic transitions.","Longitudinal field measurements at three different nights also yield null results with a precision of 2 G. This study refutes the claims of kG-strength dipolar or tangled magnetic field in o Peg.","This star is effectively non-magnetic, with the surface magnetic field characteristics no different from those of other Am stars."],"url":"http://arxiv.org/abs/2405.12778v1","category":"astro-ph.SR"}
{"created":"2024-05-21 13:25:35","title":"On-demand heralded MIR single-photon source using a cascaded quantum system","abstract":"We propose a novel mechanism for generating single photons in the mid-Infrared (MIR) using a solid-state or molecular quantum emitter. The scheme utilises cavity QED effects to selectively enhance a Frank-Condon transition, deterministically preparing a single Fock state of a polar phonon mode. By coupling the phonon mode to an antenna, the resulting excitation is then radiated to the far field as a single photon with a frequency matching the phonon mode. By combining macroscopic QED calculations with methods from open quantum system theory, we show that optimal parameters to generate these MIR photons occur for modest light-matter coupling strengths, which are achievable with state-of-the-art technologies. Combined, the cascaded system we propose provides a new quasi-deterministic source of heralded single photons in a regime of the electromagnetic spectrum where this previously was not possible.","sentences":["We propose a novel mechanism for generating single photons in the mid-Infrared (MIR) using a solid-state or molecular quantum emitter.","The scheme utilises cavity QED effects to selectively enhance a Frank-Condon transition, deterministically preparing a single Fock state of a polar phonon mode.","By coupling the phonon mode to an antenna, the resulting excitation is then radiated to the far field as a single photon with a frequency matching the phonon mode.","By combining macroscopic QED calculations with methods from open quantum system theory, we show that optimal parameters to generate these MIR photons occur for modest light-matter coupling strengths, which are achievable with state-of-the-art technologies.","Combined, the cascaded system we propose provides a new quasi-deterministic source of heralded single photons in a regime of the electromagnetic spectrum where this previously was not possible."],"url":"http://arxiv.org/abs/2405.12777v1","category":"physics.optics"}
{"created":"2024-05-21 13:24:07","title":"Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances","abstract":"Discovering the semantics of multimodal utterances is essential for understanding human language and enhancing human-machine interactions. Existing methods manifest limitations in leveraging nonverbal information for discerning complex semantics in unsupervised scenarios. This paper introduces a novel unsupervised multimodal clustering method (UMC), making a pioneering contribution to this field. UMC introduces a unique approach to constructing augmentation views for multimodal data, which are then used to perform pre-training to establish well-initialized representations for subsequent clustering. An innovative strategy is proposed to dynamically select high-quality samples as guidance for representation learning, gauged by the density of each sample's nearest neighbors. Besides, it is equipped to automatically determine the optimal value for the top-$K$ parameter in each cluster to refine sample selection. Finally, both high- and low-quality samples are used to learn representations conducive to effective clustering. We build baselines on benchmark multimodal intent and dialogue act datasets. UMC shows remarkable improvements of 2-6\\% scores in clustering metrics over state-of-the-art methods, marking the first successful endeavor in this domain. The complete code and data are available at https://github.com/thuiar/UMC.","sentences":["Discovering the semantics of multimodal utterances is essential for understanding human language and enhancing human-machine interactions.","Existing methods manifest limitations in leveraging nonverbal information for discerning complex semantics in unsupervised scenarios.","This paper introduces a novel unsupervised multimodal clustering method (UMC), making a pioneering contribution to this field.","UMC introduces a unique approach to constructing augmentation views for multimodal data, which are then used to perform pre-training to establish well-initialized representations for subsequent clustering.","An innovative strategy is proposed to dynamically select high-quality samples as guidance for representation learning, gauged by the density of each sample's nearest neighbors.","Besides, it is equipped to automatically determine the optimal value for the top-$K$ parameter in each cluster to refine sample selection.","Finally, both high- and low-quality samples are used to learn representations conducive to effective clustering.","We build baselines on benchmark multimodal intent and dialogue act datasets.","UMC shows remarkable improvements of 2-6\\% scores in clustering metrics over state-of-the-art methods, marking the first successful endeavor in this domain.","The complete code and data are available at https://github.com/thuiar/UMC."],"url":"http://arxiv.org/abs/2405.12775v1","category":"cs.MM"}
{"created":"2024-05-21 13:24:05","title":"Blind Separation of Vibration Sources using Deep Learning and Deconvolution","abstract":"Vibrations of rotating machinery primarily originate from two sources, both of which are distorted by the machine's transfer function on their way to the sensor: the dominant gear-related vibrations and a low-energy signal linked to bearing faults. The proposed method facilitates the blind separation of vibration sources, eliminating the need for any information about the monitored equipment or external measurements. This method estimates both sources in two stages: initially, the gear signal is isolated using a dilated CNN, followed by the estimation of the bearing fault signal using the squared log envelope of the residual. The effect of the transfer function is removed from both sources using a novel whitening-based deconvolution method (WBD). Both simulation and experimental results demonstrate the method's ability to detect bearing failures early when no additional information is available. This study considers both local and distributed bearing faults, assuming that the vibrations are recorded under stable operating conditions.","sentences":["Vibrations of rotating machinery primarily originate from two sources, both of which are distorted by the machine's transfer function on their way to the sensor: the dominant gear-related vibrations and a low-energy signal linked to bearing faults.","The proposed method facilitates the blind separation of vibration sources, eliminating the need for any information about the monitored equipment or external measurements.","This method estimates both sources in two stages: initially, the gear signal is isolated using a dilated CNN, followed by the estimation of the bearing fault signal using the squared log envelope of the residual.","The effect of the transfer function is removed from both sources using a novel whitening-based deconvolution method (WBD).","Both simulation and experimental results demonstrate the method's ability to detect bearing failures early when no additional information is available.","This study considers both local and distributed bearing faults, assuming that the vibrations are recorded under stable operating conditions."],"url":"http://arxiv.org/abs/2405.12774v1","category":"cs.LG"}
{"created":"2024-05-21 13:23:59","title":"Towards nonlinear optics with M\u00f6ssbauer nuclei using x-ray cavities","abstract":"Strong excitation of nuclear resonances, particularly of Mo\\\"ossbauer nuclei, has been a longstanding goal and the advance of novel x-ray sources is promising new options in this regard. Here we map out the necessary experimental conditions for the more general goal of realizing nonlinear optics with nuclei and compare with available technology. In particular, we present a comprehensive theory of nonlinear nuclear excitation in thin-film x-ray cavities by focused x-ray pulses. We thereby identify cavity geometries with broad resonances which allow one to boost the nuclear excitation even at moderately tight focusing.","sentences":["Strong excitation of nuclear resonances, particularly of Mo\\\"ossbauer nuclei, has been a longstanding goal and the advance of novel x-ray sources is promising new options in this regard.","Here we map out the necessary experimental conditions for the more general goal of realizing nonlinear optics with nuclei and compare with available technology.","In particular, we present a comprehensive theory of nonlinear nuclear excitation in thin-film x-ray cavities by focused x-ray pulses.","We thereby identify cavity geometries with broad resonances which allow one to boost the nuclear excitation even at moderately tight focusing."],"url":"http://arxiv.org/abs/2405.12773v1","category":"quant-ph"}
{"created":"2024-05-21 13:23:11","title":"All-fiber, near-infrared, laser system at 780nm for atom cooling","abstract":"Cold neutral atoms are a promising \"architecture\" for novel and innovative quantum technology applications. These range from quantum sensors for magnetic fields, interferometers for gravimetry measurements, to quantum computing and simulations. Within this technological landscape, reliable laser systems are crucial in order to cool and manipulate atoms efficiently. We present the design, experimental realization and characterization of a relatively simple, compact, and economical laser system for quantum technology applications. It operates at 780 nm in order to work with Rubidium atoms, and it is entirely based on fiber components. This improves its reliability, and makes it less complex and more versatile with respect to a free-space system.   In order to cut costs, we take advantage of already available commercial fiber devices at 1560 nm: due to their ubiquity in telecom applications they are in fact cheap and standardized. To operate at Rubidium wavelengths the system employs Second-Harmonic Generation (SHG) of the 1560 nm infrared radiation: two semiconductor lasers at 1560 nm (one for cooling, one for repumping) are combined, amplified in an Erbium-Doped Fiber Amplifier (EDFA), and frequency-doubled in a Periodically-Poled Lithium Niobate (PPLN) crystal.   We characterize the amplitude stability of the lasers, their frequency noise, and the SHG efficiency. The measured features (rms relative amplitude noise of 3$\\times$10$^{-4}$ at 1 s; linewidths well below 1 MHz) make our system a suitable candidate for applications in quantum sensors and atom interferometers.","sentences":["Cold neutral atoms are a promising \"architecture\" for novel and innovative quantum technology applications.","These range from quantum sensors for magnetic fields, interferometers for gravimetry measurements, to quantum computing and simulations.","Within this technological landscape, reliable laser systems are crucial in order to cool and manipulate atoms efficiently.","We present the design, experimental realization and characterization of a relatively simple, compact, and economical laser system for quantum technology applications.","It operates at 780 nm in order to work with Rubidium atoms, and it is entirely based on fiber components.","This improves its reliability, and makes it less complex and more versatile with respect to a free-space system.   ","In order to cut costs, we take advantage of already available commercial fiber devices at 1560 nm: due to their ubiquity in telecom applications they are in fact cheap and standardized.","To operate at Rubidium wavelengths the system employs Second-Harmonic Generation (SHG) of the 1560 nm infrared radiation: two semiconductor lasers at 1560 nm (one for cooling, one for repumping) are combined, amplified in an Erbium-Doped Fiber Amplifier (EDFA), and frequency-doubled in a Periodically-Poled Lithium Niobate (PPLN) crystal.   ","We characterize the amplitude stability of the lasers, their frequency noise, and the SHG efficiency.","The measured features (rms relative amplitude noise of 3$\\times$10$^{-4}$ at 1 s; linewidths well below 1 MHz) make our system a suitable candidate for applications in quantum sensors and atom interferometers."],"url":"http://arxiv.org/abs/2405.12770v1","category":"physics.atom-ph"}
{"created":"2024-05-21 13:22:56","title":"Local Supersymmetry Enhancement and Black Hole Microstates","abstract":"In string theory, black holes can be made from brane systems that are partially delocalized in string theory's extra dimensions. It is expected that they correspond to an ensemble of pure, horizonless microstates sourced by localized versions of this brane system. This thesis aims at studying the relationship between supersymmetric black holes and their microstates. In particular, we apply a concept called local supersymmetry enhancement to 1/4-BPS and 1/8-BPS systems that correspond to two-, three-, and four-charge black holes. This concept locally increases the number of preserved supercharges to 16 by adding dipole charges to the system. These bound states are then expected to be horizonless and describe the microstates of the black hole. First, we list all dipole charges for 1/4-BPS systems in Type II string theory. Then, we apply these to generate two structures to enhance 1/8-BPS systems, one with exclusively internal dipole charges and the other with a non-trivial shape in the non-compact dimensions. Thus, we identify the dipole charges for the microstates of various supersymmetric black holes, including new internal dipole charges for the D1-D5-P black hole and external ones for the D2-D2-D2-D6 black hole. These dipole charges reveal the local structure of the black hole's microstates and will be used in the construction of the corresponding supergravity solutions.","sentences":["In string theory, black holes can be made from brane systems that are partially delocalized in string theory's extra dimensions.","It is expected that they correspond to an ensemble of pure, horizonless microstates sourced by localized versions of this brane system.","This thesis aims at studying the relationship between supersymmetric black holes and their microstates.","In particular, we apply a concept called local supersymmetry enhancement to 1/4-BPS and 1/8-BPS systems that correspond to two-, three-, and four-charge black holes.","This concept locally increases the number of preserved supercharges to 16 by adding dipole charges to the system.","These bound states are then expected to be horizonless and describe the microstates of the black hole.","First, we list all dipole charges for 1/4-BPS systems in Type II string theory.","Then, we apply these to generate two structures to enhance 1/8-BPS systems, one with exclusively internal dipole charges and the other with a non-trivial shape in the non-compact dimensions.","Thus, we identify the dipole charges for the microstates of various supersymmetric black holes, including new internal dipole charges for the D1-D5-P black hole and external ones for the D2-D2-D2-D6 black hole.","These dipole charges reveal the local structure of the black hole's microstates and will be used in the construction of the corresponding supergravity solutions."],"url":"http://arxiv.org/abs/2405.12769v1","category":"hep-th"}
{"created":"2024-05-21 13:22:47","title":"Ponzi Funds","abstract":"Many active funds hold concentrated portfolios. Flow-driven trading in these securities causes price pressure, which pushes up the funds' existing positions resulting in realized returns. We decompose fund returns into a price pressure (self-inflated) and a fundamental component and show that when allocating capital across funds, investors are unable to identify whether realized returns are self-inflated or fundamental. Because investors chase self-inflated fund returns at a high frequency, even short-lived impact meaningfully affects fund flows at longer time scales. The combination of price impact and return chasing causes an endogenous feedback loop and a reallocation of wealth to early fund investors, which unravels once the price pressure reverts. We find that flows chasing self-inflated returns predict bubbles in ETFs and their subsequent crashes, and lead to a daily wealth reallocation of 500 Million from ETFs alone. We provide a simple regulatory reporting measure -- fund illiquidity -- which captures a fund's potential for self-inflated returns.","sentences":["Many active funds hold concentrated portfolios.","Flow-driven trading in these securities causes price pressure, which pushes up the funds' existing positions resulting in realized returns.","We decompose fund returns into a price pressure (self-inflated) and a fundamental component and show that when allocating capital across funds, investors are unable to identify whether realized returns are self-inflated or fundamental.","Because investors chase self-inflated fund returns at a high frequency, even short-lived impact meaningfully affects fund flows at longer time scales.","The combination of price impact and return chasing causes an endogenous feedback loop and a reallocation of wealth to early fund investors, which unravels once the price pressure reverts.","We find that flows chasing self-inflated returns predict bubbles in ETFs and their subsequent crashes, and lead to a daily wealth reallocation of 500 Million from ETFs alone.","We provide a simple regulatory reporting measure -- fund illiquidity -- which captures a fund's potential for self-inflated returns."],"url":"http://arxiv.org/abs/2405.12768v1","category":"q-fin.GN"}
{"created":"2024-05-21 13:19:10","title":"Test Oracle Automation in the era of LLMs","abstract":"The effectiveness of a test suite in detecting faults highly depends on the correctness and completeness of its test oracles. Large Language Models (LLMs) have already demonstrated remarkable proficiency in tackling diverse software testing tasks, such as automated test generation and program repair. This paper aims to enable discussions on the potential of using LLMs for test oracle automation, along with the challenges that may emerge during the generation of various types of oracles. Additionally, our aim is to initiate discussions on the primary threats that SE researchers must consider when employing LLMs for oracle automation, encompassing concerns regarding oracle deficiencies and data leakages.","sentences":["The effectiveness of a test suite in detecting faults highly depends on the correctness and completeness of its test oracles.","Large Language Models (LLMs) have already demonstrated remarkable proficiency in tackling diverse software testing tasks, such as automated test generation and program repair.","This paper aims to enable discussions on the potential of using LLMs for test oracle automation, along with the challenges that may emerge during the generation of various types of oracles.","Additionally, our aim is to initiate discussions on the primary threats that SE researchers must consider when employing LLMs for oracle automation, encompassing concerns regarding oracle deficiencies and data leakages."],"url":"http://arxiv.org/abs/2405.12766v1","category":"cs.SE"}
{"created":"2024-05-21 13:18:54","title":"Faster linear-sze And-Or path and adder circuits","abstract":"We consider the fundamental problem of constructing fast and small circuits for binary addition. We propose a new algorithm with running time $\\mathcal O(n \\log_2 n)$ for constructing linear-size $n$-bit adder circuits with a significantly better depth guarantee compared to previous approaches: Our circuits have a depth of at most $\\log_2 n + \\log_2 \\log_2 n + \\log_2 \\log_2 \\log_2 n + \\text{const}$, improving upon the previously best circuits by [12] with a depth of at most $\\log_2 n + 8 \\sqrt{\\log_2 n} + 6 \\log_2 \\log_2 n + \\text{const}$. Hence, we decrease the gap to the lower bound of $\\log_2 n + \\log_2 \\log_2 n + \\text{const}$ by [5] significantly from $\\mathcal O (\\sqrt{\\log_2 n})$ to $\\mathcal O(\\log_2 \\log_2 \\log_2 n)$.   Our core routine is a new algorithm for the construction of a circuit for a single carry bit, or, more generally, for an And-Or path, i.e., a Boolean function of type $t_0 \\lor ( t_1 \\land (t_2 \\lor ( \\dots t_{m-1}) \\dots ))$. We compute linear-size And-Or path circuits with a depth of at most $\\log_2 m + \\log_2 \\log_2 m + 0.65$ in time $\\mathcal O(m \\log_2 m)$. These are the first And-Or path circuits known that, up to an additive constant, match the lower bound by [5] and at the same time have a linear size. The previously fastest And-Or path circuits are only by an additive constant worse in depth, but have a much higher size in the order of $\\mathcal O (m \\log_2 m)$.","sentences":["We consider the fundamental problem of constructing fast and small circuits for binary addition.","We propose a new algorithm with running time $\\mathcal O(n \\log_2 n)$ for constructing linear-size $n$-bit adder circuits with a significantly better depth guarantee compared to previous approaches: Our circuits have a depth of at most $\\log_2 n + \\log_2 \\log_2 n + \\log_2 \\log_2 \\log_2 n + \\text{const}$, improving upon the previously best circuits by [12] with a depth of at most $\\log_2 n + 8 \\sqrt{\\log_2 n} + 6 \\log_2 \\log_2 n + \\text{const}$. Hence, we decrease the gap to the lower bound of $\\log_2 n + \\log_2 \\log_2 n + \\text{const}$ by [5] significantly from $\\mathcal O (\\sqrt{\\log_2 n})$ to $\\mathcal O(\\log_2 \\log_2 \\log_2 n)$.   Our core routine is a new algorithm for the construction of a circuit for a single carry bit, or, more generally, for an And-Or path, i.e., a Boolean function of type $t_0 \\lor ( t_1 \\land (t_2 \\lor ( \\dots t_{m-1}) \\dots ))","$.","We compute linear-size And-Or path circuits with a depth of at most $\\log_2 m + \\log_2 \\log_2 m + 0.65$ in time $\\mathcal O(m \\log_2 m)$.","These are the first And-Or path circuits known that, up to an additive constant, match the lower bound by [5] and at the same time have a linear size.","The previously fastest And-Or path circuits are only by an additive constant worse in depth, but have a much higher size in the order of $\\mathcal O (m \\log_2 m)$."],"url":"http://arxiv.org/abs/2405.12765v1","category":"cs.DS"}
{"created":"2024-05-21 13:17:53","title":"Asymptotic vanishing of cohomology in triangulated categories","abstract":"Given a graded-commutative ring acting centrally on a triangulated category, our main result shows that if cohomology of a pair of objects of the triangulated category is finitely generated over the ring acting centrally, then the asymptotic vanishing of the cohomology is well-behaved. In particular, enough consecutive asymptotic vanishing of cohomology implies all eventual vanishing. Several key applications are also given.","sentences":["Given a graded-commutative ring acting centrally on a triangulated category, our main result shows that if cohomology of a pair of objects of the triangulated category is finitely generated over the ring acting centrally, then the asymptotic vanishing of the cohomology is well-behaved.","In particular, enough consecutive asymptotic vanishing of cohomology implies all eventual vanishing.","Several key applications are also given."],"url":"http://arxiv.org/abs/2405.12763v1","category":"math.KT"}
{"created":"2024-05-21 13:15:19","title":"Clarabel: An interior-point solver for conic programs with quadratic objectives","abstract":"We present a general-purpose interior-point solver for convex optimization problems with conic constraints. Our method is based on a homogeneous embedding method originally developed for general monotone complementarity problems and more recently applied to operator splitting methods, and here specialized to an interior-point method for problems with quadratic objectives. We allow for a variety of standard symmetric and non-symmetric cones, and provide support for chordal decomposition methods in the case of semidefinite cones. We describe the implementation of this method in the open-source solver Clarabel, and provide a detailed numerical evaluation of its performance versus several state-of-the-art solvers on a wide range of standard benchmarks problems. Clarabel is faster and more robust than competing commercial and open-source solvers across a range of test sets, with a particularly large performance advantage for problems with quadratic objectives. Clarabel is currently distributed as a standard solver for the Python CVXPY optimization suite.","sentences":["We present a general-purpose interior-point solver for convex optimization problems with conic constraints.","Our method is based on a homogeneous embedding method originally developed for general monotone complementarity problems and more recently applied to operator splitting methods, and here specialized to an interior-point method for problems with quadratic objectives.","We allow for a variety of standard symmetric and non-symmetric cones, and provide support for chordal decomposition methods in the case of semidefinite cones.","We describe the implementation of this method in the open-source solver Clarabel, and provide a detailed numerical evaluation of its performance versus several state-of-the-art solvers on a wide range of standard benchmarks problems.","Clarabel is faster and more robust than competing commercial and open-source solvers across a range of test sets, with a particularly large performance advantage for problems with quadratic objectives.","Clarabel is currently distributed as a standard solver for the Python CVXPY optimization suite."],"url":"http://arxiv.org/abs/2405.12762v1","category":"math.OC"}
{"created":"2024-05-21 13:12:30","title":"Generalized Strauss conjecture for semilinear wave equations on $\\mathbb{R}^3$","abstract":"In this manuscript, we focus on the more delicate nonlinearity of the semilinear wave equation $$\\partial_{t}^2 u-\\Delta_{\\mathbb{R}^3}u=|u|^{p_S}\\mu(|u|)\\ ,u(0,x)=\\varepsilon u_0,\\ u_t(0,x)=\\varepsilon u_1\\ ,$$ where $p_S=1+\\sqrt{2}$ is the Strauss critical index in $n=3$, and $\\mu$ is a modulus of continuity. Inspired by Chen, Reissig\\cite{Chen_2024} and Ebert, Girardi, Reissig\\cite{MR4163528}, we investigate the sharp condition of $\\mu$ as the threshold between the global existence and blow up with small data. We obtain the almost sharp results in this paper, which in particular disproves the conjecture in \\cite{Chen_2024}.","sentences":["In this manuscript, we focus on the more delicate nonlinearity of the semilinear wave equation $$\\partial_{t}^2 u-\\Delta_{\\mathbb{R}^3}u=|u|^{p_S}\\mu(|u|)\\ ,u(0,x)=\\varepsilon u_0,\\ u_t(0,x)=\\varepsilon u_1\\ ,$$ where $p_S=1+\\sqrt{2}$ is the Strauss critical index in $n=3$, and $\\mu$ is a modulus of continuity.","Inspired by Chen, Reissig\\cite{Chen_2024} and Ebert, Girardi, Reissig\\cite{MR4163528}, we investigate the sharp condition of $\\mu$ as the threshold between the global existence and blow up with small data.","We obtain the almost sharp results in this paper, which in particular disproves the conjecture in \\cite{Chen_2024}."],"url":"http://arxiv.org/abs/2405.12761v1","category":"math.AP"}
{"created":"2024-05-21 13:06:41","title":"Progress Measures for Grokking on Real-world Datasets","abstract":"Grokking, a phenomenon where machine learning models generalize long after overfitting, has been primarily observed and studied in algorithmic tasks. This paper explores grokking in real-world datasets using deep neural networks for classification under the cross-entropy loss. We challenge the prevalent hypothesis that the $L_2$ norm of weights is the primary cause of grokking by demonstrating that grokking can occur outside the expected range of weight norms. To better understand grokking, we introduce three new progress measures: activation sparsity, absolute weight entropy, and approximate local circuit complexity. These measures are conceptually related to generalization and demonstrate a stronger correlation with grokking in real-world datasets compared to weight norms. Our findings suggest that while weight norms might usually correlate with grokking and our progress measures, they are not causative, and our proposed measures provide a better understanding of the dynamics of grokking.","sentences":["Grokking, a phenomenon where machine learning models generalize long after overfitting, has been primarily observed and studied in algorithmic tasks.","This paper explores grokking in real-world datasets using deep neural networks for classification under the cross-entropy loss.","We challenge the prevalent hypothesis that the $L_2$ norm of weights is the primary cause of grokking by demonstrating that grokking can occur outside the expected range of weight norms.","To better understand grokking, we introduce three new progress measures: activation sparsity, absolute weight entropy, and approximate local circuit complexity.","These measures are conceptually related to generalization and demonstrate a stronger correlation with grokking in real-world datasets compared to weight norms.","Our findings suggest that while weight norms might usually correlate with grokking and our progress measures, they are not causative, and our proposed measures provide a better understanding of the dynamics of grokking."],"url":"http://arxiv.org/abs/2405.12755v1","category":"cs.LG"}
{"created":"2024-05-21 13:04:53","title":"Neural Operator for Accelerating Coronal Magnetic Field Model","abstract":"Studying the sun's outer atmosphere is challenging due to its complex magnetic fields impacting solar activities. Magnetohydrodynamics (MHD) simulations help model these interactions but are extremely time-consuming (usually on a scale of days). Our research applies the Fourier Neural Operator (FNO) to accelerate the coronal magnetic field modeling, specifically, the Bifrost MHD model. We apply Tensorized FNO (TFNO) to generate solutions from partial differential equations (PDEs) over a 3D domain efficiently. TFNO's performance is compared with other deep learning methods, highlighting its accuracy and scalability. Physics analysis confirms that TFNO is reliable and capable of accelerating MHD simulations with high precision. This advancement improves efficiency in data handling, enhances predictive capabilities, and provides a better understanding of magnetic topologies.","sentences":["Studying the sun's outer atmosphere is challenging due to its complex magnetic fields impacting solar activities.","Magnetohydrodynamics (MHD) simulations help model these interactions but are extremely time-consuming (usually on a scale of days).","Our research applies the Fourier Neural Operator (FNO) to accelerate the coronal magnetic field modeling, specifically, the Bifrost MHD model.","We apply Tensorized FNO (TFNO) to generate solutions from partial differential equations (PDEs) over a 3D domain efficiently.","TFNO's performance is compared with other deep learning methods, highlighting its accuracy and scalability.","Physics analysis confirms that TFNO is reliable and capable of accelerating MHD simulations with high precision.","This advancement improves efficiency in data handling, enhances predictive capabilities, and provides a better understanding of magnetic topologies."],"url":"http://arxiv.org/abs/2405.12754v1","category":"astro-ph.SR"}
{"created":"2024-05-21 13:04:52","title":"The Laplace and Leray transforms on some (weakly) convex domains in $\\mathbb{C}^2$","abstract":"The space of Laplace transforms of holomorphic Hardy-space functions have been characterized as weighted Bergman spaces of entire functions in two cases: that of planar convex domains (Lutsenko--Yumulmukhametov, 1991), and that of strongly convex domains in higher dimensions (Lindholm, 2002). In this paper, we establish such a Paley--Weiner result for a class of (weakly) convex Reinhardt domains in $\\mathbb{C}^2$ that are well-modelled by the so-called egg domains. We consider Hardy spaces on these domains with respect to a canonical choice of boundary Monge--Ampere measure. This class of domains was introduced by Barrett--Lanzani (2009) to study the $L^2$-boundedness of the Leray transform in the absence of either strongly convexity or $\\mathcal{C}^2$-regularity. The boundedness of the Leray transform plays a crucial role in understanding the image of the Laplace transform. As a supplementary result, we expand the known class of convex Reinhardt domains for which the Leray transform is $L^2$-bounded (with respect to the aforementioned choice of boundary measure). Finally, we also produce an example to show that the Lutsenko--Yumulmukhametov result cannot be expected to generalize to all convex domains in higher dimensions.","sentences":["The space of Laplace transforms of holomorphic Hardy-space functions have been characterized as weighted Bergman spaces of entire functions in two cases: that of planar convex domains (Lutsenko--Yumulmukhametov, 1991), and that of strongly convex domains in higher dimensions (Lindholm, 2002).","In this paper, we establish such a Paley--Weiner result for a class of (weakly) convex Reinhardt domains in $\\mathbb{C}^2$ that are well-modelled by the so-called egg domains.","We consider Hardy spaces on these domains with respect to a canonical choice of boundary Monge--Ampere measure.","This class of domains was introduced by Barrett--Lanzani (2009) to study the $L^2$-boundedness of the Leray transform in the absence of either strongly convexity or $\\mathcal{C}^2$-regularity.","The boundedness of the Leray transform plays a crucial role in understanding the image of the Laplace transform.","As a supplementary result, we expand the known class of convex Reinhardt domains for which the Leray transform is $L^2$-bounded (with respect to the aforementioned choice of boundary measure).","Finally, we also produce an example to show that the Lutsenko--Yumulmukhametov result cannot be expected to generalize to all convex domains in higher dimensions."],"url":"http://arxiv.org/abs/2405.12753v1","category":"math.CV"}
{"created":"2024-05-21 13:04:10","title":"C3L: Content Correlated Vision-Language Instruction Tuning Data Generation via Contrastive Learning","abstract":"Vision-Language Instruction Tuning (VLIT) is a critical training phase for Large Vision-Language Models (LVLMs). With the improving capabilities of open-source LVLMs, researchers have increasingly turned to generate VLIT data by using open-source LVLMs and achieved significant progress. However, such data generation approaches are bottlenecked by the following challenges: 1) Since multi-modal models tend to be influenced by prior language knowledge, directly using LVLMs to generate VLIT data would inevitably lead to low content relevance between generated data and images. 2) To improve the ability of the models to generate VLIT data, previous methods have incorporated an additional training phase to boost the generative capacity. This process hurts the generalization of the models to unseen inputs (i.e., \"exposure bias\" problem). In this paper, we propose a new Content Correlated VLIT data generation via Contrastive Learning (C3L). Specifically, we design a new content relevance module which enhances the content relevance between VLIT data and images by computing Image Instruction Correspondence Scores S(I2C). Moreover, a contrastive learning module is introduced to further boost the VLIT data generation capability of the LVLMs. A large number of automatic measures on four benchmarks show the effectiveness of our method.","sentences":["Vision-Language Instruction Tuning (VLIT) is a critical training phase for Large Vision-Language Models (LVLMs).","With the improving capabilities of open-source LVLMs, researchers have increasingly turned to generate VLIT data by using open-source LVLMs and achieved significant progress.","However, such data generation approaches are bottlenecked by the following challenges: 1) Since multi-modal models tend to be influenced by prior language knowledge, directly using LVLMs to generate VLIT data would inevitably lead to low content relevance between generated data and images.","2) To improve the ability of the models to generate VLIT data, previous methods have incorporated an additional training phase to boost the generative capacity.","This process hurts the generalization of the models to unseen inputs (i.e., \"exposure bias\" problem).","In this paper, we propose a new Content Correlated VLIT data generation via Contrastive Learning (C3L).","Specifically, we design a new content relevance module which enhances the content relevance between VLIT data and images by computing Image Instruction Correspondence Scores S(I2C).","Moreover, a contrastive learning module is introduced to further boost the VLIT data generation capability of the LVLMs.","A large number of automatic measures on four benchmarks show the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.12752v1","category":"cs.CV"}
{"created":"2024-05-21 13:02:27","title":"Generative AI and Large Language Models for Cyber Security: All Insights You Need","abstract":"This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.","sentences":["This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs).","We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection.","We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA.","Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions.","We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques.","Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses.","We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research.","In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG).","These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response.","Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats."],"url":"http://arxiv.org/abs/2405.12750v1","category":"cs.CR"}
{"created":"2024-05-21 12:56:50","title":"Phase transitions and departure statistics of critically loaded queues: oscillating cumulants and generalized BRAVO","abstract":"Queueing theory is used for modeling biological processes, traffic flows and many more real-life situations. Beyond that, queues describe systems out of equilibrium and can thus be considered as minimal models of non-equilibrium statistical mechanics. We demonstrate that non-equilibrium phase transitions of queues in the steady state are accompanied by a nontrivial flow of departing customers. Our analytical results show that the cumulants of the departure statistics deviate strongly from Poissonian values and oscillate in the vicinity of phase transitions, i.e., if a critical load is approached. The load-dependent oscillations of the cumulants generalize the BRAVO effect (Balancing Reduces Asymptotic Variance of Outputs) in queues and may occur in other boundary-driven non-equilibrium systems.","sentences":["Queueing theory is used for modeling biological processes, traffic flows and many more real-life situations.","Beyond that, queues describe systems out of equilibrium and can thus be considered as minimal models of non-equilibrium statistical mechanics.","We demonstrate that non-equilibrium phase transitions of queues in the steady state are accompanied by a nontrivial flow of departing customers.","Our analytical results show that the cumulants of the departure statistics deviate strongly from Poissonian values and oscillate in the vicinity of phase transitions, i.e., if a critical load is approached.","The load-dependent oscillations of the cumulants generalize the BRAVO effect (Balancing Reduces Asymptotic Variance of Outputs) in queues and may occur in other boundary-driven non-equilibrium systems."],"url":"http://arxiv.org/abs/2405.12746v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-21 12:55:15","title":"The Echoes of Multilinguality: Tracing Cultural Value Shifts during LM Fine-tuning","abstract":"Texts written in different languages reflect different culturally-dependent beliefs of their writers. Thus, we expect multilingual LMs (MLMs), that are jointly trained on a concatenation of text in multiple languages, to encode different cultural values for each language. Yet, as the 'multilinguality' of these LMs is driven by cross-lingual sharing, we also have reason to belief that cultural values bleed over from one language into another. This limits the use of MLMs in practice, as apart from being proficient in generating text in multiple languages, creating language technology that can serve a community also requires the output of LMs to be sensitive to their biases (Naous et al., 2023). Yet, little is known about how cultural values emerge and evolve in MLMs (Hershcovich et al., 2022a). We are the first to study how languages can exert influence on the cultural values encoded for different test languages, by studying how such values are revised during fine-tuning. Focusing on the fine-tuning stage allows us to study the interplay between value shifts when exposed to new linguistic experience from different data sources and languages. Lastly, we use a training data attribution method to find patterns in the fine-tuning examples, and the languages that they come from, that tend to instigate value shifts.","sentences":["Texts written in different languages reflect different culturally-dependent beliefs of their writers.","Thus, we expect multilingual LMs (MLMs), that are jointly trained on a concatenation of text in multiple languages, to encode different cultural values for each language.","Yet, as the 'multilinguality' of these LMs is driven by cross-lingual sharing, we also have reason to belief that cultural values bleed over from one language into another.","This limits the use of MLMs in practice, as apart from being proficient in generating text in multiple languages, creating language technology that can serve a community also requires the output of LMs to be sensitive to their biases (Naous et al., 2023).","Yet, little is known about how cultural values emerge and evolve in MLMs (Hershcovich et al., 2022a).","We are the first to study how languages can exert influence on the cultural values encoded for different test languages, by studying how such values are revised during fine-tuning.","Focusing on the fine-tuning stage allows us to study the interplay between value shifts when exposed to new linguistic experience from different data sources and languages.","Lastly, we use a training data attribution method to find patterns in the fine-tuning examples, and the languages that they come from, that tend to instigate value shifts."],"url":"http://arxiv.org/abs/2405.12744v1","category":"cs.CL"}
{"created":"2024-05-21 12:55:01","title":"Consumer lying in online reviews: recent evidence","abstract":"The persistence of lying by some consumers in their online posts of experiences with businesses is problematic, and taints the global pool of information that is used for decision making by people that assume they are true accounts of experiences. This study is based on data from my dissertation about fake online Google reviews of restaurants (Berry, 2024), and leverages an instrument that quantifies the trust of people. The findings are based on a sample of n=351, and provide a general proxy for lying in online reviews, and sketch out the characteristics of a typical person that has the propensity to be untruthful. A predictive model of posting untrue online reviews is constructed. The findings have wider implications for the study and monitoring of deceptive behavior, including the propagation of misinformation, and a means of quantifying the potential for antisocial behavior as measured by the trust of people instrument in Berry (2024). Directions for future research and limitations are also discussed.","sentences":["The persistence of lying by some consumers in their online posts of experiences with businesses is problematic, and taints the global pool of information that is used for decision making by people that assume they are true accounts of experiences.","This study is based on data from my dissertation about fake online Google reviews of restaurants (Berry, 2024), and leverages an instrument that quantifies the trust of people.","The findings are based on a sample of n=351, and provide a general proxy for lying in online reviews, and sketch out the characteristics of a typical person that has the propensity to be untruthful.","A predictive model of posting untrue online reviews is constructed.","The findings have wider implications for the study and monitoring of deceptive behavior, including the propagation of misinformation, and a means of quantifying the potential for antisocial behavior as measured by the trust of people instrument in Berry (2024).","Directions for future research and limitations are also discussed."],"url":"http://arxiv.org/abs/2405.12743v1","category":"econ.GN"}
{"created":"2024-05-21 12:53:34","title":"Multi-Subject Personalization","abstract":"Creative story illustration requires a consistent interplay of multiple characters or objects. However, conventional text-to-image models face significant challenges while producing images featuring multiple personalized subjects. For example, they distort the subject rendering, or the text descriptions fail to render coherent subject interactions. We present Multi-Subject Personalization (MSP) to alleviate some of these challenges. We implement MSP using Stable Diffusion and assess our approach against other text-to-image models, showcasing its consistent generation of good-quality images representing intended subjects and interactions.","sentences":["Creative story illustration requires a consistent interplay of multiple characters or objects.","However, conventional text-to-image models face significant challenges while producing images featuring multiple personalized subjects.","For example, they distort the subject rendering, or the text descriptions fail to render coherent subject interactions.","We present Multi-Subject Personalization (MSP) to alleviate some of these challenges.","We implement MSP using Stable Diffusion and assess our approach against other text-to-image models, showcasing its consistent generation of good-quality images representing intended subjects and interactions."],"url":"http://arxiv.org/abs/2405.12742v1","category":"cs.CV"}
{"created":"2024-05-21 12:45:56","title":"The generalized Fuglede's conjecture holds for a class of Cantor-Moran measures","abstract":"Suppose ${\\bf b}=\\{b_n\\}_{n=1}^{\\infty}$ is a sequence of integers bigger than 1 and ${\\bf D}=\\{{\\mathcal D}_{n}\\}_{n=1}^{\\infty}$ is a sequence of consecutive digit sets. Let $\\mu_{{\\bf b},{\\bf D}}$ be the Cantor-Moran measure defined by \\begin{eqnarray*}   \\mu_{{\\bf b},{\\bf D}}&=& \\delta_{\\frac{1}{b_1}{\\mathcal D}_{1}}\\ast\\delta_{\\frac{1}{b_1b_2}{\\mathcal D}_{2}}\\ast \\delta_{\\frac{1}{b_1b_2b_3}{\\mathcal D}_{3}}\\ast\\cdots.   \\end{eqnarray*}   We prove that $L^2(\\mu_{{\\bf b},{\\bf D}})$ possesses an exponential orthonormal basis if and only if $\\mu_{{\\bf b},{\\bf D}}\\ast\\nu={\\mathcal L}_{[0,N_1/b_1]}$ for some Borel probability measure $\\nu$.   This theorem shows that the generalized Fuglede's conjecture is true for such Cantor-Moran measure. An immediate consequence of this result is the equivalence between the existence of an exponential orthonormal basis and the integral tiling of ${\\bf D}_n={\\mathcal D}_{n}+b_n{\\mathcal D}_{n-1}+b_2\\cdots b_n{\\mathcal D}_{1}$ for $n\\geq1$.","sentences":["Suppose ${\\bf b}=\\{b_n\\}_{n=1}^{\\infty}$ is a sequence of integers bigger than 1 and ${\\bf D}=\\{{\\mathcal D}_{n}\\}_{n=1}^{\\infty}$ is a sequence of consecutive digit sets.","Let $\\mu_{{\\bf b},{\\bf D}}$ be the Cantor-Moran measure defined by \\begin{eqnarray*}   \\mu_{{\\bf b},{\\bf D}}&=& \\delta_{\\frac{1}{b_1}{\\mathcal D}_{1}}\\ast\\delta_{\\frac{1}{b_1b_2}{\\mathcal D}_{2}}\\ast \\delta_{\\frac{1}{b_1b_2b_3}{\\mathcal D}_{3}}\\ast\\cdots.   ","\\end{eqnarray*}   We prove that $L^2(\\mu_{{\\bf b},{\\bf D}})$ possesses an exponential orthonormal basis if and only if $\\mu_{{\\bf b},{\\bf D}}\\ast\\nu={\\mathcal L}_{[0,N_1/b_1]}$ for some Borel probability measure $\\nu$.   This theorem shows that the generalized Fuglede's conjecture is true for such Cantor-Moran measure.","An immediate consequence of this result is the equivalence between the existence of an exponential orthonormal basis and the integral tiling of ${\\bf D}_n={\\mathcal D}_{n}+b_n{\\mathcal D}_{n-1}+b_2\\cdots","b_n{\\mathcal D}_{1}$ for $n\\geq1$."],"url":"http://arxiv.org/abs/2405.12738v1","category":"math.FA"}
{"created":"2024-05-21 12:44:56","title":"Hyperuniformity of random measures on Euclidean and hyperbolic spaces","abstract":"We investigate lower asymptotic bounds of number variances for invariant locally square-integrable random measures on Euclidean and real hyperbolic spaces. In the Euclidean case we show that there are subsequences of radii for which the number variance grows at least as fast as the volume of the boundary of Euclidean balls, generalizing a classical result of Beck. With regards to real hyperbolic spaces we prove that random measures are never geometrically hyperuniform and if the random measure admits non-trivial complementary series diffraction, then it is hyperfluctuating. Moreover, we define spectral hyperuniformity and stealth of random measures on real hyperbolic spaces in terms of vanishing of the complementary series diffraction and sub-Poissonian decay of the principal series diffraction around the Harish-Chandra $\\Xi$-function.","sentences":["We investigate lower asymptotic bounds of number variances for invariant locally square-integrable random measures on Euclidean and real hyperbolic spaces.","In the Euclidean case we show that there are subsequences of radii for which the number variance grows at least as fast as the volume of the boundary of Euclidean balls, generalizing a classical result of Beck.","With regards to real hyperbolic spaces we prove that random measures are never geometrically hyperuniform and if the random measure admits non-trivial complementary series diffraction, then it is hyperfluctuating.","Moreover, we define spectral hyperuniformity and stealth of random measures on real hyperbolic spaces in terms of vanishing of the complementary series diffraction and sub-Poissonian decay of the principal series diffraction around the Harish-Chandra $\\Xi$-function."],"url":"http://arxiv.org/abs/2405.12737v1","category":"math.PR"}
{"created":"2024-05-21 12:44:43","title":"Predicting the Influence of Adverse Weather on Pedestrian Detection with Automotive Radar and Lidar Sensors","abstract":"Pedestrians are among the most endangered traffic participants in road traffic. While pedestrian detection in nominal conditions is well established, the sensor and, therefore, the pedestrian detection performance degrades under adverse weather conditions. Understanding the influences of rain and fog on a specific radar and lidar sensor requires extensive testing, and if the sensors' specifications are altered, a retesting effort is required. These challenges are addressed in this paper, firstly by conducting comprehensive measurements collecting empirical data of pedestrian detection performance under varying rain and fog intensities in a controlled environment, and secondly, by introducing a dedicated \\textit{Weather Filter} (WF) model that predicts the effects of rain and fog on a user-specified radar and lidar on pedestrian detection performance. We use a state-of-the-art baseline model representing the physical relation of sensor specifications, which, however, lacks the representation of secondary weather effects, e.g., changes in pedestrian reflectivity or droplets on a sensor, and adjust it with empirical data to account for such. We find that our measurement results are in agreement with existent literature related to weather degredation and our WF outperforms the baseline model in predicting weather effects on pedestrian detection while only requiring a minimal testing effort.","sentences":["Pedestrians are among the most endangered traffic participants in road traffic.","While pedestrian detection in nominal conditions is well established, the sensor and, therefore, the pedestrian detection performance degrades under adverse weather conditions.","Understanding the influences of rain and fog on a specific radar and lidar sensor requires extensive testing, and if the sensors' specifications are altered, a retesting effort is required.","These challenges are addressed in this paper, firstly by conducting comprehensive measurements collecting empirical data of pedestrian detection performance under varying rain and fog intensities in a controlled environment, and secondly, by introducing a dedicated \\textit{Weather Filter} (WF) model that predicts the effects of rain and fog on a user-specified radar and lidar on pedestrian detection performance.","We use a state-of-the-art baseline model representing the physical relation of sensor specifications, which, however, lacks the representation of secondary weather effects, e.g., changes in pedestrian reflectivity or droplets on a sensor, and adjust it with empirical data to account for such.","We find that our measurement results are in agreement with existent literature related to weather degredation and our WF outperforms the baseline model in predicting weather effects on pedestrian detection while only requiring a minimal testing effort."],"url":"http://arxiv.org/abs/2405.12736v1","category":"cs.CV"}
{"created":"2024-05-21 12:37:36","title":"From Today's Code to Tomorrow's Symphony: The AI Transformation of Developer's Routine by 2030","abstract":"In the rapidly evolving landscape of software engineering, the integration of Artificial Intelligence (AI) into the Software Development Life-Cycle (SDLC) heralds a transformative era for developers. Recently, we have assisted to a pivotal shift towards AI-assisted programming, exemplified by tools like GitHub Copilot and OpenAI's ChatGPT, which have become a crucial element for coding, debugging, and software design. In this paper we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers' roles from manual coders to orchestrators of AI-driven development ecosystems. We envision HyperAssistant, an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development. We emphasize AI as a complementary force, augmenting developers' capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions. Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security and creativity.","sentences":["In the rapidly evolving landscape of software engineering, the integration of Artificial Intelligence (AI) into the Software Development Life-Cycle (SDLC) heralds a transformative era for developers.","Recently, we have assisted to a pivotal shift towards AI-assisted programming, exemplified by tools like GitHub Copilot and OpenAI's ChatGPT, which have become a crucial element for coding, debugging, and software design.","In this paper we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers' roles from manual coders to orchestrators of AI-driven development ecosystems.","We envision HyperAssistant, an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development.","We emphasize AI as a complementary force, augmenting developers' capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions.","Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security and creativity."],"url":"http://arxiv.org/abs/2405.12731v1","category":"cs.SE"}
{"created":"2024-05-21 12:36:50","title":"Conditions for tractability of the weighted $L_p$-discrepancy and integration in non-homogeneous tensor product spaces","abstract":"We study tractability properties of the weighted $L_p$-discrepancy. The concept of {\\it weighted} discrepancy was introduced by Sloan and Wo\\'{z}\\-nia\\-kowski in 1998 in order to prove a weighted version of the Koksma-Hlawka inequality for the error of quasi-Monte Carlo integration rules. The weights have the aim to model the influence of different coordinates of integrands on the error. A discrepancy is said to be tractable if the information complexity, i.e., the minimal number $N$ of points such that the discrepancy is less than the initial discrepancy times an error threshold $\\varepsilon$, does not grow exponentially fast with the dimension. In this case there are various notions of tractabilities used in order to classify the exact rate.   For even integer parameters $p$ there are sufficient conditions on the weights available in literature, which guarantee the one or other notion of tractability. In the present paper we prove matching sufficient conditions (upper bounds) and neccessary conditions (lower bounds) for polynomial and weak tractability for all $p \\in (1, \\infty)$.   The proofs of the lower bounds are based on a general result for the information complexity of integration with positive quadrature formulas for tensor product spaces. In order to demonstrate this lower bound we consider as a second application the integration of tensor products of polynomials of degree at most 2.","sentences":["We study tractability properties of the weighted $L_p$-discrepancy.","The concept of {\\it weighted} discrepancy was introduced by Sloan and Wo\\'{z}\\-nia\\-kowski in 1998 in order to prove a weighted version of the Koksma-Hlawka inequality for the error of quasi-Monte Carlo integration rules.","The weights have the aim to model the influence of different coordinates of integrands on the error.","A discrepancy is said to be tractable if the information complexity, i.e., the minimal number $N$ of points such that the discrepancy is less than the initial discrepancy times an error threshold $\\varepsilon$, does not grow exponentially fast with the dimension.","In this case there are various notions of tractabilities used in order to classify the exact rate.   ","For even integer parameters $p$ there are sufficient conditions on the weights available in literature, which guarantee the one or other notion of tractability.","In the present paper we prove matching sufficient conditions (upper bounds) and neccessary conditions (lower bounds) for polynomial and weak tractability for all $p \\in (1, \\infty)$.   The proofs of the lower bounds are based on a general result for the information complexity of integration with positive quadrature formulas for tensor product spaces.","In order to demonstrate this lower bound we consider as a second application the integration of tensor products of polynomials of degree at most 2."],"url":"http://arxiv.org/abs/2405.12729v1","category":"math.NA"}
{"created":"2024-05-21 12:34:03","title":"Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations","abstract":"We address the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera, a key step towards the autonomous rendezvous and proximity operations required by future Active Debris Removal missions. We present a novel method that enables an \"off-the-shelf\" spacecraft pose estimator, which is supposed to known the target CAD model, to be applied on an unknown target. Our method relies on an in-the wild NeRF, i.e., a Neural Radiance Field that employs learnable appearance embeddings to represent varying illumination conditions found in natural scenes. We train the NeRF model using a sparse collection of images that depict the target, and in turn generate a large dataset that is diverse both in terms of viewpoint and illumination. This dataset is then used to train the pose estimation network. We validate our method on the Hardware-In-the-Loop images of SPEED+ that emulate lighting conditions close to those encountered on orbit. We demonstrate that our method successfully enables the training of an off-the-shelf spacecraft pose estimation network from a sparse set of images. Furthermore, we show that a network trained using our method performs similarly to a model trained on synthetic images generated using the CAD model of the target.","sentences":["We address the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera, a key step towards the autonomous rendezvous and proximity operations required by future Active Debris Removal missions.","We present a novel method that enables an \"off-the-shelf\" spacecraft pose estimator, which is supposed to known the target CAD model, to be applied on an unknown target.","Our method relies on an in-the wild NeRF, i.e., a Neural Radiance Field that employs learnable appearance embeddings to represent varying illumination conditions found in natural scenes.","We train the NeRF model using a sparse collection of images that depict the target, and in turn generate a large dataset that is diverse both in terms of viewpoint and illumination.","This dataset is then used to train the pose estimation network.","We validate our method on the Hardware-In-the-Loop images of SPEED+ that emulate lighting conditions close to those encountered on orbit.","We demonstrate that our method successfully enables the training of an off-the-shelf spacecraft pose estimation network from a sparse set of images.","Furthermore, we show that a network trained using our method performs similarly to a model trained on synthetic images generated using the CAD model of the target."],"url":"http://arxiv.org/abs/2405.12728v1","category":"cs.CV"}
{"created":"2024-05-21 12:21:45","title":"StarLKNet: Star Mixup with Large Kernel Networks for Palm Vein Identification","abstract":"As a representative of a new generation of biometrics, vein identification technology offers a high level of security and convenience. Convolutional neural networks (CNNs), a prominent class of deep learning architectures, have been extensively utilized for vein identification. Since their performance and robustness are limited by small Effective Receptive Fields (e.g. 3$\\times$3 kernels) and insufficient training samples, however, they are unable to extract global feature representations from vein images in an effective manner. To address these issues, we propose StarLKNet, a large kernel convolution-based palm-vein identification network, with the Mixup approach. Our StarMix learns effectively the distribution of vein features to expand samples. To enable CNNs to capture comprehensive feature representations from palm-vein images, we explored the effect of convolutional kernel size on the performance of palm-vein identification networks and designed LaKNet, a network leveraging large kernel convolution and gating mechanism. In light of the current state of knowledge, this represents an inaugural instance of the deployment of a CNN with large kernels in the domain of vein identification. Extensive experiments were conducted to validate the performance of StarLKNet on two public palm-vein datasets. The results demonstrated that StarMix provided superior augmentation, and LakNet exhibited more stable performance gains compared to mainstream approaches, resulting in the highest recognition accuracy and lowest identification error.","sentences":["As a representative of a new generation of biometrics, vein identification technology offers a high level of security and convenience.","Convolutional neural networks (CNNs), a prominent class of deep learning architectures, have been extensively utilized for vein identification.","Since their performance and robustness are limited by small Effective Receptive Fields (e.g. 3$\\times$3 kernels) and insufficient training samples, however, they are unable to extract global feature representations from vein images in an effective manner.","To address these issues, we propose StarLKNet, a large kernel convolution-based palm-vein identification network, with the Mixup approach.","Our StarMix learns effectively the distribution of vein features to expand samples.","To enable CNNs to capture comprehensive feature representations from palm-vein images, we explored the effect of convolutional kernel size on the performance of palm-vein identification networks and designed LaKNet, a network leveraging large kernel convolution and gating mechanism.","In light of the current state of knowledge, this represents an inaugural instance of the deployment of a CNN with large kernels in the domain of vein identification.","Extensive experiments were conducted to validate the performance of StarLKNet on two public palm-vein datasets.","The results demonstrated that StarMix provided superior augmentation, and LakNet exhibited more stable performance gains compared to mainstream approaches, resulting in the highest recognition accuracy and lowest identification error."],"url":"http://arxiv.org/abs/2405.12721v1","category":"cs.CV"}
{"created":"2024-05-21 12:19:34","title":"A note on the Thom morphism for the classifying space of certain Lie groups and gauge groups","abstract":"We give a complete description of which non-torsion generators are not in the image of the Thom morphism from complex cobordism to integral cohomology for the classifying space of exceptional Lie groups except for E_8. We then show that the Thom morphism is not surjective for the classifying space of the gauge group of a principal E_7-bundle over the four-dimensional sphere. We use the results to detect nontrivial elements in the kernel of the reduced Thom morphism for Lie groups and their classifying spaces.","sentences":["We give a complete description of which non-torsion generators are not in the image of the Thom morphism from complex cobordism to integral cohomology for the classifying space of exceptional Lie groups except for E_8.","We then show that the Thom morphism is not surjective for the classifying space of the gauge group of a principal E_7-bundle over the four-dimensional sphere.","We use the results to detect nontrivial elements in the kernel of the reduced Thom morphism for Lie groups and their classifying spaces."],"url":"http://arxiv.org/abs/2405.12717v1","category":"math.AT"}
{"created":"2024-05-21 12:19:17","title":"Reinforcement Learning Enabled Peer-to-Peer Energy Trading for Dairy Farms","abstract":"Farm businesses are increasingly adopting renewables to enhance energy efficiency and reduce reliance on fossil fuels and the grid. This shift aims to decrease dairy farms' dependence on traditional electricity grids by enabling the sale of surplus renewable energy in Peer-to-Peer markets. However, the dynamic nature of farm communities poses challenges, requiring specialized algorithms for P2P energy trading. To address this, the Multi-Agent Peer-to-Peer Dairy Farm Energy Simulator (MAPDES) has been developed, providing a platform to experiment with Reinforcement Learning techniques. The simulations demonstrate significant cost savings, including a 43% reduction in electricity expenses, a 42% decrease in peak demand, and a 1.91% increase in energy sales compared to baseline scenarios lacking peer-to-peer energy trading or renewable energy sources.","sentences":["Farm businesses are increasingly adopting renewables to enhance energy efficiency and reduce reliance on fossil fuels and the grid.","This shift aims to decrease dairy farms' dependence on traditional electricity grids by enabling the sale of surplus renewable energy in Peer-to-Peer markets.","However, the dynamic nature of farm communities poses challenges, requiring specialized algorithms for P2P energy trading.","To address this, the Multi-Agent Peer-to-Peer Dairy Farm Energy Simulator (MAPDES) has been developed, providing a platform to experiment with Reinforcement Learning techniques.","The simulations demonstrate significant cost savings, including a 43% reduction in electricity expenses, a 42% decrease in peak demand, and a 1.91% increase in energy sales compared to baseline scenarios lacking peer-to-peer energy trading or renewable energy sources."],"url":"http://arxiv.org/abs/2405.12716v1","category":"cs.AI"}
{"created":"2024-05-21 12:16:20","title":"RecGPT: Generative Pre-training for Text-based Recommendation","abstract":"We present the first domain-adapted and fully-trained large language model, RecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for text-based recommendation. Experimental results on rating prediction and sequential recommendation tasks show that our model, RecGPT-7B-Instruct, outperforms previous strong baselines. We are releasing our RecGPT models as well as their pre-training and fine-tuning datasets to facilitate future research and downstream applications in text-based recommendation. Public \"huggingface\" links to our RecGPT models and datasets are available at: https://github.com/VinAIResearch/RecGPT","sentences":["We present the first domain-adapted and fully-trained large language model, RecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for text-based recommendation.","Experimental results on rating prediction and sequential recommendation tasks show that our model, RecGPT-7B-Instruct, outperforms previous strong baselines.","We are releasing our RecGPT models as well as their pre-training and fine-tuning datasets to facilitate future research and downstream applications in text-based recommendation.","Public \"huggingface\" links to our RecGPT models and datasets are available at: https://github.com/VinAIResearch/RecGPT"],"url":"http://arxiv.org/abs/2405.12715v1","category":"cs.IR"}
{"created":"2024-05-21 12:04:56","title":"Dynamic Identity-Guided Attention Network for Visible-Infrared Person Re-identification","abstract":"Visible-infrared person re-identification (VI-ReID) aims to match people with the same identity between visible and infrared modalities. VI-ReID is a challenging task due to the large differences in individual appearance under different modalities. Existing methods generally try to bridge the cross-modal differences at image or feature level, which lacks exploring the discriminative embeddings. Effectively minimizing these cross-modal discrepancies relies on obtaining representations that are guided by identity and consistent across modalities, while also filtering out representations that are irrelevant to identity. To address these challenges, we introduce a dynamic identity-guided attention network (DIAN) to mine identity-guided and modality-consistent embeddings, facilitating effective bridging the gap between different modalities. Specifically, in DIAN, to pursue a semantically richer representation, we first use orthogonal projection to fuse the features from two connected coarse and fine layers. Furthermore, we first use dynamic convolution kernels to mine identity-guided and modality-consistent representations. More notably, a cross embedding balancing loss is introduced to effectively bridge cross-modal discrepancies by above embeddings. Experimental results on SYSU-MM01 and RegDB datasets show that DIAN achieves state-of-the-art performance. Specifically, for indoor search on SYSU-MM01, our method achieves 86.28% rank-1 accuracy and 87.41% mAP, respectively. Our code will be available soon.","sentences":["Visible-infrared person re-identification (VI-ReID) aims to match people with the same identity between visible and infrared modalities.","VI-ReID is a challenging task due to the large differences in individual appearance under different modalities.","Existing methods generally try to bridge the cross-modal differences at image or feature level, which lacks exploring the discriminative embeddings.","Effectively minimizing these cross-modal discrepancies relies on obtaining representations that are guided by identity and consistent across modalities, while also filtering out representations that are irrelevant to identity.","To address these challenges, we introduce a dynamic identity-guided attention network (DIAN) to mine identity-guided and modality-consistent embeddings, facilitating effective bridging the gap between different modalities.","Specifically, in DIAN, to pursue a semantically richer representation, we first use orthogonal projection to fuse the features from two connected coarse and fine layers.","Furthermore, we first use dynamic convolution kernels to mine identity-guided and modality-consistent representations.","More notably, a cross embedding balancing loss is introduced to effectively bridge cross-modal discrepancies by above embeddings.","Experimental results on SYSU-MM01 and RegDB datasets show that DIAN achieves state-of-the-art performance.","Specifically, for indoor search on SYSU-MM01, our method achieves 86.28% rank-1 accuracy and 87.41% mAP, respectively.","Our code will be available soon."],"url":"http://arxiv.org/abs/2405.12713v1","category":"cs.CV"}
{"created":"2024-05-21 12:04:55","title":"From Human-to-Human to Human-to-Bot Conversations in Software Engineering","abstract":"Software developers use natural language to interact not only with other humans, but increasingly also with chatbots. These interactions have different properties and flow differently based on what goal the developer wants to achieve and who they interact with. In this paper, we aim to understand the dynamics of conversations that occur during modern software development after the integration of AI and chatbots, enabling a deeper recognition of the advantages and disadvantages of including chatbot interactions in addition to human conversations in collaborative work. We compile existing conversation attributes with humans and NLU-based chatbots and adapt them to the context of software development. Then, we extend the comparison to include LLM-powered chatbots based on an observational study. We present similarities and differences between human-to-human and human-to-bot conversations, also distinguishing between NLU- and LLM-based chatbots. Furthermore, we discuss how understanding the differences among the conversation styles guides the developer on how to shape their expectations from a conversation and consequently support the communication within a software team. We conclude that the recent conversation styles that we observe with LLM-chatbots can not replace conversations with humans due to certain attributes regarding social aspects despite their ability to support productivity and decrease the developers' mental load.","sentences":["Software developers use natural language to interact not only with other humans, but increasingly also with chatbots.","These interactions have different properties and flow differently based on what goal the developer wants to achieve and who they interact with.","In this paper, we aim to understand the dynamics of conversations that occur during modern software development after the integration of AI and chatbots, enabling a deeper recognition of the advantages and disadvantages of including chatbot interactions in addition to human conversations in collaborative work.","We compile existing conversation attributes with humans and NLU-based chatbots and adapt them to the context of software development.","Then, we extend the comparison to include LLM-powered chatbots based on an observational study.","We present similarities and differences between human-to-human and human-to-bot conversations, also distinguishing between NLU- and LLM-based chatbots.","Furthermore, we discuss how understanding the differences among the conversation styles guides the developer on how to shape their expectations from a conversation and consequently support the communication within a software team.","We conclude that the recent conversation styles that we observe with LLM-chatbots can not replace conversations with humans due to certain attributes regarding social aspects despite their ability to support productivity and decrease the developers' mental load."],"url":"http://arxiv.org/abs/2405.12712v1","category":"cs.SE"}
{"created":"2024-05-21 12:00:01","title":"A Masked Semi-Supervised Learning Approach for Otago Micro Labels Recognition","abstract":"The Otago Exercise Program (OEP) serves as a vital rehabilitation initiative for older adults, aiming to enhance their strength and balance, and consequently prevent falls. While Human Activity Recognition (HAR) systems have been widely employed in recognizing the activities of individuals, existing systems focus on the duration of macro activities (i.e. a sequence of repetitions of the same exercise), neglecting the ability to discern micro activities (i.e. the individual repetitions of the exercises), in the case of OEP. This study presents a novel semi-supervised machine learning approach aimed at bridging this gap in recognizing the micro activities of OEP. To manage the limited dataset size, our model utilizes a Transformer encoder for feature extraction, subsequently classified by a Temporal Convolutional Network (TCN). Simultaneously, the Transformer encoder is employed for masked unsupervised learning to reconstruct input signals. Results indicate that the masked unsupervised learning task enhances the performance of the supervised learning (classification task), as evidenced by f1-scores surpassing the clinically applicable threshold of 0.8. From the micro activities, two clinically relevant outcomes emerge: counting the number of repetitions of each exercise and calculating the velocity during chair rising. These outcomes enable the automatic monitoring of exercise intensity and difficulty in the daily lives of older adults.","sentences":["The Otago Exercise Program (OEP) serves as a vital rehabilitation initiative for older adults, aiming to enhance their strength and balance, and consequently prevent falls.","While Human Activity Recognition (HAR) systems have been widely employed in recognizing the activities of individuals, existing systems focus on the duration of macro activities (i.e. a sequence of repetitions of the same exercise), neglecting the ability to discern micro activities (i.e. the individual repetitions of the exercises), in the case of OEP.","This study presents a novel semi-supervised machine learning approach aimed at bridging this gap in recognizing the micro activities of OEP.","To manage the limited dataset size, our model utilizes a Transformer encoder for feature extraction, subsequently classified by a Temporal Convolutional Network (TCN).","Simultaneously, the Transformer encoder is employed for masked unsupervised learning to reconstruct input signals.","Results indicate that the masked unsupervised learning task enhances the performance of the supervised learning (classification task), as evidenced by f1-scores surpassing the clinically applicable threshold of 0.8.","From the micro activities, two clinically relevant outcomes emerge: counting the number of repetitions of each exercise and calculating the velocity during chair rising.","These outcomes enable the automatic monitoring of exercise intensity and difficulty in the daily lives of older adults."],"url":"http://arxiv.org/abs/2405.12711v1","category":"cs.LG"}
{"created":"2024-05-21 11:59:36","title":"Text-Video Retrieval with Global-Local Semantic Consistent Learning","abstract":"Adapting large-scale image-text pre-training models, e.g., CLIP, to the video domain represents the current state-of-the-art for text-video retrieval. The primary approaches involve transferring text-video pairs to a common embedding space and leveraging cross-modal interactions on specific entities for semantic alignment. Though effective, these paradigms entail prohibitive computational costs, leading to inefficient retrieval. To address this, we propose a simple yet effective method, Global-Local Semantic Consistent Learning (GLSCL), which capitalizes on latent shared semantics across modalities for text-video retrieval. Specifically, we introduce a parameter-free global interaction module to explore coarse-grained alignment. Then, we devise a shared local interaction module that employs several learnable queries to capture latent semantic concepts for learning fine-grained alignment. Furthermore, an Inter-Consistency Loss (ICL) is devised to accomplish the concept alignment between the visual query and corresponding textual query, and an Intra-Diversity Loss (IDL) is developed to repulse the distribution within visual (textual) queries to generate more discriminative concepts. Extensive experiments on five widely used benchmarks (i.e., MSR-VTT, MSVD, DiDeMo, LSMDC, and ActivityNet) substantiate the superior effectiveness and efficiency of the proposed method. Remarkably, our method achieves comparable performance with SOTA as well as being nearly 220 times faster in terms of computational cost. Code is available at: https://github.com/zchoi/GLSCL.","sentences":["Adapting large-scale image-text pre-training models, e.g., CLIP, to the video domain represents the current state-of-the-art for text-video retrieval.","The primary approaches involve transferring text-video pairs to a common embedding space and leveraging cross-modal interactions on specific entities for semantic alignment.","Though effective, these paradigms entail prohibitive computational costs, leading to inefficient retrieval.","To address this, we propose a simple yet effective method, Global-Local Semantic Consistent Learning (GLSCL), which capitalizes on latent shared semantics across modalities for text-video retrieval.","Specifically, we introduce a parameter-free global interaction module to explore coarse-grained alignment.","Then, we devise a shared local interaction module that employs several learnable queries to capture latent semantic concepts for learning fine-grained alignment.","Furthermore, an Inter-Consistency Loss (ICL) is devised to accomplish the concept alignment between the visual query and corresponding textual query, and an Intra-Diversity Loss (IDL) is developed to repulse the distribution within visual (textual) queries to generate more discriminative concepts.","Extensive experiments on five widely used benchmarks (i.e., MSR-VTT, MSVD, DiDeMo, LSMDC, and ActivityNet) substantiate the superior effectiveness and efficiency of the proposed method.","Remarkably, our method achieves comparable performance with SOTA as well as being nearly 220 times faster in terms of computational cost.","Code is available at: https://github.com/zchoi/GLSCL."],"url":"http://arxiv.org/abs/2405.12710v1","category":"cs.CV"}
{"created":"2024-05-21 11:50:35","title":"Constructions of bounded solutions of $div\\, {\\mathbf u}=f$ in critical spaces","abstract":"We construct uniformly bounded solutions of the equation $div\\, {\\mathbf u}=f$ for arbitrary data $f$ in the critical spaces $L^d(\\Omega)$, where $\\Omega$ is a domain of ${\\mathbb R}^d$. This question was addressed by Bourgain & Brezis, [On the equation ${\\rm div}\\, Y=f$ and application to control of phases, JAMS 16(2) (2003) 393-426], who proved that although the problem has a uniformly bounded solution, it is critical in the sense that there exists no linear solution operator for general $L^d$-data. We first discuss the validity of this existence result under weaker conditions than $f\\in L^d(\\Omega)$, and then focus our work on constructive processes for such uniformly bounded solutions. In the $d=2$ case, we present a direct one-step explicit construction, which generalizes for $d>2$ to a $(d-1)$-step construction based on induction. An explicit construction is proposed for compactly supported data in $L^{2,\\infty}(\\Omega)$ in the $d=2$ case. We also present constructive approaches based on optimization of a certain loss functional adapted to the problem. This approach provides a two-step construction in the $d=2$ case. This optimization is used as the building block of a hierarchical multistep process introduced in [E. Tadmor, Hierarchical construction of bounded solutions in critical regularity spaces, CPAM 69(6) (2016) 1087-1109] that converges to a solution in more general situations.","sentences":["We construct uniformly bounded solutions of the equation $div\\, {\\mathbf u}=f$ for arbitrary data $f$ in the critical spaces $L^d(\\Omega)$, where $\\Omega$ is a domain of ${\\mathbb R}^d$.","This question was addressed by Bourgain & Brezis,","[On the equation ${\\rm div}\\, Y=f$ and application to control of phases, JAMS 16(2) (2003) 393-426], who proved that although the problem has a uniformly bounded solution, it is critical in the sense that there exists no linear solution operator for general $L^d$-data.","We first discuss the validity of this existence result under weaker conditions than $f\\in L^d(\\Omega)$, and then focus our work on constructive processes for such uniformly bounded solutions.","In the $d=2$ case, we present a direct one-step explicit construction, which generalizes for $d>2$ to a $(d-1)$-step construction based on induction.","An explicit construction is proposed for compactly supported data in $L^{2,\\infty}(\\Omega)$ in the $d=2$ case.","We also present constructive approaches based on optimization of a certain loss functional adapted to the problem.","This approach provides a two-step construction in the $d=2$ case.","This optimization is used as the building block of a hierarchical multistep process introduced in [E. Tadmor, Hierarchical construction of bounded solutions in critical regularity spaces, CPAM 69(6) (2016) 1087-1109] that converges to a solution in more general situations."],"url":"http://arxiv.org/abs/2405.12703v1","category":"math.AP"}
{"created":"2024-05-21 11:50:16","title":"OLAPH: Improving Factuality in Biomedical Long-form Question Answering","abstract":"In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims, highlighting the need for an automated method to evaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset reconstructed using long-form question-answering datasets related to the biomedical domain. We use MedLFQA to facilitate the automatic evaluations of factuality. We also propose OLAPH, a simple and novel framework that enables the improvement of factuality through automatic evaluations. The OLAPH framework iteratively trains LLMs to mitigate hallucinations using sampling predictions and preference optimization. In other words, we iteratively set the highest-scoring response as a preferred response derived from sampling predictions and train LLMs to align with the preferred response that improves factuality. We highlight that, even on evaluation metrics not used during training, LLMs trained with our OLAPH framework demonstrate significant performance improvement in factuality. Our findings reveal that a 7B LLM trained with our OLAPH framework can provide long answers comparable to the medical experts' answers in terms of factuality. We believe that our work could shed light on gauging the long-text generation ability of LLMs in the medical domain. Our code and datasets are available at https://github.com/dmis-lab/OLAPH}{https://github.com/dmis-lab/OLAPH.","sentences":["In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs).","Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims, highlighting the need for an automated method to evaluate those claims.","Thus, we introduce MedLFQA, a benchmark dataset reconstructed using long-form question-answering datasets related to the biomedical domain.","We use MedLFQA to facilitate the automatic evaluations of factuality.","We also propose OLAPH, a simple and novel framework that enables the improvement of factuality through automatic evaluations.","The OLAPH framework iteratively trains LLMs to mitigate hallucinations using sampling predictions and preference optimization.","In other words, we iteratively set the highest-scoring response as a preferred response derived from sampling predictions and train LLMs to align with the preferred response that improves factuality.","We highlight that, even on evaluation metrics not used during training, LLMs trained with our OLAPH framework demonstrate significant performance improvement in factuality.","Our findings reveal that a 7B LLM trained with our OLAPH framework can provide long answers comparable to the medical experts' answers in terms of factuality.","We believe that our work could shed light on gauging the long-text generation ability of LLMs in the medical domain.","Our code and datasets are available at https://github.com/dmis-lab/OLAPH}{https://github.com/dmis-lab/OLAPH."],"url":"http://arxiv.org/abs/2405.12701v1","category":"cs.CL"}
{"created":"2024-05-21 11:33:15","title":"EW one-loop corrections to the longitudinally polarized Drell-Yan process. II) Charged-current case","abstract":"Complete one-loop electroweak radiative corrections to the charged-current Drell-Yan processes $pp \\to \\ell^{+}\\nu_{\\ell}(+X)$ and $pp \\to \\ell^{-}\\bar{\\nu}_{\\ell}(+X)$ are presented for the case of longitudinal polarization of initial particles for the first time. The results can be used to obtain precise predictions for the kinematic distributions of polarized $W^{\\pm}$ production cross sections, and various types of polarized single- and double-spin asymmetries, which are the sources of polarized parton distribution functions. Numerical results are obtained using the Monte-Carlo generator ReneSANCe. This study is a contribution to the research program of the STAR and PHENIX collaborations in experiments of RHIC.","sentences":["Complete one-loop electroweak radiative corrections to the charged-current Drell-Yan processes $pp \\to \\ell^{+}\\nu_{\\ell}(+X)$ and $pp \\to \\ell^{-}\\bar{\\nu}_{\\ell}(+X)$ are presented for the case of longitudinal polarization of initial particles for the first time.","The results can be used to obtain precise predictions for the kinematic distributions of polarized $W^{\\pm}$ production cross sections, and various types of polarized single- and double-spin asymmetries, which are the sources of polarized parton distribution functions.","Numerical results are obtained using the Monte-Carlo generator ReneSANCe.","This study is a contribution to the research program of the STAR and PHENIX collaborations in experiments of RHIC."],"url":"http://arxiv.org/abs/2405.12692v1","category":"hep-ph"}
{"created":"2024-05-21 11:22:27","title":"Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text","abstract":"AI-generated text detection has attracted increasing attention as powerful language models approach human-level generation. Limited work is devoted to detecting (partially) AI-paraphrased texts. However, AI paraphrasing is commonly employed in various application scenarios for text refinement and diversity. To this end, we propose a novel detection framework, paraphrased text span detection (PTD), aiming to identify paraphrased text spans within a text. Different from text-level detection, PTD takes in the full text and assigns each of the sentences with a score indicating the paraphrasing degree. We construct a dedicated dataset, PASTED, for paraphrased text span detection. Both in-distribution and out-of-distribution results demonstrate the effectiveness of PTD models in identifying AI-paraphrased text spans. Statistical and model analysis explains the crucial role of the surrounding context of the paraphrased text spans. Extensive experiments show that PTD models can generalize to versatile paraphrasing prompts and multiple paraphrased text spans. We release our resources at https://github.com/Linzwcs/PASTED.","sentences":["AI-generated text detection has attracted increasing attention as powerful language models approach human-level generation.","Limited work is devoted to detecting (partially) AI-paraphrased texts.","However, AI paraphrasing is commonly employed in various application scenarios for text refinement and diversity.","To this end, we propose a novel detection framework, paraphrased text span detection (PTD), aiming to identify paraphrased text spans within a text.","Different from text-level detection, PTD takes in the full text and assigns each of the sentences with a score indicating the paraphrasing degree.","We construct a dedicated dataset, PASTED, for paraphrased text span detection.","Both in-distribution and out-of-distribution results demonstrate the effectiveness of PTD models in identifying AI-paraphrased text spans.","Statistical and model analysis explains the crucial role of the surrounding context of the paraphrased text spans.","Extensive experiments show that PTD models can generalize to versatile paraphrasing prompts and multiple paraphrased text spans.","We release our resources at https://github.com/Linzwcs/PASTED."],"url":"http://arxiv.org/abs/2405.12689v1","category":"cs.CL"}
{"created":"2024-05-21 11:19:51","title":"Rethinking the Effective Field Theory formulation of Gravity","abstract":"General relativity is highly successful in explaining a wide range of gravitational phenomena including the gravitational waves emitted by binary systems and the shadows cast by supermassive black holes. From a modern perspective the theory is not fundamental though, but constitutes the lowest order term in an effective field theory description of the gravitational force. As a consequence, the gravitational dynamics should receive corrections by higher-derivative terms. This essay discusses structural aspects associated with these corrections and summarizes their imprint on static, spherically symmetric geometries. Along these lines, we critically reassess the common practice of using local field redefinitions in order to simplify the dynamics at the danger of shifting physics effects into sectors which are beyond the approximation under consideration.","sentences":["General relativity is highly successful in explaining a wide range of gravitational phenomena including the gravitational waves emitted by binary systems and the shadows cast by supermassive black holes.","From a modern perspective the theory is not fundamental though, but constitutes the lowest order term in an effective field theory description of the gravitational force.","As a consequence, the gravitational dynamics should receive corrections by higher-derivative terms.","This essay discusses structural aspects associated with these corrections and summarizes their imprint on static, spherically symmetric geometries.","Along these lines, we critically reassess the common practice of using local field redefinitions in order to simplify the dynamics at the danger of shifting physics effects into sectors which are beyond the approximation under consideration."],"url":"http://arxiv.org/abs/2405.12685v1","category":"gr-qc"}
{"created":"2024-05-21 11:19:50","title":"Model Free Prediction with Uncertainty Assessment","abstract":"Deep nonparametric regression, characterized by the utilization of deep neural networks to learn target functions, has emerged as a focal point of research attention in recent years. Despite considerable progress in understanding convergence rates, the absence of asymptotic properties hinders rigorous statistical inference. To address this gap, we propose a novel framework that transforms the deep estimation paradigm into a platform conducive to conditional mean estimation, leveraging the conditional diffusion model. Theoretically, we develop an end-to-end convergence rate for the conditional diffusion model and establish the asymptotic normality of the generated samples. Consequently, we are equipped to construct confidence regions, facilitating robust statistical inference. Furthermore, through numerical experiments, we empirically validate the efficacy of our proposed methodology.","sentences":["Deep nonparametric regression, characterized by the utilization of deep neural networks to learn target functions, has emerged as a focal point of research attention in recent years.","Despite considerable progress in understanding convergence rates, the absence of asymptotic properties hinders rigorous statistical inference.","To address this gap, we propose a novel framework that transforms the deep estimation paradigm into a platform conducive to conditional mean estimation, leveraging the conditional diffusion model.","Theoretically, we develop an end-to-end convergence rate for the conditional diffusion model and establish the asymptotic normality of the generated samples.","Consequently, we are equipped to construct confidence regions, facilitating robust statistical inference.","Furthermore, through numerical experiments, we empirically validate the efficacy of our proposed methodology."],"url":"http://arxiv.org/abs/2405.12684v1","category":"stat.ML"}
{"created":"2024-05-21 11:14:16","title":"A Multimodal Learning-based Approach for Autonomous Landing of UAV","abstract":"In the field of autonomous Unmanned Aerial Vehicles (UAVs) landing, conventional approaches fall short in delivering not only the required precision but also the resilience against environmental disturbances. Yet, learning-based algorithms can offer promising solutions by leveraging their ability to learn the intelligent behaviour from data. On one hand, this paper introduces a novel multimodal transformer-based Deep Learning detector, that can provide reliable positioning for precise autonomous landing. It surpasses standard approaches by addressing individual sensor limitations, achieving high reliability even in diverse weather and sensor failure conditions. It was rigorously validated across varying environments, achieving optimal true positive rates and average precisions of up to 90%. On the other hand, it is proposed a Reinforcement Learning (RL) decision-making model, based on a Deep Q-Network (DQN) rationale. Initially trained in sumlation, its adaptive behaviour is successfully transferred and validated in a real outdoor scenario. Furthermore, this approach demonstrates rapid inference times of approximately 5ms, validating its applicability on edge devices.","sentences":["In the field of autonomous Unmanned Aerial Vehicles (UAVs) landing, conventional approaches fall short in delivering not only the required precision but also the resilience against environmental disturbances.","Yet, learning-based algorithms can offer promising solutions by leveraging their ability to learn the intelligent behaviour from data.","On one hand, this paper introduces a novel multimodal transformer-based Deep Learning detector, that can provide reliable positioning for precise autonomous landing.","It surpasses standard approaches by addressing individual sensor limitations, achieving high reliability even in diverse weather and sensor failure conditions.","It was rigorously validated across varying environments, achieving optimal true positive rates and average precisions of up to 90%.","On the other hand, it is proposed a Reinforcement Learning (RL) decision-making model, based on a Deep Q-Network (DQN) rationale.","Initially trained in sumlation, its adaptive behaviour is successfully transferred and validated in a real outdoor scenario.","Furthermore, this approach demonstrates rapid inference times of approximately 5ms, validating its applicability on edge devices."],"url":"http://arxiv.org/abs/2405.12681v1","category":"cs.CV"}
{"created":"2024-05-21 10:56:57","title":"Towards an AI/ML-defined Radio for Wi-Fi: Overview, Challenges, and Roadmap","abstract":"Will AI/ML-defined radios become a reality in the near future? In this paper, we introduce the concept of an AI/ML-defined radio - a radio architecture specifically designed to support AI/ML-based optimization and decision-making in communication functions - and depict its promised benefits and potential challenges. Additionally, we discuss a potential roadmap for the development and adoption of AI/ML-defined radios, and highlight the enablers for addressing their associated challenges. While we offer a general overview of the AI/ML-defined radio concept, our focus throughout the paper remains on Wi-Fi, a wireless technology that may significantly benefit from the integration of AI/ML-defined radios, owing to its inherent decentralized management and operation within unlicensed frequency bands.","sentences":["Will AI/ML-defined radios become a reality in the near future?","In this paper, we introduce the concept of an AI/ML-defined radio - a radio architecture specifically designed to support AI/ML-based optimization and decision-making in communication functions - and depict its promised benefits and potential challenges.","Additionally, we discuss a potential roadmap for the development and adoption of AI/ML-defined radios, and highlight the enablers for addressing their associated challenges.","While we offer a general overview of the AI/ML-defined radio concept, our focus throughout the paper remains on Wi-Fi, a wireless technology that may significantly benefit from the integration of AI/ML-defined radios, owing to its inherent decentralized management and operation within unlicensed frequency bands."],"url":"http://arxiv.org/abs/2405.12675v1","category":"cs.NI"}
{"created":"2024-05-21 10:51:16","title":"Existence and non-uniqueness of cone spherical metrics with prescribed singularities on a compact Riemann surface with positive genus","abstract":"Cone spherical metrics, defined on compact Riemann surfaces, are conformal metrics with constant curvature one and finitely many cone singularities. Such a metric is termed \\textit{reducible} if a developing map of the metric has monodromy in ${\\rm U(1)}$, and \\textit{irreducible} otherwise. Utilizing the polystable extensions of two line bundles on a compact Riemann surface $X$ with genus $g_X>0$, we establish the following three primary results concerning these metrics with cone angles in $2\\pi{\\mathbb Z}_{>1}$:   \\begin{itemize} \\item[(1)] Given an effective divisor $D$ with an odd degree surpassing $2g_X$ on $X$, we find the existence of an effective divisor $D'$ in the complete linear system $|D|$ that can be represented by at least two distinct irreducible cone spherical metrics on $X$.   \\item[(2)] For a generic effective divisor $D$ with an even degree and $\\deg D\\geq 6g_X-2$ on $X$, we can identify an arcwise connected Borel subset in $|D|$ that demonstrates a Hausdorff dimension of no less than $\\big(\\deg D-4g_{X}+2\\big)$. Within this subset, each divisor $D'$ can be distinctly represented by a family of reducible metrics, defined by a single real parameter.   \\item[(3)] For an effective divisor $D$ with $\\deg D=2$ on an elliptic curve, we can identify a Borel subset in $|D|$ that is arcwise connected, showcasing a Hausdorff dimension of one. Within this subset, each divisor $D'$ can be distinctly represented by a family of reducible metrics, defined by a single real parameter.","sentences":["Cone spherical metrics, defined on compact Riemann surfaces, are conformal metrics with constant curvature one and finitely many cone singularities.","Such a metric is termed \\textit{reducible} if a developing map of the metric has monodromy in ${\\rm U(1)}$, and \\textit{irreducible} otherwise.","Utilizing the polystable extensions of two line bundles on a compact Riemann surface $X$ with genus $g_X>0$, we establish the following three primary results concerning these metrics with cone angles in $2\\pi{\\mathbb Z}_{>1}$:   \\begin{itemize} \\item[(1)]","Given an effective divisor $D$ with an odd degree surpassing $2g_X$ on $X$, we find the existence of an effective divisor $D'$ in the complete linear system $|D|$ that can be represented by at least two distinct irreducible cone spherical metrics on $X$.   \\item[(2)]","For a generic effective divisor $D$ with an even degree and $\\deg D\\geq 6g_X-2$ on $X$, we can identify an arcwise connected Borel subset in $|D|$ that demonstrates a Hausdorff dimension of no less than $\\big(\\deg D-4g_{X}+2\\big)$. Within this subset, each divisor $D'$ can be distinctly represented by a family of reducible metrics, defined by a single real parameter.   \\item[(3)]","For an effective divisor $D$ with $\\deg D=2$ on an elliptic curve, we can identify a Borel subset in $|D|$ that is arcwise connected, showcasing a Hausdorff dimension of one.","Within this subset, each divisor $D'$ can be distinctly represented by a family of reducible metrics, defined by a single real parameter."],"url":"http://arxiv.org/abs/2405.12673v1","category":"math.DG"}
{"created":"2024-05-21 10:42:04","title":"Scalar Quasi-Normal Modes of a Loop Quantum Black Hole","abstract":"We compute the Quasi-Normal Mode (QNM) frequencies for scalar perturbations for modified Schwarzschild black holes in Loop Quantum Gravity. We study the singularity-free polymerized metric characterised by two parameters encoding loop quantum effects: the minimal area gap $a_0$ and the polymeric deformation parameter $P$. We perform numerical computations using Leaver's continued fraction method and compare our results to other semi-analytical methods and existing literature. We study the effects on the QNM spectrum of variation of both deformation parameters and systematically compare to the standard Schwarzschild case. In particular we find that the scalar fundamental mode is modified from the third decimal for values of $P$ in accordance with the most recent astrophysical constraints. We also show that qualitative differences arise for highly damped modes: on the one hand, a new crossing of the imaginary axis occurs for high values of $a_0$ and, on the other hand, increasing $P$ produces a positive shift of the real part and an increase of the spacing in imaginary part between modes.","sentences":["We compute the Quasi-Normal Mode (QNM) frequencies for scalar perturbations for modified Schwarzschild black holes in Loop Quantum Gravity.","We study the singularity-free polymerized metric characterised by two parameters encoding loop quantum effects: the minimal area gap $a_0$ and the polymeric deformation parameter $P$.","We perform numerical computations using Leaver's continued fraction method and compare our results to other semi-analytical methods and existing literature.","We study the effects on the QNM spectrum of variation of both deformation parameters and systematically compare to the standard Schwarzschild case.","In particular we find that the scalar fundamental mode is modified from the third decimal for values of $P$ in accordance with the most recent astrophysical constraints.","We also show that qualitative differences arise for highly damped modes: on the one hand, a new crossing of the imaginary axis occurs for high values of $a_0$ and, on the other hand, increasing $P$ produces a positive shift of the real part and an increase of the spacing in imaginary part between modes."],"url":"http://arxiv.org/abs/2405.12671v1","category":"gr-qc"}
{"created":"2024-05-21 10:33:35","title":"Short and simple introduction to Bellman filtering and smoothing","abstract":"Based on Bellman's dynamic-programming principle, Lange (2024) presents an approximate method for filtering, smoothing and parameter estimation for possibly non-linear and/or non-Gaussian state-space models. While the approach applies more generally, this pedagogical note highlights the main results in the case where (i) the state transition remains linear and Gaussian while (ii) the observation density is log-concave and sufficiently smooth in the state variable. I demonstrate how Kalman's (1960) filter and Rauch et al.'s (1965) smoother can be obtained as special cases within the proposed framework. The main aim is to present non-experts (and my own students) with an accessible introduction, enabling them to implement the proposed methods.","sentences":["Based on Bellman's dynamic-programming principle, Lange (2024) presents an approximate method for filtering, smoothing and parameter estimation for possibly non-linear and/or non-Gaussian state-space models.","While the approach applies more generally, this pedagogical note highlights the main results in the case where (i) the state transition remains linear and Gaussian while (ii) the observation density is log-concave and sufficiently smooth in the state variable.","I demonstrate how Kalman's (1960) filter and Rauch et al.'s (1965) smoother can be obtained as special cases within the proposed framework.","The main aim is to present non-experts (and my own students) with an accessible introduction, enabling them to implement the proposed methods."],"url":"http://arxiv.org/abs/2405.12668v1","category":"stat.ME"}
{"created":"2024-05-21 10:27:42","title":"Spatial Mode Multiplexing for Fiber-Coupled IM/DD Optical Wireless Links with Misalignment","abstract":"Optical wireless communication (OWC) emerges as a pivotal solution for achieving terabit-level aggregate throughput in next-generation wireless networks. With the mature high-speed transceivers and advanced (de)multiplexing techniques designed for fiber optics, fiber-coupled OWC can be seamlessly integrated into existing ultra-high-speed networks such as data centres. In particular, OWC leveraging spatial mode multiplexing (SMM) and few-mode fiber (FMF) coupling can significantly increase capacity, though misalignment may reduce performance. This paper presents a thorough investigation into the SMM-enabled FMF coupling OWC systems affected by link misalignment, specifically focusing on systems with intensity modulation with direct detection (IM/DD) receivers. A theoretical analysis is conducted to assess the fiber coupling efficiency of the considered system in the presence of both pointing error and angle of arrival (AOA) fluctuations caused by random device vibrations. Our model elucidates the dependence of coupling efficiency to the order of the incident modes, highlighting the critical role of beam properties in system performance. To mitigate the intermodal crosstalk arising from link misalignment, we employ zero-forcing beamforming (ZFBF) to enhance the overall aggregated data rate. Through extensive numerical results, we identify optimal system configurations encompassing aperture design and mode selection, leading to a capacity boost exceeding 200%.","sentences":["Optical wireless communication (OWC) emerges as a pivotal solution for achieving terabit-level aggregate throughput in next-generation wireless networks.","With the mature high-speed transceivers and advanced (de)multiplexing techniques designed for fiber optics, fiber-coupled OWC can be seamlessly integrated into existing ultra-high-speed networks such as data centres.","In particular, OWC leveraging spatial mode multiplexing (SMM) and few-mode fiber (FMF) coupling can significantly increase capacity, though misalignment may reduce performance.","This paper presents a thorough investigation into the SMM-enabled FMF coupling OWC systems affected by link misalignment, specifically focusing on systems with intensity modulation with direct detection (IM/DD) receivers.","A theoretical analysis is conducted to assess the fiber coupling efficiency of the considered system in the presence of both pointing error and angle of arrival (AOA) fluctuations caused by random device vibrations.","Our model elucidates the dependence of coupling efficiency to the order of the incident modes, highlighting the critical role of beam properties in system performance.","To mitigate the intermodal crosstalk arising from link misalignment, we employ zero-forcing beamforming (ZFBF) to enhance the overall aggregated data rate.","Through extensive numerical results, we identify optimal system configurations encompassing aperture design and mode selection, leading to a capacity boost exceeding 200%."],"url":"http://arxiv.org/abs/2405.12667v1","category":"eess.SY"}
{"created":"2024-05-21 10:27:34","title":"SYMPLEX: Controllable Symbolic Music Generation using Simplex Diffusion with Vocabulary Priors","abstract":"We present a new approach for fast and controllable generation of symbolic music based on the simplex diffusion, which is essentially a diffusion process operating on probabilities rather than the signal space. This objective has been applied in domains such as natural language processing but here we apply it to generating 4-bar multi-instrument music loops using an orderless representation. We show that our model can be steered with vocabulary priors, which affords a considerable level control over the music generation process, for instance, infilling in time and pitch and choice of instrumentation -- all without task-specific model adaptation or applying extrinsic control.","sentences":["We present a new approach for fast and controllable generation of symbolic music based on the simplex diffusion, which is essentially a diffusion process operating on probabilities rather than the signal space.","This objective has been applied in domains such as natural language processing but here we apply it to generating 4-bar multi-instrument music loops using an orderless representation.","We show that our model can be steered with vocabulary priors, which affords a considerable level control over the music generation process, for instance, infilling in time and pitch and choice of instrumentation -- all without task-specific model adaptation or applying extrinsic control."],"url":"http://arxiv.org/abs/2405.12666v1","category":"cs.SD"}
{"created":"2024-05-21 10:24:06","title":"LAGA: Layered 3D Avatar Generation and Customization via Gaussian Splatting","abstract":"Creating and customizing a 3D clothed avatar from textual descriptions is a critical and challenging task. Traditional methods often treat the human body and clothing as inseparable, limiting users' ability to freely mix and match garments. In response to this limitation, we present LAyered Gaussian Avatar (LAGA), a carefully designed framework enabling the creation of high-fidelity decomposable avatars with diverse garments. By decoupling garments from avatar, our framework empowers users to conviniently edit avatars at the garment level. Our approach begins by modeling the avatar using a set of Gaussian points organized in a layered structure, where each layer corresponds to a specific garment or the human body itself. To generate high-quality garments for each layer, we introduce a coarse-to-fine strategy for diverse garment generation and a novel dual-SDS loss function to maintain coherence between the generated garments and avatar components, including the human body and other garments. Moreover, we introduce three regularization losses to guide the movement of Gaussians for garment transfer, allowing garments to be freely transferred to various avatars. Extensive experimentation demonstrates that our approach surpasses existing methods in the generation of 3D clothed humans.","sentences":["Creating and customizing a 3D clothed avatar from textual descriptions is a critical and challenging task.","Traditional methods often treat the human body and clothing as inseparable, limiting users' ability to freely mix and match garments.","In response to this limitation, we present LAyered Gaussian Avatar (LAGA), a carefully designed framework enabling the creation of high-fidelity decomposable avatars with diverse garments.","By decoupling garments from avatar, our framework empowers users to conviniently edit avatars at the garment level.","Our approach begins by modeling the avatar using a set of Gaussian points organized in a layered structure, where each layer corresponds to a specific garment or the human body itself.","To generate high-quality garments for each layer, we introduce a coarse-to-fine strategy for diverse garment generation and a novel dual-SDS loss function to maintain coherence between the generated garments and avatar components, including the human body and other garments.","Moreover, we introduce three regularization losses to guide the movement of Gaussians for garment transfer, allowing garments to be freely transferred to various avatars.","Extensive experimentation demonstrates that our approach surpasses existing methods in the generation of 3D clothed humans."],"url":"http://arxiv.org/abs/2405.12663v1","category":"cs.GR"}
{"created":"2024-05-21 10:19:32","title":"An $L^2$-bound for the Barban-Vehov weights","abstract":"Let $\\lambda$ the Barban--Vehov weights, defined in $(1)$. Let $X\\ge z_1\\ge100$ and $z_2=z_1^\\tau$ for some $\\tau>1$. We prove that   \\begin{equation*}   \\sum_{n\\le   X}\\frac{1}{n}\\Bigl(\\sum_{\\substack{d|n}}\\lambda_d\\Bigr)^2   \\le   f(\\tau)\\frac{\\log X}{\\log (z_2/z_1)},   \\end{equation*}   for a completely determined function $f:(1,\\infty)\\to\\mathbb{R}_{>0}$.   In particular, we may take $f(2)=30$, saving more than a factor of $5$ on what was the best known result for $\\tau=2$. Two related estimates are also provided for general $\\tau>1$.","sentences":["Let $\\lambda$ the Barban--Vehov weights, defined in $(1)$. Let $X\\ge z_1\\ge100$ and $z_2=z_1^\\tau$ for some $\\tau>1$. We prove that   \\begin{equation*}   \\sum_{n\\le   X}\\frac{1}{n}\\Bigl(\\sum_{\\substack{d|n}}\\lambda_d\\Bigr)^2   \\le   f(\\tau)\\frac{\\log X}{\\log","(z_2/z_1)},   \\end{equation*}   for a completely determined function $f:(1,\\infty)\\to\\mathbb{R}_{>0}$.   In particular, we may take $f(2)=30$, saving more than a factor of $5$ on what was the best known result for $\\tau=2$. Two related estimates are also provided for general $\\tau>1$."],"url":"http://arxiv.org/abs/2405.12662v1","category":"math.NT"}
{"created":"2024-05-21 10:18:45","title":"EmoEdit: Evoking Emotions through Image Manipulation","abstract":"Affective Image Manipulation (AIM) seeks to modify user-provided images to evoke specific emotional responses. This task is inherently complex due to its twofold objective: significantly evoking the intended emotion, while preserving the original image composition. Existing AIM methods primarily adjust color and style, often failing to elicit precise and profound emotional shifts. Drawing on psychological insights, we extend AIM by incorporating content modifications to enhance emotional impact. We introduce EmoEdit, a novel two-stage framework comprising emotion attribution and image editing. In the emotion attribution stage, we leverage a Vision-Language Model (VLM) to create hierarchies of semantic factors that represent abstract emotions. In the image editing stage, the VLM identifies the most relevant factors for the provided image, and guides a generative editing model to perform affective modifications. A ranking technique that we developed selects the best edit, balancing between emotion fidelity and structure integrity. To validate EmoEdit, we assembled a dataset of 416 images, categorized into positive, negative, and neutral classes. Our method is evaluated both qualitatively and quantitatively, demonstrating superior performance compared to existing state-of-the-art techniques. Additionally, we showcase EmoEdit's potential in various manipulation tasks, including emotion-oriented and semantics-oriented editing.","sentences":["Affective Image Manipulation (AIM) seeks to modify user-provided images to evoke specific emotional responses.","This task is inherently complex due to its twofold objective: significantly evoking the intended emotion, while preserving the original image composition.","Existing AIM methods primarily adjust color and style, often failing to elicit precise and profound emotional shifts.","Drawing on psychological insights, we extend AIM by incorporating content modifications to enhance emotional impact.","We introduce EmoEdit, a novel two-stage framework comprising emotion attribution and image editing.","In the emotion attribution stage, we leverage a Vision-Language Model (VLM) to create hierarchies of semantic factors that represent abstract emotions.","In the image editing stage, the VLM identifies the most relevant factors for the provided image, and guides a generative editing model to perform affective modifications.","A ranking technique that we developed selects the best edit, balancing between emotion fidelity and structure integrity.","To validate EmoEdit, we assembled a dataset of 416 images, categorized into positive, negative, and neutral classes.","Our method is evaluated both qualitatively and quantitatively, demonstrating superior performance compared to existing state-of-the-art techniques.","Additionally, we showcase EmoEdit's potential in various manipulation tasks, including emotion-oriented and semantics-oriented editing."],"url":"http://arxiv.org/abs/2405.12661v1","category":"cs.CV"}
{"created":"2024-05-21 10:14:50","title":"Mitigating Overconfidence in Out-of-Distribution Detection by Capturing Extreme Activations","abstract":"Detecting out-of-distribution (OOD) instances is crucial for the reliable deployment of machine learning models in real-world scenarios. OOD inputs are commonly expected to cause a more uncertain prediction in the primary task; however, there are OOD cases for which the model returns a highly confident prediction. This phenomenon, denoted as \"overconfidence\", presents a challenge to OOD detection. Specifically, theoretical evidence indicates that overconfidence is an intrinsic property of certain neural network architectures, leading to poor OOD detection. In this work, we address this issue by measuring extreme activation values in the penultimate layer of neural networks and then leverage this proxy of overconfidence to improve on several OOD detection baselines. We test our method on a wide array of experiments spanning synthetic data and real-world data, tabular and image datasets, multiple architectures such as ResNet and Transformer, different training loss functions, and include the scenarios examined in previous theoretical work. Compared to the baselines, our method often grants substantial improvements, with double-digit increases in OOD detection AUC, and it does not damage performance in any scenario.","sentences":["Detecting out-of-distribution (OOD) instances is crucial for the reliable deployment of machine learning models in real-world scenarios.","OOD inputs are commonly expected to cause a more uncertain prediction in the primary task; however, there are OOD cases for which the model returns a highly confident prediction.","This phenomenon, denoted as \"overconfidence\", presents a challenge to OOD detection.","Specifically, theoretical evidence indicates that overconfidence is an intrinsic property of certain neural network architectures, leading to poor OOD detection.","In this work, we address this issue by measuring extreme activation values in the penultimate layer of neural networks and then leverage this proxy of overconfidence to improve on several OOD detection baselines.","We test our method on a wide array of experiments spanning synthetic data and real-world data, tabular and image datasets, multiple architectures such as ResNet and Transformer, different training loss functions, and include the scenarios examined in previous theoretical work.","Compared to the baselines, our method often grants substantial improvements, with double-digit increases in OOD detection AUC, and it does not damage performance in any scenario."],"url":"http://arxiv.org/abs/2405.12658v1","category":"cs.LG"}
{"created":"2024-05-21 10:11:03","title":"On Edwards' Speculation and a New Variational Method for the Zeros of the $Z$-Function","abstract":"In his foundational book, Edwards introduced a unique \"speculation\" regarding the possible theoretical origins of the Riemann Hypothesis, based on the properties of the Riemann-Siegel formula. Essentially Edwards asks whether one can find a method to transition from zeros of $Z_0(t)=cos(\\theta(t))$, where $\\theta(t)$ is Riemann-Siegel theta function, to zeros of $Z(t)$, the Hardy $Z$-function. However, when applied directly to the classical Riemann-Siegel formula, it faces significant obstacles in forming a robust plausibility argument for the Riemann Hypothesis.   In a recent work, we introduced an alternative to the Riemann-Siegel formula that utilizes series acceleration techniques. In this paper, we explore Edwards' speculation through the lens of our accelerated approach, which avoids many of the challenges encountered in the classical case. Our approach leads to the description of a novel variational framework for relating zeros of $Z_0(t)$ to zeros of $Z(t)$ through paths in a high-dimensional parameter space $\\mathcal{Z}_N$, recasting the RH as a modern non-linear optimization problem.","sentences":["In his foundational book, Edwards introduced a unique \"speculation\" regarding the possible theoretical origins of the Riemann Hypothesis, based on the properties of the Riemann-Siegel formula.","Essentially Edwards asks whether one can find a method to transition from zeros of $Z_0(t)=cos(\\theta(t))$, where $\\theta(t)$ is Riemann-Siegel theta function, to zeros of $Z(t)$, the Hardy $Z$-function.","However, when applied directly to the classical Riemann-Siegel formula, it faces significant obstacles in forming a robust plausibility argument for the Riemann Hypothesis.   ","In a recent work, we introduced an alternative to the Riemann-Siegel formula that utilizes series acceleration techniques.","In this paper, we explore Edwards' speculation through the lens of our accelerated approach, which avoids many of the challenges encountered in the classical case.","Our approach leads to the description of a novel variational framework for relating zeros of $Z_0(t)$ to zeros of $Z(t)$ through paths in a high-dimensional parameter space $\\mathcal{Z}_N$, recasting the RH as a modern non-linear optimization problem."],"url":"http://arxiv.org/abs/2405.12657v1","category":"math.GM"}
{"created":"2024-05-21 10:10:56","title":"Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction","abstract":"Extrapolation in Large language models (LLMs) for open-ended inquiry encounters two pivotal issues: (1) hallucination and (2) expensive training costs. These issues present challenges for LLMs in specialized domains and personalized data, requiring truthful responses and low fine-tuning costs. Existing works attempt to tackle the problem by augmenting the input of a smaller language model with information from a knowledge graph (KG). However, they have two limitations: (1) failing to extract relevant information from a large one-hop neighborhood in KG and (2) applying the same augmentation strategy for KGs with different characteristics that may result in low performance. Moreover, open-ended inquiry typically yields multiple responses, further complicating extrapolation. We propose a new task, the extreme multi-label KG link prediction task, to enable a model to perform extrapolation with multiple responses using structured real-world knowledge. Our retriever identifies relevant one-hop neighbors by considering entity, relation, and textual data together. Our experiments demonstrate that (1) KGs with different characteristics require different augmenting strategies, and (2) augmenting the language model's input with textual data improves task performance significantly. By incorporating the retrieval-augmented framework with KG, our framework, with a small parameter size, is able to extrapolate based on a given KG. The code can be obtained on GitHub: https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git","sentences":["Extrapolation in Large language models (LLMs) for open-ended inquiry encounters two pivotal issues: (1) hallucination and (2) expensive training costs.","These issues present challenges for LLMs in specialized domains and personalized data, requiring truthful responses and low fine-tuning costs.","Existing works attempt to tackle the problem by augmenting the input of a smaller language model with information from a knowledge graph (KG).","However, they have two limitations: (1) failing to extract relevant information from a large one-hop neighborhood in KG and (2) applying the same augmentation strategy for KGs with different characteristics that may result in low performance.","Moreover, open-ended inquiry typically yields multiple responses, further complicating extrapolation.","We propose a new task, the extreme multi-label KG link prediction task, to enable a model to perform extrapolation with multiple responses using structured real-world knowledge.","Our retriever identifies relevant one-hop neighbors by considering entity, relation, and textual data together.","Our experiments demonstrate that (1) KGs with different characteristics require different augmenting strategies, and (2) augmenting the language model's input with textual data improves task performance significantly.","By incorporating the retrieval-augmented framework with KG, our framework, with a small parameter size, is able to extrapolate based on a given KG.","The code can be obtained on GitHub: https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git"],"url":"http://arxiv.org/abs/2405.12656v1","category":"cs.CL"}
{"created":"2024-05-21 10:07:29","title":"Utilizing Description Logics for Global Explanations of Heterogeneous Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) are effective for node classification in graph-structured data, but they lack explainability, especially at the global level. Current research mainly utilizes subgraphs of the input as local explanations or generates new graphs as global explanations. However, these graph-based methods are limited in their ability to explain classes with multiple sufficient explanations. To provide more expressive explanations, we propose utilizing class expressions (CEs) from the field of description logic (DL). Our approach explains heterogeneous graphs with different types of nodes using CEs in the EL description logic. To identify the best explanation among multiple candidate explanations, we employ and compare two different scoring functions: (1) For a given CE, we construct multiple graphs, have the GNN make a prediction for each graph, and aggregate the predicted scores. (2) We score the CE in terms of fidelity, i.e., we compare the predictions of the GNN to the predictions by the CE on a separate validation set. Instead of subgraph-based explanations, we offer CE-based explanations.","sentences":["Graph Neural Networks (GNNs) are effective for node classification in graph-structured data, but they lack explainability, especially at the global level.","Current research mainly utilizes subgraphs of the input as local explanations or generates new graphs as global explanations.","However, these graph-based methods are limited in their ability to explain classes with multiple sufficient explanations.","To provide more expressive explanations, we propose utilizing class expressions (CEs) from the field of description logic (DL).","Our approach explains heterogeneous graphs with different types of nodes using CEs in the EL description logic.","To identify the best explanation among multiple candidate explanations, we employ and compare two different scoring functions: (1) For a given CE, we construct multiple graphs, have the GNN make a prediction for each graph, and aggregate the predicted scores.","(2) We score the CE in terms of fidelity, i.e., we compare the predictions of the GNN to the predictions by the CE on a separate validation set.","Instead of subgraph-based explanations, we offer CE-based explanations."],"url":"http://arxiv.org/abs/2405.12654v1","category":"cs.AI"}
{"created":"2024-05-21 09:56:48","title":"Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable Term Frequency","abstract":"Scene graph generation (SGG) is an important task in image understanding because it represents the relationships between objects in an image as a graph structure, making it possible to understand the semantic relationships between objects intuitively. Previous SGG studies used a message-passing neural networks (MPNN) to update features, which can effectively reflect information about surrounding objects. However, these studies have failed to reflect the co-occurrence of objects during SGG generation. In addition, they only addressed the long-tail problem of the training dataset from the perspectives of sampling and learning methods. To address these two problems, we propose CooK, which reflects the Co-occurrence Knowledge between objects, and the learnable term frequency-inverse document frequency (TF-l-IDF) to solve the long-tail problem. We applied the proposed model to the SGG benchmark dataset, and the results showed a performance improvement of up to 3.8% compared with existing state-of-the-art models in SGGen subtask. The proposed method exhibits generalization ability from the results obtained, showing uniform performance improvement for all MPNN models.","sentences":["Scene graph generation (SGG) is an important task in image understanding because it represents the relationships between objects in an image as a graph structure, making it possible to understand the semantic relationships between objects intuitively.","Previous SGG studies used a message-passing neural networks (MPNN) to update features, which can effectively reflect information about surrounding objects.","However, these studies have failed to reflect the co-occurrence of objects during SGG generation.","In addition, they only addressed the long-tail problem of the training dataset from the perspectives of sampling and learning methods.","To address these two problems, we propose CooK, which reflects the Co-occurrence Knowledge between objects, and the learnable term frequency-inverse document frequency (TF-l-IDF) to solve the long-tail problem.","We applied the proposed model to the SGG benchmark dataset, and the results showed a performance improvement of up to 3.8% compared with existing state-of-the-art models in SGGen subtask.","The proposed method exhibits generalization ability from the results obtained, showing uniform performance improvement for all MPNN models."],"url":"http://arxiv.org/abs/2405.12648v1","category":"cs.CV"}
{"created":"2024-05-21 09:56:39","title":"Determining the purity of single-helical proteins from electronic specific heat measurements","abstract":"We present a theoretical investigation of the electronic specific heat (ESH) at constant volume (Cv) of single-helical proteins modeled within the tight-binding (TB) framework. We study the effects of helical symmetry, long-range hopping, environment and biological defects on thermal properties. We employ a general TB model to incorporate all parameters relevant to the helical structure of the protein. In order to provide additional insights into our results for the ESH, we also study the electronic density of states for various disorder strengths. We observe that the variation of the specific heat with disorder is very different in low and high temperature regimes, though the variation of ESH with temperature possesses a universal pattern upon varying disorder strengths related to environmental effects. Lastly, we propose an interesting application of the ESH spectra of proteins. We show that by studying the ESH of single-helical proteins, one can distinguish a defective sample from a pure one. This observation can serve as the basis of a screening technique that can be applied prior to a whole genome testing, thereby saving valuable time & resources.","sentences":["We present a theoretical investigation of the electronic specific heat (ESH) at constant volume (Cv) of single-helical proteins modeled within the tight-binding (TB) framework.","We study the effects of helical symmetry, long-range hopping, environment and biological defects on thermal properties.","We employ a general TB model to incorporate all parameters relevant to the helical structure of the protein.","In order to provide additional insights into our results for the ESH, we also study the electronic density of states for various disorder strengths.","We observe that the variation of the specific heat with disorder is very different in low and high temperature regimes, though the variation of ESH with temperature possesses a universal pattern upon varying disorder strengths related to environmental effects.","Lastly, we propose an interesting application of the ESH spectra of proteins.","We show that by studying the ESH of single-helical proteins, one can distinguish a defective sample from a pure one.","This observation can serve as the basis of a screening technique that can be applied prior to a whole genome testing, thereby saving valuable time & resources."],"url":"http://arxiv.org/abs/2405.12647v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 09:55:51","title":"Implementing feature binding through dendritic networks of a single neuron","abstract":"A single neuron receives an extensive array of synaptic inputs through its dendrites, raising the fundamental question of how these inputs undergo integration and summation, culminating in the initiation of spikes in the soma. Experimental and computational investigations have revealed various modes of integration operations that include linear, superlinear, and sublinear summation. Interestingly, distinct neuron types exhibit diverse patterns of dendritic integration contingent upon the spatial distribution of dendrites. The functional implications of these specific integration modalities remain largely unexplored. In this study, we employ the Purkinje cell as a model system to investigate these intricate questions. Our findings reveal that Purkinje cells (PCs) generally exhibit sublinear summation across their expansive dendrites. The degree of sublinearity is dynamically modulated by both spatial and temporal input. Strong sublinearity necessitates that the synaptic distribution in PCs be globally scattered sensitive, whereas weak sublinearity facilitates the generation of complex firing patterns in PCs. Leveraging dendritic branches characterized by strong sublinearity as computational units, we demonstrate that a neuron can adeptly address the feature-binding problem. Collectively, these results offer a systematic perspective on the functional role of dendritic sublinearity, providing inspiration for a broader understanding of dendritic integration across various neuronal types.","sentences":["A single neuron receives an extensive array of synaptic inputs through its dendrites, raising the fundamental question of how these inputs undergo integration and summation, culminating in the initiation of spikes in the soma.","Experimental and computational investigations have revealed various modes of integration operations that include linear, superlinear, and sublinear summation.","Interestingly, distinct neuron types exhibit diverse patterns of dendritic integration contingent upon the spatial distribution of dendrites.","The functional implications of these specific integration modalities remain largely unexplored.","In this study, we employ the Purkinje cell as a model system to investigate these intricate questions.","Our findings reveal that Purkinje cells (PCs) generally exhibit sublinear summation across their expansive dendrites.","The degree of sublinearity is dynamically modulated by both spatial and temporal input.","Strong sublinearity necessitates that the synaptic distribution in PCs be globally scattered sensitive, whereas weak sublinearity facilitates the generation of complex firing patterns in PCs.","Leveraging dendritic branches characterized by strong sublinearity as computational units, we demonstrate that a neuron can adeptly address the feature-binding problem.","Collectively, these results offer a systematic perspective on the functional role of dendritic sublinearity, providing inspiration for a broader understanding of dendritic integration across various neuronal types."],"url":"http://arxiv.org/abs/2405.12645v1","category":"q-bio.NC"}
{"created":"2024-05-21 09:55:34","title":"Unified formalism for the emergence of space in non-equilibrium description","abstract":"Previous studies indicate that the expansion law in emergence of space can be derived from the first law f thermodynamics. It has been proposed a unified formulation for the expansion law applicable to a general set of gravity theories in equilibrium description. In that formulation, which is based on the first law of thermodynamics, the non-equilibrium terms are ignored. Additionally, the structure of surface degrees of freedom in that formulation deviates from the standard notion, where $N_{sur} \\ne 4S$ in general theories of gravity. This motivates us to develop a new unified formulation for the expansion law by incorporating non-equilibrium terms into the first law of thermodynamics. In this work, we formulate a unified expansion law in a non-equilibrium context, utilizing the first law of thermodynamics along with the definition of an effective Misner-Sharp energy. Compared to previous generalizations of the expansion law, our formulation reconciles the basic definition of surface degrees of freedom $N_{sur} = 4S$ for a general set of gravity theories. The unified expansion law in non-equilibrium not only highlights the direct relationship between the rate of areal volume and the difference in degrees of freedom between the surface and bulk $(N_{sur}-N_{bulk})$, but also reveals an inverse correlation with the density of surface degrees of freedom. We also demonstrate that the unified expansion law is instrumental in deriving the expansion law for a general set of gravity theories, including those with higher-order curvature corrections such as $f(R)$ theories of gravity, which require a non-equilibrium description.","sentences":["Previous studies indicate that the expansion law in emergence of space can be derived from the first law f thermodynamics.","It has been proposed a unified formulation for the expansion law applicable to a general set of gravity theories in equilibrium description.","In that formulation, which is based on the first law of thermodynamics, the non-equilibrium terms are ignored.","Additionally, the structure of surface degrees of freedom in that formulation deviates from the standard notion, where $N_{sur} \\ne 4S$ in general theories of gravity.","This motivates us to develop a new unified formulation for the expansion law by incorporating non-equilibrium terms into the first law of thermodynamics.","In this work, we formulate a unified expansion law in a non-equilibrium context, utilizing the first law of thermodynamics along with the definition of an effective Misner-Sharp energy.","Compared to previous generalizations of the expansion law, our formulation reconciles the basic definition of surface degrees of freedom $N_{sur} = 4S$ for a general set of gravity theories.","The unified expansion law in non-equilibrium not only highlights the direct relationship between the rate of areal volume and the difference in degrees of freedom between the surface and bulk $(N_{sur}-N_{bulk})$, but also reveals an inverse correlation with the density of surface degrees of freedom.","We also demonstrate that the unified expansion law is instrumental in deriving the expansion law for a general set of gravity theories, including those with higher-order curvature corrections such as $f(R)$ theories of gravity, which require a non-equilibrium description."],"url":"http://arxiv.org/abs/2405.12644v1","category":"gr-qc"}
{"created":"2024-05-21 09:47:33","title":"Fight Fire with Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks?","abstract":"With the increasing utilization of large language models such as ChatGPT during software development, it has become crucial to verify the quality of code content it generates. Recent studies proposed utilizing ChatGPT as both a developer and tester for multi-agent collaborative software development. The multi-agent collaboration empowers ChatGPT to produce test reports for its generated code, enabling it to self-verify the code content and fix bugs based on these reports. However, these studies did not assess the effectiveness of the generated test reports in validating the code. Therefore, we conduct a comprehensive empirical investigation to evaluate ChatGPT's self-verification capability in code generation, code completion, and program repair. We request ChatGPT to (1) generate correct code and then self-verify its correctness; (2) complete code without vulnerabilities and then self-verify for the presence of vulnerabilities; and (3) repair buggy code and then self-verify whether the bugs are resolved. Our findings on two code generation datasets, one code completion dataset, and two program repair datasets reveal the following observations: (1) ChatGPT often erroneously predicts its generated incorrect code as correct. (2) The self-contradictory hallucinations in ChatGPT's behavior arise. (3) The self-verification capability of ChatGPT can be enhanced by asking the guiding question, which queries whether ChatGPT agrees with assertions about incorrectly generated or repaired code and vulnerabilities in completed code. (4) Using test reports generated by ChatGPT can identify more vulnerabilities in completed code, but the explanations for incorrectly generated code and failed repairs are mostly inaccurate in the test reports. Based on these findings, we provide implications for further research or development using ChatGPT.","sentences":["With the increasing utilization of large language models such as ChatGPT during software development, it has become crucial to verify the quality of code content it generates.","Recent studies proposed utilizing ChatGPT as both a developer and tester for multi-agent collaborative software development.","The multi-agent collaboration empowers ChatGPT to produce test reports for its generated code, enabling it to self-verify the code content and fix bugs based on these reports.","However, these studies did not assess the effectiveness of the generated test reports in validating the code.","Therefore, we conduct a comprehensive empirical investigation to evaluate ChatGPT's self-verification capability in code generation, code completion, and program repair.","We request ChatGPT to (1) generate correct code and then self-verify its correctness; (2) complete code without vulnerabilities and then self-verify for the presence of vulnerabilities; and (3) repair buggy code and then self-verify whether the bugs are resolved.","Our findings on two code generation datasets, one code completion dataset, and two program repair datasets reveal the following observations: (1) ChatGPT often erroneously predicts its generated incorrect code as correct.","(2) The self-contradictory hallucinations in ChatGPT's behavior arise.","(3) The self-verification capability of ChatGPT can be enhanced by asking the guiding question, which queries whether ChatGPT agrees with assertions about incorrectly generated or repaired code and vulnerabilities in completed code.","(4) Using test reports generated by ChatGPT can identify more vulnerabilities in completed code, but the explanations for incorrectly generated code and failed repairs are mostly inaccurate in the test reports.","Based on these findings, we provide implications for further research or development using ChatGPT."],"url":"http://arxiv.org/abs/2405.12641v1","category":"cs.SE"}
{"created":"2024-05-21 09:44:34","title":"Constraining the stochastic gravitational wave background using the future lunar seismometers","abstract":"Motivated by the old idea of using the moon as a resonant gravitational-wave (GW) detector, as well as the recent updates in modeling the lunar response to GWs, } we re-evaluate the feasibility of using a network of lunar seismometers to constrain the stochastic GW background (SGWB). In particular, using the updated model of the lunar response, we derive the pattern functions for the two polarizations of GW. With these pattern functions, we further calculate the overlap reduction functions for a network of lunar seismometers, where we have relaxed the conventional assumption that lunar seismometers are perfectly leveled to measure only the vertical acceleration. We apply our calculation to two future lunar projects, namely, Chang'e and the Lunar Gravitational-Wave Antenna (LGWA). We find that the two projects could constrain the SGWB to a level of $\\Omega_{\\text{GW}}^{\\text{Chang'e}} < 79$ and $\\Omega_{\\text{GW}}^{\\text{LGWA}} < 6.7 \\times 10^{-11}$, respectively. These results are better than the constraints placed previously on the SGWB in the mid-frequency band (around $10^{-3}- 10~\\text{Hz}$) by various types of experiments.","sentences":["Motivated by the old idea of using the moon as a resonant gravitational-wave (GW) detector, as well as the recent updates in modeling the lunar response to GWs, } we re-evaluate the feasibility of using a network of lunar seismometers to constrain the stochastic GW background (SGWB).","In particular, using the updated model of the lunar response, we derive the pattern functions for the two polarizations of GW.","With these pattern functions, we further calculate the overlap reduction functions for a network of lunar seismometers, where we have relaxed the conventional assumption that lunar seismometers are perfectly leveled to measure only the vertical acceleration.","We apply our calculation to two future lunar projects, namely, Chang'e and the Lunar Gravitational-Wave Antenna (LGWA).","We find that the two projects could constrain the SGWB to a level of $\\Omega_{\\text{GW}}^{\\text{Chang'e}} < 79$ and $\\Omega_{\\text{GW}}^{\\text{LGWA}} < 6.7 \\times 10^{-11}$, respectively.","These results are better than the constraints placed previously on the SGWB in the mid-frequency band (around $10^{-3}- 10~\\text{Hz}$) by various types of experiments."],"url":"http://arxiv.org/abs/2405.12640v1","category":"gr-qc"}
{"created":"2024-05-21 09:40:52","title":"Integrable Structure of Higher Spin CFT and the ODE/IM Correspondence","abstract":"We study two dimensional systems with extended conformal symmetry generated by the ${\\mathcal W}_3$ algebra. These are expected to have an infinite number of commuting conserved charges, which we refer to as the quantum Boussinesq charges. We compute the eigenvalues of the quantum Boussinesq charges in both the vacuum and first excited states of the higher spin module through the ODE/IM correspondence. By studying the higher spin conformal field theory on the torus, we also calculate thermal correlators involving the energy-momentum tensor and the spin-3 current by making use of the Zhu recursion relations. By combining these results, we show that it is possible to derive the current densities, whose integrals are the quantum Boussinesq charges. We also evaluate the thermal expectation values of the conserved charges, and show that these are quasi-modular differential operators acting on the character of the higher spin module.","sentences":["We study two dimensional systems with extended conformal symmetry generated by the ${\\mathcal W}_3$ algebra.","These are expected to have an infinite number of commuting conserved charges, which we refer to as the quantum Boussinesq charges.","We compute the eigenvalues of the quantum Boussinesq charges in both the vacuum and first excited states of the higher spin module through the ODE/IM correspondence.","By studying the higher spin conformal field theory on the torus, we also calculate thermal correlators involving the energy-momentum tensor and the spin-3 current by making use of the Zhu recursion relations.","By combining these results, we show that it is possible to derive the current densities, whose integrals are the quantum Boussinesq charges.","We also evaluate the thermal expectation values of the conserved charges, and show that these are quasi-modular differential operators acting on the character of the higher spin module."],"url":"http://arxiv.org/abs/2405.12636v1","category":"hep-th"}
{"created":"2024-05-21 09:39:55","title":"TempoScale: A Cloud Workloads Prediction Approach Integrating Short-Term and Long-Term Information","abstract":"Cloud native solutions are widely applied in various fields, placing higher demands on the efficient management and utilization of resource platforms. To achieve the efficiency, load forecasting and elastic scaling have become crucial technologies for dynamically adjusting cloud resources to meet user demands and minimizing resource waste. However, existing prediction-based methods lack comprehensive analysis and integration of load characteristics across different time scales. For instance, long-term trend analysis helps reveal long-term changes in load and resource demand, thereby supporting proactive resource allocation over longer periods, while short-term volatility analysis can examine short-term fluctuations in load and resource demand, providing support for real-time scheduling and rapid response. In response to this, our research introduces TempoScale, which aims to enhance the comprehensive understanding of temporal variations in cloud workloads, enabling more intelligent and adaptive decision-making for elastic scaling. TempoScale utilizes the Complete Ensemble Empirical Mode Decomposition with Adaptive Noise algorithm to decompose time-series load data into multiple Intrinsic Mode Functions (IMF) and a Residual Component (RC). First, we integrate the IMF, which represents both long-term trends and short-term fluctuations, into the time series prediction model to obtain intermediate results. Then, these intermediate results, along with the RC, are transferred into a fully connected layer to obtain the final result. Finally, this result is fed into the resource management system based on Kubernetes for resource scaling. Our proposed approach can reduce the Mean Square Error by 5.80% to 30.43% compared to the baselines, and reduce the average response time by 5.58% to 31.15%.","sentences":["Cloud native solutions are widely applied in various fields, placing higher demands on the efficient management and utilization of resource platforms.","To achieve the efficiency, load forecasting and elastic scaling have become crucial technologies for dynamically adjusting cloud resources to meet user demands and minimizing resource waste.","However, existing prediction-based methods lack comprehensive analysis and integration of load characteristics across different time scales.","For instance, long-term trend analysis helps reveal long-term changes in load and resource demand, thereby supporting proactive resource allocation over longer periods, while short-term volatility analysis can examine short-term fluctuations in load and resource demand, providing support for real-time scheduling and rapid response.","In response to this, our research introduces TempoScale, which aims to enhance the comprehensive understanding of temporal variations in cloud workloads, enabling more intelligent and adaptive decision-making for elastic scaling.","TempoScale utilizes the Complete Ensemble Empirical Mode Decomposition with Adaptive Noise algorithm to decompose time-series load data into multiple Intrinsic Mode Functions (IMF) and a Residual Component (RC).","First, we integrate the IMF, which represents both long-term trends and short-term fluctuations, into the time series prediction model to obtain intermediate results.","Then, these intermediate results, along with the RC, are transferred into a fully connected layer to obtain the final result.","Finally, this result is fed into the resource management system based on Kubernetes for resource scaling.","Our proposed approach can reduce the Mean Square Error by 5.80% to 30.43% compared to the baselines, and reduce the average response time by 5.58% to 31.15%."],"url":"http://arxiv.org/abs/2405.12635v1","category":"cs.DC"}
{"created":"2024-05-21 09:38:56","title":"Automating Attendance Management in Human Resources: A Design Science Approach Using Computer Vision and Facial Recognition","abstract":"Haar Cascade is a cost-effective and user-friendly machine learning-based algorithm for detecting objects in images and videos. Unlike Deep Learning algorithms, which typically require significant resources and expensive computing costs, it uses simple image processing techniques like edge detection and Haar features that are easy to comprehend and implement. By combining Haar Cascade with OpenCV2 on an embedded computer like the NVIDIA Jetson Nano, this system can accurately detect and match faces in a database for attendance tracking. This system aims to achieve several specific objectives that set it apart from existing solutions. It leverages Haar Cascade, enriched with carefully selected Haar features, such as Haar-like wavelets, and employs advanced edge detection techniques. These techniques enable precise face detection and matching in both images and videos, contributing to high accuracy and robust performance. By doing so, it minimizes manual intervention and reduces errors, thereby strengthening accountability. Additionally, the integration of OpenCV2 and the NVIDIA Jetson Nano optimizes processing efficiency, making it suitable for resource-constrained environments. This system caters to a diverse range of educational institutions, including schools, colleges, vocational training centers, and various workplace settings such as small businesses, offices, and factories. ... The system's affordability and efficiency democratize attendance management technology, making it accessible to a broader audience. Consequently, it has the potential to transform attendance tracking and management practices, ultimately leading to heightened productivity and accountability. In conclusion, this system represents a groundbreaking approach to attendance tracking and management...","sentences":["Haar Cascade is a cost-effective and user-friendly machine learning-based algorithm for detecting objects in images and videos.","Unlike Deep Learning algorithms, which typically require significant resources and expensive computing costs, it uses simple image processing techniques like edge detection and Haar features that are easy to comprehend and implement.","By combining Haar Cascade with OpenCV2 on an embedded computer like the NVIDIA Jetson Nano, this system can accurately detect and match faces in a database for attendance tracking.","This system aims to achieve several specific objectives that set it apart from existing solutions.","It leverages Haar Cascade, enriched with carefully selected Haar features, such as Haar-like wavelets, and employs advanced edge detection techniques.","These techniques enable precise face detection and matching in both images and videos, contributing to high accuracy and robust performance.","By doing so, it minimizes manual intervention and reduces errors, thereby strengthening accountability.","Additionally, the integration of OpenCV2 and the NVIDIA Jetson Nano optimizes processing efficiency, making it suitable for resource-constrained environments.","This system caters to a diverse range of educational institutions, including schools, colleges, vocational training centers, and various workplace settings such as small businesses, offices, and factories.","...","The system's affordability and efficiency democratize attendance management technology, making it accessible to a broader audience.","Consequently, it has the potential to transform attendance tracking and management practices, ultimately leading to heightened productivity and accountability.","In conclusion, this system represents a groundbreaking approach to attendance tracking and management..."],"url":"http://arxiv.org/abs/2405.12633v1","category":"cs.CV"}
{"created":"2024-05-21 09:33:31","title":"Exploration of Masked and Causal Language Modelling for Text Generation","abstract":"Large Language Models (LLMs) have revolutionised the field of Natural Language Processing (NLP) and have achieved state-of-the-art performance in practically every task in this field. However, the prevalent approach used in text generation, Causal Language Modelling (CLM), which generates text sequentially from left to right, inherently limits the freedom of the model, which does not decide when and where each token is generated. In contrast, Masked Language Modelling (MLM), primarily used for language understanding tasks, can generate tokens anywhere in the text and any order. This paper conducts an extensive comparison of MLM and CLM approaches for text generation tasks. To do so, we pre-train several language models of comparable sizes on three different datasets, namely 1) medical discharge summaries, 2) movie plot synopses, and 3) authorship verification datasets. To assess the quality of the generations, we first employ quantitative metrics and then perform a qualitative human evaluation to analyse coherence and grammatical correctness. In addition, we evaluate the usefulness of the generated texts by using them in three different downstream tasks: 1) Entity Recognition, 2) Text Classification, and 3) Authorship Verification. The results show that MLM consistently outperforms CLM in text generation across all datasets, with higher quantitative scores and better coherence in the generated text. The study also finds \\textit{no strong correlation} between the quality of the generated text and the performance of the models in the downstream tasks. With this study, we show that MLM for text generation has great potential for future research and provides direction for future studies in this area.","sentences":["Large Language Models (LLMs) have revolutionised the field of Natural Language Processing (NLP) and have achieved state-of-the-art performance in practically every task in this field.","However, the prevalent approach used in text generation, Causal Language Modelling (CLM), which generates text sequentially from left to right, inherently limits the freedom of the model, which does not decide when and where each token is generated.","In contrast, Masked Language Modelling (MLM), primarily used for language understanding tasks, can generate tokens anywhere in the text and any order.","This paper conducts an extensive comparison of MLM and CLM approaches for text generation tasks.","To do so, we pre-train several language models of comparable sizes on three different datasets, namely 1) medical discharge summaries, 2) movie plot synopses, and 3) authorship verification datasets.","To assess the quality of the generations, we first employ quantitative metrics and then perform a qualitative human evaluation to analyse coherence and grammatical correctness.","In addition, we evaluate the usefulness of the generated texts by using them in three different downstream tasks: 1) Entity Recognition, 2) Text Classification, and 3) Authorship Verification.","The results show that MLM consistently outperforms CLM in text generation across all datasets, with higher quantitative scores and better coherence in the generated text.","The study also finds \\textit{no strong correlation} between the quality of the generated text and the performance of the models in the downstream tasks.","With this study, we show that MLM for text generation has great potential for future research and provides direction for future studies in this area."],"url":"http://arxiv.org/abs/2405.12630v1","category":"cs.CL"}
{"created":"2024-05-21 09:30:47","title":"Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots","abstract":"Robots playing soccer often rely on hard-coded behaviors that struggle to generalize when the game environment change. In this paper, we propose a temporal logic based approach that allows robots' behaviors and goals to adapt to the semantics of the environment. In particular, we present a hierarchical representation of soccer in which the robot selects the level of operation based on the perceived semantic characteristics of the environment, thus modifying dynamically the set of rules and goals to apply. The proposed approach enables the robot to operate in unstructured environments, just as it happens when humans go from soccer played on an official field to soccer played on a street. Three different use cases set in different scenarios are presented to demonstrate the effectiveness of the proposed approach.","sentences":["Robots playing soccer often rely on hard-coded behaviors that struggle to generalize when the game environment change.","In this paper, we propose a temporal logic based approach that allows robots' behaviors and goals to adapt to the semantics of the environment.","In particular, we present a hierarchical representation of soccer in which the robot selects the level of operation based on the perceived semantic characteristics of the environment, thus modifying dynamically the set of rules and goals to apply.","The proposed approach enables the robot to operate in unstructured environments, just as it happens when humans go from soccer played on an official field to soccer played on a street.","Three different use cases set in different scenarios are presented to demonstrate the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2405.12628v1","category":"cs.RO"}
{"created":"2024-05-21 09:29:11","title":"On disjoint dynamical properties and Lipschitz-free spaces","abstract":"The notion of disjoint $\\mathcal{A}$-transitivity for a Furstenberg family $\\mathcal{A}$ is introduced with the aim to generalize properties derived from disjoint hypercyclic operators. We begin a systematic study by showing some of the basic properties, including necessary conditions to inherit the property on the whole space from an invariant linearly dense set containing the origin. As a consequence, we continue the study of the link between non-linear and linear dynamics through Lipschitz-free spaces by presenting some necessary conditions to obtain disjoint $\\mathcal{A}$-transitivity for families of Lipschitz-free operators on $\\mathcal{F}(M)$ expressed in terms of conditions in the underlying metric space $M$.","sentences":["The notion of disjoint $\\mathcal{A}$-transitivity for a Furstenberg family $\\mathcal{A}$ is introduced with the aim to generalize properties derived from disjoint hypercyclic operators.","We begin a systematic study by showing some of the basic properties, including necessary conditions to inherit the property on the whole space from an invariant linearly dense set containing the origin.","As a consequence, we continue the study of the link between non-linear and linear dynamics through Lipschitz-free spaces by presenting some necessary conditions to obtain disjoint $\\mathcal{A}$-transitivity for families of Lipschitz-free operators on $\\mathcal{F}(M)$ expressed in terms of conditions in the underlying metric space $M$."],"url":"http://arxiv.org/abs/2405.12626v1","category":"math.FA"}
{"created":"2024-05-21 09:23:39","title":"Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition","abstract":"Recent work on dialogue-based collaborative plan acquisition (CPA) has suggested that Theory of Mind (ToM) modelling can improve missing knowledge prediction in settings with asymmetric skill-sets and knowledge. Although ToM was claimed to be important for effective collaboration, its real impact on this novel task remains under-explored. By representing plans as graphs and by exploiting task-specific constraints we show that, as performance on CPA nearly doubles when predicting one's own missing knowledge, the improvements due to ToM modelling diminish. This phenomenon persists even when evaluating existing baseline methods. To better understand the relevance of ToM for CPA, we report a principled performance comparison of models with and without ToM features. Results across different models and ablations consistently suggest that learned ToM features are indeed more likely to reflect latent patterns in the data with no perceivable link to ToM. This finding calls for a deeper understanding of the role of ToM in CPA and beyond, as well as new methods for modelling and evaluating mental states in computational collaborative agents.","sentences":["Recent work on dialogue-based collaborative plan acquisition (CPA) has suggested that Theory of Mind (ToM) modelling can improve missing knowledge prediction in settings with asymmetric skill-sets and knowledge.","Although ToM was claimed to be important for effective collaboration, its real impact on this novel task remains under-explored.","By representing plans as graphs and by exploiting task-specific constraints we show that, as performance on CPA nearly doubles when predicting one's own missing knowledge, the improvements due to ToM modelling diminish.","This phenomenon persists even when evaluating existing baseline methods.","To better understand the relevance of ToM for CPA, we report a principled performance comparison of models with and without ToM features.","Results across different models and ablations consistently suggest that learned ToM features are indeed more likely to reflect latent patterns in the data with no perceivable link to ToM.","This finding calls for a deeper understanding of the role of ToM in CPA and beyond, as well as new methods for modelling and evaluating mental states in computational collaborative agents."],"url":"http://arxiv.org/abs/2405.12621v1","category":"cs.AI"}
{"created":"2024-05-21 09:12:20","title":"Quantifying Emergence in Large Language Models","abstract":"Emergence, broadly conceptualized as the ``intelligent'' behaviors of LLMs, has recently been studied and proved challenging to quantify due to the lack of a measurable definition. Most commonly, it has been estimated statistically through model performances across extensive datasets and tasks, which consumes significant resources. In addition, such estimation is difficult to interpret and may not accurately reflect the models' intrinsic emergence. In this work, we propose a quantifiable solution for estimating emergence. Inspired by emergentism in dynamics, we quantify the strength of emergence by comparing the entropy reduction of the macroscopic (semantic) level with that of the microscopic (token) level, both of which are derived from the representations within the transformer block. Using a low-cost estimator, our quantification method demonstrates consistent behaviors across a suite of LMs (GPT-2, GEMMA, etc.) under both in-context learning (ICL) and natural sentences. Empirical results show that (1) our method gives consistent measurements which align with existing observations based on performance metrics, validating the effectiveness of our emergence quantification; (2) our proposed metric uncovers novel emergence patterns such as the correlations between the variance of our metric and the number of ``shots'' in ICL, which further suggests a new way of interpreting hallucinations in LLMs; (3) we offer a potential solution towards estimating the emergence of larger and closed-resource LMs via smaller LMs like GPT-2. Our codes are available at: https://github.com/Zodiark-ch/Emergence-of-LLMs/.","sentences":["Emergence, broadly conceptualized as the ``intelligent'' behaviors of LLMs, has recently been studied and proved challenging to quantify due to the lack of a measurable definition.","Most commonly, it has been estimated statistically through model performances across extensive datasets and tasks, which consumes significant resources.","In addition, such estimation is difficult to interpret and may not accurately reflect the models' intrinsic emergence.","In this work, we propose a quantifiable solution for estimating emergence.","Inspired by emergentism in dynamics, we quantify the strength of emergence by comparing the entropy reduction of the macroscopic (semantic) level with that of the microscopic (token) level, both of which are derived from the representations within the transformer block.","Using a low-cost estimator, our quantification method demonstrates consistent behaviors across a suite of LMs (GPT-2, GEMMA, etc.)","under both in-context learning (ICL) and natural sentences.","Empirical results show that (1) our method gives consistent measurements which align with existing observations based on performance metrics, validating the effectiveness of our emergence quantification; (2) our proposed metric uncovers novel emergence patterns such as the correlations between the variance of our metric and the number of ``shots'' in ICL, which further suggests a new way of interpreting hallucinations in LLMs; (3) we offer a potential solution towards estimating the emergence of larger and closed-resource LMs via smaller LMs like GPT-2.","Our codes are available at: https://github.com/Zodiark-ch/Emergence-of-LLMs/."],"url":"http://arxiv.org/abs/2405.12617v1","category":"cs.CL"}
{"created":"2024-05-21 09:10:51","title":"Learning Causal Dynamics Models in Object-Oriented Environments","abstract":"Causal dynamics models (CDMs) have demonstrated significant potential in addressing various challenges in reinforcement learning. To learn CDMs, recent studies have performed causal discovery to capture the causal dependencies among environmental variables. However, the learning of CDMs is still confined to small-scale environments due to computational complexity and sample efficiency constraints. This paper aims to extend CDMs to large-scale object-oriented environments, which consist of a multitude of objects classified into different categories. We introduce the Object-Oriented CDM (OOCDM) that shares causalities and parameters among objects belonging to the same class. Furthermore, we propose a learning method for OOCDM that enables it to adapt to a varying number of objects. Experiments on large-scale tasks indicate that OOCDM outperforms existing CDMs in terms of causal discovery, prediction accuracy, generalization, and computational efficiency.","sentences":["Causal dynamics models (CDMs) have demonstrated significant potential in addressing various challenges in reinforcement learning.","To learn CDMs, recent studies have performed causal discovery to capture the causal dependencies among environmental variables.","However, the learning of CDMs is still confined to small-scale environments due to computational complexity and sample efficiency constraints.","This paper aims to extend CDMs to large-scale object-oriented environments, which consist of a multitude of objects classified into different categories.","We introduce the Object-Oriented CDM (OOCDM) that shares causalities and parameters among objects belonging to the same class.","Furthermore, we propose a learning method for OOCDM that enables it to adapt to a varying number of objects.","Experiments on large-scale tasks indicate that OOCDM outperforms existing CDMs in terms of causal discovery, prediction accuracy, generalization, and computational efficiency."],"url":"http://arxiv.org/abs/2405.12615v1","category":"cs.LG"}
{"created":"2024-05-21 09:07:47","title":"Efficient modeling of sub-kilometer surface wind with Gaussian processes and neural networks","abstract":"Accurately representing surface weather at the sub-kilometer scale is crucial for optimal decision-making in a wide range of applications. This motivates the use of statistical techniques to provide accurate and calibrated probabilistic predictions at a lower cost compared to numerical simulations. Wind represents a particularly challenging variable to model due to its high spatial and temporal variability. This paper presents a novel approach that integrates Gaussian processes (GPs) and neural networks to model surface wind gusts, leveraging multiple data sources, including numerical weather prediction (NWP) models, digital elevation models (DEM), and in-situ measurements. Results demonstrate the added value of modeling the multivariate covariance structure of the variable of interest, as opposed to only applying a univariate probabilistic regression approach. Modeling the covariance enables the optimal integration of observed measurements from ground stations, which is shown to reduce the continuous ranked probability score compared to the baseline. Moreover, it allows the direct generation of realistic fields that are also marginally calibrated, aided by scalable techniques such as Random Fourier Features (RFF) and pathwise conditioning. We discuss the effect of different modeling choices, as well as different degrees of approximation, and present our results for a case study.","sentences":["Accurately representing surface weather at the sub-kilometer scale is crucial for optimal decision-making in a wide range of applications.","This motivates the use of statistical techniques to provide accurate and calibrated probabilistic predictions at a lower cost compared to numerical simulations.","Wind represents a particularly challenging variable to model due to its high spatial and temporal variability.","This paper presents a novel approach that integrates Gaussian processes (GPs) and neural networks to model surface wind gusts, leveraging multiple data sources, including numerical weather prediction (NWP) models, digital elevation models (DEM), and in-situ measurements.","Results demonstrate the added value of modeling the multivariate covariance structure of the variable of interest, as opposed to only applying a univariate probabilistic regression approach.","Modeling the covariance enables the optimal integration of observed measurements from ground stations, which is shown to reduce the continuous ranked probability score compared to the baseline.","Moreover, it allows the direct generation of realistic fields that are also marginally calibrated, aided by scalable techniques such as Random Fourier Features (RFF) and pathwise conditioning.","We discuss the effect of different modeling choices, as well as different degrees of approximation, and present our results for a case study."],"url":"http://arxiv.org/abs/2405.12614v1","category":"physics.ao-ph"}
{"created":"2024-05-21 09:06:36","title":"Tagengo: A Multilingual Chat Dataset","abstract":"Open source large language models (LLMs) have shown great improvements in recent times. However, many of these models are focused solely on popular spoken languages. We present a high quality dataset of more than 70k prompt-response pairs in 74 languages which consist of human generated prompts and synthetic responses. We use this dataset to train a state-of-the-art open source English LLM to chat multilingually. We evaluate our model on MT-Bench chat benchmarks in 6 languages, finding that our multilingual model outperforms previous state-of-the-art open source LLMs across each language. We further find that training on more multilingual data is beneficial to the performance in a chosen target language (Japanese) compared to simply training on only data in that language. These results indicate the necessity of training on large amounts of high quality multilingual data to make a more accessible LLM.","sentences":["Open source large language models (LLMs) have shown great improvements in recent times.","However, many of these models are focused solely on popular spoken languages.","We present a high quality dataset of more than 70k prompt-response pairs in 74 languages which consist of human generated prompts and synthetic responses.","We use this dataset to train a state-of-the-art open source English LLM to chat multilingually.","We evaluate our model on MT-Bench chat benchmarks in 6 languages, finding that our multilingual model outperforms previous state-of-the-art open source LLMs across each language.","We further find that training on more multilingual data is beneficial to the performance in a chosen target language (Japanese) compared to simply training on only data in that language.","These results indicate the necessity of training on large amounts of high quality multilingual data to make a more accessible LLM."],"url":"http://arxiv.org/abs/2405.12612v1","category":"cs.CL"}
{"created":"2024-05-21 09:02:43","title":"A 22 percent increase in the German minimum wage: nothing crazy!","abstract":"We present the first empirical evidence on the 22 percent increase in the German minimum wage, implemented in 2022, raising it from Euro 9.82 to 10.45 in July and to Euro 12 in October. Leveraging the German Earnings Survey, a large and novel data source comprising around 8 million employee-level observations reported by employers each month, we apply a difference-in-difference-in-differences approach to analyze the policy's impact on hourly wages, monthly earnings, employment, and working hours. Our findings reveal significant positive effects on wages, affirming the policy's intended benefits for low-wage workers. Interestingly, we identify a negative effect on working hours, mainly driven by minijobbers. The hours effect results in an implied labor demand elasticity w.r.t. the employment volume of -0.17 which only partially offsets the monthly wage gains. We neither observe a negative effect on the individual's employment retention nor the regional employment levels.","sentences":["We present the first empirical evidence on the 22 percent increase in the German minimum wage, implemented in 2022, raising it from Euro 9.82 to 10.45 in July and to Euro 12 in October.","Leveraging the German Earnings Survey, a large and novel data source comprising around 8 million employee-level observations reported by employers each month, we apply a difference-in-difference-in-differences approach to analyze the policy's impact on hourly wages, monthly earnings, employment, and working hours.","Our findings reveal significant positive effects on wages, affirming the policy's intended benefits for low-wage workers.","Interestingly, we identify a negative effect on working hours, mainly driven by minijobbers.","The hours effect results in an implied labor demand elasticity w.r.t.","the employment volume of -0.17 which only partially offsets the monthly wage gains.","We neither observe a negative effect on the individual's employment retention nor the regional employment levels."],"url":"http://arxiv.org/abs/2405.12608v1","category":"econ.GN"}
{"created":"2024-05-21 08:58:59","title":"Generating ultrastable glasses by homogenizing the local virial stress","abstract":"In recent years, the possibility of algorithmically preparing ultra-stable glasses (UG), i.e., states that lie very deep in the potential energy landscape, has considerably expanded our understanding of the glassy state. In this work, we report on a new protocol for ultrastable glass preparation that iteratively modifies the particle diameters to reduce local virial stress fluctuations. We apply the algorithm to an additive Lennard-Jones mixture and show that, compared to the states obtained via thermal annealing, virial homogenized glasses (VHG) are characterized by a considerable increase in both kinetic stability and the number of locally favored structures (icosahedra). We also consider the melting dynamics during heating ramps and show that it occurs via an accumulation of localized events. Our results highlight the connection between the thermodynamic and mechanical stability of ultra-stable glassy states.","sentences":["In recent years, the possibility of algorithmically preparing ultra-stable glasses (UG), i.e., states that lie very deep in the potential energy landscape, has considerably expanded our understanding of the glassy state.","In this work, we report on a new protocol for ultrastable glass preparation that iteratively modifies the particle diameters to reduce local virial stress fluctuations.","We apply the algorithm to an additive Lennard-Jones mixture and show that, compared to the states obtained via thermal annealing, virial homogenized glasses (VHG) are characterized by a considerable increase in both kinetic stability and the number of locally favored structures (icosahedra).","We also consider the melting dynamics during heating ramps and show that it occurs via an accumulation of localized events.","Our results highlight the connection between the thermodynamic and mechanical stability of ultra-stable glassy states."],"url":"http://arxiv.org/abs/2405.12605v1","category":"cond-mat.soft"}
{"created":"2024-05-21 08:57:44","title":"Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming","abstract":"With the proliferation of red-teaming strategies for Large Language Models (LLMs), the deficiency in the literature about improving the safety and robustness of LLM defense strategies is becoming increasingly pronounced. This paper introduces the LLM-based \\textbf{sentinel} model as a plug-and-play prefix module designed to reconstruct the input prompt with just a few ($<30$) additional tokens, effectively reducing toxicity in responses from target LLMs. The sentinel model naturally overcomes the \\textit{parameter inefficiency} and \\textit{limited model accessibility} for fine-tuning large target models. We employ an interleaved training regimen using Proximal Policy Optimization (PPO) to optimize both red team and sentinel models dynamically, incorporating a value head-sharing mechanism inspired by the multi-agent centralized critic to manage the complex interplay between agents. Our extensive experiments across text-to-text and text-to-image demonstrate the effectiveness of our approach in mitigating toxic outputs, even when dealing with larger models like \\texttt{Llama-2}, \\texttt{GPT-3.5} and \\texttt{Stable-Diffusion}, highlighting the potential of our framework in enhancing safety and robustness in various applications.","sentences":["With the proliferation of red-teaming strategies for Large Language Models (LLMs), the deficiency in the literature about improving the safety and robustness of LLM defense strategies is becoming increasingly pronounced.","This paper introduces the LLM-based \\textbf{sentinel} model as a plug-and-play prefix module designed to reconstruct the input prompt with just a few ($<30$) additional tokens, effectively reducing toxicity in responses from target LLMs.","The sentinel model naturally overcomes the \\textit{parameter inefficiency} and \\textit{limited model accessibility} for fine-tuning large target models.","We employ an interleaved training regimen using Proximal Policy Optimization (PPO) to optimize both red team and sentinel models dynamically, incorporating a value head-sharing mechanism inspired by the multi-agent centralized critic to manage the complex interplay between agents.","Our extensive experiments across text-to-text and text-to-image demonstrate the effectiveness of our approach in mitigating toxic outputs, even when dealing with larger models like \\texttt{Llama-2}, \\texttt{GPT-3.5} and \\texttt{Stable-Diffusion}, highlighting the potential of our framework in enhancing safety and robustness in various applications."],"url":"http://arxiv.org/abs/2405.12604v1","category":"cs.CL"}
{"created":"2024-05-21 08:55:58","title":"Interacting Kerr-Newman Electromagnetic Fields","abstract":"In this paper, we study some of the properties of the $G \\to 0$ limit of the Kerr-Newman solution of Einstein-Maxwell equations. Noting Carter's observation of the near equality between the $g = 2$ gyromagnetic ratio in the Kerr-Newman solution and that of the electron, we discuss additional such coincidences relating to the Kerr-Newman multipoles and properties of the electron. In contrast to the Coulomb field, this spinning Maxwell field has a finite Lagrangian. Moreover, by evaluating the Lagrangian for the superposition of two such Kerr-Newman electromagnetic fields on a flat background, we are able to find their interaction potential. This yields a correction to the Coulomb interaction due to the spin of the field.","sentences":["In this paper, we study some of the properties of the $G \\to 0$ limit of the Kerr-Newman solution of Einstein-Maxwell equations.","Noting Carter's observation of the near equality between the $g = 2$ gyromagnetic ratio in the Kerr-Newman solution and that of the electron, we discuss additional such coincidences relating to the Kerr-Newman multipoles and properties of the electron.","In contrast to the Coulomb field, this spinning Maxwell field has a finite Lagrangian.","Moreover, by evaluating the Lagrangian for the superposition of two such Kerr-Newman electromagnetic fields on a flat background, we are able to find their interaction potential.","This yields a correction to the Coulomb interaction due to the spin of the field."],"url":"http://arxiv.org/abs/2405.12602v1","category":"gr-qc"}
{"created":"2024-05-21 08:55:10","title":"FFAM: Feature Factorization Activation Map for Explanation of 3D Detectors","abstract":"LiDAR-based 3D object detection has made impressive progress recently, yet most existing models are black-box, lacking interpretability. Previous explanation approaches primarily focus on analyzing image-based models and are not readily applicable to LiDAR-based 3D detectors. In this paper, we propose a feature factorization activation map (FFAM) to generate high-quality visual explanations for 3D detectors. FFAM employs non-negative matrix factorization to generate concept activation maps and subsequently aggregates these maps to obtain a global visual explanation. To achieve object-specific visual explanations, we refine the global visual explanation using the feature gradient of a target object. Additionally, we introduce a voxel upsampling strategy to align the scale between the activation map and input point cloud. We qualitatively and quantitatively analyze FFAM with multiple detectors on several datasets. Experimental results validate the high-quality visual explanations produced by FFAM. The Code will be available at \\url{https://github.com/Say2L/FFAM.git}.","sentences":["LiDAR-based 3D object detection has made impressive progress recently, yet most existing models are black-box, lacking interpretability.","Previous explanation approaches primarily focus on analyzing image-based models and are not readily applicable to LiDAR-based 3D detectors.","In this paper, we propose a feature factorization activation map (FFAM) to generate high-quality visual explanations for 3D detectors.","FFAM employs non-negative matrix factorization to generate concept activation maps and subsequently aggregates these maps to obtain a global visual explanation.","To achieve object-specific visual explanations, we refine the global visual explanation using the feature gradient of a target object.","Additionally, we introduce a voxel upsampling strategy to align the scale between the activation map and input point cloud.","We qualitatively and quantitatively analyze FFAM with multiple detectors on several datasets.","Experimental results validate the high-quality visual explanations produced by FFAM.","The Code will be available at \\url{https://github.com/Say2L/FFAM.git}."],"url":"http://arxiv.org/abs/2405.12601v1","category":"cs.CV"}
{"created":"2024-05-21 08:46:20","title":"Available energy of plasmas with small fluctuations","abstract":"The available energy of a plasma is defined as the maximum amount by which the plasma energy can be lowered by volume-preserving rearrangements in phase space, a so-called Gardner re-stacking. A general expression is derived for the available energy of a nearly homogeneous plasma and is shown to be closely related to the Helmholtz free energy, which it can never exceed. A number of explicit examples are given.","sentences":["The available energy of a plasma is defined as the maximum amount by which the plasma energy can be lowered by volume-preserving rearrangements in phase space, a so-called Gardner re-stacking.","A general expression is derived for the available energy of a nearly homogeneous plasma and is shown to be closely related to the Helmholtz free energy, which it can never exceed.","A number of explicit examples are given."],"url":"http://arxiv.org/abs/2405.12599v1","category":"physics.plasm-ph"}
{"created":"2024-05-21 08:44:42","title":"Machine learning of quantum channels on NISQ devices","abstract":"World-wide efforts aim at the realization of advanced quantum simulators and processors. However, despite the development of intricate hardware and pulse control systems, it may still not be generally known which effective quantum dynamics, or channels, are implemented on these devices. To systematically infer those, we propose a neural-network algorithm approximating generic discrete-time dynamics through the repeated action of an effective quantum channel. We test our approach considering time-periodic Lindblad dynamics as well as non-unitary subsystem dynamics in many-body unitary circuits. Moreover, we exploit it to investigate cross-talk effects on the ibmq_ehningen quantum processor, which showcases our method as a practically applicable tool for inferring quantum channels when the exact nature of the underlying dynamics on the physical device is not known a priori. While the present approach is tailored for learning Markovian dynamics, we discuss how it can be adapted to also capture generic non-Markovian discrete-time evolutions.","sentences":["World-wide efforts aim at the realization of advanced quantum simulators and processors.","However, despite the development of intricate hardware and pulse control systems, it may still not be generally known which effective quantum dynamics, or channels, are implemented on these devices.","To systematically infer those, we propose a neural-network algorithm approximating generic discrete-time dynamics through the repeated action of an effective quantum channel.","We test our approach considering time-periodic Lindblad dynamics as well as non-unitary subsystem dynamics in many-body unitary circuits.","Moreover, we exploit it to investigate cross-talk effects on the ibmq_ehningen quantum processor, which showcases our method as a practically applicable tool for inferring quantum channels when the exact nature of the underlying dynamics on the physical device is not known a priori.","While the present approach is tailored for learning Markovian dynamics, we discuss how it can be adapted to also capture generic non-Markovian discrete-time evolutions."],"url":"http://arxiv.org/abs/2405.12598v1","category":"quant-ph"}
{"created":"2024-05-21 08:35:10","title":"Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression","abstract":"Key-value~(KV) caching is an important technique to accelerate the inference of large language models~(LLMs), but incurs significant memory overhead. To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment. In this paper, we introduce \\textbf{DecoQuant}, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache. Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors. Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range. Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor. Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference and develop an efficient dequantization kernel tailored specifically for DecoQuant. Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a $\\sim$75\\% reduction in memory footprint while maintaining comparable generation quality.","sentences":["Key-value~(KV) caching is an important technique to accelerate the inference of large language models~(LLMs), but incurs significant memory overhead.","To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment.","In this paper, we introduce \\textbf{DecoQuant}, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache.","Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors.","Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range.","Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor.","Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference and develop an efficient dequantization kernel tailored specifically for DecoQuant.","Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a $\\sim$75\\% reduction in memory footprint while maintaining comparable generation quality."],"url":"http://arxiv.org/abs/2405.12591v1","category":"cs.CL"}
{"created":"2024-05-21 08:34:39","title":"Maverick-Aware Shapley Valuation for Client Selection in Federated Learning","abstract":"Federated Learning (FL) allows clients to train a model collaboratively without sharing their private data. One key challenge in practical FL systems is data heterogeneity, particularly in handling clients with rare data, also referred to as Mavericks. These clients own one or more data classes exclusively, and the model performance becomes poor without their participation. Thus, utilizing Mavericks throughout training is crucial. In this paper, we first design a Maverick-aware Shapley valuation that fairly evaluates the contribution of Mavericks. The main idea is to compute the clients' Shapley values (SV) class-wise, i.e., per label. Next, we propose FedMS, a Maverick-Shapley client selection mechanism for FL that intelligently selects the clients that contribute the most in each round, by employing our Maverick-aware SV-based contribution score. We show that, compared to an extensive list of baselines, FedMS achieves better model performance and fairer Shapley Rewards distribution.","sentences":["Federated Learning (FL) allows clients to train a model collaboratively without sharing their private data.","One key challenge in practical FL systems is data heterogeneity, particularly in handling clients with rare data, also referred to as Mavericks.","These clients own one or more data classes exclusively, and the model performance becomes poor without their participation.","Thus, utilizing Mavericks throughout training is crucial.","In this paper, we first design a Maverick-aware Shapley valuation that fairly evaluates the contribution of Mavericks.","The main idea is to compute the clients' Shapley values (SV) class-wise, i.e., per label.","Next, we propose FedMS, a Maverick-Shapley client selection mechanism for FL that intelligently selects the clients that contribute the most in each round, by employing our Maverick-aware SV-based contribution score.","We show that, compared to an extensive list of baselines, FedMS achieves better model performance and fairer Shapley Rewards distribution."],"url":"http://arxiv.org/abs/2405.12590v1","category":"cs.LG"}
{"created":"2024-05-21 08:32:59","title":"An Improved Robust Total Logistic Distance Metric algorithm for Generalized Gaussian Noise and Noisy Input","abstract":"Although the known maximum total generalized correntropy (MTGC) and generalized maximum blakezisserman total correntropy (GMBZTC) algorithms can maintain good performance under the errors-in-variables (EIV) model disrupted by generalized Gaussian noise, their requirement for manual ad-justment of parameters is excessive, greatly increasing the practical difficulty of use. To solve this problem, the total arctangent based on logical distance metric (TACLDM) algo-rithm is proposed by utilizing the advantage of few parameters in logical distance metric (LDM) theory and the convergence behavior is improved by the arctangent function. Compared with other competing algorithms, the TACLDM algorithm not only has fewer parameters, but also has better robustness to generalized Gaussian noise and significantly reduces the steady-state error. Furthermore, the analysis of the algorithm in the generalized Gaussian noise environment is analyzed in detail in this paper. Finally, computer simulations demonstrate the outstanding performance of the TACLDM algorithm and the rigorous theoretical deduction in this paper.","sentences":["Although the known maximum total generalized correntropy (MTGC) and generalized maximum blakezisserman total correntropy (GMBZTC) algorithms can maintain good performance under the errors-in-variables (EIV) model disrupted by generalized Gaussian noise, their requirement for manual ad-justment of parameters is excessive, greatly increasing the practical difficulty of use.","To solve this problem, the total arctangent based on logical distance metric (TACLDM) algo-rithm is proposed by utilizing the advantage of few parameters in logical distance metric (LDM) theory and the convergence behavior is improved by the arctangent function.","Compared with other competing algorithms, the TACLDM algorithm not only has fewer parameters, but also has better robustness to generalized Gaussian noise and significantly reduces the steady-state error.","Furthermore, the analysis of the algorithm in the generalized Gaussian noise environment is analyzed in detail in this paper.","Finally, computer simulations demonstrate the outstanding performance of the TACLDM algorithm and the rigorous theoretical deduction in this paper."],"url":"http://arxiv.org/abs/2405.12589v1","category":"eess.SP"}
{"created":"2024-05-21 08:27:21","title":"Ergodic Unobservable MDPs: Decidability of Approximation","abstract":"Unobservable Markov decision processes (UMDPs) serve as a prominent mathematical framework for modeling sequential decision-making problems. A key aspect in computational analysis is the consideration of decidability, which concerns the existence of algorithms. In general, the computation of the exact and approximated values is undecidable for UMDPs with the long-run average objective. Building on matrix product theory and ergodic properties, we introduce a novel subclass of UMDPs, termed ergodic UMDPs. Our main result demonstrates that approximating the value within this subclass is decidable. However, we show that the exact problem remains undecidable. Finally, we discuss the primary challenges of extending these results to partially observable Markov decision processes.","sentences":["Unobservable Markov decision processes (UMDPs) serve as a prominent mathematical framework for modeling sequential decision-making problems.","A key aspect in computational analysis is the consideration of decidability, which concerns the existence of algorithms.","In general, the computation of the exact and approximated values is undecidable for UMDPs with the long-run average objective.","Building on matrix product theory and ergodic properties, we introduce a novel subclass of UMDPs, termed ergodic UMDPs.","Our main result demonstrates that approximating the value within this subclass is decidable.","However, we show that the exact problem remains undecidable.","Finally, we discuss the primary challenges of extending these results to partially observable Markov decision processes."],"url":"http://arxiv.org/abs/2405.12583v1","category":"math.OC"}
{"created":"2024-05-21 08:24:05","title":"Hybrid Digital-Analog Semantic Communications","abstract":"Digital and analog semantic communications (SemCom) face inherent limitations such as data security concerns in analog SemCom, as well as leveling-off and cliff-edge effects in digital SemCom. In order to overcome these challenges, we propose a novel SemCom framework and a corresponding system called HDA-DeepSC, which leverages a hybrid digital-analog approach for multimedia transmission. This is achieved through the introduction of digital-analog allocation and fusion modules. To strike a balance between data rate and distortion, we design new loss functions that take into account long-distance dependencies in the semantic distortion constraint, essential information recovery in the channel distortion constraint, and optimal bit stream generation in the rate constraint. Additionally, we propose denoising diffusion-based signal detection techniques, which involve carefully designed variance schedules and sampling algorithms to refine transmitted signals. Through extensive numerical experiments, we will demonstrate that HDA-DeepSC exhibits robustness to channel variations and is capable of supporting various communication scenarios. Our proposed framework outperforms existing benchmarks in terms of peak signal-to-noise ratio and multi-scale structural similarity, showcasing its superiority in semantic communication quality.","sentences":["Digital and analog semantic communications (SemCom) face inherent limitations such as data security concerns in analog SemCom, as well as leveling-off and cliff-edge effects in digital SemCom.","In order to overcome these challenges, we propose a novel SemCom framework and a corresponding system called HDA-DeepSC, which leverages a hybrid digital-analog approach for multimedia transmission.","This is achieved through the introduction of digital-analog allocation and fusion modules.","To strike a balance between data rate and distortion, we design new loss functions that take into account long-distance dependencies in the semantic distortion constraint, essential information recovery in the channel distortion constraint, and optimal bit stream generation in the rate constraint.","Additionally, we propose denoising diffusion-based signal detection techniques, which involve carefully designed variance schedules and sampling algorithms to refine transmitted signals.","Through extensive numerical experiments, we will demonstrate that HDA-DeepSC exhibits robustness to channel variations and is capable of supporting various communication scenarios.","Our proposed framework outperforms existing benchmarks in terms of peak signal-to-noise ratio and multi-scale structural similarity, showcasing its superiority in semantic communication quality."],"url":"http://arxiv.org/abs/2405.12580v1","category":"eess.SP"}
{"created":"2024-05-21 08:23:54","title":"Mining the Explainability and Generalization: Fact Verification Based on Self-Instruction","abstract":"Fact-checking based on commercial LLMs has become mainstream. Although these methods offer high explainability, it falls short in accuracy compared to traditional fine-tuning approaches, and data security is also a significant concern. In this paper, we propose a self-instruction based fine-tuning approach for fact-checking that balances accuracy and explainability. Our method consists of Data Augmentation and Improved DPO fine-tuning. The former starts by instructing the model to generate both positive and negative explanations based on claim-evidence pairs and labels, then sampling the dataset according to our customized difficulty standards. The latter employs our proposed improved DPO to fine-tune the model using the generated samples. We fine-tune the smallest-scale LLaMA-7B model and evaluate it on the challenging fact-checking datasets FEVEROUS and HOVER, utilizing four fine-tuning methods and three few-shot learning methods for comparison. The experiments demonstrate that our approach not only retains accuracy comparable to, or even surpassing, traditional fine-tuning methods, but also generates fluent explanation text. Moreover, it also exhibit high generalization performance. Our method is the first to leverage self-supervised learning for fact-checking and innovatively combines contrastive learning and improved DPO in fine-tuning LLMs, as shown in the experiments.","sentences":["Fact-checking based on commercial LLMs has become mainstream.","Although these methods offer high explainability, it falls short in accuracy compared to traditional fine-tuning approaches, and data security is also a significant concern.","In this paper, we propose a self-instruction based fine-tuning approach for fact-checking that balances accuracy and explainability.","Our method consists of Data Augmentation and Improved DPO fine-tuning.","The former starts by instructing the model to generate both positive and negative explanations based on claim-evidence pairs and labels, then sampling the dataset according to our customized difficulty standards.","The latter employs our proposed improved DPO to fine-tune the model using the generated samples.","We fine-tune the smallest-scale LLaMA-7B model and evaluate it on the challenging fact-checking datasets FEVEROUS and HOVER, utilizing four fine-tuning methods and three few-shot learning methods for comparison.","The experiments demonstrate that our approach not only retains accuracy comparable to, or even surpassing, traditional fine-tuning methods, but also generates fluent explanation text.","Moreover, it also exhibit high generalization performance.","Our method is the first to leverage self-supervised learning for fact-checking and innovatively combines contrastive learning and improved DPO in fine-tuning LLMs, as shown in the experiments."],"url":"http://arxiv.org/abs/2405.12579v1","category":"cs.CL"}
{"created":"2024-05-21 08:22:30","title":"Trend to equilibrium for degenerate reaction-diffusion systems coming out of chemistry","abstract":"The trend to equilibrium for reaction-diffusion systems coming out of chemistry is investigated, in the case when reaction processes might happen only on some open subsets of the domain. A special case has been studied recently in [Desvillettes, L., \\\\& Phung, K. D. (2022). Journal of Differential Equations, 338, 227-255] using log convexity technique from controllability theory, which in turn requires some amount of regularity for the solutions, and is difficult to generalise to more general systems. In this paper, we prove the convergence to equilibrium directly using vector-valued functional inequalities. One major advantage of our approach is that it allows to deal with nonlinearities of arbitrary orders, for which only global renormalised solutions are known to globally exist. For a specific situation where solutions are known to be bounded, we also prove the convergence to equilibrium when the diffusion as well as the reaction rates are degenerate. For this situation, we also treat the case of reactions happening in a set of strictly positive measure which may have an empty interior.","sentences":["The trend to equilibrium for reaction-diffusion systems coming out of chemistry is investigated, in the case when reaction processes might happen only on some open subsets of the domain.","A special case has been studied recently in [Desvillettes, L., \\\\& Phung, K. D. (2022).","Journal of Differential Equations, 338, 227-255] using log convexity technique from controllability theory, which in turn requires some amount of regularity for the solutions, and is difficult to generalise to more general systems.","In this paper, we prove the convergence to equilibrium directly using vector-valued functional inequalities.","One major advantage of our approach is that it allows to deal with nonlinearities of arbitrary orders, for which only global renormalised solutions are known to globally exist.","For a specific situation where solutions are known to be bounded, we also prove the convergence to equilibrium when the diffusion as well as the reaction rates are degenerate.","For this situation, we also treat the case of reactions happening in a set of strictly positive measure which may have an empty interior."],"url":"http://arxiv.org/abs/2405.12578v1","category":"math.AP"}
{"created":"2024-05-21 08:19:04","title":"Ulrich ranks of Veronese varieties and equivariant instantons","abstract":"We construct Ulrich bundles on Veronese threefolds of arbitrary degree as generic deformations of symmetric squares of equivariant instanton bundles on the projective space, thus classifying the rank of Ulrich bundles on such varieties and proving a conjecture of Costa and Mir{\\'o}-Roig.","sentences":["We construct Ulrich bundles on Veronese threefolds of arbitrary degree as generic deformations of symmetric squares of equivariant instanton bundles on the projective space, thus classifying the rank of Ulrich bundles on such varieties and proving a conjecture of Costa and Mir{\\'o}-Roig."],"url":"http://arxiv.org/abs/2405.12574v1","category":"math.AC"}
{"created":"2024-05-21 08:18:28","title":"EchoPT: A Pretrained Transformer Architecture that Predicts 2D In-Air Sonar Images for Mobile Robotics","abstract":"The predictive brain hypothesis suggests that perception can be interpreted as the process of minimizing the error between predicted perception tokens generated by an internal world model and actual sensory input tokens. When implementing working examples of this hypothesis in the context of in-air sonar, significant difficulties arise due to the sparse nature of the reflection model that governs ultrasonic sensing. Despite these challenges, creating consistent world models using sonar data is crucial for implementing predictive processing of ultrasound data in robotics. In an effort to enable robust robot behavior using ultrasound as the sole exteroceptive sensor modality, this paper introduces EchoPT, a pretrained transformer architecture designed to predict 2D sonar images from previous sensory data and robot ego-motion information. We detail the transformer architecture that drives EchoPT and compare the performance of our model to several state-of-the-art techniques. In addition to presenting and evaluating our EchoPT model, we demonstrate the effectiveness of this predictive perception approach in two robotic tasks.","sentences":["The predictive brain hypothesis suggests that perception can be interpreted as the process of minimizing the error between predicted perception tokens generated by an internal world model and actual sensory input tokens.","When implementing working examples of this hypothesis in the context of in-air sonar, significant difficulties arise due to the sparse nature of the reflection model that governs ultrasonic sensing.","Despite these challenges, creating consistent world models using sonar data is crucial for implementing predictive processing of ultrasound data in robotics.","In an effort to enable robust robot behavior using ultrasound as the sole exteroceptive sensor modality, this paper introduces EchoPT, a pretrained transformer architecture designed to predict 2D sonar images from previous sensory data and robot ego-motion information.","We detail the transformer architecture that drives EchoPT and compare the performance of our model to several state-of-the-art techniques.","In addition to presenting and evaluating our EchoPT model, we demonstrate the effectiveness of this predictive perception approach in two robotic tasks."],"url":"http://arxiv.org/abs/2405.12573v1","category":"cs.RO"}
{"created":"2024-05-21 08:14:05","title":"Monte Carlos for tau lepton -- Standard Model and New Physics signatures","abstract":"One of purposes for High Energy accelerator experiments is confrontation of theory and measurements in ever new realms. Any new agreement extends theory applicability domain, any discrepancy hints to unexplained. That calls for better calculations or new, deeper theory. Often one has to search for small contributions over large Standard Model background. Multi-dimensional signatures and complex background subtraction cuts imply that Monte Carlo techniques are indispensable.   My talk included description of: KKMC Monte Carlo for e+e- to tau+tau- (n gamma), (with tau decays), generation of additional lepton pairs of SM and New Physics, and of anomalous magnetic and electric dipole moments effects.","sentences":["One of purposes for High Energy accelerator experiments is confrontation of theory and measurements in ever new realms.","Any new agreement extends theory applicability domain, any discrepancy hints to unexplained.","That calls for better calculations or new, deeper theory.","Often one has to search for small contributions over large Standard Model background.","Multi-dimensional signatures and complex background subtraction cuts imply that Monte Carlo techniques are indispensable.   ","My talk included description of: KKMC Monte Carlo for e+e- to tau+tau- (n gamma), (with tau decays), generation of additional lepton pairs of SM and New Physics, and of anomalous magnetic and electric dipole moments effects."],"url":"http://arxiv.org/abs/2405.12570v1","category":"hep-ph"}
{"created":"2024-05-21 08:09:44","title":"Representability of G-functions as rational functions in hypergeometric series","abstract":"Fres\\'an and Jossen have given a negative answer to a question of Siegel about the representability of every $E$-function as a polynomial with algebraic coefficients in $E$-functions of type ${}_pF_q[\\underline{a};\\underline{b};\\gamma x^{q-p+1}]$ with $q\\geq p\\geq 0$, $\\gamma \\in \\overline{\\mathbb Q}$ and rational parameters $\\underline{a}, \\underline{b}$. In this paper, we study, in a more general context, a similar question for $G$-functions asked by Fischler and the second author: can every $G$-function be represented as a polynomial with algebraic coefficients in $G$-functions of type $\\mu(x)\\cdot {}_pF_{p-1}[\\underline{a};\\underline{b};\\lambda(x)]$ with $p\\ge 1$, rational parameters $\\underline{a},\\underline{b}$ and $\\mu,\\lambda$ algebraic over $\\mathbb Q(x)$ with $\\lambda(0)=0$? They have shown the answer to be negative under a generalization of Grothendieck's Period Conjecture and a technical assumption on the~$\\lambda$'s. Using differential Galois theory, we prove that, for every $N\\in \\mathbb N$, there exists a $G$-function which can not be represented as a rational function with coefficients in $\\overline{\\mathbb C(x)}$ of solutions of linear differential equations with coefficients in $\\mathbb C(x)$ and at most $N$ singularities in $\\mathbb{P}^1 (\\mathbb C)$. As a corollary, we deduce that not all $G$-functions can be represented as a rational function in hypergeometric series of the above mentioned type, when the $\\lambda$'s are rational functions with degrees of their numerators and denominators bounded by an arbitrarily large fixed constant. This provides an unconditional negative answer to the question asked by Fischler and the second author for such~$\\lambda$'s.","sentences":["Fres\\'an and Jossen have given a negative answer to a question of Siegel about the representability of every $E$-function as a polynomial with algebraic coefficients in $E$-functions of type ${}_pF_q[\\underline{a};\\underline{b};\\gamma x^{q-p+1}]$ with $q\\geq p\\geq 0$, $\\gamma \\in \\overline{\\mathbb Q}$ and rational parameters $\\underline{a}, \\underline{b}$.","In this paper, we study, in a more general context, a similar question for $G$-functions asked by Fischler and the second author: can every $G$-function be represented as a polynomial with algebraic coefficients in $G$-functions of type $\\mu(x)\\cdot {}_pF_{p-1}[\\underline{a};\\underline{b};\\lambda(x)]$ with $p\\ge 1$, rational parameters $\\underline{a},\\underline{b}$ and $\\mu,\\lambda$ algebraic over $\\mathbb Q(x)$ with $\\lambda(0)=0$?","They have shown the answer to be negative under a generalization of Grothendieck's Period Conjecture and a technical assumption on the~$\\lambda$'s.","Using differential Galois theory, we prove that, for every $N\\in \\mathbb N$, there exists a $G$-function which can not be represented as a rational function with coefficients in $\\overline{\\mathbb C(x)}$ of solutions of linear differential equations with coefficients in $\\mathbb C(x)$ and at most $N$ singularities in $\\mathbb{P}^1 (\\mathbb C)$. As a corollary, we deduce that not all $G$-functions can be represented as a rational function in hypergeometric series of the above mentioned type, when the $\\lambda$'s are rational functions with degrees of their numerators and denominators bounded by an arbitrarily large fixed constant.","This provides an unconditional negative answer to the question asked by Fischler and the second author for such~$\\lambda$'s."],"url":"http://arxiv.org/abs/2405.12568v1","category":"math.CA"}
{"created":"2024-05-21 08:06:13","title":"ProtT3: Protein-to-Text Generation for Text-based Protein Understanding","abstract":"Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks. However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data. Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts. To address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding. ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation. This collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM's representation space and the LM's input space. Unlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation. To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval. Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components. Our code is available at https://github.com/acharkq/ProtT3.","sentences":["Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks.","However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data.","Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts.","To address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding.","ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation.","This collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM's representation space and the LM's input space.","Unlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation.","To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval.","Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components.","Our code is available at https://github.com/acharkq/ProtT3."],"url":"http://arxiv.org/abs/2405.12564v1","category":"q-bio.QM"}
{"created":"2024-05-21 07:57:51","title":"Quantum roots for Kac-Moody root systems and finiteness properties of the Kac-Moody affine Bruhat order","abstract":"Let $G$ be a split Kac-Moody group over a local field. In their study of the Iwahori-Hecke algebra of $G$, A.Braverman, D. Kazhdan and M. Patnaik defined a partial order - called the affine Bruhat order - on the extended affine Weyl semi-group $W^+$ of $G$. In this paper, we study finiteness questions for covers and co-covers of $W^+$, generalizing results of A. Welch. In particular we prove that the intervals for this order are finite. Our results rely on the finiteness of the set of quantum roots of arbitrary Kac-Moody root systems, which we prove. We also obtain a classification of quantum roots.","sentences":["Let $G$ be a split Kac-Moody group over a local field.","In their study of the Iwahori-Hecke algebra of $G$, A.Braverman, D. Kazhdan and M. Patnaik defined a partial order - called the affine Bruhat order - on the extended affine Weyl semi-group $W^+$ of $G$. In this paper, we study finiteness questions for covers and co-covers of $W^+$, generalizing results of A. Welch.","In particular we prove that the intervals for this order are finite.","Our results rely on the finiteness of the set of quantum roots of arbitrary Kac-Moody root systems, which we prove.","We also obtain a classification of quantum roots."],"url":"http://arxiv.org/abs/2405.12559v1","category":"math.RT"}
{"created":"2024-05-21 07:56:57","title":"Entanglement transitions in SU(1, 1) quantum dynamics: applications to Bose-Einstein condensates and periodically driven coupled oscillators","abstract":"We study the entanglement properties in non-equilibrium quantum systems with the SU(1, 1) structure. Through M\\\"obius transformation, we map the dynamics of these systems following a sudden quench or a periodic drive onto three distinct trajectories on the Poincar\\'e disc, corresponding the heating, non-heating, and a phase boundary describing these non-equilibrium quantum states. We consider two experimentally feasible systems where their quantum dynamics exhibit the SU(1, 1) structure: the quench dynamics of the Bose-Einstein condensates and the periodically driven coupled oscillators. In both cases, the heating, non-heating phases, and their boundary manifest through distinct signatures in the phonon population where exponential, oscillatory, and linear growths classify these phases. Similarly, the entanglement entropy and negativity also exhibit distinct behaviors (linearly, oscillatory, and logarithmic growths) characterizing these phases, respectively. Notibly, for the periodically driven coupled oscillators, the non-equilibrium properties are characterized by two sets of SU(1, 1) generators. The corresponding two sets of the trajectories on two Poincar\\'e discs lead to a more complex phase diagram. We identify two distinct phases within the heating region discernible solely by the growth rate of the entanglement entropy, where a discontinuity is observed when varying the parameters across the phase boundary within in heating region. This discontinuity is not observed in the phonon population.","sentences":["We study the entanglement properties in non-equilibrium quantum systems with the SU(1, 1) structure.","Through M\\\"obius transformation, we map the dynamics of these systems following a sudden quench or a periodic drive onto three distinct trajectories on the Poincar\\'e disc, corresponding the heating, non-heating, and a phase boundary describing these non-equilibrium quantum states.","We consider two experimentally feasible systems where their quantum dynamics exhibit the SU(1, 1) structure: the quench dynamics of the Bose-Einstein condensates and the periodically driven coupled oscillators.","In both cases, the heating, non-heating phases, and their boundary manifest through distinct signatures in the phonon population where exponential, oscillatory, and linear growths classify these phases.","Similarly, the entanglement entropy and negativity also exhibit distinct behaviors (linearly, oscillatory, and logarithmic growths) characterizing these phases, respectively.","Notibly, for the periodically driven coupled oscillators, the non-equilibrium properties are characterized by two sets of SU(1, 1) generators.","The corresponding two sets of the trajectories on two Poincar\\'e discs lead to a more complex phase diagram.","We identify two distinct phases within the heating region discernible solely by the growth rate of the entanglement entropy, where a discontinuity is observed when varying the parameters across the phase boundary within in heating region.","This discontinuity is not observed in the phonon population."],"url":"http://arxiv.org/abs/2405.12558v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-21 07:56:46","title":"On the approximation of the Hardy $Z$-function via high-order sections","abstract":"Sections of the Hardy $Z$-function are given by $Z_N(t) := \\sum_{k=1}^{N} \\frac{cos(\\theta(t)-ln(k) t) }{\\sqrt{k}}$ for any $N \\in \\mathbb{N}$. Sections approximate the Hardy $Z$-function in two ways: (a) $2Z_{\\widetilde{N}(t)}(t)$ is the Hardy-Littlewood approximate functional equation (AFE) approximation for $\\widetilde{N}(t) = \\left [ \\sqrt{\\frac{t}{2 \\pi}} \\right ]$. (b) $Z_{N(t)}(t)$ is Spira's approximation for $N(t) = \\left [\\frac{t}{2} \\right ]$. Spira conjectured, based on experimental observations, that, contrary to the classical approximation $(a)$, approximation (b) satisfies the Riemann Hypothesis (RH) in the sense that all of its zeros are real. We present theoretical justification for Spira's conjecture, via new techniques of acceleration of series, showing that it is essentially equivalent to RH itself.","sentences":["Sections of the Hardy $Z$-function are given by $Z_N(t) :","= \\sum_{k=1}^{N} \\frac{cos(\\theta(t)-ln(k) t) }{\\sqrt{k}}$ for any $N \\in \\mathbb{N}$. Sections approximate the Hardy $Z$-function in two ways: (a) $2Z_{\\widetilde{N}(t)}(t)$ is the Hardy-Littlewood approximate functional equation (AFE) approximation for $\\widetilde{N}(t) = \\left","[ \\sqrt{\\frac{t}{2 \\pi}} \\right ]$.","(b) $Z_{N(t)}(t)$ is Spira's approximation for $N(t) =","\\left","[\\frac{t}{2} \\right ]$.","Spira conjectured, based on experimental observations, that, contrary to the classical approximation $(a)$, approximation (b) satisfies the Riemann Hypothesis (RH) in the sense that all of its zeros are real.","We present theoretical justification for Spira's conjecture, via new techniques of acceleration of series, showing that it is essentially equivalent to RH itself."],"url":"http://arxiv.org/abs/2405.12557v1","category":"math.GM"}
{"created":"2024-05-21 07:48:21","title":"Superconductivity in 2D systems enhanced by nonadiabatic phonon-production effects","abstract":"We investigate the dynamical effects of electron-phonon coupling (EPC) on the superconducting properties of two-dimensional (2D) systems, calculating the Eliashberg function in terms of dynamically renormalized phonons. By studying different approximations for the phonon self-energy, we identify the important role of charge fluctuations in shaping the superconductivity properties, not only through the renormalization of phonon frequencies and damping rates but also through structural changes in the phonon spectral function. With the dynamical effects treated consistently, we argue that a part of the phonon spectral weight necessarily shifts to low frequencies due to the coupling to the 2D gapless plasmon. Furthermore, we find that the EPC leads to excess phonon spectral weight as well - i.e., phonon production - which generally tends to enhance the transition temperature. Our calculations point out that the influence of phonon production becomes greater as the density and the effective mass of electrons increase.","sentences":["We investigate the dynamical effects of electron-phonon coupling (EPC) on the superconducting properties of two-dimensional (2D) systems, calculating the Eliashberg function in terms of dynamically renormalized phonons.","By studying different approximations for the phonon self-energy, we identify the important role of charge fluctuations in shaping the superconductivity properties, not only through the renormalization of phonon frequencies and damping rates but also through structural changes in the phonon spectral function.","With the dynamical effects treated consistently, we argue that a part of the phonon spectral weight necessarily shifts to low frequencies due to the coupling to the 2D gapless plasmon.","Furthermore, we find that the EPC leads to excess phonon spectral weight as well - i.e., phonon production - which generally tends to enhance the transition temperature.","Our calculations point out that the influence of phonon production becomes greater as the density and the effective mass of electrons increase."],"url":"http://arxiv.org/abs/2405.12554v1","category":"cond-mat.supr-con"}
{"created":"2024-05-21 07:37:36","title":"Colloidal bubble propulsion mediated through viscous flows","abstract":"Bubble-propelled catalytic colloids stand out as a uniquely efficient design for artificial controllable micromachines, but so far lack a general theoretical framework that explains the physics of their propulsion. Here we develop a combined diffusive and hydrodynamic theory of bubble growth near a spherical catalytic colloid, that allows us to explain the underlying mechanism and the influence of environmental and material parameters. We identify two dimensionless groups, related to colloidal activity and the background fluid, that govern a saddle-node bifurcation of the bubble growth dynamics, and calculate the generated flows analytically for both slip and no slip boundary conditions on the bubble. We finish with a discussion of the assumptions and predictions of our model in the context of existing experimental results, and conclude that some of the observed behaviour, notably the ratchet-like gait, stems from peculiarities of the experimental setup rather than fundamental physics of the propulsive mechanism.","sentences":["Bubble-propelled catalytic colloids stand out as a uniquely efficient design for artificial controllable micromachines, but so far lack a general theoretical framework that explains the physics of their propulsion.","Here we develop a combined diffusive and hydrodynamic theory of bubble growth near a spherical catalytic colloid, that allows us to explain the underlying mechanism and the influence of environmental and material parameters.","We identify two dimensionless groups, related to colloidal activity and the background fluid, that govern a saddle-node bifurcation of the bubble growth dynamics, and calculate the generated flows analytically for both slip and no slip boundary conditions on the bubble.","We finish with a discussion of the assumptions and predictions of our model in the context of existing experimental results, and conclude that some of the observed behaviour, notably the ratchet-like gait, stems from peculiarities of the experimental setup rather than fundamental physics of the propulsive mechanism."],"url":"http://arxiv.org/abs/2405.12552v1","category":"physics.flu-dyn"}
{"created":"2024-05-21 07:37:31","title":"RA: A machine based rational agent, Part 1","abstract":"RA is a software package that couples machine learning with formal reasoning in an attempt to find the laws that generate the empirical data that it has been given access to. A brief outline of RA in its initial stage of development is presented. Particular emphasis is given to current design strategies that aim to endow RA with the ability to construct its own conjectures of which it constructs proofs.","sentences":["RA is a software package that couples machine learning with formal reasoning in an attempt to find the laws that generate the empirical data that it has been given access to.","A brief outline of RA in its initial stage of development is presented.","Particular emphasis is given to current design strategies that aim to endow RA with the ability to construct its own conjectures of which it constructs proofs."],"url":"http://arxiv.org/abs/2405.12551v1","category":"cs.LO"}
{"created":"2024-05-21 07:34:49","title":"Blockchain-based AI Methods for Managing Industrial IoT: Recent Developments, Integration Challenges and Opportunities","abstract":"Currently, Blockchain (BC), Artificial Intelligence (AI), and smart Industrial Internet of Things (IIoT) are not only leading promising technologies in the world, but also these technologies facilitate the current society to develop the standard of living and make it easier for users. However, these technologies have been applied in various domains for different purposes. Then, these are successfully assisted in developing the desired system, such as-smart cities, homes, manufacturers, education, and industries. Moreover, these technologies need to consider various issues-security, privacy, confidentiality, scalability, and application challenges in diverse fields. In this context, with the increasing demand for these issues solutions, the authors present a comprehensive survey on the AI approaches with BC in the smart IIoT. Firstly, we focus on state-of-the-art overviews regarding AI, BC, and smart IoT applications. Then, we provide the benefits of integrating these technologies and discuss the established methods, tools, and strategies efficiently. Most importantly, we highlight the various issues--security, stability, scalability, and confidentiality and guide the way of addressing strategy and methods. Furthermore, the individual and collaborative benefits of applications have been discussed. Lastly, we are extensively concerned about the open research challenges and potential future guidelines based on BC-based AI approaches in the intelligent IIoT system.","sentences":["Currently, Blockchain (BC), Artificial Intelligence (AI), and smart Industrial Internet of Things (IIoT) are not only leading promising technologies in the world, but also these technologies facilitate the current society to develop the standard of living and make it easier for users.","However, these technologies have been applied in various domains for different purposes.","Then, these are successfully assisted in developing the desired system, such as-smart cities, homes, manufacturers, education, and industries.","Moreover, these technologies need to consider various issues-security, privacy, confidentiality, scalability, and application challenges in diverse fields.","In this context, with the increasing demand for these issues solutions, the authors present a comprehensive survey on the AI approaches with BC in the smart IIoT.","Firstly, we focus on state-of-the-art overviews regarding AI, BC, and smart IoT applications.","Then, we provide the benefits of integrating these technologies and discuss the established methods, tools, and strategies efficiently.","Most importantly, we highlight the various issues--security, stability, scalability, and confidentiality and guide the way of addressing strategy and methods.","Furthermore, the individual and collaborative benefits of applications have been discussed.","Lastly, we are extensively concerned about the open research challenges and potential future guidelines based on BC-based AI approaches in the intelligent IIoT system."],"url":"http://arxiv.org/abs/2405.12550v1","category":"cs.CR"}
{"created":"2024-05-21 07:34:19","title":"The Schwarzian Approach in Sturm-Liouville Problems","abstract":"A novel method for finding the eigenvalues of a Sturm-Liouville problem is developed. Following the minimalist approach the problem is transformed to a single first-order differential equation with appropriate boundary conditions. Although the resulting equation is nonlinear, its form allows to find the general solution by adding a second part to a particular solution. This splitting of the general solution in two parts involves the Schwarzian derivative, hence the name of the approach. The eigenvalues that correspond to acceptable solutions asymptotically can be found by requiring the second part to correct the diverging behavior of the particular solution. The method can be applied to many different areas of physics, such as the Schr\\\"odinger equation in quantum mechanics and stability problems in fluid dynamics. Examples are presented.","sentences":["A novel method for finding the eigenvalues of a Sturm-Liouville problem is developed.","Following the minimalist approach the problem is transformed to a single first-order differential equation with appropriate boundary conditions.","Although the resulting equation is nonlinear, its form allows to find the general solution by adding a second part to a particular solution.","This splitting of the general solution in two parts involves the Schwarzian derivative, hence the name of the approach.","The eigenvalues that correspond to acceptable solutions asymptotically can be found by requiring the second part to correct the diverging behavior of the particular solution.","The method can be applied to many different areas of physics, such as the Schr\\\"odinger equation in quantum mechanics and stability problems in fluid dynamics.","Examples are presented."],"url":"http://arxiv.org/abs/2405.12549v1","category":"math-ph"}
{"created":"2024-05-21 07:32:27","title":"Solar wind data analysis aided by synthetic modeling: a better understanding of plasma-frame variations from temporal data","abstract":"In-situ measurements of the solar wind, a turbulent and anisotropic plasma flow originating at the Sun, are mostly carried out by single spacecraft, resulting in one-dimensional time series. The conversion of these measurements to the spatial frame of the plasma is a great challenge, but required for direct comparison of the measurements with MHD turbulence theories. Here we present a toolkit, based on the synthetic modeling of solar wind fluctuations as two-dimensional noise maps with adjustable spectral and power anisotropy, that can help with the temporal-spatial conversion of real data. Specifically, by following the spacecraft trajectory through a noise map (relative velocity and angle relative to some mean magnetic field) with properties tuned to mimic those of the solar wind, the likelihood that the temporal data fluctuations represent parallel or perpendicular fluctuations in the plasma frame can be quantified by correlating structure functions of the noise map. Synthetic temporal data can also be generated, which can provide a testing ground for analysis applied to the solar wind data. We demonstrate this tool by investigating Parker Solar Probe's E7 trajectory and data, and showcase several possible ways in which it can be used. We find that whether temporal variations in the spacecraft frame come from parallel or perpendicular variations in the plasma frame strongly depends on the spectral and power anisotropy of the measured wind. Data analysis assisted by such underlying synthetic models as presented here could open up new ways to interpret measurements in the future, specifically in the more reliable determination of plasma frame quantities from temporal measurements.","sentences":["In-situ measurements of the solar wind, a turbulent and anisotropic plasma flow originating at the Sun, are mostly carried out by single spacecraft, resulting in one-dimensional time series.","The conversion of these measurements to the spatial frame of the plasma is a great challenge, but required for direct comparison of the measurements with MHD turbulence theories.","Here we present a toolkit, based on the synthetic modeling of solar wind fluctuations as two-dimensional noise maps with adjustable spectral and power anisotropy, that can help with the temporal-spatial conversion of real data.","Specifically, by following the spacecraft trajectory through a noise map (relative velocity and angle relative to some mean magnetic field) with properties tuned to mimic those of the solar wind, the likelihood that the temporal data fluctuations represent parallel or perpendicular fluctuations in the plasma frame can be quantified by correlating structure functions of the noise map.","Synthetic temporal data can also be generated, which can provide a testing ground for analysis applied to the solar wind data.","We demonstrate this tool by investigating Parker Solar Probe's E7 trajectory and data, and showcase several possible ways in which it can be used.","We find that whether temporal variations in the spacecraft frame come from parallel or perpendicular variations in the plasma frame strongly depends on the spectral and power anisotropy of the measured wind.","Data analysis assisted by such underlying synthetic models as presented here could open up new ways to interpret measurements in the future, specifically in the more reliable determination of plasma frame quantities from temporal measurements."],"url":"http://arxiv.org/abs/2405.12547v1","category":"astro-ph.SR"}
{"created":"2024-05-21 07:18:26","title":"Like Humans to Few-Shot Learning through Knowledge Permeation of Vision and Text","abstract":"Few-shot learning aims to generalize the recognizer from seen categories to an entirely novel scenario. With only a few support samples, several advanced methods initially introduce class names as prior knowledge for identifying novel classes. However, obstacles still impede achieving a comprehensive understanding of how to harness the mutual advantages of visual and textual knowledge. In this paper, we propose a coherent Bidirectional Knowledge Permeation strategy called BiKop, which is grounded in a human intuition: A class name description offers a general representation, whereas an image captures the specificity of individuals. BiKop primarily establishes a hierarchical joint general-specific representation through bidirectional knowledge permeation. On the other hand, considering the bias of joint representation towards the base set, we disentangle base-class-relevant semantics during training, thereby alleviating the suppression of potential novel-class-relevant information. Experiments on four challenging benchmarks demonstrate the remarkable superiority of BiKop. Our code will be publicly available.","sentences":["Few-shot learning aims to generalize the recognizer from seen categories to an entirely novel scenario.","With only a few support samples, several advanced methods initially introduce class names as prior knowledge for identifying novel classes.","However, obstacles still impede achieving a comprehensive understanding of how to harness the mutual advantages of visual and textual knowledge.","In this paper, we propose a coherent Bidirectional Knowledge Permeation strategy called BiKop, which is grounded in a human intuition: A class name description offers a general representation, whereas an image captures the specificity of individuals.","BiKop primarily establishes a hierarchical joint general-specific representation through bidirectional knowledge permeation.","On the other hand, considering the bias of joint representation towards the base set, we disentangle base-class-relevant semantics during training, thereby alleviating the suppression of potential novel-class-relevant information.","Experiments on four challenging benchmarks demonstrate the remarkable superiority of BiKop.","Our code will be publicly available."],"url":"http://arxiv.org/abs/2405.12543v1","category":"cs.CV"}
{"created":"2024-05-21 07:16:12","title":"DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing Outcomes from Sensor Data and Expert Knowledge","abstract":"Large language models (LLMs) have the potential to transform digital healthcare, as evidenced by recent advances in LLM-based virtual doctors. However, current approaches rely on patient's subjective descriptions of symptoms, causing increased misdiagnosis. Recognizing the value of daily data from smart devices, we introduce a novel LLM-based multi-turn consultation virtual doctor system, DrHouse, which incorporates three significant contributions: 1) It utilizes sensor data from smart devices in the diagnosis process, enhancing accuracy and reliability. 2) DrHouse leverages continuously updating medical databases such as Up-to-Date and PubMed to ensure our model remains at diagnostic standard's forefront. 3) DrHouse introduces a novel diagnostic algorithm that concurrently evaluates potential diseases and their likelihood, facilitating more nuanced and informed medical assessments. Through multi-turn interactions, DrHouse determines the next steps, such as accessing daily data from smart devices or requesting in-lab tests, and progressively refines its diagnoses. Evaluations on three public datasets and our self-collected datasets show that DrHouse can achieve up to an 18.8% increase in diagnosis accuracy over the state-of-the-art baselines. The results of a 32-participant user study show that 75% medical experts and 91.7% patients are willing to use DrHouse.","sentences":["Large language models (LLMs) have the potential to transform digital healthcare, as evidenced by recent advances in LLM-based virtual doctors.","However, current approaches rely on patient's subjective descriptions of symptoms, causing increased misdiagnosis.","Recognizing the value of daily data from smart devices, we introduce a novel LLM-based multi-turn consultation virtual doctor system, DrHouse, which incorporates three significant contributions: 1) It utilizes sensor data from smart devices in the diagnosis process, enhancing accuracy and reliability.","2) DrHouse leverages continuously updating medical databases such as Up-to-Date and PubMed to ensure our model remains at diagnostic standard's forefront.","3) DrHouse introduces a novel diagnostic algorithm that concurrently evaluates potential diseases and their likelihood, facilitating more nuanced and informed medical assessments.","Through multi-turn interactions, DrHouse determines the next steps, such as accessing daily data from smart devices or requesting in-lab tests, and progressively refines its diagnoses.","Evaluations on three public datasets and our self-collected datasets show that DrHouse can achieve up to an 18.8% increase in diagnosis accuracy over the state-of-the-art baselines.","The results of a 32-participant user study show that 75% medical experts and 91.7% patients are willing to use DrHouse."],"url":"http://arxiv.org/abs/2405.12541v1","category":"cs.AI"}
{"created":"2024-05-21 07:12:27","title":"Context-Enhanced Video Moment Retrieval with Large Language Models","abstract":"Current methods for Video Moment Retrieval (VMR) struggle to align complex situations involving specific environmental details, character descriptions, and action narratives. To tackle this issue, we propose a Large Language Model-guided Moment Retrieval (LMR) approach that employs the extensive knowledge of Large Language Models (LLMs) to improve video context representation as well as cross-modal alignment, facilitating accurate localization of target moments. Specifically, LMR introduces a context enhancement technique with LLMs to generate crucial target-related context semantics. These semantics are integrated with visual features for producing discriminative video representations. Finally, a language-conditioned transformer is designed to decode free-form language queries, on the fly, using aligned video representations for moment retrieval. Extensive experiments demonstrate that LMR achieves state-of-the-art results, outperforming the nearest competitor by up to 3.28\\% and 4.06\\% on the challenging QVHighlights and Charades-STA benchmarks, respectively. More importantly, the performance gains are significantly higher for localization of complex queries.","sentences":["Current methods for Video Moment Retrieval (VMR) struggle to align complex situations involving specific environmental details, character descriptions, and action narratives.","To tackle this issue, we propose a Large Language Model-guided Moment Retrieval (LMR) approach that employs the extensive knowledge of Large Language Models (LLMs) to improve video context representation as well as cross-modal alignment, facilitating accurate localization of target moments.","Specifically, LMR introduces a context enhancement technique with LLMs to generate crucial target-related context semantics.","These semantics are integrated with visual features for producing discriminative video representations.","Finally, a language-conditioned transformer is designed to decode free-form language queries, on the fly, using aligned video representations for moment retrieval.","Extensive experiments demonstrate that LMR achieves state-of-the-art results, outperforming the nearest competitor by up to 3.28\\% and 4.06\\% on the challenging QVHighlights and Charades-STA benchmarks, respectively.","More importantly, the performance gains are significantly higher for localization of complex queries."],"url":"http://arxiv.org/abs/2405.12540v1","category":"cs.CV"}
{"created":"2024-05-21 07:07:44","title":"Bridging the Intent Gap: Knowledge-Enhanced Visual Generation","abstract":"For visual content generation, discrepancies between user intentions and the generated content have been a longstanding problem. This discrepancy arises from two main factors. First, user intentions are inherently complex, with subtle details not fully captured by input prompts. The absence of such details makes it challenging for generative models to accurately reflect the intended meaning, leading to a mismatch between the desired and generated output. Second, generative models trained on visual-label pairs lack the comprehensive knowledge to accurately represent all aspects of the input data in their generated outputs. To address these challenges, we propose a knowledge-enhanced iterative refinement framework for visual content generation. We begin by analyzing and identifying the key challenges faced by existing generative models. Then, we introduce various knowledge sources, including human insights, pre-trained models, logic rules, and world knowledge, which can be leveraged to address these challenges. Furthermore, we propose a novel visual generation framework that incorporates a knowledge-based feedback module to iteratively refine the generation process. This module gradually improves the alignment between the generated content and user intentions. We demonstrate the efficacy of the proposed framework through preliminary results, highlighting the potential of knowledge-enhanced generative models for intention-aligned content generation.","sentences":["For visual content generation, discrepancies between user intentions and the generated content have been a longstanding problem.","This discrepancy arises from two main factors.","First, user intentions are inherently complex, with subtle details not fully captured by input prompts.","The absence of such details makes it challenging for generative models to accurately reflect the intended meaning, leading to a mismatch between the desired and generated output.","Second, generative models trained on visual-label pairs lack the comprehensive knowledge to accurately represent all aspects of the input data in their generated outputs.","To address these challenges, we propose a knowledge-enhanced iterative refinement framework for visual content generation.","We begin by analyzing and identifying the key challenges faced by existing generative models.","Then, we introduce various knowledge sources, including human insights, pre-trained models, logic rules, and world knowledge, which can be leveraged to address these challenges.","Furthermore, we propose a novel visual generation framework that incorporates a knowledge-based feedback module to iteratively refine the generation process.","This module gradually improves the alignment between the generated content and user intentions.","We demonstrate the efficacy of the proposed framework through preliminary results, highlighting the potential of knowledge-enhanced generative models for intention-aligned content generation."],"url":"http://arxiv.org/abs/2405.12538v1","category":"cs.CV"}
{"created":"2024-05-21 07:00:59","title":"Method of Kinetic Energy Reconstruction from Time-of-Flight Mass Spectra","abstract":"We present a method for the reconstruction of ion kinetic energy distributions from ion time-of-flight mass spectra through ion trajectory simulations. In particular, this method is applicable to complicated spectrometer geometries with largely anisotropic ion collection efficiencies. A calibration procedure using a single ion mass peak allows the accurate determination of parameters related to the spectrometer calibration, experimental alignment and instrument response function, which improves the agreement between simulations and experiment. The calibrated simulation is used to generate a set of basis functions for the time-of-flight spectra, which are then used to transform from time-of-flight to kinetic-energy spectra. We demonstrate this reconstruction method on a recent pump-probe experiment by Asmussen et al. (J. D. Asmussen et al., Phys. Chem. Chem. Phys., 23, 15138, (2021)) on helium nanodroplets and retrieve time-resolved kinetic-energy-release spectra for the ions from ion time-of-flight spectra.","sentences":["We present a method for the reconstruction of ion kinetic energy distributions from ion time-of-flight mass spectra through ion trajectory simulations.","In particular, this method is applicable to complicated spectrometer geometries with largely anisotropic ion collection efficiencies.","A calibration procedure using a single ion mass peak allows the accurate determination of parameters related to the spectrometer calibration, experimental alignment and instrument response function, which improves the agreement between simulations and experiment.","The calibrated simulation is used to generate a set of basis functions for the time-of-flight spectra, which are then used to transform from time-of-flight to kinetic-energy spectra.","We demonstrate this reconstruction method on a recent pump-probe experiment by Asmussen et al.","(J. D. Asmussen et al., Phys.","Chem.","Chem.","Phys., 23, 15138, (2021)) on helium nanodroplets and retrieve time-resolved kinetic-energy-release spectra for the ions from ion time-of-flight spectra."],"url":"http://arxiv.org/abs/2405.12536v1","category":"physics.atm-clus"}
{"created":"2024-05-21 06:53:37","title":"Is Planckian discreteness observable in cosmology?","abstract":"A Planck scale inflationary era -- in a quantum gravity theory predicting discreteness of quantum geometry at the fundamental scale -- produces the scale invariant spectrum of inhomogeneities with very small tensor-to-scalar ratio of perturbations and a hot big bang leading to a natural dark matter genesis scenario. Here we evoke the possibility that some of the major puzzles in cosmology would have an explanation rooted in quantum gravity.","sentences":["A Planck scale inflationary era -- in a quantum gravity theory predicting discreteness of quantum geometry at the fundamental scale -- produces the scale invariant spectrum of inhomogeneities with very small tensor-to-scalar ratio of perturbations and a hot big bang leading to a natural dark matter genesis scenario.","Here we evoke the possibility that some of the major puzzles in cosmology would have an explanation rooted in quantum gravity."],"url":"http://arxiv.org/abs/2405.12534v1","category":"gr-qc"}
{"created":"2024-05-21 06:46:37","title":"PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference","abstract":"Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.","sentences":["Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots.","To accelerate inference, we store computed keys and values (KV cache) in the GPU memory.","Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache.","However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation.","To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights.","Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context.","PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance.","Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache."],"url":"http://arxiv.org/abs/2405.12532v1","category":"cs.CL"}
{"created":"2024-05-21 06:43:03","title":"CustomText: Customized Textual Image Generation using Diffusion Models","abstract":"Textual image generation spans diverse fields like advertising, education, product packaging, social media, information visualization, and branding. Despite recent strides in language-guided image synthesis using diffusion models, current models excel in image generation but struggle with accurate text rendering and offer limited control over font attributes. In this paper, we aim to enhance the synthesis of high-quality images with precise text customization, thereby contributing to the advancement of image generation models. We call our proposed method CustomText. Our implementation leverages a pre-trained TextDiffuser model to enable control over font color, background, and types. Additionally, to address the challenge of accurately rendering small-sized fonts, we train the ControlNet model for a consistency decoder, significantly enhancing text-generation performance. We assess the performance of CustomText in comparison to previous methods of textual image generation on the publicly available CTW-1500 dataset and a self-curated dataset for small-text generation, showcasing superior results.","sentences":["Textual image generation spans diverse fields like advertising, education, product packaging, social media, information visualization, and branding.","Despite recent strides in language-guided image synthesis using diffusion models, current models excel in image generation but struggle with accurate text rendering and offer limited control over font attributes.","In this paper, we aim to enhance the synthesis of high-quality images with precise text customization, thereby contributing to the advancement of image generation models.","We call our proposed method CustomText.","Our implementation leverages a pre-trained TextDiffuser model to enable control over font color, background, and types.","Additionally, to address the challenge of accurately rendering small-sized fonts, we train the ControlNet model for a consistency decoder, significantly enhancing text-generation performance.","We assess the performance of CustomText in comparison to previous methods of textual image generation on the publicly available CTW-1500 dataset and a self-curated dataset for small-text generation, showcasing superior results."],"url":"http://arxiv.org/abs/2405.12531v1","category":"cs.CV"}
{"created":"2024-05-21 06:41:24","title":"Multi-hop Multi-RIS Wireless Communication Systems: Multi-reflection Path Scheduling and Beamforming","abstract":"Reconfigurable intelligent surface (RIS) provides a promising way to proactively augment propagation environments for better transmission performance in wireless communications. Existing multi-RIS works mainly focus on link-level optimization with predetermined transmission paths, which cannot be directly extended to system-level management, since they neither consider the interference caused by undesired scattering of RISs, nor the performance balancing between different transmission paths. To address this, we study an innovative multi-hop multi-RIS communication system, where a base station (BS) transmits information to a set of distributed users over multi-RIS configuration space in a multi-hop manner. The signals for each user are subsequently reflected by the selected RISs via multi-reflection line-of-sight (LoS) links. To ensure that all users have fair access to the system to avoid excessive number of RISs serving one user, we aim to find the optimal beam reflecting path for each user, while judiciously determining the path scheduling strategies with the corresponding beamforming design to ensure the fairness. Due to the presence of interference caused by undesired scattering of RISs, it is highly challenging to solve the formulated multi-RIS multi-path beamforming optimization problem. To solve it, we first derive the optimal RISs' phase shifts and the corresponding reflecting path selection for each user based on its practical deployment location. With the optimized multi-reflection paths, we obtain a feasible user grouping pattern for effective interference mitigation by constructing the maximum independent sets (MISs). Finally, we propose a joint heuristic algorithm to iteratively update the beamforming vectors and the group scheduling policies to maximize the minimum equivalent data rate of all users.","sentences":["Reconfigurable intelligent surface (RIS) provides a promising way to proactively augment propagation environments for better transmission performance in wireless communications.","Existing multi-RIS works mainly focus on link-level optimization with predetermined transmission paths, which cannot be directly extended to system-level management, since they neither consider the interference caused by undesired scattering of RISs, nor the performance balancing between different transmission paths.","To address this, we study an innovative multi-hop multi-RIS communication system, where a base station (BS) transmits information to a set of distributed users over multi-RIS configuration space in a multi-hop manner.","The signals for each user are subsequently reflected by the selected RISs via multi-reflection line-of-sight (LoS) links.","To ensure that all users have fair access to the system to avoid excessive number of RISs serving one user, we aim to find the optimal beam reflecting path for each user, while judiciously determining the path scheduling strategies with the corresponding beamforming design to ensure the fairness.","Due to the presence of interference caused by undesired scattering of RISs, it is highly challenging to solve the formulated multi-RIS multi-path beamforming optimization problem.","To solve it, we first derive the optimal RISs' phase shifts and the corresponding reflecting path selection for each user based on its practical deployment location.","With the optimized multi-reflection paths, we obtain a feasible user grouping pattern for effective interference mitigation by constructing the maximum independent sets (MISs).","Finally, we propose a joint heuristic algorithm to iteratively update the beamforming vectors and the group scheduling policies to maximize the minimum equivalent data rate of all users."],"url":"http://arxiv.org/abs/2405.12530v1","category":"cs.NI"}
{"created":"2024-05-21 06:37:03","title":"SirLLM: Streaming Infinite Retentive LLM","abstract":"As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential. However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs' pre-trained text length, there is a dramatic decline in text generation capabilities. Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs. Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model's long-term memory capabilities.   Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory. We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness. When having a coversation, \"A sir could forget himself,\" but SirLLM never does! Our code is publicly available at https://github.com/Zoeyyao27/SirLLM","sentences":["As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential.","However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs' pre-trained text length, there is a dramatic decline in text generation capabilities.","Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs.","Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model's long-term memory capabilities.   ","Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning.","SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory.","We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors.","Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness.","When having a coversation, \"A sir could forget himself,\" but SirLLM never does!","Our code is publicly available at https://github.com/Zoeyyao27/SirLLM"],"url":"http://arxiv.org/abs/2405.12528v1","category":"cs.CL"}
{"created":"2024-05-21 06:31:55","title":"Enhanced Terahertz Emission from the Wakefield of CO2 Laser-Created Plasma","abstract":"High-field terahertz (THz) pulse generation is investigated through the interaction of an intense single-color CO2 laser pulse with helium (He) gas targets. Employing multi-dimensional Particle-In-Cell (PIC) simulations, this study reveals a substantial enhancement in THz generation efficiency, even with a single-color laser pulse interacting with gas targets in the self-modulated-laser-wakefield (SMLWF) regime. Our study demonstrates that in the presence of photoionization, a synergistic interplay of laser self-modulation, self-focusing, and local pump depletion leads to the generation of robust THz pulses polarized parallel to the laser electric field. The dependence of THz generation efficiency on target density and laser pulse duration has been investigated. Our study identifies a favourable parametric regime for producing THz fields with amplitudes reaching hundreds of GV/m, surpassing those reported in previous studies.","sentences":["High-field terahertz (THz) pulse generation is investigated through the interaction of an intense single-color CO2 laser pulse with helium (He) gas targets.","Employing multi-dimensional Particle-In-Cell (PIC) simulations, this study reveals a substantial enhancement in THz generation efficiency, even with a single-color laser pulse interacting with gas targets in the self-modulated-laser-wakefield (SMLWF) regime.","Our study demonstrates that in the presence of photoionization, a synergistic interplay of laser self-modulation, self-focusing, and local pump depletion leads to the generation of robust THz pulses polarized parallel to the laser electric field.","The dependence of THz generation efficiency on target density and laser pulse duration has been investigated.","Our study identifies a favourable parametric regime for producing THz fields with amplitudes reaching hundreds of GV/m, surpassing those reported in previous studies."],"url":"http://arxiv.org/abs/2405.12526v1","category":"physics.plasm-ph"}
{"created":"2024-05-21 06:27:12","title":"Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models","abstract":"Machine unlearning empowers individuals with the `right to be forgotten' by removing their private or sensitive information encoded in machine learning models. However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts. To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps. SIU consists of two key aspects: (i) Constructing Multifaceted fine-tuning data. We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii) Jointly training loss. To synchronously forget the visual recognition of concepts and preserve the utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence Loss combined with Cross Entropy loss. Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation. Experimental results on MMUBench show that SIU completely surpasses the performance of existing methods. Furthermore, we surprisingly find that SIU can avoid invasive membership inference attacks and jailbreak attacks. To the best of our knowledge, we are the first to explore MU in MLLMs. We will release the code and benchmark in the near future.","sentences":["Machine unlearning empowers individuals with the `right to be forgotten' by removing their private or sensitive information encoded in machine learning models.","However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts.","To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps.","SIU consists of two key aspects: (i) Constructing Multifaceted fine-tuning data.","We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii) Jointly training loss.","To synchronously forget the visual recognition of concepts and preserve the utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence Loss combined with Cross Entropy loss.","Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation.","Experimental results on MMUBench show that SIU completely surpasses the performance of existing methods.","Furthermore, we surprisingly find that SIU can avoid invasive membership inference attacks and jailbreak attacks.","To the best of our knowledge, we are the first to explore MU in MLLMs.","We will release the code and benchmark in the near future."],"url":"http://arxiv.org/abs/2405.12523v1","category":"cs.CV"}
{"created":"2024-05-21 06:23:47","title":"Unleash Graph Neural Networks from Heavy Tuning","abstract":"Graph Neural Networks (GNNs) are deep-learning architectures designed for graph-type data, where understanding relationships among individual observations is crucial. However, achieving promising GNN performance, especially on unseen data, requires comprehensive hyperparameter tuning and meticulous training. Unfortunately, these processes come with high computational costs and significant human effort. Additionally, conventional searching algorithms such as grid search may result in overfitting on validation data, diminishing generalization accuracy. To tackle these challenges, we propose a graph conditional latent diffusion framework (GNN-Diff) to generate high-performing GNNs directly by learning from checkpoints saved during a light-tuning coarse search. Our method: (1) unleashes GNN training from heavy tuning and complex search space design; (2) produces GNN parameters that outperform those obtained through comprehensive grid search; and (3) establishes higher-quality generation for GNNs compared to diffusion frameworks designed for general neural networks.","sentences":["Graph Neural Networks (GNNs) are deep-learning architectures designed for graph-type data, where understanding relationships among individual observations is crucial.","However, achieving promising GNN performance, especially on unseen data, requires comprehensive hyperparameter tuning and meticulous training.","Unfortunately, these processes come with high computational costs and significant human effort.","Additionally, conventional searching algorithms such as grid search may result in overfitting on validation data, diminishing generalization accuracy.","To tackle these challenges, we propose a graph conditional latent diffusion framework (GNN-Diff) to generate high-performing GNNs directly by learning from checkpoints saved during a light-tuning coarse search.","Our method: (1) unleashes GNN training from heavy tuning and complex search space design; (2) produces GNN parameters that outperform those obtained through comprehensive grid search; and (3) establishes higher-quality generation for GNNs compared to diffusion frameworks designed for general neural networks."],"url":"http://arxiv.org/abs/2405.12521v1","category":"cs.LG"}
{"created":"2024-05-21 06:16:42","title":"MOSS: A Large-scale Open Microscopic Traffic Simulation System","abstract":"In the research of Intelligent Transportation Systems (ITS), traffic simulation is a key procedure for the evaluation of new methods and optimization of strategies. However, existing traffic simulation systems face two challenges. First, how to balance simulation scale with realism is a dilemma. Second, it is hard to simulate realistic results, which requires realistic travel demand data and simulator. These problems limit computer-aided optimization of traffic management strategies for large-scale road networks and reduce the usability of traffic simulations in areas where real-world travel demand data are lacking. To address these problems, we design and implement MObility Simulation System (MOSS). MOSS adopts GPU acceleration to significantly improve the efficiency and scale of microscopic traffic simulation, which enables realistic and fast simulations for large-scale road networks. It provides realistic travel Origin-Destination (OD) matrices generation through a pre-trained generative neural network model based on publicly available data on a global scale, such as satellite imagery, to help researchers build meaningful travel demand data. It also provides a complete open toolchain to help users with road network construction, demand generation, simulation, and result analysis. The whole toolchain including the simulator can be accessed at https://moss.fiblab.net and the codes are open-source for community collaboration.","sentences":["In the research of Intelligent Transportation Systems (ITS), traffic simulation is a key procedure for the evaluation of new methods and optimization of strategies.","However, existing traffic simulation systems face two challenges.","First, how to balance simulation scale with realism is a dilemma.","Second, it is hard to simulate realistic results, which requires realistic travel demand data and simulator.","These problems limit computer-aided optimization of traffic management strategies for large-scale road networks and reduce the usability of traffic simulations in areas where real-world travel demand data are lacking.","To address these problems, we design and implement MObility Simulation System (MOSS).","MOSS adopts GPU acceleration to significantly improve the efficiency and scale of microscopic traffic simulation, which enables realistic and fast simulations for large-scale road networks.","It provides realistic travel Origin-Destination (OD) matrices generation through a pre-trained generative neural network model based on publicly available data on a global scale, such as satellite imagery, to help researchers build meaningful travel demand data.","It also provides a complete open toolchain to help users with road network construction, demand generation, simulation, and result analysis.","The whole toolchain including the simulator can be accessed at https://moss.fiblab.net and the codes are open-source for community collaboration."],"url":"http://arxiv.org/abs/2405.12520v1","category":"cs.DC"}
{"created":"2024-05-21 06:12:24","title":"MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation","abstract":"Graph Neural Networks (GNNs) have shown remarkable success in molecular tasks, yet their interpretability remains challenging. Traditional model-level explanation methods like XGNN and GNNInterpreter often fail to identify valid substructures like rings, leading to questionable interpretability. This limitation stems from XGNN's atom-by-atom approach and GNNInterpreter's reliance on average graph embeddings, which overlook the essential structural elements crucial for molecules. To address these gaps, we introduce an innovative \\textbf{M}otif-b\\textbf{A}sed \\textbf{G}NN \\textbf{E}xplainer (MAGE) that uses motifs as fundamental units for generating explanations. Our approach begins with extracting potential motifs through a motif decomposition technique. Then, we utilize an attention-based learning method to identify class-specific motifs. Finally, we employ a motif-based graph generator for each class to create molecular graph explanations based on these class-specific motifs. This novel method not only incorporates critical substructures into the explanations but also guarantees their validity, yielding results that are human-understandable. Our proposed method's effectiveness is demonstrated through quantitative and qualitative assessments conducted on six real-world molecular datasets.","sentences":["Graph Neural Networks (GNNs) have shown remarkable success in molecular tasks, yet their interpretability remains challenging.","Traditional model-level explanation methods like XGNN and GNNInterpreter often fail to identify valid substructures like rings, leading to questionable interpretability.","This limitation stems from XGNN's atom-by-atom approach and GNNInterpreter's reliance on average graph embeddings, which overlook the essential structural elements crucial for molecules.","To address these gaps, we introduce an innovative \\textbf{M}otif-b\\textbf{A}sed \\textbf{G}NN \\textbf{E}xplainer (MAGE) that uses motifs as fundamental units for generating explanations.","Our approach begins with extracting potential motifs through a motif decomposition technique.","Then, we utilize an attention-based learning method to identify class-specific motifs.","Finally, we employ a motif-based graph generator for each class to create molecular graph explanations based on these class-specific motifs.","This novel method not only incorporates critical substructures into the explanations but also guarantees their validity, yielding results that are human-understandable.","Our proposed method's effectiveness is demonstrated through quantitative and qualitative assessments conducted on six real-world molecular datasets."],"url":"http://arxiv.org/abs/2405.12519v1","category":"cs.LG"}
{"created":"2024-05-21 06:03:54","title":"Generalized Baker's result and stability of functional equations using fixed point results","abstract":"Hyre-Ulam stability of functional equation in single variable is studied in non-triangular metric spaces. We derive it as applications of some fixed point results developed on the said structure. A general version of Baker's theorem is also deduced as a consequence.","sentences":["Hyre-Ulam stability of functional equation in single variable is studied in non-triangular metric spaces.","We derive it as applications of some fixed point results developed on the said structure.","A general version of Baker's theorem is also deduced as a consequence."],"url":"http://arxiv.org/abs/2405.12515v1","category":"math.FA"}
{"created":"2024-05-21 06:00:51","title":"Future You: A Conversation with an AI-Generated Future Self Reduces Anxiety, Negative Emotions, and Increases Future Self-Continuity","abstract":"We introduce \"Future You,\" an interactive, brief, single-session, digital chat intervention designed to improve future self-continuity--the degree of connection an individual feels with a temporally distant future self--a characteristic that is positively related to mental health and wellbeing. Our system allows users to chat with a relatable yet AI-powered virtual version of their future selves that is tuned to their future goals and personal qualities. To make the conversation realistic, the system generates a \"synthetic memory\"--a unique backstory for each user--that creates a throughline between the user's present age (between 18-30) and their life at age 60. The \"Future You\" character also adopts the persona of an age-progressed image of the user's present self. After a brief interaction with the \"Future You\" character, users reported decreased anxiety, and increased future self-continuity. This is the first study successfully demonstrating the use of personalized AI-generated characters to improve users' future self-continuity and wellbeing.","sentences":["We introduce \"Future You,\" an interactive, brief, single-session, digital chat intervention designed to improve future self-continuity--the degree of connection an individual feels with a temporally distant future self--a characteristic that is positively related to mental health and wellbeing.","Our system allows users to chat with a relatable yet AI-powered virtual version of their future selves that is tuned to their future goals and personal qualities.","To make the conversation realistic, the system generates a \"synthetic memory\"--a unique backstory for each user--that creates a throughline between the user's present age (between 18-30) and their life at age 60.","The \"Future You\" character also adopts the persona of an age-progressed image of the user's present self.","After a brief interaction with the \"Future You\" character, users reported decreased anxiety, and increased future self-continuity.","This is the first study successfully demonstrating the use of personalized AI-generated characters to improve users' future self-continuity and wellbeing."],"url":"http://arxiv.org/abs/2405.12514v1","category":"cs.HC"}
{"created":"2024-05-21 05:54:27","title":"Fully Randomized Pointers","abstract":"Software security continues to be a critical concern for programs implemented in low-level programming languages such as C and C++. Many defenses have been proposed in the current literature, each with different trade-offs including performance, compatibility, and attack resistance. One general class of defense is pointer randomization or authentication, where invalid object access (e.g., memory errors) is obfuscated or denied. Many defenses rely on the program termination (e.g., crashing) to abort attacks, with the implicit assumption that an adversary cannot \"brute force\" the defense with multiple attack attempts. However, such assumptions do not always hold, such as hardware speculative execution attacks or network servers configured to restart on error. In such cases, we argue that most existing defenses provide only weak effective security.   In this paper, we propose Fully Randomized Pointers (FRP) as a stronger memory error defense that is resistant to even brute force attacks. The key idea is to fully randomize pointer bits -- as much as possible while also preserving binary compatibility -- rendering the relationships between pointers highly unpredictable. Furthermore, the very high degree of randomization renders brute force attacks impractical -- providing strong effective security compared to existing work. We design a new FRP encoding that is: (1) compatible with existing binary code (without recompilation); (2) decoupled from the underlying object layout; and (3) can be efficiently decoded on-the-fly to the underlying memory address. We prototype FRP in the form of a software implementation (BlueFat) to test security and compatibility, and a proof-of-concept hardware implementation (GreenFat) to evaluate performance. We show that FRP is secure, practical, and compatible at the binary level, while a hardware implementation can achieve low performance overheads (<10%).","sentences":["Software security continues to be a critical concern for programs implemented in low-level programming languages such as C and C++.","Many defenses have been proposed in the current literature, each with different trade-offs including performance, compatibility, and attack resistance.","One general class of defense is pointer randomization or authentication, where invalid object access (e.g., memory errors) is obfuscated or denied.","Many defenses rely on the program termination (e.g., crashing) to abort attacks, with the implicit assumption that an adversary cannot \"brute force\" the defense with multiple attack attempts.","However, such assumptions do not always hold, such as hardware speculative execution attacks or network servers configured to restart on error.","In such cases, we argue that most existing defenses provide only weak effective security.   ","In this paper, we propose Fully Randomized Pointers (FRP) as a stronger memory error defense that is resistant to even brute force attacks.","The key idea is to fully randomize pointer bits -- as much as possible while also preserving binary compatibility -- rendering the relationships between pointers highly unpredictable.","Furthermore, the very high degree of randomization renders brute force attacks impractical -- providing strong effective security compared to existing work.","We design a new FRP encoding that is: (1) compatible with existing binary code (without recompilation); (2) decoupled from the underlying object layout; and (3) can be efficiently decoded on-the-fly to the underlying memory address.","We prototype FRP in the form of a software implementation (BlueFat) to test security and compatibility, and a proof-of-concept hardware implementation (GreenFat) to evaluate performance.","We show that FRP is secure, practical, and compatible at the binary level, while a hardware implementation can achieve low performance overheads (<10%)."],"url":"http://arxiv.org/abs/2405.12513v1","category":"cs.CR"}
{"created":"2024-05-21 05:47:42","title":"Rethink Predicting the Optical Flow with the Kinetics Perspective","abstract":"Optical flow estimation is one of the fundamental tasks in low-level computer vision, which describes the pixel-wise displacement and can be used in many other tasks. From the apparent aspect, the optical flow can be viewed as the correlation between the pixels in consecutive frames, so continuously refining the correlation volume can achieve an outstanding performance. However, it will make the method have a catastrophic computational complexity. Not only that, the error caused by the occlusion regions of the successive frames will be amplified through the inaccurate warp operation. These challenges can not be solved only from the apparent view, so this paper rethinks the optical flow estimation from the kinetics viewpoint.We propose a method combining the apparent and kinetics information from this motivation. The proposed method directly predicts the optical flow from the feature extracted from images instead of building the correlation volume, which will improve the efficiency of the whole network. Meanwhile, the proposed method involves a new differentiable warp operation that simultaneously considers the warping and occlusion. Moreover, the proposed method blends the kinetics feature with the apparent feature through the novel self-supervised loss function. Furthermore, comprehensive experiments and ablation studies prove that the proposed novel insight into how to predict the optical flow can achieve the better performance of the state-of-the-art methods, and in some metrics, the proposed method outperforms the correlation-based method, especially in situations containing occlusion and fast moving. The code will be public.","sentences":["Optical flow estimation is one of the fundamental tasks in low-level computer vision, which describes the pixel-wise displacement and can be used in many other tasks.","From the apparent aspect, the optical flow can be viewed as the correlation between the pixels in consecutive frames, so continuously refining the correlation volume can achieve an outstanding performance.","However, it will make the method have a catastrophic computational complexity.","Not only that, the error caused by the occlusion regions of the successive frames will be amplified through the inaccurate warp operation.","These challenges can not be solved only from the apparent view, so this paper rethinks the optical flow estimation from the kinetics viewpoint.","We propose a method combining the apparent and kinetics information from this motivation.","The proposed method directly predicts the optical flow from the feature extracted from images instead of building the correlation volume, which will improve the efficiency of the whole network.","Meanwhile, the proposed method involves a new differentiable warp operation that simultaneously considers the warping and occlusion.","Moreover, the proposed method blends the kinetics feature with the apparent feature through the novel self-supervised loss function.","Furthermore, comprehensive experiments and ablation studies prove that the proposed novel insight into how to predict the optical flow can achieve the better performance of the state-of-the-art methods, and in some metrics, the proposed method outperforms the correlation-based method, especially in situations containing occlusion and fast moving.","The code will be public."],"url":"http://arxiv.org/abs/2405.12512v1","category":"cs.CV"}
{"created":"2024-05-21 05:47:02","title":"Quantum Computing for Databases: Overview and Challenges","abstract":"In the decades, the general field of quantum computing has experienced remarkable progress since its inception. A plethora of researchers not only proposed quantum algorithms showing the power of quantum computing but also constructed the prototype of quantum computers, making it walk into our tangible reality. Those remarkable advancements in quantum computing have opened doors for novel applications, one of which is quantum databases. Researchers are trying to use a paradigm brought by quantum computing to revolutionize various aspects of database management systems. In this paper, we envision the synergy between quantum computing and databases with two perspectives: Quantum computing-enabled technology, and quantum computing-inspired technology. Based on this classification, we present a detailed overview of the research attained in this area, aiming to show the landscape of the field and draw a road map of future directions.","sentences":["In the decades, the general field of quantum computing has experienced remarkable progress since its inception.","A plethora of researchers not only proposed quantum algorithms showing the power of quantum computing but also constructed the prototype of quantum computers, making it walk into our tangible reality.","Those remarkable advancements in quantum computing have opened doors for novel applications, one of which is quantum databases.","Researchers are trying to use a paradigm brought by quantum computing to revolutionize various aspects of database management systems.","In this paper, we envision the synergy between quantum computing and databases with two perspectives: Quantum computing-enabled technology, and quantum computing-inspired technology.","Based on this classification, we present a detailed overview of the research attained in this area, aiming to show the landscape of the field and draw a road map of future directions."],"url":"http://arxiv.org/abs/2405.12511v1","category":"cs.DB"}
{"created":"2024-05-21 05:31:03","title":"NOVA-3D: Non-overlapped Views for 3D Anime Character Reconstruction","abstract":"In the animation industry, 3D modelers typically rely on front and back non-overlapped concept designs to guide the 3D modeling of anime characters. However, there is currently a lack of automated approaches for generating anime characters directly from these 2D designs. In light of this, we explore a novel task of reconstructing anime characters from non-overlapped views. This presents two main challenges: existing multi-view approaches cannot be directly applied due to the absence of overlapping regions, and there is a scarcity of full-body anime character data and standard benchmarks. To bridge the gap, we present Non-Overlapped Views for 3D \\textbf{A}nime Character Reconstruction (NOVA-3D), a new framework that implements a method for view-aware feature fusion to learn 3D-consistent features effectively and synthesizes full-body anime characters from non-overlapped front and back views directly. To facilitate this line of research, we collected the NOVA-Human dataset, which comprises multi-view images and accurate camera parameters for 3D anime characters. Extensive experiments demonstrate that the proposed method outperforms baseline approaches, achieving superior reconstruction of anime characters with exceptional detail fidelity. In addition, to further verify the effectiveness of our method, we applied it to the animation head reconstruction task and improved the state-of-the-art baseline to 94.453 in SSIM, 7.726 in LPIPS, and 19.575 in PSNR on average. Codes and datasets are available at https://wanghongsheng01.github.io/NOVA-3D/.","sentences":["In the animation industry, 3D modelers typically rely on front and back non-overlapped concept designs to guide the 3D modeling of anime characters.","However, there is currently a lack of automated approaches for generating anime characters directly from these 2D designs.","In light of this, we explore a novel task of reconstructing anime characters from non-overlapped views.","This presents two main challenges: existing multi-view approaches cannot be directly applied due to the absence of overlapping regions, and there is a scarcity of full-body anime character data and standard benchmarks.","To bridge the gap, we present Non-Overlapped Views for 3D \\textbf{A}nime Character Reconstruction (NOVA-3D), a new framework that implements a method for view-aware feature fusion to learn 3D-consistent features effectively and synthesizes full-body anime characters from non-overlapped front and back views directly.","To facilitate this line of research, we collected the NOVA-Human dataset, which comprises multi-view images and accurate camera parameters for 3D anime characters.","Extensive experiments demonstrate that the proposed method outperforms baseline approaches, achieving superior reconstruction of anime characters with exceptional detail fidelity.","In addition, to further verify the effectiveness of our method, we applied it to the animation head reconstruction task and improved the state-of-the-art baseline to 94.453 in SSIM, 7.726 in LPIPS, and 19.575 in PSNR on average.","Codes and datasets are available at https://wanghongsheng01.github.io/NOVA-3D/."],"url":"http://arxiv.org/abs/2405.12505v1","category":"cs.CV"}
{"created":"2024-05-21 05:20:04","title":"CLRKDNet: Speeding up Lane Detection with Knowledge Distillation","abstract":"Road lanes are integral components of the visual perception systems in intelligent vehicles, playing a pivotal role in safe navigation. In lane detection tasks, balancing accuracy with real-time performance is essential, yet existing methods often sacrifice one for the other. To address this trade-off, we introduce CLRKDNet, a streamlined model that balances detection accuracy with real-time performance. The state-of-the-art model CLRNet has demonstrated exceptional performance across various datasets, yet its computational overhead is substantial due to its Feature Pyramid Network (FPN) and muti-layer detection head architecture. Our method simplifies both the FPN structure and detection heads, redesigning them to incorporate a novel teacher-student distillation process alongside a newly introduced series of distillation losses. This combination reduces inference time by up to 60% while maintaining detection accuracy comparable to CLRNet. This strategic balance of accuracy and speed makes CLRKDNet a viable solution for real-time lane detection tasks in autonomous driving applications.","sentences":["Road lanes are integral components of the visual perception systems in intelligent vehicles, playing a pivotal role in safe navigation.","In lane detection tasks, balancing accuracy with real-time performance is essential, yet existing methods often sacrifice one for the other.","To address this trade-off, we introduce CLRKDNet, a streamlined model that balances detection accuracy with real-time performance.","The state-of-the-art model CLRNet has demonstrated exceptional performance across various datasets, yet its computational overhead is substantial due to its Feature Pyramid Network (FPN) and muti-layer detection head architecture.","Our method simplifies both the FPN structure and detection heads, redesigning them to incorporate a novel teacher-student distillation process alongside a newly introduced series of distillation losses.","This combination reduces inference time by up to 60% while maintaining detection accuracy comparable to CLRNet.","This strategic balance of accuracy and speed makes CLRKDNet a viable solution for real-time lane detection tasks in autonomous driving applications."],"url":"http://arxiv.org/abs/2405.12503v1","category":"cs.CV"}
{"created":"2024-05-21 05:17:43","title":"EntropyStop: Unsupervised Deep Outlier Detection with Loss Entropy","abstract":"Unsupervised Outlier Detection (UOD) is an important data mining task. With the advance of deep learning, deep Outlier Detection (OD) has received broad interest. Most deep UOD models are trained exclusively on clean datasets to learn the distribution of the normal data, which requires huge manual efforts to clean the real-world data if possible. Instead of relying on clean datasets, some approaches directly train and detect on unlabeled contaminated datasets, leading to the need for methods that are robust to such conditions. Ensemble methods emerged as a superior solution to enhance model robustness against contaminated training sets. However, the training time is greatly increased by the ensemble.   In this study, we investigate the impact of outliers on the training phase, aiming to halt training on unlabeled contaminated datasets before performance degradation. Initially, we noted that blending normal and anomalous data causes AUC fluctuations, a label-dependent measure of detection accuracy. To circumvent the need for labels, we propose a zero-label entropy metric named Loss Entropy for loss distribution, enabling us to infer optimal stopping points for training without labels. Meanwhile, we theoretically demonstrate negative correlation between entropy metric and the label-based AUC. Based on this, we develop an automated early-stopping algorithm, EntropyStop, which halts training when loss entropy suggests the maximum model detection capability. We conduct extensive experiments on ADBench (including 47 real datasets), and the overall results indicate that AutoEncoder (AE) enhanced by our approach not only achieves better performance than ensemble AEs but also requires under 1\\% of training time. Lastly, our proposed metric and early-stopping approach are evaluated on other deep OD models, exhibiting their broad potential applicability.","sentences":["Unsupervised Outlier Detection (UOD) is an important data mining task.","With the advance of deep learning, deep Outlier Detection (OD) has received broad interest.","Most deep UOD models are trained exclusively on clean datasets to learn the distribution of the normal data, which requires huge manual efforts to clean the real-world data if possible.","Instead of relying on clean datasets, some approaches directly train and detect on unlabeled contaminated datasets, leading to the need for methods that are robust to such conditions.","Ensemble methods emerged as a superior solution to enhance model robustness against contaminated training sets.","However, the training time is greatly increased by the ensemble.   ","In this study, we investigate the impact of outliers on the training phase, aiming to halt training on unlabeled contaminated datasets before performance degradation.","Initially, we noted that blending normal and anomalous data causes AUC fluctuations, a label-dependent measure of detection accuracy.","To circumvent the need for labels, we propose a zero-label entropy metric named Loss Entropy for loss distribution, enabling us to infer optimal stopping points for training without labels.","Meanwhile, we theoretically demonstrate negative correlation between entropy metric and the label-based AUC.","Based on this, we develop an automated early-stopping algorithm, EntropyStop, which halts training when loss entropy suggests the maximum model detection capability.","We conduct extensive experiments on ADBench (including 47 real datasets), and the overall results indicate that AutoEncoder (AE) enhanced by our approach not only achieves better performance than ensemble AEs but also requires under 1\\% of training time.","Lastly, our proposed metric and early-stopping approach are evaluated on other deep OD models, exhibiting their broad potential applicability."],"url":"http://arxiv.org/abs/2405.12502v1","category":"cs.LG"}
{"created":"2024-05-21 15:43:25","title":"Out of equilibrium response and fluctuation-dissipation violations across scales in flocking systems","abstract":"Flocking systems are known to be strongly out of equilibrium. Energy input occurs at the individual level to ensure self-propulsion, and the individual motility in turn contributes to ordering, enhancing information propagation and strengthening collective motion. However, even beyond ordering, a crucial feature of natural aggregations is response. How, then, do off-equilibrium features affect the response of the system? In this work, we consider a minimal model of flocking and investigate response behavior under directional perturbations. We show that equilibrium dynamical fluctuation-dissipation relations between response and correlations are violated, both at the local and at the global level. The amount of violation peaks at the ordering transition, exactly as for the entropy production rate. Entropy is always produced locally and connected to the local fluctuation-dissipation violation via Harada-Sasa relationships. However, cooperative mechanisms close to the transition spread off-equilibrium effects to the whole system, producing an out of equilibrium response on the global scale. Our findings elucidate the role of activity and interactions in the cost repartition of collective behavior and explain what observed in experiments on natural living groups.","sentences":["Flocking systems are known to be strongly out of equilibrium.","Energy input occurs at the individual level to ensure self-propulsion, and the individual motility in turn contributes to ordering, enhancing information propagation and strengthening collective motion.","However, even beyond ordering, a crucial feature of natural aggregations is response.","How, then, do off-equilibrium features affect the response of the system?","In this work, we consider a minimal model of flocking and investigate response behavior under directional perturbations.","We show that equilibrium dynamical fluctuation-dissipation relations between response and correlations are violated, both at the local and at the global level.","The amount of violation peaks at the ordering transition, exactly as for the entropy production rate.","Entropy is always produced locally and connected to the local fluctuation-dissipation violation via Harada-Sasa relationships.","However, cooperative mechanisms close to the transition spread off-equilibrium effects to the whole system, producing an out of equilibrium response on the global scale.","Our findings elucidate the role of activity and interactions in the cost repartition of collective behavior and explain what observed in experiments on natural living groups."],"url":"http://arxiv.org/abs/2405.12874v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-21 14:57:04","title":"A Dataset and Baselines for Measuring and Predicting the Music Piece Memorability","abstract":"Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks. Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity. Inspired by this phenomenon, we focus on measuring and predicting music memorability. To achieve this, we collect a new music piece dataset with reliable memorability labels using a novel interactive experimental procedure. We then train baselines to predict and analyze music memorability, leveraging both interpretable features and audio mel-spectrograms as inputs. To the best of our knowledge, we are the first to explore music memorability using data-driven deep learning-based methods. Through a series of experiments and ablation studies, we demonstrate that while there is room for improvement, predicting music memorability with limited data is possible. Certain intrinsic elements, such as higher valence, arousal, and faster tempo, contribute to memorable music. As prediction techniques continue to evolve, real-life applications like music recommendation systems and music style transfer will undoubtedly benefit from this new area of research.","sentences":["Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks.","Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity.","Inspired by this phenomenon, we focus on measuring and predicting music memorability.","To achieve this, we collect a new music piece dataset with reliable memorability labels using a novel interactive experimental procedure.","We then train baselines to predict and analyze music memorability, leveraging both interpretable features and audio mel-spectrograms as inputs.","To the best of our knowledge, we are the first to explore music memorability using data-driven deep learning-based methods.","Through a series of experiments and ablation studies, we demonstrate that while there is room for improvement, predicting music memorability with limited data is possible.","Certain intrinsic elements, such as higher valence, arousal, and faster tempo, contribute to memorable music.","As prediction techniques continue to evolve, real-life applications like music recommendation systems and music style transfer will undoubtedly benefit from this new area of research."],"url":"http://arxiv.org/abs/2405.12847v1","category":"cs.IR"}
{"created":"2024-05-21 13:58:19","title":"Precision measurement of the branching fraction of \\boldmath $J/\u03c8\\rightarrow K^+K^-$ via $\u03c8(2S)\\rightarrow \u03c0^+\u03c0^-J/\u03c8$","abstract":"Using a sample of $448.1 \\times 10^6$ $\\psi(2S)$ events collected with the BESIII detector, we perform a study of the decay $J/\\psi\\rightarrow K^+K^-$ via $\\psi(2S)\\rightarrow \\pi^+\\pi^-J/\\psi$.   The branching fraction of $J/\\psi\\rightarrow K^+K^-$ is determined to be $\\mathcal{B}_{K^+K^-}=(3.072\\pm 0.023({\\rm stat.})\\pm 0.050({\\rm syst.}))\\times 10^{-4}$, which is consistent with previous measurements but with significantly improved precision.","sentences":["Using a sample of $448.1 \\times 10^6$ $\\psi(2S)$ events collected with the BESIII detector, we perform a study of the decay $J/\\psi\\rightarrow K^+K^-$ via $\\psi(2S)\\rightarrow \\pi^+\\pi^-J/\\psi$.   The branching fraction of $J/\\psi\\rightarrow K^+K^-$ is determined to be $\\mathcal{B}_{K^+K^-}=(3.072\\pm 0.023({\\rm stat.})\\pm 0.050({\\rm syst.}))\\times 10^{-4}$, which is consistent with previous measurements but with significantly improved precision."],"url":"http://arxiv.org/abs/2405.12809v1","category":"hep-ex"}
{"created":"2024-05-21 13:48:07","title":"Refined Graph Encoder Embedding via Self-Training and Latent Community Recovery","abstract":"This paper introduces a refined graph encoder embedding method, enhancing the original graph encoder embedding using linear transformation, self-training, and hidden community recovery within observed communities. We provide the theoretical rationale for the refinement procedure, demonstrating how and why our proposed method can effectively identify useful hidden communities via stochastic block models, and how the refinement method leads to improved vertex embedding and better decision boundaries for subsequent vertex classification. The efficacy of our approach is validated through a collection of simulated and real-world graph data.","sentences":["This paper introduces a refined graph encoder embedding method, enhancing the original graph encoder embedding using linear transformation, self-training, and hidden community recovery within observed communities.","We provide the theoretical rationale for the refinement procedure, demonstrating how and why our proposed method can effectively identify useful hidden communities via stochastic block models, and how the refinement method leads to improved vertex embedding and better decision boundaries for subsequent vertex classification.","The efficacy of our approach is validated through a collection of simulated and real-world graph data."],"url":"http://arxiv.org/abs/2405.12797v1","category":"cs.SI"}
{"created":"2024-05-21 13:40:30","title":"Anticipating Object State Changes","abstract":"Anticipating object state changes in images and videos is a challenging problem whose solution has important implications in vision-based scene understanding, automated monitoring systems, and action planning. In this work, we propose the first method for solving this problem. The proposed method predicts object state changes that will occur in the near future as a result of yet unseen human actions. To address this new problem, we propose a novel framework that integrates learnt visual features that represent the recent visual information, with natural language (NLP) features that represent past object state changes and actions. Leveraging the extensive and challenging Ego4D dataset which provides a large-scale collection of first-person perspective videos across numerous interaction scenarios, we introduce new curated annotation data for the object state change anticipation task (OSCA), noted as Ego4D-OSCA. An extensive experimental evaluation was conducted that demonstrates the efficacy of the proposed method in predicting object state changes in dynamic scenarios. The proposed work underscores the potential of integrating video and linguistic cues to enhance the predictive performance of video understanding systems. Moreover, it lays the groundwork for future research on the new task of object state change anticipation. The source code and the new annotation data (Ego4D-OSCA) will be made publicly available.","sentences":["Anticipating object state changes in images and videos is a challenging problem whose solution has important implications in vision-based scene understanding, automated monitoring systems, and action planning.","In this work, we propose the first method for solving this problem.","The proposed method predicts object state changes that will occur in the near future as a result of yet unseen human actions.","To address this new problem, we propose a novel framework that integrates learnt visual features that represent the recent visual information, with natural language (NLP) features that represent past object state changes and actions.","Leveraging the extensive and challenging Ego4D dataset which provides a large-scale collection of first-person perspective videos across numerous interaction scenarios, we introduce new curated annotation data for the object state change anticipation task (OSCA), noted as Ego4D-OSCA.","An extensive experimental evaluation was conducted that demonstrates the efficacy of the proposed method in predicting object state changes in dynamic scenarios.","The proposed work underscores the potential of integrating video and linguistic cues to enhance the predictive performance of video understanding systems.","Moreover, it lays the groundwork for future research on the new task of object state change anticipation.","The source code and the new annotation data (Ego4D-OSCA) will be made publicly available."],"url":"http://arxiv.org/abs/2405.12789v1","category":"cs.CV"}
{"created":"2024-05-21 11:40:39","title":"Goanna: Resolving Haskell Type Errors With Minimal Correction Subsets","abstract":"Statically typed languages offer significant advantages, such as bug prevention, enhanced code quality, and reduced maintenance costs. However, these benefits often come at the expense of a steep learning curve and a slower development pace. Haskell, known for its expressive and strict type system, poses challenges for inexperienced programmers in learning and using its type system, especially in debugging type errors. We introduce Goanna, a novel tool that serves as a type checker and an interactive type error debugging tool for Haskell. When encountering type errors, Goanna identifies a comprehensive list of potential causes and resolutions based on the minimum correction subsets (MCS) enumeration. We evaluated Goanna's effectiveness using 86 diverse Haskell programs from online discourse, demonstrating its ability to accurately identify and resolve type errors. Additionally, we present a collection of techniques and heuristics to enhance Goanna's suggestion-based error diagnosis and show their effectiveness from our evaluation.","sentences":["Statically typed languages offer significant advantages, such as bug prevention, enhanced code quality, and reduced maintenance costs.","However, these benefits often come at the expense of a steep learning curve and a slower development pace.","Haskell, known for its expressive and strict type system, poses challenges for inexperienced programmers in learning and using its type system, especially in debugging type errors.","We introduce Goanna, a novel tool that serves as a type checker and an interactive type error debugging tool for Haskell.","When encountering type errors, Goanna identifies a comprehensive list of potential causes and resolutions based on the minimum correction subsets (MCS) enumeration.","We evaluated Goanna's effectiveness using 86 diverse Haskell programs from online discourse, demonstrating its ability to accurately identify and resolve type errors.","Additionally, we present a collection of techniques and heuristics to enhance Goanna's suggestion-based error diagnosis and show their effectiveness from our evaluation."],"url":"http://arxiv.org/abs/2405.12697v1","category":"cs.HC"}
{"created":"2024-05-21 11:22:20","title":"Study of $b$-hadron decays to $\u039b_c^+ h^- h^{\\prime -}$ final states","abstract":"Decays of $\\Xi_b^-$ and $\\Omega_b^-$ baryons to $\\Lambda_c^+ h^- h^{\\prime -}$ final states, with $h^- h^{\\prime -}$ being $\\pi^-\\pi^-$, $K^-\\pi^-$ and $K^-K^-$ meson pairs, are searched for using data collected with the LHCb detector. The data sample studied corresponds to an integrated luminosity of $8.7\\,\\mathrm{fb}^{-1}$ of $pp$ collisions collected at centre-of-mass energies $\\sqrt{s} = 7$, $8$ and $13\\,\\mathrm{Te\\kern -0.1em V}$. The products of the relative branching fractions and fragmentation fractions for each signal mode, relative to the $B^- \\to \\Lambda_c^+ \\overline{p} \\pi^-$ mode, are measured, with $\\Xi_{b}^- \\to\\Lambda_{c}^+ K^- \\pi^-$, $\\Xi_{b}^- \\to\\Lambda_{c}^+ K^- K^-$ and $\\Omega_{b}^- \\to\\Lambda_{c}^+ K^- K^-$ decays being observed at over $5\\,\\sigma$ significance. The $\\Xi_{b}^- \\to\\Lambda_{c}^+ K^- \\pi^-$ mode is also used to measure the $\\Xi_{b}^-$ production asymmetry, which is found to be consistent with zero. In addition, the $B^- \\to \\Lambda_{c}^+ \\overline{p} K^-$ decay is observed for the first time, and its branching fraction is measured relative to that of the $B^- \\to \\Lambda_{c}^+ \\overline{p} \\pi^-$ mode.","sentences":["Decays of $\\Xi_b^-$ and $\\Omega_b^-$ baryons to $\\Lambda_c^+ h^- h^{\\prime -}$ final states, with $h^- h^{\\prime -}$ being $\\pi^-\\pi^-$, $K^-\\pi^-$ and $K^-K^-$ meson pairs, are searched for using data collected with the LHCb detector.","The data sample studied corresponds to an integrated luminosity of $8.7\\,\\mathrm{fb}^{-1}$ of $pp$ collisions collected at centre-of-mass energies $\\sqrt{s} = 7$, $8$ and $13\\,\\mathrm{Te\\kern -0.1em","V}$.","The products of the relative branching fractions and fragmentation fractions for each signal mode, relative to the $B^- \\to \\Lambda_c^+ \\overline{p} \\pi^-$ mode, are measured, with $\\Xi_{b}^- \\to\\Lambda_{c}^+ K^- \\pi^-$, $\\Xi_{b}^- \\to\\Lambda_{c}^+ K^- K^-$ and $\\Omega_{b}^- \\to\\Lambda_{c}^+ K^- K^-$ decays being observed at over $5\\,\\sigma$ significance.","The $\\Xi_{b}^- \\to\\Lambda_{c}^+ K^- \\pi^-$ mode is also used to measure the $\\Xi_{b}^-$ production asymmetry, which is found to be consistent with zero.","In addition, the $B^- \\to \\Lambda_{c}^+ \\overline{p} K^-$ decay is observed for the first time, and its branching fraction is measured relative to that of the $B^- \\to \\Lambda_{c}^+ \\overline{p} \\pi^-$ mode."],"url":"http://arxiv.org/abs/2405.12688v1","category":"hep-ex"}
{"created":"2024-05-21 09:16:38","title":"MentalQA: An Annotated Arabic Corpus for Questions and Answers of Mental Healthcare","abstract":"Mental health disorders significantly impact people globally, regardless of background, education, or socioeconomic status. However, access to adequate care remains a challenge, particularly for underserved communities with limited resources. Text mining tools offer immense potential to support mental healthcare by assisting professionals in diagnosing and treating patients. This study addresses the scarcity of Arabic mental health resources for developing such tools. We introduce MentalQA, a novel Arabic dataset featuring conversational-style question-and-answer (QA) interactions. To ensure data quality, we conducted a rigorous annotation process using a well-defined schema with quality control measures. Data was collected from a question-answering medical platform. The annotation schema for mental health questions and corresponding answers draws upon existing classification schemes with some modifications. Question types encompass six distinct categories: diagnosis, treatment, anatomy \\& physiology, epidemiology, healthy lifestyle, and provider choice. Answer strategies include information provision, direct guidance, and emotional support. Three experienced annotators collaboratively annotated the data to ensure consistency. Our findings demonstrate high inter-annotator agreement, with Fleiss' Kappa of $0.61$ for question types and $0.98$ for answer strategies. In-depth analysis revealed insightful patterns, including variations in question preferences across age groups and a strong correlation between question types and answer strategies. MentalQA offers a valuable foundation for developing Arabic text mining tools capable of supporting mental health professionals and individuals seeking information.","sentences":["Mental health disorders significantly impact people globally, regardless of background, education, or socioeconomic status.","However, access to adequate care remains a challenge, particularly for underserved communities with limited resources.","Text mining tools offer immense potential to support mental healthcare by assisting professionals in diagnosing and treating patients.","This study addresses the scarcity of Arabic mental health resources for developing such tools.","We introduce MentalQA, a novel Arabic dataset featuring conversational-style question-and-answer (QA) interactions.","To ensure data quality, we conducted a rigorous annotation process using a well-defined schema with quality control measures.","Data was collected from a question-answering medical platform.","The annotation schema for mental health questions and corresponding answers draws upon existing classification schemes with some modifications.","Question types encompass six distinct categories: diagnosis, treatment, anatomy \\& physiology, epidemiology, healthy lifestyle, and provider choice.","Answer strategies include information provision, direct guidance, and emotional support.","Three experienced annotators collaboratively annotated the data to ensure consistency.","Our findings demonstrate high inter-annotator agreement, with Fleiss' Kappa of $0.61$ for question types and $0.98$ for answer strategies.","In-depth analysis revealed insightful patterns, including variations in question preferences across age groups and a strong correlation between question types and answer strategies.","MentalQA offers a valuable foundation for developing Arabic text mining tools capable of supporting mental health professionals and individuals seeking information."],"url":"http://arxiv.org/abs/2405.12619v1","category":"cs.CL"}
{"created":"2024-05-21 09:01:00","title":"S3O: A Dual-Phase Approach for Reconstructing Dynamic Shape and Skeleton of Articulated Objects from Single Monocular Video","abstract":"Reconstructing dynamic articulated objects from a singular monocular video is challenging, requiring joint estimation of shape, motion, and camera parameters from limited views. Current methods typically demand extensive computational resources and training time, and require additional human annotations such as predefined parametric models, camera poses, and key points, limiting their generalizability. We propose Synergistic Shape and Skeleton Optimization (S3O), a novel two-phase method that forgoes these prerequisites and efficiently learns parametric models including visible shapes and underlying skeletons. Conventional strategies typically learn all parameters simultaneously, leading to interdependencies where a single incorrect prediction can result in significant errors. In contrast, S3O adopts a phased approach: it first focuses on learning coarse parametric models, then progresses to motion learning and detail addition. This method substantially lowers computational complexity and enhances robustness in reconstruction from limited viewpoints, all without requiring additional annotations. To address the current inadequacies in 3D reconstruction from monocular video benchmarks, we collected the PlanetZoo dataset. Our experimental evaluations on standard benchmarks and the PlanetZoo dataset affirm that S3O provides more accurate 3D reconstruction, and plausible skeletons, and reduces the training time by approximately 60% compared to the state-of-the-art, thus advancing the state of the art in dynamic object reconstruction.","sentences":["Reconstructing dynamic articulated objects from a singular monocular video is challenging, requiring joint estimation of shape, motion, and camera parameters from limited views.","Current methods typically demand extensive computational resources and training time, and require additional human annotations such as predefined parametric models, camera poses, and key points, limiting their generalizability.","We propose Synergistic Shape and Skeleton Optimization (S3O), a novel two-phase method that forgoes these prerequisites and efficiently learns parametric models including visible shapes and underlying skeletons.","Conventional strategies typically learn all parameters simultaneously, leading to interdependencies where a single incorrect prediction can result in significant errors.","In contrast, S3O adopts a phased approach: it first focuses on learning coarse parametric models, then progresses to motion learning and detail addition.","This method substantially lowers computational complexity and enhances robustness in reconstruction from limited viewpoints, all without requiring additional annotations.","To address the current inadequacies in 3D reconstruction from monocular video benchmarks, we collected the PlanetZoo dataset.","Our experimental evaluations on standard benchmarks and the PlanetZoo dataset affirm that S3O provides more accurate 3D reconstruction, and plausible skeletons, and reduces the training time by approximately 60% compared to the state-of-the-art, thus advancing the state of the art in dynamic object reconstruction."],"url":"http://arxiv.org/abs/2405.12607v1","category":"cs.CV"}
{"created":"2024-05-21 08:08:00","title":"Marginal and training-conditional guarantees in one-shot federated conformal prediction","abstract":"We study conformal prediction in the one-shot federated learning setting. The main goal is to compute marginally and training-conditionally valid prediction sets, at the server-level, in only one round of communication between the agents and the server. Using the quantile-of-quantiles family of estimators and split conformal prediction, we introduce a collection of computationally-efficient and distribution-free algorithms that satisfy the aforementioned requirements. Our approaches come from theoretical results related to order statistics and the analysis of the Beta-Beta distribution. We also prove upper bounds on the coverage of all proposed algorithms when the nonconformity scores are almost surely distinct. For algorithms with training-conditional guarantees, these bounds are of the same order of magnitude as those of the centralized case. Remarkably, this implies that the one-shot federated learning setting entails no significant loss compared to the centralized case. Our experiments confirm that our algorithms return prediction sets with coverage and length similar to those obtained in a centralized setting.","sentences":["We study conformal prediction in the one-shot federated learning setting.","The main goal is to compute marginally and training-conditionally valid prediction sets, at the server-level, in only one round of communication between the agents and the server.","Using the quantile-of-quantiles family of estimators and split conformal prediction, we introduce a collection of computationally-efficient and distribution-free algorithms that satisfy the aforementioned requirements.","Our approaches come from theoretical results related to order statistics and the analysis of the Beta-Beta distribution.","We also prove upper bounds on the coverage of all proposed algorithms when the nonconformity scores are almost surely distinct.","For algorithms with training-conditional guarantees, these bounds are of the same order of magnitude as those of the centralized case.","Remarkably, this implies that the one-shot federated learning setting entails no significant loss compared to the centralized case.","Our experiments confirm that our algorithms return prediction sets with coverage and length similar to those obtained in a centralized setting."],"url":"http://arxiv.org/abs/2405.12567v1","category":"math.ST"}
{"created":"2024-05-21 06:06:32","title":"Explosive synchronization in coupled stars","abstract":"We study the effect of network topology on the collective dynamics of an oscillator ensemble. Specifically, we explore explosive synchronization in a system of interacting star networks. Explosive synchronization is characterized by an abrupt transition from an incoherent state to a coherent state. In this study, we couple multiple star networks through their hubs and study the emergent dynamics as a function of coupling strength. The dynamics of each node satisfies the equation of a Kuramoto oscillator. We observe that for a small inter-star coupling strength, the hysteresis width between the forward and backward transition point is minimal, which increases with an increase in the inter-star coupling strength. This observation is independent of the size of the network. Further, we find that the backward transition point is independent of the number of stars coupled together and the inter-star coupling strength, which is also verified using the Watanabe and Strogatz (WS) theory.","sentences":["We study the effect of network topology on the collective dynamics of an oscillator ensemble.","Specifically, we explore explosive synchronization in a system of interacting star networks.","Explosive synchronization is characterized by an abrupt transition from an incoherent state to a coherent state.","In this study, we couple multiple star networks through their hubs and study the emergent dynamics as a function of coupling strength.","The dynamics of each node satisfies the equation of a Kuramoto oscillator.","We observe that for a small inter-star coupling strength, the hysteresis width between the forward and backward transition point is minimal, which increases with an increase in the inter-star coupling strength.","This observation is independent of the size of the network.","Further, we find that the backward transition point is independent of the number of stars coupled together and the inter-star coupling strength, which is also verified using the Watanabe and Strogatz (WS) theory."],"url":"http://arxiv.org/abs/2405.12516v1","category":"nlin.CD"}
{"created":"2024-05-21 04:37:23","title":"Phishing Email Detection Using Inputs From Artificial Intelligence","abstract":"Enterprise security is increasingly being threatened by social engineering attacks, such as phishing, which deceive employees into giving access to enterprise data. To protect both the users themselves and enterprise data, more and more organizations provide cyber security training that seeks to teach employees/customers to identify and report suspicious content. By its very nature, such training seeks to focus on signals that are likely to persist across a wide range of attacks. Further, it expects the user to apply the learnings from these training on e-mail messages that were not filtered by existing, automatic enterprise security (e.g., spam filters and commercial phishing detection software). However, relying on such training now shifts the detection of phishing from an automatic process to a human driven one which is fallible especially when a user errs due to distraction, forgetfulness, etc. In this work we explore treating this type of detection as a natural language processing task and modifying training pipelines accordingly. We present a dataset with annotated labels where these labels are created from the classes of signals that users are typically asked to identify in such training. We also present baseline classifier models trained on these classes of labels. With a comparative analysis of performance between human annotators and the models on these labels, we provide insights which can contribute to the improvement of the respective curricula for both machine and human training.","sentences":["Enterprise security is increasingly being threatened by social engineering attacks, such as phishing, which deceive employees into giving access to enterprise data.","To protect both the users themselves and enterprise data, more and more organizations provide cyber security training that seeks to teach employees/customers to identify and report suspicious content.","By its very nature, such training seeks to focus on signals that are likely to persist across a wide range of attacks.","Further, it expects the user to apply the learnings from these training on e-mail messages that were not filtered by existing, automatic enterprise security (e.g., spam filters and commercial phishing detection software).","However, relying on such training now shifts the detection of phishing from an automatic process to a human driven one which is fallible especially when a user errs due to distraction, forgetfulness, etc.","In this work we explore treating this type of detection as a natural language processing task and modifying training pipelines accordingly.","We present a dataset with annotated labels where these labels are created from the classes of signals that users are typically asked to identify in such training.","We also present baseline classifier models trained on these classes of labels.","With a comparative analysis of performance between human annotators and the models on these labels, we provide insights which can contribute to the improvement of the respective curricula for both machine and human training."],"url":"http://arxiv.org/abs/2405.12494v1","category":"cs.CR"}
{"created":"2024-05-21 04:18:57","title":"Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks","abstract":"Exploring the loss landscape offers insights into the inherent principles of deep neural networks (DNNs). Recent work suggests an additional asymmetry of the valley beyond the flat and sharp ones, yet without thoroughly examining its causes or implications. Our study methodically explores the factors affecting the symmetry of DNN valleys, encompassing (1) the dataset, network architecture, initialization, and hyperparameters that influence the convergence point; and (2) the magnitude and direction of the noise for 1D visualization. Our major observation shows that the {\\it degree of sign consistency} between the noise and the convergence point is a critical indicator of valley symmetry. Theoretical insights from the aspects of ReLU activation and softmax function could explain the interesting phenomenon. Our discovery propels novel understanding and applications in the scenario of Model Fusion: (1) the efficacy of interpolating separate models significantly correlates with their sign consistency ratio, and (2) imposing sign alignment during federated learning emerges as an innovative approach for model parameter alignment.","sentences":["Exploring the loss landscape offers insights into the inherent principles of deep neural networks (DNNs).","Recent work suggests an additional asymmetry of the valley beyond the flat and sharp ones, yet without thoroughly examining its causes or implications.","Our study methodically explores the factors affecting the symmetry of DNN valleys, encompassing (1) the dataset, network architecture, initialization, and hyperparameters that influence the convergence point; and (2) the magnitude and direction of the noise for 1D visualization.","Our major observation shows that the {\\it degree of sign consistency} between the noise and the convergence point is a critical indicator of valley symmetry.","Theoretical insights from the aspects of ReLU activation and softmax function could explain the interesting phenomenon.","Our discovery propels novel understanding and applications in the scenario of Model Fusion: (1) the efficacy of interpolating separate models significantly correlates with their sign consistency ratio, and (2) imposing sign alignment during federated learning emerges as an innovative approach for model parameter alignment."],"url":"http://arxiv.org/abs/2405.12489v1","category":"cs.LG"}
{"created":"2024-05-21 04:08:07","title":"Time Matters: Enhancing Pre-trained News Recommendation Models with Robust User Dwell Time Injection","abstract":"Large Language Models (LLMs) have revolutionized text comprehension, leading to State-of-the-Art (SOTA) news recommendation models that utilize LLMs for in-depth news understanding. Despite this, accurately modeling user preferences remains challenging due to the inherent uncertainty of click behaviors. Techniques like multi-head attention in Transformers seek to alleviate this by capturing interactions among clicks, yet they fall short in integrating explicit feedback signals. User Dwell Time emerges as a powerful indicator, offering the potential to enhance the weak signals emanating from clicks. Nonetheless, its real-world applicability is questionable, especially when dwell time data collection is subject to delays. To bridge this gap, this paper proposes two novel and robust dwell time injection strategies, namely Dwell time Weight (DweW) and Dwell time Aware (DweA). Dwe} concentrates on refining Effective User Clicks through detailed analysis of dwell time, integrating with initial behavioral inputs to construct a more robust user preference. DweA empowers the model with awareness of dwell time information, thereby facilitating autonomous adjustment of attention values in user modeling. This enhancement sharpens the model's ability to accurately identify user preferences. In our experiment using the real-world news dataset from MSN website, we validated that our two strategies significantly improve recommendation performance, favoring high-quality news. Crucially, our approaches exhibit robustness to user dwell time information, maintaining their ability to recommend high-quality content even in extreme cases where dwell time data is entirely missing.","sentences":["Large Language Models (LLMs) have revolutionized text comprehension, leading to State-of-the-Art (SOTA) news recommendation models that utilize LLMs for in-depth news understanding.","Despite this, accurately modeling user preferences remains challenging due to the inherent uncertainty of click behaviors.","Techniques like multi-head attention in Transformers seek to alleviate this by capturing interactions among clicks, yet they fall short in integrating explicit feedback signals.","User Dwell Time emerges as a powerful indicator, offering the potential to enhance the weak signals emanating from clicks.","Nonetheless, its real-world applicability is questionable, especially when dwell time data collection is subject to delays.","To bridge this gap, this paper proposes two novel and robust dwell time injection strategies, namely Dwell time Weight (DweW) and Dwell time Aware (DweA).","Dwe} concentrates on refining Effective User Clicks through detailed analysis of dwell time, integrating with initial behavioral inputs to construct a more robust user preference.","DweA empowers the model with awareness of dwell time information, thereby facilitating autonomous adjustment of attention values in user modeling.","This enhancement sharpens the model's ability to accurately identify user preferences.","In our experiment using the real-world news dataset from MSN website, we validated that our two strategies significantly improve recommendation performance, favoring high-quality news.","Crucially, our approaches exhibit robustness to user dwell time information, maintaining their ability to recommend high-quality content even in extreme cases where dwell time data is entirely missing."],"url":"http://arxiv.org/abs/2405.12486v1","category":"cs.IR"}
{"created":"2024-05-21 03:33:07","title":"GASE: Graph Attention Sampling with Edges Fusion for Solving Vehicle Routing Problems","abstract":"Learning-based methods have become increasingly popular for solving vehicle routing problems due to their near-optimal performance and fast inference speed. Among them, the combination of deep reinforcement learning and graph representation allows for the abstraction of node topology structures and features in an encoder-decoder style. Such an approach makes it possible to solve routing problems end-to-end without needing complicated heuristic operators designed by domain experts. Existing research studies have been focusing on novel encoding and decoding structures via various neural network models to enhance the node embedding representation. Despite the sophisticated approaches applied, there is a noticeable lack of consideration for the graph-theoretic properties inherent to routing problems. Moreover, the potential ramifications of inter-nodal interactions on the decision-making efficacy of the models have not been adequately explored. To bridge this gap, we propose an adaptive Graph Attention Sampling with the Edges Fusion framework (GASE),where nodes' embedding is determined through attention calculation from certain highly correlated neighbourhoods and edges, utilizing a filtered adjacency matrix. In detail, the selections of particular neighbours and adjacency edges are led by a multi-head attention mechanism, contributing directly to the message passing and node embedding in graph attention sampling networks. Furthermore, we incorporate an adaptive actor-critic algorithm with policy improvements to expedite the training convergence. We then conduct comprehensive experiments against baseline methods on learning-based VRP tasks from different perspectives. Our proposed model outperforms the existing methods by 2.08\\%-6.23\\% and shows stronger generalization ability, achieving state-of-the-art performance on randomly generated instances and real-world datasets.","sentences":["Learning-based methods have become increasingly popular for solving vehicle routing problems due to their near-optimal performance and fast inference speed.","Among them, the combination of deep reinforcement learning and graph representation allows for the abstraction of node topology structures and features in an encoder-decoder style.","Such an approach makes it possible to solve routing problems end-to-end without needing complicated heuristic operators designed by domain experts.","Existing research studies have been focusing on novel encoding and decoding structures via various neural network models to enhance the node embedding representation.","Despite the sophisticated approaches applied, there is a noticeable lack of consideration for the graph-theoretic properties inherent to routing problems.","Moreover, the potential ramifications of inter-nodal interactions on the decision-making efficacy of the models have not been adequately explored.","To bridge this gap, we propose an adaptive Graph Attention Sampling with the Edges Fusion framework (GASE),where nodes' embedding is determined through attention calculation from certain highly correlated neighbourhoods and edges, utilizing a filtered adjacency matrix.","In detail, the selections of particular neighbours and adjacency edges are led by a multi-head attention mechanism, contributing directly to the message passing and node embedding in graph attention sampling networks.","Furthermore, we incorporate an adaptive actor-critic algorithm with policy improvements to expedite the training convergence.","We then conduct comprehensive experiments against baseline methods on learning-based VRP tasks from different perspectives.","Our proposed model outperforms the existing methods by 2.08\\%-6.23\\% and shows stronger generalization ability, achieving state-of-the-art performance on randomly generated instances and real-world datasets."],"url":"http://arxiv.org/abs/2405.12475v1","category":"cs.LG"}
{"created":"2024-05-21 03:25:32","title":"Learning Partially Aligned Item Representation for Cross-Domain Sequential Recommendation","abstract":"Cross-domain sequential recommendation (CDSR) aims to uncover and transfer users' sequential preferences across multiple recommendation domains. While significant endeavors have been made, they primarily concentrated on developing advanced transfer modules and aligning user representations using self-supervised learning techniques. However, the problem of aligning item representations has received limited attention, and misaligned item representations can potentially lead to sub-optimal sequential modeling and user representation alignment. To this end, we propose a model-agnostic framework called \\textbf{C}ross-domain item representation \\textbf{A}lignment for \\textbf{C}ross-\\textbf{D}omain \\textbf{S}equential \\textbf{R}ecommendation (\\textbf{CA-CDSR}), which achieves sequence-aware generation and adaptively partial alignment for item representations. Specifically, we first develop a sequence-aware feature augmentation strategy, which captures both collaborative and sequential item correlations, thus facilitating holistic item representation generation. Next, we conduct an empirical study to investigate the partial representation alignment problem from a spectrum perspective. It motivates us to devise an adaptive spectrum filter, achieving partial alignment adaptively. Furthermore, the aligned item representations can be fed into different sequential encoders to obtain user representations. The entire framework is optimized in a multi-task learning paradigm with an annealing strategy. Extensive experiments have demonstrated that CA-CDSR can surpass state-of-the-art baselines by a significant margin and can effectively align items in representation spaces to enhance performance.","sentences":["Cross-domain sequential recommendation (CDSR) aims to uncover and transfer users' sequential preferences across multiple recommendation domains.","While significant endeavors have been made, they primarily concentrated on developing advanced transfer modules and aligning user representations using self-supervised learning techniques.","However, the problem of aligning item representations has received limited attention, and misaligned item representations can potentially lead to sub-optimal sequential modeling and user representation alignment.","To this end, we propose a model-agnostic framework called \\textbf{C}ross-domain item representation \\textbf{A}lignment for \\textbf{C}ross-\\textbf{D}omain \\textbf{S}equential \\textbf{R}ecommendation (\\textbf{CA-CDSR}), which achieves sequence-aware generation and adaptively partial alignment for item representations.","Specifically, we first develop a sequence-aware feature augmentation strategy, which captures both collaborative and sequential item correlations, thus facilitating holistic item representation generation.","Next, we conduct an empirical study to investigate the partial representation alignment problem from a spectrum perspective.","It motivates us to devise an adaptive spectrum filter, achieving partial alignment adaptively.","Furthermore, the aligned item representations can be fed into different sequential encoders to obtain user representations.","The entire framework is optimized in a multi-task learning paradigm with an annealing strategy.","Extensive experiments have demonstrated that CA-CDSR can surpass state-of-the-art baselines by a significant margin and can effectively align items in representation spaces to enhance performance."],"url":"http://arxiv.org/abs/2405.12473v1","category":"cs.IR"}
{"created":"2024-05-21 03:12:50","title":"Optimizing Generative AI Networking: A Dual Perspective with Multi-Agent Systems and Mixture of Experts","abstract":"In the continued development of next-generation networking and artificial intelligence content generation (AIGC) services, the integration of multi-agent systems (MAS) and the mixture of experts (MoE) frameworks is becoming increasingly important. Motivated by this, this article studies the contrasting and converging of MAS and MoE in AIGC-enabled networking. First, we discuss the architectural designs, operational procedures, and inherent advantages of using MAS and MoE in generative AI to explore its functionality and applications fully. Next, we review the applications of MAS and MoE frameworks in content generation and resource allocation, emphasizing their impact on networking operations. Subsequently, we propose a novel multi-agent-enabled MoE-proximal policy optimization (MoE-PPO) framework for 3D object generation and data transfer scenarios. The framework uses MAS for dynamic task coordination of each network service provider agent and MoE for expert-driven execution of respective tasks, thereby improving overall system efficiency and adaptability. The simulation results demonstrate the effectiveness of our proposed framework and significantly improve the performance indicators under different network conditions. Finally, we outline potential future research directions.","sentences":["In the continued development of next-generation networking and artificial intelligence content generation (AIGC) services, the integration of multi-agent systems (MAS) and the mixture of experts (MoE) frameworks is becoming increasingly important.","Motivated by this, this article studies the contrasting and converging of MAS and MoE in AIGC-enabled networking.","First, we discuss the architectural designs, operational procedures, and inherent advantages of using MAS and MoE in generative AI to explore its functionality and applications fully.","Next, we review the applications of MAS and MoE frameworks in content generation and resource allocation, emphasizing their impact on networking operations.","Subsequently, we propose a novel multi-agent-enabled MoE-proximal policy optimization (MoE-PPO) framework for 3D object generation and data transfer scenarios.","The framework uses MAS for dynamic task coordination of each network service provider agent and MoE for expert-driven execution of respective tasks, thereby improving overall system efficiency and adaptability.","The simulation results demonstrate the effectiveness of our proposed framework and significantly improve the performance indicators under different network conditions.","Finally, we outline potential future research directions."],"url":"http://arxiv.org/abs/2405.12472v1","category":"cs.NI"}
{"created":"2024-05-21 03:04:14","title":"Leveraging Diverse Data Generation for Adaptable Zero-Shot Dialogue State Tracking","abstract":"This work demonstrates that substantial gains in zero-shot dialogue state tracking (DST) accuracy can be achieved by increasing the diversity of training data using synthetic data generation techniques. Current DST training resources are severely limited in the number of application domains and slot types they cover due to the high costs of data collection, resulting in limited adaptability to new domains. The presented work overcomes this challenge using a novel, fully automatic data generation approach to create synthetic zero-shot DST training resources. Unlike previous approaches for generating DST data, the presented approach generates entirely new application domains to generate dialogues, complete with silver dialogue state annotations and slot descriptions. This approach is used to create the D0T dataset for training zero-shot DST models, which covers an unprecedented 1,000+ domains. Experiments performed on the MultiWOZ benchmark indicate that training models on diverse synthetic data yields a performance improvement of +6.7% Joint Goal Accuracy, achieving results competitive with much larger models.","sentences":["This work demonstrates that substantial gains in zero-shot dialogue state tracking (DST) accuracy can be achieved by increasing the diversity of training data using synthetic data generation techniques.","Current DST training resources are severely limited in the number of application domains and slot types they cover due to the high costs of data collection, resulting in limited adaptability to new domains.","The presented work overcomes this challenge using a novel, fully automatic data generation approach to create synthetic zero-shot DST training resources.","Unlike previous approaches for generating DST data, the presented approach generates entirely new application domains to generate dialogues, complete with silver dialogue state annotations and slot descriptions.","This approach is used to create the D0T dataset for training zero-shot DST models, which covers an unprecedented 1,000+ domains.","Experiments performed on the MultiWOZ benchmark indicate that training models on diverse synthetic data yields a performance improvement of +6.7% Joint Goal Accuracy, achieving results competitive with much larger models."],"url":"http://arxiv.org/abs/2405.12468v1","category":"cs.CL"}
{"created":"2024-05-21 02:39:46","title":"Evaluation of Connected Vehicle Identification-Aware Mixed Traffic Freeway Cooperative Merging","abstract":"Cooperative on-ramp merging control for connected automated vehicles (CAVs) has been extensively investigated. However, they did neglect the connected vehicle identification process, which is a must for CAV cooperations. In this paper, we introduced a connected vehicle identification system (VIS) into the on-ramp merging control process for the first time and proposed an evaluation framework to assess the impacts of VIS on on-ramp merging performance. First, the mixed-traffic cooperative merging problem was formulated. Then, a real-world merging trajectory dataset was processed to generate dangerous merging scenarios. Aiming at resolving the potential collision risks in mixed traffic where CAVs and traditional human-driven vehicles (THVs) coexist, we proposed on-ramp merging strategies for CAVs in different mixed traffic situations considering the connected vehicle identification process. The performances were evaluated via simulations. Results indicated that while safety was assured for all cases with CAVs, the cases with VIS had delayed initiation of cooperation, limiting the range of cooperative merging and leading to increased fuel consumption and acceleration variations.","sentences":["Cooperative on-ramp merging control for connected automated vehicles (CAVs) has been extensively investigated.","However, they did neglect the connected vehicle identification process, which is a must for CAV cooperations.","In this paper, we introduced a connected vehicle identification system (VIS) into the on-ramp merging control process for the first time and proposed an evaluation framework to assess the impacts of VIS on on-ramp merging performance.","First, the mixed-traffic cooperative merging problem was formulated.","Then, a real-world merging trajectory dataset was processed to generate dangerous merging scenarios.","Aiming at resolving the potential collision risks in mixed traffic where CAVs and traditional human-driven vehicles (THVs) coexist, we proposed on-ramp merging strategies for CAVs in different mixed traffic situations considering the connected vehicle identification process.","The performances were evaluated via simulations.","Results indicated that while safety was assured for all cases with CAVs, the cases with VIS had delayed initiation of cooperation, limiting the range of cooperative merging and leading to increased fuel consumption and acceleration variations."],"url":"http://arxiv.org/abs/2405.12464v1","category":"eess.SY"}
{"created":"2024-05-21 02:39:45","title":"Stochastic Learning of Computational Resource Usage as Graph Structured Multimarginal Schr\u00f6dinger Bridge","abstract":"We propose to learn the time-varying stochastic computational resource usage of software as a graph structured Schr\\\"odinger bridge problem. In general, learning the computational resource usage from data is challenging because resources such as the number of CPU instructions and the number of last level cache requests are both time-varying and statistically correlated. Our proposed method enables learning the joint time-varying stochasticity in computational resource usage from the measured profile snapshots in a nonparametric manner. The method can be used to predict the most-likely time-varying distribution of computational resource availability at a desired time. We provide detailed algorithms for stochastic learning in both single and multi-core cases, discuss the convergence guarantees, computational complexities, and demonstrate their practical use in two case studies: a single-core nonlinear model predictive controller, and a synthetic multi-core software.","sentences":["We propose to learn the time-varying stochastic computational resource usage of software as a graph structured Schr\\\"odinger bridge problem.","In general, learning the computational resource usage from data is challenging because resources such as the number of CPU instructions and the number of last level cache requests are both time-varying and statistically correlated.","Our proposed method enables learning the joint time-varying stochasticity in computational resource usage from the measured profile snapshots in a nonparametric manner.","The method can be used to predict the most-likely time-varying distribution of computational resource availability at a desired time.","We provide detailed algorithms for stochastic learning in both single and multi-core cases, discuss the convergence guarantees, computational complexities, and demonstrate their practical use in two case studies: a single-core nonlinear model predictive controller, and a synthetic multi-core software."],"url":"http://arxiv.org/abs/2405.12463v1","category":"math.OC"}
{"created":"2024-05-21 02:37:47","title":"Boosting X-formers with Structured Matrix for Long Sequence Time Series Forecasting","abstract":"Transformer-based models for long sequence time series forecasting (LSTF) problems have gained significant attention due to their exceptional forecasting precision. As the cornerstone of these models, the self-attention mechanism poses a challenge to efficient training and inference due to its quadratic time complexity. In this article, we propose a novel architectural design for Transformer-based models in LSTF, leveraging a substitution framework that incorporates Surrogate Attention Blocks and Surrogate FFN Blocks. The framework aims to boost any well-designed model's efficiency without sacrificing its accuracy. We further establish the equivalence of the Surrogate Attention Block to the self-attention mechanism in terms of both expressiveness and trainability. Through extensive experiments encompassing nine Transformer-based models across five time series tasks, we observe an average performance improvement of 9.45% while achieving a significant reduction in model size by 46%","sentences":["Transformer-based models for long sequence time series forecasting (LSTF) problems have gained significant attention due to their exceptional forecasting precision.","As the cornerstone of these models, the self-attention mechanism poses a challenge to efficient training and inference due to its quadratic time complexity.","In this article, we propose a novel architectural design for Transformer-based models in LSTF, leveraging a substitution framework that incorporates Surrogate Attention Blocks and Surrogate FFN Blocks.","The framework aims to boost any well-designed model's efficiency without sacrificing its accuracy.","We further establish the equivalence of the Surrogate Attention Block to the self-attention mechanism in terms of both expressiveness and trainability.","Through extensive experiments encompassing nine Transformer-based models across five time series tasks, we observe an average performance improvement of 9.45% while achieving a significant reduction in model size by 46%"],"url":"http://arxiv.org/abs/2405.12462v1","category":"cs.LG"}
{"created":"2024-05-21 02:37:45","title":"WorldAfford: Affordance Grounding based on Natural Language Instructions","abstract":"Affordance grounding aims to localize the interaction regions for the manipulated objects in the scene image according to given instructions. A critical challenge in affordance grounding is that the embodied agent should understand human instructions and analyze which tools in the environment can be used, as well as how to use these tools to accomplish the instructions. Most recent works primarily supports simple action labels as input instructions for localizing affordance regions, failing to capture complex human objectives. Moreover, these approaches typically identify affordance regions of only a single object in object-centric images, ignoring the object context and struggling to localize affordance regions of multiple objects in complex scenes for practical applications. To address this concern, for the first time, we introduce a new task of affordance grounding based on natural language instructions, extending it from previously using simple labels for complex human instructions. For this new task, we propose a new framework, WorldAfford. We design a novel Affordance Reasoning Chain-of-Thought Prompting to reason about affordance knowledge from LLMs more precisely and logically. Subsequently, we use SAM and CLIP to localize the objects related to the affordance knowledge in the image. We identify the affordance regions of the objects through an affordance region localization module. To benchmark this new task and validate our framework, an affordance grounding dataset, LLMaFF, is constructed. We conduct extensive experiments to verify that WorldAfford performs state-of-the-art on both the previous AGD20K and the new LLMaFF dataset. In particular, WorldAfford can localize the affordance regions of multiple objects and provide an alternative when objects in the environment cannot fully match the given instruction.","sentences":["Affordance grounding aims to localize the interaction regions for the manipulated objects in the scene image according to given instructions.","A critical challenge in affordance grounding is that the embodied agent should understand human instructions and analyze which tools in the environment can be used, as well as how to use these tools to accomplish the instructions.","Most recent works primarily supports simple action labels as input instructions for localizing affordance regions, failing to capture complex human objectives.","Moreover, these approaches typically identify affordance regions of only a single object in object-centric images, ignoring the object context and struggling to localize affordance regions of multiple objects in complex scenes for practical applications.","To address this concern, for the first time, we introduce a new task of affordance grounding based on natural language instructions, extending it from previously using simple labels for complex human instructions.","For this new task, we propose a new framework, WorldAfford.","We design a novel Affordance Reasoning Chain-of-Thought Prompting to reason about affordance knowledge from LLMs more precisely and logically.","Subsequently, we use SAM and CLIP to localize the objects related to the affordance knowledge in the image.","We identify the affordance regions of the objects through an affordance region localization module.","To benchmark this new task and validate our framework, an affordance grounding dataset, LLMaFF, is constructed.","We conduct extensive experiments to verify that WorldAfford performs state-of-the-art on both the previous AGD20K and the new LLMaFF dataset.","In particular, WorldAfford can localize the affordance regions of multiple objects and provide an alternative when objects in the environment cannot fully match the given instruction."],"url":"http://arxiv.org/abs/2405.12461v1","category":"cs.CV"}
{"created":"2024-05-21 02:31:26","title":"Studying Up Public Sector AI: How Networks of Power Relations Shape Agency Decisions Around AI Design and Use","abstract":"As public sector agencies rapidly introduce new AI tools in high-stakes domains like social services, it becomes critical to understand how decisions to adopt these tools are made in practice. We borrow from the anthropological practice to ``study up'' those in positions of power, and reorient our study of public sector AI around those who have the power and responsibility to make decisions about the role that AI tools will play in their agency. Through semi-structured interviews and design activities with 16 agency decision-makers, we examine how decisions about AI design and adoption are influenced by their interactions with and assumptions about other actors within these agencies (e.g., frontline workers and agency leaders), as well as those above (legal systems and contracted companies), and below (impacted communities). By centering these networks of power relations, our findings shed light on how infrastructural, legal, and social factors create barriers and disincentives to the involvement of a broader range of stakeholders in decisions about AI design and adoption. Agency decision-makers desired more practical support for stakeholder involvement around public sector AI to help overcome the knowledge and power differentials they perceived between them and other stakeholders (e.g., frontline workers and impacted community members). Building on these findings, we discuss implications for future research and policy around actualizing participatory AI approaches in public sector contexts.","sentences":["As public sector agencies rapidly introduce new AI tools in high-stakes domains like social services, it becomes critical to understand how decisions to adopt these tools are made in practice.","We borrow from the anthropological practice to ``study up'' those in positions of power, and reorient our study of public sector AI around those who have the power and responsibility to make decisions about the role that AI tools will play in their agency.","Through semi-structured interviews and design activities with 16 agency decision-makers, we examine how decisions about AI design and adoption are influenced by their interactions with and assumptions about other actors within these agencies (e.g., frontline workers and agency leaders), as well as those above (legal systems and contracted companies), and below (impacted communities).","By centering these networks of power relations, our findings shed light on how infrastructural, legal, and social factors create barriers and disincentives to the involvement of a broader range of stakeholders in decisions about AI design and adoption.","Agency decision-makers desired more practical support for stakeholder involvement around public sector AI to help overcome the knowledge and power differentials they perceived between them and other stakeholders (e.g., frontline workers and impacted community members).","Building on these findings, we discuss implications for future research and policy around actualizing participatory AI approaches in public sector contexts."],"url":"http://arxiv.org/abs/2405.12458v1","category":"cs.HC"}
{"created":"2024-05-21 02:06:40","title":"Prompt-Enhanced Spatio-Temporal Graph Transfer Learning","abstract":"Spatio-temporal graph neural networks have demonstrated efficacy in capturing complex dependencies for urban computing tasks such as forecasting and kriging. However, their performance is constrained by the reliance on extensive data for training on specific tasks, which limits their adaptability to new urban domains with varied demands. Although transfer learning has been proposed to address this problem by leveraging knowledge across domains, cross-task generalization remains underexplored in spatio-temporal graph transfer learning methods due to the absence of a unified framework. To bridge this gap, we propose Spatio-Temporal Graph Prompting (STGP), a prompt-enhanced transfer learning framework capable of adapting to diverse tasks in data-scarce domains. Specifically, we first unify different tasks into a single template and introduce a task-agnostic network architecture that aligns with this template. This approach enables the capture of spatio-temporal dependencies shared across tasks. Furthermore, we employ learnable prompts to achieve domain and task transfer in a two-stage prompting pipeline, enabling the prompts to effectively capture domain knowledge and task-specific properties at each stage. Extensive experiments demonstrate that STGP outperforms state-of-the-art baselines in three downstream tasks forecasting, kriging, and extrapolation by a notable margin.","sentences":["Spatio-temporal graph neural networks have demonstrated efficacy in capturing complex dependencies for urban computing tasks such as forecasting and kriging.","However, their performance is constrained by the reliance on extensive data for training on specific tasks, which limits their adaptability to new urban domains with varied demands.","Although transfer learning has been proposed to address this problem by leveraging knowledge across domains, cross-task generalization remains underexplored in spatio-temporal graph transfer learning methods due to the absence of a unified framework.","To bridge this gap, we propose Spatio-Temporal Graph Prompting (STGP), a prompt-enhanced transfer learning framework capable of adapting to diverse tasks in data-scarce domains.","Specifically, we first unify different tasks into a single template and introduce a task-agnostic network architecture that aligns with this template.","This approach enables the capture of spatio-temporal dependencies shared across tasks.","Furthermore, we employ learnable prompts to achieve domain and task transfer in a two-stage prompting pipeline, enabling the prompts to effectively capture domain knowledge and task-specific properties at each stage.","Extensive experiments demonstrate that STGP outperforms state-of-the-art baselines in three downstream tasks forecasting, kriging, and extrapolation by a notable margin."],"url":"http://arxiv.org/abs/2405.12452v1","category":"cs.LG"}
{"created":"2024-05-21 02:00:54","title":"PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4","abstract":"The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications. These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language. LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL). However, the context size of the prompt is limited by the number of tokens an LLM can process. This limitation becomes significant as the size of UML class models increases. In this study, we introduce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation. PathOCL addresses the limitations of LLMs, specifically their token processing limit and the challenges posed by large UML class models. PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification. Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model. Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models.","sentences":["The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications.","These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language.","LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL).","However, the context size of the prompt is limited by the number of tokens an LLM can process.","This limitation becomes significant as the size of UML class models increases.","In this study, we introduce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation.","PathOCL addresses the limitations of LLMs, specifically their token processing limit and the challenges posed by large UML class models.","PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification.","Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model.","Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models."],"url":"http://arxiv.org/abs/2405.12450v1","category":"cs.SE"}
{"created":"2024-05-21 01:35:36","title":"Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation","abstract":"Concept recommendation aims to suggest the next concept for learners to study based on their knowledge states and the human knowledge system. While knowledge states can be predicted using knowledge tracing models, previous approaches have not effectively integrated the human knowledge system into the process of designing these educational models. In the era of rapidly evolving Large Language Models (LLMs), many fields have begun using LLMs to generate and encode text, introducing external knowledge. However, integrating LLMs into concept recommendation presents two urgent challenges: 1) How to construct text for concepts that effectively incorporate the human knowledge system? 2) How to adapt non-smooth, anisotropic text encodings effectively for concept recommendation? In this paper, we propose a novel Structure and Knowledge Aware Representation learning framework for concept Recommendation (SKarREC). We leverage factual knowledge from LLMs as well as the precedence and succession relationships between concepts obtained from the knowledge graph to construct textual representations of concepts. Furthermore, we propose a graph-based adapter to adapt anisotropic text embeddings to the concept recommendation task. This adapter is pre-trained through contrastive learning on the knowledge graph to get a smooth and structure-aware concept representation. Then, it's fine-tuned through the recommendation task, forming a text-to-knowledge-to-recommendation adaptation pipeline, which effectively constructs a structure and knowledge-aware concept representation. Our method does a better job than previous adapters in transforming text encodings for application in concept recommendation. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed approach.","sentences":["Concept recommendation aims to suggest the next concept for learners to study based on their knowledge states and the human knowledge system.","While knowledge states can be predicted using knowledge tracing models, previous approaches have not effectively integrated the human knowledge system into the process of designing these educational models.","In the era of rapidly evolving Large Language Models (LLMs), many fields have begun using LLMs to generate and encode text, introducing external knowledge.","However, integrating LLMs into concept recommendation presents two urgent challenges: 1) How to construct text for concepts that effectively incorporate the human knowledge system?","2) How to adapt non-smooth, anisotropic text encodings effectively for concept recommendation?","In this paper, we propose a novel Structure and Knowledge Aware Representation learning framework for concept Recommendation (SKarREC).","We leverage factual knowledge from LLMs as well as the precedence and succession relationships between concepts obtained from the knowledge graph to construct textual representations of concepts.","Furthermore, we propose a graph-based adapter to adapt anisotropic text embeddings to the concept recommendation task.","This adapter is pre-trained through contrastive learning on the knowledge graph to get a smooth and structure-aware concept representation.","Then, it's fine-tuned through the recommendation task, forming a text-to-knowledge-to-recommendation adaptation pipeline, which effectively constructs a structure and knowledge-aware concept representation.","Our method does a better job than previous adapters in transforming text encodings for application in concept recommendation.","Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2405.12442v1","category":"cs.IR"}
{"created":"2024-05-21 01:31:17","title":"CoCo Matrix: Taxonomy of Cognitive Contributions in Co-writing with Intelligent Agents","abstract":"In recent years, there has been a growing interest in employing intelligent agents in writing. Previous work emphasizes the evaluation of the quality of end product-whether it was coherent and polished, overlooking the journey that led to the product, which is an invaluable dimension of the creative process. To understand how to recognize human efforts in co-writing with intelligent writing systems, we adapt Flower and Hayes' cognitive process theory of writing and propose CoCo Matrix, a two-dimensional taxonomy of entropy and information gain, to depict the new human-agent co-writing model. We define four quadrants and situate thirty-four published systems within the taxonomy. Our research found that low entropy and high information gain systems are under-explored, yet offer promising future directions in writing tasks that benefit from the agent's divergent planning and the human's focused translation. CoCo Matrix, not only categorizes different writing systems but also deepens our understanding of the cognitive processes in human-agent co-writing. By analyzing minimal changes in the writing process, CoCo Matrix serves as a proxy for the writer's mental model, allowing writers to reflect on their contributions. This reflection is facilitated through the measured metrics of information gain and entropy, which provide insights irrespective of the writing system used.","sentences":["In recent years, there has been a growing interest in employing intelligent agents in writing.","Previous work emphasizes the evaluation of the quality of end product-whether it was coherent and polished, overlooking the journey that led to the product, which is an invaluable dimension of the creative process.","To understand how to recognize human efforts in co-writing with intelligent writing systems, we adapt Flower and Hayes' cognitive process theory of writing and propose CoCo Matrix, a two-dimensional taxonomy of entropy and information gain, to depict the new human-agent co-writing model.","We define four quadrants and situate thirty-four published systems within the taxonomy.","Our research found that low entropy and high information gain systems are under-explored, yet offer promising future directions in writing tasks that benefit from the agent's divergent planning and the human's focused translation.","CoCo Matrix, not only categorizes different writing systems but also deepens our understanding of the cognitive processes in human-agent co-writing.","By analyzing minimal changes in the writing process, CoCo Matrix serves as a proxy for the writer's mental model, allowing writers to reflect on their contributions.","This reflection is facilitated through the measured metrics of information gain and entropy, which provide insights irrespective of the writing system used."],"url":"http://arxiv.org/abs/2405.12438v1","category":"cs.HC"}
{"created":"2024-05-21 01:16:34","title":"LLM+Reasoning+Planning for supporting incomplete user queries in presence of APIs","abstract":"Recent availability of Large Language Models (LLMs) has led to the development of numerous LLM-based approaches aimed at providing natural language interfaces for various end-user tasks. These end-user tasks in turn can typically be accomplished by orchestrating a given set of APIs. In practice, natural language task requests (user queries) are often incomplete, i.e., they may not contain all the information required by the APIs. While LLMs excel at natural language processing (NLP) tasks, they frequently hallucinate on missing information or struggle with orchestrating the APIs. The key idea behind our proposed approach is to leverage logical reasoning and classical AI planning along with an LLM for accurately answering user queries including identification and gathering of any missing information in these queries. Our approach uses an LLM and ASP (Answer Set Programming) solver to translate a user query to a representation in Planning Domain Definition Language (PDDL) via an intermediate representation in ASP. We introduce a special API \"get_info_api\" for gathering missing information. We model all the APIs as PDDL actions in a way that supports dataflow between the APIs. Our approach then uses a classical AI planner to generate an orchestration of API calls (including calls to get_info_api) to answer the user query. Our evaluation results show that our approach significantly outperforms a pure LLM based approach by achieving over 95\\% success rate in most cases on a dataset containing complete and incomplete single goal and multi-goal queries where the multi-goal queries may or may not require dataflow among the APIs.","sentences":["Recent availability of Large Language Models (LLMs) has led to the development of numerous LLM-based approaches aimed at providing natural language interfaces for various end-user tasks.","These end-user tasks in turn can typically be accomplished by orchestrating a given set of APIs.","In practice, natural language task requests (user queries) are often incomplete, i.e., they may not contain all the information required by the APIs.","While LLMs excel at natural language processing (NLP) tasks, they frequently hallucinate on missing information or struggle with orchestrating the APIs.","The key idea behind our proposed approach is to leverage logical reasoning and classical AI planning along with an LLM for accurately answering user queries including identification and gathering of any missing information in these queries.","Our approach uses an LLM and ASP (Answer Set Programming) solver to translate a user query to a representation in Planning Domain Definition Language (PDDL) via an intermediate representation in ASP.","We introduce a special API \"get_info_api\" for gathering missing information.","We model all the APIs as PDDL actions in a way that supports dataflow between the APIs.","Our approach then uses a classical AI planner to generate an orchestration of API calls (including calls to get_info_api) to answer the user query.","Our evaluation results show that our approach significantly outperforms a pure LLM based approach by achieving over 95\\% success rate in most cases on a dataset containing complete and incomplete single goal and multi-goal queries where the multi-goal queries may or may not require dataflow among the APIs."],"url":"http://arxiv.org/abs/2405.12433v1","category":"cs.AI"}
{"created":"2024-05-21 01:14:33","title":"Data Sharing at the Edge of the Network: A Disturbance Resilient Multi-modal ITS","abstract":"Mobility-as-a-Service (MaaS) is a paradigm that encourages the shift from private cars to more sustainable alternative mobility services. MaaS provides services that enhances and enables multiple modes of transport to operate seamlessly and bringing Multimodal Intelligent Transport Systems (M-ITS) closer to reality. This requires sharing and integration of data collected from multiple sources including modes of transports, sensors, and end-users' devices to allow a seamless and integrated services especially during unprecedented disturbances. This paper discusses the interactions among transportation modes, networks, potential disturbance scenarios, and adaptation strategies to mitigate their impact on MaaS. We particularly discuss the need to share data between the modes of transport and relevant entities that are at the vicinity of each other, taking advantage of edge computing technology to avoid any latency due to communication to the cloud and privacy concerns. However, when sharing at the edge, bandwidth, storage, and computational limitations must be considered.","sentences":["Mobility-as-a-Service (MaaS) is a paradigm that encourages the shift from private cars to more sustainable alternative mobility services.","MaaS provides services that enhances and enables multiple modes of transport to operate seamlessly and bringing Multimodal Intelligent Transport Systems (M-ITS) closer to reality.","This requires sharing and integration of data collected from multiple sources including modes of transports, sensors, and end-users' devices to allow a seamless and integrated services especially during unprecedented disturbances.","This paper discusses the interactions among transportation modes, networks, potential disturbance scenarios, and adaptation strategies to mitigate their impact on MaaS. We particularly discuss the need to share data between the modes of transport and relevant entities that are at the vicinity of each other, taking advantage of edge computing technology to avoid any latency due to communication to the cloud and privacy concerns.","However, when sharing at the edge, bandwidth, storage, and computational limitations must be considered."],"url":"http://arxiv.org/abs/2405.12431v1","category":"cs.DC"}
{"created":"2024-05-20 23:59:26","title":"A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback","abstract":"Inverse Reinforcement Learning (IRL) and Reinforcement Learning from Human Feedback (RLHF) are pivotal methodologies in reward learning, which involve inferring and shaping the underlying reward function of sequential decision-making problems based on observed human demonstrations and feedback. Most prior work in reward learning has relied on prior knowledge or assumptions about decision or preference models, potentially leading to robustness issues. In response, this paper introduces a novel linear programming (LP) framework tailored for offline reward learning. Utilizing pre-collected trajectories without online exploration, this framework estimates a feasible reward set from the primal-dual optimality conditions of a suitably designed LP, and offers an optimality guarantee with provable sample efficiency. Our LP framework also enables aligning the reward functions with human feedback, such as pairwise trajectory comparison data, while maintaining computational tractability and sample efficiency. We demonstrate that our framework potentially achieves better performance compared to the conventional maximum likelihood estimation (MLE) approach through analytical examples and numerical experiments.","sentences":["Inverse Reinforcement Learning (IRL) and Reinforcement Learning from Human Feedback (RLHF) are pivotal methodologies in reward learning, which involve inferring and shaping the underlying reward function of sequential decision-making problems based on observed human demonstrations and feedback.","Most prior work in reward learning has relied on prior knowledge or assumptions about decision or preference models, potentially leading to robustness issues.","In response, this paper introduces a novel linear programming (LP) framework tailored for offline reward learning.","Utilizing pre-collected trajectories without online exploration, this framework estimates a feasible reward set from the primal-dual optimality conditions of a suitably designed LP, and offers an optimality guarantee with provable sample efficiency.","Our LP framework also enables aligning the reward functions with human feedback, such as pairwise trajectory comparison data, while maintaining computational tractability and sample efficiency.","We demonstrate that our framework potentially achieves better performance compared to the conventional maximum likelihood estimation (MLE) approach through analytical examples and numerical experiments."],"url":"http://arxiv.org/abs/2405.12421v1","category":"cs.LG"}
{"created":"2024-05-20 23:11:10","title":"Segmentation of dense and multi-species bacterial colonies achieved using models trained on synthetic microscopy images","abstract":"The spread of microbial infections is governed by the self-organization of bacteria on surfaces. Limitations of live imaging techniques make collective behaviors in clinically relevant systems challenging to quantify. Here, we develop novel experimental and image analysis techniques for high-fidelity single-cell segmentation of bacterial colonies. Machine learning-based segmentation models are trained solely using synthetic microscopy images that are processed to look realistic using state-of-the-art image-to-image translation methods, requiring no biophysical modeling. Accurate single-cell segmentation is achieved for densely packed single-species colonies and multi-species colonies of common pathogenic bacteria, even under suboptimal imaging conditions and for both brightfield and confocal laser scanning microscopy. The resulting data provide quantitative insights into the self-organization of bacteria on soft surfaces. Thanks to their high adaptability and relatively simple implementation, these methods promise to greatly facilitate quantitative descriptions of bacterial infections in varied environments.","sentences":["The spread of microbial infections is governed by the self-organization of bacteria on surfaces.","Limitations of live imaging techniques make collective behaviors in clinically relevant systems challenging to quantify.","Here, we develop novel experimental and image analysis techniques for high-fidelity single-cell segmentation of bacterial colonies.","Machine learning-based segmentation models are trained solely using synthetic microscopy images that are processed to look realistic using state-of-the-art image-to-image translation methods, requiring no biophysical modeling.","Accurate single-cell segmentation is achieved for densely packed single-species colonies and multi-species colonies of common pathogenic bacteria, even under suboptimal imaging conditions and for both brightfield and confocal laser scanning microscopy.","The resulting data provide quantitative insights into the self-organization of bacteria on soft surfaces.","Thanks to their high adaptability and relatively simple implementation, these methods promise to greatly facilitate quantitative descriptions of bacterial infections in varied environments."],"url":"http://arxiv.org/abs/2405.12407v1","category":"physics.bio-ph"}
{"created":"2024-05-20 22:51:05","title":"Diffusion for World Modeling: Visual Details Matter in Atari","abstract":"World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. To foster future research on diffusion for world modeling, we release our code, agents and playable world models at https://github.com/eloialonso/diamond.","sentences":["World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner.","Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics.","However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning.","Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents.","Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model.","We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance.","DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model.","To foster future research on diffusion for world modeling, we release our code, agents and playable world models at https://github.com/eloialonso/diamond."],"url":"http://arxiv.org/abs/2405.12399v1","category":"cs.LG"}
{"created":"2024-05-20 21:59:07","title":"Automated Anomaly Detection on European XFEL Klystrons","abstract":"High-power multi-beam klystrons represent a key component to amplify RF to generate the accelerating field of the superconducting radio frequency (SRF) cavities at European XFEL. Exchanging these high-power components takes time and effort, thus it is necessary to minimize maintenance and downtime and at the same time maximize the device's operation. In an attempt to explore the behavior of klystrons using machine learning, we completed a series of experiments on our klystrons to determine various operational modes and conduct feature extraction and dimensionality reduction to extract the most valuable information about a normal operation. To analyze recorded data we used state-of-the-art data-driven learning techniques and recognized the most promising components that might help us better understand klystron operational states and identify early on possible faults or anomalies.","sentences":["High-power multi-beam klystrons represent a key component to amplify RF to generate the accelerating field of the superconducting radio frequency (SRF) cavities at European XFEL.","Exchanging these high-power components takes time and effort, thus it is necessary to minimize maintenance and downtime and at the same time maximize the device's operation.","In an attempt to explore the behavior of klystrons using machine learning, we completed a series of experiments on our klystrons to determine various operational modes and conduct feature extraction and dimensionality reduction to extract the most valuable information about a normal operation.","To analyze recorded data we used state-of-the-art data-driven learning techniques and recognized the most promising components that might help us better understand klystron operational states and identify early on possible faults or anomalies."],"url":"http://arxiv.org/abs/2405.12391v1","category":"physics.acc-ph"}
{"created":"2024-05-20 21:50:19","title":"A Metric-based Principal Curve Approach for Learning One-dimensional Manifold","abstract":"Principal curve is a well-known statistical method oriented in manifold learning using concepts from differential geometry. In this paper, we propose a novel metric-based principal curve (MPC) method that learns one-dimensional manifold of spatial data. Synthetic datasets Real applications using MNIST dataset show that our method can learn the one-dimensional manifold well in terms of the shape.","sentences":["Principal curve is a well-known statistical method oriented in manifold learning using concepts from differential geometry.","In this paper, we propose a novel metric-based principal curve (MPC) method that learns one-dimensional manifold of spatial data.","Synthetic datasets Real applications using MNIST dataset show that our method can learn the one-dimensional manifold well in terms of the shape."],"url":"http://arxiv.org/abs/2405.12390v1","category":"stat.ML"}
{"created":"2024-05-20 21:23:25","title":"Flow Through Porous Media at the Percolation Transition","abstract":"We study low-Reynolds-number fluid flow through a two-dimensional porous medium modeled as a Lorentz gas. Using extensive finite element simulations we fully resolve the flow fields for packing fractions approaching the percolation threshold. Near the percolation transition, we find a power-law scaling of the flow rate versus the pressure drop with an exponent of $\\approx 5/2$, which has been predicted earlier by mapping the macroscopic flow to a discrete flow network [Phys. Rev. Lett. 54, 1985]. Importantly, we observe a rounding of the scaling behavior at small system sizes, which can be rationalized via a finite-size scaling ansatz. Finally, we show that the distribution of the kinetic energy exhibits a power-law scaling over several decades at small energies, originating from collections of self-similar, viscous eddies in the dead-end-channels. Our results lay the foundation for unraveling critical behavior of complex fluids omnipresent in biological and geophysical systems.","sentences":["We study low-Reynolds-number fluid flow through a two-dimensional porous medium modeled as a Lorentz gas.","Using extensive finite element simulations we fully resolve the flow fields for packing fractions approaching the percolation threshold.","Near the percolation transition, we find a power-law scaling of the flow rate versus the pressure drop with an exponent of $\\approx 5/2$, which has been predicted earlier by mapping the macroscopic flow to a discrete flow network [Phys.","Rev. Lett.","54, 1985].","Importantly, we observe a rounding of the scaling behavior at small system sizes, which can be rationalized via a finite-size scaling ansatz.","Finally, we show that the distribution of the kinetic energy exhibits a power-law scaling over several decades at small energies, originating from collections of self-similar, viscous eddies in the dead-end-channels.","Our results lay the foundation for unraveling critical behavior of complex fluids omnipresent in biological and geophysical systems."],"url":"http://arxiv.org/abs/2405.12381v1","category":"physics.flu-dyn"}
{"created":"2024-05-20 20:37:44","title":"Layout Agnostic Human Activity Recognition in Smart Homes through Textual Descriptions Of Sensor Triggers (TDOST)","abstract":"Human activity recognition (HAR) using ambient sensors in smart homes has numerous applications for human healthcare and wellness. However, building general-purpose HAR models that can be deployed to new smart home environments requires a significant amount of annotated sensor data and training overhead. Most smart homes vary significantly in their layouts, i.e., floor plans and the specifics of sensors embedded, resulting in low generalizability of HAR models trained for specific homes. We address this limitation by introducing a novel, layout-agnostic modeling approach for HAR systems in smart homes that utilizes the transferrable representational capacity of natural language descriptions of raw sensor data. To this end, we generate Textual Descriptions Of Sensor Triggers (TDOST) that encapsulate the surrounding trigger conditions and provide cues for underlying activities to the activity recognition models. Leveraging textual embeddings, rather than raw sensor data, we create activity recognition systems that predict standard activities across homes without either (re-)training or adaptation on target homes. Through an extensive evaluation, we demonstrate the effectiveness of TDOST-based models in unseen smart homes through experiments on benchmarked CASAS datasets. Furthermore, we conduct a detailed analysis of how the individual components of our approach affect downstream activity recognition performance.","sentences":["Human activity recognition (HAR) using ambient sensors in smart homes has numerous applications for human healthcare and wellness.","However, building general-purpose HAR models that can be deployed to new smart home environments requires a significant amount of annotated sensor data and training overhead.","Most smart homes vary significantly in their layouts, i.e., floor plans and the specifics of sensors embedded, resulting in low generalizability of HAR models trained for specific homes.","We address this limitation by introducing a novel, layout-agnostic modeling approach for HAR systems in smart homes that utilizes the transferrable representational capacity of natural language descriptions of raw sensor data.","To this end, we generate Textual Descriptions Of Sensor Triggers (TDOST) that encapsulate the surrounding trigger conditions and provide cues for underlying activities to the activity recognition models.","Leveraging textual embeddings, rather than raw sensor data, we create activity recognition systems that predict standard activities across homes without either (re-)training or adaptation on target homes.","Through an extensive evaluation, we demonstrate the effectiveness of TDOST-based models in unseen smart homes through experiments on benchmarked CASAS datasets.","Furthermore, we conduct a detailed analysis of how the individual components of our approach affect downstream activity recognition performance."],"url":"http://arxiv.org/abs/2405.12368v1","category":"cs.AI"}
{"created":"2024-05-20 20:37:27","title":"Large-Scale Multi-Center CT and MRI Segmentation of Pancreas with Deep Learning","abstract":"Automated volumetric segmentation of the pancreas on cross-sectional imaging is needed for diagnosis and follow-up of pancreatic diseases. While CT-based pancreatic segmentation is more established, MRI-based segmentation methods are understudied, largely due to a lack of publicly available datasets, benchmarking research efforts, and domain-specific deep learning methods. In this retrospective study, we collected a large dataset (767 scans from 499 participants) of T1-weighted (T1W) and T2-weighted (T2W) abdominal MRI series from five centers between March 2004 and November 2022. We also collected CT scans of 1,350 patients from publicly available sources for benchmarking purposes. We developed a new pancreas segmentation method, called PanSegNet, combining the strengths of nnUNet and a Transformer network with a new linear attention module enabling volumetric computation. We tested PanSegNet's accuracy in cross-modality (a total of 2,117 scans) and cross-center settings with Dice and Hausdorff distance (HD95) evaluation metrics. We used Cohen's kappa statistics for intra and inter-rater agreement evaluation and paired t-tests for volume and Dice comparisons, respectively. For segmentation accuracy, we achieved Dice coefficients of 88.3% (std: 7.2%, at case level) with CT, 85.0% (std: 7.9%) with T1W MRI, and 86.3% (std: 6.4%) with T2W MRI. There was a high correlation for pancreas volume prediction with R^2 of 0.91, 0.84, and 0.85 for CT, T1W, and T2W, respectively. We found moderate inter-observer (0.624 and 0.638 for T1W and T2W MRI, respectively) and high intra-observer agreement scores. All MRI data is made available at https://osf.io/kysnj/. Our source code is available at https://github.com/NUBagciLab/PaNSegNet.","sentences":["Automated volumetric segmentation of the pancreas on cross-sectional imaging is needed for diagnosis and follow-up of pancreatic diseases.","While CT-based pancreatic segmentation is more established, MRI-based segmentation methods are understudied, largely due to a lack of publicly available datasets, benchmarking research efforts, and domain-specific deep learning methods.","In this retrospective study, we collected a large dataset (767 scans from 499 participants) of T1-weighted (T1W) and T2-weighted (T2W) abdominal MRI series from five centers between March 2004 and November 2022.","We also collected CT scans of 1,350 patients from publicly available sources for benchmarking purposes.","We developed a new pancreas segmentation method, called PanSegNet, combining the strengths of nnUNet and a Transformer network with a new linear attention module enabling volumetric computation.","We tested PanSegNet's accuracy in cross-modality (a total of 2,117 scans) and cross-center settings with Dice and Hausdorff distance (HD95) evaluation metrics.","We used Cohen's kappa statistics for intra and inter-rater agreement evaluation and paired t-tests for volume and Dice comparisons, respectively.","For segmentation accuracy, we achieved Dice coefficients of 88.3% (std: 7.2%, at case level) with CT, 85.0% (std: 7.9%) with T1W MRI, and 86.3% (std: 6.4%) with T2W MRI.","There was a high correlation for pancreas volume prediction with R^2 of 0.91, 0.84, and 0.85 for CT, T1W, and T2W, respectively.","We found moderate inter-observer (0.624 and 0.638 for T1W and T2W MRI, respectively) and high intra-observer agreement scores.","All MRI data is made available at https://osf.io/kysnj/. Our source code is available at https://github.com/NUBagciLab/PaNSegNet."],"url":"http://arxiv.org/abs/2405.12367v1","category":"eess.IV"}
{"created":"2024-05-20 20:06:42","title":"A Study on Optimization Techniques for Variational Quantum Circuits in Reinforcement Learning","abstract":"Quantum Computing aims to streamline machine learning, making it more effective with fewer trainable parameters. This reduction of parameters can speed up the learning process and reduce the use of computational resources. However, in the current phase of quantum computing development, known as the noisy intermediate-scale quantum era (NISQ), learning is difficult due to a limited number of qubits and widespread quantum noise. To overcome these challenges, researchers are focusing on variational quantum circuits (VQCs). VQCs are hybrid algorithms that merge a quantum circuit, which can be adjusted through parameters, with traditional classical optimization techniques. These circuits require only few qubits for effective learning. Recent studies have presented new ways of applying VQCs to reinforcement learning, showing promising results that warrant further exploration. This study investigates the effects of various techniques -- data re-uploading, input scaling, output scaling -- and introduces exponential learning rate decay in the quantum proximal policy optimization algorithm's actor-VQC. We assess these methods in the popular Frozen Lake and Cart Pole environments. Our focus is on their ability to reduce the number of parameters in the VQC without losing effectiveness. Our findings indicate that data re-uploading and an exponential learning rate decay significantly enhance hyperparameter stability and overall performance. While input scaling does not improve parameter efficiency, output scaling effectively manages greediness, leading to increased learning speed and robustness.","sentences":["Quantum Computing aims to streamline machine learning, making it more effective with fewer trainable parameters.","This reduction of parameters can speed up the learning process and reduce the use of computational resources.","However, in the current phase of quantum computing development, known as the noisy intermediate-scale quantum era (NISQ), learning is difficult due to a limited number of qubits and widespread quantum noise.","To overcome these challenges, researchers are focusing on variational quantum circuits (VQCs).","VQCs are hybrid algorithms that merge a quantum circuit, which can be adjusted through parameters, with traditional classical optimization techniques.","These circuits require only few qubits for effective learning.","Recent studies have presented new ways of applying VQCs to reinforcement learning, showing promising results that warrant further exploration.","This study investigates the effects of various techniques -- data re-uploading, input scaling, output scaling -- and introduces exponential learning rate decay in the quantum proximal policy optimization algorithm's actor-VQC.","We assess these methods in the popular Frozen Lake and Cart Pole environments.","Our focus is on their ability to reduce the number of parameters in the VQC without losing effectiveness.","Our findings indicate that data re-uploading and an exponential learning rate decay significantly enhance hyperparameter stability and overall performance.","While input scaling does not improve parameter efficiency, output scaling effectively manages greediness, leading to increased learning speed and robustness."],"url":"http://arxiv.org/abs/2405.12354v1","category":"quant-ph"}
{"created":"2024-05-20 20:03:51","title":"TinyM$^2$Net-V3: Memory-Aware Compressed Multimodal Deep Neural Networks for Sustainable Edge Deployment","abstract":"The advancement of sophisticated artificial intelligence (AI) algorithms has led to a notable increase in energy usage and carbon dioxide emissions, intensifying concerns about climate change. This growing problem has brought the environmental sustainability of AI technologies to the forefront, especially as they expand across various sectors. In response to these challenges, there is an urgent need for the development of sustainable AI solutions. These solutions must focus on energy-efficient embedded systems that are capable of handling diverse data types even in environments with limited resources, thereby ensuring both technological progress and environmental responsibility. Integrating complementary multimodal data into tiny machine learning models for edge devices is challenging due to increased complexity, latency, and power consumption. This work introduces TinyM$^2$Net-V3, a system that processes different modalities of complementary data, designs deep neural network (DNN) models, and employs model compression techniques including knowledge distillation and low bit-width quantization with memory-aware considerations to fit models within lower memory hierarchy levels, reducing latency and enhancing energy efficiency on resource-constrained devices. We evaluated TinyM$^2$Net-V3 in two multimodal case studies: COVID-19 detection using cough, speech, and breathing audios, and pose classification from depth and thermal images. With tiny inference models (6 KB and 58 KB), we achieved 92.95% and 90.7% accuracies, respectively. Our tiny machine learning models, deployed on resource limited hardware, demonstrated low latencies within milliseconds and very high power efficiency.","sentences":["The advancement of sophisticated artificial intelligence (AI) algorithms has led to a notable increase in energy usage and carbon dioxide emissions, intensifying concerns about climate change.","This growing problem has brought the environmental sustainability of AI technologies to the forefront, especially as they expand across various sectors.","In response to these challenges, there is an urgent need for the development of sustainable AI solutions.","These solutions must focus on energy-efficient embedded systems that are capable of handling diverse data types even in environments with limited resources, thereby ensuring both technological progress and environmental responsibility.","Integrating complementary multimodal data into tiny machine learning models for edge devices is challenging due to increased complexity, latency, and power consumption.","This work introduces TinyM$^2$Net-V3, a system that processes different modalities of complementary data, designs deep neural network (DNN) models, and employs model compression techniques including knowledge distillation and low bit-width quantization with memory-aware considerations to fit models within lower memory hierarchy levels, reducing latency and enhancing energy efficiency on resource-constrained devices.","We evaluated TinyM$^2$Net-V3 in two multimodal case studies: COVID-19 detection using cough, speech, and breathing audios, and pose classification from depth and thermal images.","With tiny inference models (6 KB and 58 KB), we achieved 92.95% and 90.7% accuracies, respectively.","Our tiny machine learning models, deployed on resource limited hardware, demonstrated low latencies within milliseconds and very high power efficiency."],"url":"http://arxiv.org/abs/2405.12353v1","category":"cs.LG"}
{"created":"2024-05-20 19:47:13","title":"Self-HWDebug: Automation of LLM Self-Instructing for Hardware Security Verification","abstract":"The rise of instruction-tuned Large Language Models (LLMs) marks a significant advancement in artificial intelligence (AI) (tailored to respond to specific prompts). Despite their popularity, applying such models to debug security vulnerabilities in hardware designs, i.e., register transfer language (RTL) modules, particularly at system-on-chip (SoC) level, presents considerable challenges. One of the main issues lies in the need for precisely designed instructions for pinpointing and mitigating the vulnerabilities, which requires substantial time and expertise from human experts. In response to this challenge, this paper proposes Self-HWDebug, an innovative framework that leverages LLMs to automatically create required debugging instructions. In Self-HWDebug, a set of already identified bugs from the most critical hardware common weakness enumeration (CWE) listings, along with mitigation resolutions, is provided to the framework, followed by prompting the LLMs to generate targeted instructions for such mitigation. The LLM-generated instructions are subsequently used as references to address vulnerabilities within the same CWE category but in totally different designs, effectively demonstrating the framework's ability to extend solutions across related security issues. Self-HWDebug significantly reduces human intervention by using the model's own output to guide debugging. Through comprehensive testing, Self-HWDebug proves not only to reduce experts' effort/time but also to even improve the quality of the debugging process.","sentences":["The rise of instruction-tuned Large Language Models (LLMs) marks a significant advancement in artificial intelligence (AI) (tailored to respond to specific prompts).","Despite their popularity, applying such models to debug security vulnerabilities in hardware designs, i.e., register transfer language (RTL) modules, particularly at system-on-chip (SoC) level, presents considerable challenges.","One of the main issues lies in the need for precisely designed instructions for pinpointing and mitigating the vulnerabilities, which requires substantial time and expertise from human experts.","In response to this challenge, this paper proposes Self-HWDebug, an innovative framework that leverages LLMs to automatically create required debugging instructions.","In Self-HWDebug, a set of already identified bugs from the most critical hardware common weakness enumeration (CWE) listings, along with mitigation resolutions, is provided to the framework, followed by prompting the LLMs to generate targeted instructions for such mitigation.","The LLM-generated instructions are subsequently used as references to address vulnerabilities within the same CWE category but in totally different designs, effectively demonstrating the framework's ability to extend solutions across related security issues.","Self-HWDebug significantly reduces human intervention by using the model's own output to guide debugging.","Through comprehensive testing, Self-HWDebug proves not only to reduce experts' effort/time but also to even improve the quality of the debugging process."],"url":"http://arxiv.org/abs/2405.12347v1","category":"cs.CR"}
{"created":"2024-05-20 18:51:42","title":"Overlap Number of Balls Model-Agnostic CounterFactuals (ONB-MACF): A Data-Morphology-based Counterfactual Generation Method for Trustworthy Artificial Intelligence","abstract":"Explainable Artificial Intelligence (XAI) is a pivotal research domain aimed at understanding the operational mechanisms of AI systems, particularly those considered ``black boxes'' due to their complex, opaque nature. XAI seeks to make these AI systems more understandable and trustworthy, providing insight into their decision-making processes. By producing clear and comprehensible explanations, XAI enables users, practitioners, and stakeholders to trust a model's decisions. This work analyses the value of data morphology strategies in generating counterfactual explanations. It introduces the Overlap Number of Balls Model-Agnostic CounterFactuals (ONB-MACF) method, a model-agnostic counterfactual generator that leverages data morphology to estimate a model's decision boundaries. The ONB-MACF method constructs hyperspheres in the data space whose covered points share a class, mapping the decision boundary. Counterfactuals are then generated by incrementally adjusting an instance's attributes towards the nearest alternate-class hypersphere, crossing the decision boundary with minimal modifications. By design, the ONB-MACF method generates feasible and sparse counterfactuals that follow the data distribution. Our comprehensive benchmark from a double perspective (quantitative and qualitative) shows that the ONB-MACF method outperforms existing state-of-the-art counterfactual generation methods across multiple quality metrics on diverse tabular datasets. This supports our hypothesis, showcasing the potential of data-morphology-based explainability strategies for trustworthy AI.","sentences":["Explainable Artificial Intelligence (XAI) is a pivotal research domain aimed at understanding the operational mechanisms of AI systems, particularly those considered ``black boxes'' due to their complex, opaque nature.","XAI seeks to make these AI systems more understandable and trustworthy, providing insight into their decision-making processes.","By producing clear and comprehensible explanations, XAI enables users, practitioners, and stakeholders to trust a model's decisions.","This work analyses the value of data morphology strategies in generating counterfactual explanations.","It introduces the Overlap Number of Balls Model-Agnostic CounterFactuals (ONB-MACF) method, a model-agnostic counterfactual generator that leverages data morphology to estimate a model's decision boundaries.","The ONB-MACF method constructs hyperspheres in the data space whose covered points share a class, mapping the decision boundary.","Counterfactuals are then generated by incrementally adjusting an instance's attributes towards the nearest alternate-class hypersphere, crossing the decision boundary with minimal modifications.","By design, the ONB-MACF method generates feasible and sparse counterfactuals that follow the data distribution.","Our comprehensive benchmark from a double perspective (quantitative and qualitative) shows that the ONB-MACF method outperforms existing state-of-the-art counterfactual generation methods across multiple quality metrics on diverse tabular datasets.","This supports our hypothesis, showcasing the potential of data-morphology-based explainability strategies for trustworthy AI."],"url":"http://arxiv.org/abs/2405.12326v1","category":"cs.LG"}
{"created":"2024-05-20 18:47:25","title":"Robust 1-norm periodograms for analysis of noisy non-Gaussian time series with irregular cadences: Application to VLBI astrometry of quasars","abstract":"Astronomical time series often have non-uniform sampling in time, or irregular cadences, with long gaps separating clusters of observations. Some of these data sets are also explicitly non-Gaussian with respect to the expected model fit, or the simple mean. The standard Lomb-Scargle periodogram is based on the least squares solution for a set of test periods and, therefore, is easily corrupted by a subset of statistical outliers or an intrinsically non-Gaussian population. It can produce completely misleading results for heavy-tailed distribution of residuals. We propose a robust 1-norm periodogram technique, which is based on the principles of robust statistical estimation. This technique can be implemented in weighted or unweighted options. The method is described in detail and compared with the classical least squares periodogram on a set of astrometric VLBI measurements of the ICRF quasar IERS B0642+449. It is uniformly applied to a collection of 259 ICRF3 quasars each with more than 200 epoch VLBI measurements, resulting in a list of 49 objects with quasi-periodic position changes above the $3\\sigma$ level, which warrant further investigation.","sentences":["Astronomical time series often have non-uniform sampling in time, or irregular cadences, with long gaps separating clusters of observations.","Some of these data sets are also explicitly non-Gaussian with respect to the expected model fit, or the simple mean.","The standard Lomb-Scargle periodogram is based on the least squares solution for a set of test periods and, therefore, is easily corrupted by a subset of statistical outliers or an intrinsically non-Gaussian population.","It can produce completely misleading results for heavy-tailed distribution of residuals.","We propose a robust 1-norm periodogram technique, which is based on the principles of robust statistical estimation.","This technique can be implemented in weighted or unweighted options.","The method is described in detail and compared with the classical least squares periodogram on a set of astrometric VLBI measurements of the ICRF quasar IERS B0642+449.","It is uniformly applied to a collection of 259 ICRF3 quasars each with more than 200 epoch VLBI measurements, resulting in a list of 49 objects with quasi-periodic position changes above the $3\\sigma$ level, which warrant further investigation."],"url":"http://arxiv.org/abs/2405.12324v1","category":"astro-ph.IM"}
{"created":"2024-05-20 18:28:07","title":"Verification of astrometrically accelerating stars from Hipparcos and Gaia: I. Methodology and application to HIP 44842","abstract":"A large number of candidate binary stars with apparent acceleration on the sky has emerged from analysis of astrometric data collected by the Hipparcos, Tycho-2, and Gaia space missions. Although the apparent acceleration can serve as a relatively reliable indicator of binarity, it provides scarce information about the orbital and physical parameters of the components. With an emphasis on the search for stellar-mass black holes and neutron stars hidden in binary systems, we start a broader effort to characterize the most promising candidates using follow-up ground-based observations. Accurate quantification of orbital and physical parameters of systems with dim or invisible companions requires combination of Hipparcos, Gaia, and precision spectroscopic measurements. In this paper, we review the necessary steps in this implementation and describe the improved Hipparcos-Gaia sample of long-term astrometric accelerations which includes correction of sky-correlated systematic errors using the vector spherical decomposition method. As an example, we study one Hipparcos star with a large acceleration, HIP 44842, where the companion is revealed to be a normal main sequence star.","sentences":["A large number of candidate binary stars with apparent acceleration on the sky has emerged from analysis of astrometric data collected by the Hipparcos, Tycho-2, and Gaia space missions.","Although the apparent acceleration can serve as a relatively reliable indicator of binarity, it provides scarce information about the orbital and physical parameters of the components.","With an emphasis on the search for stellar-mass black holes and neutron stars hidden in binary systems, we start a broader effort to characterize the most promising candidates using follow-up ground-based observations.","Accurate quantification of orbital and physical parameters of systems with dim or invisible companions requires combination of Hipparcos, Gaia, and precision spectroscopic measurements.","In this paper, we review the necessary steps in this implementation and describe the improved Hipparcos-Gaia sample of long-term astrometric accelerations which includes correction of sky-correlated systematic errors using the vector spherical decomposition method.","As an example, we study one Hipparcos star with a large acceleration, HIP 44842, where the companion is revealed to be a normal main sequence star."],"url":"http://arxiv.org/abs/2405.12315v1","category":"astro-ph.SR"}
{"created":"2024-05-20 18:04:59","title":"Perturbing the Gradient for Alleviating Meta Overfitting","abstract":"The reason for Meta Overfitting can be attributed to two factors: Mutual Non-exclusivity and the Lack of diversity, consequent to which a single global function can fit the support set data of all the meta-training tasks and fail to generalize to new unseen tasks. This issue is evidenced by low error rates on the meta-training tasks, but high error rates on new tasks. However, there can be a number of novel solutions to this problem keeping in mind any of the two objectives to be attained, i.e. to increase diversity in the tasks and to reduce the confidence of the model for some of the tasks. In light of the above, this paper proposes a number of solutions to tackle meta-overfitting on few-shot learning settings, such as few-shot sinusoid regression and few shot classification. Our proposed approaches demonstrate improved generalization performance compared to state-of-the-art baselines for learning in a non-mutually exclusive task setting. Overall, this paper aims to provide insights into tackling overfitting in meta-learning and to advance the field towards more robust and generalizable models.","sentences":["The reason for Meta Overfitting can be attributed to two factors: Mutual Non-exclusivity and the Lack of diversity, consequent to which a single global function can fit the support set data of all the meta-training tasks and fail to generalize to new unseen tasks.","This issue is evidenced by low error rates on the meta-training tasks, but high error rates on new tasks.","However, there can be a number of novel solutions to this problem keeping in mind any of the two objectives to be attained, i.e. to increase diversity in the tasks and to reduce the confidence of the model for some of the tasks.","In light of the above, this paper proposes a number of solutions to tackle meta-overfitting on few-shot learning settings, such as few-shot sinusoid regression and few shot classification.","Our proposed approaches demonstrate improved generalization performance compared to state-of-the-art baselines for learning in a non-mutually exclusive task setting.","Overall, this paper aims to provide insights into tackling overfitting in meta-learning and to advance the field towards more robust and generalizable models."],"url":"http://arxiv.org/abs/2405.12299v1","category":"cs.LG"}
{"created":"2024-05-20 18:00:36","title":"Theory of fractional quantum Hall liquids coupled to quantum light and emergent graviton-polaritons","abstract":"Recent breakthrough experiments have demonstrated how it is now possible to explore the dynamics of quantum Hall states interacting with quantum electromagnetic cavity fields. While the impact of strongly coupled non-local cavity modes on integer quantum Hall physics has been recently addressed, its effects on fractional quantum Hall (FQH) liquids -- and, more generally, fractionalized states of matter -- remain largely unexplored. In this work, we develop a theoretical framework for the understanding of FQH states coupled to quantum light. In particular, combining analytical arguments with tensor network simulations, we study the dynamics of a $\\nu=1/3$ Laughlin state in a single-mode cavity with finite electric field gradients. We find that the topological signatures of the FQH state remain robust against the non-local cavity vacuum fluctuations, as indicated by the endurance of the quantized Hall resistivity. The entanglement spectra, however, carry direct fingerprints of light-matter entanglement and topology, revealing peculiar polaritonic replicas of the $U(1)$ counting. As a further response to cavity fluctuations, we also find a squeezed FQH geometry, encoded in long-wavelength correlations. We additionally observe that moving to strong cavity field gradients leads to an instability towards a sliding Tomonaga-Luttinger liquid phase, featuring a strong density modulation in the gradient direction. Finally, by exploring the low-energy excited spectrum inside the FQH phase, we identify a new quasiparticle, the graviton-polariton, arising from the hybridization between quadrupolar FQH collective excitations (known as gravitons) and light. We discuss the experimental implications of our findings and possible extension of our results to more complex scenarios.","sentences":["Recent breakthrough experiments have demonstrated how it is now possible to explore the dynamics of quantum Hall states interacting with quantum electromagnetic cavity fields.","While the impact of strongly coupled non-local cavity modes on integer quantum Hall physics has been recently addressed, its effects on fractional quantum Hall (FQH) liquids -- and, more generally, fractionalized states of matter -- remain largely unexplored.","In this work, we develop a theoretical framework for the understanding of FQH states coupled to quantum light.","In particular, combining analytical arguments with tensor network simulations, we study the dynamics of a $\\nu=1/3$ Laughlin state in a single-mode cavity with finite electric field gradients.","We find that the topological signatures of the FQH state remain robust against the non-local cavity vacuum fluctuations, as indicated by the endurance of the quantized Hall resistivity.","The entanglement spectra, however, carry direct fingerprints of light-matter entanglement and topology, revealing peculiar polaritonic replicas of the $U(1)$ counting.","As a further response to cavity fluctuations, we also find a squeezed FQH geometry, encoded in long-wavelength correlations.","We additionally observe that moving to strong cavity field gradients leads to an instability towards a sliding Tomonaga-Luttinger liquid phase, featuring a strong density modulation in the gradient direction.","Finally, by exploring the low-energy excited spectrum inside the FQH phase, we identify a new quasiparticle, the graviton-polariton, arising from the hybridization between quadrupolar FQH collective excitations (known as gravitons) and light.","We discuss the experimental implications of our findings and possible extension of our results to more complex scenarios."],"url":"http://arxiv.org/abs/2405.12292v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 17:59:21","title":"Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning","abstract":"Recent studies indicate that large multimodal models (LMMs) are highly robust against natural distribution shifts, often surpassing previous baselines. Despite this, domain-specific adaptation is still necessary, particularly in specialized areas like healthcare. Due to the impracticality of fine-tuning LMMs given their vast parameter space, this work investigates in-context learning (ICL) as an effective alternative for enhancing LMMs' adaptability. We find that the success of ICL heavily relies on the choice of demonstration, mirroring challenges seen in large language models but introducing unique complexities for LMMs facing distribution shifts. Our study addresses this by evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context examples through a nearest example search based on feature similarity. We uncover that its effectiveness is limited by the deficiencies of pre-trained vision encoders under distribution shift scenarios. To address these challenges, we propose InvariantSelectPR, a novel method leveraging Class-conditioned Contrastive Invariance (CCI) for more robust demonstration selection. Specifically, CCI enhances pre-trained vision encoders by improving their discriminative capabilities across different classes and ensuring invariance to domain-specific variations. This enhancement allows the encoders to effectively identify and retrieve the most informative examples, which are then used to guide LMMs in adapting to new query samples under varying distributions. Our experiments show that InvariantSelectPR substantially improves the adaptability of LMMs, achieving significant performance gains on benchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on Camelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the baseline zero-shot performance.","sentences":["Recent studies indicate that large multimodal models (LMMs) are highly robust against natural distribution shifts, often surpassing previous baselines.","Despite this, domain-specific adaptation is still necessary, particularly in specialized areas like healthcare.","Due to the impracticality of fine-tuning LMMs given their vast parameter space, this work investigates in-context learning (ICL) as an effective alternative for enhancing LMMs' adaptability.","We find that the success of ICL heavily relies on the choice of demonstration, mirroring challenges seen in large language models but introducing unique complexities for LMMs facing distribution shifts.","Our study addresses this by evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context examples through a nearest example search based on feature similarity.","We uncover that its effectiveness is limited by the deficiencies of pre-trained vision encoders under distribution shift scenarios.","To address these challenges, we propose InvariantSelectPR, a novel method leveraging Class-conditioned Contrastive Invariance (CCI) for more robust demonstration selection.","Specifically, CCI enhances pre-trained vision encoders by improving their discriminative capabilities across different classes and ensuring invariance to domain-specific variations.","This enhancement allows the encoders to effectively identify and retrieve the most informative examples, which are then used to guide LMMs in adapting to new query samples under varying distributions.","Our experiments show that InvariantSelectPR substantially improves the adaptability of LMMs, achieving significant performance gains on benchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on Camelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the baseline zero-shot performance."],"url":"http://arxiv.org/abs/2405.12217v1","category":"cs.CV"}
{"created":"2024-05-20 17:52:40","title":"EGAN: Evolutional GAN for Ransomware Evasion","abstract":"Adversarial Training is a proven defense strategy against adversarial malware. However, generating adversarial malware samples for this type of training presents a challenge because the resulting adversarial malware needs to remain evasive and functional. This work proposes an attack framework, EGAN, to address this limitation. EGAN leverages an Evolution Strategy and Generative Adversarial Network to select a sequence of attack actions that can mutate a Ransomware file while preserving its original functionality. We tested this framework on popular AI-powered commercial antivirus systems listed on VirusTotal and demonstrated that our framework is capable of bypassing the majority of these systems. Moreover, we evaluated whether the EGAN attack framework can evade other commercial non-AI antivirus solutions. Our results indicate that the adversarial ransomware generated can increase the probability of evading some of them.","sentences":["Adversarial Training is a proven defense strategy against adversarial malware.","However, generating adversarial malware samples for this type of training presents a challenge because the resulting adversarial malware needs to remain evasive and functional.","This work proposes an attack framework, EGAN, to address this limitation.","EGAN leverages an Evolution Strategy and Generative Adversarial Network to select a sequence of attack actions that can mutate a Ransomware file while preserving its original functionality.","We tested this framework on popular AI-powered commercial antivirus systems listed on VirusTotal and demonstrated that our framework is capable of bypassing the majority of these systems.","Moreover, we evaluated whether the EGAN attack framework can evade other commercial non-AI antivirus solutions.","Our results indicate that the adversarial ransomware generated can increase the probability of evading some of them."],"url":"http://arxiv.org/abs/2405.12266v1","category":"cs.CR"}
{"created":"2024-05-20 17:45:26","title":"Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving","abstract":"Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.   To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.","sentences":["Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes.","Today's best LLMs clearly possess some reasoning processes.","The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task.","We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels.","These coarse skill labels look interpretable to humans.   ","To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments.","(a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.","(b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed.","Then it is presented with randomly selected exemplar solved questions associated with that skill label.","This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models.","The methodology presented is domain-agnostic, even though this article applies it to math problems."],"url":"http://arxiv.org/abs/2405.12205v1","category":"cs.AI"}
{"created":"2024-05-20 17:39:29","title":"Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution","abstract":"In this work, we present an arbitrary-scale super-resolution (SR) method to enhance the resolution of scientific data, which often involves complex challenges such as continuity, multi-scale physics, and the intricacies of high-frequency signals. Grounded in operator learning, the proposed method is resolution-invariant. The core of our model is a hierarchical neural operator that leverages a Galerkin-type self-attention mechanism, enabling efficient learning of mappings between function spaces. Sinc filters are used to facilitate the information transfer across different levels in the hierarchy, thereby ensuring representation equivalence in the proposed neural operator. Additionally, we introduce a learnable prior structure that is derived from the spectral resizing of the input data. This loss prior is model-agnostic and is designed to dynamically adjust the weighting of pixel contributions, thereby balancing gradients effectively across the model. We conduct extensive experiments on diverse datasets from different domains and demonstrate consistent improvements compared to strong baselines, which consist of various state-of-the-art SR methods.","sentences":["In this work, we present an arbitrary-scale super-resolution (SR) method to enhance the resolution of scientific data, which often involves complex challenges such as continuity, multi-scale physics, and the intricacies of high-frequency signals.","Grounded in operator learning, the proposed method is resolution-invariant.","The core of our model is a hierarchical neural operator that leverages a Galerkin-type self-attention mechanism, enabling efficient learning of mappings between function spaces.","Sinc filters are used to facilitate the information transfer across different levels in the hierarchy, thereby ensuring representation equivalence in the proposed neural operator.","Additionally, we introduce a learnable prior structure that is derived from the spectral resizing of the input data.","This loss prior is model-agnostic and is designed to dynamically adjust the weighting of pixel contributions, thereby balancing gradients effectively across the model.","We conduct extensive experiments on diverse datasets from different domains and demonstrate consistent improvements compared to strong baselines, which consist of various state-of-the-art SR methods."],"url":"http://arxiv.org/abs/2405.12202v1","category":"cs.CV"}
{"created":"2024-05-20 17:31:16","title":"Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey","abstract":"As Large Language Models (LLMs), including ChatGPT and analogous systems, continue to advance, their robust natural language processing capabilities and diverse applications have garnered considerable attention. Nonetheless, despite the increasing acknowledgment of the convergence of Artificial Intelligence (AI) and Software Engineering (SE), there is a lack of studies involving the impact of this convergence on the practices and perceptions of software developers. Understanding how software developers perceive and engage with AI tools, such as ChatGPT, is essential for elucidating the impact and potential challenges of incorporating AI-driven tools in the software development process. In this paper, we conducted a survey with 207 software developers to understand the impact of ChatGPT on software quality, productivity, and job satisfaction. Furthermore, the study delves into developers' expectations regarding future adaptations of ChatGPT, concerns about potential job displacement, and perspectives on regulatory interventions.","sentences":["As Large Language Models (LLMs), including ChatGPT and analogous systems, continue to advance, their robust natural language processing capabilities and diverse applications have garnered considerable attention.","Nonetheless, despite the increasing acknowledgment of the convergence of Artificial Intelligence (AI) and Software Engineering (SE), there is a lack of studies involving the impact of this convergence on the practices and perceptions of software developers.","Understanding how software developers perceive and engage with AI tools, such as ChatGPT, is essential for elucidating the impact and potential challenges of incorporating AI-driven tools in the software development process.","In this paper, we conducted a survey with 207 software developers to understand the impact of ChatGPT on software quality, productivity, and job satisfaction.","Furthermore, the study delves into developers' expectations regarding future adaptations of ChatGPT, concerns about potential job displacement, and perspectives on regulatory interventions."],"url":"http://arxiv.org/abs/2405.12195v1","category":"cs.SE"}
{"created":"2024-05-20 17:29:44","title":"Histotripsy of blood clots within a hollow cylindrical transducer for aspiration thrombectomy applications","abstract":"Thrombolytic occlusions in stroke, pulmonary embolism and the peripheral vasculature are increasingly treated with aspiration, a catheter-based approach that employs suction to extract clots through a hollow catheter lumen. Unfortunately, aspiration is frequently unsuccessful in extracting more challenging clots, which can become corked in the distal tip. We hypothesize that clot extraction can be enhanced by using histotripsy to degrade the mechanical integrity of clot material within the lumen of a hollow cylindrical transducer which can be situated at the tip of an aspiration catheter. To demonstrate the feasibility of degrading clot material within the lumen of a hollow cylindrical transducer, the effect of pulsing schemes on lesion generation within clots was assessed using a retracted clot model. A radially polarized cylindrical transducer (2.5 mm and 3.3 mm for inner and outer diameter, 2.5 mm length, PZT) working at 6.1 MHz was used to degrade retracted porcine clots with pulse lengths of 10, 20, and 100 us, pulse repetition frequencies (PRF) of 100, 500, and 1000 Hz, using treatment times from 0.1 to 10 seconds (n = 5 clots per condition). 3D ultrasound scans and bisected optical examinations of treated clots confirmed the formation of liquified zones. Lesions could form within 0.1 seconds along the central axis of the transducer and then grow in diameter and length over time. The lesion volume was found to be highly dependent on the exposure scheme, with the largest lesion volume associated with the 10 us pulse length 1000 kHz PRF case. Collectively these results demonstrate the feasibility of degrading blood clots within hollow cylindrical transducers, which suggests their potential for enhancing aspiration based mechanical thrombectomy.","sentences":["Thrombolytic occlusions in stroke, pulmonary embolism and the peripheral vasculature are increasingly treated with aspiration, a catheter-based approach that employs suction to extract clots through a hollow catheter lumen.","Unfortunately, aspiration is frequently unsuccessful in extracting more challenging clots, which can become corked in the distal tip.","We hypothesize that clot extraction can be enhanced by using histotripsy to degrade the mechanical integrity of clot material within the lumen of a hollow cylindrical transducer which can be situated at the tip of an aspiration catheter.","To demonstrate the feasibility of degrading clot material within the lumen of a hollow cylindrical transducer, the effect of pulsing schemes on lesion generation within clots was assessed using a retracted clot model.","A radially polarized cylindrical transducer (2.5 mm and 3.3 mm for inner and outer diameter, 2.5 mm length, PZT) working at 6.1 MHz was used to degrade retracted porcine clots with pulse lengths of 10, 20, and 100 us, pulse repetition frequencies (PRF) of 100, 500, and 1000 Hz, using treatment times from 0.1 to 10 seconds (n = 5 clots per condition).","3D ultrasound scans and bisected optical examinations of treated clots confirmed the formation of liquified zones.","Lesions could form within 0.1 seconds along the central axis of the transducer and then grow in diameter and length over time.","The lesion volume was found to be highly dependent on the exposure scheme, with the largest lesion volume associated with the 10 us pulse length 1000 kHz","PRF case.","Collectively these results demonstrate the feasibility of degrading blood clots within hollow cylindrical transducers, which suggests their potential for enhancing aspiration based mechanical thrombectomy."],"url":"http://arxiv.org/abs/2405.12194v1","category":"physics.med-ph"}
{"created":"2024-05-20 17:24:02","title":"Quantitative asymptotics for polynomial patterns in the primes","abstract":"We prove quantitative estimates for averages of the von Mangoldt and M\\\"obius functions along polynomial progressions $n+P_1(m),\\ldots, n+P_k(m)$ for a large class of polynomials $P_i$. The error terms obtained save an arbitrary power of logarithm, matching the classical Siegel--Walfisz error term. These results give the first quantitative bounds for the Tao--Ziegler polynomial patterns in the primes result, and in the M\\\"obius case they are new even qualitatively for some collections of polynomials.   The proofs are based on a quantitative generalised von Neumann theorem of Peluse, a recent result of Leng on strong bounds for the Gowers uniformity of the primes, and analysis of a ``Siegel model'' for the von Mangoldt function along polynomial progressions.","sentences":["We prove quantitative estimates for averages of the von Mangoldt and M\\\"obius functions along polynomial progressions $n+P_1(m),\\ldots, n+P_k(m)$ for a large class of polynomials $P_i$. The error terms obtained save an arbitrary power of logarithm, matching the classical Siegel--Walfisz error term.","These results give the first quantitative bounds for the Tao--Ziegler polynomial patterns in the primes result, and in the M\\\"obius case they are new even qualitatively for some collections of polynomials.   ","The proofs are based on a quantitative generalised von Neumann theorem of Peluse, a recent result of Leng on strong bounds for the Gowers uniformity of the primes, and analysis of a ``Siegel model'' for the von Mangoldt function along polynomial progressions."],"url":"http://arxiv.org/abs/2405.12190v1","category":"math.NT"}
{"created":"2024-05-20 17:09:58","title":"Multi-order Graph Clustering with Adaptive Node-level Weight Learning","abstract":"Current graph clustering methods emphasize individual node and edge con nections, while ignoring higher-order organization at the level of motif. Re cently, higher-order graph clustering approaches have been designed by motif based hypergraphs. However, these approaches often suffer from hypergraph fragmentation issue seriously, which degrades the clustering performance greatly. Moreover, real-world graphs usually contain diverse motifs, with nodes participating in multiple motifs. A key challenge is how to achieve precise clustering results by integrating information from multiple motifs at the node level. In this paper, we propose a multi-order graph clustering model (MOGC) to integrate multiple higher-order structures and edge connections at node level. MOGC employs an adaptive weight learning mechanism to au tomatically adjust the contributions of different motifs for each node. This not only tackles hypergraph fragmentation issue but enhances clustering accuracy. MOGC is efficiently solved by an alternating minimization algo rithm. Experiments on seven real-world datasets illustrate the effectiveness of MOGC.","sentences":["Current graph clustering methods emphasize individual node and edge con nections, while ignoring higher-order organization at the level of motif.","Re cently, higher-order graph clustering approaches have been designed by motif based hypergraphs.","However, these approaches often suffer from hypergraph fragmentation issue seriously, which degrades the clustering performance greatly.","Moreover, real-world graphs usually contain diverse motifs, with nodes participating in multiple motifs.","A key challenge is how to achieve precise clustering results by integrating information from multiple motifs at the node level.","In this paper, we propose a multi-order graph clustering model (MOGC) to integrate multiple higher-order structures and edge connections at node level.","MOGC employs an adaptive weight learning mechanism to au tomatically adjust the contributions of different motifs for each node.","This not only tackles hypergraph fragmentation issue but enhances clustering accuracy.","MOGC is efficiently solved by an alternating minimization algo rithm.","Experiments on seven real-world datasets illustrate the effectiveness of MOGC."],"url":"http://arxiv.org/abs/2405.12183v1","category":"cs.LG"}
{"created":"2024-05-20 17:06:24","title":"Building Temporal Kernels with Orthogonal Polynomials","abstract":"We introduce a class of models named PLEIADES (PoLynomial Expansion In Adaptive Distributed Event-based Systems), which contains temporal convolution kernels generated from orthogonal polynomial basis functions. We focus on interfacing these networks with event-based data to perform online spatiotemporal classification and detection with low latency. By virtue of using structured temporal kernels and event-based data, we have the freedom to vary the sample rate of the data along with the discretization step-size of the network without additional finetuning. We experimented with three event-based benchmarks and obtained state-of-the-art results on all three by large margins with significantly smaller memory and compute costs. We achieved: 1) 99.59% accuracy with 192K parameters on the DVS128 hand gesture recognition dataset and 100% with a small additional output filter; 2) 99.58% test accuracy with 277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with 576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.","sentences":["We introduce a class of models named PLEIADES (PoLynomial Expansion In Adaptive Distributed Event-based Systems), which contains temporal convolution kernels generated from orthogonal polynomial basis functions.","We focus on interfacing these networks with event-based data to perform online spatiotemporal classification and detection with low latency.","By virtue of using structured temporal kernels and event-based data, we have the freedom to vary the sample rate of the data along with the discretization step-size of the network without additional finetuning.","We experimented with three event-based benchmarks and obtained state-of-the-art results on all three by large margins with significantly smaller memory and compute costs.","We achieved: 1) 99.59% accuracy with 192K parameters on the DVS128 hand gesture recognition dataset and 100% with a small additional output filter; 2) 99.58% test accuracy with 277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with 576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset."],"url":"http://arxiv.org/abs/2405.12179v1","category":"cs.LG"}
{"created":"2024-05-20 16:51:25","title":"Open-Source Assessments of AI Capabilities: The Proliferation of AI Analysis Tools, Replicating Competitor Models, and the Zhousidun Dataset","abstract":"The integration of artificial intelligence (AI) into military capabilities has become a norm for major military power across the globe. Understanding how these AI models operate is essential for maintaining strategic advantages and ensuring security. This paper demonstrates an open-source methodology for analyzing military AI models through a detailed examination of the Zhousidun dataset, a Chinese-originated dataset that exhaustively labels critical components on American and Allied destroyers. By demonstrating the replication of a state-of-the-art computer vision model on this dataset, we illustrate how open-source tools can be leveraged to assess and understand key military AI capabilities. This methodology offers a robust framework for evaluating the performance and potential of AI-enabled military capabilities, thus enhancing the accuracy and reliability of strategic assessments.","sentences":["The integration of artificial intelligence (AI) into military capabilities has become a norm for major military power across the globe.","Understanding how these AI models operate is essential for maintaining strategic advantages and ensuring security.","This paper demonstrates an open-source methodology for analyzing military AI models through a detailed examination of the Zhousidun dataset, a Chinese-originated dataset that exhaustively labels critical components on American and Allied destroyers.","By demonstrating the replication of a state-of-the-art computer vision model on this dataset, we illustrate how open-source tools can be leveraged to assess and understand key military AI capabilities.","This methodology offers a robust framework for evaluating the performance and potential of AI-enabled military capabilities, thus enhancing the accuracy and reliability of strategic assessments."],"url":"http://arxiv.org/abs/2405.12167v1","category":"cs.CY"}
{"created":"2024-05-20 16:47:22","title":"Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging","abstract":"The rapid advancement of large language models has given rise to a plethora of applications across a myriad of real-world tasks, mainly centered on aligning with human intent. However, the complexities inherent in human intent necessitate a dependence on labor-intensive and time-consuming human evaluation. To alleviate this constraint, we delve into the paradigm of employing open-source large language models as evaluators, aligning with the prevailing trend of utilizing GPT-4. Particularly, we present a step-by-step evaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained \\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through bran\\textbf{C}hing and bridging. Specifically, the branching operation dissects the evaluation task into various dimensions and granularities, thereby alleviating the challenges associated with evaluation. Concurrently, the bridging operation amalgamates diverse training datasets, augmenting the variety of evaluation tasks. In experimental trials, our 7B model consistently outperforms open-source larger-scale evaluation models across various widely adopted benchmarks in terms of both \\textit{Agreement} and \\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ the fine-grained correction capabilities induced by the evaluation model to refine multiple model responses, and the results show that the refinement elevates the quality of responses, leading to an improvement of 1-2 points on the MT-Bench. Our code is available at Github\\footnote{\\url{https://github.com/dropreg/Fennec}}.","sentences":["The rapid advancement of large language models has given rise to a plethora of applications across a myriad of real-world tasks, mainly centered on aligning with human intent.","However, the complexities inherent in human intent necessitate a dependence on labor-intensive and time-consuming human evaluation.","To alleviate this constraint, we delve into the paradigm of employing open-source large language models as evaluators, aligning with the prevailing trend of utilizing GPT-4.","Particularly, we present a step-by-step evaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained \\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through bran\\textbf{C}hing and bridging.","Specifically, the branching operation dissects the evaluation task into various dimensions and granularities, thereby alleviating the challenges associated with evaluation.","Concurrently, the bridging operation amalgamates diverse training datasets, augmenting the variety of evaluation tasks.","In experimental trials, our 7B model consistently outperforms open-source larger-scale evaluation models across various widely adopted benchmarks in terms of both \\textit{Agreement} and \\textit{Consistency}, closely approaching the capabilities of GPT-4.","We employ the fine-grained correction capabilities induced by the evaluation model to refine multiple model responses, and the results show that the refinement elevates the quality of responses, leading to an improvement of 1-2 points on the MT-Bench.","Our code is available at Github\\footnote{\\url{https://github.com/dropreg/Fennec}}."],"url":"http://arxiv.org/abs/2405.12163v1","category":"cs.CL"}
{"created":"2024-05-20 16:23:40","title":"Bangladeshi Native Vehicle Detection in Wild","abstract":"The success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems. To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh. 17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images. Each image width is set to at least 1280px. The dataset's average vehicle bounding box-to-image ratio is 4.7036. This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios. In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset's effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use. The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union (IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities.","sentences":["The success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems.","To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh.","17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images.","Each image width is set to at least 1280px.","The dataset's average vehicle bounding box-to-image ratio is 4.7036.","This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios.","In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8.","These dataset's effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use.","The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union (IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774.","The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95.","The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities."],"url":"http://arxiv.org/abs/2405.12150v1","category":"cs.CV"}
{"created":"2024-05-20 16:19:02","title":"Eliciting Problem Specifications via Large Language Models","abstract":"Cognitive systems generally require a human to translate a problem definition into some specification that the cognitive system can use to attempt to solve the problem or perform the task. In this paper, we illustrate that large language models (LLMs) can be utilized to map a problem class, defined in natural language, into a semi-formal specification that can then be utilized by an existing reasoning and learning system to solve instances from the problem class. We present the design of LLM-enabled cognitive task analyst agent(s). Implemented with LLM agents, this system produces a definition of problem spaces for tasks specified in natural language. LLM prompts are derived from the definition of problem spaces in the AI literature and general problem-solving strategies (Polya's How to Solve It). A cognitive system can then use the problem-space specification, applying domain-general problem solving strategies (\"weak methods\" such as search), to solve multiple instances of problems from the problem class. This result, while preliminary, suggests the potential for speeding cognitive systems research via disintermediation of problem formulation while also retaining core capabilities of cognitive systems, such as robust inference and online learning.","sentences":["Cognitive systems generally require a human to translate a problem definition into some specification that the cognitive system can use to attempt to solve the problem or perform the task.","In this paper, we illustrate that large language models (LLMs) can be utilized to map a problem class, defined in natural language, into a semi-formal specification that can then be utilized by an existing reasoning and learning system to solve instances from the problem class.","We present the design of LLM-enabled cognitive task analyst agent(s).","Implemented with LLM agents, this system produces a definition of problem spaces for tasks specified in natural language.","LLM prompts are derived from the definition of problem spaces in the AI literature and general problem-solving strategies (Polya's How to Solve It).","A cognitive system can then use the problem-space specification, applying domain-general problem solving strategies (\"weak methods\" such as search), to solve multiple instances of problems from the problem class.","This result, while preliminary, suggests the potential for speeding cognitive systems research via disintermediation of problem formulation while also retaining core capabilities of cognitive systems, such as robust inference and online learning."],"url":"http://arxiv.org/abs/2405.12147v1","category":"cs.AI"}
{"created":"2024-05-20 15:52:33","title":"Auger photoemission as a laser-like coherent cathode","abstract":"In pursuit of quantum advancements across disciplines, a bright and coherent electron source is expected to be a cornerstone of diverse applications including electron microscopy, laser accelerators, and free electron lasers. Current cathodes, such as cold field and photoemission, can generate high-quality electron beams with different cathode materials, geometric configurations, and laser excitation profiles, but their maintenance of both quantum coherence and high beam brightness suffers from the space-charge repulsion of many electrons. Here, we propose a new mechanism to provide collective emission of coherent electrons based on Auger photoemission. Our approach leverages a photon-induced four-level Auger process that necessitates a combination of photoemission and Auger recombination. The Auger electrons, energized through a recycling process of photoelectrons, emit collectively into the vacuum as secondary electrons. We compare coherent and incoherent Auger photoemission, identifying that the working condition of the coherent photoemission requires population inversion, akin to the four-level laser system. Our work provides insights for experimental realization and nanofabrication of Auger photocathodes, addressing a critical need in advancing quantum technologies relating to correlated coherent sources.","sentences":["In pursuit of quantum advancements across disciplines, a bright and coherent electron source is expected to be a cornerstone of diverse applications including electron microscopy, laser accelerators, and free electron lasers.","Current cathodes, such as cold field and photoemission, can generate high-quality electron beams with different cathode materials, geometric configurations, and laser excitation profiles, but their maintenance of both quantum coherence and high beam brightness suffers from the space-charge repulsion of many electrons.","Here, we propose a new mechanism to provide collective emission of coherent electrons based on Auger photoemission.","Our approach leverages a photon-induced four-level Auger process that necessitates a combination of photoemission and Auger recombination.","The Auger electrons, energized through a recycling process of photoelectrons, emit collectively into the vacuum as secondary electrons.","We compare coherent and incoherent Auger photoemission, identifying that the working condition of the coherent photoemission requires population inversion, akin to the four-level laser system.","Our work provides insights for experimental realization and nanofabrication of Auger photocathodes, addressing a critical need in advancing quantum technologies relating to correlated coherent sources."],"url":"http://arxiv.org/abs/2405.12133v1","category":"quant-ph"}
{"created":"2024-05-20 15:42:23","title":"Prompt Learning for Generalized Vehicle Routing","abstract":"Neural combinatorial optimization (NCO) is a promising learning-based approach to solving various vehicle routing problems without much manual algorithm design. However, the current NCO methods mainly focus on the in-distribution performance, while the real-world problem instances usually come from different distributions. A costly fine-tuning approach or generalized model retraining from scratch could be needed to tackle the out-of-distribution instances. Unlike the existing methods, this work investigates an efficient prompt learning approach in NCO for cross-distribution adaptation. To be concrete, we propose a novel prompt learning method to facilitate fast zero-shot adaptation of a pre-trained model to solve routing problem instances from different distributions. The proposed model learns a set of prompts among various distributions and then selects the best-matched one to prompt a pre-trained attention model for each problem instance. Extensive experiments show that the proposed prompt learning approach facilitates the fast adaptation of pre-trained routing models. It also outperforms existing generalized models on both in-distribution prediction and zero-shot generalization to a diverse set of new tasks. Our code implementation is available online https://github.com/FeiLiu36/PromptVRP.","sentences":["Neural combinatorial optimization (NCO) is a promising learning-based approach to solving various vehicle routing problems without much manual algorithm design.","However, the current NCO methods mainly focus on the in-distribution performance, while the real-world problem instances usually come from different distributions.","A costly fine-tuning approach or generalized model retraining from scratch could be needed to tackle the out-of-distribution instances.","Unlike the existing methods, this work investigates an efficient prompt learning approach in NCO for cross-distribution adaptation.","To be concrete, we propose a novel prompt learning method to facilitate fast zero-shot adaptation of a pre-trained model to solve routing problem instances from different distributions.","The proposed model learns a set of prompts among various distributions and then selects the best-matched one to prompt a pre-trained attention model for each problem instance.","Extensive experiments show that the proposed prompt learning approach facilitates the fast adaptation of pre-trained routing models.","It also outperforms existing generalized models on both in-distribution prediction and zero-shot generalization to a diverse set of new tasks.","Our code implementation is available online https://github.com/FeiLiu36/PromptVRP."],"url":"http://arxiv.org/abs/2405.12262v1","category":"cs.LG"}
{"created":"2024-05-20 15:41:38","title":"Monitored long-range interacting systems: spin-wave theory for quantum trajectories","abstract":"We introduce a stochastic spin-wave theory tailored to describe quantum trajectories in continuously monitored long-range interacting spin systems. Our method, based on the bosonization of spin-wave excitations on top of a strong collective polarization, enables the efficient simulation of large-scale interacting spins, offering insights into nonlinear features of the dynamics such as entanglement and trajectory correlations. We showcase the versatility of our framework by exploring an entanglement phase transition in a monitored spin system with power-law interactions and dwelling on how our method mitigates the experimental challenges of post-selection in detecting monitored quantum phases.","sentences":["We introduce a stochastic spin-wave theory tailored to describe quantum trajectories in continuously monitored long-range interacting spin systems.","Our method, based on the bosonization of spin-wave excitations on top of a strong collective polarization, enables the efficient simulation of large-scale interacting spins, offering insights into nonlinear features of the dynamics such as entanglement and trajectory correlations.","We showcase the versatility of our framework by exploring an entanglement phase transition in a monitored spin system with power-law interactions and dwelling on how our method mitigates the experimental challenges of post-selection in detecting monitored quantum phases."],"url":"http://arxiv.org/abs/2405.12124v1","category":"quant-ph"}
{"created":"2024-05-20 15:37:55","title":"Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation","abstract":"Large language models (LLMs) are revolutionizing conversational recommender systems by adeptly indexing item content, understanding complex conversational contexts, and generating relevant item titles. However, controlling the distribution of recommended items remains a challenge. This leads to suboptimal performance due to the failure to capture rapidly changing data distributions, such as item popularity, on targeted conversational recommendation platforms. In conversational recommendation, LLMs recommend items by generating the titles (as multiple tokens) autoregressively, making it difficult to obtain and control the recommendations over all items. Thus, we propose a Reindex-Then-Adapt (RTA) framework, which converts multi-token item titles into single tokens within LLMs, and then adjusts the probability distributions over these single-token item titles accordingly. The RTA framework marries the benefits of both LLMs and traditional recommender systems (RecSys): understanding complex queries as LLMs do; while efficiently controlling the recommended item distributions in conversational recommendations as traditional RecSys do. Our framework demonstrates improved accuracy metrics across three different conversational recommendation datasets and two adaptation settings","sentences":["Large language models (LLMs) are revolutionizing conversational recommender systems by adeptly indexing item content, understanding complex conversational contexts, and generating relevant item titles.","However, controlling the distribution of recommended items remains a challenge.","This leads to suboptimal performance due to the failure to capture rapidly changing data distributions, such as item popularity, on targeted conversational recommendation platforms.","In conversational recommendation, LLMs recommend items by generating the titles (as multiple tokens) autoregressively, making it difficult to obtain and control the recommendations over all items.","Thus, we propose a Reindex-Then-Adapt (RTA) framework, which converts multi-token item titles into single tokens within LLMs, and then adjusts the probability distributions over these single-token item titles accordingly.","The RTA framework marries the benefits of both LLMs and traditional recommender systems (RecSys): understanding complex queries as LLMs do; while efficiently controlling the recommended item distributions in conversational recommendations as traditional RecSys do.","Our framework demonstrates improved accuracy metrics across three different conversational recommendation datasets and two adaptation settings"],"url":"http://arxiv.org/abs/2405.12119v1","category":"cs.IR"}
{"created":"2024-05-20 15:15:17","title":"Collective Quantum Entanglement in Molecular Cavity Optomechanics","abstract":"We propose an optomechanical scheme for reaching quantum entanglement in vibration polaritons. The system involves $N$ molecules, whose vibrations can be fairly entangled with plasmonic cavities. We find that the vibration-photon entanglement can exist at room temperature and is robust against thermal noise. We further demonstrate the quantum entanglement between the vibrational modes through the plasmonic cavities, which shows a delocalized nature and an incredible enhancement with the number of molecules. The underlying mechanism for the entanglement is attributed to the strong vibration-cavity coupling which possesses collectivity. Our results provide a molecular optomechanical scheme which offers a promising platform for the study of noise-free quantum resources and macroscopic quantum phenomena.","sentences":["We propose an optomechanical scheme for reaching quantum entanglement in vibration polaritons.","The system involves $N$ molecules, whose vibrations can be fairly entangled with plasmonic cavities.","We find that the vibration-photon entanglement can exist at room temperature and is robust against thermal noise.","We further demonstrate the quantum entanglement between the vibrational modes through the plasmonic cavities, which shows a delocalized nature and an incredible enhancement with the number of molecules.","The underlying mechanism for the entanglement is attributed to the strong vibration-cavity coupling which possesses collectivity.","Our results provide a molecular optomechanical scheme which offers a promising platform for the study of noise-free quantum resources and macroscopic quantum phenomena."],"url":"http://arxiv.org/abs/2405.12102v2","category":"quant-ph"}
{"created":"2024-05-20 15:15:14","title":"Sustainable business decision modelling with blockchain and digital twins: A survey","abstract":"Industry 4.0 and beyond will rely heavily on sustainable Business Decision Modelling (BDM) that can be accelerated by blockchain and Digital Twin (DT) solutions. BDM is built on models and frameworks refined by key identification factors, data analysis, and mathematical or computational aspects applicable to complex business scenarios. Gaining actionable intelligence from collected data for BDM requires a carefully considered infrastructure to ensure data transparency, security, accessibility and sustainability. Organisations should consider social, economic and environmental factors (based on the triple bottom line approach) to ensure sustainability when integrating such an infrastructure. These sustainability features directly impact BDM concerning resource optimisation, stakeholder engagement, regulatory compliance and environmental impacts. To further understand these segments, taxonomies are defined to evaluate blockchain and DT sustainability features based on an in-depth review of the current state-of-the-art research. Detailed comparative evaluations provide insight into the reachability of the sustainable solution in terms of ideologies, access control and performance overheads. Several research questions are put forward to motivate further research that significantly impacts BDM. Finally, a case study based on an exemplary supply chain management system is presented to show the interoperability of blockchain and DT with BDM.","sentences":["Industry 4.0 and beyond will rely heavily on sustainable Business Decision Modelling (BDM) that can be accelerated by blockchain and Digital Twin (DT) solutions.","BDM is built on models and frameworks refined by key identification factors, data analysis, and mathematical or computational aspects applicable to complex business scenarios.","Gaining actionable intelligence from collected data for BDM requires a carefully considered infrastructure to ensure data transparency, security, accessibility and sustainability.","Organisations should consider social, economic and environmental factors (based on the triple bottom line approach) to ensure sustainability when integrating such an infrastructure.","These sustainability features directly impact BDM concerning resource optimisation, stakeholder engagement, regulatory compliance and environmental impacts.","To further understand these segments, taxonomies are defined to evaluate blockchain and DT sustainability features based on an in-depth review of the current state-of-the-art research.","Detailed comparative evaluations provide insight into the reachability of the sustainable solution in terms of ideologies, access control and performance overheads.","Several research questions are put forward to motivate further research that significantly impacts BDM.","Finally, a case study based on an exemplary supply chain management system is presented to show the interoperability of blockchain and DT with BDM."],"url":"http://arxiv.org/abs/2405.12101v1","category":"cs.NI"}
{"created":"2024-05-20 14:43:46","title":"GAN-GRID: A Novel Generative Attack on Smart Grid Stability Prediction","abstract":"The smart grid represents a pivotal innovation in modernizing the electricity sector, offering an intelligent, digitalized energy network capable of optimizing energy delivery from source to consumer. It hence represents the backbone of the energy sector of a nation. Due to its central role, the availability of the smart grid is paramount and is hence necessary to have in-depth control of its operations and safety. To this aim, researchers developed multiple solutions to assess the smart grid's stability and guarantee that it operates in a safe state. Artificial intelligence and Machine learning algorithms have proven to be effective measures to accurately predict the smart grid's stability. Despite the presence of known adversarial attacks and potential solutions, currently, there exists no standardized measure to protect smart grids against this threat, leaving them open to new adversarial attacks. In this paper, we propose GAN-GRID a novel adversarial attack targeting the stability prediction system of a smart grid tailored to real-world constraints. Our findings reveal that an adversary armed solely with the stability model's output, devoid of data or model knowledge, can craft data classified as stable with an Attack Success Rate (ASR) of 0.99. Also by manipulating authentic data and sensor values, the attacker can amplify grid issues, potentially undetected due to a compromised stability prediction system. These results underscore the imperative of fortifying smart grid security mechanisms against adversarial manipulation to uphold system stability and reliability.","sentences":["The smart grid represents a pivotal innovation in modernizing the electricity sector, offering an intelligent, digitalized energy network capable of optimizing energy delivery from source to consumer.","It hence represents the backbone of the energy sector of a nation.","Due to its central role, the availability of the smart grid is paramount and is hence necessary to have in-depth control of its operations and safety.","To this aim, researchers developed multiple solutions to assess the smart grid's stability and guarantee that it operates in a safe state.","Artificial intelligence and Machine learning algorithms have proven to be effective measures to accurately predict the smart grid's stability.","Despite the presence of known adversarial attacks and potential solutions, currently, there exists no standardized measure to protect smart grids against this threat, leaving them open to new adversarial attacks.","In this paper, we propose GAN-GRID a novel adversarial attack targeting the stability prediction system of a smart grid tailored to real-world constraints.","Our findings reveal that an adversary armed solely with the stability model's output, devoid of data or model knowledge, can craft data classified as stable with an Attack Success Rate (ASR) of 0.99.","Also by manipulating authentic data and sensor values, the attacker can amplify grid issues, potentially undetected due to a compromised stability prediction system.","These results underscore the imperative of fortifying smart grid security mechanisms against adversarial manipulation to uphold system stability and reliability."],"url":"http://arxiv.org/abs/2405.12076v1","category":"cs.CR"}
{"created":"2024-05-20 14:41:48","title":"Goal-Oriented Communication for Networked Control Assisted by Reconfigurable Meta-Surfaces","abstract":"In this paper, we develop a theoretical framework for goal-oriented communication assisted by reconfigurable meta-surfaces in the context of networked control systems. The relation to goal-oriented communication stems from the fact that optimization of the phase shifts of the meta-surfaces is guided by the performance of networked control systems tasks. To that end, we consider a networked control system in which a set of sensors observe the states of a set of physical processes, and communicate this information over an unreliable wireless channel assisted by a reconfigurable intelligent surface with multiple reflecting elements to a set of controllers that correct the behaviors of the physical processes based on the received information. Our objective is to find the optimal control policy for the controllers and the optimal phase policy for the reconfigurable intelligent surface that jointly minimize a regulation cost function associated with the networked control system. We characterize these policies, and also propose an approximate solution based on a semi-definite relaxation technique.","sentences":["In this paper, we develop a theoretical framework for goal-oriented communication assisted by reconfigurable meta-surfaces in the context of networked control systems.","The relation to goal-oriented communication stems from the fact that optimization of the phase shifts of the meta-surfaces is guided by the performance of networked control systems tasks.","To that end, we consider a networked control system in which a set of sensors observe the states of a set of physical processes, and communicate this information over an unreliable wireless channel assisted by a reconfigurable intelligent surface with multiple reflecting elements to a set of controllers that correct the behaviors of the physical processes based on the received information.","Our objective is to find the optimal control policy for the controllers and the optimal phase policy for the reconfigurable intelligent surface that jointly minimize a regulation cost function associated with the networked control system.","We characterize these policies, and also propose an approximate solution based on a semi-definite relaxation technique."],"url":"http://arxiv.org/abs/2405.12073v1","category":"cs.IT"}
{"created":"2024-05-20 14:40:26","title":"AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements","abstract":"Image understanding is a foundational task in computer vision, with recent applications emerging in soccer posture analysis. However, existing publicly available datasets lack comprehensive information, notably in the form of posture sequences and 2D pose annotations. Moreover, current analysis models often rely on interpretable linear models (e.g., PCA and regression), limiting their capacity to capture non-linear spatiotemporal relationships in complex and diverse scenarios. To address these gaps, we introduce the 3D Shot Posture (3DSP) dataset in soccer broadcast videos, which represents the most extensive sports image dataset with 2D pose annotations to our knowledge. Additionally, we present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear approach for embedding pose sequences. Furthermore, we propose AutoSoccerPose, a pipeline aimed at semi-automating 2D and 3D pose estimation and posture analysis. While achieving full automation proved challenging, we provide a foundational baseline, extending its utility beyond the scope of annotated data. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present posture analysis results based on 3DSP. The dataset, code, and models are available at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.","sentences":["Image understanding is a foundational task in computer vision, with recent applications emerging in soccer posture analysis.","However, existing publicly available datasets lack comprehensive information, notably in the form of posture sequences and","2D pose annotations.","Moreover, current analysis models often rely on interpretable linear models (e.g., PCA and regression), limiting their capacity to capture non-linear spatiotemporal relationships in complex and diverse scenarios.","To address these gaps, we introduce the 3D Shot Posture (3DSP) dataset in soccer broadcast videos, which represents the most extensive sports image dataset with 2D pose annotations to our knowledge.","Additionally, we present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear approach for embedding pose sequences.","Furthermore, we propose AutoSoccerPose, a pipeline aimed at semi-automating 2D and 3D pose estimation and posture analysis.","While achieving full automation proved challenging, we provide a foundational baseline, extending its utility beyond the scope of annotated data.","We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present posture analysis results based on 3DSP.","The dataset, code, and models are available at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset."],"url":"http://arxiv.org/abs/2405.12070v1","category":"cs.CV"}
{"created":"2024-05-20 14:16:06","title":"EXACT: Towards a platform for empirically benchmarking Machine Learning model explanation methods","abstract":"The evolving landscape of explainable artificial intelligence (XAI) aims to improve the interpretability of intricate machine learning (ML) models, yet faces challenges in formalisation and empirical validation, being an inherently unsupervised process. In this paper, we bring together various benchmark datasets and novel performance metrics in an initial benchmarking platform, the Explainable AI Comparison Toolkit (EXACT), providing a standardised foundation for evaluating XAI methods. Our datasets incorporate ground truth explanations for class-conditional features, and leveraging novel quantitative metrics, this platform assesses the performance of post-hoc XAI methods in the quality of the explanations they produce. Our recent findings have highlighted the limitations of popular XAI methods, as they often struggle to surpass random baselines, attributing significance to irrelevant features. Moreover, we show the variability in explanations derived from different equally performing model architectures. This initial benchmarking platform therefore aims to allow XAI researchers to test and assure the high quality of their newly developed methods.","sentences":["The evolving landscape of explainable artificial intelligence (XAI) aims to improve the interpretability of intricate machine learning (ML) models, yet faces challenges in formalisation and empirical validation, being an inherently unsupervised process.","In this paper, we bring together various benchmark datasets and novel performance metrics in an initial benchmarking platform, the Explainable AI Comparison Toolkit (EXACT), providing a standardised foundation for evaluating XAI methods.","Our datasets incorporate ground truth explanations for class-conditional features, and leveraging novel quantitative metrics, this platform assesses the performance of post-hoc XAI methods in the quality of the explanations they produce.","Our recent findings have highlighted the limitations of popular XAI methods, as they often struggle to surpass random baselines, attributing significance to irrelevant features.","Moreover, we show the variability in explanations derived from different equally performing model architectures.","This initial benchmarking platform therefore aims to allow XAI researchers to test and assure the high quality of their newly developed methods."],"url":"http://arxiv.org/abs/2405.12261v1","category":"cs.LG"}
{"created":"2024-05-20 14:10:53","title":"Tensor-network-based variational Monte Carlo approach to the non-equilibrium steady state of open quantum systems","abstract":"We introduce a novel method of efficiently simulating the non-equilibrium steady state of large many-body open quantum systems with highly non-local interactions, based on a variational Monte Carlo optimization of a matrix product operator ansatz. Our approach outperforms and offers several advantages over comparable algorithms, such as an improved scaling of the computational cost with respect to the bond dimension for periodic systems. We showcase the versatility of our approach by studying the phase diagrams and correlation functions of the dissipative quantum Ising model with collective dephasing and long-ranged power law interactions for spin chains of up to $N=100$ spins.","sentences":["We introduce a novel method of efficiently simulating the non-equilibrium steady state of large many-body open quantum systems with highly non-local interactions, based on a variational Monte Carlo optimization of a matrix product operator ansatz.","Our approach outperforms and offers several advantages over comparable algorithms, such as an improved scaling of the computational cost with respect to the bond dimension for periodic systems.","We showcase the versatility of our approach by studying the phase diagrams and correlation functions of the dissipative quantum Ising model with collective dephasing and long-ranged power law interactions for spin chains of up to $N=100$ spins."],"url":"http://arxiv.org/abs/2405.12044v1","category":"quant-ph"}
{"created":"2024-05-20 14:03:05","title":"KG-RAG: Bridging the Gap Between Knowledge and Creativity","abstract":"Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems. LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks. This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering). The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially. Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks.","sentences":["Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems.","LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks.","This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs.","The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering).","The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially.","Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks."],"url":"http://arxiv.org/abs/2405.12035v1","category":"cs.AI"}
{"created":"2024-05-20 13:14:26","title":"Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning","abstract":"Offline meta reinforcement learning (OMRL) has emerged as a promising approach for interaction avoidance and strong generalization performance by leveraging pre-collected data and meta-learning techniques. Previous context-based approaches predominantly rely on the intuition that maximizing the mutual information between the task and the task representation ($I(Z;M)$) can lead to performance improvements. Despite achieving attractive results, the theoretical justification of performance improvement for such intuition has been lacking. Motivated by the return discrepancy scheme in the model-based RL field, we find that maximizing $I(Z;M)$ can be interpreted as consistently raising the lower bound of the expected return for a given policy conditioning on the optimal task representation. However, this optimization process ignores the task representation shift between two consecutive updates, which may lead to performance improvement collapse. To address this problem, we turn to use the framework of performance difference bound to consider the impacts of task representation shift explicitly. We demonstrate that by reining the task representation shift, it is possible to achieve monotonic performance improvements, thereby showcasing the advantage against previous approaches. To make it practical, we design an easy yet highly effective algorithm RETRO (\\underline{RE}ining \\underline{T}ask \\underline{R}epresentation shift in context-based \\underline{O}ffline meta reinforcement learning) with only adding one line of code compared to the backbone. Empirical results validate its state-of-the-art (SOTA) asymptotic performance, training stability and training-time consumption on MuJoCo and MetaWorld benchmarks.","sentences":["Offline meta reinforcement learning (OMRL) has emerged as a promising approach for interaction avoidance and strong generalization performance by leveraging pre-collected data and meta-learning techniques.","Previous context-based approaches predominantly rely on the intuition that maximizing the mutual information between the task and the task representation ($I(Z;M)$) can lead to performance improvements.","Despite achieving attractive results, the theoretical justification of performance improvement for such intuition has been lacking.","Motivated by the return discrepancy scheme in the model-based RL field, we find that maximizing $I(Z;M)$ can be interpreted as consistently raising the lower bound of the expected return for a given policy conditioning on the optimal task representation.","However, this optimization process ignores the task representation shift between two consecutive updates, which may lead to performance improvement collapse.","To address this problem, we turn to use the framework of performance difference bound to consider the impacts of task representation shift explicitly.","We demonstrate that by reining the task representation shift, it is possible to achieve monotonic performance improvements, thereby showcasing the advantage against previous approaches.","To make it practical, we design an easy yet highly effective algorithm RETRO (\\underline{RE}ining \\underline{T}ask \\underline{R}epresentation shift in context-based \\underline{O}ffline meta reinforcement learning) with only adding one line of code compared to the backbone.","Empirical results validate its state-of-the-art (SOTA) asymptotic performance, training stability and training-time consumption on MuJoCo and MetaWorld benchmarks."],"url":"http://arxiv.org/abs/2405.12001v1","category":"cs.LG"}
{"created":"2024-05-20 13:09:52","title":"Learning to connect in action: Measuring and understanding the emergence of boundary spanners in volatile times","abstract":"Collective intelligence of diverse groups is key for tackling many of today's grand challenges such as fostering resilience and climate adaptation. Information exchange across such diverse groups is crucial for collective intelligence, especially in volatile environments. To facilitate inter-group information exchange, Informational Boundary Spanners (IBSs) as pivotal information exchange 'hubs' are promising. However, the mechanisms that drive the emergence of IBSs remain poorly understood. To address this gap there is first a need for a method to identify and measure the emergence of IBSs. Second, an Agent-Based Modelling (ABM) framework is not available to systematically study mechanisms for the emergence of IBSs in volatile environments. Third, even though the ability to learn who provides high-quality information is thought to be essential to explain the emergence of IBSs, a rigorous test of this mechanism is missing. The learning mechanism is formalized using an ABM framework, with the model's outputs analyzed using the proposed IBS emergence measurement method. To illustrate both the method and the learning mechanism, we present a case study focused on information sharing in the volatile environment of a disaster. The study shows that learning constitutes a mechanism for the emergence of effective IBSs in (a) low-volatility environments characterised by low uncertainty and (b) in high-volatility environments characterised by rapid change if the number of inter-group connections is sufficient. With the method and model, this paper aims to lay the foundations for exploring mechanisms for the emergence of IBSs that facilitate inter-group information exchange. This article advances collective intelligence by providing the essential elements for measuring and understanding the emergence of IBSs and exploring the effect of learning on their emergence in volatile environments.","sentences":["Collective intelligence of diverse groups is key for tackling many of today's grand challenges such as fostering resilience and climate adaptation.","Information exchange across such diverse groups is crucial for collective intelligence, especially in volatile environments.","To facilitate inter-group information exchange, Informational Boundary Spanners (IBSs) as pivotal information exchange 'hubs' are promising.","However, the mechanisms that drive the emergence of IBSs remain poorly understood.","To address this gap there is first a need for a method to identify and measure the emergence of IBSs.","Second, an Agent-Based Modelling (ABM) framework is not available to systematically study mechanisms for the emergence of IBSs in volatile environments.","Third, even though the ability to learn who provides high-quality information is thought to be essential to explain the emergence of IBSs, a rigorous test of this mechanism is missing.","The learning mechanism is formalized using an ABM framework, with the model's outputs analyzed using the proposed IBS emergence measurement method.","To illustrate both the method and the learning mechanism, we present a case study focused on information sharing in the volatile environment of a disaster.","The study shows that learning constitutes a mechanism for the emergence of effective IBSs in (a) low-volatility environments characterised by low uncertainty and (b) in high-volatility environments characterised by rapid change if the number of inter-group connections is sufficient.","With the method and model, this paper aims to lay the foundations for exploring mechanisms for the emergence of IBSs that facilitate inter-group information exchange.","This article advances collective intelligence by providing the essential elements for measuring and understanding the emergence of IBSs and exploring the effect of learning on their emergence in volatile environments."],"url":"http://arxiv.org/abs/2405.11998v1","category":"cs.MA"}
{"created":"2024-05-20 12:39:24","title":"Generalization Ability of Feature-based Performance Prediction Models: A Statistical Analysis across Benchmarks","abstract":"This study examines the generalization ability of algorithm performance prediction models across various benchmark suites. Comparing the statistical similarity between the problem collections with the accuracy of performance prediction models that are based on exploratory landscape analysis features, we observe that there is a positive correlation between these two measures. Specifically, when the high-dimensional feature value distributions between training and testing suites lack statistical significance, the model tends to generalize well, in the sense that the testing errors are in the same range as the training errors. Two experiments validate these findings: one involving the standard benchmark suites, the BBOB and CEC collections, and another using five collections of affine combinations of BBOB problem instances.","sentences":["This study examines the generalization ability of algorithm performance prediction models across various benchmark suites.","Comparing the statistical similarity between the problem collections with the accuracy of performance prediction models that are based on exploratory landscape analysis features, we observe that there is a positive correlation between these two measures.","Specifically, when the high-dimensional feature value distributions between training and testing suites lack statistical significance, the model tends to generalize well, in the sense that the testing errors are in the same range as the training errors.","Two experiments validate these findings: one involving the standard benchmark suites, the BBOB and CEC collections, and another using five collections of affine combinations of BBOB problem instances."],"url":"http://arxiv.org/abs/2405.12259v1","category":"cs.LG"}
{"created":"2024-05-20 12:33:42","title":"A review on the use of large language models as virtual tutors","abstract":"Transformer architectures contribute to managing long-term dependencies for Natural Language Processing, representing one of the most recent changes in the field. These architectures are the basis of the innovative, cutting-edge Large Language Models (LLMs) that have produced a huge buzz in several fields and industrial sectors, among the ones education stands out. Accordingly, these generative Artificial Intelligence-based solutions have directed the change in techniques and the evolution in educational methods and contents, along with network infrastructure, towards high-quality learning. Given the popularity of LLMs, this review seeks to provide a comprehensive overview of those solutions designed specifically to generate and evaluate educational materials and which involve students and teachers in their design or experimental plan. To the best of our knowledge, this is the first review of educational applications (e.g., student assessment) of LLMs. As expected, the most common role of these systems is as virtual tutors for automatic question generation. Moreover, the most popular models are GTP-3 and BERT. However, due to the continuous launch of new generative models, new works are expected to be published shortly.","sentences":["Transformer architectures contribute to managing long-term dependencies for Natural Language Processing, representing one of the most recent changes in the field.","These architectures are the basis of the innovative, cutting-edge Large Language Models (LLMs) that have produced a huge buzz in several fields and industrial sectors, among the ones education stands out.","Accordingly, these generative Artificial Intelligence-based solutions have directed the change in techniques and the evolution in educational methods and contents, along with network infrastructure, towards high-quality learning.","Given the popularity of LLMs, this review seeks to provide a comprehensive overview of those solutions designed specifically to generate and evaluate educational materials and which involve students and teachers in their design or experimental plan.","To the best of our knowledge, this is the first review of educational applications (e.g., student assessment) of LLMs.","As expected, the most common role of these systems is as virtual tutors for automatic question generation.","Moreover, the most popular models are GTP-3 and BERT.","However, due to the continuous launch of new generative models, new works are expected to be published shortly."],"url":"http://arxiv.org/abs/2405.11983v1","category":"cs.CL"}
{"created":"2024-05-20 12:31:11","title":"Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space","abstract":"Deep reinforcement learning (DRL) algorithms can suffer from modeling errors between the simulation and the real world. Many studies use adversarial learning to generate perturbation during training process to model the discrepancy and improve the robustness of DRL. However, most of these approaches use a fixed parameter to control the intensity of the adversarial perturbation, which can lead to a trade-off between average performance and robustness. In fact, finding the optimal parameter of the perturbation is challenging, as excessive perturbations may destabilize training and compromise agent performance, while insufficient perturbations may not impart enough information to enhance robustness. To keep the training stable while improving robustness, we propose a simple but effective method, namely, Adaptive Adversarial Perturbation (A2P), which can dynamically select appropriate adversarial perturbations for each sample. Specifically, we propose an adaptive adversarial coefficient framework to adjust the effect of the adversarial perturbation during training. By designing a metric for the current intensity of the perturbation, our method can calculate the suitable perturbation levels based on the current relative performance. The appealing feature of our method is that it is simple to deploy in real-world applications and does not require accessing the simulator in advance. The experiments in MuJoCo show that our method can improve the training stability and learn a robust policy when migrated to different test environments. The code is available at https://github.com/Lqm00/A2P-SAC.","sentences":["Deep reinforcement learning (DRL) algorithms can suffer from modeling errors between the simulation and the real world.","Many studies use adversarial learning to generate perturbation during training process to model the discrepancy and improve the robustness of DRL.","However, most of these approaches use a fixed parameter to control the intensity of the adversarial perturbation, which can lead to a trade-off between average performance and robustness.","In fact, finding the optimal parameter of the perturbation is challenging, as excessive perturbations may destabilize training and compromise agent performance, while insufficient perturbations may not impart enough information to enhance robustness.","To keep the training stable while improving robustness, we propose a simple but effective method, namely, Adaptive Adversarial Perturbation (A2P), which can dynamically select appropriate adversarial perturbations for each sample.","Specifically, we propose an adaptive adversarial coefficient framework to adjust the effect of the adversarial perturbation during training.","By designing a metric for the current intensity of the perturbation, our method can calculate the suitable perturbation levels based on the current relative performance.","The appealing feature of our method is that it is simple to deploy in real-world applications and does not require accessing the simulator in advance.","The experiments in MuJoCo show that our method can improve the training stability and learn a robust policy when migrated to different test environments.","The code is available at https://github.com/Lqm00/A2P-SAC."],"url":"http://arxiv.org/abs/2405.11982v1","category":"cs.LG"}
{"created":"2024-05-20 12:18:15","title":"SM-DTW: Stability Modulated Dynamic Time Warping for signature verification","abstract":"Building upon findings in computational model of handwriting learning and execution, we introduce the concept of stability to explain the difference between the actual movements performed during multiple execution of the subject's signature, and conjecture that the most stable parts of the signature should play a paramount role in evaluating the similarity between a questioned signature and the reference ones during signature verification. We then introduce the Stability Modulated Dynamic Time Warping algorithm for incorporating the stability regions, i.e. the most similar parts between two signatures, into the distance measure between a pair of signatures computed by the Dynamic Time Warping for signature verification. Experiments were conducted on two datasets largely adopted for performance evaluation. Experimental results show that the proposed algorithm improves the performance of the baseline system and compares favourably with other top performing signature verification systems.","sentences":["Building upon findings in computational model of handwriting learning and execution, we introduce the concept of stability to explain the difference between the actual movements performed during multiple execution of the subject's signature, and conjecture that the most stable parts of the signature should play a paramount role in evaluating the similarity between a questioned signature and the reference ones during signature verification.","We then introduce the Stability Modulated Dynamic Time Warping algorithm for incorporating the stability regions, i.e. the most similar parts between two signatures, into the distance measure between a pair of signatures computed by the Dynamic Time Warping for signature verification.","Experiments were conducted on two datasets largely adopted for performance evaluation.","Experimental results show that the proposed algorithm improves the performance of the baseline system and compares favourably with other top performing signature verification systems."],"url":"http://arxiv.org/abs/2405.11978v1","category":"cs.CV"}
{"created":"2024-05-20 12:10:26","title":"A Stochastic Sampling Approach to Privacy","abstract":"This paper proposes an optimal stochastic sampling approach to privacy, in which a sensor observes a process which is correlated to private information. In out set-up, a sampler decides to keep or discard the sensor's observations. The kept samples are shared with an adversary who might attempt to infer the private process based on the sampler's output. The privacy leakages are captured with the mutual information between the private process and sampler's output. We cast the optimal sampling design as an optimization problem with two objectives: (i) minimizing the reconstruction error of the observed process using the sampler's output, (ii) reducing the privacy leakages. We first show the optimal reconstruction policy is deterministic and can be obtained by solving a one-step optimization problem at each time step. We also derive the optimality equations of the privacy-sampler for a general class of processes via the dynamic decomposition method, and show the sampler controls the adversary's belief about the private input. Also, we propose a simplified design for linear Gaussian processes by restricting the sampling policy to a special collection. We show that the optimal reconstruction of the system state and the private process is similar to Kalman filter in the linear Gaussian case, and the objective of the sampler design problem can be analytically expressed based on a conditional mean and covariance matrix. Furthermore, we develop an numerical algorithm to optimize the sampling and reconstruction policies, wherein the policy gradient theorem for the optimal sampling design is derived based on the implicit function theorem. Finally, we verify our design and show it capabilities in state reconstruction, privacy protection and data size reduction via simulations.","sentences":["This paper proposes an optimal stochastic sampling approach to privacy, in which a sensor observes a process which is correlated to private information.","In out set-up, a sampler decides to keep or discard the sensor's observations.","The kept samples are shared with an adversary who might attempt to infer the private process based on the sampler's output.","The privacy leakages are captured with the mutual information between the private process and sampler's output.","We cast the optimal sampling design as an optimization problem with two objectives: (i) minimizing the reconstruction error of the observed process using the sampler's output, (ii) reducing the privacy leakages.","We first show the optimal reconstruction policy is deterministic and can be obtained by solving a one-step optimization problem at each time step.","We also derive the optimality equations of the privacy-sampler for a general class of processes via the dynamic decomposition method, and show the sampler controls the adversary's belief about the private input.","Also, we propose a simplified design for linear Gaussian processes by restricting the sampling policy to a special collection.","We show that the optimal reconstruction of the system state and the private process is similar to Kalman filter in the linear Gaussian case, and the objective of the sampler design problem can be analytically expressed based on a conditional mean and covariance matrix.","Furthermore, we develop an numerical algorithm to optimize the sampling and reconstruction policies, wherein the policy gradient theorem for the optimal sampling design is derived based on the implicit function theorem.","Finally, we verify our design and show it capabilities in state reconstruction, privacy protection and data size reduction via simulations."],"url":"http://arxiv.org/abs/2405.11975v1","category":"eess.SY"}
{"created":"2024-05-20 11:47:31","title":"Conditional Shift-Robust Conformal Prediction for Graph Neural Network","abstract":"Graph Neural Networks (GNNs) have emerged as potent tools for predicting outcomes in graph-structured data. Despite their efficacy, a significant drawback of GNNs lies in their limited ability to provide robust uncertainty estimates, posing challenges to their reliability in contexts where errors carry significant consequences. Moreover, GNNs typically excel in in-distribution settings, assuming that training and test data follow identical distributions: a condition often unmet in real-world graph data scenarios. In this article, we leverage conformal prediction, a widely recognized statistical technique for quantifying uncertainty by transforming predictive model outputs into prediction sets, to address uncertainty quantification in GNN predictions amidst conditional shift \\footnote{Representing the change in conditional probability distribution $P(label |input)$ from source domain to target domain.} in graph-based semi-supervised learning (SSL). Additionally, we propose a novel loss function aimed at refining model predictions by minimizing conditional shift in latent stages. Termed Conditional Shift Robust (CondSR) conformal prediction for GNNs, our approach CondSR is model-agnostic and adaptable to various classification models. We validate the effectiveness of our method on standard graph benchmark datasets, integrating it with state-of-the-art GNNs in node classification tasks. The code implementation is publicly available for further exploration and experimentation.","sentences":["Graph Neural Networks (GNNs) have emerged as potent tools for predicting outcomes in graph-structured data.","Despite their efficacy, a significant drawback of GNNs lies in their limited ability to provide robust uncertainty estimates, posing challenges to their reliability in contexts where errors carry significant consequences.","Moreover, GNNs typically excel in in-distribution settings, assuming that training and test data follow identical distributions: a condition often unmet in real-world graph data scenarios.","In this article, we leverage conformal prediction, a widely recognized statistical technique for quantifying uncertainty by transforming predictive model outputs into prediction sets, to address uncertainty quantification in GNN predictions amidst conditional shift \\footnote{Representing the change in conditional probability distribution $P(label |input)$ from source domain to target domain.}","in graph-based semi-supervised learning (SSL).","Additionally, we propose a novel loss function aimed at refining model predictions by minimizing conditional shift in latent stages.","Termed Conditional Shift Robust (CondSR) conformal prediction for GNNs, our approach CondSR is model-agnostic and adaptable to various classification models.","We validate the effectiveness of our method on standard graph benchmark datasets, integrating it with state-of-the-art GNNs in node classification tasks.","The code implementation is publicly available for further exploration and experimentation."],"url":"http://arxiv.org/abs/2405.11968v1","category":"cs.LG"}
{"created":"2024-05-20 11:47:19","title":"Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home","abstract":"One of the new trends in the development of recommendation algorithms is the dissemination of their capabilities to support the population in managing their health. This article focuses on the problem of improving the effectiveness of cardiovascular diseases (CVD) prevention, since CVD is the leading cause of death worldwide. To address this issue, a knowledge-based recommendation algorithm was proposed to support self-management of CVD risk factors in adults at home. The proposed algorithm is based on the original multidimensional recommendation model and on a new user profile model, which includes predictive assessments of CVD health in addition to its current ones as outlined in official guidelines. The main feature of the proposed algorithm is the combination of rule-based logic with the capabilities of a large language model in generating human-like text for explanatory component of multidimensional recommendation. The verification and evaluation of the proposed algorithm showed the usefulness of the proposed recommendation algorithm for supporting adults in self-management of their CVD risk factors at home. As follows from the comparison with similar knowledge-based recommendation algorithms, the proposed algorithm evaluates a larger number of CVD risk factors and has a greater information and semantic capacity of the generated recommendations.","sentences":["One of the new trends in the development of recommendation algorithms is the dissemination of their capabilities to support the population in managing their health.","This article focuses on the problem of improving the effectiveness of cardiovascular diseases (CVD) prevention, since CVD is the leading cause of death worldwide.","To address this issue, a knowledge-based recommendation algorithm was proposed to support self-management of CVD risk factors in adults at home.","The proposed algorithm is based on the original multidimensional recommendation model and on a new user profile model, which includes predictive assessments of CVD health in addition to its current ones as outlined in official guidelines.","The main feature of the proposed algorithm is the combination of rule-based logic with the capabilities of a large language model in generating human-like text for explanatory component of multidimensional recommendation.","The verification and evaluation of the proposed algorithm showed the usefulness of the proposed recommendation algorithm for supporting adults in self-management of their CVD risk factors at home.","As follows from the comparison with similar knowledge-based recommendation algorithms, the proposed algorithm evaluates a larger number of CVD risk factors and has a greater information and semantic capacity of the generated recommendations."],"url":"http://arxiv.org/abs/2405.11967v1","category":"cs.IR"}
{"created":"2024-05-20 11:47:13","title":"Multiple-Choice Questions are Efficient and Robust LLM Evaluators","abstract":"We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructed by collecting answers and incorrect predictions on GSM8K and MATH from over 50 open-source models. Through extensive experiments, we show that LLMs' performance on the MC versions of these two popular benchmarks is strongly correlated with their performance on the original versions, and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30. Following a similar procedure, we also introduce PythonIO, a new program output prediction MC dataset constructed from two other popular LLM evaluation benchmarks HumanEval and MBPP. Our data and code are available at https://github.com/Geralt-Targaryen/MC-Evaluation.","sentences":["We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructed by collecting answers and incorrect predictions on GSM8K and MATH from over 50 open-source models.","Through extensive experiments, we show that LLMs' performance on the MC versions of these two popular benchmarks is strongly correlated with their performance on the original versions, and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30.","Following a similar procedure, we also introduce PythonIO, a new program output prediction MC dataset constructed from two other popular LLM evaluation benchmarks HumanEval and MBPP.","Our data and code are available at https://github.com/Geralt-Targaryen/MC-Evaluation."],"url":"http://arxiv.org/abs/2405.11966v2","category":"cs.CL"}
{"created":"2024-05-20 11:40:23","title":"Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment","abstract":"Large language models (LLMs) have transformed AI and achieved breakthrough performance on a wide range of tasks that require human intelligence. In science, perhaps the most interesting application of LLMs is for hypothesis formation. A feature of LLMs, which results from their probabilistic structure, is that the output text is not necessarily a valid inference from the training text. These are 'hallucinations', and are a serious problem in many applications. However, in science, hallucinations may be useful: they are novel hypotheses whose validity may be tested by laboratory experiments. Here we experimentally test the use of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment. We applied the LLM GPT4 to hypothesize novel pairs of FDA-approved non-cancer drugs that target the MCF7 breast cancer cell line relative to the non-tumorigenic breast cell line MCF10A. In the first round of laboratory experiments GPT4 succeeded in discovering three drug combinations (out of 12 tested) with synergy scores above the positive controls. These combinations were itraconazole + atenolol, disulfiram + simvastatin and dipyridamole + mebendazole. GPT4 was then asked to generate new combinations after considering its initial results. It then discovered three more combinations with positive synergy scores (out of four tested), these were disulfiram + fulvestrant, mebendazole + quinacrine and disulfiram + quinacrine. A limitation of GPT4 as a generator of hypotheses was that its explanations for them were formulaic and unconvincing. We conclude that LLMs are an exciting novel source of scientific hypotheses.","sentences":["Large language models (LLMs) have transformed AI and achieved breakthrough performance on a wide range of tasks that require human intelligence.","In science, perhaps the most interesting application of LLMs is for hypothesis formation.","A feature of LLMs, which results from their probabilistic structure, is that the output text is not necessarily a valid inference from the training text.","These are 'hallucinations', and are a serious problem in many applications.","However, in science, hallucinations may be useful: they are novel hypotheses whose validity may be tested by laboratory experiments.","Here we experimentally test the use of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment.","We applied the LLM GPT4 to hypothesize novel pairs of FDA-approved non-cancer drugs that target the MCF7 breast cancer cell line relative to the non-tumorigenic breast cell line MCF10A.","In the first round of laboratory experiments GPT4 succeeded in discovering three drug combinations (out of 12 tested) with synergy scores above the positive controls.","These combinations were itraconazole + atenolol, disulfiram + simvastatin and dipyridamole + mebendazole.","GPT4 was then asked to generate new combinations after considering its initial results.","It then discovered three more combinations with positive synergy scores (out of four tested), these were disulfiram + fulvestrant, mebendazole + quinacrine and disulfiram + quinacrine.","A limitation of GPT4 as a generator of hypotheses was that its explanations for them were formulaic and unconvincing.","We conclude that LLMs are an exciting novel source of scientific hypotheses."],"url":"http://arxiv.org/abs/2405.12258v1","category":"q-bio.QM"}
{"created":"2024-05-20 11:39:55","title":"Quantifying Individual and Joint Module Impact in Modular Optimization Frameworks","abstract":"This study explores the influence of modules on the performance of modular optimization frameworks for continuous single-objective black-box optimization. There is an extensive variety of modules to choose from when designing algorithm variants, however, there is a rather limited understanding of how each module individually influences the algorithm performance and how the modules interact with each other when combined. We use the functional ANOVA (f-ANOVA) framework to quantify the influence of individual modules and module combinations for two algorithms, the modular Covariance Matrix Adaptation (modCMA) and the modular Differential Evolution (modDE). We analyze the performance data from 324 modCMA and 576 modDE variants on the BBOB benchmark collection, for two problem dimensions, and three computational budgets. Noteworthy findings include the identification of important modules that strongly influence the performance of modCMA, such as the~\\textit{weights\\ option} and~\\textit{mirrored} modules for low dimensional problems, and the~\\textit{base\\ sampler} for high dimensional problems. The large individual influence of the~\\textit{lpsr} module makes it very important for the performance of modDE, regardless of the problem dimensionality and the computational budget. When comparing modCMA and modDE, modDE undergoes a shift from individual modules being more influential, to module combinations being more influential, while modCMA follows the opposite pattern, with an increase in problem dimensionality and computational budget.","sentences":["This study explores the influence of modules on the performance of modular optimization frameworks for continuous single-objective black-box optimization.","There is an extensive variety of modules to choose from when designing algorithm variants, however, there is a rather limited understanding of how each module individually influences the algorithm performance and how the modules interact with each other when combined.","We use the functional ANOVA (f-ANOVA) framework to quantify the influence of individual modules and module combinations for two algorithms, the modular Covariance Matrix Adaptation (modCMA) and the modular Differential Evolution (modDE).","We analyze the performance data from 324 modCMA and 576 modDE variants on the BBOB benchmark collection, for two problem dimensions, and three computational budgets.","Noteworthy findings include the identification of important modules that strongly influence the performance of modCMA, such as the~\\textit{weights\\ option} and~\\textit{mirrored} modules for low dimensional problems, and the~\\textit{base\\ sampler} for high dimensional problems.","The large individual influence of the~\\textit{lpsr} module makes it very important for the performance of modDE, regardless of the problem dimensionality and the computational budget.","When comparing modCMA and modDE, modDE undergoes a shift from individual modules being more influential, to module combinations being more influential, while modCMA follows the opposite pattern, with an increase in problem dimensionality and computational budget."],"url":"http://arxiv.org/abs/2405.11964v1","category":"cs.NE"}
{"created":"2024-05-20 10:30:36","title":"Biomedical Entity Linking for Dutch: Fine-tuning a Self-alignment BERT Model on an Automatically Generated Wikipedia Corpus","abstract":"Biomedical entity linking, a main component in automatic information extraction from health-related texts, plays a pivotal role in connecting textual entities (such as diseases, drugs and body parts mentioned by patients) to their corresponding concepts in a structured biomedical knowledge base. The task remains challenging despite recent developments in natural language processing. This paper presents the first evaluated biomedical entity linking model for the Dutch language. We use MedRoBERTa.nl as base model and perform second-phase pretraining through self-alignment on a Dutch biomedical ontology extracted from the UMLS and Dutch SNOMED. We derive a corpus from Wikipedia of ontology-linked Dutch biomedical entities in context and fine-tune our model on this dataset. We evaluate our model on the Dutch portion of the Mantra GSC-corpus and achieve 54.7% classification accuracy and 69.8% 1-distance accuracy. We then perform a case study on a collection of unlabeled, patient-support forum data and show that our model is hampered by the limited quality of the preceding entity recognition step. Manual evaluation of small sample indicates that of the correctly extracted entities, around 65% is linked to the correct concept in the ontology. Our results indicate that biomedical entity linking in a language other than English remains challenging, but our Dutch model can be used to for high-level analysis of patient-generated text.","sentences":["Biomedical entity linking, a main component in automatic information extraction from health-related texts, plays a pivotal role in connecting textual entities (such as diseases, drugs and body parts mentioned by patients) to their corresponding concepts in a structured biomedical knowledge base.","The task remains challenging despite recent developments in natural language processing.","This paper presents the first evaluated biomedical entity linking model for the Dutch language.","We use MedRoBERTa.nl as base model and perform second-phase pretraining through self-alignment on a Dutch biomedical ontology extracted from the UMLS and Dutch SNOMED.","We derive a corpus from Wikipedia of ontology-linked Dutch biomedical entities in context and fine-tune our model on this dataset.","We evaluate our model on the Dutch portion of the Mantra GSC-corpus and achieve 54.7% classification accuracy and 69.8% 1-distance accuracy.","We then perform a case study on a collection of unlabeled, patient-support forum data and show that our model is hampered by the limited quality of the preceding entity recognition step.","Manual evaluation of small sample indicates that of the correctly extracted entities, around 65% is linked to the correct concept in the ontology.","Our results indicate that biomedical entity linking in a language other than English remains challenging, but our Dutch model can be used to for high-level analysis of patient-generated text."],"url":"http://arxiv.org/abs/2405.11941v1","category":"cs.CL"}
{"created":"2024-05-20 10:25:03","title":"Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation","abstract":"This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in machine translation (MT), particularly for domain adaptation and low-resource languages. We implement the self-improvement process by fine-tuning the model on its MBR-decoded forward translations. By employing COMET as the MBR utility metric, we aim to achieve the reranking of translations that better aligns with human preferences. The paper explores the iterative application of this approach and the potential need for language-specific MBR utility metrics. The results demonstrate significant enhancements in translation quality for all examined language pairs, including successful application to domain-adapted models and generalisation to low-resource settings. This highlights the potential of COMET-guided MBR for efficient MT self-improvement in various scenarios.","sentences":["This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in machine translation (MT), particularly for domain adaptation and low-resource languages.","We implement the self-improvement process by fine-tuning the model on its MBR-decoded forward translations.","By employing COMET as the MBR utility metric, we aim to achieve the reranking of translations that better aligns with human preferences.","The paper explores the iterative application of this approach and the potential need for language-specific MBR utility metrics.","The results demonstrate significant enhancements in translation quality for all examined language pairs, including successful application to domain-adapted models and generalisation to low-resource settings.","This highlights the potential of COMET-guided MBR for efficient MT self-improvement in various scenarios."],"url":"http://arxiv.org/abs/2405.11937v1","category":"cs.CL"}
{"created":"2024-05-20 10:24:10","title":"UAV-VisLoc: A Large-scale Dataset for UAV Visual Localization","abstract":"The application of unmanned aerial vehicles (UAV) has been widely extended recently. It is crucial to ensure accurate latitude and longitude coordinates for UAVs, especially when the global navigation satellite systems (GNSS) are disrupted and unreliable. Existing visual localization methods achieve autonomous visual localization without error accumulation by matching the ground-down view image of UAV with the ortho satellite maps. However, collecting UAV ground-down view images across diverse locations is costly, leading to a scarcity of large-scale datasets for real-world scenarios. Existing datasets for UAV visual localization are often limited to small geographic areas or are focused only on urban regions with distinct textures. To address this, we define the UAV visual localization task by determining the UAV's real position coordinates on a large-scale satellite map based on the captured ground-down view. In this paper, we present a large-scale dataset, UAV-VisLoc, to facilitate the UAV visual localization task. This dataset comprises images from diverse drones across 11 locations in China, capturing a range of topographical features. The dataset features images from fixed-wing drones and multi-terrain drones, captured at different altitudes and orientations. Our dataset includes 6,742 drone images and 11 satellite maps, with metadata such as latitude, longitude, altitude, and capture date. Our dataset is tailored to support both the training and testing of models by providing a diverse and extensive data.","sentences":["The application of unmanned aerial vehicles (UAV) has been widely extended recently.","It is crucial to ensure accurate latitude and longitude coordinates for UAVs, especially when the global navigation satellite systems (GNSS) are disrupted and unreliable.","Existing visual localization methods achieve autonomous visual localization without error accumulation by matching the ground-down view image of UAV with the ortho satellite maps.","However, collecting UAV ground-down view images across diverse locations is costly, leading to a scarcity of large-scale datasets for real-world scenarios.","Existing datasets for UAV visual localization are often limited to small geographic areas or are focused only on urban regions with distinct textures.","To address this, we define the UAV visual localization task by determining the UAV's real position coordinates on a large-scale satellite map based on the captured ground-down view.","In this paper, we present a large-scale dataset, UAV-VisLoc, to facilitate the UAV visual localization task.","This dataset comprises images from diverse drones across 11 locations in China, capturing a range of topographical features.","The dataset features images from fixed-wing drones and multi-terrain drones, captured at different altitudes and orientations.","Our dataset includes 6,742 drone images and 11 satellite maps, with metadata such as latitude, longitude, altitude, and capture date.","Our dataset is tailored to support both the training and testing of models by providing a diverse and extensive data."],"url":"http://arxiv.org/abs/2405.11936v1","category":"cs.CV"}
{"created":"2024-05-20 10:06:33","title":"\"Set It Up!\": Functional Object Arrangement with Compositional Generative Models","abstract":"This paper studies the challenge of developing robots capable of understanding under-specified instructions for creating functional object arrangements, such as \"set up a dining table for two\"; previous arrangement approaches have focused on much more explicit instructions, such as \"put object A on the table.\" We introduce a framework, SetItUp, for learning to interpret under-specified instructions. SetItUp takes a small number of training examples and a human-crafted program sketch to uncover arrangement rules for specific scene types. By leveraging an intermediate graph-like representation of abstract spatial relationships among objects, SetItUp decomposes the arrangement problem into two subproblems: i) learning the arrangement patterns from limited data and ii) grounding these abstract relationships into object poses. SetItUp leverages large language models (LLMs) to propose the abstract spatial relationships among objects in novel scenes as the constraints to be satisfied; then, it composes a library of diffusion models associated with these abstract relationships to find object poses that satisfy the constraints. We validate our framework on a dataset comprising study desks, dining tables, and coffee tables, with the results showing superior performance in generating physically plausible, functional, and aesthetically pleasing object arrangements compared to existing models.","sentences":["This paper studies the challenge of developing robots capable of understanding under-specified instructions for creating functional object arrangements, such as \"set up a dining table for two\"; previous arrangement approaches have focused on much more explicit instructions, such as \"put object A on the table.\"","We introduce a framework, SetItUp, for learning to interpret under-specified instructions.","SetItUp takes a small number of training examples and a human-crafted program sketch to uncover arrangement rules for specific scene types.","By leveraging an intermediate graph-like representation of abstract spatial relationships among objects, SetItUp decomposes the arrangement problem into two subproblems: i) learning the arrangement patterns from limited data and ii) grounding these abstract relationships into object poses.","SetItUp leverages large language models (LLMs) to propose the abstract spatial relationships among objects in novel scenes as the constraints to be satisfied; then, it composes a library of diffusion models associated with these abstract relationships to find object poses that satisfy the constraints.","We validate our framework on a dataset comprising study desks, dining tables, and coffee tables, with the results showing superior performance in generating physically plausible, functional, and aesthetically pleasing object arrangements compared to existing models."],"url":"http://arxiv.org/abs/2405.11928v1","category":"cs.RO"}
{"created":"2024-05-20 09:59:03","title":"Rate Optimality and Phase Transition for User-Level Local Differential Privacy","abstract":"Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection.   In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations. We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation. In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies.   In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting. Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \\gtrsim s \\log (d)$, up to logarithmic factors. This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints.","sentences":["Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection.   ","In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations.","We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation.","In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies.   ","In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting.","Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \\gtrsim s \\log (d)$, up to logarithmic factors.","This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints."],"url":"http://arxiv.org/abs/2405.11923v1","category":"math.ST"}
{"created":"2024-05-20 09:57:29","title":"On Efficient and Statistical Quality Estimation for Data Annotation","abstract":"Annotated datasets are an essential ingredient to train, evaluate, compare and productionalize supervised machine learning models. It is therefore imperative that annotations are of high quality. For their creation, good quality management and thereby reliable quality estimates are needed. Then, if quality is insufficient during the annotation process, rectifying measures can be taken to improve it. Quality estimation is often performed by having experts manually label instances as correct or incorrect. But checking all annotated instances tends to be expensive. Therefore, in practice, usually only subsets are inspected; sizes are chosen mostly without justification or regard to statistical power and more often than not, are relatively small. Basing estimates on small sample sizes, however, can lead to imprecise values for the error rate. Using unnecessarily large sample sizes costs money that could be better spent, for instance on more annotations. Therefore, we first describe in detail how to use confidence intervals for finding the minimal sample size needed to estimate the annotation error rate. Then, we propose applying acceptance sampling as an alternative to error rate estimation We show that acceptance sampling can reduce the required sample sizes up to 50% while providing the same statistical guarantees.","sentences":["Annotated datasets are an essential ingredient to train, evaluate, compare and productionalize supervised machine learning models.","It is therefore imperative that annotations are of high quality.","For their creation, good quality management and thereby reliable quality estimates are needed.","Then, if quality is insufficient during the annotation process, rectifying measures can be taken to improve it.","Quality estimation is often performed by having experts manually label instances as correct or incorrect.","But checking all annotated instances tends to be expensive.","Therefore, in practice, usually only subsets are inspected; sizes are chosen mostly without justification or regard to statistical power and more often than not, are relatively small.","Basing estimates on small sample sizes, however, can lead to imprecise values for the error rate.","Using unnecessarily large sample sizes costs money that could be better spent, for instance on more annotations.","Therefore, we first describe in detail how to use confidence intervals for finding the minimal sample size needed to estimate the annotation error rate.","Then, we propose applying acceptance sampling as an alternative to error rate estimation We show that acceptance sampling can reduce the required sample sizes up to 50% while providing the same statistical guarantees."],"url":"http://arxiv.org/abs/2405.11919v1","category":"cs.LG"}
{"created":"2024-05-20 09:47:22","title":"PULL: PU-Learning-based Accurate Link Prediction","abstract":"Given an edge-incomplete graph, how can we accurately find the missing links? The link prediction in edge-incomplete graphs aims to discover the missing relations between entities when their relationships are represented as a graph. Edge-incomplete graphs are prevalent in real-world due to practical limitations, such as not checking all users when adding friends in a social network. Addressing the problem is crucial for various tasks, including recommending friends in social networks and finding references in citation networks. However, previous approaches rely heavily on the given edge-incomplete (observed) graph, making it challenging to consider the missing (unobserved) links during training. In this paper, we propose PULL (PU-Learning-based Link predictor), an accurate link prediction method based on the positive-unlabeled (PU) learning. PULL treats the observed edges in the training graph as positive examples, and the unconnected node pairs as unlabeled ones. PULL effectively prevents the link predictor from overfitting to the observed graph by proposing latent variables for every edge, and leveraging the expected graph structure with respect to the variables. Extensive experiments on five real-world datasets show that PULL consistently outperforms the baselines for predicting links in edge-incomplete graphs.","sentences":["Given an edge-incomplete graph, how can we accurately find the missing links?","The link prediction in edge-incomplete graphs aims to discover the missing relations between entities when their relationships are represented as a graph.","Edge-incomplete graphs are prevalent in real-world due to practical limitations, such as not checking all users when adding friends in a social network.","Addressing the problem is crucial for various tasks, including recommending friends in social networks and finding references in citation networks.","However, previous approaches rely heavily on the given edge-incomplete (observed) graph, making it challenging to consider the missing (unobserved) links during training.","In this paper, we propose PULL (PU-Learning-based Link predictor), an accurate link prediction method based on the positive-unlabeled (PU) learning.","PULL treats the observed edges in the training graph as positive examples, and the unconnected node pairs as unlabeled ones.","PULL effectively prevents the link predictor from overfitting to the observed graph by proposing latent variables for every edge, and leveraging the expected graph structure with respect to the variables.","Extensive experiments on five real-world datasets show that PULL consistently outperforms the baselines for predicting links in edge-incomplete graphs."],"url":"http://arxiv.org/abs/2405.11911v1","category":"cs.AI"}
{"created":"2024-05-20 09:44:24","title":"3D Reconfigurable Intelligent Surfaces for Satellite-Terrestrial Networks","abstract":"This paper proposes a three-dimensional (3D) satellite-terrestrial communication network assisted with reconfigurable intelligent surfaces (RISs). Using stochastic geometry models, we present an original framework to derive tractable yet accurate closed-form expressions for coverage probability and ergodic capacity in the presence of fading. A homogeneous Poisson point process models the satellites on a sphere, while RISs are randomly deployed in a 3D cylindrical region. We consider nonidentical channels that correspond to different RISs and follow the {\\kappa}-{\\mu} fading distribution. We verify the high accuracy of the adopted approach through Monte Carlo simulations and demonstrate the significant improvement in system performance due to using RISs. Furthermore, we comprehensively study the effect of the different system parameters on its performance using the derived analytical expressions, which enable system engineers to predict and optimize the expected downlink coverage and capacity performance analytically.","sentences":["This paper proposes a three-dimensional (3D) satellite-terrestrial communication network assisted with reconfigurable intelligent surfaces (RISs).","Using stochastic geometry models, we present an original framework to derive tractable yet accurate closed-form expressions for coverage probability and ergodic capacity in the presence of fading.","A homogeneous Poisson point process models the satellites on a sphere, while RISs are randomly deployed in a 3D cylindrical region.","We consider nonidentical channels that correspond to different RISs and follow the {\\kappa}-{\\mu} fading distribution.","We verify the high accuracy of the adopted approach through Monte Carlo simulations and demonstrate the significant improvement in system performance due to using RISs.","Furthermore, we comprehensively study the effect of the different system parameters on its performance using the derived analytical expressions, which enable system engineers to predict and optimize the expected downlink coverage and capacity performance analytically."],"url":"http://arxiv.org/abs/2405.11909v2","category":"eess.SP"}
{"created":"2024-05-20 09:15:36","title":"Unveiling and Manipulating Prompt Influence in Large Language Models","abstract":"Prompts play a crucial role in guiding the responses of Large Language Models (LLMs). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies. To address this, we propose Token Distribution Dynamics (TDD), a \\textcolor{black}{simple yet effective} approach to unveil and manipulate the role of prompts in generating LLM outputs. TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency. It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary. We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance. Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs. Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering. Empirical results underscore TDD's proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content.","sentences":["Prompts play a crucial role in guiding the responses of Large Language Models (LLMs).","However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored.","Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies.","To address this, we propose Token Distribution Dynamics (TDD), a \\textcolor{black}{simple yet effective} approach to unveil and manipulate the role of prompts in generating LLM outputs.","TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency.","It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary.","We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance.","Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs.","Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering.","Empirical results underscore TDD's proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content."],"url":"http://arxiv.org/abs/2405.11891v1","category":"cs.CL"}
{"created":"2024-05-20 08:54:03","title":"Out-of-Distribution Detection with a Single Unconditional Diffusion Model","abstract":"Out-of-distribution (OOD) detection is a critical task in machine learning that seeks to identify abnormal samples. Traditionally, unsupervised methods utilize a deep generative model for OOD detection. However, such approaches necessitate a different model when evaluating abnormality against a new distribution. With the emergence of foundational generative models, this paper explores whether a single generalist model can also perform OOD detection across diverse tasks. To that end, we introduce our method, Diffusion Paths, (DiffPath) in this work. DiffPath proposes to utilize a single diffusion model originally trained to perform unconditional generation for OOD detection. Specifically, we introduce a novel technique of measuring the rate-of-change and curvature of the diffusion paths connecting samples to the standard normal. Extensive experiments show that with a single model, DiffPath outperforms prior work on a variety of OOD tasks involving different distributions. Our code is publicly available at https://github.com/clear-nus/diffpath.","sentences":["Out-of-distribution (OOD) detection is a critical task in machine learning that seeks to identify abnormal samples.","Traditionally, unsupervised methods utilize a deep generative model for OOD detection.","However, such approaches necessitate a different model when evaluating abnormality against a new distribution.","With the emergence of foundational generative models, this paper explores whether a single generalist model can also perform OOD detection across diverse tasks.","To that end, we introduce our method, Diffusion Paths, (DiffPath) in this work.","DiffPath proposes to utilize a single diffusion model originally trained to perform unconditional generation for OOD detection.","Specifically, we introduce a novel technique of measuring the rate-of-change and curvature of the diffusion paths connecting samples to the standard normal.","Extensive experiments show that with a single model, DiffPath outperforms prior work on a variety of OOD tasks involving different distributions.","Our code is publicly available at https://github.com/clear-nus/diffpath."],"url":"http://arxiv.org/abs/2405.11881v1","category":"cs.LG"}
{"created":"2024-05-20 08:51:03","title":"Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs","abstract":"In this study, we propose an axiomatic system to define and quantify the precise memorization and in-context reasoning effects used by the large language model (LLM) for language generation. These effects are formulated as non-linear interactions between tokens/words encoded by the LLM. Specifically, the axiomatic system enables us to categorize the memorization effects into foundational memorization effects and chaotic memorization effects, and further classify in-context reasoning effects into enhanced inference patterns, eliminated inference patterns, and reversed inference patterns. Besides, the decomposed effects satisfy the sparsity property and the universal matching property, which mathematically guarantee that the LLM's confidence score can be faithfully decomposed into the memorization effects and in-context reasoning effects. Experiments show that the clear disentanglement of memorization effects and in-context reasoning effects enables a straightforward examination of detailed inference patterns encoded by LLMs.","sentences":["In this study, we propose an axiomatic system to define and quantify the precise memorization and in-context reasoning effects used by the large language model (LLM) for language generation.","These effects are formulated as non-linear interactions between tokens/words encoded by the LLM.","Specifically, the axiomatic system enables us to categorize the memorization effects into foundational memorization effects and chaotic memorization effects, and further classify in-context reasoning effects into enhanced inference patterns, eliminated inference patterns, and reversed inference patterns.","Besides, the decomposed effects satisfy the sparsity property and the universal matching property, which mathematically guarantee that the LLM's confidence score can be faithfully decomposed into the memorization effects and in-context reasoning effects.","Experiments show that the clear disentanglement of memorization effects and in-context reasoning effects enables a straightforward examination of detailed inference patterns encoded by LLMs."],"url":"http://arxiv.org/abs/2405.11880v1","category":"cs.LG"}
{"created":"2024-05-20 08:41:15","title":"A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus","abstract":"Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available at https://github.com/Eduard6421/RONLI.","sentences":["Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding.","Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language.","To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels.","We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines.","Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography.","Our dataset and code to reproduce the baselines are available at https://github.com/Eduard6421/RONLI."],"url":"http://arxiv.org/abs/2405.11877v2","category":"cs.CL"}
{"created":"2024-05-20 08:23:28","title":"Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process","abstract":"Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) are two fundamental processes for enhancing the capabilities of Language Models (LMs) post pre-training, aligning them better with human preferences. Although SFT advances in training efficiency, RLHF delivers better alignment, thus they are often combined. However, common practices simply apply them sequentially without unifying their optimization targets, resulting in a trade-off between fitting different objectives, and ignoring the opportunities to bridge the paradigm gap and take the strength from both. To obtain a unified understanding, we interpret SFT and RLHF using two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP) framework. This modeling shows that SFT is only a specialized case of RLHF with inferior estimation and optimization. RLHF evaluates the quality of model's entire generated answer, whereas SFT only scores predicted tokens based on preceding tokens from target answers. Therefore, SFT overestimates the ability of model, leading to inferior optimization. Building on this view, we introduce Intuitive Fine-tuning (IFT) to integrate SFT and RLHF into a single process. IFT captures LMs' intuitive sense of the entire answers through a temporal residual connection, while using a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to sequential recipes of SFT and some typical alignment methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT.","sentences":["Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) are two fundamental processes for enhancing the capabilities of Language Models (LMs) post pre-training, aligning them better with human preferences.","Although SFT advances in training efficiency, RLHF delivers better alignment, thus they are often combined.","However, common practices simply apply them sequentially without unifying their optimization targets, resulting in a trade-off between fitting different objectives, and ignoring the opportunities to bridge the paradigm gap and take the strength from both.","To obtain a unified understanding, we interpret SFT and RLHF using two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP) framework.","This modeling shows that SFT is only a specialized case of RLHF with inferior estimation and optimization.","RLHF evaluates the quality of model's entire generated answer, whereas SFT only scores predicted tokens based on preceding tokens from target answers.","Therefore, SFT overestimates the ability of model, leading to inferior optimization.","Building on this view, we introduce Intuitive Fine-tuning (IFT) to integrate SFT and RLHF into a single process.","IFT captures LMs' intuitive sense of the entire answers through a temporal residual connection, while using a single policy and the same volume of non-preference-labeled data as SFT.","Our experiments show that IFT performs comparably or even superiorly to sequential recipes of SFT and some typical alignment methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities.","An explainable Frozen Lake game further validates the effectiveness of IFT."],"url":"http://arxiv.org/abs/2405.11870v1","category":"cs.CL"}
{"created":"2024-05-20 08:19:10","title":"Towards Graph Contrastive Learning: A Survey and Beyond","abstract":"In recent years, deep learning on graphs has achieved remarkable success in various domains. However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature. To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress. SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data. While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature. Thus, this survey aims to fill this gap by offering a dedicated survey on GCL. We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives. Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios. We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field.","sentences":["In recent years, deep learning on graphs has achieved remarkable success in various domains.","However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature.","To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress.","SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data.","While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature.","Thus, this survey aims to fill this gap by offering a dedicated survey on GCL.","We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives.","Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios.","We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field."],"url":"http://arxiv.org/abs/2405.11868v1","category":"cs.LG"}
{"created":"2024-05-20 08:16:34","title":"CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English","abstract":"Modern named entity recognition systems have steadily improved performance in the age of larger and more powerful neural models. However, over the past several years, the state-of-the-art has seemingly hit another plateau on the benchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into the test outputs of the highest-performing NER models, conducting a fine-grained evaluation of their performance by introducing new document-level annotations on the test set. We go beyond F1 scores by categorizing errors in order to interpret the true state of the art for NER and guide future work. We review previous attempts at correcting the various flaws of the test set and introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis.","sentences":["Modern named entity recognition systems have steadily improved performance in the age of larger and more powerful neural models.","However, over the past several years, the state-of-the-art has seemingly hit another plateau on the benchmark CoNLL-03 English dataset.","In this paper, we perform a deep dive into the test outputs of the highest-performing NER models, conducting a fine-grained evaluation of their performance by introducing new document-level annotations on the test set.","We go beyond F1 scores by categorizing errors in order to interpret the true state of the art for NER and guide future work.","We review previous attempts at correcting the various flaws of the test set and introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis."],"url":"http://arxiv.org/abs/2405.11865v1","category":"cs.CL"}
{"created":"2024-05-20 07:47:06","title":"Alternators For Sequence Modeling","abstract":"This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. When alternators are used as generative models, the FTN produces interpretable low-dimensional latent variables that capture the dynamics governing the observations. When alternators are used as sequence-to-sequence predictors, the FTN learns to predict the observed features. In both cases, the OTN learns to produce sequences that match the data. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience, to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and outperform strong baselines such as neural ODEs and diffusion models in the domains we studied.","sentences":["This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences.","An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN).","The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle.","The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories.","Alternators are versatile.","They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors.","When alternators are used as generative models, the FTN produces interpretable low-dimensional latent variables that capture the dynamics governing the observations.","When alternators are used as sequence-to-sequence predictors, the FTN learns to predict the observed features.","In both cases, the OTN learns to produce sequences that match the data.","Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories.","We showcase the capabilities of alternators in three applications.","We first used alternators to model the Lorenz equations, often used to describe chaotic behavior.","We then applied alternators to Neuroscience, to map brain activity to physical activity.","Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting.","In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and outperform strong baselines such as neural ODEs and diffusion models in the domains we studied."],"url":"http://arxiv.org/abs/2405.11848v1","category":"stat.ML"}
{"created":"2024-05-20 07:38:19","title":"NeRTCAM: CAM-Based CMOS Implementation of Reference Frames for Neuromorphic Processors","abstract":"Neuromorphic architectures mimicking biological neural networks have been proposed as a much more efficient alternative to conventional von Neumann architectures for the exploding compute demands of AI workloads. Recent neuroscience theory on intelligence suggests that Cortical Columns (CCs) are the fundamental compute units in the neocortex and intelligence arises from CC's ability to store, predict and infer information via structured Reference Frames (RFs). Based on this theory, recent works have demonstrated brain-like visual object recognition using software simulation. Our work is the first attempt towards direct CMOS implementation of Reference Frames for building CC-based neuromorphic processors. We propose NeRTCAM (Neuromorphic Reverse Ternary Content Addressable Memory), a CAM-based building block that supports the key operations (store, predict, infer) required to perform inference using RFs. NeRTCAM architecture is presented in detail including its key components. All designs are implemented in SystemVerilog and synthesized in 7nm CMOS, and hardware complexity scaling is evaluated for varying storage sizes. NeRTCAM system for biologically motivated MNIST inference with a storage size of 1024 entries incurs just 0.15 mm^2 area, 400 mW power and 9.18 us critical path latency, demonstrating the feasibility of direct CMOS implementation of CAM-based Reference Frames.","sentences":["Neuromorphic architectures mimicking biological neural networks have been proposed as a much more efficient alternative to conventional von Neumann architectures for the exploding compute demands of AI workloads.","Recent neuroscience theory on intelligence suggests that Cortical Columns (CCs) are the fundamental compute units in the neocortex and intelligence arises from CC's ability to store, predict and infer information via structured Reference Frames (RFs).","Based on this theory, recent works have demonstrated brain-like visual object recognition using software simulation.","Our work is the first attempt towards direct CMOS implementation of Reference Frames for building CC-based neuromorphic processors.","We propose NeRTCAM (Neuromorphic Reverse Ternary Content Addressable Memory), a CAM-based building block that supports the key operations (store, predict, infer) required to perform inference using RFs.","NeRTCAM architecture is presented in detail including its key components.","All designs are implemented in SystemVerilog and synthesized in 7nm CMOS, and hardware complexity scaling is evaluated for varying storage sizes.","NeRTCAM system for biologically motivated MNIST inference with a storage size of 1024 entries incurs just 0.15 mm^2 area, 400 mW power and 9.18 us critical path latency, demonstrating the feasibility of direct CMOS implementation of CAM-based Reference Frames."],"url":"http://arxiv.org/abs/2405.11844v1","category":"cs.AR"}
{"created":"2024-05-20 07:34:48","title":"Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities","abstract":"Facing the current debate on whether Large Language Models (LLMs) attain near-human intelligence levels (Mitchell & Krakauer, 2023; Bubeck et al., 2023; Kosinski, 2023; Shiffrin & Mitchell, 2023; Ullman, 2023), the current study introduces a benchmark for evaluating social intelligence, one of the most distinctive aspects of human cognition. We developed a comprehensive theoretical framework for social dynamics and introduced two evaluation tasks: Inverse Reasoning (IR) and Inverse Inverse Planning (IIP). Our approach also encompassed a computational model based on recursive Bayesian inference, adept at elucidating diverse human behavioral patterns. Extensive experiments and detailed analyses revealed that humans surpassed the latest GPT models in overall performance, zero-shot learning, one-shot generalization, and adaptability to multi-modalities. Notably, GPT models demonstrated social intelligence only at the most basic order (order = 0), in stark contrast to human social intelligence (order >= 2). Further examination indicated a propensity of LLMs to rely on pattern recognition for shortcuts, casting doubt on their possession of authentic human-level social intelligence. Our codes, dataset, appendix and human data are released at https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence.","sentences":["Facing the current debate on whether Large Language Models (LLMs) attain near-human intelligence levels (Mitchell & Krakauer, 2023; Bubeck et al., 2023; Kosinski, 2023; Shiffrin & Mitchell, 2023; Ullman, 2023), the current study introduces a benchmark for evaluating social intelligence, one of the most distinctive aspects of human cognition.","We developed a comprehensive theoretical framework for social dynamics and introduced two evaluation tasks: Inverse Reasoning (IR) and Inverse Inverse Planning (IIP).","Our approach also encompassed a computational model based on recursive Bayesian inference, adept at elucidating diverse human behavioral patterns.","Extensive experiments and detailed analyses revealed that humans surpassed the latest GPT models in overall performance, zero-shot learning, one-shot generalization, and adaptability to multi-modalities.","Notably, GPT models demonstrated social intelligence only at the most basic order (order = 0), in stark contrast to human social intelligence (order >= 2).","Further examination indicated a propensity of LLMs to rely on pattern recognition for shortcuts, casting doubt on their possession of authentic human-level social intelligence.","Our codes, dataset, appendix and human data are released at https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence."],"url":"http://arxiv.org/abs/2405.11841v1","category":"cs.AI"}
{"created":"2024-05-20 07:25:09","title":"Improving the Explain-Any-Concept by Introducing Nonlinearity to the Trainable Surrogate Model","abstract":"In the evolving field of Explainable AI (XAI), interpreting the decisions of deep neural networks (DNNs) in computer vision tasks is an important process. While pixel-based XAI methods focus on identifying significant pixels, existing concept-based XAI methods use pre-defined or human-annotated concepts. The recently proposed Segment Anything Model (SAM) achieved a significant step forward to prepare automatic concept sets via comprehensive instance segmentation. Building upon this, the Explain Any Concept (EAC) model emerged as a flexible method for explaining DNN decisions. EAC model is based on using a surrogate model which has one trainable linear layer to simulate the target model. In this paper, by introducing an additional nonlinear layer to the original surrogate model, we show that we can improve the performance of the EAC model. We compare our proposed approach to the original EAC model and report improvements obtained on both ImageNet and MS COCO datasets.","sentences":["In the evolving field of Explainable AI (XAI), interpreting the decisions of deep neural networks (DNNs) in computer vision tasks is an important process.","While pixel-based XAI methods focus on identifying significant pixels, existing concept-based XAI methods use pre-defined or human-annotated concepts.","The recently proposed Segment Anything Model (SAM) achieved a significant step forward to prepare automatic concept sets via comprehensive instance segmentation.","Building upon this, the Explain Any Concept (EAC) model emerged as a flexible method for explaining DNN decisions.","EAC model is based on using a surrogate model which has one trainable linear layer to simulate the target model.","In this paper, by introducing an additional nonlinear layer to the original surrogate model, we show that we can improve the performance of the EAC model.","We compare our proposed approach to the original EAC model and report improvements obtained on both ImageNet and MS COCO datasets."],"url":"http://arxiv.org/abs/2405.11837v1","category":"cs.CV"}
{"created":"2024-05-20 06:43:33","title":"Data quality control system and long-term performance monitor of the LHAASO-KM2A","abstract":"The KM2A is the largest sub-array of the Large High Altitude Air Shower Observatory (LHAASO). It consists of 5216 electromagnetic particle detectors (EDs) and 1188 muon detectors (MDs). The data recorded by the EDs and MDs are used to reconstruct primary information of cosmic ray and gamma-ray showers. This information is used for physical analysis in gamma-ray astronomy and cosmic ray physics. To ensure the reliability of the LHAASO-KM2A data, a three-level quality control system has been established. It is used to monitor the status of detector units, stability of reconstructed parameters and the performance of the array based on observations of the Crab Nebula and Moon shadow. This paper will introduce the control system and its application on the LHAASO-KM2A data collected from August 2021 to July 2023. During this period, the pointing and angular resolution of the array were stable. From the observations of the Moon shadow and Crab Nebula, the results achieved using the two methods are consistent with each other. According to the observation of the Crab Nebula at energies from 25 TeV to 100 TeV, the time averaged pointing errors are estimated to be $-0.003^{\\circ} \\pm 0.005^{\\circ}$ and $0.001^{\\circ} \\pm 0.006^{\\circ}$ in the R.A. and Dec directions, respectively.","sentences":["The KM2A is the largest sub-array of the Large High Altitude Air Shower Observatory (LHAASO).","It consists of 5216 electromagnetic particle detectors (EDs) and 1188 muon detectors (MDs).","The data recorded by the EDs and MDs are used to reconstruct primary information of cosmic ray and gamma-ray showers.","This information is used for physical analysis in gamma-ray astronomy and cosmic ray physics.","To ensure the reliability of the LHAASO-KM2A data, a three-level quality control system has been established.","It is used to monitor the status of detector units, stability of reconstructed parameters and the performance of the array based on observations of the Crab Nebula and Moon shadow.","This paper will introduce the control system and its application on the LHAASO-KM2A data collected from August 2021 to July 2023.","During this period, the pointing and angular resolution of the array were stable.","From the observations of the Moon shadow and Crab Nebula, the results achieved using the two methods are consistent with each other.","According to the observation of the Crab Nebula at energies from 25 TeV to 100 TeV, the time averaged pointing errors are estimated to be $-0.003^{\\circ} \\pm 0.005^{\\circ}$ and $0.001^{\\circ} \\pm 0.006^{\\circ}$ in the R.A. and Dec directions, respectively."],"url":"http://arxiv.org/abs/2405.11826v1","category":"hep-ex"}
{"created":"2024-05-20 06:34:47","title":"Stereo-Knowledge Distillation from dpMV to Dual Pixels for Light Field Video Reconstruction","abstract":"Dual pixels contain disparity cues arising from the defocus blur. This disparity information is useful for many vision tasks ranging from autonomous driving to 3D creative realism. However, directly estimating disparity from dual pixels is less accurate. This work hypothesizes that distilling high-precision dark stereo knowledge, implicitly or explicitly, to efficient dual-pixel student networks enables faithful reconstructions. This dark knowledge distillation should also alleviate stereo-synchronization setup and calibration costs while dramatically increasing parameter and inference time efficiency. We collect the first and largest 3-view dual-pixel video dataset, dpMV, to validate our explicit dark knowledge distillation hypothesis. We show that these methods outperform purely monocular solutions, especially in challenging foreground-background separation regions using faithful guidance from dual pixels. Finally, we demonstrate an unconventional use case unlocked by dpMV and implicit dark knowledge distillation from an ensemble of teachers for Light Field (LF) video reconstruction. Our LF video reconstruction method is the fastest and most temporally consistent to date. It remains competitive in reconstruction fidelity while offering many other essential properties like high parameter efficiency, implicit disocclusion handling, zero-shot cross-dataset transfer, geometrically consistent inference on higher spatial-angular resolutions, and adaptive baseline control. All source code is available at the anonymous repository https://github.com/Aryan-Garg.","sentences":["Dual pixels contain disparity cues arising from the defocus blur.","This disparity information is useful for many vision tasks ranging from autonomous driving to 3D creative realism.","However, directly estimating disparity from dual pixels is less accurate.","This work hypothesizes that distilling high-precision dark stereo knowledge, implicitly or explicitly, to efficient dual-pixel student networks enables faithful reconstructions.","This dark knowledge distillation should also alleviate stereo-synchronization setup and calibration costs while dramatically increasing parameter and inference time efficiency.","We collect the first and largest 3-view dual-pixel video dataset, dpMV, to validate our explicit dark knowledge distillation hypothesis.","We show that these methods outperform purely monocular solutions, especially in challenging foreground-background separation regions using faithful guidance from dual pixels.","Finally, we demonstrate an unconventional use case unlocked by dpMV and implicit dark knowledge distillation from an ensemble of teachers for Light Field (LF) video reconstruction.","Our LF video reconstruction method is the fastest and most temporally consistent to date.","It remains competitive in reconstruction fidelity while offering many other essential properties like high parameter efficiency, implicit disocclusion handling, zero-shot cross-dataset transfer, geometrically consistent inference on higher spatial-angular resolutions, and adaptive baseline control.","All source code is available at the anonymous repository https://github.com/Aryan-Garg."],"url":"http://arxiv.org/abs/2405.11823v1","category":"cs.CV"}
{"created":"2024-05-20 06:23:22","title":"Transfer Learning for CSI-based Positioning with Multi-environment Meta-learning","abstract":"Utilizing deep learning (DL) techniques for radio-based positioning of user equipment (UE) through channel state information (CSI) fingerprints has demonstrated significant potential. DL models can extract complex characteristics from the CSI fingerprints of a particular environment and accurately predict the position of a UE. Nonetheless, the effectiveness of the DL model trained on CSI fingerprints is highly dependent on the particular training environment, limiting the trained model's applicability across different environments. This paper proposes a novel DL model structure consisting of two parts, where the first part aims at identifying features that are independent from any specific environment, while the second part combines those features in an environment specific way with the goal of positioning. To train such a two-part model, we propose the multi-environment meta-learning (MEML) approach for the first part to facilitate training across various environments, while the second part of the model is trained solely on data from a specific environment. Our findings indicate that employing the MEML approach for initializing the weights of the DL model for a new unseen environment significantly boosts the accuracy of UE positioning in the new target environment as well the reliability of its uncertainty estimation. This method outperforms traditional transfer learning methods, whether direct transfer learning (DTL) between environments or completely training from scratch with data from a new environment. The proposed approach is verified with real measurements for both line-of-sight (LOS) and non-LOS (NLOS) environments.","sentences":["Utilizing deep learning (DL) techniques for radio-based positioning of user equipment (UE) through channel state information (CSI) fingerprints has demonstrated significant potential.","DL models can extract complex characteristics from the CSI fingerprints of a particular environment and accurately predict the position of a UE.","Nonetheless, the effectiveness of the DL model trained on CSI fingerprints is highly dependent on the particular training environment, limiting the trained model's applicability across different environments.","This paper proposes a novel DL model structure consisting of two parts, where the first part aims at identifying features that are independent from any specific environment, while the second part combines those features in an environment specific way with the goal of positioning.","To train such a two-part model, we propose the multi-environment meta-learning (MEML) approach for the first part to facilitate training across various environments, while the second part of the model is trained solely on data from a specific environment.","Our findings indicate that employing the MEML approach for initializing the weights of the DL model for a new unseen environment significantly boosts the accuracy of UE positioning in the new target environment as well the reliability of its uncertainty estimation.","This method outperforms traditional transfer learning methods, whether direct transfer learning (DTL) between environments or completely training from scratch with data from a new environment.","The proposed approach is verified with real measurements for both line-of-sight (LOS) and non-LOS (NLOS) environments."],"url":"http://arxiv.org/abs/2405.11816v1","category":"eess.SP"}
{"created":"2024-05-20 06:21:15","title":"Climatic & Anthropogenic Hazards to the Nasca World Heritage: Application of Remote Sensing, AI, and Flood Modelling","abstract":"Preservation of the Nasca geoglyphs at the UNESCO World Heritage Site in Peru is urgent as natural and human impact accelerates. More frequent weather extremes such as flashfloods threaten Nasca artifacts. We demonstrate that runoff models based on (sub-)meter scale, LiDAR-derived digital elevation data can highlight AI-detected geoglyphs that are in danger of erosion. We recommend measures of mitigation to protect the famous \"lizard\", \"tree\", and \"hand\" geoglyphs located close by, or even cut by the Pan-American Highway.","sentences":["Preservation of the Nasca geoglyphs at the UNESCO World Heritage Site in Peru is urgent as natural and human impact accelerates.","More frequent weather extremes such as flashfloods threaten Nasca artifacts.","We demonstrate that runoff models based on (sub-)meter scale, LiDAR-derived digital elevation data can highlight AI-detected geoglyphs that are in danger of erosion.","We recommend measures of mitigation to protect the famous \"lizard\", \"tree\", and \"hand\" geoglyphs located close by, or even cut by the Pan-American Highway."],"url":"http://arxiv.org/abs/2405.11814v1","category":"cs.CV"}
{"created":"2024-05-20 06:03:55","title":"Distill-then-prune: An Efficient Compression Framework for Real-time Stereo Matching Network on Edge Devices","abstract":"In recent years, numerous real-time stereo matching methods have been introduced, but they often lack accuracy. These methods attempt to improve accuracy by introducing new modules or integrating traditional methods. However, the improvements are only modest. In this paper, we propose a novel strategy by incorporating knowledge distillation and model pruning to overcome the inherent trade-off between speed and accuracy. As a result, we obtained a model that maintains real-time performance while delivering high accuracy on edge devices. Our proposed method involves three key steps. Firstly, we review state-of-the-art methods and design our lightweight model by removing redundant modules from those efficient models through a comparison of their contributions. Next, we leverage the efficient model as the teacher to distill knowledge into the lightweight model. Finally, we systematically prune the lightweight model to obtain the final model. Through extensive experiments conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform ablation studies to analyze the effectiveness of each module and present our state-of-the-art results.","sentences":["In recent years, numerous real-time stereo matching methods have been introduced, but they often lack accuracy.","These methods attempt to improve accuracy by introducing new modules or integrating traditional methods.","However, the improvements are only modest.","In this paper, we propose a novel strategy by incorporating knowledge distillation and model pruning to overcome the inherent trade-off between speed and accuracy.","As a result, we obtained a model that maintains real-time performance while delivering high accuracy on edge devices.","Our proposed method involves three key steps.","Firstly, we review state-of-the-art methods and design our lightweight model by removing redundant modules from those efficient models through a comparison of their contributions.","Next, we leverage the efficient model as the teacher to distill knowledge into the lightweight model.","Finally, we systematically prune the lightweight model to obtain the final model.","Through extensive experiments conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform ablation studies to analyze the effectiveness of each module and present our state-of-the-art results."],"url":"http://arxiv.org/abs/2405.11809v1","category":"cs.CV"}
{"created":"2024-05-20 05:55:08","title":"(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts","abstract":"Recent advancements in machine translation (MT) have significantly enhanced translation quality across various domains. However, the translation of literary texts remains a formidable challenge due to their complex language, figurative expressions, and cultural nuances. In this work, we introduce a novel multi-agent framework based on large language models (LLMs) for literary translation, implemented as a company called TransAgents, which mirrors traditional translation publication process by leveraging the collective capabilities of multiple agents, to address the intricate demands of translating literary works. To evaluate the effectiveness of our system, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP). MHP assesses translations from the perspective of monolingual readers of the target language, while BLP uses advanced LLMs to compare translations directly with the original texts. Empirical findings indicate that despite lower d-BLEU scores, translations from TransAgents are preferred by both human evaluators and LLMs over human-written references, particularly in genres requiring domain-specific knowledge. We also highlight the strengths and limitations of TransAgents through case studies and suggests directions for future research.","sentences":["Recent advancements in machine translation (MT) have significantly enhanced translation quality across various domains.","However, the translation of literary texts remains a formidable challenge due to their complex language, figurative expressions, and cultural nuances.","In this work, we introduce a novel multi-agent framework based on large language models (LLMs) for literary translation, implemented as a company called TransAgents, which mirrors traditional translation publication process by leveraging the collective capabilities of multiple agents, to address the intricate demands of translating literary works.","To evaluate the effectiveness of our system, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP).","MHP assesses translations from the perspective of monolingual readers of the target language, while BLP uses advanced LLMs to compare translations directly with the original texts.","Empirical findings indicate that despite lower d-BLEU scores, translations from TransAgents are preferred by both human evaluators and LLMs over human-written references, particularly in genres requiring domain-specific knowledge.","We also highlight the strengths and limitations of TransAgents through case studies and suggests directions for future research."],"url":"http://arxiv.org/abs/2405.11804v1","category":"cs.CL"}
{"created":"2024-05-20 05:48:20","title":"Counterfactual Explanation-Based Badminton Motion Guidance Generation Using Wearable Sensors","abstract":"This study proposes a framework for enhancing the stroke quality of badminton players by generating personalized motion guides, utilizing a multimodal wearable dataset. These guides are based on counterfactual algorithms and aim to reduce the performance gap between novice and expert players. Our approach provides joint-level guidance through visualizable data to assist players in improving their movements without requiring expert knowledge. The method was evaluated against a traditional algorithm using metrics to assess validity, proximity, and plausibility, including arithmetic measures and motion-specific evaluation metrics. Our evaluation demonstrates that the proposed framework can generate motions that maintain the essence of original movements while enhancing stroke quality, providing closer guidance than direct expert motion replication. The results highlight the potential of our approach for creating personalized sports motion guides by generating counterfactual motion guidance for arbitrary input motion samples of badminton strokes.","sentences":["This study proposes a framework for enhancing the stroke quality of badminton players by generating personalized motion guides, utilizing a multimodal wearable dataset.","These guides are based on counterfactual algorithms and aim to reduce the performance gap between novice and expert players.","Our approach provides joint-level guidance through visualizable data to assist players in improving their movements without requiring expert knowledge.","The method was evaluated against a traditional algorithm using metrics to assess validity, proximity, and plausibility, including arithmetic measures and motion-specific evaluation metrics.","Our evaluation demonstrates that the proposed framework can generate motions that maintain the essence of original movements while enhancing stroke quality, providing closer guidance than direct expert motion replication.","The results highlight the potential of our approach for creating personalized sports motion guides by generating counterfactual motion guidance for arbitrary input motion samples of badminton strokes."],"url":"http://arxiv.org/abs/2405.11802v1","category":"cs.HC"}
{"created":"2024-05-20 05:46:38","title":"Generative AI in Higher Education: A Global Perspective of Institutional Adoption Policies and Guidelines","abstract":"Integrating generative AI (GAI) into higher education is crucial for preparing a future generation of GAI-literate students. Yet a thorough understanding of the global institutional adoption policy remains absent, with most of the prior studies focused on the Global North and the promises and challenges of GAI, lacking a theoretical lens. This study utilizes the Diffusion of Innovations Theory to examine GAI adoption strategies in higher education across 40 universities from six global regions. It explores the characteristics of GAI innovation, including compatibility, trialability, and observability, and analyses the communication channels and roles and responsibilities outlined in university policies and guidelines. The findings reveal a proactive approach by universities towards GAI integration, emphasizing academic integrity, teaching and learning enhancement, and equity. Despite a cautious yet optimistic stance, a comprehensive policy framework is needed to evaluate the impacts of GAI integration and establish effective communication strategies that foster broader stakeholder engagement. The study highlights the importance of clear roles and responsibilities among faculty, students, and administrators for successful GAI integration, supporting a collaborative model for navigating the complexities of GAI in education. This study contributes insights for policymakers in crafting detailed strategies for its integration.","sentences":["Integrating generative AI (GAI) into higher education is crucial for preparing a future generation of GAI-literate students.","Yet a thorough understanding of the global institutional adoption policy remains absent, with most of the prior studies focused on the Global North and the promises and challenges of GAI, lacking a theoretical lens.","This study utilizes the Diffusion of Innovations Theory to examine GAI adoption strategies in higher education across 40 universities from six global regions.","It explores the characteristics of GAI innovation, including compatibility, trialability, and observability, and analyses the communication channels and roles and responsibilities outlined in university policies and guidelines.","The findings reveal a proactive approach by universities towards GAI integration, emphasizing academic integrity, teaching and learning enhancement, and equity.","Despite a cautious yet optimistic stance, a comprehensive policy framework is needed to evaluate the impacts of GAI integration and establish effective communication strategies that foster broader stakeholder engagement.","The study highlights the importance of clear roles and responsibilities among faculty, students, and administrators for successful GAI integration, supporting a collaborative model for navigating the complexities of GAI in education.","This study contributes insights for policymakers in crafting detailed strategies for its integration."],"url":"http://arxiv.org/abs/2405.11800v1","category":"cs.CY"}
{"created":"2024-05-20 05:28:22","title":"ViViD: Video Virtual Try-on using Diffusion Models","abstract":"Video virtual try-on aims to transfer a clothing item onto the video of a target person. Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results. In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on. Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism. To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis. Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date. Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results. The dataset, codes, and weights will be publicly available. Project page: https://becauseimbatman0.github.io/ViViD.","sentences":["Video virtual try-on aims to transfer a clothing item onto the video of a target person.","Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results.","In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on.","Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism.","To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis.","Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date.","Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results.","The dataset, codes, and weights will be publicly available.","Project page: https://becauseimbatman0.github.io/ViViD."],"url":"http://arxiv.org/abs/2405.11794v1","category":"cs.CV"}
{"created":"2024-05-20 05:23:56","title":"MM-Retinal: Knowledge-Enhanced Foundational Pretraining with Fundus Image-Text Expertise","abstract":"Current fundus image analysis models are predominantly built for specific tasks relying on individual datasets. The learning process is usually based on data-driven paradigm without prior knowledge, resulting in poor transferability and generalizability. To address this issue, we propose MM-Retinal, a multi-modal dataset that encompasses high-quality image-text pairs collected from professional fundus diagram books. Moreover, enabled by MM-Retinal, we present a novel Knowledge-enhanced foundational pretraining model which incorporates Fundus Image-Text expertise, called KeepFIT. It is designed with image similarity-guided text revision and mixed training strategy to infuse expert knowledge. Our proposed fundus foundation model achieves state-of-the-art performance across six unseen downstream tasks and holds excellent generalization ability in zero-shot and few-shot scenarios. MM-Retinal and KeepFIT are available at https://github.com/lxirich/MM-Retinal.","sentences":["Current fundus image analysis models are predominantly built for specific tasks relying on individual datasets.","The learning process is usually based on data-driven paradigm without prior knowledge, resulting in poor transferability and generalizability.","To address this issue, we propose MM-Retinal, a multi-modal dataset that encompasses high-quality image-text pairs collected from professional fundus diagram books.","Moreover, enabled by MM-Retinal, we present a novel Knowledge-enhanced foundational pretraining model which incorporates Fundus Image-Text expertise, called KeepFIT.","It is designed with image similarity-guided text revision and mixed training strategy to infuse expert knowledge.","Our proposed fundus foundation model achieves state-of-the-art performance across six unseen downstream tasks and holds excellent generalization ability in zero-shot and few-shot scenarios.","MM-Retinal and KeepFIT are available at https://github.com/lxirich/MM-Retinal."],"url":"http://arxiv.org/abs/2405.11793v1","category":"cs.CV"}
{"created":"2024-05-20 05:05:14","title":"Reward-Punishment Reinforcement Learning with Maximum Entropy","abstract":"We introduce the ``soft Deep MaxPain'' (softDMP) algorithm, which integrates the optimization of long-term policy entropy into reward-punishment reinforcement learning objectives. Our motivation is to facilitate a smoother variation of operators utilized in the updating of action values beyond traditional ``max'' and ``min'' operators, where the goal is enhancing sample efficiency and robustness. We also address two unresolved issues from the previous Deep MaxPain method. Firstly, we investigate how the negated (``flipped'') pain-seeking sub-policy, derived from the punishment action value, collaborates with the ``min'' operator to effectively learn the punishment module and how softDMP's smooth learning operator provides insights into the ``flipping'' trick. Secondly, we tackle the challenge of data collection for learning the punishment module to mitigate inconsistencies arising from the involvement of the ``flipped'' sub-policy (pain-avoidance sub-policy) in the unified behavior policy. We empirically explore the first issue in two discrete Markov Decision Process (MDP) environments, elucidating the crucial advancements of the DMP approach and the necessity for soft treatments on the hard operators. For the second issue, we propose a probabilistic classifier based on the ratio of the pain-seeking sub-policy to the sum of the pain-seeking and goal-reaching sub-policies. This classifier assigns roll-outs to separate replay buffers for updating reward and punishment action-value functions, respectively. Our framework demonstrates superior performance in Turtlebot 3's maze navigation tasks under the ROS Gazebo simulation.","sentences":["We introduce the ``soft Deep MaxPain'' (softDMP) algorithm, which integrates the optimization of long-term policy entropy into reward-punishment reinforcement learning objectives.","Our motivation is to facilitate a smoother variation of operators utilized in the updating of action values beyond traditional ``max'' and ``min'' operators, where the goal is enhancing sample efficiency and robustness.","We also address two unresolved issues from the previous Deep MaxPain method.","Firstly, we investigate how the negated (``flipped'') pain-seeking sub-policy, derived from the punishment action value, collaborates with the ``min'' operator to effectively learn the punishment module and how softDMP's smooth learning operator provides insights into the ``flipping'' trick.","Secondly, we tackle the challenge of data collection for learning the punishment module to mitigate inconsistencies arising from the involvement of the ``flipped'' sub-policy (pain-avoidance sub-policy) in the unified behavior policy.","We empirically explore the first issue in two discrete Markov Decision Process (MDP) environments, elucidating the crucial advancements of the DMP approach and the necessity for soft treatments on the hard operators.","For the second issue, we propose a probabilistic classifier based on the ratio of the pain-seeking sub-policy to the sum of the pain-seeking and goal-reaching sub-policies.","This classifier assigns roll-outs to separate replay buffers for updating reward and punishment action-value functions, respectively.","Our framework demonstrates superior performance in Turtlebot 3's maze navigation tasks under the ROS Gazebo simulation."],"url":"http://arxiv.org/abs/2405.11784v1","category":"cs.LG"}
{"created":"2024-05-20 05:02:12","title":"Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing","abstract":"In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties. Specifically, by analyzing 150 hypothetical MOF structures consisting of 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $H_{2}$ uptake values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset. Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 85.7% and 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake, respectively. Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake datasets. Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake, respectively. Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs.","sentences":["In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties.","Specifically, by analyzing 150 hypothetical MOF structures consisting of 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $H_{2}$ uptake values.","We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset.","Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 85.7% and 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake, respectively.","Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake datasets.","Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake, respectively.","Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs."],"url":"http://arxiv.org/abs/2405.11783v1","category":"cs.LG"}
{"created":"2024-05-20 04:36:02","title":"Efficient Multi-agent Reinforcement Learning by Planning","abstract":"Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks. Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios. In contrast, model-based reinforcement learning (MBRL), particularly algorithms integrating planning, such as MuZero, has demonstrated superhuman performance with limited data in many tasks. Hence, we aim to boost the sample efficiency of MARL by adopting model-based approaches. However, incorporating planning and search methods into multi-agent systems poses significant challenges. The expansive action space of multi-agent systems often necessitates leveraging the nearly-independent property of agents to accelerate learning. To tackle this issue, we propose the MAZero algorithm, which combines a centralized model with Monte Carlo Tree Search (MCTS) for policy search. We design a novel network structure to facilitate distributed execution and parameter sharing. To enhance search efficiency in deterministic environments with sizable action spaces, we introduce two novel techniques: Optimistic Search Lambda (OS($\\lambda$)) and Advantage-Weighted Policy Optimization (AWPO). Extensive experiments on the SMAC benchmark demonstrate that MAZero outperforms model-free approaches in terms of sample efficiency and provides comparable or better performance than existing model-based methods in terms of both sample and computational efficiency. Our code is available at https://github.com/liuqh16/MAZero.","sentences":["Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks.","Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios.","In contrast, model-based reinforcement learning (MBRL), particularly algorithms integrating planning, such as MuZero, has demonstrated superhuman performance with limited data in many tasks.","Hence, we aim to boost the sample efficiency of MARL by adopting model-based approaches.","However, incorporating planning and search methods into multi-agent systems poses significant challenges.","The expansive action space of multi-agent systems often necessitates leveraging the nearly-independent property of agents to accelerate learning.","To tackle this issue, we propose the MAZero algorithm, which combines a centralized model with Monte Carlo Tree Search (MCTS) for policy search.","We design a novel network structure to facilitate distributed execution and parameter sharing.","To enhance search efficiency in deterministic environments with sizable action spaces, we introduce two novel techniques: Optimistic Search Lambda (OS($\\lambda$)) and Advantage-Weighted Policy Optimization (AWPO).","Extensive experiments on the SMAC benchmark demonstrate that MAZero outperforms model-free approaches in terms of sample efficiency and provides comparable or better performance than existing model-based methods in terms of both sample and computational efficiency.","Our code is available at https://github.com/liuqh16/MAZero."],"url":"http://arxiv.org/abs/2405.11778v1","category":"cs.LG"}
{"created":"2024-05-20 04:32:51","title":"Active Exploration for Real-Time Haptic Training","abstract":"Tactile perception is important for robotic systems that interact with the world through touch. Touch is an active sense in which tactile measurements depend on the contact properties of an interaction--e.g., velocity, force, acceleration--as well as properties of the sensor and object under test. These dependencies make training tactile perceptual models challenging. Additionally, the effects of limited sensor life and the near-field nature of tactile sensors preclude the practical collection of exhaustive data sets even for fairly simple objects. Active learning provides a mechanism for focusing on only the most informative aspects of an object during data collection. Here we employ an active learning approach that uses a data-driven model's entropy as an uncertainty measure and explore relative to that entropy conditioned on the sensor state variables. Using a coverage-based ergodic controller, we train perceptual models in near-real time. We demonstrate our approach using a biomimentic sensor, exploring \"tactile scenes\" composed of shapes, textures, and objects. Each learned representation provides a perceptual sensor model for a particular tactile scene. Models trained on actively collected data outperform their randomly collected counterparts in real-time training tests. Additionally, we find that the resulting network entropy maps can be used to identify high salience portions of a tactile scene.","sentences":["Tactile perception is important for robotic systems that interact with the world through touch.","Touch is an active sense in which tactile measurements depend on the contact properties of an interaction--e.g., velocity, force, acceleration--as well as properties of the sensor and object under test.","These dependencies make training tactile perceptual models challenging.","Additionally, the effects of limited sensor life and the near-field nature of tactile sensors preclude the practical collection of exhaustive data sets even for fairly simple objects.","Active learning provides a mechanism for focusing on only the most informative aspects of an object during data collection.","Here we employ an active learning approach that uses a data-driven model's entropy as an uncertainty measure and explore relative to that entropy conditioned on the sensor state variables.","Using a coverage-based ergodic controller, we train perceptual models in near-real time.","We demonstrate our approach using a biomimentic sensor, exploring \"tactile scenes\" composed of shapes, textures, and objects.","Each learned representation provides a perceptual sensor model for a particular tactile scene.","Models trained on actively collected data outperform their randomly collected counterparts in real-time training tests.","Additionally, we find that the resulting network entropy maps can be used to identify high salience portions of a tactile scene."],"url":"http://arxiv.org/abs/2405.11776v1","category":"cs.RO"}
{"created":"2024-05-20 04:05:30","title":"Uni-Mol Docking V2: Towards Realistic and Accurate Binding Pose Prediction","abstract":"In recent years, machine learning (ML) methods have emerged as promising alternatives for molecular docking, offering the potential for high accuracy without incurring prohibitive computational costs. However, recent studies have indicated that these ML models may overfit to quantitative metrics while neglecting the physical constraints inherent in the problem. In this work, we present Uni-Mol Docking V2, which demonstrates a remarkable improvement in performance, accurately predicting the binding poses of 77+% of ligands in the PoseBusters benchmark with an RMSD value of less than 2.0 {\\AA}, and 75+% passing all quality checks. This represents a significant increase from the 62% achieved by the previous Uni-Mol Docking model. Notably, our Uni-Mol Docking approach generates chemically accurate predictions, circumventing issues such as chirality inversions and steric clashes that have plagued previous ML models. Furthermore, we observe enhanced performance in terms of high-quality predictions (RMSD values of less than 1.0 {\\AA} and 1.5 {\\AA}) and physical soundness when Uni-Mol Docking is combined with more physics-based methods like Uni-Dock. Our results represent a significant advancement in the application of artificial intelligence for scientific research, adopting a holistic approach to ligand docking that is well-suited for industrial applications in virtual screening and drug design. The code, data and service for Uni-Mol Docking are publicly available for use and further development in https://github.com/dptech-corp/Uni-Mol.","sentences":["In recent years, machine learning (ML) methods have emerged as promising alternatives for molecular docking, offering the potential for high accuracy without incurring prohibitive computational costs.","However, recent studies have indicated that these ML models may overfit to quantitative metrics while neglecting the physical constraints inherent in the problem.","In this work, we present Uni-Mol Docking V2, which demonstrates a remarkable improvement in performance, accurately predicting the binding poses of 77+% of ligands in the PoseBusters benchmark with an RMSD value of less than 2.0 {\\AA}, and 75+% passing all quality checks.","This represents a significant increase from the 62% achieved by the previous Uni-Mol Docking model.","Notably, our Uni-Mol Docking approach generates chemically accurate predictions, circumventing issues such as chirality inversions and steric clashes that have plagued previous ML models.","Furthermore, we observe enhanced performance in terms of high-quality predictions (RMSD values of less than 1.0 {\\AA} and 1.5 {\\AA}) and physical soundness when Uni-Mol Docking is combined with more physics-based methods like Uni-Dock.","Our results represent a significant advancement in the application of artificial intelligence for scientific research, adopting a holistic approach to ligand docking that is well-suited for industrial applications in virtual screening and drug design.","The code, data and service for Uni-Mol Docking are publicly available for use and further development in https://github.com/dptech-corp/Uni-Mol."],"url":"http://arxiv.org/abs/2405.11769v1","category":"q-bio.BM"}
{"created":"2024-05-20 03:52:41","title":"From SHAP Scores to Feature Importance Scores","abstract":"A central goal of eXplainable Artificial Intelligence (XAI) is to assign relative importance to the features of a Machine Learning (ML) model given some prediction. The importance of this task of explainability by feature attribution is illustrated by the ubiquitous recent use of tools such as SHAP and LIME. Unfortunately, the exact computation of feature attributions, using the game-theoretical foundation underlying SHAP and LIME, can yield manifestly unsatisfactory results, that tantamount to reporting misleading relative feature importance. Recent work targeted rigorous feature attribution, by studying axiomatic aggregations of features based on logic-based definitions of explanations by feature selection. This paper shows that there is an essential relationship between feature attribution and a priori voting power, and that those recently proposed axiomatic aggregations represent a few instantiations of the range of power indices studied in the past. Furthermore, it remains unclear how some of the most widely used power indices might be exploited as feature importance scores (FISs), i.e. the use of power indices in XAI, and which of these indices would be the best suited for the purposes of XAI by feature attribution, namely in terms of not producing results that could be deemed as unsatisfactory. This paper proposes novel desirable properties that FISs should exhibit. In addition, the paper also proposes novel FISs exhibiting the proposed properties. Finally, the paper conducts a rigorous analysis of the best-known power indices in terms of the proposed properties.","sentences":["A central goal of eXplainable Artificial Intelligence (XAI) is to assign relative importance to the features of a Machine Learning (ML) model given some prediction.","The importance of this task of explainability by feature attribution is illustrated by the ubiquitous recent use of tools such as SHAP and LIME.","Unfortunately, the exact computation of feature attributions, using the game-theoretical foundation underlying SHAP and LIME, can yield manifestly unsatisfactory results, that tantamount to reporting misleading relative feature importance.","Recent work targeted rigorous feature attribution, by studying axiomatic aggregations of features based on logic-based definitions of explanations by feature selection.","This paper shows that there is an essential relationship between feature attribution and a priori voting power, and that those recently proposed axiomatic aggregations represent a few instantiations of the range of power indices studied in the past.","Furthermore, it remains unclear how some of the most widely used power indices might be exploited as feature importance scores (FISs), i.e. the use of power indices in XAI, and which of these indices would be the best suited for the purposes of XAI by feature attribution, namely in terms of not producing results that could be deemed as unsatisfactory.","This paper proposes novel desirable properties that FISs should exhibit.","In addition, the paper also proposes novel FISs exhibiting the proposed properties.","Finally, the paper conducts a rigorous analysis of the best-known power indices in terms of the proposed properties."],"url":"http://arxiv.org/abs/2405.11766v1","category":"cs.AI"}
{"created":"2024-05-20 03:48:45","title":"DATR: Unsupervised Domain Adaptive Detection Transformer with Dataset-Level Adaptation and Prototypical Alignment","abstract":"Object detectors frequently encounter significant performance degradation when confronted with domain gaps between collected data (source domain) and data from real-world applications (target domain). To address this task, numerous unsupervised domain adaptive detectors have been proposed, leveraging carefully designed feature alignment techniques. However, these techniques primarily align instance-level features in a class-agnostic manner, overlooking the differences between extracted features from different categories, which results in only limited improvement. Furthermore, the scope of current alignment modules is often restricted to a limited batch of images, failing to learn the entire dataset-level cues, thereby severely constraining the detector's generalization ability to the target domain. To this end, we introduce a strong DETR-based detector named Domain Adaptive detection TRansformer (DATR) for unsupervised domain adaptation of object detection. Firstly, we propose the Class-wise Prototypes Alignment (CPA) module, which effectively aligns cross-domain features in a class-aware manner by bridging the gap between object detection task and domain adaptation task. Then, the designed Dataset-level Alignment Scheme (DAS) explicitly guides the detector to achieve global representation and enhance inter-class distinguishability of instance-level features across the entire dataset, which spans both domains, by leveraging contrastive learning. Moreover, DATR incorporates a mean-teacher based self-training framework, utilizing pseudo-labels generated by the teacher model to further mitigate domain bias. Extensive experimental results demonstrate superior performance and generalization capabilities of our proposed DATR in multiple domain adaptation scenarios. Code is released at https://github.com/h751410234/DATR.","sentences":["Object detectors frequently encounter significant performance degradation when confronted with domain gaps between collected data (source domain) and data from real-world applications (target domain).","To address this task, numerous unsupervised domain adaptive detectors have been proposed, leveraging carefully designed feature alignment techniques.","However, these techniques primarily align instance-level features in a class-agnostic manner, overlooking the differences between extracted features from different categories, which results in only limited improvement.","Furthermore, the scope of current alignment modules is often restricted to a limited batch of images, failing to learn the entire dataset-level cues, thereby severely constraining the detector's generalization ability to the target domain.","To this end, we introduce a strong DETR-based detector named Domain Adaptive detection TRansformer (DATR) for unsupervised domain adaptation of object detection.","Firstly, we propose the Class-wise Prototypes Alignment (CPA) module, which effectively aligns cross-domain features in a class-aware manner by bridging the gap between object detection task and domain adaptation task.","Then, the designed Dataset-level Alignment Scheme (DAS) explicitly guides the detector to achieve global representation and enhance inter-class distinguishability of instance-level features across the entire dataset, which spans both domains, by leveraging contrastive learning.","Moreover, DATR incorporates a mean-teacher based self-training framework, utilizing pseudo-labels generated by the teacher model to further mitigate domain bias.","Extensive experimental results demonstrate superior performance and generalization capabilities of our proposed DATR in multiple domain adaptation scenarios.","Code is released at https://github.com/h751410234/DATR."],"url":"http://arxiv.org/abs/2405.11765v1","category":"cs.CV"}
{"created":"2024-05-20 03:35:13","title":"Fed-Credit: Robust Federated Learning with Credibility Management","abstract":"Aiming at privacy preservation, Federated Learning (FL) is an emerging machine learning approach enabling model training on decentralized devices or data sources. The learning mechanism of FL relies on aggregating parameter updates from individual clients. However, this process may pose a potential security risk due to the presence of malicious devices. Existing solutions are either costly due to the use of compute-intensive technology, or restrictive for reasons of strong assumptions such as the prior knowledge of the number of attackers and how they attack. Few methods consider both privacy constraints and uncertain attack scenarios. In this paper, we propose a robust FL approach based on the credibility management scheme, called Fed-Credit. Unlike previous studies, our approach does not require prior knowledge of the nodes and the data distribution. It maintains and employs a credibility set, which weighs the historical clients' contributions based on the similarity between the local models and global model, to adjust the global model update. The subtlety of Fed-Credit is that the time decay and attitudinal value factor are incorporated into the dynamic adjustment of the reputation weights and it boasts a computational complexity of O(n) (n is the number of the clients). We conducted extensive experiments on the MNIST and CIFAR-10 datasets under 5 types of attacks. The results exhibit superior accuracy and resilience against adversarial attacks, all while maintaining comparatively low computational complexity. Among these, on the Non-IID CIFAR-10 dataset, our algorithm exhibited performance enhancements of 19.5% and 14.5%, respectively, in comparison to the state-of-the-art algorithm when dealing with two types of data poisoning attacks.","sentences":["Aiming at privacy preservation, Federated Learning (FL) is an emerging machine learning approach enabling model training on decentralized devices or data sources.","The learning mechanism of FL relies on aggregating parameter updates from individual clients.","However, this process may pose a potential security risk due to the presence of malicious devices.","Existing solutions are either costly due to the use of compute-intensive technology, or restrictive for reasons of strong assumptions such as the prior knowledge of the number of attackers and how they attack.","Few methods consider both privacy constraints and uncertain attack scenarios.","In this paper, we propose a robust FL approach based on the credibility management scheme, called Fed-Credit.","Unlike previous studies, our approach does not require prior knowledge of the nodes and the data distribution.","It maintains and employs a credibility set, which weighs the historical clients' contributions based on the similarity between the local models and global model, to adjust the global model update.","The subtlety of Fed-Credit is that the time decay and attitudinal value factor are incorporated into the dynamic adjustment of the reputation weights and it boasts a computational complexity of O(n) (n is the number of the clients).","We conducted extensive experiments on the MNIST and CIFAR-10 datasets under 5 types of attacks.","The results exhibit superior accuracy and resilience against adversarial attacks, all while maintaining comparatively low computational complexity.","Among these, on the Non-IID CIFAR-10 dataset, our algorithm exhibited performance enhancements of 19.5% and 14.5%, respectively, in comparison to the state-of-the-art algorithm when dealing with two types of data poisoning attacks."],"url":"http://arxiv.org/abs/2405.11758v1","category":"cs.LG"}
{"created":"2024-05-20 03:10:22","title":"Configurable Mirror Descent: Towards a Unification of Decision Making","abstract":"Decision-making problems, categorized as single-agent, e.g., Atari, cooperative multi-agent, e.g., Hanabi, competitive multi-agent, e.g., Hold'em poker, and mixed cooperative and competitive, e.g., football, are ubiquitous in the real world. Various methods are proposed to address the specific decision-making problems. Despite the successes in specific categories, these methods typically evolve independently and cannot generalize to other categories. Therefore, a fundamental question for decision-making is: \\emph{Can we develop \\textbf{a single algorithm} to tackle \\textbf{ALL} categories of decision-making problems?} There are several main challenges to address this question: i) different decision-making categories involve different numbers of agents and different relationships between agents, ii) different categories have different solution concepts and evaluation measures, and iii) there lacks a comprehensive benchmark covering all the categories. This work presents a preliminary attempt to address the question with three main contributions. i) We propose the generalized mirror descent (GMD), a generalization of MD variants, which considers multiple historical policies and works with a broader class of Bregman divergences. ii) We propose the configurable mirror descent (CMD) where a meta-controller is introduced to dynamically adjust the hyper-parameters in GMD conditional on the evaluation measures. iii) We construct the \\textsc{GameBench} with 15 academic-friendly games across different decision-making categories. Extensive experiments demonstrate that CMD achieves empirically competitive or better outcomes compared to baselines while providing the capability of exploring diverse dimensions of decision making.","sentences":["Decision-making problems, categorized as single-agent, e.g., Atari, cooperative multi-agent, e.g., Hanabi, competitive multi-agent, e.g., Hold'em poker, and mixed cooperative and competitive, e.g., football, are ubiquitous in the real world.","Various methods are proposed to address the specific decision-making problems.","Despite the successes in specific categories, these methods typically evolve independently and cannot generalize to other categories.","Therefore, a fundamental question for decision-making is: \\emph{Can we develop \\textbf{a single algorithm} to tackle \\textbf{ALL} categories of decision-making problems?}","There are several main challenges to address this question: i) different decision-making categories involve different numbers of agents and different relationships between agents, ii) different categories have different solution concepts and evaluation measures, and iii) there lacks a comprehensive benchmark covering all the categories.","This work presents a preliminary attempt to address the question with three main contributions.","i)","We propose the generalized mirror descent (GMD), a generalization of MD variants, which considers multiple historical policies and works with a broader class of Bregman divergences.","ii)","We propose the configurable mirror descent (CMD) where a meta-controller is introduced to dynamically adjust the hyper-parameters in GMD conditional on the evaluation measures.","iii) We construct the \\textsc{GameBench} with 15 academic-friendly games across different decision-making categories.","Extensive experiments demonstrate that CMD achieves empirically competitive or better outcomes compared to baselines while providing the capability of exploring diverse dimensions of decision making."],"url":"http://arxiv.org/abs/2405.11746v1","category":"cs.AI"}
{"created":"2024-05-20 02:43:04","title":"Learning Future Representation with Synthetic Observations for Sample-efficient Reinforcement Learning","abstract":"In visual Reinforcement Learning (RL), upstream representation learning largely determines the effect of downstream policy learning. Employing auxiliary tasks allows the agent to enhance visual representation in a targeted manner, thereby improving the sample efficiency and performance of downstream RL. Prior advanced auxiliary tasks all focus on how to extract as much information as possible from limited experience (including observations, actions, and rewards) through their different auxiliary objectives, whereas in this article, we first start from another perspective: auxiliary training data. We try to improve auxiliary representation learning for RL by enriching auxiliary training data, proposing \\textbf{L}earning \\textbf{F}uture representation with \\textbf{S}ynthetic observations \\textbf{(LFS)}, a novel self-supervised RL approach. Specifically, we propose a training-free method to synthesize observations that may contain future information, as well as a data selection approach to eliminate unqualified synthetic noise. The remaining synthetic observations and real observations then serve as the auxiliary data to achieve a clustering-based temporal association task for representation learning. LFS allows the agent to access and learn observations that have not yet appeared in advance, so as to quickly understand and exploit them when they occur later. In addition, LFS does not rely on rewards or actions, which means it has a wider scope of application (e.g., learning from video) than recent advanced auxiliary tasks. Extensive experiments demonstrate that our LFS exhibits state-of-the-art RL sample efficiency on challenging continuous control and enables advanced visual pre-training based on action-free video demonstrations.","sentences":["In visual Reinforcement Learning (RL), upstream representation learning largely determines the effect of downstream policy learning.","Employing auxiliary tasks allows the agent to enhance visual representation in a targeted manner, thereby improving the sample efficiency and performance of downstream RL.","Prior advanced auxiliary tasks all focus on how to extract as much information as possible from limited experience (including observations, actions, and rewards) through their different auxiliary objectives, whereas in this article, we first start from another perspective: auxiliary training data.","We try to improve auxiliary representation learning for RL by enriching auxiliary training data, proposing \\textbf{L}earning \\textbf{F}uture representation with \\textbf{S}ynthetic observations \\textbf{(LFS)}, a novel self-supervised RL approach.","Specifically, we propose a training-free method to synthesize observations that may contain future information, as well as a data selection approach to eliminate unqualified synthetic noise.","The remaining synthetic observations and real observations then serve as the auxiliary data to achieve a clustering-based temporal association task for representation learning.","LFS allows the agent to access and learn observations that have not yet appeared in advance, so as to quickly understand and exploit them when they occur later.","In addition, LFS does not rely on rewards or actions, which means it has a wider scope of application (e.g., learning from video) than recent advanced auxiliary tasks.","Extensive experiments demonstrate that our LFS exhibits state-of-the-art RL sample efficiency on challenging continuous control and enables advanced visual pre-training based on action-free video demonstrations."],"url":"http://arxiv.org/abs/2405.11740v1","category":"cs.LG"}
{"created":"2024-05-20 02:41:21","title":"Contactless Polysomnography: What Radio Waves Tell Us about Sleep","abstract":"The ability to assess sleep at home, capture sleep stages, and detect the occurrence of apnea (without on-body sensors) simply by analyzing the radio waves bouncing off people's bodies while they sleep is quite powerful. Such a capability would allow for longitudinal data collection in patients' homes, informing our understanding of sleep and its interaction with various diseases and their therapeutic responses, both in clinical trials and routine care. In this article, we develop an advanced machine learning algorithm for passively monitoring sleep and nocturnal breathing from radio waves reflected off people while asleep. Validation results in comparison with the gold standard (i.e., polysomnography) (n=849) demonstrate that the model captures the sleep hypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake, Light Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and measures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI = [0.93, 0.97]). Notably, the model exhibits equitable performance across race, sex, and age. Moreover, the model uncovers informative interactions between sleep stages and a range of diseases including neurological, psychiatric, cardiovascular, and immunological disorders. These findings not only hold promise for clinical practice and interventional trials but also underscore the significance of sleep as a fundamental component in understanding and managing various diseases.","sentences":["The ability to assess sleep at home, capture sleep stages, and detect the occurrence of apnea (without on-body sensors) simply by analyzing the radio waves bouncing off people's bodies while they sleep is quite powerful.","Such a capability would allow for longitudinal data collection in patients' homes, informing our understanding of sleep and its interaction with various diseases and their therapeutic responses, both in clinical trials and routine care.","In this article, we develop an advanced machine learning algorithm for passively monitoring sleep and nocturnal breathing from radio waves reflected off people while asleep.","Validation results in comparison with the gold standard (i.e., polysomnography) (n=849) demonstrate that the model captures the sleep hypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake, Light Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and measures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI =","[0.93, 0.97]).","Notably, the model exhibits equitable performance across race, sex, and age.","Moreover, the model uncovers informative interactions between sleep stages and a range of diseases including neurological, psychiatric, cardiovascular, and immunological disorders.","These findings not only hold promise for clinical practice and interventional trials but also underscore the significance of sleep as a fundamental component in understanding and managing various diseases."],"url":"http://arxiv.org/abs/2405.11739v1","category":"cs.LG"}
{"created":"2024-05-20 02:24:58","title":"Enhanced Deterministic Approximation Algorithm for Non-monotone Submodular Maximization under Knapsack Constraint with Linear Query Complexity","abstract":"In this work, we consider the Submodular Maximization under Knapsack (SMK) constraint problem over the ground set of size $n$. The problem recently attracted a lot of attention due to its applications in various domains of combination optimization, artificial intelligence, and machine learning. We improve the approximation factor of the fastest deterministic algorithm from $6+\\epsilon$ to $5+\\epsilon$ while keeping the best query complexity of $O(n)$, where $\\epsilon >0$ is a constant parameter. Our technique is based on optimizing the performance of two components: the threshold greedy subroutine and the building of two disjoint sets as candidate solutions. Besides, by carefully analyzing the cost of candidate solutions, we obtain a tighter approximation factor.","sentences":["In this work, we consider the Submodular Maximization under Knapsack (SMK) constraint problem over the ground set of size $n$. The problem recently attracted a lot of attention due to its applications in various domains of combination optimization, artificial intelligence, and machine learning.","We improve the approximation factor of the fastest deterministic algorithm from $6+\\epsilon$ to $5+\\epsilon$ while keeping the best query complexity of $O(n)$, where $\\epsilon >0$ is a constant parameter.","Our technique is based on optimizing the performance of two components: the threshold greedy subroutine and the building of two disjoint sets as candidate solutions.","Besides, by carefully analyzing the cost of candidate solutions, we obtain a tighter approximation factor."],"url":"http://arxiv.org/abs/2405.12252v1","category":"cs.DS"}
{"created":"2024-05-20 01:57:34","title":"Token-wise Influential Training Data Retrieval for Large Language Models","abstract":"Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation? In this paper, we proposed RapidIn, a scalable framework adapting to LLMs for estimating the influence of each training data. The proposed framework consists of two stages: caching and retrieval. First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory. Then, given a generation, RapidIn efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports multi-GPU parallelization to substantially accelerate caching and retrieval. Our empirical result confirms the efficiency and effectiveness of RapidIn.","sentences":["Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation?","In this paper, we proposed RapidIn, a scalable framework adapting to LLMs for estimating the influence of each training data.","The proposed framework consists of two stages: caching and retrieval.","First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory.","Then, given a generation, RapidIn efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup.","Moreover, RapidIn supports multi-GPU parallelization to substantially accelerate caching and retrieval.","Our empirical result confirms the efficiency and effectiveness of RapidIn."],"url":"http://arxiv.org/abs/2405.11724v1","category":"cs.CL"}
{"created":"2024-05-20 01:47:28","title":"AI Algorithm for Predicting and Optimizing Trajectory of UAV Swarm","abstract":"This paper explores the application of Artificial Intelligence (AI) techniques for generating the trajectories of fleets of Unmanned Aerial Vehicles (UAVs). The two main challenges addressed include accurately predicting the paths of UAVs and efficiently avoiding collisions between them. Firstly, the paper systematically applies a diverse set of activation functions to a Feedforward Neural Network (FFNN) with a single hidden layer, which enhances the accuracy of the predicted path compared to previous work.   Secondly, we introduce a novel activation function, AdaptoSwelliGauss, which is a sophisticated fusion of Swish and Elliott activations, seamlessly integrated with a scaled and shifted Gaussian component. Swish facilitates smooth transitions, Elliott captures abrupt trajectory changes, and the scaled and shifted Gaussian enhances robustness against noise. This dynamic combination is specifically designed to excel in capturing the complexities of UAV trajectory prediction. This new activation function gives substantially better accuracy than all existing activation functions.   Thirdly, we propose a novel Integrated Collision Detection, Avoidance, and Batching (ICDAB) strategy that merges two complementary UAV collision avoidance techniques: changing UAV trajectories and altering their starting times, also referred to as batching. This integration helps overcome the disadvantages of both - reduction in the number of trajectory manipulations, which avoids overly convoluted paths in the first technique, and smaller batch sizes, which reduce overall takeoff time in the second.","sentences":["This paper explores the application of Artificial Intelligence (AI) techniques for generating the trajectories of fleets of Unmanned Aerial Vehicles (UAVs).","The two main challenges addressed include accurately predicting the paths of UAVs and efficiently avoiding collisions between them.","Firstly, the paper systematically applies a diverse set of activation functions to a Feedforward Neural Network (FFNN) with a single hidden layer, which enhances the accuracy of the predicted path compared to previous work.   ","Secondly, we introduce a novel activation function, AdaptoSwelliGauss, which is a sophisticated fusion of Swish and Elliott activations, seamlessly integrated with a scaled and shifted Gaussian component.","Swish facilitates smooth transitions, Elliott captures abrupt trajectory changes, and the scaled and shifted Gaussian enhances robustness against noise.","This dynamic combination is specifically designed to excel in capturing the complexities of UAV trajectory prediction.","This new activation function gives substantially better accuracy than all existing activation functions.   ","Thirdly, we propose a novel Integrated Collision Detection, Avoidance, and Batching (ICDAB) strategy that merges two complementary UAV collision avoidance techniques: changing UAV trajectories and altering their starting times, also referred to as batching.","This integration helps overcome the disadvantages of both - reduction in the number of trajectory manipulations, which avoids overly convoluted paths in the first technique, and smaller batch sizes, which reduce overall takeoff time in the second."],"url":"http://arxiv.org/abs/2405.11722v1","category":"cs.RO"}
{"created":"2024-05-20 01:29:45","title":"Semantic Trajectory Data Mining with LLM-Informed POI Classification","abstract":"Human travel trajectory mining is crucial for transportation systems, enhancing route optimization, traffic management, and the study of human travel patterns. Previous rule-based approaches without the integration of semantic information show a limitation in both efficiency and accuracy. Semantic information, such as activity types inferred from Points of Interest (POI) data, can significantly enhance the quality of trajectory mining. However, integrating these insights is challenging, as many POIs have incomplete feature information, and current learning-based POI algorithms require the integrity of datasets to do the classification. In this paper, we introduce a novel pipeline for human travel trajectory mining. Our approach first leverages the strong inferential and comprehension capabilities of large language models (LLMs) to annotate POI with activity types and then uses a Bayesian-based algorithm to infer activity for each stay point in a trajectory. In our evaluation using the OpenStreetMap (OSM) POI dataset, our approach achieves a 93.4% accuracy and a 96.1% F-1 score in POI classification, and a 91.7% accuracy with a 92.3% F-1 score in activity inference.","sentences":["Human travel trajectory mining is crucial for transportation systems, enhancing route optimization, traffic management, and the study of human travel patterns.","Previous rule-based approaches without the integration of semantic information show a limitation in both efficiency and accuracy.","Semantic information, such as activity types inferred from Points of Interest (POI) data, can significantly enhance the quality of trajectory mining.","However, integrating these insights is challenging, as many POIs have incomplete feature information, and current learning-based POI algorithms require the integrity of datasets to do the classification.","In this paper, we introduce a novel pipeline for human travel trajectory mining.","Our approach first leverages the strong inferential and comprehension capabilities of large language models (LLMs) to annotate POI with activity types and then uses a Bayesian-based algorithm to infer activity for each stay point in a trajectory.","In our evaluation using the OpenStreetMap (OSM) POI dataset, our approach achieves a 93.4% accuracy and a 96.1% F-1 score in POI classification, and a 91.7% accuracy with a 92.3% F-1 score in activity inference."],"url":"http://arxiv.org/abs/2405.11715v1","category":"cs.AI"}
{"created":"2024-05-20 01:22:21","title":"Decentralized Privacy Preservation for Critical Connections in Graphs","abstract":"Many real-world interconnections among entities can be characterized as graphs. Collecting local graph information with balanced privacy and data utility has garnered notable interest recently. This paper delves into the problem of identifying and protecting critical information of entity connections for individual participants in a graph based on cohesive subgraph searches. This problem has not been addressed in the literature. To address the problem, we propose to extract the critical connections of a queried vertex using a fortress-like cohesive subgraph model known as $p$-cohesion. A user's connections within a fortress are obfuscated when being released, to protect critical information about the user. Novel merit and penalty score functions are designed to measure each participant's critical connections in the minimal $p$-cohesion, facilitating effective identification of the connections. We further propose to preserve the privacy of a vertex enquired by only protecting its critical connections when responding to queries raised by data collectors. We prove that, under the decentralized differential privacy (DDP) mechanism, one's response satisfies $(\\varepsilon, \\delta)$-DDP when its critical connections are protected while the rest remains unperturbed. The effectiveness of our proposed method is demonstrated through extensive experiments on real-life graph datasets.","sentences":["Many real-world interconnections among entities can be characterized as graphs.","Collecting local graph information with balanced privacy and data utility has garnered notable interest recently.","This paper delves into the problem of identifying and protecting critical information of entity connections for individual participants in a graph based on cohesive subgraph searches.","This problem has not been addressed in the literature.","To address the problem, we propose to extract the critical connections of a queried vertex using a fortress-like cohesive subgraph model known as $p$-cohesion.","A user's connections within a fortress are obfuscated when being released, to protect critical information about the user.","Novel merit and penalty score functions are designed to measure each participant's critical connections in the minimal $p$-cohesion, facilitating effective identification of the connections.","We further propose to preserve the privacy of a vertex enquired by only protecting its critical connections when responding to queries raised by data collectors.","We prove that, under the decentralized differential privacy (DDP) mechanism, one's response satisfies $(\\varepsilon, \\delta)$-DDP when its critical connections are protected while the rest remains unperturbed.","The effectiveness of our proposed method is demonstrated through extensive experiments on real-life graph datasets."],"url":"http://arxiv.org/abs/2405.11713v1","category":"cs.CR"}
{"created":"2024-05-20 01:15:57","title":"Trust, Because You Can't Verify:Privacy and Security Hurdles in Education Technology Acquisition Practices","abstract":"The education technology (EdTech) landscape is expanding rapidly in higher education institutes (HEIs). This growth brings enormous complexity. Protecting the extensive data collected by these tools is crucial for HEIs. Privacy incidents of data breaches and misuses can have dire security and privacy consequences on the data subjects, particularly students, who are often compelled to use these tools. This urges an in-depth understanding of HEI and EdTech vendor dynamics, which is largely understudied.   To address this gap, we conduct a semi-structured interview study with 13 participants who are in the EdTech leadership roles at seven HEIs. Our study uncovers the EdTech acquisition process in the HEI context, the consideration of security and privacy issues throughout that process, the pain points of HEI personnel in establishing adequate security and privacy protection mechanisms in service contracts, and their struggle in holding vendors accountable due to a lack of visibility into their system and power-asymmetry, among other reasons. We discuss certain observations about the status quo and conclude with recommendations to improve the situation.","sentences":["The education technology (EdTech) landscape is expanding rapidly in higher education institutes (HEIs).","This growth brings enormous complexity.","Protecting the extensive data collected by these tools is crucial for HEIs.","Privacy incidents of data breaches and misuses can have dire security and privacy consequences on the data subjects, particularly students, who are often compelled to use these tools.","This urges an in-depth understanding of HEI and EdTech vendor dynamics, which is largely understudied.   ","To address this gap, we conduct a semi-structured interview study with 13 participants who are in the EdTech leadership roles at seven HEIs.","Our study uncovers the EdTech acquisition process in the HEI context, the consideration of security and privacy issues throughout that process, the pain points of HEI personnel in establishing adequate security and privacy protection mechanisms in service contracts, and their struggle in holding vendors accountable due to a lack of visibility into their system and power-asymmetry, among other reasons.","We discuss certain observations about the status quo and conclude with recommendations to improve the situation."],"url":"http://arxiv.org/abs/2405.11712v1","category":"cs.CY"}
{"created":"2024-05-20 01:04:40","title":"OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework","abstract":"As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance. However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models. We present OpenRLHF, an open-source framework enabling efficient RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques. Empowering state-of-the-art LLM development, OpenRLHF's code is available at https://github.com/OpenLLMAI/OpenRLHF.","sentences":["As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance.","However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models.","We present OpenRLHF, an open-source framework enabling efficient RLHF scaling.","Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches.","Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness.","OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques.","Empowering state-of-the-art LLM development, OpenRLHF's code is available at https://github.com/OpenLLMAI/OpenRLHF."],"url":"http://arxiv.org/abs/2405.11143v1","category":"cs.AI"}
{"created":"2024-05-20 00:28:00","title":"Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!","abstract":"There is increasing evidence that question-answering (QA) systems with Large Language Models (LLMs), which employ a knowledge graph/semantic representation of an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy compared to systems that answer questions directly on SQL databases (i.e. Text-to-SQL). Our previous benchmark research showed that by using a knowledge graph, the accuracy improved from 16% to 54%. The question remains: how can we further improve the accuracy and reduce the error rate? Building on the observations of our previous research where the inaccurate LLM-generated SPARQL queries followed incorrect paths, we present an approach that consists of 1) Ontology-based Query Check (OBQC): detects errors by leveraging the ontology of the knowledge graph to check if the LLM-generated SPARQL query matches the semantic of ontology and 2) LLM Repair: use the error explanations with an LLM to repair the SPARQL query. Using the chat with the data benchmark, our primary finding is that our approach increases the overall accuracy to 72% including an additional 8% of \"I don't know\" unknown results. Thus, the overall error rate is 20%. These results provide further evidence that investing knowledge graphs, namely the ontology, provides higher accuracy for LLM powered question answering systems.","sentences":["There is increasing evidence that question-answering (QA) systems with Large Language Models (LLMs), which employ a knowledge graph/semantic representation of an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy compared to systems that answer questions directly on SQL databases (i.e. Text-to-SQL).","Our previous benchmark research showed that by using a knowledge graph, the accuracy improved from 16% to 54%.","The question remains: how can we further improve the accuracy and reduce the error rate?","Building on the observations of our previous research where the inaccurate LLM-generated SPARQL queries followed incorrect paths, we present an approach that consists of 1) Ontology-based Query Check (OBQC):","detects errors by leveraging the ontology of the knowledge graph to check if the LLM-generated SPARQL query matches the semantic of ontology and 2) LLM Repair: use the error explanations with an LLM to repair the SPARQL query.","Using the chat with the data benchmark, our primary finding is that our approach increases the overall accuracy to 72% including an additional 8% of \"I don't know\" unknown results.","Thus, the overall error rate is 20%.","These results provide further evidence that investing knowledge graphs, namely the ontology, provides higher accuracy for LLM powered question answering systems."],"url":"http://arxiv.org/abs/2405.11706v1","category":"cs.AI"}
{"created":"2024-05-20 00:10:00","title":"Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks","abstract":"The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.","sentences":["The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies.","Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint.","By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice.","In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation.","By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy.","In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research.","In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models."],"url":"http://arxiv.org/abs/2405.11704v1","category":"cs.LG"}
{"created":"2024-05-19 23:08:15","title":"Multi-Objective Learning Model Predictive Control","abstract":"Multi-Objective Learning Model Predictive Control is a novel data-driven control scheme which improves a system's closed-loop performance with respect to several control objectives over iterations of a repeated task. At each task iteration, collected system data is used to construct terminal components of a Model Predictive Controller. The formulation presented in this paper ensures that closed-loop control performance improves between successive iterations with respect to each objective. We provide proofs of recursive feasibility and performance improvement, and show that the converged policy is Pareto optimal. Simulation results demonstrate the applicability of the proposed approach.","sentences":["Multi-Objective Learning Model Predictive Control is a novel data-driven control scheme which improves a system's closed-loop performance with respect to several control objectives over iterations of a repeated task.","At each task iteration, collected system data is used to construct terminal components of a Model Predictive Controller.","The formulation presented in this paper ensures that closed-loop control performance improves between successive iterations with respect to each objective.","We provide proofs of recursive feasibility and performance improvement, and show that the converged policy is Pareto optimal.","Simulation results demonstrate the applicability of the proposed approach."],"url":"http://arxiv.org/abs/2405.11698v1","category":"eess.SY"}
{"created":"2024-05-19 23:05:53","title":"AMMeBa: A Large-Scale Survey and Dataset of Media-Based Misinformation In-The-Wild","abstract":"The prevalence and harms of online misinformation is a perennial concern for internet platforms, institutions and society at large. Over time, information shared online has become more media-heavy and misinformation has readily adapted to these new modalities. The rise of generative AI-based tools, which provide widely-accessible methods for synthesizing realistic audio, images, video and human-like text, have amplified these concerns. Despite intense public interest and significant press coverage, quantitative information on the prevalence and modality of media-based misinformation remains scarce. Here, we present the results of a two-year study using human raters to annotate online media-based misinformation, mostly focusing on images, based on claims assessed in a large sample of publicly-accessible fact checks with the ClaimReview markup. We present an image typology, designed to capture aspects of the image and manipulation relevant to the image's role in the misinformation claim. We visualize the distribution of these types over time. We show the rise of generative AI-based content in misinformation claims, and that its commonality is a relatively recent phenomenon, occurring significantly after heavy press coverage. We also show \"simple\" methods dominated historically, particularly context manipulations, and continued to hold a majority as of the end of data collection in November 2023. The dataset, Annotated Misinformation, Media-Based (AMMeBa), is publicly-available, and we hope that these data will serve as both a means of evaluating mitigation methods in a realistic setting and as a first-of-its-kind census of the types and modalities of online misinformation.","sentences":["The prevalence and harms of online misinformation is a perennial concern for internet platforms, institutions and society at large.","Over time, information shared online has become more media-heavy and misinformation has readily adapted to these new modalities.","The rise of generative AI-based tools, which provide widely-accessible methods for synthesizing realistic audio, images, video and human-like text, have amplified these concerns.","Despite intense public interest and significant press coverage, quantitative information on the prevalence and modality of media-based misinformation remains scarce.","Here, we present the results of a two-year study using human raters to annotate online media-based misinformation, mostly focusing on images, based on claims assessed in a large sample of publicly-accessible fact checks with the ClaimReview markup.","We present an image typology, designed to capture aspects of the image and manipulation relevant to the image's role in the misinformation claim.","We visualize the distribution of these types over time.","We show the rise of generative AI-based content in misinformation claims, and that its commonality is a relatively recent phenomenon, occurring significantly after heavy press coverage.","We also show \"simple\" methods dominated historically, particularly context manipulations, and continued to hold a majority as of the end of data collection in November 2023.","The dataset, Annotated Misinformation, Media-Based (AMMeBa), is publicly-available, and we hope that these data will serve as both a means of evaluating mitigation methods in a realistic setting and as a first-of-its-kind census of the types and modalities of online misinformation."],"url":"http://arxiv.org/abs/2405.11697v2","category":"cs.CY"}
{"created":"2024-05-19 22:51:22","title":"PBI: Position-Based Dynamics Handles Updated Lagrangian Inelasticity","abstract":"Position-based Dynamics (PBD) and its extension, eXtended Position-based Dynamics (XPBD), have been predominantly applied to compliant constrained dynamics, with their potential in finite strain inelasticity remaining underexplored. XPBD stands in contrast to other meshless methods, such as the Material Point Method (MPM). MPM is based on discretizing the weak form of governing partial differential equations within a continuum domain, coupled with a hybrid Lagrangian-Eulerian method for tracking deformation gradients. In contrast, XPBD generally entails applying specific constraints, whether hard or compliant, to collections of point masses. This paper revisits this perception, investigating the potential of XPBD in handling inelastic materials that are described with continuum mechanics based yield surfaces and elastoplastic flow rules. Our inspiration is that a robust estimation of the velocity gradient is key to effectively tracking deformation gradients in any meshless context. By further incorporating implicit inelastic constitutive relationships, we introduce an updated Lagrangian augmentation to XPBD. This enhancement enables the simulation of elastoplastic, viscoplastic, and granular substances following their standard constitutive laws. We demonstrate the effectiveness of our method through high-resolution and real-time simulations of diverse materials such as snow, sand, and plasticine, and its integration with standard XPBD simulations of cloth and water.","sentences":["Position-based Dynamics (PBD) and its extension, eXtended Position-based Dynamics (XPBD), have been predominantly applied to compliant constrained dynamics, with their potential in finite strain inelasticity remaining underexplored.","XPBD stands in contrast to other meshless methods, such as the Material Point Method (MPM).","MPM is based on discretizing the weak form of governing partial differential equations within a continuum domain, coupled with a hybrid Lagrangian-Eulerian method for tracking deformation gradients.","In contrast, XPBD generally entails applying specific constraints, whether hard or compliant, to collections of point masses.","This paper revisits this perception, investigating the potential of XPBD in handling inelastic materials that are described with continuum mechanics based yield surfaces and elastoplastic flow rules.","Our inspiration is that a robust estimation of the velocity gradient is key to effectively tracking deformation gradients in any meshless context.","By further incorporating implicit inelastic constitutive relationships, we introduce an updated Lagrangian augmentation to XPBD.","This enhancement enables the simulation of elastoplastic, viscoplastic, and granular substances following their standard constitutive laws.","We demonstrate the effectiveness of our method through high-resolution and real-time simulations of diverse materials such as snow, sand, and plasticine, and its integration with standard XPBD simulations of cloth and water."],"url":"http://arxiv.org/abs/2405.11694v1","category":"cs.GR"}
{"created":"2024-05-19 22:44:00","title":"Your Transformer is Secretly Linear","abstract":"This paper reveals a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer. Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance. Moreover, in our pretraining experiments on smaller models we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE and as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.","sentences":["This paper reveals a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM and others.","We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99).","However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer.","Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance.","Moreover, in our pretraining experiments on smaller models we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity.","This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE and as well successfully decreases the linearity of the models.","This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed."],"url":"http://arxiv.org/abs/2405.12250v1","category":"cs.LG"}
{"created":"2024-05-19 21:26:11","title":"Deep Ensemble Art Style Recognition","abstract":"The massive digitization of artworks during the last decades created the need for categorization, analysis, and management of huge amounts of data related to abstract concepts, highlighting a challenging problem in the field of computer science. The rapid progress of artificial intelligence and neural networks has provided tools and technologies that seem worthy of the challenge. Recognition of various art features in artworks has gained attention in the deep learning society. In this paper, we are concerned with the problem of art style recognition using deep networks. We compare the performance of 8 different deep architectures (VGG16, VGG19, ResNet50, ResNet152, Inception-V3, DenseNet121, DenseNet201 and Inception-ResNet-V2), on two different art datasets, including 3 architectures that have never been used on this task before, leading to state-of-the-art performance. We study the effect of data preprocessing prior to applying a deep learning model. We introduce a stacking ensemble method combining the results of first-stage classifiers through a meta-classifier, with the innovation of a versatile approach based on multiple models that extract and recognize different characteristics of the input, creating a more consistent model compared to existing works and achieving state-of-the-art accuracy on the largest art dataset available (WikiArt - 68,55%). We also discuss the impact of the data and art styles themselves on the performance of our models forming a manifold perspective on the problem.","sentences":["The massive digitization of artworks during the last decades created the need for categorization, analysis, and management of huge amounts of data related to abstract concepts, highlighting a challenging problem in the field of computer science.","The rapid progress of artificial intelligence and neural networks has provided tools and technologies that seem worthy of the challenge.","Recognition of various art features in artworks has gained attention in the deep learning society.","In this paper, we are concerned with the problem of art style recognition using deep networks.","We compare the performance of 8 different deep architectures (VGG16, VGG19, ResNet50, ResNet152, Inception-V3, DenseNet121, DenseNet201 and Inception-ResNet-V2), on two different art datasets, including 3 architectures that have never been used on this task before, leading to state-of-the-art performance.","We study the effect of data preprocessing prior to applying a deep learning model.","We introduce a stacking ensemble method combining the results of first-stage classifiers through a meta-classifier, with the innovation of a versatile approach based on multiple models that extract and recognize different characteristics of the input, creating a more consistent model compared to existing works and achieving state-of-the-art accuracy on the largest art dataset available (WikiArt - 68,55%).","We also discuss the impact of the data and art styles themselves on the performance of our models forming a manifold perspective on the problem."],"url":"http://arxiv.org/abs/2405.11675v1","category":"cs.CV"}
{"created":"2024-05-19 20:33:21","title":"Do No Harm: A Counterfactual Approach to Safe Reinforcement Learning","abstract":"Reinforcement Learning (RL) for control has become increasingly popular due to its ability to learn rich feedback policies that take into account uncertainty and complex representations of the environment. When considering safety constraints, constrained optimization approaches, where agents are penalized for constraint violations, are commonly used. In such methods, if agents are initialized in, or must visit, states where constraint violation might be inevitable, it is unclear how much they should be penalized. We address this challenge by formulating a constraint on the counterfactual harm of the learned policy compared to a default, safe policy. In a philosophical sense this formulation only penalizes the learner for constraint violations that it caused; in a practical sense it maintains feasibility of the optimal control problem. We present simulation studies on a rover with uncertain road friction and a tractor-trailer parking environment that demonstrate our constraint formulation enables agents to learn safer policies than contemporary constrained RL methods.","sentences":["Reinforcement Learning (RL) for control has become increasingly popular due to its ability to learn rich feedback policies that take into account uncertainty and complex representations of the environment.","When considering safety constraints, constrained optimization approaches, where agents are penalized for constraint violations, are commonly used.","In such methods, if agents are initialized in, or must visit, states where constraint violation might be inevitable, it is unclear how much they should be penalized.","We address this challenge by formulating a constraint on the counterfactual harm of the learned policy compared to a default, safe policy.","In a philosophical sense this formulation only penalizes the learner for constraint violations that it caused; in a practical sense it maintains feasibility of the optimal control problem.","We present simulation studies on a rover with uncertain road friction and a tractor-trailer parking environment that demonstrate our constraint formulation enables agents to learn safer policies than contemporary constrained RL methods."],"url":"http://arxiv.org/abs/2405.11669v1","category":"cs.LG"}
{"created":"2024-05-19 20:06:38","title":"On the Expressivity of Recurrent Neural Cascades with Identity","abstract":"Recurrent Neural Cascades (RNC) are the class of recurrent neural networks with no cyclic dependencies among recurrent neurons. Their subclass RNC+ with positive recurrent weights has been shown to be closely connected to the star-free regular languages, which are the expressivity of many well-established temporal logics. The existing expressivity results show that the regular languages captured by RNC+ are the star-free ones, and they leave open the possibility that RNC+ may capture languages beyond regular. We exclude this possibility for languages that include an identity element, i.e., an input that can occur an arbitrary number of times without affecting the output. Namely, in the presence of an identity element, we show that the languages captured by RNC+ are exactly the star-free regular languages. Identity elements are ubiquitous in temporal patterns, and hence our results apply to a large number of applications. The implications of our results go beyond expressivity. At their core, we establish a close structural correspondence between RNC+ and semiautomata cascades, showing that every neuron can be equivalently captured by a three-state semiautomaton. A notable consequence of this result is that RNC+ are no more succinct than cascades of three-state semiautomata.","sentences":["Recurrent Neural Cascades (RNC) are the class of recurrent neural networks with no cyclic dependencies among recurrent neurons.","Their subclass RNC+ with positive recurrent weights has been shown to be closely connected to the star-free regular languages, which are the expressivity of many well-established temporal logics.","The existing expressivity results show that the regular languages captured by RNC+ are the star-free ones, and they leave open the possibility that RNC+ may capture languages beyond regular.","We exclude this possibility for languages that include an identity element, i.e., an input that can occur an arbitrary number of times without affecting the output.","Namely, in the presence of an identity element, we show that the languages captured by RNC+ are exactly the star-free regular languages.","Identity elements are ubiquitous in temporal patterns, and hence our results apply to a large number of applications.","The implications of our results go beyond expressivity.","At their core, we establish a close structural correspondence between RNC+ and semiautomata cascades, showing that every neuron can be equivalently captured by a three-state semiautomaton.","A notable consequence of this result is that RNC+ are no more succinct than cascades of three-state semiautomata."],"url":"http://arxiv.org/abs/2405.11657v1","category":"cs.LG"}
{"created":"2024-05-19 20:01:29","title":"URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images","abstract":"Constructing simulation scenes that are both visually and physically realistic is a problem of practical interest in domains ranging from robotics to computer vision. This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems. However, building simulation models is often still done by hand. A graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties. While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with 'natural' kinematic and dynamic structures. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models. We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments.","sentences":["Constructing simulation scenes that are both visually and physically realistic is a problem of practical interest in domains ranging from robotics to computer vision.","This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems.","However, building simulation models is often still done by hand.","A graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.","While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with 'natural' kinematic and dynamic structures.","To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets.","To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models.","We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism.","We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies.","We then robustly deploy in the real world for tasks like articulated object manipulation.","In doing so, our work provides both a pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments."],"url":"http://arxiv.org/abs/2405.11656v1","category":"cs.RO"}
{"created":"2024-05-19 19:51:41","title":"Track Anything Rapter(TAR)","abstract":"Object tracking is a fundamental task in computer vision with broad practical applications across various domains, including traffic monitoring, robotics, and autonomous vehicle tracking. In this project, we aim to develop a sophisticated aerial vehicle system known as Track Anything Raptor (TAR), designed to detect, segment, and track objects of interest based on user-provided multimodal queries, such as text, images, and clicks. TAR utilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate the relative pose of the queried object. The tracking problem is approached as a Visual Servoing task, enabling the UAV to consistently focus on the object through advanced motion planning and control algorithms. We showcase how the integration of these foundational models with a custom high-level control algorithm results in a highly stable and precise tracking system deployed on a custom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the tracking algorithm's performance, we compare it against Vicon-based ground truth. Additionally, we evaluate the reliability of the foundational models in aiding tracking in scenarios involving occlusions. Finally, we test and validate the model's ability to work seamlessly with multiple modalities, such as click, bounding box, and image templates.","sentences":["Object tracking is a fundamental task in computer vision with broad practical applications across various domains, including traffic monitoring, robotics, and autonomous vehicle tracking.","In this project, we aim to develop a sophisticated aerial vehicle system known as Track Anything Raptor (TAR), designed to detect, segment, and track objects of interest based on user-provided multimodal queries, such as text, images, and clicks.","TAR utilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate the relative pose of the queried object.","The tracking problem is approached as a Visual Servoing task, enabling the UAV to consistently focus on the object through advanced motion planning and control algorithms.","We showcase how the integration of these foundational models with a custom high-level control algorithm results in a highly stable and precise tracking system deployed on a custom-built PX4 Autopilot-enabled Voxl2 M500 drone.","To validate the tracking algorithm's performance, we compare it against Vicon-based ground truth.","Additionally, we evaluate the reliability of the foundational models in aiding tracking in scenarios involving occlusions.","Finally, we test and validate the model's ability to work seamlessly with multiple modalities, such as click, bounding box, and image templates."],"url":"http://arxiv.org/abs/2405.11655v1","category":"cs.CV"}
{"created":"2024-05-19 19:32:12","title":"Movie Revenue Prediction using Machine Learning Models","abstract":"In the contemporary film industry, accurately predicting a movie's earnings is paramount for maximizing profitability. This project aims to develop a machine learning model for predicting movie earnings based on input features like the movie name, the MPAA rating of the movie, the genre of the movie, the year of release of the movie, the IMDb Rating, the votes by the watchers, the director, the writer and the leading cast, the country of production of the movie, the budget of the movie, the production company and the runtime of the movie. Through a structured methodology involving data collection, preprocessing, analysis, model selection, evaluation, and improvement, a robust predictive model is constructed. Linear Regression, Decision Trees, Random Forest Regression, Bagging, XGBoosting and Gradient Boosting have been trained and tested. Model improvement strategies include hyperparameter tuning and cross-validation. The resulting model offers promising accuracy and generalization, facilitating informed decision-making in the film industry to maximize profits.","sentences":["In the contemporary film industry, accurately predicting a movie's earnings is paramount for maximizing profitability.","This project aims to develop a machine learning model for predicting movie earnings based on input features like the movie name, the MPAA rating of the movie, the genre of the movie, the year of release of the movie, the IMDb Rating, the votes by the watchers, the director, the writer and the leading cast, the country of production of the movie, the budget of the movie, the production company and the runtime of the movie.","Through a structured methodology involving data collection, preprocessing, analysis, model selection, evaluation, and improvement, a robust predictive model is constructed.","Linear Regression, Decision Trees, Random Forest Regression, Bagging, XGBoosting and Gradient Boosting have been trained and tested.","Model improvement strategies include hyperparameter tuning and cross-validation.","The resulting model offers promising accuracy and generalization, facilitating informed decision-making in the film industry to maximize profits."],"url":"http://arxiv.org/abs/2405.11651v1","category":"cs.LG"}
{"created":"2024-05-21 17:57:33","title":"A Sound Type System for Secure Currency Flow","abstract":"In this paper we focus on TinySol, a minimal calculus for Solidity smart contracts, introduced by Bartoletti et al. We start by rephrasing its syntax (to emphasise its object-oriented flavour) and give a new big-step operational semantics. We then use it to define two security properties, namely call integrity and noninterference. These two properties have some similarities in their definition, in that they both require that some part of a program is not influenced by the other part. However, we show that the two properties are actually incomparable. Nevertheless, we provide a type system for noninterference and show that well-typed programs satisfy call integrity as well; hence, programs that are accepted by our type system satisfy both properties. We finally discuss the practical usability of the type system and its limitations by means of some simple examples.","sentences":["In this paper we focus on TinySol, a minimal calculus for Solidity smart contracts, introduced by Bartoletti et al.","We start by rephrasing its syntax (to emphasise its object-oriented flavour) and give a new big-step operational semantics.","We then use it to define two security properties, namely call integrity and noninterference.","These two properties have some similarities in their definition, in that they both require that some part of a program is not influenced by the other part.","However, we show that the two properties are actually incomparable.","Nevertheless, we provide a type system for noninterference and show that well-typed programs satisfy call integrity as well; hence, programs that are accepted by our type system satisfy both properties.","We finally discuss the practical usability of the type system and its limitations by means of some simple examples."],"url":"http://arxiv.org/abs/2405.12976v1","category":"cs.PL"}
{"created":"2024-05-21 17:56:45","title":"Systematic comparison of neural networks used in discovering strong gravitational lenses","abstract":"Efficient algorithms are being developed to search for strong gravitational lens systems owing to increasing large imaging surveys. Neural networks have been successfully used to discover galaxy-scale lens systems in imaging surveys such as the Kilo Degree Survey, Hyper-Suprime Cam (HSC) Survey and Dark Energy Survey over the last few years. Thus, it has become imperative to understand how some of these networks compare, their strengths and the role of the training datasets as most of the networks make use of supervised learning algorithms. In this work, we present the first-of-its-kind systematic comparison and benchmarking of networks from four teams that have analysed the HSC Survey data. Each team has designed their training samples and developed neural networks independently but coordinated apriori in reserving specific datasets strictly for test purposes. The test sample consists of mock lenses, real (candidate) lenses and real non-lenses gathered from various sources to benchmark and characterise the performance of each of the network. While each team's network performed much better on their own constructed test samples compared to those from others, all networks performed comparable on the test sample with real (candidate) lenses and non-lenses. We also investigate the impact of swapping the training samples amongst the teams while retaining the same network architecture. We find that this resulted in improved performance for some networks. These results have direct implications on measures to be taken for lens searches with upcoming imaging surveys such as the Rubin-Legacy Survey of Space and Time, Roman and Euclid.","sentences":["Efficient algorithms are being developed to search for strong gravitational lens systems owing to increasing large imaging surveys.","Neural networks have been successfully used to discover galaxy-scale lens systems in imaging surveys such as the Kilo Degree Survey, Hyper-Suprime Cam (HSC) Survey and Dark Energy Survey over the last few years.","Thus, it has become imperative to understand how some of these networks compare, their strengths and the role of the training datasets as most of the networks make use of supervised learning algorithms.","In this work, we present the first-of-its-kind systematic comparison and benchmarking of networks from four teams that have analysed the HSC Survey data.","Each team has designed their training samples and developed neural networks independently but coordinated apriori in reserving specific datasets strictly for test purposes.","The test sample consists of mock lenses, real (candidate) lenses and real non-lenses gathered from various sources to benchmark and characterise the performance of each of the network.","While each team's network performed much better on their own constructed test samples compared to those from others, all networks performed comparable on the test sample with real (candidate) lenses and non-lenses.","We also investigate the impact of swapping the training samples amongst the teams while retaining the same network architecture.","We find that this resulted in improved performance for some networks.","These results have direct implications on measures to be taken for lens searches with upcoming imaging surveys such as the Rubin-Legacy Survey of Space and Time, Roman and Euclid."],"url":"http://arxiv.org/abs/2405.12975v1","category":"astro-ph.GA"}
{"created":"2024-05-21 17:48:19","title":"Dieudonn\u00e9 theory via cohomology classifying stacks II","abstract":"In this paper, we prove two results: first, we use crystalline cohomology of classifying stacks to directly reconstruct the classical Dieudonn\\'e module of a finite, $p$-power rank, commutative group scheme $G$ over a perfect field $k$ of characteristic $p>0$. As a consequence, we give a new proof of the isomorphism $\\sigma^* M(G) \\simeq \\mathrm{Ext}^1 (G, \\mathcal{O}^{\\mathrm{crys}})$ due to Berthelot--Breen--Messing using stacky methods combined with the theory of de Rham--Witt complexes. Additionally, we show that finite locally free commutative group schemes of $p$-power rank over a quasisyntomic base ring embed fully faithfully into the category of prismatic $F$-gauges, which extends the work of Ansch\\\"utz and Le Bras on prismatic Dieudonn\\'e theory.","sentences":["In this paper, we prove two results: first, we use crystalline cohomology of classifying stacks to directly reconstruct the classical Dieudonn\\'e module of a finite, $p$-power rank, commutative group scheme $G$ over a perfect field $k$ of characteristic $p>0$. As a consequence, we give a new proof of the isomorphism $\\sigma^* M(G)","\\simeq \\mathrm{Ext}^1 (G, \\mathcal{O}^{\\mathrm{crys}})$ due to Berthelot--Breen--Messing using stacky methods combined with the theory of de Rham--Witt complexes.","Additionally, we show that finite locally free commutative group schemes of $p$-power rank over a quasisyntomic base ring embed fully faithfully into the category of prismatic $F$-gauges, which extends the work of Ansch\\\"utz and Le Bras on prismatic Dieudonn\\'e theory."],"url":"http://arxiv.org/abs/2405.12967v1","category":"math.AG"}
{"created":"2024-05-21 17:45:17","title":"Differential Walk on Spheres","abstract":"We introduce a Monte Carlo method for evaluating derivatives of the solution to a partial differential equation (PDE) with respect to problem parameters (such as domain geometry or boundary conditions). Derivatives can be evaluated at arbitrary points without performing a global solve, or constructing a volumetric grid or mesh. The method is hence well-suited to inverse problems with complex geometry, such as PDE-constrained shape optimization. Like other walk on spheres (WoS) algorithms, our method is trivial to parallelize, and is agnostic to boundary representation (meshes, splines, implicit surfaces etc.), supporting large topological changes. We focus in particular on screened Poisson equations, which model diverse problems from scientific and geometric computing. As in differentiable rendering, we jointly estimate derivatives with respect to all parameters -- hence, cost does not grow significantly with parameter count. In practice, even noisy derivative estimates exhibit fast, stable convergence for stochastic gradient-based optimization -- as we show via examples from thermal design, shape from diffusion, and computer graphics.","sentences":["We introduce a Monte Carlo method for evaluating derivatives of the solution to a partial differential equation (PDE) with respect to problem parameters (such as domain geometry or boundary conditions).","Derivatives can be evaluated at arbitrary points without performing a global solve, or constructing a volumetric grid or mesh.","The method is hence well-suited to inverse problems with complex geometry, such as PDE-constrained shape optimization.","Like other walk on spheres (WoS) algorithms, our method is trivial to parallelize, and is agnostic to boundary representation (meshes, splines, implicit surfaces etc.), supporting large topological changes.","We focus in particular on screened Poisson equations, which model diverse problems from scientific and geometric computing.","As in differentiable rendering, we jointly estimate derivatives with respect to all parameters -- hence, cost does not grow significantly with parameter count.","In practice, even noisy derivative estimates exhibit fast, stable convergence for stochastic gradient-based optimization -- as we show via examples from thermal design, shape from diffusion, and computer graphics."],"url":"http://arxiv.org/abs/2405.12964v1","category":"cs.GR"}
{"created":"2024-05-21 17:40:48","title":"Modeling for Non-exponential Production Systems Using Parts Flow Data: Model Parameter Estimation and Performance Analysis","abstract":"Mathematical modeling of production systems is the foundation of all model-based approaches for production system analysis, design, improvement, and control. To construct such a model for the stochastic process of the production system more efficiently, a new modeling approach has been proposed that reversely identifies the model parameters using system performance metrics (e.g., production rate, work-in-process, etc.) derived from the parts flow data. This paper extends this performance metrics-based modeling approach to non-exponential serial production lines. Since no analytical expressions of performance metrics are available for non-exponential systems, we use neural network surrogate models to calculate those performance metrics as functions in terms of the system parameters. Then, based on the surrogate models and given performance metrics, the machine parameters are estimated by solving a constrained optimization problem that minimizes the mean square error of the performance metrics resulting from the estimated parameters compared to the true ones. With the designed multi-start particle swarm optimization algorithm, we find that multiple non-unique combinations of machine parameters can lead to practically the same system performance metrics and a linear relationship of the reliability parameters from these obtained estimations is observed. Besides, model sensitivity analysis is implemented to verify the robustness of the different combinations of machine parameters even under the potential improvement scenarios.","sentences":["Mathematical modeling of production systems is the foundation of all model-based approaches for production system analysis, design, improvement, and control.","To construct such a model for the stochastic process of the production system more efficiently, a new modeling approach has been proposed that reversely identifies the model parameters using system performance metrics (e.g., production rate, work-in-process, etc.)","derived from the parts flow data.","This paper extends this performance metrics-based modeling approach to non-exponential serial production lines.","Since no analytical expressions of performance metrics are available for non-exponential systems, we use neural network surrogate models to calculate those performance metrics as functions in terms of the system parameters.","Then, based on the surrogate models and given performance metrics, the machine parameters are estimated by solving a constrained optimization problem that minimizes the mean square error of the performance metrics resulting from the estimated parameters compared to the true ones.","With the designed multi-start particle swarm optimization algorithm, we find that multiple non-unique combinations of machine parameters can lead to practically the same system performance metrics and a linear relationship of the reliability parameters from these obtained estimations is observed.","Besides, model sensitivity analysis is implemented to verify the robustness of the different combinations of machine parameters even under the potential improvement scenarios."],"url":"http://arxiv.org/abs/2405.12962v1","category":"eess.SY"}
{"created":"2024-05-21 17:32:29","title":"A description based on optimal transport for a class of stochastic McKean-Vlasov control problems","abstract":"We study the convergence of an $N$-particle Markovian controlled system to the solution of a family of stochastic McKean-Vlasov control problems, either with a finite horizon or Schr\\\"odinger type cost functional. Specifically, under suitable assumptions, we prove the convergence of the value functions, the fixed-time probability distributions, and the relative entropy of their path-space probability laws. These proofs are based on a Benamou-Brenier type reformulation of the problem and a superposition principle, both of which are tools from the theory of optimal transport.","sentences":["We study the convergence of an $N$-particle Markovian controlled system to the solution of a family of stochastic McKean-Vlasov control problems, either with a finite horizon or Schr\\\"odinger type cost functional.","Specifically, under suitable assumptions, we prove the convergence of the value functions, the fixed-time probability distributions, and the relative entropy of their path-space probability laws.","These proofs are based on a Benamou-Brenier type reformulation of the problem and a superposition principle, both of which are tools from the theory of optimal transport."],"url":"http://arxiv.org/abs/2405.12960v1","category":"math.PR"}
{"created":"2024-05-21 17:28:21","title":"GA-NIFS: Witnessing the complex assembly of a massive star-forming system at $z=5.7$","abstract":"We present observations of the $z\\sim5.7$ Lyman-break galaxy HZ10 with the JWST/NIRSpec IFU in high and low spectral resolution (G395H, $R\\sim2700$ and PRISM, $R\\sim100$, respectively), as part of the GA-NIFS program. By spatially resolving the source, we find evidence for three spatially and spectrally distinct regions of line emission along with one region of strong continuum emission, all within a projected distance of $<10$kpc. The R2700 data features strong detections in H$\\beta$, [OIII]$\\lambda\\lambda4959{,}5007$, [NII]$\\lambda\\lambda6548{,}6584$, H$\\alpha$, and [SII]$\\lambda\\lambda6716{,}6731$. The R100 data additionally contains a strong detection of the Ly$\\alpha$ break, rest-UV continuum, and [OII]$\\lambda\\lambda3726{,}3729$. None of the detected lines present strong evidence for AGN excitation from line diagnostic diagrams, and no high-ionisation lines are detected. Using the detected lines, we constrain the electron density $\\left( \\rm \\log_{10}\\left( n_e / cm^{-3}\\right)\\sim2.5-3.3\\right)$ and metallicity ($\\sim0.5-0.7$ solar) in each component. Spaxel-by-spaxel fits of each cube reveal a strong east-west velocity gradient and significant line asymmetries (indicating tidal features or outflows). The western component features a very red UV slope ($\\beta_{UV}\\sim-1$) and significant H$\\alpha$ emission, suggesting an evolved population and active star formation. From a comparison to high resolution [CII]$158\\mu$m imaging obtained with the Atacama Large Millimetre/submillimetre Array (ALMA), we find that the continuum emitter is associated with a gas-poor stellar population. Altogether, these data suggest that HZ10 represents an ongoing merger, with a complex distribution of stars, gas, and dust $<1$Gyr after the Big Bang.","sentences":["We present observations of the $z\\sim5.7$ Lyman-break galaxy HZ10 with the JWST/NIRSpec IFU in high and low spectral resolution (G395H, $R\\sim2700$ and PRISM, $R\\sim100$, respectively), as part of the GA-NIFS program.","By spatially resolving the source, we find evidence for three spatially and spectrally distinct regions of line emission along with one region of strong continuum emission, all within a projected distance of $<10$kpc.","The R2700 data features strong detections in H$\\beta$, [OIII]$\\lambda\\lambda4959{,}5007$, [NII]$\\lambda\\lambda6548{,}6584$, H$\\alpha$, and [SII]$\\lambda\\lambda6716{,}6731$. The R100 data additionally contains a strong detection of the Ly$\\alpha$ break, rest-UV continuum, and [OII]$\\lambda\\lambda3726{,}3729$. None of the detected lines present strong evidence for AGN excitation from line diagnostic diagrams, and no high-ionisation lines are detected.","Using the detected lines, we constrain the electron density $\\left( \\rm \\log_{10}\\left( n_e / cm^{-3}\\right)\\sim2.5-3.3\\right)$ and metallicity ($\\sim0.5-0.7$ solar) in each component.","Spaxel-by-spaxel fits of each cube reveal a strong east-west velocity gradient and significant line asymmetries (indicating tidal features or outflows).","The western component features a very red UV slope ($\\beta_{UV}\\sim-1$) and significant H$\\alpha$ emission, suggesting an evolved population and active star formation.","From a comparison to high resolution [CII]$158\\mu$m imaging obtained with the Atacama Large Millimetre/submillimetre Array (ALMA), we find that the continuum emitter is associated with a gas-poor stellar population.","Altogether, these data suggest that HZ10 represents an ongoing merger, with a complex distribution of stars, gas, and dust $<1$Gyr after the Big Bang."],"url":"http://arxiv.org/abs/2405.12955v1","category":"astro-ph.GA"}
{"created":"2024-05-21 17:17:17","title":"AMFD: Distillation via Adaptive Multimodal Fusion for Multispectral Pedestrian Detection","abstract":"Multispectral pedestrian detection has been shown to be effective in improving performance within complex illumination scenarios. However, prevalent double-stream networks in multispectral detection employ two separate feature extraction branches for multi-modal data, leading to nearly double the inference time compared to single-stream networks utilizing only one feature extraction branch. This increased inference time has hindered the widespread employment of multispectral pedestrian detection in embedded devices for autonomous systems. To address this limitation, various knowledge distillation methods have been proposed. However, traditional distillation methods focus only on the fusion features and ignore the large amount of information in the original multi-modal features, thereby restricting the student network's performance. To tackle the challenge, we introduce the Adaptive Modal Fusion Distillation (AMFD) framework, which can fully utilize the original modal features of the teacher network. Specifically, a Modal Extraction Alignment (MEA) module is utilized to derive learning weights for student networks, integrating focal and global attention mechanisms. This methodology enables the student network to acquire optimal fusion strategies independent from that of teacher network without necessitating an additional feature fusion module. Furthermore, we present the SMOD dataset, a well-aligned challenging multispectral dataset for detection. Extensive experiments on the challenging KAIST, LLVIP and SMOD datasets are conducted to validate the effectiveness of AMFD. The results demonstrate that our method outperforms existing state-of-the-art methods in both reducing log-average Miss Rate and improving mean Average Precision. The code is available at https://github.com/bigD233/AMFD.git.","sentences":["Multispectral pedestrian detection has been shown to be effective in improving performance within complex illumination scenarios.","However, prevalent double-stream networks in multispectral detection employ two separate feature extraction branches for multi-modal data, leading to nearly double the inference time compared to single-stream networks utilizing only one feature extraction branch.","This increased inference time has hindered the widespread employment of multispectral pedestrian detection in embedded devices for autonomous systems.","To address this limitation, various knowledge distillation methods have been proposed.","However, traditional distillation methods focus only on the fusion features and ignore the large amount of information in the original multi-modal features, thereby restricting the student network's performance.","To tackle the challenge, we introduce the Adaptive Modal Fusion Distillation (AMFD) framework, which can fully utilize the original modal features of the teacher network.","Specifically, a Modal Extraction Alignment (MEA) module is utilized to derive learning weights for student networks, integrating focal and global attention mechanisms.","This methodology enables the student network to acquire optimal fusion strategies independent from that of teacher network without necessitating an additional feature fusion module.","Furthermore, we present the SMOD dataset, a well-aligned challenging multispectral dataset for detection.","Extensive experiments on the challenging KAIST, LLVIP and SMOD datasets are conducted to validate the effectiveness of AMFD.","The results demonstrate that our method outperforms existing state-of-the-art methods in both reducing log-average Miss Rate and improving mean Average Precision.","The code is available at https://github.com/bigD233/AMFD.git."],"url":"http://arxiv.org/abs/2405.12944v1","category":"cs.CV"}
{"created":"2024-05-21 17:14:10","title":"Metacognitive particles, mental action and the sense of agency","abstract":"This paper articulates metacognition using the language of statistical physics and Bayesian mechanics. Metacognitive beliefs, defined as beliefs about beliefs, find a natural description within this formalism, which allows us to define the dynamics of 'metacognitive particles', i.e., systems possessing metacognitive beliefs. We further unpack this typology of metacognitive systems by distinguishing passive and active metacognitive particles, where active particles are endowed with the capacity for mental actions that update the parameters of other beliefs. We provide arguments for the necessity of this architecture in the emergence of a subjective sense of agency and the experience of being separate from the environment. The motivation is to pave the way towards a mathematical and physical understanding of cognition -- and higher forms thereof -- furthering the study and formalization of cognitive science in the language of mathematical physics.","sentences":["This paper articulates metacognition using the language of statistical physics and Bayesian mechanics.","Metacognitive beliefs, defined as beliefs about beliefs, find a natural description within this formalism, which allows us to define the dynamics of 'metacognitive particles', i.e., systems possessing metacognitive beliefs.","We further unpack this typology of metacognitive systems by distinguishing passive and active metacognitive particles, where active particles are endowed with the capacity for mental actions that update the parameters of other beliefs.","We provide arguments for the necessity of this architecture in the emergence of a subjective sense of agency and the experience of being separate from the environment.","The motivation is to pave the way towards a mathematical and physical understanding of cognition -- and higher forms thereof -- furthering the study and formalization of cognitive science in the language of mathematical physics."],"url":"http://arxiv.org/abs/2405.12941v1","category":"q-bio.NC"}
{"created":"2024-05-21 17:12:19","title":"Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models","abstract":"Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks. Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency. However, this approach fails in scenarios where the correct answers are in the minority. We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers. To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains. Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task. Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods. Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods.","sentences":["Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks.","Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency.","However, this approach fails in scenarios where the correct answers are in the minority.","We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers.","To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains.","Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task.","Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods.","Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods."],"url":"http://arxiv.org/abs/2405.12939v1","category":"cs.CL"}
{"created":"2024-05-21 17:09:01","title":"Hybrid PDE-ODE Models for Efficient Simulation of Infection Spread in Epidemiology","abstract":"This paper introduces a novel hybrid mathematical modeling approach that effectively couples Partial Differential Equations (PDEs) with Ordinary Differential Equations (ODEs), exemplified through the simulation of epidemiological processes. The hybrid model aims to integrate the spatially detailed representation of disease dynamics provided by PDEs with the computational efficiency of ODEs. In the presented epidemiological use-case, this integration allows for the rapid assessment of public health interventions and the potential impact of infectious diseases across large populations. We discuss the theoretical formulation of the hybrid PDE-ODE model, including the governing equations and boundary conditions. The model's capabilities are demonstrated through detailed simulations of disease spread in synthetic environments and real-world scenarios, specifically focusing on the regions of Lombardy, Italy, and Berlin, Germany. Results indicate that the hybrid model achieves a balance between computational speed and accuracy, making it a valuable tool for policymakers in real-time decision-making and scenario analysis in epidemiology and potentially in other fields requiring similar modeling approaches.","sentences":["This paper introduces a novel hybrid mathematical modeling approach that effectively couples Partial Differential Equations (PDEs) with Ordinary Differential Equations (ODEs), exemplified through the simulation of epidemiological processes.","The hybrid model aims to integrate the spatially detailed representation of disease dynamics provided by PDEs with the computational efficiency of ODEs.","In the presented epidemiological use-case, this integration allows for the rapid assessment of public health interventions and the potential impact of infectious diseases across large populations.","We discuss the theoretical formulation of the hybrid PDE-ODE model, including the governing equations and boundary conditions.","The model's capabilities are demonstrated through detailed simulations of disease spread in synthetic environments and real-world scenarios, specifically focusing on the regions of Lombardy, Italy, and Berlin, Germany.","Results indicate that the hybrid model achieves a balance between computational speed and accuracy, making it a valuable tool for policymakers in real-time decision-making and scenario analysis in epidemiology and potentially in other fields requiring similar modeling approaches."],"url":"http://arxiv.org/abs/2405.12938v1","category":"math.DS"}
{"created":"2024-05-21 17:05:02","title":"Address-Specific Sustainable Accommodation Choice Through Real-World Data Integration","abstract":"Consumers wish to choose sustainable accommodation for their travels, and in the case of corporations, may be required to do so. Yet accommodation marketplaces provide no meaningful capability for sustainable choice: typically CO2 estimates are provided that are identical for all accommodation of the same type across an entire country. We propose a decision support system that enables real choice of sustainable accommodation. We develop a data-driven address- specific metric called EcoGrade, which integrates government approved datasets and uses interpolation where data is sparse. We validate the metric on 10,000 UK addresses in 10 cities, showing the match of our interpolations to reality is statistically significant. We show how the metric has been embedded into a decision support system for a global accommodation marketplace and tested by real users over several months with positive user feedback. In the EU, forty percent of final energy consumption is from buildings. We need to encourage all building owners to make their accommodation more efficient. The rental sector is one area where change can occur rapidly, as rented accommodation is renovated frequently. We anticipate our decision support system using EcoGrade will encourage this positive change.","sentences":["Consumers wish to choose sustainable accommodation for their travels, and in the case of corporations, may be required to do so.","Yet accommodation marketplaces provide no meaningful capability for sustainable choice: typically CO2 estimates are provided that are identical for all accommodation of the same type across an entire country.","We propose a decision support system that enables real choice of sustainable accommodation.","We develop a data-driven address- specific metric called EcoGrade, which integrates government approved datasets and uses interpolation where data is sparse.","We validate the metric on 10,000 UK addresses in 10 cities, showing the match of our interpolations to reality is statistically significant.","We show how the metric has been embedded into a decision support system for a global accommodation marketplace and tested by real users over several months with positive user feedback.","In the EU, forty percent of final energy consumption is from buildings.","We need to encourage all building owners to make their accommodation more efficient.","The rental sector is one area where change can occur rapidly, as rented accommodation is renovated frequently.","We anticipate our decision support system using EcoGrade will encourage this positive change."],"url":"http://arxiv.org/abs/2405.12934v1","category":"cs.CY"}
{"created":"2024-05-21 16:59:23","title":"Small cross section of the synthesis of darmstadtium in the $^{48}$Ca+$^{232}$Th reaction","abstract":"The smallness of the cross section of evaporation residues formed in the hot fusion reaction $^{48}$Ca+$^{232}$Th is analyzed by the dinuclear system model (DNS). The capture probability has been calculated by solving the dynamical equations of motion for the relative distance between the centers-of-mass of the DNS nuclei. Fusion of nuclei is considered as evolution of the DNS to a stable compound nucleus. The fusion probability has a bell-like shape and quasifission is one of reasons causing smallness of the yield of the evaporation residues products. Another reason is the decrease of the fission barrier for the isotopes $^{275-285}$Dm related with the shell effects in the neutron structure. The agreement of the theoretical results obtained for the yield of the evaporation residues with the experimental data measured in the Factory of superheavy elements of Joint Institute for Nuclear Research is well.","sentences":["The smallness of the cross section of evaporation residues formed in the hot fusion reaction $^{48}$Ca+$^{232}$Th is analyzed by the dinuclear system model (DNS).","The capture probability has been calculated by solving the dynamical equations of motion for the relative distance between the centers-of-mass of the DNS nuclei.","Fusion of nuclei is considered as evolution of the DNS to a stable compound nucleus.","The fusion probability has a bell-like shape and quasifission is one of reasons causing smallness of the yield of the evaporation residues products.","Another reason is the decrease of the fission barrier for the isotopes $^{275-285}$Dm related with the shell effects in the neutron structure.","The agreement of the theoretical results obtained for the yield of the evaporation residues with the experimental data measured in the Factory of superheavy elements of Joint Institute for Nuclear Research is well."],"url":"http://arxiv.org/abs/2405.12932v1","category":"nucl-th"}
{"created":"2024-05-21 16:59:21","title":"Enabling Additive Manufacturing Part Inspection of Digital Twins via Collaborative Virtual Reality","abstract":"Digital twins (DTs) are an emerging capability in additive manufacturing (AM), set to revolutionize design optimization, inspection, in situ monitoring, and root cause analysis. AM DTs typically incorporate multimodal data streams, ranging from machine toolpaths and in-process imaging to X-ray CT scans and performance metrics. Despite the evolution of DT platforms, challenges remain in effectively inspecting them for actionable insights, either individually or in a multidisciplinary team setting. Quality assurance, manufacturing departments, pilot labs, and plant operations must collaborate closely to reliably produce parts at scale. This is particularly crucial in AM where complex structures require a collaborative and multidisciplinary approach. Additionally, the large-scale data originating from different modalities and their inherent 3D nature pose significant hurdles for traditional 2D desktop-based inspection methods. To address these challenges and increase the value proposition of DTs, we introduce a novel virtual reality (VR) framework to facilitate collaborative and real-time inspection of DTs in AM. This framework includes advanced features for intuitive alignment and visualization of multimodal data, visual occlusion management, streaming large-scale volumetric data, and collaborative tools, substantially improving the inspection of AM components and processes to fully exploit the potential of DTs in AM.","sentences":["Digital twins (DTs) are an emerging capability in additive manufacturing (AM), set to revolutionize design optimization, inspection, in situ monitoring, and root cause analysis.","AM DTs typically incorporate multimodal data streams, ranging from machine toolpaths and in-process imaging to X-ray CT scans and performance metrics.","Despite the evolution of DT platforms, challenges remain in effectively inspecting them for actionable insights, either individually or in a multidisciplinary team setting.","Quality assurance, manufacturing departments, pilot labs, and plant operations must collaborate closely to reliably produce parts at scale.","This is particularly crucial in AM where complex structures require a collaborative and multidisciplinary approach.","Additionally, the large-scale data originating from different modalities and their inherent 3D nature pose significant hurdles for traditional 2D desktop-based inspection methods.","To address these challenges and increase the value proposition of DTs, we introduce a novel virtual reality (VR) framework to facilitate collaborative and real-time inspection of DTs in AM.","This framework includes advanced features for intuitive alignment and visualization of multimodal data, visual occlusion management, streaming large-scale volumetric data, and collaborative tools, substantially improving the inspection of AM components and processes to fully exploit the potential of DTs in AM."],"url":"http://arxiv.org/abs/2405.12931v1","category":"cs.HC"}
{"created":"2024-05-21 16:58:35","title":"Pytorch-Wildlife: A Collaborative Deep Learning Framework for Conservation","abstract":"The alarming decline in global biodiversity, driven by various factors, underscores the urgent need for large-scale wildlife monitoring. In response, scientists have turned to automated deep learning methods for data processing in wildlife monitoring. However, applying these advanced methods in real-world scenarios is challenging due to their complexity and the need for specialized knowledge, primarily because of technical challenges and interdisciplinary barriers.   To address these challenges, we introduce Pytorch-Wildlife, an open-source deep learning platform built on PyTorch. It is designed for creating, modifying, and sharing powerful AI models. This platform emphasizes usability and accessibility, making it accessible to individuals with limited or no technical background. It also offers a modular codebase to simplify feature expansion and further development. Pytorch-Wildlife offers an intuitive, user-friendly interface, accessible through local installation or Hugging Face, for animal detection and classification in images and videos. As two real-world applications, Pytorch-Wildlife has been utilized to train animal classification models for species recognition in the Amazon Rainforest and for invasive opossum recognition in the Galapagos Islands. The Opossum model achieves 98% accuracy, and the Amazon model has 92% recognition accuracy for 36 animals in 90% of the data. As Pytorch-Wildlife evolves, we aim to integrate more conservation tasks, addressing various environmental challenges. Pytorch-Wildlife is available at https://github.com/microsoft/CameraTraps.","sentences":["The alarming decline in global biodiversity, driven by various factors, underscores the urgent need for large-scale wildlife monitoring.","In response, scientists have turned to automated deep learning methods for data processing in wildlife monitoring.","However, applying these advanced methods in real-world scenarios is challenging due to their complexity and the need for specialized knowledge, primarily because of technical challenges and interdisciplinary barriers.   ","To address these challenges, we introduce Pytorch-Wildlife, an open-source deep learning platform built on PyTorch.","It is designed for creating, modifying, and sharing powerful AI models.","This platform emphasizes usability and accessibility, making it accessible to individuals with limited or no technical background.","It also offers a modular codebase to simplify feature expansion and further development.","Pytorch-Wildlife offers an intuitive, user-friendly interface, accessible through local installation or Hugging Face, for animal detection and classification in images and videos.","As two real-world applications, Pytorch-Wildlife has been utilized to train animal classification models for species recognition in the Amazon Rainforest and for invasive opossum recognition in the Galapagos Islands.","The Opossum model achieves 98% accuracy, and the Amazon model has 92% recognition accuracy for 36 animals in 90% of the data.","As Pytorch-Wildlife evolves, we aim to integrate more conservation tasks, addressing various environmental challenges.","Pytorch-Wildlife is available at https://github.com/microsoft/CameraTraps."],"url":"http://arxiv.org/abs/2405.12930v1","category":"cs.CV"}
{"created":"2024-05-21 16:41:23","title":"The $L_p$-dual space of a semisimple Lie group","abstract":"Let $G$ be a semisimple Lie group. We describe the irreducible representations of $G$ by linear isometries on $L_p$-spaces for $p\\in (1,+\\infty)$ with $p\\neq 2.$ More precisely, we show that, for every such representation $\\pi,$ there exists a parabolic subgroup $Q$ of $G$ such that $\\pi$ is equivalent to the natural representation of $G$ on $L_p(G/Q)$ twisted by a unitary character of $Q.$ When $G$ is of real rank one, we give a complete classification of the possible irreducible representations of $G$ on an $L_p$-space for $p\\neq 2,$ up to equivalence.","sentences":["Let $G$ be a semisimple Lie group.","We describe the irreducible representations of $G$ by linear isometries on $L_p$-spaces for $p\\in (1,+\\infty)$ with $p\\neq 2.$ More precisely, we show that, for every such representation $\\pi,$ there exists a parabolic subgroup $Q$ of $G$ such that $\\pi$ is equivalent to the natural representation of $G$ on $L_p(G/Q)$ twisted by a unitary character of $Q.$ When $G$ is of real rank one, we give a complete classification of the possible irreducible representations of $G$ on an $L_p$-space for $p\\neq 2,$ up to equivalence."],"url":"http://arxiv.org/abs/2405.12919v1","category":"math.RT"}
{"created":"2024-05-21 16:33:19","title":"The implications of state aggregation in deteriorating Markov Decision Processes with optimal threshold policies","abstract":"Markov Decision Processes (MDPs) are mathematical models of sequential decision-making under uncertainty that have found applications in healthcare, manufacturing, logistics, and others. In these models, a decision-maker observes the state of a stochastic process and determines which action to take with the goal of maximizing the expected total discounted rewards received. In many applications, the state space of the true system is large and there may be limited observations out of certain states to estimate the transition probability matrix. To overcome this, modelers will aggregate the true states into ``superstates\" resulting in a smaller state space. This aggregation process improves computational tractability and increases the number of observations among superstates. Thus, the modeler's choice of state space leads to a trade-off in transition probability estimates. While coarser discretization of the state space gives more observations in each state to estimate the transition probability matrix, this comes at the cost of precision in the state characterization and resulting policy recommendations. In this paper, we consider the implications of this modeling decision on the resulting policies from MDPs for which the true model is expected to have a threshold policy that is optimal. We analyze these MDPs and provide conditions under which the aggregated MDP will also have an optimal threshold policy. Using a simulation study, we explore the trade-offs between more fine and more coarse aggregation. We explore the the show that there is the highest potential for policy improvement on larger state spaces, but that aggregated MDPs are preferable under limited data. We discuss how these findings the implications of our findings for modelers who must select which state space design to use.","sentences":["Markov Decision Processes (MDPs) are mathematical models of sequential decision-making under uncertainty that have found applications in healthcare, manufacturing, logistics, and others.","In these models, a decision-maker observes the state of a stochastic process and determines which action to take with the goal of maximizing the expected total discounted rewards received.","In many applications, the state space of the true system is large and there may be limited observations out of certain states to estimate the transition probability matrix.","To overcome this, modelers will aggregate the true states into ``superstates\" resulting in a smaller state space.","This aggregation process improves computational tractability and increases the number of observations among superstates.","Thus, the modeler's choice of state space leads to a trade-off in transition probability estimates.","While coarser discretization of the state space gives more observations in each state to estimate the transition probability matrix, this comes at the cost of precision in the state characterization and resulting policy recommendations.","In this paper, we consider the implications of this modeling decision on the resulting policies from MDPs for which the true model is expected to have a threshold policy that is optimal.","We analyze these MDPs and provide conditions under which the aggregated MDP will also have an optimal threshold policy.","Using a simulation study, we explore the trade-offs between more fine and more coarse aggregation.","We explore the the show that there is the highest potential for policy improvement on larger state spaces, but that aggregated MDPs are preferable under limited data.","We discuss how these findings the implications of our findings for modelers who must select which state space design to use."],"url":"http://arxiv.org/abs/2405.12912v1","category":"math.OC"}
{"created":"2024-05-21 16:27:04","title":"Extremum Seeking is Stable for Scalar Maps that are Strictly but Not Strongly Convex","abstract":"For a map that is strictly but not strongly convex, model-based gradient extremum seeking has an eigenvalue of zero at the extremum, i.e., it fails at exponential convergence. Interestingly, perturbation-based model-free extremum seeking has a negative Jacobian, in the average, meaning that its (practical) convergence is exponential, even though the map's Hessian is zero at the extremum. While these observations for the gradient algorithm are not trivial, we focus in this paper on an even more nontrivial study of the same phenomenon for Newton-based extremum seeking control (NESC).   NESC is a second-order method which corrects for the unknown Hessian of the unknown map, not only in order to speed up parameter convergence, but also (1) to make the convergence rate user-assignable in spite of the unknown Hessian, and (2) to equalize the convergence rates in different directions for multivariable maps. Previous NESC work established stability only for maps whose Hessians are strictly positive definite everywhere, so the Hessian is invertible everywhere. For a scalar map, we establish the rather unexpected property that, even when the map behind is strictly convex but not strongly convex, i.e., when the Hessian may be zero, NESC guarantees practical asymptotic stability, semiglobally. While a model-based Newton-based algorithm would run into non-invertibility of the Hessian, the perturbation-based NESC, surprisingly, avoids this challenge by leveraging the fact that the average of the perturbation-based Hessian estimate is always positive, even though the actual Hessian may be zero.","sentences":["For a map that is strictly but not strongly convex, model-based gradient extremum seeking has an eigenvalue of zero at the extremum, i.e., it fails at exponential convergence.","Interestingly, perturbation-based model-free extremum seeking has a negative Jacobian, in the average, meaning that its (practical) convergence is exponential, even though the map's Hessian is zero at the extremum.","While these observations for the gradient algorithm are not trivial, we focus in this paper on an even more nontrivial study of the same phenomenon for Newton-based extremum seeking control (NESC).   ","NESC is a second-order method which corrects for the unknown Hessian of the unknown map, not only in order to speed up parameter convergence, but also (1) to make the convergence rate user-assignable in spite of the unknown Hessian, and (2) to equalize the convergence rates in different directions for multivariable maps.","Previous NESC work established stability only for maps whose Hessians are strictly positive definite everywhere, so the Hessian is invertible everywhere.","For a scalar map, we establish the rather unexpected property that, even when the map behind is strictly convex but not strongly convex, i.e., when the Hessian may be zero, NESC guarantees practical asymptotic stability, semiglobally.","While a model-based Newton-based algorithm would run into non-invertibility of the Hessian, the perturbation-based NESC, surprisingly, avoids this challenge by leveraging the fact that the average of the perturbation-based Hessian estimate is always positive, even though the actual Hessian may be zero."],"url":"http://arxiv.org/abs/2405.12908v1","category":"math.OC"}
{"created":"2024-05-21 16:18:48","title":"A single crystal study of Kagome metals U$_2$Mn$_3$Ge and U$_2$Fe$_3$Ge","abstract":"Single crystals of U$_2$Mn$_3$Ge and and U$_2$Fe$_3$Ge with a Kagome lattice structure were synthesized using a high-temperature self-flux crystal growth method. The physical properties of these crystals were characterized through measurements of resistivity, magnetism, and specific heat. U$_2$Fe$_3$Ge exhibits ferromagnetic ground state and Anomalous Hall Effect, and U$_2$Mn$_3$Ge demonstrates a complex magnetic structure. Both compounds exhibit large Sommerfeld coefficient, indicating coexistence of heavy Fermion behavior with magnetism. Our results suggest that this U$_2$TM$_3$Ge (TM = Mn, Fe, Co) family is a promising platform to investigate the interplay of magnetism, Kondo physics and the Kagome lattice.","sentences":["Single crystals of U$_2$Mn$_3$Ge and and U$_2$Fe$_3$Ge with a Kagome lattice structure were synthesized using a high-temperature self-flux crystal growth method.","The physical properties of these crystals were characterized through measurements of resistivity, magnetism, and specific heat.","U$_2$Fe$_3$Ge exhibits ferromagnetic ground state and Anomalous Hall Effect, and U$_2$Mn$_3$Ge demonstrates a complex magnetic structure.","Both compounds exhibit large Sommerfeld coefficient, indicating coexistence of heavy Fermion behavior with magnetism.","Our results suggest that this U$_2$TM$_3$Ge (TM = Mn, Fe, Co) family is a promising platform to investigate the interplay of magnetism, Kondo physics and the Kagome lattice."],"url":"http://arxiv.org/abs/2405.12905v1","category":"cond-mat.str-el"}
{"created":"2024-05-21 16:17:01","title":"A low-loss and broadband all-fiber acousto-optic circulator","abstract":"The introduction of low-loss optical fibers probably represents the single most important advance in the growth of our telecommunication system. To meet our needs for secure communications, it is likely that our classical network will soon be operating alongside what is known as a quantum network. The latter is very sensitive to loss and thus poses new constraints to the performance of current fiber components. In particular, recent quantum network prototypes underlined the absence of low-loss non-reciprocal fiber-based devices. Here, we present a solution to this issue by realizing low-loss (0.81 dB), broadband (at least 50 GHz bandwidth) and high-extinction (up to 27 dB) circulators, based on Mach-Zehnder interferometers including so-called fiber null-couplers. The latter are directional couplers, whose splitting-ratio can be controlled by launching acoustic waves along the coupling region. Fabricated from standard single-mode fibers and actuated electrically, these circulators can be made to fit any existing optical fiber networks and could turn out to be key for the transmission and processing of optically encoded quantum information.","sentences":["The introduction of low-loss optical fibers probably represents the single most important advance in the growth of our telecommunication system.","To meet our needs for secure communications, it is likely that our classical network will soon be operating alongside what is known as a quantum network.","The latter is very sensitive to loss and thus poses new constraints to the performance of current fiber components.","In particular, recent quantum network prototypes underlined the absence of low-loss non-reciprocal fiber-based devices.","Here, we present a solution to this issue by realizing low-loss (0.81 dB), broadband (at least 50 GHz bandwidth) and high-extinction (up to 27 dB) circulators, based on Mach-Zehnder interferometers including so-called fiber null-couplers.","The latter are directional couplers, whose splitting-ratio can be controlled by launching acoustic waves along the coupling region.","Fabricated from standard single-mode fibers and actuated electrically, these circulators can be made to fit any existing optical fiber networks and could turn out to be key for the transmission and processing of optically encoded quantum information."],"url":"http://arxiv.org/abs/2405.12903v1","category":"physics.optics"}
{"created":"2024-05-21 16:15:02","title":"Diffusion of brightened dark excitons in a high-angle incommensurate Moir\u00e9 homobilayer","abstract":"The last few years have witnessed a surge in interest and research efforts in the field of twistronics, especially in low-angle twisted bilayers of transition metal dichalocogenides. These novel material platforms have been demonstrated to host periodic arrays of excitonic quantum emitters, interlayer excitons with long lifetimes, and exotic many-body states. While much remains to be known and understood about these heterostructures, the field of high-angle, incommensurate bilayers is even less explored. At twist angles larger than a few degrees, the presence of periodicity in these bilayers becomes chaotic, making the systems essentially aperiodic and incommensurate in nature due to the limitations of fabrication techniques. In this work, we demonstrate the emergence of a brightened dark intralayer exciton in twisted molybdenum diselenide homobilayer. We show that this dark exciton diffuses across the excitation spot more efficiently as compared to trions or excitons, reaching diffusion lengths greater than 4 microns. Temperature-dependent spectra provide corroborative evidence and reveal a brightened dark trion. Our results reveal some of the richness of the physics of these high-angle systems.","sentences":["The last few years have witnessed a surge in interest and research efforts in the field of twistronics, especially in low-angle twisted bilayers of transition metal dichalocogenides.","These novel material platforms have been demonstrated to host periodic arrays of excitonic quantum emitters, interlayer excitons with long lifetimes, and exotic many-body states.","While much remains to be known and understood about these heterostructures, the field of high-angle, incommensurate bilayers is even less explored.","At twist angles larger than a few degrees, the presence of periodicity in these bilayers becomes chaotic, making the systems essentially aperiodic and incommensurate in nature due to the limitations of fabrication techniques.","In this work, we demonstrate the emergence of a brightened dark intralayer exciton in twisted molybdenum diselenide homobilayer.","We show that this dark exciton diffuses across the excitation spot more efficiently as compared to trions or excitons, reaching diffusion lengths greater than 4 microns.","Temperature-dependent spectra provide corroborative evidence and reveal a brightened dark trion.","Our results reveal some of the richness of the physics of these high-angle systems."],"url":"http://arxiv.org/abs/2405.12901v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 16:07:48","title":"Principles of bursty mRNA expression and irreversibility in single cells and extrinsically varying populations","abstract":"The canonical model of mRNA expression is the telegraph model, describing a gene that switches on and off, subject to transcription and decay. It describes steady-state mRNA distributions that subscribe to transcription in bursts with first-order decay, referred to as super-Poissonian expression. Using a telegraph-like model, I propose an answer to the question of why gene expression is bursty in the first place, and what benefits it confers. Using analytics for the entropy production rate, I find that entropy production is maximal when the on and off switching rates between the gene states are approximately equal. This is related to a lower bound on the free energy necessary to keep the system out of equilibrium, meaning that bursty gene expression may have evolved in part due to free energy efficiency. It is shown that there are trade-offs between having slow nuclear export, which can reduce cytoplasmic mRNA noise, and the energy required to keep the system out of equilibrium -- nuclear compartmentalization comes with an associated free energy cost. At the population level, I find that extrinsic variation, manifested in cell-to-cell differences in kinetic parameters, can make the system more or less reversible -- and potentially energy efficient -- depending on where the noise is located. This highlights that there evolutionary constraints on the suppression of extrinsic noise, whose origin is in cellular heterogeneity, in addition to intrinsic randomness arising from molecular collisions. Finally, I investigate the partially observed nature of most mRNA expression data which seems to obey detailed balance, yet remains unavoidably out-of-equilibrium.","sentences":["The canonical model of mRNA expression is the telegraph model, describing a gene that switches on and off, subject to transcription and decay.","It describes steady-state mRNA distributions that subscribe to transcription in bursts with first-order decay, referred to as super-Poissonian expression.","Using a telegraph-like model, I propose an answer to the question of why gene expression is bursty in the first place, and what benefits it confers.","Using analytics for the entropy production rate, I find that entropy production is maximal when the on and off switching rates between the gene states are approximately equal.","This is related to a lower bound on the free energy necessary to keep the system out of equilibrium, meaning that bursty gene expression may have evolved in part due to free energy efficiency.","It is shown that there are trade-offs between having slow nuclear export, which can reduce cytoplasmic mRNA noise, and the energy required to keep the system out of equilibrium -- nuclear compartmentalization comes with an associated free energy cost.","At the population level, I find that extrinsic variation, manifested in cell-to-cell differences in kinetic parameters, can make the system more or less reversible -- and potentially energy efficient -- depending on where the noise is located.","This highlights that there evolutionary constraints on the suppression of extrinsic noise, whose origin is in cellular heterogeneity, in addition to intrinsic randomness arising from molecular collisions.","Finally, I investigate the partially observed nature of most mRNA expression data which seems to obey detailed balance, yet remains unavoidably out-of-equilibrium."],"url":"http://arxiv.org/abs/2405.12897v1","category":"q-bio.SC"}
{"created":"2024-05-21 15:55:14","title":"Featuring nuanced electronic band structure in gapped multilayer graphene","abstract":"Moir\\'e systems featuring flat electronic bands exhibit a vast landscape of emergent exotic quantum states, making them one of the resourceful platforms in condensed matter physics in recent times. Tuning these systems via twist angle and the electric field greatly enhances our comprehension of their strongly correlated ground states. Here, we report a technique to investigate the nuanced intricacies of band structures in dual-gated multilayer graphene systems. We utilize the Landau levels of a decoupled monolayer graphene to extract the electric field-dependent bilayer graphene charge neutrality point gap. Then, we extend this method to analyze the evolution of the band gap and the flat bandwidth in twisted mono-bilayer graphene. The band gap maximizes at the same displacement field where the flat bandwidth minimizes, indicating the strongest electron-electron correlation, which is supported by directly observing the emergence of a strongly correlated phase. Moreover, we extract integer and fractional gaps to further demonstrate the strength of this method. Our technique gives a new perspective and paves the way for improving the understanding of electronic band structure in versatile flat band systems.","sentences":["Moir\\'e systems featuring flat electronic bands exhibit a vast landscape of emergent exotic quantum states, making them one of the resourceful platforms in condensed matter physics in recent times.","Tuning these systems via twist angle and the electric field greatly enhances our comprehension of their strongly correlated ground states.","Here, we report a technique to investigate the nuanced intricacies of band structures in dual-gated multilayer graphene systems.","We utilize the Landau levels of a decoupled monolayer graphene to extract the electric field-dependent bilayer graphene charge neutrality point gap.","Then, we extend this method to analyze the evolution of the band gap and the flat bandwidth in twisted mono-bilayer graphene.","The band gap maximizes at the same displacement field where the flat bandwidth minimizes, indicating the strongest electron-electron correlation, which is supported by directly observing the emergence of a strongly correlated phase.","Moreover, we extract integer and fractional gaps to further demonstrate the strength of this method.","Our technique gives a new perspective and paves the way for improving the understanding of electronic band structure in versatile flat band systems."],"url":"http://arxiv.org/abs/2405.12885v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 15:54:03","title":"Centralized vs Decentralized Monitors for Hyperproperties","abstract":"This paper focuses on the runtime verification of hyperproperties expressed in HypermuHML, an expressive yet simple logic for describing properties of sets of traces. To this end, we first consider a simple language of monitors that can observe sets of system executions and report verdicts w.r.t. a given HypermuHML formula. In this setting, a unique omniscient monitor observes all system traces, and, in this sense, it is 'centralized'. However, in a possibly distributed system, having a centralized entity is undesirable; hence, we also provide a language for 'decentralized' monitors, where each trace has its own monitor, and monitors for different traces can yield a unique verdict by communicating their observations. For both the centralized and the decentralized settings, we provide a synthesis procedure that, given a formula, yields a monitor that is correct (i.e., sound and violation complete). A key step in proving the correctness of the synthesis for decentralized monitors is a result showing that, for each formula, the synthesized centralized monitor and its corresponding decentralized one are weakly bisimilar for a suitable notion of weak bisimulation.","sentences":["This paper focuses on the runtime verification of hyperproperties expressed in HypermuHML, an expressive yet simple logic for describing properties of sets of traces.","To this end, we first consider a simple language of monitors that can observe sets of system executions and report verdicts w.r.t.","a given HypermuHML formula.","In this setting, a unique omniscient monitor observes all system traces, and, in this sense, it is 'centralized'.","However, in a possibly distributed system, having a centralized entity is undesirable; hence, we also provide a language for 'decentralized' monitors, where each trace has its own monitor, and monitors for different traces can yield a unique verdict by communicating their observations.","For both the centralized and the decentralized settings, we provide a synthesis procedure that, given a formula, yields a monitor that is correct (i.e., sound and violation complete).","A key step in proving the correctness of the synthesis for decentralized monitors is a result showing that, for each formula, the synthesized centralized monitor and its corresponding decentralized one are weakly bisimilar for a suitable notion of weak bisimulation."],"url":"http://arxiv.org/abs/2405.12882v1","category":"cs.LO"}
{"created":"2024-05-21 15:39:11","title":"Efficient Influence Minimization via Node Blocking","abstract":"Given a graph G, a budget k and a misinformation seed set S, Influence Minimization (IMIN) via node blocking aims to find a set of k nodes to be blocked such that the expected spread of S is minimized. This problem finds important applications in suppressing the spread of misinformation and has been extensively studied in the literature. However, existing solutions for IMIN still incur significant computation overhead, especially when k becomes large. In addition, there is still no approximation solution with non-trivial theoretical guarantee for IMIN via node blocking prior to our work. In this paper, we conduct the first attempt to propose algorithms that yield data-dependent approximation guarantees. Based on the Sandwich framework, we first develop submodular and monotonic lower and upper bounds for our non-submodular objective function and prove the computation of proposed bounds is \\#P-hard. In addition, two advanced sampling methods are proposed to estimate the value of bounding functions. Moreover, we develop two novel martingale-based concentration bounds to reduce the sample complexity and design two non-trivial algorithms that provide (1-1/e-\\epsilon)-approximate solutions to our bounding functions. Comprehensive experiments on 9 real-world datasets are conducted to validate the efficiency and effectiveness of the proposed techniques. Compared with the state-of-the-art methods, our solutions can achieve up to two orders of magnitude speedup and provide theoretical guarantees for the quality of returned results.","sentences":["Given a graph G, a budget k and a misinformation seed set S, Influence Minimization (IMIN) via node blocking aims to find a set of k nodes to be blocked such that the expected spread of S is minimized.","This problem finds important applications in suppressing the spread of misinformation and has been extensively studied in the literature.","However, existing solutions for IMIN still incur significant computation overhead, especially when k becomes large.","In addition, there is still no approximation solution with non-trivial theoretical guarantee for IMIN via node blocking prior to our work.","In this paper, we conduct the first attempt to propose algorithms that yield data-dependent approximation guarantees.","Based on the Sandwich framework, we first develop submodular and monotonic lower and upper bounds for our non-submodular objective function and prove the computation of proposed bounds is \\#P-hard.","In addition, two advanced sampling methods are proposed to estimate the value of bounding functions.","Moreover, we develop two novel martingale-based concentration bounds to reduce the sample complexity and design two non-trivial algorithms that provide (1-1/e-\\epsilon)-approximate solutions to our bounding functions.","Comprehensive experiments on 9 real-world datasets are conducted to validate the efficiency and effectiveness of the proposed techniques.","Compared with the state-of-the-art methods, our solutions can achieve up to two orders of magnitude speedup and provide theoretical guarantees for the quality of returned results."],"url":"http://arxiv.org/abs/2405.12871v1","category":"cs.DB"}
{"created":"2024-05-21 15:28:05","title":"Principal component analysis of absorbing state phase transitions","abstract":"We perform a principal component analysis (PCA) of two one-dimensional lattice models belonging to distinct nonequilibrium universality classes - directed bond percolation and branching and annihilating random walks with even number of offspring. We find that the uncentered PCA of datasets storing various system's configurations can be successfully used to determine the critical properties of these nonequilibrium phase transitions. In particular, in both cases, we obtain good estimates of the critical point and the dynamical critical exponent of the models. For directed bond percolation we are, furthermore, able to extract critical exponents associated with the correlation length and the order parameter. We discuss the relation of our analysis with low-rank approximations of datasets.","sentences":["We perform a principal component analysis (PCA) of two one-dimensional lattice models belonging to distinct nonequilibrium universality classes - directed bond percolation and branching and annihilating random walks with even number of offspring.","We find that the uncentered PCA of datasets storing various system's configurations can be successfully used to determine the critical properties of these nonequilibrium phase transitions.","In particular, in both cases, we obtain good estimates of the critical point and the dynamical critical exponent of the models.","For directed bond percolation we are, furthermore, able to extract critical exponents associated with the correlation length and the order parameter.","We discuss the relation of our analysis with low-rank approximations of datasets."],"url":"http://arxiv.org/abs/2405.12863v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-21 15:14:16","title":"High-Field Microscale NMR Spectroscopy with NV Centers in Dipolarly-Coupled Samples","abstract":"Diamond-based quantum sensors have enabled high-resolution NMR spectroscopy at the microscale in scenarios where fast molecular motion averages out dipolar interactions among target nuclei. However, in samples with low-diffusion, ubiquitous dipolar couplings challenge the extraction of relevant spectroscopic information. In this work we present a protocol that enables the scanning of nuclear spins in dipolarly-coupled samples at high magnetic fields with a sensor based on nitrogen vacancy (NV) ensembles. Our protocol is based on the synchronized delivery of radio frequency (RF) and microwave (MW) radiation to eliminate couplings among nuclei in the scanned sample and to efficiently extract target energy-shifts from the sample's magnetization dynamics. In addition, the method is designed to operate at high magnetic fields leading to a larger sample thermal polarization, thus to an increased NMR signal. The precision of our method is ultimately limited by the coherence time of the sample, allowing for accurate identification of relevant energy shifts in solid-state systems.","sentences":["Diamond-based quantum sensors have enabled high-resolution NMR spectroscopy at the microscale in scenarios where fast molecular motion averages out dipolar interactions among target nuclei.","However, in samples with low-diffusion, ubiquitous dipolar couplings challenge the extraction of relevant spectroscopic information.","In this work we present a protocol that enables the scanning of nuclear spins in dipolarly-coupled samples at high magnetic fields with a sensor based on nitrogen vacancy (NV) ensembles.","Our protocol is based on the synchronized delivery of radio frequency (RF) and microwave (MW) radiation to eliminate couplings among nuclei in the scanned sample and to efficiently extract target energy-shifts from the sample's magnetization dynamics.","In addition, the method is designed to operate at high magnetic fields leading to a larger sample thermal polarization, thus to an increased NMR signal.","The precision of our method is ultimately limited by the coherence time of the sample, allowing for accurate identification of relevant energy shifts in solid-state systems."],"url":"http://arxiv.org/abs/2405.12857v1","category":"quant-ph"}
{"created":"2024-05-21 15:11:11","title":"Application Layer Cyber Deception without Developer Interaction","abstract":"Cyber deception techniques that are tightly intertwined with applications pose significant technical challenges in production systems. Security measures are usually the responsibility of a system operator, but they are typically limited to accessing built software artifacts, not their source code. This limitation makes it particularly challenging to deploy cyber deception techniques at application runtime and without full control over the software development lifecycle. This work reviews 19 technical methods to accomplish this and evaluates them based on technical, topological, operational, and efficacy properties. We find some novel techniques beyond honeypots and reverse proxies that seem to have received little research interest despite their promise for cyber deception. We believe that overcoming these technical challenges can drive the adoption of more dynamic and personalized cyber deception techniques, tailored to specific classes of applications.","sentences":["Cyber deception techniques that are tightly intertwined with applications pose significant technical challenges in production systems.","Security measures are usually the responsibility of a system operator, but they are typically limited to accessing built software artifacts, not their source code.","This limitation makes it particularly challenging to deploy cyber deception techniques at application runtime and without full control over the software development lifecycle.","This work reviews 19 technical methods to accomplish this and evaluates them based on technical, topological, operational, and efficacy properties.","We find some novel techniques beyond honeypots and reverse proxies that seem to have received little research interest despite their promise for cyber deception.","We believe that overcoming these technical challenges can drive the adoption of more dynamic and personalized cyber deception techniques, tailored to specific classes of applications."],"url":"http://arxiv.org/abs/2405.12852v1","category":"cs.CR"}
{"created":"2024-05-21 14:58:24","title":"A conservative relaxation Crank-Nicolson finite element method for the Schr\u00f6dinger-Poisson equation","abstract":"In this paper, we propose a novel mass and energy conservative relaxation Crank-Nicolson finite element method for the Schr\\\"{o}dinger-Poisson equation. Utilizing only a single auxiliary variable, we simultaneously reformulate the distinct nonlinear terms present in both the Schr\\\"{o}dinger equation and the Poisson equation into their equivalent expressions, constructing an equivalent system to the original Schr\\\"{o}dinger-Poisson equation. Our proposed scheme, derived from this new system, operates linearly and bypasses the need to solve the nonlinear coupled equation, thus eliminating the requirement for iterative techniques. We in turn rigorously derive error estimates for the proposed scheme, demonstrating second-order accuracy in time and $(k+1)$th order accuracy in space when employing polynomials of degree up to $k$. Numerical experiments validate the accuracy and effectiveness of our method and emphasize its conservation properties over long-time simulations.","sentences":["In this paper, we propose a novel mass and energy conservative relaxation Crank-Nicolson finite element method for the Schr\\\"{o}dinger-Poisson equation.","Utilizing only a single auxiliary variable, we simultaneously reformulate the distinct nonlinear terms present in both the Schr\\\"{o}dinger equation and the Poisson equation into their equivalent expressions, constructing an equivalent system to the original Schr\\\"{o}dinger-Poisson equation.","Our proposed scheme, derived from this new system, operates linearly and bypasses the need to solve the nonlinear coupled equation, thus eliminating the requirement for iterative techniques.","We in turn rigorously derive error estimates for the proposed scheme, demonstrating second-order accuracy in time and $(k+1)$th order accuracy in space when employing polynomials of degree up to $k$. Numerical experiments validate the accuracy and effectiveness of our method and emphasize its conservation properties over long-time simulations."],"url":"http://arxiv.org/abs/2405.12848v1","category":"math.NA"}
{"created":"2024-05-21 14:56:33","title":"Thermally emitting isolated neutron star candidates from the SRG/eROSITA All-Sky Survey","abstract":"The SRG/eROSITA All-Sky Survey (eRASS) allows for the creation of a complete sample of X-ray dim isolated neutron stars (XDINSs), which will significantly facilitate the study of their population properties, evolution, and connection to other families of isolated neutron stars (INSs). In this work, we conduct a systematic search for XDINSs on the western Galactic hemisphere and discuss the resulting candidate sample. Consistently with the properties of the known XDINSs, we selected all eRASS sources possessing a soft X-ray spectral distribution and that are unlikely to be associated with optical or infrared sources. Our selection criteria allowed us to recover all known XDINSs and previously proposed candidates. In addition, we put forward 33 new candidate members for dedicated follow-up identification campaigns. We found the resulting candidate sample to be about 30-50% complete, mainly due to source confusion and the stringent cross-matching criteria adopted. The candidates of the sample presented here can be divided into two groups: 13 soft and 20 somewhat hard X-ray emitters. Interestingly, the thermal nature, spatial distribution, lack of known counterparts, and absence of significant flux variability of the candidates in the first group agree well with the properties of other confirmed thermally emitting INSs. For the candidates in the second group, the current observational data do not allow one to discern between rotation-powered or recycled pulsars, cataclysmic variables, or quiescent neutron stars in binary systems or even to rule out an extragalactic nature. On the basis of population synthesis and the estimated source completeness of the search, we expect that between one and three new XDINSs are among the already singled-out list of XDINS candidates - a long-sought increase in the proposed number of members of this elusive class of X-ray emitters.","sentences":["The SRG/eROSITA All-Sky Survey (eRASS) allows for the creation of a complete sample of X-ray dim isolated neutron stars (XDINSs), which will significantly facilitate the study of their population properties, evolution, and connection to other families of isolated neutron stars (INSs).","In this work, we conduct a systematic search for XDINSs on the western Galactic hemisphere and discuss the resulting candidate sample.","Consistently with the properties of the known XDINSs, we selected all eRASS sources possessing a soft X-ray spectral distribution and that are unlikely to be associated with optical or infrared sources.","Our selection criteria allowed us to recover all known XDINSs and previously proposed candidates.","In addition, we put forward 33 new candidate members for dedicated follow-up identification campaigns.","We found the resulting candidate sample to be about 30-50% complete, mainly due to source confusion and the stringent cross-matching criteria adopted.","The candidates of the sample presented here can be divided into two groups: 13 soft and 20 somewhat hard X-ray emitters.","Interestingly, the thermal nature, spatial distribution, lack of known counterparts, and absence of significant flux variability of the candidates in the first group agree well with the properties of other confirmed thermally emitting INSs.","For the candidates in the second group, the current observational data do not allow one to discern between rotation-powered or recycled pulsars, cataclysmic variables, or quiescent neutron stars in binary systems or even to rule out an extragalactic nature.","On the basis of population synthesis and the estimated source completeness of the search, we expect that between one and three new XDINSs are among the already singled-out list of XDINS candidates - a long-sought increase in the proposed number of members of this elusive class of X-ray emitters."],"url":"http://arxiv.org/abs/2405.12846v1","category":"astro-ph.HE"}
{"created":"2024-05-21 14:45:34","title":"GotFunding: A grant recommendation system based on scientific articles","abstract":"Obtaining funding is an important part of becoming a successful scientist. Junior faculty spend a great deal of time finding the right agencies and programs that best match their research profile. But what are the factors that influence the best publication--grant matching? Some universities might employ pre-award personnel to understand these factors, but not all institutions can afford to hire them. Historical records of publications funded by grants can help us understand the matching process and also help us develop recommendation systems to automate it. In this work, we present \\textsc{GotFunding} (Grant recOmmendaTion based on past FUNDING), a recommendation system trained on National Institutes of Health's (NIH) grant--publication records. Our system achieves a high performance (NDCG@1 = 0.945) by casting the problem as learning to rank. By analyzing the features that make predictions effective, our results show that the ranking considers most important 1) the year difference between publication and grant grant, 2) the amount of information provided in the publication, and 3) the relevance of the publication to the grant. We discuss future improvements of the system and an online tool for scientists to try.","sentences":["Obtaining funding is an important part of becoming a successful scientist.","Junior faculty spend a great deal of time finding the right agencies and programs that best match their research profile.","But what are the factors that influence the best publication--grant matching?","Some universities might employ pre-award personnel to understand these factors, but not all institutions can afford to hire them.","Historical records of publications funded by grants can help us understand the matching process and also help us develop recommendation systems to automate it.","In this work, we present \\textsc{GotFunding} (Grant recOmmendaTion based on past FUNDING), a recommendation system trained on National Institutes of Health's (NIH) grant--publication records.","Our system achieves a high performance (NDCG@1 = 0.945) by casting the problem as learning to rank.","By analyzing the features that make predictions effective, our results show that the ranking considers most important 1) the year difference between publication and grant grant, 2) the amount of information provided in the publication, and 3) the relevance of the publication to the grant.","We discuss future improvements of the system and an online tool for scientists to try."],"url":"http://arxiv.org/abs/2405.12840v1","category":"cs.IR"}
{"created":"2024-05-21 14:44:15","title":"An Experimental Study of C-Band Channel Model in Integrated LEO Satellite and Terrestrial Systems","abstract":"This paper studies the channel model for the integrated satellite-terrestrial networks operating at C-band under deployment in dense urban and rural areas. Particularly, the interference channel from the low-earth-orbit (LEO) satellite to the dense urban area is analyzed carefully under the impact of the environment's characteristics, i.e., the building density, building height, and the elevation angle. Subsequently, the experimental results show the strong relationships between these characteristics and the channel gain loss. Especially, the functions of channel gain loss are obtained by utilizing the model-fitting approach that can be used as the basis for studying future works of integration of satellite and terrestrial networks (ISTNs).","sentences":["This paper studies the channel model for the integrated satellite-terrestrial networks operating at C-band under deployment in dense urban and rural areas.","Particularly, the interference channel from the low-earth-orbit (LEO) satellite to the dense urban area is analyzed carefully under the impact of the environment's characteristics, i.e., the building density, building height, and the elevation angle.","Subsequently, the experimental results show the strong relationships between these characteristics and the channel gain loss.","Especially, the functions of channel gain loss are obtained by utilizing the model-fitting approach that can be used as the basis for studying future works of integration of satellite and terrestrial networks (ISTNs)."],"url":"http://arxiv.org/abs/2405.12839v1","category":"eess.SP"}
{"created":"2024-05-21 14:40:34","title":"Lagrangian multiform for cyclotomic Gaudin models","abstract":"We construct a Lagrangian multiform for the class of cyclotomic (rational) Gaudin models by formulating its hierarchy within the Lie dialgebra framework of Semenov-Tian-Shansky and by using the framework of Lagrangian multiforms on coadjoint orbits. This provides the first example of a Lagrangian multiform for an integrable hierarchy whose classical $r$-matrix is non-skew-symmetric and spectral parameter-dependent. As an important by-product of the construction, we obtain a Lagrangian multiform for the periodic Toda chain by choosing an appropriate realisation of the cyclotomic Gaudin Lax matrix. This fills a gap in the landscape of Toda models as only the open and infinite chains had been previously cast into the Lagrangian multiform framework. A slightly different choice of realisation produces the so-called DST model. We demonstrate the versatility of the framework by coupling the periodic Toda chain with the DST model and by obtaining a Lagrangian multiform for the corresponding integrable hierarchy.","sentences":["We construct a Lagrangian multiform for the class of cyclotomic (rational) Gaudin models by formulating its hierarchy within the Lie dialgebra framework of Semenov-Tian-Shansky and by using the framework of Lagrangian multiforms on coadjoint orbits.","This provides the first example of a Lagrangian multiform for an integrable hierarchy whose classical $r$-matrix is non-skew-symmetric and spectral parameter-dependent.","As an important by-product of the construction, we obtain a Lagrangian multiform for the periodic Toda chain by choosing an appropriate realisation of the cyclotomic Gaudin Lax matrix.","This fills a gap in the landscape of Toda models as only the open and infinite chains had been previously cast into the Lagrangian multiform framework.","A slightly different choice of realisation produces the so-called DST model.","We demonstrate the versatility of the framework by coupling the periodic Toda chain with the DST model and by obtaining a Lagrangian multiform for the corresponding integrable hierarchy."],"url":"http://arxiv.org/abs/2405.12837v1","category":"math-ph"}
{"created":"2024-05-21 14:38:25","title":"$SU(2)$-bundles over highly connected $8$-manifolds","abstract":"In this paper, we analyze the possible homotopy types of the total space of a principal $SU(2)$-bundle over a $3$-connected $8$-dimensional Poincar\\'{e} duality complex. Along the way, we also classify the $3$-connected $11$-dimensional complexes $E$ formed from a wedge of $S^4$ and $S^7$ by attaching a $11$-cell.","sentences":["In this paper, we analyze the possible homotopy types of the total space of a principal $SU(2)$-bundle over a $3$-connected $8$-dimensional Poincar\\'{e} duality complex.","Along the way, we also classify the $3$-connected $11$-dimensional complexes $E$ formed from a wedge of $S^4$ and $S^7$ by attaching a $11$-cell."],"url":"http://arxiv.org/abs/2405.12835v1","category":"math.AT"}
{"created":"2024-05-21 14:29:14","title":"Phase-field analysis for brittle fracture in ferroelectric materials with flexoelectric effect","abstract":"Understanding the nature of brittle failure in ferroelectric materials is essential, but difficult due to the complex interaction between mechanical and electrical concentrated fields near the crack tip. In this work, an extended phase-field model incorporating multiple order parameters is constructed to analyze the coupled evolution of fracture and domain behavior in ferroelectric materials. The strain gradient is incorporated into the governing equations to evaluate the impact of the flexoelectric effect during the crack propagation process. Our advanced phase-field model demonstrated that, with the consideration of the flexoelectric effect, both the crack extension rate and crack path are related to the initial polarization direction. This phenomenon is associated with the eigenstrain induced by the flexoelectric effect. This study provides in-depth insight into the fracture behavior of ferroelectric materials. The developed model framework can also be employed to investigate electromechanical coupling failures in more complex ferroelectric structures.","sentences":["Understanding the nature of brittle failure in ferroelectric materials is essential, but difficult due to the complex interaction between mechanical and electrical concentrated fields near the crack tip.","In this work, an extended phase-field model incorporating multiple order parameters is constructed to analyze the coupled evolution of fracture and domain behavior in ferroelectric materials.","The strain gradient is incorporated into the governing equations to evaluate the impact of the flexoelectric effect during the crack propagation process.","Our advanced phase-field model demonstrated that, with the consideration of the flexoelectric effect, both the crack extension rate and crack path are related to the initial polarization direction.","This phenomenon is associated with the eigenstrain induced by the flexoelectric effect.","This study provides in-depth insight into the fracture behavior of ferroelectric materials.","The developed model framework can also be employed to investigate electromechanical coupling failures in more complex ferroelectric structures."],"url":"http://arxiv.org/abs/2405.12826v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 14:17:17","title":"Non-equilibrium dynamic hyperuniform states","abstract":"Disordered hyperuniform structures are an exotic state of matter having suppressed density fluctuations at large length-scale similar to perfect crystals and quasicrystals but without any long range orientational order. In the past decade, an increasing number of non-equilibrium systems were found to have dynamic hyperuniform states, which have emerged as a new research direction coupling both non-equilibrium physics and hyperuniformity. Here we review the recent progress in understanding dynamic hyperuniform states found in various non-equilibrium systems, including the critical hyperuniformity in absorbing phase transitions, non-equilibrium hyperuniform fluids and the hyperuniform structures in phase separating systems via spinodal decomposition.","sentences":["Disordered hyperuniform structures are an exotic state of matter having suppressed density fluctuations at large length-scale similar to perfect crystals and quasicrystals but without any long range orientational order.","In the past decade, an increasing number of non-equilibrium systems were found to have dynamic hyperuniform states, which have emerged as a new research direction coupling both non-equilibrium physics and hyperuniformity.","Here we review the recent progress in understanding dynamic hyperuniform states found in various non-equilibrium systems, including the critical hyperuniformity in absorbing phase transitions, non-equilibrium hyperuniform fluids and the hyperuniform structures in phase separating systems via spinodal decomposition."],"url":"http://arxiv.org/abs/2405.12818v1","category":"cond-mat.soft"}
{"created":"2024-05-21 14:16:48","title":"Curve of Growth Analysis of SZ Lyn","abstract":"We present one high-resolution and a time series of 561 low-resolution follow-up spectroscopic observations of SZ Lyn. It is a high-amplitude Delta Scuti-type pulsating star in a binary system. The photometric observations reveal the existence of radial and non-radial oscillation modes in SZ Lyn. In spectroscopy, the variation of equivalent width of the line profiles reflects the temperature variations. The equivalent widths of the Balmer lines, H-alpha, H-hbeta, and H-gamma were measured over the pulsation cycle of SZ Lyn using time sequence spectra. Hence, the temperature profile of SZ Lyn was derived using the curve of growth analysis. Furthermore, the stellar parameters were determined through the best fit analysis of observed and synthetic high-resolution spectral lines. The best fit determines a model of Teff=6750 K, log(g)=3.5 dex, and vrot=10 km/s for solar abundance.","sentences":["We present one high-resolution and a time series of 561 low-resolution follow-up spectroscopic observations of SZ Lyn.","It is a high-amplitude Delta Scuti-type pulsating star in a binary system.","The photometric observations reveal the existence of radial and non-radial oscillation modes in SZ Lyn.","In spectroscopy, the variation of equivalent width of the line profiles reflects the temperature variations.","The equivalent widths of the Balmer lines, H-alpha, H-hbeta, and H-gamma were measured over the pulsation cycle of SZ Lyn using time sequence spectra.","Hence, the temperature profile of SZ Lyn was derived using the curve of growth analysis.","Furthermore, the stellar parameters were determined through the best fit analysis of observed and synthetic high-resolution spectral lines.","The best fit determines a model of Teff=6750 K, log(g)=3.5 dex, and vrot=10 km/s for solar abundance."],"url":"http://arxiv.org/abs/2405.12817v1","category":"astro-ph.SR"}
{"created":"2024-05-21 14:15:52","title":"Could a Computer Architect Understand our Brain?","abstract":"This paper presents a highly speculative model encompassing the cortex, thalamus, and hippocampus of the mammalian brain. While the majority of computational neuroscience models are founded upon empirical evidence, this model is predicated upon a hardware proposal for a machine learning accelerator. Such a device was designed to perform a specific task, such as speech recognition. The design process employed the principles and techniques typically used by computer architects in the design of devices such as processors. However, it also sought to maintain plausibility with biological systems in accordance with the current understanding of the mammalian brain. In the course of our research, we have identified a functional framework that may help to fill the gaps in current neuroscience, thereby facilitating the explanations for many elusive cognitive-level effects. This paper does not describe the device itself or the rationale behind the design decision, but instead, it presents a concise description of the derived model. In brief, the model provides a functional definition of the cortical column and its structural definition by the minicolumns. It also offers a descriptive model for the corticothalamic and corticostriatal loops, a functional proposal for the hippocampal complex, and a simplified view of the brainstem circuitry involved in auditory processing. The proposed model appears to provide an explanation for a number of cognitive phenomena, including some ERP effects, bottom-up and top-down attention, and the relationship between phenomena such as the cocktail party effect, anterograde and retrograde amnesia following hippocampal complex damage, and so forth.","sentences":["This paper presents a highly speculative model encompassing the cortex, thalamus, and hippocampus of the mammalian brain.","While the majority of computational neuroscience models are founded upon empirical evidence, this model is predicated upon a hardware proposal for a machine learning accelerator.","Such a device was designed to perform a specific task, such as speech recognition.","The design process employed the principles and techniques typically used by computer architects in the design of devices such as processors.","However, it also sought to maintain plausibility with biological systems in accordance with the current understanding of the mammalian brain.","In the course of our research, we have identified a functional framework that may help to fill the gaps in current neuroscience, thereby facilitating the explanations for many elusive cognitive-level effects.","This paper does not describe the device itself or the rationale behind the design decision, but instead, it presents a concise description of the derived model.","In brief, the model provides a functional definition of the cortical column and its structural definition by the minicolumns.","It also offers a descriptive model for the corticothalamic and corticostriatal loops, a functional proposal for the hippocampal complex, and a simplified view of the brainstem circuitry involved in auditory processing.","The proposed model appears to provide an explanation for a number of cognitive phenomena, including some ERP effects, bottom-up and top-down attention, and the relationship between phenomena such as the cocktail party effect, anterograde and retrograde amnesia following hippocampal complex damage, and so forth."],"url":"http://arxiv.org/abs/2405.12815v1","category":"q-bio.NC"}
{"created":"2024-05-21 14:13:46","title":"A stable poro-mechanical formulation for Material Point Methods leveraging overlapping meshes and multi-field ghost penalisation","abstract":"The Material Point Method (MPM) is widely used to analyse coupled (solid-water) problems under large deformations/displacements. However, if not addressed carefully, MPM u-p formulations for poro-mechanics can be affected by two major sources of instability. Firstly, inf-sup condition violation can arise when the spaces for the displacement and pressure fields are not chosen correctly, resulting in an unstable pressure field. Secondly, the intrinsic nature of particle-based discretisation makes the MPM an unfitted mesh-based method, which can affect the system's condition number and solvability, particularly when background mesh elements are poorly populated. This work proposes a solution to both problems. The inf-sup condition is avoided using two overlapping meshes, a coarser one for the pressure and a finer one for the displacement. This approach does not require stabilisation of the primary equations since it is stable by design and is particularly valuable for low-order shape functions. As for the system's poor condition number, a face ghost penalisation method is added to both the primary equations, which constitutes a novelty in the context of MPM mixed formulations. This study frequently makes use of the theories of functional analysis or the unfitted Finite Element Method (FEM). Although these theories may not directly apply to the MPM, they provide a robust and logical basis for the research. These rationales are further supported by three numerical examples, which encompass both elastic and elasto-plastic cases and drained and undrained conditions.","sentences":["The Material Point Method (MPM) is widely used to analyse coupled (solid-water) problems under large deformations/displacements.","However, if not addressed carefully, MPM u-p formulations for poro-mechanics can be affected by two major sources of instability.","Firstly, inf-sup condition violation can arise when the spaces for the displacement and pressure fields are not chosen correctly, resulting in an unstable pressure field.","Secondly, the intrinsic nature of particle-based discretisation makes the MPM an unfitted mesh-based method, which can affect the system's condition number and solvability, particularly when background mesh elements are poorly populated.","This work proposes a solution to both problems.","The inf-sup condition is avoided using two overlapping meshes, a coarser one for the pressure and a finer one for the displacement.","This approach does not require stabilisation of the primary equations since it is stable by design and is particularly valuable for low-order shape functions.","As for the system's poor condition number, a face ghost penalisation method is added to both the primary equations, which constitutes a novelty in the context of MPM mixed formulations.","This study frequently makes use of the theories of functional analysis or the unfitted Finite Element Method (FEM).","Although these theories may not directly apply to the MPM, they provide a robust and logical basis for the research.","These rationales are further supported by three numerical examples, which encompass both elastic and elasto-plastic cases and drained and undrained conditions."],"url":"http://arxiv.org/abs/2405.12814v1","category":"math.NA"}
{"created":"2024-05-21 13:57:17","title":"Cross-disciplinary Reactor-to-Repository Framework for Evaluating Spent Nuclear Fuel from Advanced Reactors","abstract":"This study presents a cross-disciplinary reactor-to-repository framework to compare different advanced reactors with respect to their spent nuclear fuel (SNF). The framework consists of (1) OpenMC for simulating neutronics, fuel depletion, and radioactive decays; (2) NWPY for computing the repository footprint for SNF disposal given the thermal constraints; and (3) PFLOTRAN for simulating radionuclide transport in the geosphere to compute the peak dose rate, which is used to quantify the repository performance and environmental impact. We first perform the meta-analysis of past comparative analyses to identify the factors led previously to inconsistent conclusions. We then demonstrate the new framework by comparing five reactor types. Significant findings are that (1) the repository footprint is neither linearly related to SNF volume nor to decay heat, due to the repository's thermal constraint, (2) fast reactors have significantly higher I-129 inventory, which is often the primarily dose contributor from repositories, and (3) the repository performance primarily depends on the waste forms. The TRISO-based reactors, in particular, have significantly higher SNF volumes, but result in smaller repository footprints and lower peak dose rates. Our analysis highlights the diversity of these reactors, each of which should be evaluated individually. The open-source framework ensures proper cross-disciplinary connections between reactor simulations and environmental assessments, as well as the transparency/traceability required for such comparative analyses. It aims to support reactor designers, repository developers and policy makers in evaluating the impact of different reactor designs, with the ultimate goal of improving the sustainability of nuclear energy systems.","sentences":["This study presents a cross-disciplinary reactor-to-repository framework to compare different advanced reactors with respect to their spent nuclear fuel (SNF).","The framework consists of (1) OpenMC for simulating neutronics, fuel depletion, and radioactive decays; (2) NWPY for computing the repository footprint for SNF disposal given the thermal constraints; and (3) PFLOTRAN for simulating radionuclide transport in the geosphere to compute the peak dose rate, which is used to quantify the repository performance and environmental impact.","We first perform the meta-analysis of past comparative analyses to identify the factors led previously to inconsistent conclusions.","We then demonstrate the new framework by comparing five reactor types.","Significant findings are that (1) the repository footprint is neither linearly related to SNF volume nor to decay heat, due to the repository's thermal constraint, (2) fast reactors have significantly higher I-129 inventory, which is often the primarily dose contributor from repositories, and (3) the repository performance primarily depends on the waste forms.","The TRISO-based reactors, in particular, have significantly higher SNF volumes, but result in smaller repository footprints and lower peak dose rates.","Our analysis highlights the diversity of these reactors, each of which should be evaluated individually.","The open-source framework ensures proper cross-disciplinary connections between reactor simulations and environmental assessments, as well as the transparency/traceability required for such comparative analyses.","It aims to support reactor designers, repository developers and policy makers in evaluating the impact of different reactor designs, with the ultimate goal of improving the sustainability of nuclear energy systems."],"url":"http://arxiv.org/abs/2405.12805v1","category":"physics.app-ph"}
{"created":"2024-05-21 13:51:47","title":"Deep Reinforcement Learning for Time-Critical Wilderness Search And Rescue Using Drones","abstract":"Traditional search and rescue methods in wilderness areas can be time-consuming and have limited coverage. Drones offer a faster and more flexible solution, but optimizing their search paths is crucial. This paper explores the use of deep reinforcement learning to create efficient search missions for drones in wilderness environments. Our approach leverages a priori data about the search area and the missing person in the form of a probability distribution map. This allows the deep reinforcement learning agent to learn optimal flight paths that maximize the probability of finding the missing person quickly. Experimental results show that our method achieves a significant improvement in search times compared to traditional coverage planning and search planning algorithms. In one comparison, deep reinforcement learning is found to outperform other algorithms by over $160\\%$, a difference that can mean life or death in real-world search operations. Additionally, unlike previous work, our approach incorporates a continuous action space enabled by cubature, allowing for more nuanced flight patterns.","sentences":["Traditional search and rescue methods in wilderness areas can be time-consuming and have limited coverage.","Drones offer a faster and more flexible solution, but optimizing their search paths is crucial.","This paper explores the use of deep reinforcement learning to create efficient search missions for drones in wilderness environments.","Our approach leverages a priori data about the search area and the missing person in the form of a probability distribution map.","This allows the deep reinforcement learning agent to learn optimal flight paths that maximize the probability of finding the missing person quickly.","Experimental results show that our method achieves a significant improvement in search times compared to traditional coverage planning and search planning algorithms.","In one comparison, deep reinforcement learning is found to outperform other algorithms by over $160\\%$, a difference that can mean life or death in real-world search operations.","Additionally, unlike previous work, our approach incorporates a continuous action space enabled by cubature, allowing for more nuanced flight patterns."],"url":"http://arxiv.org/abs/2405.12800v1","category":"cs.RO"}
{"created":"2024-05-21 13:43:05","title":"Large deviation for Gibbs probabilities at zero temperature and invariant idempotent probabilities for iterated function systems","abstract":"We consider two compact metric spaces $J$ and $X$ and a uniform contractible iterated function system $\\{\\phi_j: X \\to X \\, | \\, j \\in J \\}$. For a Lipschitz continuous function $A$ on $J \\times X$ and for each $\\beta>0$ we consider the Gibbs probability $\\rho_{_{\\beta A}}$. Our goal is to study a large deviation principle for such family of probabilities as $\\beta \\to +\\infty$ and its connections with idempotent probabilities. In the non-place dependent case ($A(j,x)=A_j,\\,\\forall x\\in X$) we will prove that $(\\rho_{_{\\beta A}})$ satisfy a LDP and $-I$ (where $I$ is the deviation function) is the density of the unique invariant idempotent probability for a mpIFS associated to $A$. In the place dependent case, we prove that, if $(\\rho_{_{\\beta A}})$ satisfy a LDP, then $-I$ is the density of an invariant idempotent probability. Such idempotent probabilities were recently characterized through the Ma\\~{n}\\'{e} potential and Aubry set, therefore we will obtain an identical characterization for $-I$.","sentences":["We consider two compact metric spaces $J$ and $X$ and a uniform contractible iterated function system $\\{\\phi_j: X \\to X \\, | \\, j \\in J \\}$. For a Lipschitz continuous function $A$ on $J \\times X$ and for each $\\beta>0$ we consider the Gibbs probability $\\rho_{_{\\beta A}}$. Our goal is to study a large deviation principle for such family of probabilities as $\\beta \\to +\\infty$ and its connections with idempotent probabilities.","In the non-place dependent case ($A(j,x)=A_j,\\,\\forall x\\in X$) we will prove that $(\\rho_{_{\\beta A}})$ satisfy a LDP and $-I$ (where $I$ is the deviation function) is the density of the unique invariant idempotent probability for a mpIFS associated to $A$.","In the place dependent case, we prove that, if $(\\rho_{_{\\beta A}})$ satisfy a LDP, then $-I$ is the density of an invariant idempotent probability.","Such idempotent probabilities were recently characterized through the Ma\\~{n}\\'{e} potential and Aubry set, therefore we will obtain an identical characterization for $-I$."],"url":"http://arxiv.org/abs/2405.12793v1","category":"math.DS"}
{"created":"2024-05-21 13:28:48","title":"Erd\u0151s' problem and $(n, \\frac{1}{3})$-separated sets","abstract":"Inspired by the Erd\\H{o}s' problem in Ramsey theory, we propose a dynamical version of the problem and answer it positively for circle maps.","sentences":["Inspired by the Erd\\H{o}s' problem in Ramsey theory, we propose a dynamical version of the problem and answer it positively for circle maps."],"url":"http://arxiv.org/abs/2405.12782v1","category":"math.DS"}
{"created":"2024-05-21 13:24:49","title":"Investigation of Electron Backscattering on Silicon Drift Detectors for the Sterile Neutrino Search with TRISTAN","abstract":"Sterile neutrinos are hypothetical particles in the minimal extension of the Standard Model of Particle Physics. They could be viable dark matter candidates if they have a mass in the keV range. The Karlsruhe tritium neutrino (KATRIN) experiment, extended with a silicon drift detector focal plane array (TRISTAN), has the potential to search for keV-scale sterile neutrinos by measuring the kinematics of the tritium $\\beta$-decay. The collaboration targets a sensitivity of $10^{-6}$ on the mixing amplitude $\\sin^2{\\Theta}$. For this challenging target, a precise understanding of the detector response is necessary. In this work, we report on the characterization of electron backscattering from the detector surface, which is one of the main effects that influence the shape of the observed energy spectrum. Measurements were performed with a tandem silicon drift detector system and a custom-designed electron source. The measured detector response and backscattering probability are in good agreement with dedicated backscattering simulations using the Geant4 simulation toolkit.","sentences":["Sterile neutrinos are hypothetical particles in the minimal extension of the Standard Model of Particle Physics.","They could be viable dark matter candidates if they have a mass in the keV range.","The Karlsruhe tritium neutrino (KATRIN) experiment, extended with a silicon drift detector focal plane array (TRISTAN), has the potential to search for keV-scale sterile neutrinos by measuring the kinematics of the tritium $\\beta$-decay.","The collaboration targets a sensitivity of $10^{-6}$ on the mixing amplitude $\\sin^2{\\Theta}$. For this challenging target, a precise understanding of the detector response is necessary.","In this work, we report on the characterization of electron backscattering from the detector surface, which is one of the main effects that influence the shape of the observed energy spectrum.","Measurements were performed with a tandem silicon drift detector system and a custom-designed electron source.","The measured detector response and backscattering probability are in good agreement with dedicated backscattering simulations using the Geant4 simulation toolkit."],"url":"http://arxiv.org/abs/2405.12776v1","category":"physics.ins-det"}
{"created":"2024-05-21 13:23:52","title":"More than just smoke and mirrors: Gas-phase polaritons for optical control of chemistry","abstract":"Gas phase molecules are a promising platform through which to elucidate the mechanisms of action and scope of polaritons for optical control of chemistry. Polaritons arise from the strong coupling of a dipole-allowed molecular transition with the photonic mode of an optical cavity. There is mounting evidence of modified reactivity under polaritonic conditions; however, the complex condensed-phase environment of most experimental demonstrations impedes mechanistic understanding of this phenomenon. While the gas phase was the playground of early efforts in atomic cavity quantum electrodynamics, we have only recently demonstrated the formation of molecular polaritons under these conditions. Studying the reactivity of isolated gas-phase molecules under strong coupling would eliminate solvent interactions and enable quantum state resolution of reaction progress. In this Perspective, we contextualize recent gas-phase efforts in the field of polariton chemistry and offer a practical guide for experiment design moving forward.","sentences":["Gas phase molecules are a promising platform through which to elucidate the mechanisms of action and scope of polaritons for optical control of chemistry.","Polaritons arise from the strong coupling of a dipole-allowed molecular transition with the photonic mode of an optical cavity.","There is mounting evidence of modified reactivity under polaritonic conditions; however, the complex condensed-phase environment of most experimental demonstrations impedes mechanistic understanding of this phenomenon.","While the gas phase was the playground of early efforts in atomic cavity quantum electrodynamics, we have only recently demonstrated the formation of molecular polaritons under these conditions.","Studying the reactivity of isolated gas-phase molecules under strong coupling would eliminate solvent interactions and enable quantum state resolution of reaction progress.","In this Perspective, we contextualize recent gas-phase efforts in the field of polariton chemistry and offer a practical guide for experiment design moving forward."],"url":"http://arxiv.org/abs/2405.12772v1","category":"physics.chem-ph"}
{"created":"2024-05-21 13:10:43","title":"Cross-spectral Gated-RGB Stereo Depth Estimation","abstract":"Gated cameras flood-illuminate a scene and capture the time-gated impulse response of a scene. By employing nanosecond-scale gates, existing sensors are capable of capturing mega-pixel gated images, delivering dense depth improving on today's LiDAR sensors in spatial resolution and depth precision. Although gated depth estimation methods deliver a million of depth estimates per frame, their resolution is still an order below existing RGB imaging methods. In this work, we combine high-resolution stereo HDR RCCB cameras with gated imaging, allowing us to exploit depth cues from active gating, multi-view RGB and multi-view NIR sensing -- multi-view and gated cues across the entire spectrum. The resulting capture system consists only of low-cost CMOS sensors and flood-illumination. We propose a novel stereo-depth estimation method that is capable of exploiting these multi-modal multi-view depth cues, including the active illumination that is measured by the RCCB camera when removing the IR-cut filter. The proposed method achieves accurate depth at long ranges, outperforming the next best existing method by 39% for ranges of 100 to 220m in MAE on accumulated LiDAR ground-truth. Our code, models and datasets are available at https://light.princeton.edu/gatedrccbstereo/ .","sentences":["Gated cameras flood-illuminate a scene and capture the time-gated impulse response of a scene.","By employing nanosecond-scale gates, existing sensors are capable of capturing mega-pixel gated images, delivering dense depth improving on today's LiDAR sensors in spatial resolution and depth precision.","Although gated depth estimation methods deliver a million of depth estimates per frame, their resolution is still an order below existing RGB imaging methods.","In this work, we combine high-resolution stereo HDR RCCB cameras with gated imaging, allowing us to exploit depth cues from active gating, multi-view RGB and multi-view NIR sensing -- multi-view and gated cues across the entire spectrum.","The resulting capture system consists only of low-cost CMOS sensors and flood-illumination.","We propose a novel stereo-depth estimation method that is capable of exploiting these multi-modal multi-view depth cues, including the active illumination that is measured by the RCCB camera when removing the IR-cut filter.","The proposed method achieves accurate depth at long ranges, outperforming the next best existing method by 39% for ranges of 100 to 220m in MAE on accumulated LiDAR ground-truth.","Our code, models and datasets are available at https://light.princeton.edu/gatedrccbstereo/ ."],"url":"http://arxiv.org/abs/2405.12759v1","category":"cs.CV"}
{"created":"2024-05-21 13:01:17","title":"The hBN defects database: a theoretical compilation of color centers in hexagonal boron nitride","abstract":"Color centers in hexagonal boron nitride (hBN) have become an intensively researched system due to their potential applications in quantum technologies. There has been a large variety of defects being fabricated, yet, for many of them, the atomic origin remains unclear. The direct imaging of the defect is technically very challenging, in particular since, in a diffraction-limited spot, there are many defects and then one has to identify the one that is optically active. Another approach is to compare the photophysical properties with theoretical simulations and identify which defect has a matching signature. It has been shown that a single property for this is insufficient and causes misassignments. Here, we publish a density functional theory (DFT)-based searchable online database covering the electronic structure of hBN defects (257 triplet and 211 singlet configurations), as well as their photophysical fingerprint (excited state lifetime, quantum efficiency, transition dipole moment and orientation, polarization visibility, and many more). All data is open-source and publicly accessible at https://h-bn.info and can be downloaded. It is possible to enter the experimentally observed defect signature and the database will output possible candidates which can be narrowed down by entering as many observed properties as possible. The database will be continuously updated with more defects and new photophysical properties (which can also be specifically requested by any users). The database therefore allows one to reliably identify defects but also investigate which defects might be promising for magnetic field sensing or quantum memory applications.","sentences":["Color centers in hexagonal boron nitride (hBN) have become an intensively researched system due to their potential applications in quantum technologies.","There has been a large variety of defects being fabricated, yet, for many of them, the atomic origin remains unclear.","The direct imaging of the defect is technically very challenging, in particular since, in a diffraction-limited spot, there are many defects and then one has to identify the one that is optically active.","Another approach is to compare the photophysical properties with theoretical simulations and identify which defect has a matching signature.","It has been shown that a single property for this is insufficient and causes misassignments.","Here, we publish a density functional theory (DFT)-based searchable online database covering the electronic structure of hBN defects (257 triplet and 211 singlet configurations), as well as their photophysical fingerprint (excited state lifetime, quantum efficiency, transition dipole moment and orientation, polarization visibility, and many more).","All data is open-source and publicly accessible at https://h-bn.info and can be downloaded.","It is possible to enter the experimentally observed defect signature and the database will output possible candidates which can be narrowed down by entering as many observed properties as possible.","The database will be continuously updated with more defects and new photophysical properties (which can also be specifically requested by any users).","The database therefore allows one to reliably identify defects but also investigate which defects might be promising for magnetic field sensing or quantum memory applications."],"url":"http://arxiv.org/abs/2405.12749v1","category":"quant-ph"}
{"created":"2024-05-21 12:59:59","title":"Hierarchical Coded Caching with Low Subpacketization and Coding Delay","abstract":"Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \\textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \\textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \\textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.","sentences":["Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory.","Motivated by practical scenarios, Karamchandani \\textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories.","Low subpacketization level coded caching schemes are desirable for practical implementations.","Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \\textit{et al.} in [4].","Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks.","Kong \\textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs.","Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20].","Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network.","This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs.","Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay.","Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters."],"url":"http://arxiv.org/abs/2405.12747v1","category":"cs.IT"}
{"created":"2024-05-21 12:55:26","title":"Phase diagram of the antiferromagnetic $J_1$-$J_2$ spin-$1$ pyrochlore Heisenberg model","abstract":"We study the phase diagram of the antiferromagnetic $J_1$-$J_2$ Heisenberg model on the pyrochlore lattice with $S=1$ spins at zero and finite temperatures. We use a combination of complementary state-of-the-art quantum many-body approaches such as density matrix renormalization group (DMRG), density-matrix purification and pseudo-Majorana functional renormalization group (PMFRG). We present an efficient approach to preserve the applicability of the PMFRG for spin-1 systems at finite temperatures despite the inevitable presence of unphysical spin states. The good performance of our methods is first demonstrated for the nearest-neighbor pyrochlore Heisenberg model where the finite temperature behavior of the specific heat and uniform susceptibility show excellent agreement within PMFRG and density-matrix purification. Including an antiferromagnetic second neighbor coupling we find that the non-magnetic ground-state phase of the nearest neighbor model extents up to $J_2/J_1 \\sim 0.02 $ within DMRG, beyond which magnetic $\\boldsymbol{k}=0$ long-range order sets in. Our PMFRG calculations find the phase transition in a similar regime $J_2/J_1\\sim 0.035(8)$ which, together with the DMRG result, provides a strong argument for the existence of a small but finite non-magnetic ground-state phase in the spin-1 pyrochlore Heisenberg model. We also discuss the origin of discrepancies between different versions of the functional renormalization group concerning the location of this phase transition.","sentences":["We study the phase diagram of the antiferromagnetic $J_1$-$J_2$ Heisenberg model on the pyrochlore lattice with $S=1$ spins at zero and finite temperatures.","We use a combination of complementary state-of-the-art quantum many-body approaches such as density matrix renormalization group (DMRG), density-matrix purification and pseudo-Majorana functional renormalization group (PMFRG).","We present an efficient approach to preserve the applicability of the PMFRG for spin-1 systems at finite temperatures despite the inevitable presence of unphysical spin states.","The good performance of our methods is first demonstrated for the nearest-neighbor pyrochlore Heisenberg model where the finite temperature behavior of the specific heat and uniform susceptibility show excellent agreement within PMFRG and density-matrix purification.","Including an antiferromagnetic second neighbor coupling we find that the non-magnetic ground-state phase of the nearest neighbor model extents up to $J_2/J_1 \\sim 0.02 $ within DMRG, beyond which magnetic $\\boldsymbol{k}=0$ long-range order sets in.","Our PMFRG calculations find the phase transition in a similar regime $J_2/J_1\\sim 0.035(8)$ which, together with the DMRG result, provides a strong argument for the existence of a small but finite non-magnetic ground-state phase in the spin-1 pyrochlore Heisenberg model.","We also discuss the origin of discrepancies between different versions of the functional renormalization group concerning the location of this phase transition."],"url":"http://arxiv.org/abs/2405.12745v1","category":"cond-mat.str-el"}
{"created":"2024-05-21 12:52:03","title":"No signature of the birth environment of exoplanets from their host stars' Mahalanobis phase space","abstract":"The architectures of extrasolar planetary systems often deviate considerably from the ``standard\" model for planet formation, which is largely based on our own Solar System. In particular, gas giants on close orbits are not predicted by planet formation theory and so some process(es) are thought to move the planets closer to their host stars. Recent research has suggested that Hot Jupiter host stars display a different phase space compared to stars that do not host Hot Jupiters. This has been attributed to these stars forming in star-forming regions of high stellar density, where dynamical interactions with passing stars have perturbed the planets. We test this hypothesis by quantifying the phase space of planet-hosting stars in dynamical N-body simulations of star-forming regions. We find that stars that retain their planets have a higher phase space than non-hosts, regardless of their initial physical density. This is because an imprint of the kinematic substructure from the regions birth is retained, as these stars have experienced fewer and less disruptive encounters than stars whose planets have been liberated and become free-floating. However, host stars whose planets remain bound but have had their orbits significantly altered by dynamical encounters are also primarily found in high phase space regimes. We therefore corroborate other research in this area which has suggested the high phase space of Hot Jupiter host stars is not caused by dynamical encounters or stellar clustering, but rather reflects an age bias in that these stars are (kinematically) younger than other exoplanet host stars.","sentences":["The architectures of extrasolar planetary systems often deviate considerably from the ``standard\" model for planet formation, which is largely based on our own Solar System.","In particular, gas giants on close orbits are not predicted by planet formation theory and so some process(es) are thought to move the planets closer to their host stars.","Recent research has suggested that Hot Jupiter host stars display a different phase space compared to stars that do not host Hot Jupiters.","This has been attributed to these stars forming in star-forming regions of high stellar density, where dynamical interactions with passing stars have perturbed the planets.","We test this hypothesis by quantifying the phase space of planet-hosting stars in dynamical N-body simulations of star-forming regions.","We find that stars that retain their planets have a higher phase space than non-hosts, regardless of their initial physical density.","This is because an imprint of the kinematic substructure from the regions birth is retained, as these stars have experienced fewer and less disruptive encounters than stars whose planets have been liberated and become free-floating.","However, host stars whose planets remain bound but have had their orbits significantly altered by dynamical encounters are also primarily found in high phase space regimes.","We therefore corroborate other research in this area which has suggested the high phase space of Hot Jupiter host stars is not caused by dynamical encounters or stellar clustering, but rather reflects an age bias in that these stars are (kinematically) younger than other exoplanet host stars."],"url":"http://arxiv.org/abs/2405.12741v1","category":"astro-ph.SR"}
{"created":"2024-05-21 12:49:39","title":"Some remarks about the Morse index for convex Hamiltonians systems","abstract":"We investigate the (linearized) Morse index of solutions to Hamiltonan systems, with a focus on convex Hamiltonians functions and sign-changing radial solutions. For strongly coupled systems, we describe the profile of the radial solutions and give an estimate of their Morse index.","sentences":["We investigate the (linearized) Morse index of solutions to Hamiltonan systems, with a focus on convex Hamiltonians functions and sign-changing radial solutions.","For strongly coupled systems, we describe the profile of the radial solutions and give an estimate of their Morse index."],"url":"http://arxiv.org/abs/2405.12740v1","category":"math.AP"}
{"created":"2024-05-21 12:47:17","title":"SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling","abstract":"Human preference alignment is critical in building powerful and reliable large language models (LLMs). However, current methods either ignore the multi-dimensionality of human preferences (e.g. helpfulness and harmlessness) or struggle with the complexity of managing multiple reward models. To address these issues, we propose Sequential Preference Optimization (SPO), a method that sequentially fine-tunes LLMs to align with multiple dimensions of human preferences. SPO avoids explicit reward modeling, directly optimizing the models to align with nuanced human preferences. We theoretically derive closed-form optimal SPO policy and loss function. Gradient analysis is conducted to show how SPO manages to fine-tune the LLMs while maintaining alignment on previously optimized dimensions. Empirical results on LLMs of different size and multiple evaluation datasets demonstrate that SPO successfully aligns LLMs across multiple dimensions of human preferences and significantly outperforms the baselines.","sentences":["Human preference alignment is critical in building powerful and reliable large language models (LLMs).","However, current methods either ignore the multi-dimensionality of human preferences (e.g. helpfulness and harmlessness) or struggle with the complexity of managing multiple reward models.","To address these issues, we propose Sequential Preference Optimization (SPO), a method that sequentially fine-tunes LLMs to align with multiple dimensions of human preferences.","SPO avoids explicit reward modeling, directly optimizing the models to align with nuanced human preferences.","We theoretically derive closed-form optimal SPO policy and loss function.","Gradient analysis is conducted to show how SPO manages to fine-tune the LLMs while maintaining alignment on previously optimized dimensions.","Empirical results on LLMs of different size and multiple evaluation datasets demonstrate that SPO successfully aligns LLMs across multiple dimensions of human preferences and significantly outperforms the baselines."],"url":"http://arxiv.org/abs/2405.12739v1","category":"cs.LG"}
{"created":"2024-05-21 12:44:13","title":"Multiple chemical tracers finally unveil the intricate NGC\\,1333 IRAS\\,4A outflow system. FAUST XVI","abstract":"The exploration of outflows in protobinary systems presents a challenging yet crucial endeavour, offering valuable insights into the dynamic interplay between protostars and their evolution. In this study, we examine the morphology and dynamics of jets and outflows within the IRAS\\,4A protobinary system. This analysis is based on ALMA observations of SiO(5--4), H$_2$CO(3$_{0,3}$--2$_{0,3}$), and HDCO(4$_{1,4}$--3$_{1,3}$) with a spatial resolution of $\\sim$150\\,au. Leveraging an astrochemical approach involving the use of diverse tracers beyond traditional ones has enabled the identification of novel features and a comprehensive understanding of the broader outflow dynamics. Our analysis reveals the presence of two jets in the redshifted emission, emanating from IRAS\\,4A1 and IRAS\\,4A2, respectively. Furthermore, we identify four distinct outflows in the region for the first time, with each protostar, 4A1 and 4A2, contributing to two of them. We characterise the morphology and orientation of each outflow, challenging previous suggestions of bends in their trajectories. The outflow cavities of IRAS\\,4A1 exhibit extensions of 10$''$ and 13$''$ with position angles (PA) of 0$^{\\circ}$ and -12$^{\\circ}$, respectively, while those of IRAS\\,4A2 are more extended, spanning 18$''$ and 25$''$ with PAs of 29$^{\\circ}$ and 26$^{\\circ}$. We propose that the misalignment of the cavities is due to a jet precession in each protostar, a notion supported by the observation that the more extended cavities of the same source exhibit lower velocities, indicating they may stem from older ejection events.","sentences":["The exploration of outflows in protobinary systems presents a challenging yet crucial endeavour, offering valuable insights into the dynamic interplay between protostars and their evolution.","In this study, we examine the morphology and dynamics of jets and outflows within the IRAS\\,4A protobinary system.","This analysis is based on ALMA observations of SiO(5--4), H$_2$CO(3$_{0,3}$--2$_{0,3}$), and HDCO(4$_{1,4}$--3$_{1,3}$) with a spatial resolution of $\\sim$150\\,au.","Leveraging an astrochemical approach involving the use of diverse tracers beyond traditional ones has enabled the identification of novel features and a comprehensive understanding of the broader outflow dynamics.","Our analysis reveals the presence of two jets in the redshifted emission, emanating from IRAS\\,4A1 and IRAS\\,4A2, respectively.","Furthermore, we identify four distinct outflows in the region for the first time, with each protostar, 4A1 and 4A2, contributing to two of them.","We characterise the morphology and orientation of each outflow, challenging previous suggestions of bends in their trajectories.","The outflow cavities of IRAS\\,4A1 exhibit extensions of 10$''$ and 13$''$ with position angles (PA) of 0$^{\\circ}$ and -12$^{\\circ}$, respectively, while those of IRAS\\,4A2 are more extended, spanning 18$''$ and 25$''$ with PAs of 29$^{\\circ}$ and 26$^{\\circ}$. We propose that the misalignment of the cavities is due to a jet precession in each protostar, a notion supported by the observation that the more extended cavities of the same source exhibit lower velocities, indicating they may stem from older ejection events."],"url":"http://arxiv.org/abs/2405.12735v1","category":"astro-ph.GA"}
{"created":"2024-05-21 12:37:45","title":"Review on modeling the societal impact of infrastructure disruptions due to disasters","abstract":"Infrastructure systems play a critical role in providing essential products and services for the functioning of modern society; however, they are vulnerable to disasters and their service disruptions can cause severe societal impacts. To protect infrastructure from disasters and reduce potential impacts, great achievements have been made in modeling interdependent infrastructure systems in past decades. In recent years, scholars have gradually shifted their research focus to understanding and modeling societal impacts of disruptions considering the fact that infrastructure systems are critical because of their role in societal functioning, especially under situations of modern societies. Exploring how infrastructure disruptions impair society to enhance resilient city has become a key field of study. By comprehensively reviewing relevant studies, this paper demonstrated the definition and types of societal impact of infrastructure disruptions, and summarized the modeling approaches into four types: extended infrastructure modeling approaches, empirical approaches, agent-based approaches, and big data-driven approaches. For each approach, this paper organized relevant literature in terms of modeling ideas, advantages, and disadvantages. Furthermore, the four approaches were compared according to several criteria, including the input data, types of societal impact, and application scope. Finally, this paper illustrated the challenges and future research directions in the field.","sentences":["Infrastructure systems play a critical role in providing essential products and services for the functioning of modern society; however, they are vulnerable to disasters and their service disruptions can cause severe societal impacts.","To protect infrastructure from disasters and reduce potential impacts, great achievements have been made in modeling interdependent infrastructure systems in past decades.","In recent years, scholars have gradually shifted their research focus to understanding and modeling societal impacts of disruptions considering the fact that infrastructure systems are critical because of their role in societal functioning, especially under situations of modern societies.","Exploring how infrastructure disruptions impair society to enhance resilient city has become a key field of study.","By comprehensively reviewing relevant studies, this paper demonstrated the definition and types of societal impact of infrastructure disruptions, and summarized the modeling approaches into four types: extended infrastructure modeling approaches, empirical approaches, agent-based approaches, and big data-driven approaches.","For each approach, this paper organized relevant literature in terms of modeling ideas, advantages, and disadvantages.","Furthermore, the four approaches were compared according to several criteria, including the input data, types of societal impact, and application scope.","Finally, this paper illustrated the challenges and future research directions in the field."],"url":"http://arxiv.org/abs/2405.12732v1","category":"cs.MA"}
{"created":"2024-05-21 12:31:30","title":"Electron collision studies on the CH$_2^+$ molecular ion","abstract":"Calculations are performed for electron collision with the methylene molecular ion CH$_2^+$ in its bent equilibrium geometry, with the goal to obtain cross sections for electron impact excitation and dissociation. The polyatomic version of the UK molecular R-matrix codes was used to perform an initial configuration-interaction calculation on the doublet and quartet states of the CH$_2^+$ ion. Subsequently, scattering calculations are performed to obtain electron impact electronic excitation and dissociation cross sections and, additionally, the bound states of the CH$_2$ molecule and Feshbach resonances in the $e$-CH$_2^+$ system.","sentences":["Calculations are performed for electron collision with the methylene molecular ion CH$_2^+$ in its bent equilibrium geometry, with the goal to obtain cross sections for electron impact excitation and dissociation.","The polyatomic version of the UK molecular R-matrix codes was used to perform an initial configuration-interaction calculation on the doublet and quartet states of the CH$_2^+$ ion.","Subsequently, scattering calculations are performed to obtain electron impact electronic excitation and dissociation cross sections and, additionally, the bound states of the CH$_2$ molecule and Feshbach resonances in the $e$-CH$_2^+$ system."],"url":"http://arxiv.org/abs/2405.12726v1","category":"physics.plasm-ph"}
{"created":"2024-05-21 12:24:01","title":"RemoCap: Disentangled Representation Learning for Motion Capture","abstract":"Reconstructing 3D human bodies from realistic motion sequences remains a challenge due to pervasive and complex occlusions. Current methods struggle to capture the dynamics of occluded body parts, leading to model penetration and distorted motion. RemoCap leverages Spatial Disentanglement (SD) and Motion Disentanglement (MD) to overcome these limitations. SD addresses occlusion interference between the target human body and surrounding objects. It achieves this by disentangling target features along the dimension axis. By aligning features based on their spatial positions in each dimension, SD isolates the target object's response within a global window, enabling accurate capture despite occlusions. The MD module employs a channel-wise temporal shuffling strategy to simulate diverse scene dynamics. This process effectively disentangles motion features, allowing RemoCap to reconstruct occluded parts with greater fidelity. Furthermore, this paper introduces a sequence velocity loss that promotes temporal coherence. This loss constrains inter-frame velocity errors, ensuring the predicted motion exhibits realistic consistency. Extensive comparisons with state-of-the-art (SOTA) methods on benchmark datasets demonstrate RemoCap's superior performance in 3D human body reconstruction. On the 3DPW dataset, RemoCap surpasses all competitors, achieving the best results in MPVPE (81.9), MPJPE (72.7), and PA-MPJPE (44.1) metrics. Codes are available at https://wanghongsheng01.github.io/RemoCap/.","sentences":["Reconstructing 3D human bodies from realistic motion sequences remains a challenge due to pervasive and complex occlusions.","Current methods struggle to capture the dynamics of occluded body parts, leading to model penetration and distorted motion.","RemoCap leverages Spatial Disentanglement (SD) and Motion Disentanglement (MD) to overcome these limitations.","SD addresses occlusion interference between the target human body and surrounding objects.","It achieves this by disentangling target features along the dimension axis.","By aligning features based on their spatial positions in each dimension, SD isolates the target object's response within a global window, enabling accurate capture despite occlusions.","The MD module employs a channel-wise temporal shuffling strategy to simulate diverse scene dynamics.","This process effectively disentangles motion features, allowing RemoCap to reconstruct occluded parts with greater fidelity.","Furthermore, this paper introduces a sequence velocity loss that promotes temporal coherence.","This loss constrains inter-frame velocity errors, ensuring the predicted motion exhibits realistic consistency.","Extensive comparisons with state-of-the-art (SOTA) methods on benchmark datasets demonstrate RemoCap's superior performance in 3D human body reconstruction.","On the 3DPW dataset, RemoCap surpasses all competitors, achieving the best results in MPVPE (81.9), MPJPE (72.7), and PA-MPJPE (44.1) metrics.","Codes are available at https://wanghongsheng01.github.io/RemoCap/."],"url":"http://arxiv.org/abs/2405.12724v1","category":"cs.CV"}
{"created":"2024-05-21 12:23:41","title":"$\\mathrm{CMC\\text{-}1}$ surfaces in hyperbolic and de Sitter spaces with Cantor ends","abstract":"We prove that on every compact Riemann surface $M$ there is a Cantor set $C \\subset M$ such that $M \\setminus C$ admits a proper conformal constant mean curvature one ($\\mathrm{CMC\\text{-}1}$) immersion into hyperbolic $3$-space $\\mathbb{H}^3$. Moreover, we obtain that every bordered Riemann surface admits an almost proper $\\mathrm{CMC\\text{-}1}$ face into de Sitter $3$-space $\\mathbb{S}_1^3$, and we show that on every compact Riemann surface $M$ there is a Cantor set $C \\subset M$ such that $M \\setminus C$ admits an almost proper $\\mathrm{CMC\\text{-}1}$ face into $\\mathbb{S}_1^3$. These results follow from different uniform approximation theorems for holomorphic null curves in $\\mathbb{C}^2 \\times \\mathbb{C}^*$ that we also establish in this paper.","sentences":["We prove that on every compact Riemann surface $M$ there is a Cantor set $C \\subset M$ such that $M \\setminus C$ admits a proper conformal constant mean curvature one ($\\mathrm{CMC\\text{-}1}$) immersion into hyperbolic $3$-space $\\mathbb{H}^3$. Moreover, we obtain that every bordered Riemann surface admits an almost proper $\\mathrm{CMC\\text{-}1}$ face into de Sitter $3$-space $\\mathbb{S}_1^3$, and we show that on every compact Riemann surface $M$ there is a Cantor set $C \\subset M$ such that $M \\setminus C$ admits an almost proper $\\mathrm{CMC\\text{-}1}$ face into $\\mathbb{S}_1^3$.","These results follow from different uniform approximation theorems for holomorphic null curves in $\\mathbb{C}^2 \\times \\mathbb{C}^*$ that we also establish in this paper."],"url":"http://arxiv.org/abs/2405.12723v1","category":"math.DG"}
{"created":"2024-05-21 12:09:34","title":"Quantum Algorithms for Nonlinear Dynamics: Revisiting Carleman Linearization with No Dissipative Conditions","abstract":"In this paper, we explore the embedding of nonlinear dynamical systems into linear ordinary differential equations (ODEs) via the Carleman linearization method. Under dissipative conditions, numerous previous works have established rigorous error bounds and linear convergence for Carleman linearization, which have facilitated the identification of quantum advantages in simulating large-scale dynamical systems. Our analysis extends these findings by exploring error bounds beyond the traditional dissipative condition, thereby broadening the scope of quantum computational benefits to a new class of dynamical regimes. This novel regime is defined by a resonance condition, and we prove how this resonance condition leads to a linear convergence with respect to the truncation level $N$ in Carleman linearization. We support our theoretical advancements with numerical experiments on a variety of models, including the Burgers' equation, Fermi-Pasta-Ulam (FPU) chains, and the Korteweg-de Vries (KdV) equations, to validate our analysis and demonstrate the practical implications.","sentences":["In this paper, we explore the embedding of nonlinear dynamical systems into linear ordinary differential equations (ODEs) via the Carleman linearization method.","Under dissipative conditions, numerous previous works have established rigorous error bounds and linear convergence for Carleman linearization, which have facilitated the identification of quantum advantages in simulating large-scale dynamical systems.","Our analysis extends these findings by exploring error bounds beyond the traditional dissipative condition, thereby broadening the scope of quantum computational benefits to a new class of dynamical regimes.","This novel regime is defined by a resonance condition, and we prove how this resonance condition leads to a linear convergence with respect to the truncation level $N$ in Carleman linearization.","We support our theoretical advancements with numerical experiments on a variety of models, including the Burgers' equation, Fermi-Pasta-Ulam (FPU) chains, and the Korteweg-de Vries (KdV) equations, to validate our analysis and demonstrate the practical implications."],"url":"http://arxiv.org/abs/2405.12714v1","category":"quant-ph"}
{"created":"2024-05-21 11:50:25","title":"Quantum-classical motion of charged particles interacting with scalar fields","abstract":"The goal of this article is to investigate the dynamics of semi-relativistic or non-relativistic charged particles in interaction with a scalar meson field. Our main contribution is the derivation of the classical dynamics of a particle-field system as an effective equation of the quantum microscopic Nelson model, in the classical limit where the value of the Planck constant approaches zero ($\\hbar\\to 0$). Thus, we prove the validity of Bohr's correspondence principle, that is to establish the transition from quantum to classical dynamics. We use a Wigner measure approach to study such transition. Then, as a consequence of this interplay between classical and quantum dynamics, we establish the global well-posedness of the classical particle-field interacting system, despite the low regularity of the related vector field, which prevents the use of a fixed point argument.","sentences":["The goal of this article is to investigate the dynamics of semi-relativistic or non-relativistic charged particles in interaction with a scalar meson field.","Our main contribution is the derivation of the classical dynamics of a particle-field system as an effective equation of the quantum microscopic Nelson model, in the classical limit where the value of the Planck constant approaches zero ($\\hbar\\to 0$).","Thus, we prove the validity of Bohr's correspondence principle, that is to establish the transition from quantum to classical dynamics.","We use a Wigner measure approach to study such transition.","Then, as a consequence of this interplay between classical and quantum dynamics, we establish the global well-posedness of the classical particle-field interacting system, despite the low regularity of the related vector field, which prevents the use of a fixed point argument."],"url":"http://arxiv.org/abs/2405.12702v1","category":"math-ph"}
{"created":"2024-05-21 11:38:45","title":"Explainable offline automatic signature verifier to support forensic handwriting examiners","abstract":"Signature verification is a critical task in many applications, including forensic science, legal judgments, and financial markets. However, current signature verification systems are often difficult to explain, which can limit their acceptance in these applications. In this paper, we propose a novel explainable offline automatic signature verifier (ASV) to support forensic handwriting examiners. Our ASV is based on a universal background model (UBM) constructed from offline signature images. It allows us to assign a questioned signature to the UBM and to a reference set of known signatures using simple distance measures. This makes it possible to explain the verifier's decision in a way that is understandable to non experts. We evaluated our ASV on publicly available databases and found that it achieves competitive performance with state of the art ASVs, even when challenging 1 versus 1 comparison are considered. Our results demonstrate that it is possible to develop an explainable ASV that is also competitive in terms of performance. We believe that our ASV has the potential to improve the acceptance of signature verification in critical applications such as forensic science and legal judgments.","sentences":["Signature verification is a critical task in many applications, including forensic science, legal judgments, and financial markets.","However, current signature verification systems are often difficult to explain, which can limit their acceptance in these applications.","In this paper, we propose a novel explainable offline automatic signature verifier (ASV) to support forensic handwriting examiners.","Our ASV is based on a universal background model (UBM) constructed from offline signature images.","It allows us to assign a questioned signature to the UBM and to a reference set of known signatures using simple distance measures.","This makes it possible to explain the verifier's decision in a way that is understandable to non experts.","We evaluated our ASV on publicly available databases and found that it achieves competitive performance with state of the art ASVs, even when challenging 1 versus 1 comparison are considered.","Our results demonstrate that it is possible to develop an explainable ASV that is also competitive in terms of performance.","We believe that our ASV has the potential to improve the acceptance of signature verification in critical applications such as forensic science and legal judgments."],"url":"http://arxiv.org/abs/2405.12695v1","category":"cs.CV"}
{"created":"2024-05-21 11:25:42","title":"Spectra and Decay Properties of Higher Lying $B_C$ Meson States","abstract":"In this work, the spectra and decay properties of $B_c$ mesons ($c\\bar{b}$) have been investigated using a non-relativistic potential model incorporating corrections from LQCD. The non-relativistic Schrodinger wave equation is solved numerically using the Matrix Numerov Method. Using the obtained masses and wave functions, decay widths, lifetime, branching ratios and radiative decay widths are computed for the $c\\bar{b}$ system. We compare the obtained results with the experimental data and with other theoretical models.","sentences":["In this work, the spectra and decay properties of $B_c$ mesons ($c\\bar{b}$) have been investigated using a non-relativistic potential model incorporating corrections from LQCD.","The non-relativistic Schrodinger wave equation is solved numerically using the Matrix Numerov Method.","Using the obtained masses and wave functions, decay widths, lifetime, branching ratios and radiative decay widths are computed for the $c\\bar{b}$ system.","We compare the obtained results with the experimental data and with other theoretical models."],"url":"http://arxiv.org/abs/2405.12691v1","category":"hep-ph"}
{"created":"2024-05-21 11:19:54","title":"Heterodimensional cycles of hyperbolic ergodic measures","abstract":"We introduce the concept of a heterodimensional cycle of hyperbolic ergodic measures and a special type of them that we call rich. Within a partially hyperbolic context, we prove that if two measures are related by a rich heterodimensional cycle, then the entire segment of probability measures linking them lies within the closure of measures supported on periodic orbits. Motivated by the occurrence of robust heterodimensional cycles of hyperbolic basic sets, we study robust rich heterodimensional cycles of measures providing a framework for this phenomenon for diffeomorphisms. In the setting of skew products, we construct an open set of maps having uncountably many measures related by rich heterodimensional cycles.","sentences":["We introduce the concept of a heterodimensional cycle of hyperbolic ergodic measures and a special type of them that we call rich.","Within a partially hyperbolic context, we prove that if two measures are related by a rich heterodimensional cycle, then the entire segment of probability measures linking them lies within the closure of measures supported on periodic orbits.","Motivated by the occurrence of robust heterodimensional cycles of hyperbolic basic sets, we study robust rich heterodimensional cycles of measures providing a framework for this phenomenon for diffeomorphisms.","In the setting of skew products, we construct an open set of maps having uncountably many measures related by rich heterodimensional cycles."],"url":"http://arxiv.org/abs/2405.12686v1","category":"math.DS"}
{"created":"2024-05-21 11:11:13","title":"Sorting in One and Two Rounds using $t$-Comparators","abstract":"We examine sorting algorithms for $n$ elements whose basic operation is comparing $t$ elements simultaneously (a $t$-comparator). We focus on algorithms that use only a single round or two rounds -- comparisons performed in the second round depend on the outcomes of the first round comparators.   We design deterministic and randomized algorithms. In the deterministic case, we show an interesting relation to design theory (namely, to 2-Steiner systems), which yields a single-round optimal algorithm for $n=t^{2^k}$ with any $k\\ge 1$ and a variety of possible values of $t$. For some values of $t$, however, no algorithm can reach the optimal (information-theoretic) bound on the number of comparators. For this case (and any other $n$ and $t$), we show an algorithm that uses at most three times as many comparators as the theoretical bound.   We also design a randomized Las-Vegas two-rounds sorting algorithm for any $n$ and $t$. Our algorithm uses an asymptotically optimal number of $O(\\max(\\frac{n^{3/2}}{t^2},\\frac{n}{t}))$ comparators, with high probability, i.e., with probability at least $1-1/n$. The analysis of this algorithm involves the gradual unveiling of randomness, using a novel technique which we coin the binary tree of deferred randomness.","sentences":["We examine sorting algorithms for $n$ elements whose basic operation is comparing $t$ elements simultaneously (a $t$-comparator).","We focus on algorithms that use only a single round or two rounds -- comparisons performed in the second round depend on the outcomes of the first round comparators.   ","We design deterministic and randomized algorithms.","In the deterministic case, we show an interesting relation to design theory (namely, to 2-Steiner systems), which yields a single-round optimal algorithm for $n=t^{2^k}$ with any $k\\ge 1$ and a variety of possible values of $t$. For some values of $t$, however, no algorithm can reach the optimal (information-theoretic) bound on the number of comparators.","For this case (and any other $n$ and $t$), we show an algorithm that uses at most three times as many comparators as the theoretical bound.   ","We also design a randomized Las-Vegas two-rounds sorting algorithm for any $n$ and $t$. Our algorithm uses an asymptotically optimal number of $O(\\max(\\frac{n^{3/2}}{t^2},\\frac{n}{t}))$ comparators, with high probability, i.e., with probability at least $1-1/n$. The analysis of this algorithm involves the gradual unveiling of randomness, using a novel technique which we coin the binary tree of deferred randomness."],"url":"http://arxiv.org/abs/2405.12678v1","category":"cs.DS"}
{"created":"2024-05-21 10:36:27","title":"Constraints on the (re-)orientation of star-disk systems through infall","abstract":"It has been consensus that star-disk systems accrete most of their mass and angular momentum during the collapse of a prestellar core, such that the rotational direction of a system is equivalent to the net rotation of the core. Recent results, however, indicate that stars experience post-collapse or late infall, during which the star and its disk is refreshed with material from the protostellar environment through accretion streamers. Apart from adding mass to the star-disk system, infall potentially supplies a substantial amount of angular momentum as the infalling material is initially not bound to the collapsing prestellar core. We investigate the orientation of infall on star-disk systems by analyzing the properties of accreting tracer particles in 3D magnetohydrodynamical simulations of a molecular cloud that is (4 pc)$^3$ in volume. In contrast to the traditional picture, where the rotational axis is inherited from the collapse of a coherent pre-stellar core, the orientation of star-disk systems can change substantially during the accretion process. In agreement with previous results that show larger contributions of late infall for increasing stellar masses, infall is more likely to lead to a prolonged change in orientation for stars of higher final mass. On average, brown dwarfs and very low mass stars are more likely to form and accrete all of their mass as part of a multiple system, while stars with final masses above a few 0.1 M$_{\\odot}$ are more likely to accrete part of their mass as single stars. Finally, we find an overall trend: the post-collapse accretion phase is more anisotropic than the early collapse phase. This result is consistent with a scenario, where mass accretion from infall occurs via infalling streamers along a preferred direction, while the initial collapse is less anisotropic albeit the fact that material is funneled through accretion channels.","sentences":["It has been consensus that star-disk systems accrete most of their mass and angular momentum during the collapse of a prestellar core, such that the rotational direction of a system is equivalent to the net rotation of the core.","Recent results, however, indicate that stars experience post-collapse or late infall, during which the star and its disk is refreshed with material from the protostellar environment through accretion streamers.","Apart from adding mass to the star-disk system, infall potentially supplies a substantial amount of angular momentum as the infalling material is initially not bound to the collapsing prestellar core.","We investigate the orientation of infall on star-disk systems by analyzing the properties of accreting tracer particles in 3D magnetohydrodynamical simulations of a molecular cloud that is (4 pc)$^3$ in volume.","In contrast to the traditional picture, where the rotational axis is inherited from the collapse of a coherent pre-stellar core, the orientation of star-disk systems can change substantially during the accretion process.","In agreement with previous results that show larger contributions of late infall for increasing stellar masses, infall is more likely to lead to a prolonged change in orientation for stars of higher final mass.","On average, brown dwarfs and very low mass stars are more likely to form and accrete all of their mass as part of a multiple system, while stars with final masses above a few 0.1 M$_{\\odot}$ are more likely to accrete part of their mass as single stars.","Finally, we find an overall trend: the post-collapse accretion phase is more anisotropic than the early collapse phase.","This result is consistent with a scenario, where mass accretion from infall occurs via infalling streamers along a preferred direction, while the initial collapse is less anisotropic albeit the fact that material is funneled through accretion channels."],"url":"http://arxiv.org/abs/2405.12670v1","category":"astro-ph.SR"}
{"created":"2024-05-21 10:15:30","title":"Geometry of convex geometries","abstract":"We prove that any convex geometry $\\mathcal{A}=(U,\\mathcal{C})$ on $n$ points and any ideal $\\mathcal{I}=(U',\\mathcal{C}')$ of $\\mathcal{A}$ can be realized as the intersection pattern of an open convex polyhedral cone $K\\subseteq {\\mathbb R}^n$ with the orthants of ${\\mathbb R}^n$. Furthermore, we show that $K$ can be chosen to have at most $m$ facets, where $m$ is the number of critical rooted circuits of $\\mathcal{A}$. We also show that any convex geometry of convex dimension $d$ is realizable in ${\\mathbb R}^d$ and that any multisimplicial complex (a basic example of an ideal of a convex geometry) of dimension $d$ is realizable in ${\\mathbb R}^{2d}$ and that this is best possible. From our results it also follows that distributive lattices of dimension $d$ are realizable in ${\\mathbb R}^{d}$ and that median systems are realizable. We leave open %the question whether each median system of dimension $d$ is realizable in ${\\mathbb R}^{O(d)}$.","sentences":["We prove that any convex geometry $\\mathcal{A}=(U,\\mathcal{C})$ on $n$ points and any ideal $\\mathcal{I}=(U',\\mathcal{C}')$ of $\\mathcal{A}$ can be realized as the intersection pattern of an open convex polyhedral cone $K\\subseteq {\\mathbb R}^n$ with the orthants of ${\\mathbb R}^n$. Furthermore, we show that $K$ can be chosen to have at most $m$ facets, where $m$ is the number of critical rooted circuits of $\\mathcal{A}$. We also show that any convex geometry of convex dimension $d$ is realizable in ${\\mathbb R}^d$ and that any multisimplicial complex (a basic example of an ideal of a convex geometry) of dimension $d$ is realizable in ${\\mathbb R}^{2d}$ and that this is best possible.","From our results it also follows that distributive lattices of dimension $d$ are realizable in ${\\mathbb R}^{d}$ and that median systems are realizable.","We leave open %the question whether each median system of dimension $d$ is realizable in ${\\mathbb R}^{O(d)}$."],"url":"http://arxiv.org/abs/2405.12660v1","category":"math.CO"}
{"created":"2024-05-21 10:08:12","title":"Lipschitz minimization and the Goldstein modulus","abstract":"Goldstein's 1977 idealized iteration for minimizing a Lipschitz objective fixes a distance - the step size - and relies on a certain approximate subgradient. That \"Goldstein subgradient\" is the shortest convex combination of objective gradients at points within that distance of the current iterate. A recent implementable Goldstein-style algorithm allows a remarkable complexity analysis (Zhang et al. 2020), and a more sophisticated variant (Davis and Jiang, 2022) leverages typical objective geometry to force near-linear convergence. To explore such methods, we introduce a new modulus, based on Goldstein subgradients, that robustly measures the slope of a Lipschitz function. We relate near-linear convergence of Goldstein-style methods to linear growth of this modulus at minimizers. We illustrate the idea computationally with a simple heuristic for Lipschitz minimization.","sentences":["Goldstein's 1977 idealized iteration for minimizing a Lipschitz objective fixes a distance - the step size - and relies on a certain approximate subgradient.","That \"Goldstein subgradient\" is the shortest convex combination of objective gradients at points within that distance of the current iterate.","A recent implementable Goldstein-style algorithm allows a remarkable complexity analysis (Zhang et al. 2020), and a more sophisticated variant (Davis and Jiang, 2022) leverages typical objective geometry to force near-linear convergence.","To explore such methods, we introduce a new modulus, based on Goldstein subgradients, that robustly measures the slope of a Lipschitz function.","We relate near-linear convergence of Goldstein-style methods to linear growth of this modulus at minimizers.","We illustrate the idea computationally with a simple heuristic for Lipschitz minimization."],"url":"http://arxiv.org/abs/2405.12655v1","category":"math.OC"}
{"created":"2024-05-21 10:02:55","title":"Edge Information Hub-Empowered 6G NTN: Latency-Oriented Resource Orchestration and Configuration","abstract":"Quick response to disasters is crucial for saving lives and reducing loss. This requires low-latency uploading of situation information to the remote command center. Since terrestrial infrastructures are often damaged in disaster areas, non-terrestrial networks (NTNs) are preferable to provide network coverage, and mobile edge computing (MEC) could be integrated to improve the latency performance. Nevertheless, the communications and computing in MEC-enabled NTNs are strongly coupled, which complicates the system design. In this paper, an edge information hub (EIH) that incorporates communication, computing and storage capabilities is proposed to synergize communication and computing and enable systematic design. We first address the joint data scheduling and resource orchestration problem to minimize the latency for uploading sensing data. The problem is solved using an optimal resource orchestration algorithm. On that basis, we propose the principles for resource configuration of the EIH considering payload constraints on size, weight and energy supply. Simulation results demonstrate the superiority of our proposed scheme in reducing the overall upload latency, thus enabling quick emergency rescue.","sentences":["Quick response to disasters is crucial for saving lives and reducing loss.","This requires low-latency uploading of situation information to the remote command center.","Since terrestrial infrastructures are often damaged in disaster areas, non-terrestrial networks (NTNs) are preferable to provide network coverage, and mobile edge computing (MEC) could be integrated to improve the latency performance.","Nevertheless, the communications and computing in MEC-enabled NTNs are strongly coupled, which complicates the system design.","In this paper, an edge information hub (EIH) that incorporates communication, computing and storage capabilities is proposed to synergize communication and computing and enable systematic design.","We first address the joint data scheduling and resource orchestration problem to minimize the latency for uploading sensing data.","The problem is solved using an optimal resource orchestration algorithm.","On that basis, we propose the principles for resource configuration of the EIH considering payload constraints on size, weight and energy supply.","Simulation results demonstrate the superiority of our proposed scheme in reducing the overall upload latency, thus enabling quick emergency rescue."],"url":"http://arxiv.org/abs/2405.12652v1","category":"cs.NI"}
{"created":"2024-05-21 10:01:22","title":"Phase transitions in full counting statistics of free fermions and directed polymers","abstract":"We consider directed polymers in 1+1 spatial dimension under action of an external repulsive potential along a line. Using the exact mapping onto imaginary time evolution of free fermions we find that for sufficiently strong potential the system of polymers undergoes a continuous configurational phase transition. The transition corresponds to merging empty regions in the dominant limit shape.","sentences":["We consider directed polymers in 1+1 spatial dimension under action of an external repulsive potential along a line.","Using the exact mapping onto imaginary time evolution of free fermions we find that for sufficiently strong potential the system of polymers undergoes a continuous configurational phase transition.","The transition corresponds to merging empty regions in the dominant limit shape."],"url":"http://arxiv.org/abs/2405.12651v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-21 09:53:20","title":"Data-driven Discovery for Robust Optimization of Semiconductor Nanowire Lasers","abstract":"Active wavelength-scale optoelectronic components are widely used in photonic integrated circuitry, however coherent sources of light -- namely optical lasers -- remain the most challenging component to integrate. Semiconductor nanowire lasers represent a flexible class of light source where each nanowire is both gain material and cavity; however, strong coupling between these properties and the performance leads to inhomogeneity across the population. While this has been studied and optimized for individual material systems, no architecture-wide insight is available. Here, nine nanowire laser material systems are studied and compared using 55,516 nanowire lasers to provide statistically robust insight into performance. These results demonstrate that, while it may be important to optimise internal quantum efficiency for certain materials, cavity effects are always critical. Our study provides a roadmap to optimize the performance of nanowire lasers made from any material: this can be achieved by ensuring a narrow spread of lengths and end-facet reflectivities.","sentences":["Active wavelength-scale optoelectronic components are widely used in photonic integrated circuitry, however coherent sources of light -- namely optical lasers -- remain the most challenging component to integrate.","Semiconductor nanowire lasers represent a flexible class of light source where each nanowire is both gain material and cavity; however, strong coupling between these properties and the performance leads to inhomogeneity across the population.","While this has been studied and optimized for individual material systems, no architecture-wide insight is available.","Here, nine nanowire laser material systems are studied and compared using 55,516 nanowire lasers to provide statistically robust insight into performance.","These results demonstrate that, while it may be important to optimise internal quantum efficiency for certain materials, cavity effects are always critical.","Our study provides a roadmap to optimize the performance of nanowire lasers made from any material: this can be achieved by ensuring a narrow spread of lengths and end-facet reflectivities."],"url":"http://arxiv.org/abs/2405.12643v1","category":"physics.optics"}
{"created":"2024-05-21 09:39:14","title":"Visuo-Tactile based Predictive Cross Modal Perception for Object Exploration in Robotics","abstract":"Autonomously exploring the unknown physical properties of novel objects such as stiffness, mass, center of mass, friction coefficient, and shape is crucial for autonomous robotic systems operating continuously in unstructured environments. We introduce a novel visuo-tactile based predictive cross-modal perception framework where initial visual observations (shape) aid in obtaining an initial prior over the object properties (mass). The initial prior improves the efficiency of the object property estimation, which is autonomously inferred via interactive non-prehensile pushing and using a dual filtering approach. The inferred properties are then used to enhance the predictive capability of the cross-modal function efficiently by using a human-inspired `surprise' formulation. We evaluated our proposed framework in the real-robotic scenario, demonstrating superior performance.","sentences":["Autonomously exploring the unknown physical properties of novel objects such as stiffness, mass, center of mass, friction coefficient, and shape is crucial for autonomous robotic systems operating continuously in unstructured environments.","We introduce a novel visuo-tactile based predictive cross-modal perception framework where initial visual observations (shape) aid in obtaining an initial prior over the object properties (mass).","The initial prior improves the efficiency of the object property estimation, which is autonomously inferred via interactive non-prehensile pushing and using a dual filtering approach.","The inferred properties are then used to enhance the predictive capability of the cross-modal function efficiently by using a human-inspired `surprise' formulation.","We evaluated our proposed framework in the real-robotic scenario, demonstrating superior performance."],"url":"http://arxiv.org/abs/2405.12634v1","category":"cs.RO"}
{"created":"2024-05-21 09:34:31","title":"Cosmic very small dust grains as a natural laboratory of mesoscopic physics: Modeling thermal and optical properties of graphite grains","abstract":"Cosmic very small dust grains (VSGs) contain 100 to 10,000 atoms, making it a mesoscopic system with specific thermal and optical characteristics due to the finite number of atoms within each grain. This paper focuses on graphite VSGs which contain free electrons. The energy level statistics devised by Kubo (1962, J.Phys.Soc.Jpn., 17, 975-986) were used for the first time to understand the thermal properties of free electrons in graphite VSGs. We showed that the shape irregularity of the grains allows graphite VSGs to absorb or emit photons at sub-millimeter wavelengths or longer; otherwise, the frequency is limited to above a few THz. Additionally, we considered the decrease in Debye temperature due to the surface effect. VSGs have an extremely small volume, resulting in limited thermal energy storage, especially at low temperatures. Since a VSG is able to emit a photon with energy smaller than its internal energy, this determines the maximum frequency of the emitted photon. We developed a Monte-Carlo simulation code to track the thermal history of a dust grain, considering the stochastic heating from the absorption of ambient photons and radiative cooling. This approach was applied to the interstellar environment to compute the spectral energy distributions from the interstellar graphite dust grains. The results showed that graphite VSGs emit not only the mid-infrared excess emission, but also a surplus emission from sub-millimeter to millimeter wavelengths.","sentences":["Cosmic very small dust grains (VSGs) contain 100 to 10,000 atoms, making it a mesoscopic system with specific thermal and optical characteristics due to the finite number of atoms within each grain.","This paper focuses on graphite VSGs which contain free electrons.","The energy level statistics devised by Kubo (1962, J.Phys.","Soc.","Jpn., 17, 975-986) were used for the first time to understand the thermal properties of free electrons in graphite VSGs.","We showed that the shape irregularity of the grains allows graphite VSGs to absorb or emit photons at sub-millimeter wavelengths or longer; otherwise, the frequency is limited to above a few THz.","Additionally, we considered the decrease in Debye temperature due to the surface effect.","VSGs have an extremely small volume, resulting in limited thermal energy storage, especially at low temperatures.","Since a VSG is able to emit a photon with energy smaller than its internal energy, this determines the maximum frequency of the emitted photon.","We developed a Monte-Carlo simulation code to track the thermal history of a dust grain, considering the stochastic heating from the absorption of ambient photons and radiative cooling.","This approach was applied to the interstellar environment to compute the spectral energy distributions from the interstellar graphite dust grains.","The results showed that graphite VSGs emit not only the mid-infrared excess emission, but also a surplus emission from sub-millimeter to millimeter wavelengths."],"url":"http://arxiv.org/abs/2405.12632v1","category":"astro-ph.GA"}
{"created":"2024-05-21 09:31:23","title":"A Local Gaussian Process Regression Approach to Frequency Response Function Estimation","abstract":"Frequency response function (FRF) estimation is a classical subject in system identification. In the past two decades, there have been remarkable advances in developing local methods for this subject, e.g., the local polynomial method, local rational method, and iterative local rational method. The recent concentrations for local methods are two issues: the model order selection and the identification of lightly damped systems. To address these two issues, we propose a new local method called local Gaussian process regression (LGPR). We show that the frequency response function locally is either analytic or resonant, and this prior knowledge can be embedded into a kernel-based regularized estimate through a dot-product kernel plus a resonance kernel induced by a second-order resonant system. The LGPR provides a new route to tackle the aforementioned issues. In the numerical simulations, the LGPR shows the best FRF estimation accuracy compared with the existing local methods, and moreover, the LGPR is more robust with respect to sample size and noise level.","sentences":["Frequency response function (FRF) estimation is a classical subject in system identification.","In the past two decades, there have been remarkable advances in developing local methods for this subject, e.g., the local polynomial method, local rational method, and iterative local rational method.","The recent concentrations for local methods are two issues: the model order selection and the identification of lightly damped systems.","To address these two issues, we propose a new local method called local Gaussian process regression (LGPR).","We show that the frequency response function locally is either analytic or resonant, and this prior knowledge can be embedded into a kernel-based regularized estimate through a dot-product kernel plus a resonance kernel induced by a second-order resonant system.","The LGPR provides a new route to tackle the aforementioned issues.","In the numerical simulations, the LGPR shows the best FRF estimation accuracy compared with the existing local methods, and moreover, the LGPR is more robust with respect to sample size and noise level."],"url":"http://arxiv.org/abs/2405.12629v1","category":"eess.SY"}
{"created":"2024-05-21 09:26:18","title":"Quantum Resonant Dimensionality Reduction and Its Application in Quantum Machine Learning","abstract":"Quantum computing is a promising candidate for accelerating machine learning tasks. Limited by the control accuracy of current quantum hardware, reducing the consumption of quantum resources is the key to achieving quantum advantage. Here, we propose a quantum resonant dimension reduction (QRDR) algorithm based on the quantum resonant transition to reduce the dimension of input data and accelerate the quantum machine learning algorithms. After QRDR, the dimension of input data $N$ can be reduced into desired scale $R$, and the effective information of the original data will be preserved correspondingly, which will reduce the computational complexity of subsequent quantum machine learning algorithms or quantum storage. QRDR operates with polylogarithmic time complexity and reduces the error dependency from the order of $1/\\epsilon^3$ to the order of $1/\\epsilon$, compared to existing algorithms. We demonstrate the performance of our algorithm combining with two types of quantum classifiers, quantum support vector machines and quantum convolutional neural networks, for classifying underwater detection targets and quantum many-body phase respectively. The simulation results indicate that reduced data improved the processing efficiency and accuracy following the application of QRDR. As quantum machine learning continues to advance, our algorithm has the potential to be utilized in a variety of computing fields.","sentences":["Quantum computing is a promising candidate for accelerating machine learning tasks.","Limited by the control accuracy of current quantum hardware, reducing the consumption of quantum resources is the key to achieving quantum advantage.","Here, we propose a quantum resonant dimension reduction (QRDR) algorithm based on the quantum resonant transition to reduce the dimension of input data and accelerate the quantum machine learning algorithms.","After QRDR, the dimension of input data $N$ can be reduced into desired scale $R$, and the effective information of the original data will be preserved correspondingly, which will reduce the computational complexity of subsequent quantum machine learning algorithms or quantum storage.","QRDR operates with polylogarithmic time complexity and reduces the error dependency from the order of $1/\\epsilon^3$ to the order of $1/\\epsilon$, compared to existing algorithms.","We demonstrate the performance of our algorithm combining with two types of quantum classifiers, quantum support vector machines and quantum convolutional neural networks, for classifying underwater detection targets and quantum many-body phase respectively.","The simulation results indicate that reduced data improved the processing efficiency and accuracy following the application of QRDR.","As quantum machine learning continues to advance, our algorithm has the potential to be utilized in a variety of computing fields."],"url":"http://arxiv.org/abs/2405.12625v1","category":"quant-ph"}
{"created":"2024-05-21 09:25:44","title":"Hydrodynamic instability of shear imposed falling film over a uniformly heated inclined undulated substrate","abstract":"Linear and weakly nonlinear stability analyses of an externally shear-imposed, gravity-driven falling film over a uniformly heated wavy substrate are studied. The longwave asymptotic expansion technique is utilized to formulate a single nonlinear free surface deflection equation. The linear stability criteria for the onset of instability are derived using the normal mode form in the linearized portion of the surface deformation equation. Linear stability theory reveals that the flow-directed sturdy external shear grows the surface wave instability by increasing the net driving force. On the contrary, the upstream-directed imposed shear may reduce the surface mode instability by restricting the gravity-driving force, which has the consequence of weakening the bulk velocity of the liquid film. However, the surface mode can be stabilized/destabilized by increasing the temperature-dependent density/surface-tension variation. Further, the bottom steepness shows dual behaviour on the surface instability depending upon the wavy wall's portion (uphill/downhill). At the downhill portion, the surface wave becomes more unstable than at the bottom substrate's uphill portion. Moreover, the multi-scale method is incorporated to obtain the complex Ginzburg-Landau equation in order to study the weakly nonlinear stability, confirming the existence of various flow regions of the liquid film. At any bottom portion (uphill/downhill), the flow-directed external shear expands the super-critical stable zones, which causes an amplification in the nonlinear wave amplitude, and the backflow-directed shear plays a counterproductive role. On the other hand, the super-critical stable region decreases or increases as long as the linear variation of density or surface tension increases with respect to the temperature, whereas the sub-critical unstable region exhibits an inverse trend.","sentences":["Linear and weakly nonlinear stability analyses of an externally shear-imposed, gravity-driven falling film over a uniformly heated wavy substrate are studied.","The longwave asymptotic expansion technique is utilized to formulate a single nonlinear free surface deflection equation.","The linear stability criteria for the onset of instability are derived using the normal mode form in the linearized portion of the surface deformation equation.","Linear stability theory reveals that the flow-directed sturdy external shear grows the surface wave instability by increasing the net driving force.","On the contrary, the upstream-directed imposed shear may reduce the surface mode instability by restricting the gravity-driving force, which has the consequence of weakening the bulk velocity of the liquid film.","However, the surface mode can be stabilized/destabilized by increasing the temperature-dependent density/surface-tension variation.","Further, the bottom steepness shows dual behaviour on the surface instability depending upon the wavy wall's portion (uphill/downhill).","At the downhill portion, the surface wave becomes more unstable than at the bottom substrate's uphill portion.","Moreover, the multi-scale method is incorporated to obtain the complex Ginzburg-Landau equation in order to study the weakly nonlinear stability, confirming the existence of various flow regions of the liquid film.","At any bottom portion (uphill/downhill), the flow-directed external shear expands the super-critical stable zones, which causes an amplification in the nonlinear wave amplitude, and the backflow-directed shear plays a counterproductive role.","On the other hand, the super-critical stable region decreases or increases as long as the linear variation of density or surface tension increases with respect to the temperature, whereas the sub-critical unstable region exhibits an inverse trend."],"url":"http://arxiv.org/abs/2405.12623v1","category":"physics.flu-dyn"}
{"created":"2024-05-21 09:19:12","title":"On the smoothability problem with rational coefficients","abstract":"We consider the problem of smoothing algebraic cycles with rational coefficients on smooth projective complex varieties up to homological equivalence. We show that a solution to this problem would be incompatible with the validity of the Hartshorne conjecture on complete intersections in projective space. We also solve unconditionally a symplectic variant of this problem.","sentences":["We consider the problem of smoothing algebraic cycles with rational coefficients on smooth projective complex varieties up to homological equivalence.","We show that a solution to this problem would be incompatible with the validity of the Hartshorne conjecture on complete intersections in projective space.","We also solve unconditionally a symplectic variant of this problem."],"url":"http://arxiv.org/abs/2405.12620v1","category":"math.AG"}
{"created":"2024-05-21 09:05:07","title":"Quantifying the local mechanical properties of twisted double bilayer graphene","abstract":"Nanomechanical measurements of minimally twisted van der Waals materials remained elusive despite their fundamental importance for device realisation. Here, we use Ultrasonic Force Microscopy (UFM) to locally quantify the variation of out-of-plane Young's modulus in minimally twisted double bilayer graphene (TDBG). We reveal a softening of the Young's modulus by 7\\% and 17\\% along single and double domain walls, respectively. Our experimental results are confirmed by force-field relaxation models. This study highlights the strong tunability of nanomechanical properties in engineered twisted materials, and paves the way for future applications of designer 2D nanomechanical systems.","sentences":["Nanomechanical measurements of minimally twisted van der Waals materials remained elusive despite their fundamental importance for device realisation.","Here, we use Ultrasonic Force Microscopy (UFM) to locally quantify the variation of out-of-plane Young's modulus in minimally twisted double bilayer graphene (TDBG).","We reveal a softening of the Young's modulus by 7\\% and 17\\% along single and double domain walls, respectively.","Our experimental results are confirmed by force-field relaxation models.","This study highlights the strong tunability of nanomechanical properties in engineered twisted materials, and paves the way for future applications of designer 2D nanomechanical systems."],"url":"http://arxiv.org/abs/2405.12610v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 09:04:48","title":"Mamba in Speech: Towards an Alternative to Self-Attention","abstract":"Transformer and its derivatives have achieved success in diverse tasks across computer vision, natural language processing, and speech processing. To reduce the complexity of computations within the multi-head self-attention mechanism in Transformer, Selective State Space Models (i.e., Mamba) were proposed as an alternative. Mamba exhibited its effectiveness in natural language processing and computer vision tasks, but its superiority has rarely been investigated in speech signal processing. This paper explores solutions for applying Mamba to speech processing using two typical speech processing tasks: speech recognition, which requires semantic and sequential information, and speech enhancement, which focuses primarily on sequential patterns. The results exhibit the superiority of bidirectional Mamba (BiMamba) for speech processing to vanilla Mamba. Moreover, experiments demonstrate the effectiveness of BiMamba as an alternative to the self-attention module in Transformer and its derivates, particularly for the semantic-aware task. The crucial technologies for transferring Mamba to speech are then summarized in ablation studies and the discussion section to offer insights for future research.","sentences":["Transformer and its derivatives have achieved success in diverse tasks across computer vision, natural language processing, and speech processing.","To reduce the complexity of computations within the multi-head self-attention mechanism in Transformer, Selective State Space Models (i.e., Mamba) were proposed as an alternative.","Mamba exhibited its effectiveness in natural language processing and computer vision tasks, but its superiority has rarely been investigated in speech signal processing.","This paper explores solutions for applying Mamba to speech processing using two typical speech processing tasks: speech recognition, which requires semantic and sequential information, and speech enhancement, which focuses primarily on sequential patterns.","The results exhibit the superiority of bidirectional Mamba (BiMamba) for speech processing to vanilla Mamba.","Moreover, experiments demonstrate the effectiveness of BiMamba as an alternative to the self-attention module in Transformer and its derivates, particularly for the semantic-aware task.","The crucial technologies for transferring Mamba to speech are then summarized in ablation studies and the discussion section to offer insights for future research."],"url":"http://arxiv.org/abs/2405.12609v1","category":"eess.AS"}
{"created":"2024-05-21 08:59:34","title":"On an equation arising by reduction of the Drinfeld-Sokolov hierarchy","abstract":"A seventh order ordinary differential equation (ODE) arising by reduction of the Drinfeld-Sokolov hierarchyis shown to be identical to a similarity reduction of an equationin the hierarchy of Sawada-Kotera.We also exhibit its link with a particular F-VI,a fourth order ODE isolated by Cosgrove which is likely to define a higher order Painlev\\'e function.","sentences":["A seventh order ordinary differential equation (ODE) arising by reduction of the Drinfeld-Sokolov hierarchyis shown to be identical to a similarity reduction of an equationin the hierarchy of Sawada-Kotera.","We also exhibit its link with a particular F-VI,a fourth order ODE isolated by Cosgrove which is likely to define a higher order Painlev\\'e function."],"url":"http://arxiv.org/abs/2405.12606v1","category":"nlin.SI"}
{"created":"2024-05-21 08:36:34","title":"V892 Tau: A tidally perturbed circumbinary disc in a triple stellar system","abstract":"V892 Tau is a young binary star surrounded by a circumbinary disc which show hints of interaction with the low-mass nearby star V892 Tau NE. The goal of this paper is to constrain the orbit of V892 Tau NE and to determine the resulting circumbinary disc dynamics. We present new ALMA observations of the V892 Tau circumbinary disc at a twice higher angular and spectral resolution. We model the data with V892 Tau as a triple system and perform a grid of hydrodynamical simulations testing several orbits of the companion. The simulation outputs are then post-processed to build synthetic maps that we compare to the observations. The 12CO emission of the disc shows clear non-Keplerian features such as spiral arms. When comparing the data with our synthetic observations, we interpret these features as ongoing interactions with the companion. Our simulations indicate that an eccentricity of 0.5 of the companion is needed to reproduce the observed disc extent and that a mutual inclination of approximately 60{\\deg} with the inner binary reproduces the measured disc tilt. In order to explain most of the features of the circumbinary disc, we propose that V892 Tau NE follows a misaligned eccentric orbit, with an eccentricity between 0.2 and 0.5 and a mutual inclination between 30{\\deg} and 60{\\deg}. Such a misaligned companion suggests the disc is oscillating and precessing with time, stabilising in an intermediate plane with a non-zero mutual inclination with the inner binary. Given that orbital configuration, we show that the stability of future planets is compromised in the second half of the disc once the gas has dissipated.","sentences":["V892 Tau is a young binary star surrounded by a circumbinary disc which show hints of interaction with the low-mass nearby star V892 Tau NE.","The goal of this paper is to constrain the orbit of V892 Tau NE and to determine the resulting circumbinary disc dynamics.","We present new ALMA observations of the V892 Tau circumbinary disc at a twice higher angular and spectral resolution.","We model the data with V892 Tau as a triple system and perform a grid of hydrodynamical simulations testing several orbits of the companion.","The simulation outputs are then post-processed to build synthetic maps that we compare to the observations.","The 12CO emission of the disc shows clear non-Keplerian features such as spiral arms.","When comparing the data with our synthetic observations, we interpret these features as ongoing interactions with the companion.","Our simulations indicate that an eccentricity of 0.5 of the companion is needed to reproduce the observed disc extent and that a mutual inclination of approximately 60{\\deg} with the inner binary reproduces the measured disc tilt.","In order to explain most of the features of the circumbinary disc, we propose that V892 Tau NE follows a misaligned eccentric orbit, with an eccentricity between 0.2 and 0.5 and a mutual inclination between 30{\\deg} and 60{\\deg}.","Such a misaligned companion suggests the disc is oscillating and precessing with time, stabilising in an intermediate plane with a non-zero mutual inclination with the inner binary.","Given that orbital configuration, we show that the stability of future planets is compromised in the second half of the disc once the gas has dissipated."],"url":"http://arxiv.org/abs/2405.12593v1","category":"astro-ph.EP"}
{"created":"2024-05-21 08:32:41","title":"An analysis of factors impacting team strengths in the Australian Football League using time-variant Bradley-Terry models","abstract":"Australian Rules Football is a field invasion game where two teams attempt to score the highest points to win. Complex machine learning algorithms have been developed to predict match outcomes post-game, but their lack of interpretability hampers an understanding of the factors that affect a team's performance. Using data from the male competition of the Australian Football League, seasons 2015 to 2023, we estimate team strengths and the factors impacting them by fitting flexible Bradley-Terry models. We successfully identify teams significantly stronger or weaker than the average, with stronger teams placing higher in the previous seasons' ladder and leading the activity in the Forward 50 zone, goal shots and scoring over their opponents. Playing at home is confirmed to create an advantage regardless of team strengths. The ability of the model to predict game results in advance is tested, with models accounting for team-specific, time-variant features predicting up to 71.5% of outcomes. Therefore, our approach can provide an interpretable understanding of team strengths and competitive game predictions, making it optimal for data-driven strategies and training.","sentences":["Australian Rules Football is a field invasion game where two teams attempt to score the highest points to win.","Complex machine learning algorithms have been developed to predict match outcomes post-game, but their lack of interpretability hampers an understanding of the factors that affect a team's performance.","Using data from the male competition of the Australian Football League, seasons 2015 to 2023, we estimate team strengths and the factors impacting them by fitting flexible Bradley-Terry models.","We successfully identify teams significantly stronger or weaker than the average, with stronger teams placing higher in the previous seasons' ladder and leading the activity in the Forward 50 zone, goal shots and scoring over their opponents.","Playing at home is confirmed to create an advantage regardless of team strengths.","The ability of the model to predict game results in advance is tested, with models accounting for team-specific, time-variant features predicting up to 71.5% of outcomes.","Therefore, our approach can provide an interpretable understanding of team strengths and competitive game predictions, making it optimal for data-driven strategies and training."],"url":"http://arxiv.org/abs/2405.12588v1","category":"stat.AP"}
{"created":"2024-05-21 08:31:09","title":"Reduction Strategies in the Lambda Calculus and Their Implementation through Derivable Abstract Machines: Introduction","abstract":"The lambda calculus since more than half a century is a model and foundation of functional programming languages. However, lambda expressions can be evaluated with different reduction strategies and thus, there is no fixed cost model nor one canonical implementation for all applications of the lambda calculus.   This article is an introduction to a dissertation is composed of four conference papers where: we present a systematic survey of reduction strategies of the lambda calculus; we take advantage of the functional correspondence as a tool for studying implementations of the lambda calculus by deriving an abstract machine for a precisely identified strong call-by-value reduction strategy; we improve it to obtain an efficient abstract machine for strong call by value and provide a time complexity analysis for the new machine with the use of a potential function; and we present the first provably efficient abstract machine for strong call by need.","sentences":["The lambda calculus since more than half a century is a model and foundation of functional programming languages.","However, lambda expressions can be evaluated with different reduction strategies and thus, there is no fixed cost model nor one canonical implementation for all applications of the lambda calculus.   ","This article is an introduction to a dissertation is composed of four conference papers where: we present a systematic survey of reduction strategies of the lambda calculus; we take advantage of the functional correspondence as a tool for studying implementations of the lambda calculus by deriving an abstract machine for a precisely identified strong call-by-value reduction strategy; we improve it to obtain an efficient abstract machine for strong call by value and provide a time complexity analysis for the new machine with the use of a potential function; and we present the first provably efficient abstract machine for strong call by need."],"url":"http://arxiv.org/abs/2405.12586v1","category":"cs.PL"}
{"created":"2024-05-21 08:19:49","title":"Carleson measures for Hardy-Sobolev spaces in the Siegel upper half-space","abstract":"We give a capacitary type characterization of Carleson measures for a class of Hardy-Sobolev spaces (also known as weighted Dirichlet spaces) on the Siegel upper half-space, introduced by Arcozzi et al. This answers in part a question raised by the same authors.","sentences":["We give a capacitary type characterization of Carleson measures for a class of Hardy-Sobolev spaces (also known as weighted Dirichlet spaces) on the Siegel upper half-space, introduced by Arcozzi et al.","This answers in part a question raised by the same authors."],"url":"http://arxiv.org/abs/2405.12576v1","category":"math.CV"}
{"created":"2024-05-21 08:17:34","title":"Stochastic porous media equation with Robin boundary conditions, gravity-driven infiltration and multiplicative noise","abstract":"We aim at studying a novel mathematical model associated to a physical phenomenon of infiltration in an homogeneous porous medium. The particularities of our system are connected to the presence of a gravitational acceleration term proportional to the level of saturation, and of a Brownian multiplicative perturbation. Furthermore, the boundary conditions intervene in a Robin manner with the distinction of the behavior along the inflow and outflow respectively. We provide qualitative results of well-posedness, the investigation being conducted through a functional approach.","sentences":["We aim at studying a novel mathematical model associated to a physical phenomenon of infiltration in an homogeneous porous medium.","The particularities of our system are connected to the presence of a gravitational acceleration term proportional to the level of saturation, and of a Brownian multiplicative perturbation.","Furthermore, the boundary conditions intervene in a Robin manner with the distinction of the behavior along the inflow and outflow respectively.","We provide qualitative results of well-posedness, the investigation being conducted through a functional approach."],"url":"http://arxiv.org/abs/2405.12572v1","category":"math.AP"}
{"created":"2024-05-21 08:07:38","title":"Unveiling Online Conspiracy Theorists: a Text-Based Approach and Characterization","abstract":"In today's digital landscape, the proliferation of conspiracy theories within the disinformation ecosystem of online platforms represents a growing concern. This paper delves into the complexities of this phenomenon. We conducted a comprehensive analysis of two distinct X (formerly known as Twitter) datasets: one comprising users with conspiracy theorizing patterns and another made of users lacking such tendencies and thus serving as a control group. The distinguishing factors between these two groups are explored across three dimensions: emotions, idioms, and linguistic features. Our findings reveal marked differences in the lexicon and language adopted by conspiracy theorists with respect to other users. We developed a machine learning classifier capable of identifying users who propagate conspiracy theories based on a rich set of 871 features. The results demonstrate high accuracy, with an average F1 score of 0.88. Moreover, this paper unveils the most discriminating characteristics that define conspiracy theory propagators.","sentences":["In today's digital landscape, the proliferation of conspiracy theories within the disinformation ecosystem of online platforms represents a growing concern.","This paper delves into the complexities of this phenomenon.","We conducted a comprehensive analysis of two distinct X (formerly known as Twitter) datasets: one comprising users with conspiracy theorizing patterns and another made of users lacking such tendencies and thus serving as a control group.","The distinguishing factors between these two groups are explored across three dimensions: emotions, idioms, and linguistic features.","Our findings reveal marked differences in the lexicon and language adopted by conspiracy theorists with respect to other users.","We developed a machine learning classifier capable of identifying users who propagate conspiracy theories based on a rich set of 871 features.","The results demonstrate high accuracy, with an average F1 score of 0.88.","Moreover, this paper unveils the most discriminating characteristics that define conspiracy theory propagators."],"url":"http://arxiv.org/abs/2405.12566v1","category":"cs.SI"}
{"created":"2024-05-21 07:59:55","title":"Gamification of IT for training in information systems management","abstract":"This article examines the integration of IT competitions, in particular Capture The Flag, into an information systems management course to fill skills gaps, particularly in the field of cybersecurity. An educational CTF team has been set up at IAE Paris-Est with the aim of developing students' skills. Workshops, challenges, and events have been organised to familiarise them with the CTFs and offer them support adapted to their level. Preliminary results show the importance of soft skills in improving cybersecurity skills. The CTF pedagogical team is continuing to experiment with and evaluate these methods to improve the accessibility and effectiveness of cybersecurity training.","sentences":["This article examines the integration of IT competitions, in particular Capture The Flag, into an information systems management course to fill skills gaps, particularly in the field of cybersecurity.","An educational CTF team has been set up at IAE Paris-Est with the aim of developing students' skills.","Workshops, challenges, and events have been organised to familiarise them with the CTFs and offer them support adapted to their level.","Preliminary results show the importance of soft skills in improving cybersecurity skills.","The CTF pedagogical team is continuing to experiment with and evaluate these methods to improve the accessibility and effectiveness of cybersecurity training."],"url":"http://arxiv.org/abs/2405.12561v1","category":"cs.CY"}
{"created":"2024-05-21 07:50:38","title":"A Subexponential Reduction from Product Partition to Subset Sum","abstract":"In this paper we study the Product Partition Problem (PPP), i.e. we are given a set of $n$ natural numbers represented on $m$ bits each and we are asked if a subset exists such that the product of the numbers in the subset equals the product of the numbers not in the subset. Our approach is to obtain the integer factorization of each number. This is the subexponential step. We then form a matrix with the exponents of the primes and show that the PPP has a solution iff some Subset Sum Problems have a common solution. Finally, using the fact that the exponents are not large we combine all the Subset Sum Problems in a single Subset Sum Problem (SSP) and show that its size is polynomial in $m,n$. We show that the PPP has a solution iff the final SSP has one.","sentences":["In this paper we study the Product Partition Problem (PPP), i.e. we are given a set of $n$ natural numbers represented on $m$ bits each and we are asked if a subset exists such that the product of the numbers in the subset equals the product of the numbers not in the subset.","Our approach is to obtain the integer factorization of each number.","This is the subexponential step.","We then form a matrix with the exponents of the primes and show that the PPP has a solution iff some Subset Sum Problems have a common solution.","Finally, using the fact that the exponents are not large we combine all the Subset Sum Problems in a single Subset Sum Problem (SSP) and show that its size is polynomial in $m,n$. We show that the PPP has a solution iff the final SSP has one."],"url":"http://arxiv.org/abs/2405.12555v1","category":"math.CO"}
{"created":"2024-05-21 07:28:49","title":"Data-driven Coordinated AC/DC Control Strategy for Frequency Safety","abstract":"With high penetrations of renewable energy and power electronics converters, less predictable operating conditions and strong uncertainties in under-frequency events pose challenges for emergency frequency control (EFC). On the other hand, the fast adjustability of converter-based sources presents opportunities to reduce economic losses from traditional load shedding for EFC. By integrating DC power emergency support, a data-driven coordinated AC/DC control strategy for frequency safety - Coordinated Emergency Frequency Control (CEFC) - has been designed. CEFC coordinates both the initiation and control amount of emergency DC power support (EDCPS) and traditional load shedding. Based on real-time power system response data, CEFC ensures system frequency safety at a minimum control cost under non-envisioned operating conditions and large power deficits. A sufficient condition where data-driven modeling errors do not affect the precision of the control strategy for power system frequency is rigorously provided. Simulation results demonstrate CEFC's adaptability, prediction accuracy, and control effectiveness.","sentences":["With high penetrations of renewable energy and power electronics converters, less predictable operating conditions and strong uncertainties in under-frequency events pose challenges for emergency frequency control (EFC).","On the other hand, the fast adjustability of converter-based sources presents opportunities to reduce economic losses from traditional load shedding for EFC.","By integrating DC power emergency support, a data-driven coordinated AC/DC control strategy for frequency safety - Coordinated Emergency Frequency Control (CEFC) - has been designed.","CEFC coordinates both the initiation and control amount of emergency DC power support (EDCPS) and traditional load shedding.","Based on real-time power system response data, CEFC ensures system frequency safety at a minimum control cost under non-envisioned operating conditions and large power deficits.","A sufficient condition where data-driven modeling errors do not affect the precision of the control strategy for power system frequency is rigorously provided.","Simulation results demonstrate CEFC's adaptability, prediction accuracy, and control effectiveness."],"url":"http://arxiv.org/abs/2405.12546v1","category":"eess.SY"}
{"created":"2024-05-21 07:23:01","title":"Interatomic Interaction Models for Magnetic Materials: Recent Advances","abstract":"Atomistic modeling is a widely employed theoretical method of computational materials science. It has found particular utility in the study of magnetic materials. Initially, magnetic empirical interatomic potentials or spin-polarized density functional theory (DFT) served as the primary models for describing interatomic interactions in atomistic simulations of magnetic systems. Furthermore, in recent years, a new class of interatomic potentials known as magnetic machine-learning interatomic potentials (magnetic MLIPs) has emerged. These MLIPs combine the computational efficiency, in terms of CPU time, of empirical potentials with the accuracy of DFT calculations. In this review, our focus lies on providing a comprehensive summary of the interatomic interaction models developed specifically for investigating magnetic materials. We also delve into the various problem classes to which these models can be applied. Finally, we offer insights into the future prospects of interatomic interaction model development for the exploration of magnetic materials.","sentences":["Atomistic modeling is a widely employed theoretical method of computational materials science.","It has found particular utility in the study of magnetic materials.","Initially, magnetic empirical interatomic potentials or spin-polarized density functional theory (DFT) served as the primary models for describing interatomic interactions in atomistic simulations of magnetic systems.","Furthermore, in recent years, a new class of interatomic potentials known as magnetic machine-learning interatomic potentials (magnetic MLIPs) has emerged.","These MLIPs combine the computational efficiency, in terms of CPU time, of empirical potentials with the accuracy of DFT calculations.","In this review, our focus lies on providing a comprehensive summary of the interatomic interaction models developed specifically for investigating magnetic materials.","We also delve into the various problem classes to which these models can be applied.","Finally, we offer insights into the future prospects of interatomic interaction model development for the exploration of magnetic materials."],"url":"http://arxiv.org/abs/2405.12544v1","category":"physics.atom-ph"}
{"created":"2024-05-21 07:12:10","title":"Capillary condensation between non-parallel walls","abstract":"We study the condensation of fluids confined by a pair of non-parallel plates of finite height $H$. We show that such a system experiences two types of condensation, termed single- and double-pinning, which can be characterized by one (single-pinning) or two (double-pinning) edge contact angles describing the shape of menisci pinned at the system edges. For both types of capillary condensation we formulate the Kelvin-like equation and determine the conditions under which the given type of condensation occurs. We construct the global phase diagram revealing a reentrant phenomenon pertinent to the change of the capillary condensation type upon varying the inclination of the walls. Asymptotic properties of the system are discussed and a link with related phase phenomena in different systems is made. Finally, we show that the change from a single- to a double-pinned state is a continuous transition, the character of which depends on the wetting properties of the walls.","sentences":["We study the condensation of fluids confined by a pair of non-parallel plates of finite height $H$.","We show that such a system experiences two types of condensation, termed single- and double-pinning, which can be characterized by one (single-pinning) or two (double-pinning) edge contact angles describing the shape of menisci pinned at the system edges.","For both types of capillary condensation we formulate the Kelvin-like equation and determine the conditions under which the given type of condensation occurs.","We construct the global phase diagram revealing a reentrant phenomenon pertinent to the change of the capillary condensation type upon varying the inclination of the walls.","Asymptotic properties of the system are discussed and a link with related phase phenomena in different systems is made.","Finally, we show that the change from a single- to a double-pinned state is a continuous transition, the character of which depends on the wetting properties of the walls."],"url":"http://arxiv.org/abs/2405.12539v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 07:05:39","title":"Flow of unitary matrices: Real-space winding numbers in one and three dimensions","abstract":"The notion of the flow introduced by Kitaev is a manifestly topological formulation of the winding number on a real lattice. First, we show in this paper that the flow is quite useful for practical numerical computations for systems without translational invariance. Second, we extend it to three dimensions. Namely, we derive a formula of the flow on a three-dimensional lattice, which corresponds to the conventional winding number when systems have translational invariance.","sentences":["The notion of the flow introduced by Kitaev is a manifestly topological formulation of the winding number on a real lattice.","First, we show in this paper that the flow is quite useful for practical numerical computations for systems without translational invariance.","Second, we extend it to three dimensions.","Namely, we derive a formula of the flow on a three-dimensional lattice, which corresponds to the conventional winding number when systems have translational invariance."],"url":"http://arxiv.org/abs/2405.12537v1","category":"nlin.CD"}
{"created":"2024-05-21 06:58:00","title":"PhiBE: A PDE-based Bellman Equation for Continuous Time Policy Evaluation","abstract":"In this paper, we address the problem of continuous-time reinforcement learning in scenarios where the dynamics follow a stochastic differential equation. When the underlying dynamics remain unknown and we have access only to discrete-time information, how can we effectively conduct policy evaluation? We first highlight that the commonly used Bellman equation (BE) is not always a reliable approximation to the true value function. We then introduce a new bellman equation, PhiBE, which integrates the discrete-time information into a PDE formulation. The new bellman equation offers a more accurate approximation to the true value function, especially in scenarios where the underlying dynamics change slowly. Moreover, we extend PhiBE to higher orders, providing increasingly accurate approximations. We conduct the error analysis for both BE and PhiBE with explicit dependence on the discounted coefficient, the reward and the dynamics. Additionally, we present a model-free algorithm to solve PhiBE when only discrete-time trajectory data is available. Numerical experiments are provided to validate the theoretical guarantees we propose.","sentences":["In this paper, we address the problem of continuous-time reinforcement learning in scenarios where the dynamics follow a stochastic differential equation.","When the underlying dynamics remain unknown and we have access only to discrete-time information, how can we effectively conduct policy evaluation?","We first highlight that the commonly used Bellman equation (BE) is not always a reliable approximation to the true value function.","We then introduce a new bellman equation, PhiBE, which integrates the discrete-time information into a PDE formulation.","The new bellman equation offers a more accurate approximation to the true value function, especially in scenarios where the underlying dynamics change slowly.","Moreover, we extend PhiBE to higher orders, providing increasingly accurate approximations.","We conduct the error analysis for both BE and PhiBE with explicit dependence on the discounted coefficient, the reward and the dynamics.","Additionally, we present a model-free algorithm to solve PhiBE when only discrete-time trajectory data is available.","Numerical experiments are provided to validate the theoretical guarantees we propose."],"url":"http://arxiv.org/abs/2405.12535v1","category":"math.OC"}
{"created":"2024-05-21 06:40:35","title":"BSN: First Photometric Light Curve Analysis of Two W-type Contact Binary Systems OP Boo and V0511 Cam","abstract":"This study presented the first light curve analysis of the OP Boo and V0511 Cam binary stars, which was conducted in the frame of the Binary Systems of South and North (BSN) Project. Photometric ground-based observations were conducted with standard filters at two observatories in the Czech Republic. We computed a new ephemeris for each of the systems using our extracted times of minima, TESS data, and additional literature. Linear fits for O-C diagrams of both systems were considered using the Markov Chain Monte Carlo (MCMC) method. The light curves were analyzed using the Wilson-Devinney (WD) binary code combined with the Monte Carlo (MC) simulation. The light curve solutions of both target systems required a cold starspot. The absolute parameters of the systems were calculated by using a P-M parameter relationship. The positions of the systems were also depicted on the Hertzsprung-Russell (HR), P-L, logMtot-logJ0, and T-M diagrams. The second component in both systems is determined to be a more massive and hotter star. Therefore, it can be concluded that both systems are W-type contact binary systems.","sentences":["This study presented the first light curve analysis of the OP Boo and V0511 Cam binary stars, which was conducted in the frame of the Binary Systems of South and North (BSN) Project.","Photometric ground-based observations were conducted with standard filters at two observatories in the Czech Republic.","We computed a new ephemeris for each of the systems using our extracted times of minima, TESS data, and additional literature.","Linear fits for O-C diagrams of both systems were considered using the Markov Chain Monte Carlo (MCMC) method.","The light curves were analyzed using the Wilson-Devinney (WD) binary code combined with the Monte Carlo (MC) simulation.","The light curve solutions of both target systems required a cold starspot.","The absolute parameters of the systems were calculated by using a P-M parameter relationship.","The positions of the systems were also depicted on the Hertzsprung-Russell (HR), P-L, logMtot-logJ0, and T-M diagrams.","The second component in both systems is determined to be a more massive and hotter star.","Therefore, it can be concluded that both systems are W-type contact binary systems."],"url":"http://arxiv.org/abs/2405.12529v1","category":"astro-ph.SR"}
{"created":"2024-05-21 06:35:25","title":"Nonlocal free-energy density functional for warm dense matter","abstract":"Finite-temperature orbital-free density functional theory (FT-OFDFT) holds significant promise for simulating warm dense matter due to its favorable scaling with both system size and temperature. However, the lack of the numerically accurate and transferable noninteracting free energy functionals results in a limit on the application of FT-OFDFT for warm dense matter simulations. Here, a nonlocal free energy functional, named XWMF, was derived by line integrals for FT-OFDFT simulations. Particularly, a designed integral path, wherein the electronic density varies from uniform to inhomogeneous, was employed to accurately describe deviations in response behavior from the uniform electron gas. The XWMF has been benchmarked by a range of warm dense matter systems including the Si, Al, H, He, and H-He mixture. The simulated results demonstrate that FT-OFDFT within XWMF achieves remarkable performance for accuracy and numerical stability. It is worth noting that XWMF exhibits a low computational cost for large-scale ab~initio simulations, offering exciting opportunities for the realistic simulations of warm dense matter systems covering a broad range of temperatures and pressures.","sentences":["Finite-temperature orbital-free density functional theory (FT-OFDFT) holds significant promise for simulating warm dense matter due to its favorable scaling with both system size and temperature.","However, the lack of the numerically accurate and transferable noninteracting free energy functionals results in a limit on the application of FT-OFDFT for warm dense matter simulations.","Here, a nonlocal free energy functional, named XWMF, was derived by line integrals for FT-OFDFT simulations.","Particularly, a designed integral path, wherein the electronic density varies from uniform to inhomogeneous, was employed to accurately describe deviations in response behavior from the uniform electron gas.","The XWMF has been benchmarked by a range of warm dense matter systems including the Si, Al, H, He, and H-He mixture.","The simulated results demonstrate that FT-OFDFT within XWMF achieves remarkable performance for accuracy and numerical stability.","It is worth noting that XWMF exhibits a low computational cost for large-scale ab~initio simulations, offering exciting opportunities for the realistic simulations of warm dense matter systems covering a broad range of temperatures and pressures."],"url":"http://arxiv.org/abs/2405.12527v1","category":"physics.comp-ph"}
{"created":"2024-05-21 06:27:25","title":"APTT: An accuracy-preserved tensor-train method for the Boltzmann-BGK equation","abstract":"Solving the Boltzmann-BGK equation with traditional numerical methods suffers from high computational and memory costs due to the curse of dimensionality. In this paper, we propose a novel accuracy-preserved tensor-train (APTT) method to efficiently solve the Boltzmann-BGK equation. A second-order finite difference scheme is applied to discretize the Boltzmann-BGK equation, resulting in a tensor algebraic system at each time step. Based on the low-rank TT representation, the tensor algebraic system is then approximated as a TT-based low-rank system, which is efficiently solved using the TT-modified alternating least-squares (TT-MALS) solver. Thanks to the low-rank TT representation, the APTT method can significantly reduce the computational and memory costs compared to traditional numerical methods. Theoretical analysis demonstrates that the APTT method maintains the same convergence rate as that of the finite difference scheme. The convergence rate and efficiency of the APTT method are validated by several benchmark test cases.","sentences":["Solving the Boltzmann-BGK equation with traditional numerical methods suffers from high computational and memory costs due to the curse of dimensionality.","In this paper, we propose a novel accuracy-preserved tensor-train (APTT) method to efficiently solve the Boltzmann-BGK equation.","A second-order finite difference scheme is applied to discretize the Boltzmann-BGK equation, resulting in a tensor algebraic system at each time step.","Based on the low-rank TT representation, the tensor algebraic system is then approximated as a TT-based low-rank system, which is efficiently solved using the TT-modified alternating least-squares (TT-MALS) solver.","Thanks to the low-rank TT representation, the APTT method can significantly reduce the computational and memory costs compared to traditional numerical methods.","Theoretical analysis demonstrates that the APTT method maintains the same convergence rate as that of the finite difference scheme.","The convergence rate and efficiency of the APTT method are validated by several benchmark test cases."],"url":"http://arxiv.org/abs/2405.12524v1","category":"math.NA"}
{"created":"2024-05-21 06:26:10","title":"Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models","abstract":"This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity and sensitivity to hyperparameters. We propose training sparse autoencoders on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples. We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations. By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits without the need for expensive ablations or architectural modifications. On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds. Notably, we require only 5-10 text examples for each task to learn robust representations. Our findings highlight the promise of discrete sparse autoencoders for scalable and efficient mechanistic interpretability, offering a new direction for analysing the inner workings of large language models.","sentences":["This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders.","Our approach addresses key limitations of existing techniques, namely computational complexity and sensitivity to hyperparameters.","We propose training sparse autoencoders on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples.","We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations.","By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits without the need for expensive ablations or architectural modifications.","On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds.","Notably, we require only 5-10 text examples for each task to learn robust representations.","Our findings highlight the promise of discrete sparse autoencoders for scalable and efficient mechanistic interpretability, offering a new direction for analysing the inner workings of large language models."],"url":"http://arxiv.org/abs/2405.12522v1","category":"cs.CL"}
{"created":"2024-05-21 06:08:59","title":"WST - Widefield Spectroscopic Telescope: Motivation, science drivers and top-level requirements for a new dedicated facility","abstract":"In this paper, we describe the wide-field spectroscopic survey telescope (WST) project. WST is a 12-metre wide-field spectroscopic survey telescope with simultaneous operation of a large field-of-view (3 sq. degree), high-multiplex (20,000) multi-object spectrograph (MOS), with both a low and high-resolution modes, and a giant 3x3 arcmin2 integral field spectrograph (IFS). In scientific capability, these specifications place WST far ahead of existing and planned facilities. In only 5 years of operation, the MOS would target 250 million galaxies and 25 million stars at low spectral resolution, plus 2 million stars at high resolution. Without need for pre-imaged targets, the IFS would deliver 4 billion spectra offering many serendipitous discoveries. Given the current investment in deep imaging surveys and noting the diagnostic power of spectroscopy, WST will fill a crucial gap in astronomical capability and work in synergy with future ground and space-based facilities. We show how it can address outstanding scientific questions in the areas of cosmology; galaxy assembly, evolution, and enrichment, including our own Milky Way; the origin of stars and planets; and time domain and multi-messenger astrophysics. WST's uniquely rich dataset may yield unforeseen discoveries in many of these areas. The telescope and instruments are designed as an integrated system and will mostly use existing technology, with the aim to minimise the carbon footprint and environmental impact. We will propose WST as the next European Southern Observatory (ESO) project after completion of the 39-metre ELT.","sentences":["In this paper, we describe the wide-field spectroscopic survey telescope (WST) project.","WST is a 12-metre wide-field spectroscopic survey telescope with simultaneous operation of a large field-of-view (3 sq. degree), high-multiplex (20,000) multi-object spectrograph (MOS), with both a low and high-resolution modes, and a giant 3x3 arcmin2 integral field spectrograph (IFS).","In scientific capability, these specifications place WST far ahead of existing and planned facilities.","In only 5 years of operation, the MOS would target 250 million galaxies and 25 million stars at low spectral resolution, plus 2 million stars at high resolution.","Without need for pre-imaged targets, the IFS would deliver 4 billion spectra offering many serendipitous discoveries.","Given the current investment in deep imaging surveys and noting the diagnostic power of spectroscopy, WST will fill a crucial gap in astronomical capability and work in synergy with future ground and space-based facilities.","We show how it can address outstanding scientific questions in the areas of cosmology; galaxy assembly, evolution, and enrichment, including our own Milky Way; the origin of stars and planets; and time domain and multi-messenger astrophysics.","WST's uniquely rich dataset may yield unforeseen discoveries in many of these areas.","The telescope and instruments are designed as an integrated system and will mostly use existing technology, with the aim to minimise the carbon footprint and environmental impact.","We will propose WST as the next European Southern Observatory (ESO) project after completion of the 39-metre ELT."],"url":"http://arxiv.org/abs/2405.12518v1","category":"astro-ph.IM"}
{"created":"2024-05-21 06:08:26","title":"Hybrid dynamics of hyperbolic automorphisms of K3 surfaces","abstract":"We study degenerating families of hyperbolic dynamics over complex K3 surfaces by means of the theory of hybrid spaces by Boucksom, Favre, and Jonsson. For an analytic family of hyperbolic automorphisms $\\{f_t: X_t\\to X_t\\}_{t\\in\\mathbb{D}^*}$ over K3 surfaces $X_t$ that is possibly meromorphically degenerating at the origin, we consider the family of invariant measures $\\{\\eta_t\\}$ on $X_t$ constructed by Cantat. The family $f_t$ induces a hyperbolic automorphism $f_{\\mathbb{C}((t))}^{\\mathop{\\mathrm{an}}}:X_{\\mathbb{C}((t))}^{\\mathop{\\mathrm{an}}}\\to X_{\\mathbb{C}((t))}^{\\mathop{\\mathrm{an}}}$ over the induced non-archimedean K3 surface, where we also have a measure $\\eta_0$ by Filip. Our main theorem states the weak convergence of $\\{\\eta_t\\}$ to $\\eta_0$ as $t\\to0$ over the induced so-called hybrid space.","sentences":["We study degenerating families of hyperbolic dynamics over complex K3 surfaces by means of the theory of hybrid spaces by Boucksom, Favre, and Jonsson.","For an analytic family of hyperbolic automorphisms $\\{f_t: X_t\\to","X_t\\}_{t\\in\\mathbb{D}^*}$ over K3 surfaces $X_t$ that is possibly meromorphically degenerating at the origin, we consider the family of invariant measures $\\{\\eta_t\\}$ on $X_t$ constructed by Cantat.","The family $f_t$ induces a hyperbolic automorphism $f_{\\mathbb{C}((t))}^{\\mathop{\\mathrm{an}}}:X_{\\mathbb{C}((t))}^{\\mathop{\\mathrm{an}}}\\to X_{\\mathbb{C}((t))}^{\\mathop{\\mathrm{an}}}$ over the induced non-archimedean K3 surface, where we also have a measure $\\eta_0$ by Filip.","Our main theorem states the weak convergence of $\\{\\eta_t\\}$ to $\\eta_0$ as $t\\to0$ over the induced so-called hybrid space."],"url":"http://arxiv.org/abs/2405.12517v1","category":"math.DS"}
{"created":"2024-05-21 05:37:04","title":"Upper bounding the quantum space complexity for computing class group and principal ideal problem","abstract":"In this paper, we calculate the upper bound on quantum space complexity of the quantum algorithms proposed by Biasse and Song (SODA'16) for solving class group computation and the principal ideal problem using the reductions to $S$-unit group computation. We follow the approach of Barbulescu and Poulalion (AFRICACRYPT'23) and the framework given by de Boer, Ducas, and Fehr (EUROCRYPT'20) and Eisentr\\\"{a}ger, Hallgren, Kitaev, and Song (STOC'14).","sentences":["In this paper, we calculate the upper bound on quantum space complexity of the quantum algorithms proposed by Biasse and Song (SODA'16) for solving class group computation and the principal ideal problem using the reductions to $S$-unit group computation.","We follow the approach of Barbulescu and Poulalion (AFRICACRYPT'23) and the framework given by de Boer, Ducas, and Fehr (EUROCRYPT'20) and Eisentr\\\"{a}ger, Hallgren, Kitaev, and Song (STOC'14)."],"url":"http://arxiv.org/abs/2405.12508v1","category":"quant-ph"}
{"created":"2024-05-21 05:29:38","title":"SN 2019tua : A Type IIb Supernova with Multiple Bumps in the Light Curves","abstract":"We present photometric and spectroscopic observations and analysis of the type IIb supernova (SN) SN 2019tua, which exhibits multiple bumps in its declining light curves between 40 and 65 days after discovery. SN 2019tua shows a time to peak of about 25 days similar to other type IIb SNe. Our observations indicate a decrease in its brightness of about 1 magnitude in the 60 days after the peak. At about days 50, and 60, its multiband light curves exhibit bumpy behavior. The complex luminosity evolution of SN 2019tua could not be well modeled with a single currently popular energy source model, e.g., radioactive decay of $^{56}$Ni, magnetar, interaction between the ejecta and a circumstellar shell. Even though the magnetar model has a smaller \\( \\chi^2 / \\text{dof} \\) value, the complex changes in SN 2019tua's brightness suggest that more than one physical process might be involved. We propose a hybrid CSM interaction plus $^{56}$Ni model to explain the bolometric light curve (LC) of SN 2019tua. The fitting results show that the ejecta mass $M_{\\rm ej} \\approx 2.4~M_\\odot$, the total CSM mass $M_{\\rm CSM} \\approx 1.0~M_\\odot$, and the $^{56}$Ni mass $M_{\\rm Ni} \\approx 0.4~M_\\odot$. The total kinetic energy of the ejecta is $E_k\\approx 0.5 \\times 10^{51}\\rm~erg$. Pre-existing multiple shells suggest that the progenitor of SN 2019tua experienced mass ejections within approximately $\\sim6 - 44$ years prior to the explosion.","sentences":["We present photometric and spectroscopic observations and analysis of the type IIb supernova (SN) SN 2019tua, which exhibits multiple bumps in its declining light curves between 40 and 65 days after discovery.","SN 2019tua shows a time to peak of about 25 days similar to other type IIb SNe.","Our observations indicate a decrease in its brightness of about 1 magnitude in the 60 days after the peak.","At about days 50, and 60, its multiband light curves exhibit bumpy behavior.","The complex luminosity evolution of SN 2019tua could not be well modeled with a single currently popular energy source model, e.g., radioactive decay of $^{56}$Ni, magnetar, interaction between the ejecta and a circumstellar shell.","Even though the magnetar model has a smaller \\( \\chi^2 / \\text{dof} \\) value, the complex changes in SN 2019tua's brightness suggest that more than one physical process might be involved.","We propose a hybrid CSM interaction plus $^{56}$Ni model to explain the bolometric light curve (LC) of SN 2019tua.","The fitting results show that the ejecta mass $M_{\\rm ej} \\approx 2.4~M_\\odot$, the total CSM mass $M_{\\rm CSM} \\approx 1.0~M_\\odot$, and the $^{56}$Ni mass $M_{\\rm Ni} \\approx 0.4~M_\\odot$. The total kinetic energy of the ejecta is $E_k\\approx 0.5 \\times 10^{51}\\rm~erg$. Pre-existing multiple shells suggest that the progenitor of SN 2019tua experienced mass ejections within approximately $\\sim6 - 44$ years prior to the explosion."],"url":"http://arxiv.org/abs/2405.12504v1","category":"astro-ph.HE"}
{"created":"2024-05-21 05:00:30","title":"Entropic associative memory for real world images","abstract":"The entropic associative memory (EAM) is a computational model of natural memory incorporating some of its putative properties of being associative, distributed, declarative, abstractive and constructive. Previous experiments satisfactorily tested the model on structured, homogeneous and conventional data: images of manuscripts digits and letters, images of clothing, and phone representations. In this work we show that EAM appropriately stores, recognizes and retrieves complex and unconventional images of animals and vehicles. Additionally, the memory system generates meaningful retrieval association chains for such complex images. The retrieved objects can be seen as proper memories, associated recollections or products of imagination.","sentences":["The entropic associative memory (EAM) is a computational model of natural memory incorporating some of its putative properties of being associative, distributed, declarative, abstractive and constructive.","Previous experiments satisfactorily tested the model on structured, homogeneous and conventional data: images of manuscripts digits and letters, images of clothing, and phone representations.","In this work we show that EAM appropriately stores, recognizes and retrieves complex and unconventional images of animals and vehicles.","Additionally, the memory system generates meaningful retrieval association chains for such complex images.","The retrieved objects can be seen as proper memories, associated recollections or products of imagination."],"url":"http://arxiv.org/abs/2405.12500v1","category":"cs.LG"}
{"created":"2024-05-21 04:53:39","title":"A Survey of Integrating Wireless Technology into Active Noise Control","abstract":"Active Noise Control (ANC) is a widely adopted technology for reducing environmental noise across various scenarios. This paper focuses on enhancing noise reduction performance, particularly through the refinement of signal quality fed into ANC systems. We discuss the main wireless technique integrated into the ANC system, equipped with some innovative algorithms, in diverse environments. Instead of using microphone arrays, which increase the computation complexity of the ANC system, to isolate multiple noise sources to improve noise reduction performance, the application of the wireless technique avoids extra computation demand. Wireless transmissions of reference, error, and control signals are also applied to improve the convergence performance of the ANC system. Furthermore, this paper lists some wireless ANC applications, such as earbuds, headphones, windows, and headrests, underscoring their adaptability and efficiency in various settings.","sentences":["Active Noise Control (ANC) is a widely adopted technology for reducing environmental noise across various scenarios.","This paper focuses on enhancing noise reduction performance, particularly through the refinement of signal quality fed into ANC systems.","We discuss the main wireless technique integrated into the ANC system, equipped with some innovative algorithms, in diverse environments.","Instead of using microphone arrays, which increase the computation complexity of the ANC system, to isolate multiple noise sources to improve noise reduction performance, the application of the wireless technique avoids extra computation demand.","Wireless transmissions of reference, error, and control signals are also applied to improve the convergence performance of the ANC system.","Furthermore, this paper lists some wireless ANC applications, such as earbuds, headphones, windows, and headrests, underscoring their adaptability and efficiency in various settings."],"url":"http://arxiv.org/abs/2405.12496v1","category":"eess.AS"}
{"created":"2024-05-21 04:30:09","title":"Visualizing, Rethinking, and Mining the Loss Landscape of Deep Neural Networks","abstract":"The loss landscape of deep neural networks (DNNs) is commonly considered complex and wildly fluctuated. However, an interesting observation is that the loss surfaces plotted along Gaussian noise directions are almost v-basin ones with the perturbed model lying on the basin. This motivates us to rethink whether the 1D or 2D subspace could cover more complex local geometry structures, and how to mine the corresponding perturbation directions. This paper systematically and gradually categorizes the 1D curves from simple to complex, including v-basin, v-side, w-basin, w-peak, and vvv-basin curves. Notably, the latter two types are already hard to obtain via the intuitive construction of specific perturbation directions, and we need to propose proper mining algorithms to plot the corresponding 1D curves. Combining these 1D directions, various types of 2D surfaces are visualized such as the saddle surfaces and the bottom of a bottle of wine that are only shown by demo functions in previous works. Finally, we propose theoretical insights from the lens of the Hessian matrix to explain the observed several interesting phenomena.","sentences":["The loss landscape of deep neural networks (DNNs) is commonly considered complex and wildly fluctuated.","However, an interesting observation is that the loss surfaces plotted along Gaussian noise directions are almost v-basin ones with the perturbed model lying on the basin.","This motivates us to rethink whether the 1D or 2D subspace could cover more complex local geometry structures, and how to mine the corresponding perturbation directions.","This paper systematically and gradually categorizes the 1D curves from simple to complex, including v-basin, v-side, w-basin, w-peak, and vvv-basin curves.","Notably, the latter two types are already hard to obtain via the intuitive construction of specific perturbation directions, and we need to propose proper mining algorithms to plot the corresponding 1D curves.","Combining these 1D directions, various types of 2D surfaces are visualized such as the saddle surfaces and the bottom of a bottle of wine that are only shown by demo functions in previous works.","Finally, we propose theoretical insights from the lens of the Hessian matrix to explain the observed several interesting phenomena."],"url":"http://arxiv.org/abs/2405.12493v1","category":"cs.LG"}
{"created":"2024-05-21 04:24:47","title":"Bridging the Gap Between Domain-specific Frameworks and Multiple Hardware Devices","abstract":"The rapid development of domain-specific frameworks has presented us with a significant challenge: The current approach of implementing solutions on a case-by-case basis incurs a theoretical complexity of O(M*N), thereby increasing the cost of porting applications to different hardware platforms. To address these challenges, we propose a systematic methodology that effectively bridges the gap between domain-specific frameworks and multiple hardware devices, reducing porting complexity to O(M+N). The approach utilizes multi-layer abstractions. Different domain-specific abstractions are employed to represent applications from various domains. These abstractions are then transformed into a unified abstraction, which is subsequently translated into combinations of primitive operators. Finally, these operators are mapped to multiple hardware platforms. The implemented unified framework supports deep learning, classical machine learning, and data analysis across X86, ARM, RISC-V, IoT devices, and GPU. It outperforms existing solutions like scikit-learn, hummingbird, Spark, and pandas, achieving impressive speedups: 1.1x to 3.83x on X86 servers, 1.06x to 4.33x on ARM IoT devices, 1.25x to 3.72x on RISC-V IoT devices, and 1.93x on GPU. The source code is available at https://github.com/BenchCouncil/bridger.git.","sentences":["The rapid development of domain-specific frameworks has presented us with a significant challenge: The current approach of implementing solutions on a case-by-case basis incurs a theoretical complexity of O(M*N), thereby increasing the cost of porting applications to different hardware platforms.","To address these challenges, we propose a systematic methodology that effectively bridges the gap between domain-specific frameworks and multiple hardware devices, reducing porting complexity to O(M+N).","The approach utilizes multi-layer abstractions.","Different domain-specific abstractions are employed to represent applications from various domains.","These abstractions are then transformed into a unified abstraction, which is subsequently translated into combinations of primitive operators.","Finally, these operators are mapped to multiple hardware platforms.","The implemented unified framework supports deep learning, classical machine learning, and data analysis across X86, ARM, RISC-V, IoT devices, and GPU.","It outperforms existing solutions like scikit-learn, hummingbird, Spark, and pandas, achieving impressive speedups: 1.1x to 3.83x on X86 servers, 1.06x to 4.33x on ARM IoT devices, 1.25x to 3.72x on RISC-V IoT devices, and 1.93x on GPU.","The source code is available at https://github.com/BenchCouncil/bridger.git."],"url":"http://arxiv.org/abs/2405.12491v1","category":"cs.SE"}
{"created":"2024-05-21 04:10:26","title":"3DSS-Mamba: 3D-Spectral-Spatial Mamba for Hyperspectral Image Classification","abstract":"Hyperspectral image (HSI) classification constitutes the fundamental research in remote sensing fields. Convolutional Neural Networks (CNNs) and Transformers have demonstrated impressive capability in capturing spectral-spatial contextual dependencies. However, these architectures suffer from limited receptive fields and quadratic computational complexity, respectively. Fortunately, recent Mamba architectures built upon the State Space Model integrate the advantages of long-range sequence modeling and linear computational efficiency, exhibiting substantial potential in low-dimensional scenarios. Motivated by this, we propose a novel 3D-Spectral-Spatial Mamba (3DSS-Mamba) framework for HSI classification, allowing for global spectral-spatial relationship modeling with greater computational efficiency. Technically, a spectral-spatial token generation (SSTG) module is designed to convert the HSI cube into a set of 3D spectral-spatial tokens. To overcome the limitations of traditional Mamba, which is confined to modeling causal sequences and inadaptable to high-dimensional scenarios, a 3D-Spectral-Spatial Selective Scanning (3DSS) mechanism is introduced, which performs pixel-wise selective scanning on 3D hyperspectral tokens along the spectral and spatial dimensions. Five scanning routes are constructed to investigate the impact of dimension prioritization. The 3DSS scanning mechanism combined with conventional mapping operations forms the 3D-spectral-spatial mamba block (3DMB), enabling the extraction of global spectral-spatial semantic representations. Experimental results and analysis demonstrate that the proposed method outperforms the state-of-the-art methods on HSI classification benchmarks.","sentences":["Hyperspectral image (HSI) classification constitutes the fundamental research in remote sensing fields.","Convolutional Neural Networks (CNNs) and Transformers have demonstrated impressive capability in capturing spectral-spatial contextual dependencies.","However, these architectures suffer from limited receptive fields and quadratic computational complexity, respectively.","Fortunately, recent Mamba architectures built upon the State Space Model integrate the advantages of long-range sequence modeling and linear computational efficiency, exhibiting substantial potential in low-dimensional scenarios.","Motivated by this, we propose a novel 3D-Spectral-Spatial Mamba (3DSS-Mamba) framework for HSI classification, allowing for global spectral-spatial relationship modeling with greater computational efficiency.","Technically, a spectral-spatial token generation (SSTG) module is designed to convert the HSI cube into a set of 3D spectral-spatial tokens.","To overcome the limitations of traditional Mamba, which is confined to modeling causal sequences and inadaptable to high-dimensional scenarios, a 3D-Spectral-Spatial Selective Scanning (3DSS) mechanism is introduced, which performs pixel-wise selective scanning on 3D hyperspectral tokens along the spectral and spatial dimensions.","Five scanning routes are constructed to investigate the impact of dimension prioritization.","The 3DSS scanning mechanism combined with conventional mapping operations forms the 3D-spectral-spatial mamba block (3DMB), enabling the extraction of global spectral-spatial semantic representations.","Experimental results and analysis demonstrate that the proposed method outperforms the state-of-the-art methods on HSI classification benchmarks."],"url":"http://arxiv.org/abs/2405.12487v1","category":"cs.CV"}
{"created":"2024-05-21 04:06:26","title":"Formal languages, spin systems, and quasicrystals","abstract":"We present a categorical formalism for context-free languages with morphisms given by correspondences obtained from rational transductions. We show that D0L-systems are a special case of the correspondences that define morphisms in this category. We construct a functorial mapping to aperiodic spin chains. We then generalize this construction to a class of mildly context sensitive grammars, the multiple-context-free grammars (MCFG), with a similar functorial mapping to spin systems in higher dimensions, with Boltzmann weights describing interacting spins on vertices of hypercubes. We show that a particular motivating example for this general construction is provided by the Korepin completely integrable model on the icosahedral quasicrystal, which we construct as the spin system associated to a multiple-context-free grammar describing the geometry of the Ammann planes quasilattice. We review the main properties of this spin system, including solvability, bulk free energy, and criticality, based on results of Baxter and the known relation to the Zamolodchikov tetrahedron equation. We show that the latter has a generalization for the Boltzmannweights on hypercubes of the spin systems associated to more general MCFGs in terms of two dual cubulations of the n-simplex. We formulate analogous questions about bulk free energy and criticality for our construction of spin systems.","sentences":["We present a categorical formalism for context-free languages with morphisms given by correspondences obtained from rational transductions.","We show that D0L-systems are a special case of the correspondences that define morphisms in this category.","We construct a functorial mapping to aperiodic spin chains.","We then generalize this construction to a class of mildly context sensitive grammars, the multiple-context-free grammars (MCFG), with a similar functorial mapping to spin systems in higher dimensions, with Boltzmann weights describing interacting spins on vertices of hypercubes.","We show that a particular motivating example for this general construction is provided by the Korepin completely integrable model on the icosahedral quasicrystal, which we construct as the spin system associated to a multiple-context-free grammar describing the geometry of the Ammann planes quasilattice.","We review the main properties of this spin system, including solvability, bulk free energy, and criticality, based on results of Baxter and the known relation to the Zamolodchikov tetrahedron equation.","We show that the latter has a generalization for the Boltzmannweights on hypercubes of the spin systems associated to more general MCFGs in terms of two dual cubulations of the n-simplex.","We formulate analogous questions about bulk free energy and criticality for our construction of spin systems."],"url":"http://arxiv.org/abs/2405.12485v1","category":"math-ph"}
{"created":"2024-05-21 04:06:14","title":"Meta-Homogenization for Knitwear Simulation","abstract":"This paper presents meta-homogenization, a spatially varying homogenization scheme for knitwear simulation. We are motivated by the observation that macro-scale fabric dynamics is strongly correlated with its underlying knitting patterns. Therefore, homogenization towards a single material is less effective when the knitting is complex and non-repetitive. Our method tackles this challenge by homogenizing the yarn-level material locally at volumetric elements. Assigning a virtual volume of a knitting structure enables us to model bending and twisting effects via a simple volume-preserving penalty and thus effectively alleviates the material nonlinearity. We employ an adjoint Gauss-Newton formulation to battle the dimensionality challenge of such per-element material optimization. This intuitive material model makes the forward simulation GPU-friendly. To this end, our pipeline also equips a novel domain-decomposed subspace solver crafted for GPU projective dynamics, which makes our simulator hundreds of times faster than the yarn-level simulator. Experiments validate the capability and effectiveness of meta-homogenization. Our method produces realistic animations of knitwear matching the quality of full-scale yarn-level simulations. It is also orders of magnitude faster than existing homogenization techniques in both the training and simulation stages.","sentences":["This paper presents meta-homogenization, a spatially varying homogenization scheme for knitwear simulation.","We are motivated by the observation that macro-scale fabric dynamics is strongly correlated with its underlying knitting patterns.","Therefore, homogenization towards a single material is less effective when the knitting is complex and non-repetitive.","Our method tackles this challenge by homogenizing the yarn-level material locally at volumetric elements.","Assigning a virtual volume of a knitting structure enables us to model bending and twisting effects via a simple volume-preserving penalty and thus effectively alleviates the material nonlinearity.","We employ an adjoint Gauss-Newton formulation to battle the dimensionality challenge of such per-element material optimization.","This intuitive material model makes the forward simulation GPU-friendly.","To this end, our pipeline also equips a novel domain-decomposed subspace solver crafted for GPU projective dynamics, which makes our simulator hundreds of times faster than the yarn-level simulator.","Experiments validate the capability and effectiveness of meta-homogenization.","Our method produces realistic animations of knitwear matching the quality of full-scale yarn-level simulations.","It is also orders of magnitude faster than existing homogenization techniques in both the training and simulation stages."],"url":"http://arxiv.org/abs/2405.12484v1","category":"cs.GR"}
{"created":"2024-05-21 04:04:52","title":"Coherence spectroscopy by the Nth power of the measured signal in an interferometer overcoming the diffraction limit","abstract":"Coherence spectroscopy has been intensively studied over the last several decades for various applications in science and engineering. The Rayleigh criterion defines the resolution limit of an interferometer, where many-wave interference beats the resolution limit of a two-slit system. On the other hand, the diffraction angle in a slit is reduced by the Kth power of the measured signal, resulting in the shot-noise limit. Here, the Kth power of the measured signal in an N-slit interferometer is studied for enhanced coherence spectroscopy to overcome the resolution limit of the original system. The Kth power to the individual intensities of the N-slit interferometer is numerically demonstrated for enhanced resolution satisfying the shot-noise limit. As a result, the Kth power of the intensity beats the resolution limit of the N-slit interferometer, in which the out-of-shelf spectrometer or wavelength meter can be a primary beneficiary of this technique. Due to the same resolution of the Heisenberg limit in quantum sensing as in the N-slit interference fringe, the proposed Kth power technique also beats the superresolution in quantum metrology.","sentences":["Coherence spectroscopy has been intensively studied over the last several decades for various applications in science and engineering.","The Rayleigh criterion defines the resolution limit of an interferometer, where many-wave interference beats the resolution limit of a two-slit system.","On the other hand, the diffraction angle in a slit is reduced by the Kth power of the measured signal, resulting in the shot-noise limit.","Here, the Kth power of the measured signal in an N-slit interferometer is studied for enhanced coherence spectroscopy to overcome the resolution limit of the original system.","The Kth power to the individual intensities of the N-slit interferometer is numerically demonstrated for enhanced resolution satisfying the shot-noise limit.","As a result, the Kth power of the intensity beats the resolution limit of the N-slit interferometer, in which the out-of-shelf spectrometer or wavelength meter can be a primary beneficiary of this technique.","Due to the same resolution of the Heisenberg limit in quantum sensing as in the N-slit interference fringe, the proposed Kth power technique also beats the superresolution in quantum metrology."],"url":"http://arxiv.org/abs/2405.12482v1","category":"quant-ph"}
{"created":"2024-05-21 04:00:25","title":"On the role of Loewner entropy in statistical mechanics of 2D Ising system","abstract":"The fundamental properties of 2-dimensional (2D) Ising system were formulated using the Loewner theory. We focus on the role of the complexity measure of the 2D geometry, referred to as the Loewner entropy, to derive the statistical-mechanical relations of the 2D Ising system by analyzing the structure of the interface (i.e., the phase separation line). For the mixing property of the discrete Loewner evolution, we assume that the Loewner driving force ${\\it\\eta_s(n)}$ obtained from the interface has a stationary property, where the autocorrelation function $\\langle{\\it\\eta_s(0)\\eta_s(n)}\\rangle $ converges in the long-time limit. Using this fact, we reconstruct the continuous Loewner evolution driven by the diffusion process whose increments correspond to the sequence of ${\\it\\eta_s(n)}$, and the fractal dimension of the generated curve was derived. We show that these formulations lead to a novel expression of the Hamiltonian, grand canonical ensemble of the system, which also are applicable for the non-equilibrium state of the system. In addition, the relations on the central limit theorem (CLT) governing the local fluctuation of the interface, the non-equilibrium free energy, and fluctuation dissipation relation (FDR) were derived using the Loewner theory. The present results suggest a possible form of the complexity-based theory of the 2D statistical mechanical systems that is applicable for the non-equilibrium states.","sentences":["The fundamental properties of 2-dimensional (2D) Ising system were formulated using the Loewner theory.","We focus on the role of the complexity measure of the 2D geometry, referred to as the Loewner entropy, to derive the statistical-mechanical relations of the 2D Ising system by analyzing the structure of the interface (i.e., the phase separation line).","For the mixing property of the discrete Loewner evolution, we assume that the Loewner driving force ${\\it\\eta_s(n)}$ obtained from the interface has a stationary property, where the autocorrelation function $\\langle{\\it\\eta_s(0)\\eta_s(n)}\\rangle $ converges in the long-time limit.","Using this fact, we reconstruct the continuous Loewner evolution driven by the diffusion process whose increments correspond to the sequence of ${\\it\\eta_s(n)}$, and the fractal dimension of the generated curve was derived.","We show that these formulations lead to a novel expression of the Hamiltonian, grand canonical ensemble of the system, which also are applicable for the non-equilibrium state of the system.","In addition, the relations on the central limit theorem (CLT) governing the local fluctuation of the interface, the non-equilibrium free energy, and fluctuation dissipation relation (FDR) were derived using the Loewner theory.","The present results suggest a possible form of the complexity-based theory of the 2D statistical mechanical systems that is applicable for the non-equilibrium states."],"url":"http://arxiv.org/abs/2405.12481v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-21 03:50:32","title":"Towards Detecting and Mitigating Cognitive Bias in Spoken Conversational Search","abstract":"Instruments such as eye-tracking devices have contributed to understanding how users interact with screen-based search engines. However, user-system interactions in audio-only channels -- as is the case for Spoken Conversational Search (SCS) -- are harder to characterize, given the lack of instruments to effectively and precisely capture interactions. Furthermore, in this era of information overload, cognitive bias can significantly impact how we seek and consume information -- especially in the context of controversial topics or multiple viewpoints. This paper draws upon insights from multiple disciplines (including information seeking, psychology, cognitive science, and wearable sensors) to provoke novel conversations in the community. To this end, we discuss future opportunities and propose a framework including multimodal instruments and methods for experimental designs and settings. We demonstrate preliminary results as an example. We also outline the challenges and offer suggestions for adopting this multimodal approach, including ethical considerations, to assist future researchers and practitioners in exploring cognitive biases in SCS.","sentences":["Instruments such as eye-tracking devices have contributed to understanding how users interact with screen-based search engines.","However, user-system interactions in audio-only channels -- as is the case for Spoken Conversational Search (SCS) -- are harder to characterize, given the lack of instruments to effectively and precisely capture interactions.","Furthermore, in this era of information overload, cognitive bias can significantly impact how we seek and consume information -- especially in the context of controversial topics or multiple viewpoints.","This paper draws upon insights from multiple disciplines (including information seeking, psychology, cognitive science, and wearable sensors) to provoke novel conversations in the community.","To this end, we discuss future opportunities and propose a framework including multimodal instruments and methods for experimental designs and settings.","We demonstrate preliminary results as an example.","We also outline the challenges and offer suggestions for adopting this multimodal approach, including ethical considerations, to assist future researchers and practitioners in exploring cognitive biases in SCS."],"url":"http://arxiv.org/abs/2405.12480v1","category":"cs.HC"}
{"created":"2024-05-21 03:42:34","title":"Efficient Economic Model Predictive Control of Water Treatment Process with Learning-based Koopman Operator","abstract":"Used water treatment plays a pivotal role in advancing environmental sustainability. Economic model predictive control holds the promise of enhancing the overall operational performance of the water treatment facilities. In this study, we propose a data-driven economic predictive control approach within the Koopman modeling framework. First, we propose a deep learning-enabled input-output Koopman modeling approach, which predicts the overall economic operational cost of the wastewater treatment process based on input data and available output measurements that are directly linked to the operational costs. Subsequently, by leveraging this learned input-output Koopman model, a convex economic predictive control scheme is developed. The resulting predictive control problem can be efficiently solved by leveraging quadratic programming solvers, and complex non-convex optimization problems are bypassed. The proposed method is applied to a benchmark wastewater treatment process. The proposed method significantly improves the overall economic operational performance of the water treatment process. Additionally, the computational efficiency of the proposed method is significantly enhanced as compared to benchmark control solutions.","sentences":["Used water treatment plays a pivotal role in advancing environmental sustainability.","Economic model predictive control holds the promise of enhancing the overall operational performance of the water treatment facilities.","In this study, we propose a data-driven economic predictive control approach within the Koopman modeling framework.","First, we propose a deep learning-enabled input-output Koopman modeling approach, which predicts the overall economic operational cost of the wastewater treatment process based on input data and available output measurements that are directly linked to the operational costs.","Subsequently, by leveraging this learned input-output Koopman model, a convex economic predictive control scheme is developed.","The resulting predictive control problem can be efficiently solved by leveraging quadratic programming solvers, and complex non-convex optimization problems are bypassed.","The proposed method is applied to a benchmark wastewater treatment process.","The proposed method significantly improves the overall economic operational performance of the water treatment process.","Additionally, the computational efficiency of the proposed method is significantly enhanced as compared to benchmark control solutions."],"url":"http://arxiv.org/abs/2405.12478v1","category":"eess.SY"}
{"created":"2024-05-21 03:40:56","title":"Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery","abstract":"Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human reconstruction, it primarily relies on 2D pixel-level supervision, overlooking the geometric complexity and topological relationships of different body parts. To address this gap, we introduce the Hierarchical Graph Human Gaussian Control (HUGS) framework for achieving high-fidelity 3D human reconstruction. Our approach involves leveraging explicitly semantic priors of body parts to ensure the consistency of geometric topology, thereby enabling the capture of the complex geometrical and topological associations among body parts. Additionally, we disentangle high-frequency features from global human features to refine surface details in body parts. Extensive experiments demonstrate that our method exhibits superior performance in human body reconstruction, particularly in enhancing surface details and accurately reconstructing body part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.","sentences":["Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human reconstruction, it primarily relies on 2D pixel-level supervision, overlooking the geometric complexity and topological relationships of different body parts.","To address this gap, we introduce the Hierarchical Graph Human Gaussian Control (HUGS) framework for achieving high-fidelity 3D human reconstruction.","Our approach involves leveraging explicitly semantic priors of body parts to ensure the consistency of geometric topology, thereby enabling the capture of the complex geometrical and topological associations among body parts.","Additionally, we disentangle high-frequency features from global human features to refine surface details in body parts.","Extensive experiments demonstrate that our method exhibits superior performance in human body reconstruction, particularly in enhancing surface details and accurately reconstructing body part junctions.","Codes are available at https://wanghongsheng01.github.io/HUGS/."],"url":"http://arxiv.org/abs/2405.12477v1","category":"cs.CV"}
{"created":"2024-05-21 03:36:13","title":"Benchmarking Fish Dataset and Evaluation Metric in Keypoint Detection - Towards Precise Fish Morphological Assessment in Aquaculture Breeding","abstract":"Accurate phenotypic analysis in aquaculture breeding necessitates the quantification of subtle morphological phenotypes. Existing datasets suffer from limitations such as small scale, limited species coverage, and inadequate annotation of keypoints for measuring refined and complex morphological phenotypes of fish body parts. To address this gap, we introduce FishPhenoKey, a comprehensive dataset comprising 23,331 high-resolution images spanning six fish species. Notably, FishPhenoKey includes 22 phenotype-oriented annotations, enabling the capture of intricate morphological phenotypes. Motivated by the nuanced evaluation of these subtle morphologies, we also propose a new evaluation metric, Percentage of Measured Phenotype (PMP). It is designed to assess the accuracy of individual keypoint positions and is highly sensitive to the phenotypes measured using the corresponding keypoints. To enhance keypoint detection accuracy, we further propose a novel loss, Anatomically-Calibrated Regularization (ACR), that can be integrated into keypoint detection models, leveraging biological insights to refine keypoint localization. Our contributions set a new benchmark in fish phenotype analysis, addressing the challenges of precise morphological quantification and opening new avenues for research in sustainable aquaculture and genetic studies. Our dataset and code are available at https://github.com/WeizhenLiuBioinform/Fish-Phenotype-Detect.","sentences":["Accurate phenotypic analysis in aquaculture breeding necessitates the quantification of subtle morphological phenotypes.","Existing datasets suffer from limitations such as small scale, limited species coverage, and inadequate annotation of keypoints for measuring refined and complex morphological phenotypes of fish body parts.","To address this gap, we introduce FishPhenoKey, a comprehensive dataset comprising 23,331 high-resolution images spanning six fish species.","Notably, FishPhenoKey includes 22 phenotype-oriented annotations, enabling the capture of intricate morphological phenotypes.","Motivated by the nuanced evaluation of these subtle morphologies, we also propose a new evaluation metric, Percentage of Measured Phenotype (PMP).","It is designed to assess the accuracy of individual keypoint positions and is highly sensitive to the phenotypes measured using the corresponding keypoints.","To enhance keypoint detection accuracy, we further propose a novel loss, Anatomically-Calibrated Regularization (ACR), that can be integrated into keypoint detection models, leveraging biological insights to refine keypoint localization.","Our contributions set a new benchmark in fish phenotype analysis, addressing the challenges of precise morphological quantification and opening new avenues for research in sustainable aquaculture and genetic studies.","Our dataset and code are available at https://github.com/WeizhenLiuBioinform/Fish-Phenotype-Detect."],"url":"http://arxiv.org/abs/2405.12476v1","category":"cs.CV"}
{"created":"2024-05-21 03:09:02","title":"Tunable Giant Anomalous Hall in a Kondo Lattice Ferromagnet UBiTe","abstract":"Kondo lattice systems are recognized for potentially hosting a variety of rich topological phases. Several pioneering studies have demonstrated significant anomalous Hall and anomalous Nernst effects in these systems, attributed to the Berry curvature of the hybridization bands. In this study, we investigate UBiTe, a ferromagnetic Kondo lattice system. Our findings reveal that the intrinsic contribution to the anomalous Hall conductivity is closely tied to the Kondo coherence temperature. Moreover, we demonstrate that slight shifts in the Fermi level across three different samples significantly influence this intrinsic contribution, in alignment with the Berry curvature localized within the narrow hybridization bands. This provides a stark contrast to the less pronounced sensitivity observed in weakly correlated Weyl semimetals, underscoring the distinctive electronic properties of Kondo lattice systems. The anomalous Hall conductivity of one samples ranks among the highest reported for topological magnetic materials.","sentences":["Kondo lattice systems are recognized for potentially hosting a variety of rich topological phases.","Several pioneering studies have demonstrated significant anomalous Hall and anomalous Nernst effects in these systems, attributed to the Berry curvature of the hybridization bands.","In this study, we investigate UBiTe, a ferromagnetic Kondo lattice system.","Our findings reveal that the intrinsic contribution to the anomalous Hall conductivity is closely tied to the Kondo coherence temperature.","Moreover, we demonstrate that slight shifts in the Fermi level across three different samples significantly influence this intrinsic contribution, in alignment with the Berry curvature localized within the narrow hybridization bands.","This provides a stark contrast to the less pronounced sensitivity observed in weakly correlated Weyl semimetals, underscoring the distinctive electronic properties of Kondo lattice systems.","The anomalous Hall conductivity of one samples ranks among the highest reported for topological magnetic materials."],"url":"http://arxiv.org/abs/2405.12471v1","category":"cond-mat.str-el"}
{"created":"2024-05-21 03:06:37","title":"Dynamical Geometry of the Haldane Model under a Quantum Quench","abstract":"We explore the time evolution of a topological system when the system undergoes a sudden quantum quench within the same nontrivial phase. Using Haldane's honeycomb model as an example, we show that equilibrium states in a topological phase can be distinguished by geometrical features, such as the characteristic momentum at which the half-occupied edge modes cross, the associated edge-mode velocity, and the winding vector about which the normalized pseudospin magnetic field winds along a great circle on the Bloch sphere. We generalize these geometrical quantities for non-equilibrium states and use them to visualize the quench dynamics of the topological system. In general, we find the pre-quench equilibrium state relaxes to the post-quench equilibrium state in an oscillatory fashion, whose amplitude decay as $t^{1/2}$. In the process, however, the characteristic winding vector of the non-equilibrium system can evolve to regimes that are not reachable with equilibrium states.","sentences":["We explore the time evolution of a topological system when the system undergoes a sudden quantum quench within the same nontrivial phase.","Using Haldane's honeycomb model as an example, we show that equilibrium states in a topological phase can be distinguished by geometrical features, such as the characteristic momentum at which the half-occupied edge modes cross, the associated edge-mode velocity, and the winding vector about which the normalized pseudospin magnetic field winds along a great circle on the Bloch sphere.","We generalize these geometrical quantities for non-equilibrium states and use them to visualize the quench dynamics of the topological system.","In general, we find the pre-quench equilibrium state relaxes to the post-quench equilibrium state in an oscillatory fashion, whose amplitude decay as $t^{1/2}$. In the process, however, the characteristic winding vector of the non-equilibrium system can evolve to regimes that are not reachable with equilibrium states."],"url":"http://arxiv.org/abs/2405.12470v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 03:05:29","title":"Last-Level Cache Side-Channel Attacks Are Feasible in the Modern Public Cloud (Extended Version)","abstract":"Last-level cache side-channel attacks have been mostly demonstrated in highly-controlled, quiescent local environments. Hence, it is unclear whether such attacks are feasible in a production cloud environment. In the cloud, side channels are flooded with noise from activities of other tenants and, in Function-as-a-Service (FaaS) workloads, the attacker has a very limited time window to mount the attack. In this paper, we show that such attacks are feasible in practice, although they require new techniques. We present an end-to-end, cross-tenant attack on a vulnerable ECDSA implementation in the public FaaS Google Cloud Run environment. We introduce several new techniques to improve every step of the attack. First, to speed-up the generation of eviction sets, we introduce L2-driven candidate address filtering and a Binary Search-based algorithm for address pruning. Second, to monitor victim memory accesses with high time resolution, we introduce Parallel Probing. Finally, we leverage power spectral density from signal processing to easily identify the victim's target cache set in the frequency domain. Overall, using these mechanisms, we extract a median value of 81% of the secret ECDSA nonce bits from a victim container in 19 seconds on average.","sentences":["Last-level cache side-channel attacks have been mostly demonstrated in highly-controlled, quiescent local environments.","Hence, it is unclear whether such attacks are feasible in a production cloud environment.","In the cloud, side channels are flooded with noise from activities of other tenants and, in Function-as-a-Service (FaaS) workloads, the attacker has a very limited time window to mount the attack.","In this paper, we show that such attacks are feasible in practice, although they require new techniques.","We present an end-to-end, cross-tenant attack on a vulnerable ECDSA implementation in the public FaaS Google Cloud Run environment.","We introduce several new techniques to improve every step of the attack.","First, to speed-up the generation of eviction sets, we introduce L2-driven candidate address filtering and a Binary Search-based algorithm for address pruning.","Second, to monitor victim memory accesses with high time resolution, we introduce Parallel Probing.","Finally, we leverage power spectral density from signal processing to easily identify the victim's target cache set in the frequency domain.","Overall, using these mechanisms, we extract a median value of 81% of the secret ECDSA nonce bits from a victim container in 19 seconds on average."],"url":"http://arxiv.org/abs/2405.12469v1","category":"cs.CR"}
{"created":"2024-05-21 02:42:19","title":"Phases and dynamics of few fermionic impurities immersed in two-dimensional boson droplets","abstract":"We unravel the ground state properties and emergent non-equilibrium dynamics of a mixture consisting of a few spin-polarized fermions embedded in a two-dimensional bosonic quantum droplet. For an increasingly attractive droplet-fermion interaction we find a transition from a spatially delocalized fermion configuration to a state where the fermions are highly localized and isolated. This process is accompanied by the rise of induced fermion-fermion interactions mediated by the droplet. Additionally, for increasing attractive droplet-fermion coupling, undulations in the droplet density occur in the vicinity of the fermions manifesting the back-action of the latter. Following interaction quenches from strong attractive to weaker droplet-fermion couplings reveals the spontaneous nucleation of complex excitation patterns in the fermion density such as ring and cross shaped structures. These stem from the enhanced interference of the fermions that remain trapped within the droplet, which emulates, to a good degree, an effective potential for the fermions. The non-negligible back-action of the droplet manifests itself in the fact that the effective potential predictions are less accurate at the level of the many-body wave function. Our results provide a paradigm for physics beyond the reduced single-component droplet model, unveiling the role of back-action in droplets and the effect of induced mediated interactions.","sentences":["We unravel the ground state properties and emergent non-equilibrium dynamics of a mixture consisting of a few spin-polarized fermions embedded in a two-dimensional bosonic quantum droplet.","For an increasingly attractive droplet-fermion interaction we find a transition from a spatially delocalized fermion configuration to a state where the fermions are highly localized and isolated.","This process is accompanied by the rise of induced fermion-fermion interactions mediated by the droplet.","Additionally, for increasing attractive droplet-fermion coupling, undulations in the droplet density occur in the vicinity of the fermions manifesting the back-action of the latter.","Following interaction quenches from strong attractive to weaker droplet-fermion couplings reveals the spontaneous nucleation of complex excitation patterns in the fermion density such as ring and cross shaped structures.","These stem from the enhanced interference of the fermions that remain trapped within the droplet, which emulates, to a good degree, an effective potential for the fermions.","The non-negligible back-action of the droplet manifests itself in the fact that the effective potential predictions are less accurate at the level of the many-body wave function.","Our results provide a paradigm for physics beyond the reduced single-component droplet model, unveiling the role of back-action in droplets and the effect of induced mediated interactions."],"url":"http://arxiv.org/abs/2405.12466v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-21 17:28:06","title":"Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction","abstract":"To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data. However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification. In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity. The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together. Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability. The proposed method is validated through both theoretical results and numerical experiments.","sentences":["To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data.","However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification.","In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity.","The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together.","Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability.","The proposed method is validated through both theoretical results and numerical experiments."],"url":"http://arxiv.org/abs/2405.12953v1","category":"stat.ME"}
{"created":"2024-05-21 16:51:28","title":"Trusting Fair Data: Leveraging Quality in Fairness-Driven Data Removal Techniques","abstract":"In this paper, we deal with bias mitigation techniques that remove specific data points from the training set to aim for a fair representation of the population in that set. Machine learning models are trained on these pre-processed datasets, and their predictions are expected to be fair. However, such approaches may exclude relevant data, making the attained subsets less trustworthy for further usage. To enhance the trustworthiness of prior methods, we propose additional requirements and objectives that the subsets must fulfill in addition to fairness: (1) group coverage, and (2) minimal data loss. While removing entire groups may improve the measured fairness, this practice is very problematic as failing to represent every group cannot be considered fair. In our second concern, we advocate for the retention of data while minimizing discrimination. By introducing a multi-objective optimization problem that considers fairness and data loss, we propose a methodology to find Pareto-optimal solutions that balance these objectives. By identifying such solutions, users can make informed decisions about the trade-off between fairness and data quality and select the most suitable subset for their application.","sentences":["In this paper, we deal with bias mitigation techniques that remove specific data points from the training set to aim for a fair representation of the population in that set.","Machine learning models are trained on these pre-processed datasets, and their predictions are expected to be fair.","However, such approaches may exclude relevant data, making the attained subsets less trustworthy for further usage.","To enhance the trustworthiness of prior methods, we propose additional requirements and objectives that the subsets must fulfill in addition to fairness: (1) group coverage, and (2) minimal data loss.","While removing entire groups may improve the measured fairness, this practice is very problematic as failing to represent every group cannot be considered fair.","In our second concern, we advocate for the retention of data while minimizing discrimination.","By introducing a multi-objective optimization problem that considers fairness and data loss, we propose a methodology to find Pareto-optimal solutions that balance these objectives.","By identifying such solutions, users can make informed decisions about the trade-off between fairness and data quality and select the most suitable subset for their application."],"url":"http://arxiv.org/abs/2405.12926v1","category":"cs.LG"}
{"created":"2024-05-21 16:45:20","title":"Is decidability of the Submonoid Membership Problem closed under finite extensions?","abstract":"We show that the rational subset membership problem in $G$ can be reduced to the submonoid membership problem in $G{\\times}H$ where $H$ is virtually Abelian. We use this to show that there is no algorithm reducing submonoid membership to a finite index subgroup uniformly for all virtually nilpotent groups. We also provide evidence towards the existence of a group $G$ with a subgroup $H<G$ of index 2, such that the submonoid membership problem is decidable in $H$ but not in $G$.","sentences":["We show that the rational subset membership problem in $G$ can be reduced to the submonoid membership problem in $G{\\times}H$ where $H$ is virtually Abelian.","We use this to show that there is no algorithm reducing submonoid membership to a finite index subgroup uniformly for all virtually nilpotent groups.","We also provide evidence towards the existence of a group $G$ with a subgroup $H<G$ of index 2, such that the submonoid membership problem is decidable in $H$ but not in $G$."],"url":"http://arxiv.org/abs/2405.12921v1","category":"math.GR"}
{"created":"2024-05-21 16:42:02","title":"Streamlining Software Reviews: Efficient Predictive Modeling with Minimal Examples","abstract":"This paper proposes a new challenge problem for software analytics. In the process we shall call \"software review\", a panel of SMEs (subject matter experts) review examples of software behavior to recommend how to improve that's software's operation. SME time is usually extremely limited so, ideally, this panel can complete this optimization task after looking at just a small number of very informative, examples.   To support this review process, we explore methods that train a predictive model to guess if some oracle will like/dislike the next example. Such a predictive model can work with the SMEs to guide them in their exploration of all the examples. Also, after the panelists leave, that model can be used as an oracle in place of the panel (to handle new examples, while the panelists are busy, elsewhere).   In 31 case studies (ranging from from high-level decisions about software processes to low-level decisions about how to configure video encoding software), we show that such predictive models can be built using as few as 12 to 30 labels. To the best of our knowledge, this paper's success with only a handful of examples (and no large language model) is unprecedented.   In accordance with the principles of open science, we offer all our code and data at https://github.com/timm/ez/tree/Stable-EMSE-paper so that others can repeat/refute/improve these results.","sentences":["This paper proposes a new challenge problem for software analytics.","In the process we shall call \"software review\", a panel of SMEs (subject matter experts) review examples of software behavior to recommend how to improve that's software's operation.","SME time is usually extremely limited so, ideally, this panel can complete this optimization task after looking at just a small number of very informative, examples.   ","To support this review process, we explore methods that train a predictive model to guess if some oracle will like/dislike the next example.","Such a predictive model can work with the SMEs to guide them in their exploration of all the examples.","Also, after the panelists leave, that model can be used as an oracle in place of the panel (to handle new examples, while the panelists are busy, elsewhere).   ","In 31 case studies (ranging from from high-level decisions about software processes to low-level decisions about how to configure video encoding software), we show that such predictive models can be built using as few as 12 to 30 labels.","To the best of our knowledge, this paper's success with only a handful of examples (and no large language model) is unprecedented.   ","In accordance with the principles of open science, we offer all our code and data at https://github.com/timm/ez/tree/Stable-EMSE-paper so that others can repeat/refute/improve these results."],"url":"http://arxiv.org/abs/2405.12920v1","category":"cs.SE"}
{"created":"2024-05-21 16:39:46","title":"Impact of inhomogeneous diffusion on secondary cosmic ray and antiproton local spectra","abstract":"Recent $\\gamma$-ray and neutrino observations seem to favor the consideration of non-uniform diffusion of cosmic rays (CRs) throughout the Galaxy. In this study, we investigate the consequences of spatially-dependent inhomogeneous propagation of CRs on the fluxes of secondary CRs and antiprotons detected at Earth. A comparison is made among different scenarios in search of potential features that may guide us toward favoring one over another in the near future. We also examine both the influence of inhomogeneous propagation in the production of secondary CRs from interactions with the gas, and the effects of this scenario on the local fluxes of antiprotons and light antinuclei produced as final products of dark matter annihilation. Our results indicate that the consideration of an inhomogeneous diffusion model could improve the compatibility of the predicted local antiproton flux with that of B, Be and Li, assuming only secondary origin of these particles. In addition, our model predicts a slightly harder local antiproton spectrum, making it more compatible with the high energy measurements of AMS-02. Finally, no significant changes are expected in the predicted local flux of antiprotons and antinuclei produced from dark matter among the different considered propagation scenarios.","sentences":["Recent $\\gamma$-ray and neutrino observations seem to favor the consideration of non-uniform diffusion of cosmic rays (CRs) throughout the Galaxy.","In this study, we investigate the consequences of spatially-dependent inhomogeneous propagation of CRs on the fluxes of secondary CRs and antiprotons detected at Earth.","A comparison is made among different scenarios in search of potential features that may guide us toward favoring one over another in the near future.","We also examine both the influence of inhomogeneous propagation in the production of secondary CRs from interactions with the gas, and the effects of this scenario on the local fluxes of antiprotons and light antinuclei produced as final products of dark matter annihilation.","Our results indicate that the consideration of an inhomogeneous diffusion model could improve the compatibility of the predicted local antiproton flux with that of B, Be and Li, assuming only secondary origin of these particles.","In addition, our model predicts a slightly harder local antiproton spectrum, making it more compatible with the high energy measurements of AMS-02.","Finally, no significant changes are expected in the predicted local flux of antiprotons and antinuclei produced from dark matter among the different considered propagation scenarios."],"url":"http://arxiv.org/abs/2405.12918v1","category":"astro-ph.HE"}
{"created":"2024-05-21 16:11:18","title":"Coisotropic reduction in Multisymplectic Geometry","abstract":"In this paper we study coisotropic reduction in multisymplectic geometry. On the one hand, we give an interpretation of Hamiltonian multivector fields as Lagrangian submanifolds and prove that $k$-coisotropic submanifolds induce a Lie subalgebra in the algebra of Hamiltonian $(k-1)$-forms, similar to how coisotropic submanifolds in symplectic geometry induce a Lie subalgebra under the Poisson bracket. On the other hand, we extend the classical result of symplectic geometry of projection of Lagrangian submanifolds in coisotropic reduction to bundles of forms, which naturally carry a multisymplectic structure.","sentences":["In this paper we study coisotropic reduction in multisymplectic geometry.","On the one hand, we give an interpretation of Hamiltonian multivector fields as Lagrangian submanifolds and prove that $k$-coisotropic submanifolds induce a Lie subalgebra in the algebra of Hamiltonian $(k-1)$-forms, similar to how coisotropic submanifolds in symplectic geometry induce a Lie subalgebra under the Poisson bracket.","On the other hand, we extend the classical result of symplectic geometry of projection of Lagrangian submanifolds in coisotropic reduction to bundles of forms, which naturally carry a multisymplectic structure."],"url":"http://arxiv.org/abs/2405.12898v1","category":"math.SG"}
{"created":"2024-05-21 16:04:32","title":"Decentralized Federated Learning Over Imperfect Communication Channels","abstract":"This paper analyzes the impact of imperfect communication channels on decentralized federated learning (D-FL) and subsequently determines the optimal number of local aggregations per training round, adapting to the network topology and imperfect channels. We start by deriving the bias of locally aggregated D-FL models under imperfect channels from the ideal global models requiring perfect channels and aggregations. The bias reveals that excessive local aggregations can accumulate communication errors and degrade convergence. Another important aspect is that we analyze a convergence upper bound of D-FL based on the bias. By minimizing the bound, the optimal number of local aggregations is identified to balance a trade-off with accumulation of communication errors in the absence of knowledge of the channels. With this knowledge, the impact of communication errors can be alleviated, allowing the convergence upper bound to decrease throughout aggregations. Experiments validate our convergence analysis and also identify the optimal number of local aggregations on two widely considered image classification tasks. It is seen that D-FL, with an optimal number of local aggregations, can outperform its potential alternatives by over 10% in training accuracy.","sentences":["This paper analyzes the impact of imperfect communication channels on decentralized federated learning (D-FL) and subsequently determines the optimal number of local aggregations per training round, adapting to the network topology and imperfect channels.","We start by deriving the bias of locally aggregated D-FL models under imperfect channels from the ideal global models requiring perfect channels and aggregations.","The bias reveals that excessive local aggregations can accumulate communication errors and degrade convergence.","Another important aspect is that we analyze a convergence upper bound of D-FL based on the bias.","By minimizing the bound, the optimal number of local aggregations is identified to balance a trade-off with accumulation of communication errors in the absence of knowledge of the channels.","With this knowledge, the impact of communication errors can be alleviated, allowing the convergence upper bound to decrease throughout aggregations.","Experiments validate our convergence analysis and also identify the optimal number of local aggregations on two widely considered image classification tasks.","It is seen that D-FL, with an optimal number of local aggregations, can outperform its potential alternatives by over 10% in training accuracy."],"url":"http://arxiv.org/abs/2405.12894v1","category":"cs.DC"}
{"created":"2024-05-21 16:04:32","title":"Implicit-ARAP: Efficient Handle-Guided Deformation of High-Resolution Meshes and Neural Fields via Local Patch Meshing","abstract":"In this work, we present the local patch mesh representation for neural signed distance fields. This technique allows to discretize local regions of the level sets of an input SDF by projecting and deforming flat patch meshes onto the level set surface, using exclusively the SDF information and its gradient. Our analysis reveals this method to be more accurate than the standard marching cubes algorithm for approximating the implicit surface. Then, we apply this representation in the setting of handle-guided deformation: we introduce two distinct pipelines, which make use of 3D neural fields to compute As-Rigid-As-Possible deformations of both high-resolution meshes and neural fields under a given set of constraints. We run a comprehensive evaluation of our method and various baselines for neural field and mesh deformation which show both pipelines achieve impressive efficiency and notable improvements in terms of quality of results and robustness. With our novel pipeline, we introduce a scalable approach to solve a well-established geometry processing problem on high-resolution meshes, and pave the way for extending other geometric tasks to the domain of implicit surfaces via local patch meshing.","sentences":["In this work, we present the local patch mesh representation for neural signed distance fields.","This technique allows to discretize local regions of the level sets of an input SDF by projecting and deforming flat patch meshes onto the level set surface, using exclusively the SDF information and its gradient.","Our analysis reveals this method to be more accurate than the standard marching cubes algorithm for approximating the implicit surface.","Then, we apply this representation in the setting of handle-guided deformation: we introduce two distinct pipelines, which make use of 3D neural fields to compute As-Rigid-As-Possible deformations of both high-resolution meshes and neural fields under a given set of constraints.","We run a comprehensive evaluation of our method and various baselines for neural field and mesh deformation which show both pipelines achieve impressive efficiency and notable improvements in terms of quality of results and robustness.","With our novel pipeline, we introduce a scalable approach to solve a well-established geometry processing problem on high-resolution meshes, and pave the way for extending other geometric tasks to the domain of implicit surfaces via local patch meshing."],"url":"http://arxiv.org/abs/2405.12895v1","category":"cs.GR"}
{"created":"2024-05-21 16:01:13","title":"DARK: Denoising, Amplification, Restoration Kit","abstract":"This paper introduces a novel lightweight computational framework for enhancing images under low-light conditions, utilizing advanced machine learning and convolutional neural networks (CNNs). Traditional enhancement techniques often fail to adequately address issues like noise, color distortion, and detail loss in challenging lighting environments. Our approach leverages insights from the Retinex theory and recent advances in image restoration networks to develop a streamlined model that efficiently processes illumination components and integrates context-sensitive enhancements through optimized convolutional blocks. This results in significantly improved image clarity and color fidelity, while avoiding over-enhancement and unnatural color shifts. Crucially, our model is designed to be lightweight, ensuring low computational demand and suitability for real-time applications on standard consumer hardware. Performance evaluations confirm that our model not only surpasses existing methods in enhancing low-light images but also maintains a minimal computational footprint.","sentences":["This paper introduces a novel lightweight computational framework for enhancing images under low-light conditions, utilizing advanced machine learning and convolutional neural networks (CNNs).","Traditional enhancement techniques often fail to adequately address issues like noise, color distortion, and detail loss in challenging lighting environments.","Our approach leverages insights from the Retinex theory and recent advances in image restoration networks to develop a streamlined model that efficiently processes illumination components and integrates context-sensitive enhancements through optimized convolutional blocks.","This results in significantly improved image clarity and color fidelity, while avoiding over-enhancement and unnatural color shifts.","Crucially, our model is designed to be lightweight, ensuring low computational demand and suitability for real-time applications on standard consumer hardware.","Performance evaluations confirm that our model not only surpasses existing methods in enhancing low-light images but also maintains a minimal computational footprint."],"url":"http://arxiv.org/abs/2405.12891v1","category":"cs.CV"}
{"created":"2024-05-21 15:54:56","title":"Asymptotic analysis at any order of Helmholtz's problem in a corner with a thin layer: an algebraic approach","abstract":"We consider the Helmholtz equation in an angular sector partially covered by a homogeneous layer of small thickness, denoted $\\varepsilon$. We propose in this work an asymptotic expansion of the solution with respect to $\\varepsilon$ at any order. This is done using matched asymptotic expansion, which consists here in introducing different asymptotic expansions of the solution in three subdomains: the vicinity of the corner, the layer and the rest of the domain. These expansions are linked through matching conditions. The presence of the corner makes these matching conditions delicate to derive because the fields have singular behaviors. Our approach is to reformulate these matching conditions purely algebraically by writing all asymptotic expansions as formal series. By using algebraic calculus we reduce the matching conditions to scalar relations linking the singular behaviors of the fields. These relations have a convolutive structure and involve some coefficients that can be computed analytically. Our asymptotic expansion is justified rigorously with error estimates.","sentences":["We consider the Helmholtz equation in an angular sector partially covered by a homogeneous layer of small thickness, denoted $\\varepsilon$. We propose in this work an asymptotic expansion of the solution with respect to $\\varepsilon$ at any order.","This is done using matched asymptotic expansion, which consists here in introducing different asymptotic expansions of the solution in three subdomains: the vicinity of the corner, the layer and the rest of the domain.","These expansions are linked through matching conditions.","The presence of the corner makes these matching conditions delicate to derive because the fields have singular behaviors.","Our approach is to reformulate these matching conditions purely algebraically by writing all asymptotic expansions as formal series.","By using algebraic calculus we reduce the matching conditions to scalar relations linking the singular behaviors of the fields.","These relations have a convolutive structure and involve some coefficients that can be computed analytically.","Our asymptotic expansion is justified rigorously with error estimates."],"url":"http://arxiv.org/abs/2405.12883v1","category":"math.AP"}
{"created":"2024-05-21 15:22:30","title":"A Quick Guide to Nearby Young Association","abstract":"Nearby associations of stars which are coeval are important benchmark laboratories because they provide robust measurements of stellar ages. The study of such coeval groups makes it possible to better understand star formation by studying the initial mass function, the binary fraction or the circumstellar disks of stars, to determine how the initially dense populations of young stars gradually disperse to form the field population, and to shed light on how the properties of stars, exoplanets and substellar objects evolve with distinct snapshots along their lifetime. The advent of large-scale missions such as Gaia is reshaping our understanding or stellar kinematics in the Solar neighborhood and beyond, and offers the opportunity to detect a large number of loose, coeval stellar associations for the first time, which evaded prior detection because of their low density or the faintness of their members. In parallel, advances in detection and characterization of exoplanets and substellar objects are starting to unveil the detailed properties of extrasolar atmospheres, as well as population-level distributions in fundamental exoplanet properties such as radii, masses, and orbital parameters. Accurate ages are still sparsely available to interpret the evolution of both exoplanets and substellar objects, and both fields are now ripe for detailed age investigations because we are starting to uncover ever-closer low-density associations that previously escaped detection, as well as exoplanets and ever lower-mass members of more distant open clusters and star-forming regions. In this paper, we review some recent advances in the identification and characterization of nearby associations, the methods by which stellar ages are measured, and some of the direct applications of the study of young associations such as the emergent field of isolated planetary-mass objects.","sentences":["Nearby associations of stars which are coeval are important benchmark laboratories because they provide robust measurements of stellar ages.","The study of such coeval groups makes it possible to better understand star formation by studying the initial mass function, the binary fraction or the circumstellar disks of stars, to determine how the initially dense populations of young stars gradually disperse to form the field population, and to shed light on how the properties of stars, exoplanets and substellar objects evolve with distinct snapshots along their lifetime.","The advent of large-scale missions such as Gaia is reshaping our understanding or stellar kinematics in the Solar neighborhood and beyond, and offers the opportunity to detect a large number of loose, coeval stellar associations for the first time, which evaded prior detection because of their low density or the faintness of their members.","In parallel, advances in detection and characterization of exoplanets and substellar objects are starting to unveil the detailed properties of extrasolar atmospheres, as well as population-level distributions in fundamental exoplanet properties such as radii, masses, and orbital parameters.","Accurate ages are still sparsely available to interpret the evolution of both exoplanets and substellar objects, and both fields are now ripe for detailed age investigations because we are starting to uncover ever-closer low-density associations that previously escaped detection, as well as exoplanets and ever lower-mass members of more distant open clusters and star-forming regions.","In this paper, we review some recent advances in the identification and characterization of nearby associations, the methods by which stellar ages are measured, and some of the direct applications of the study of young associations such as the emergent field of isolated planetary-mass objects."],"url":"http://arxiv.org/abs/2405.12860v1","category":"astro-ph.SR"}
{"created":"2024-05-21 15:20:34","title":"Defect-assisted reversible phase transition in mono- and few-layer ReS$_2$","abstract":"Transition metal dichalcogenide (TMD) materials have attracted substantial interest due to their remarkable excitonic, optical, electrical, and mechanical properties, which are highly dependent on their crystal structure. Controlling the crystal structure of these materials is essential for fine-tuning their performance, $\\textit{e.g.}$, linear and nonlinear optical, as well as charge transport properties. While various phase-switching TMD materials, like molybdenum telluride (MoTe$_2$), are available, their transitions are often irreversible. Here, we investigate the mechanism of a light-induced reversible phase transition in mono- and bilayer flakes of rhenium disulfide (ReS$_2$). Our observations, based on scanning transmission electron microscopy, nonlinear spectroscopy, and density functional theory calculations, reveal a transition from the ground T$''$ (double distorted T) to the metastable H$'$ (distorted H) phase under femtosecond laser irradiation or influence of highly-energetic electrons. We show that the formation of sulfur vacancies facilitates this phenomenon. Our findings pave the way towards actively manipulating the crystal structure of ReS$_2$ and possibly its heterostructures.","sentences":["Transition metal dichalcogenide (TMD) materials have attracted substantial interest due to their remarkable excitonic, optical, electrical, and mechanical properties, which are highly dependent on their crystal structure.","Controlling the crystal structure of these materials is essential for fine-tuning their performance, $\\textit{e.g.}$, linear and nonlinear optical, as well as charge transport properties.","While various phase-switching TMD materials, like molybdenum telluride (MoTe$_2$), are available, their transitions are often irreversible.","Here, we investigate the mechanism of a light-induced reversible phase transition in mono- and bilayer flakes of rhenium disulfide (ReS$_2$).","Our observations, based on scanning transmission electron microscopy, nonlinear spectroscopy, and density functional theory calculations, reveal a transition from the ground T$''$ (double distorted T) to the metastable H$'$ (distorted H) phase under femtosecond laser irradiation or influence of highly-energetic electrons.","We show that the formation of sulfur vacancies facilitates this phenomenon.","Our findings pave the way towards actively manipulating the crystal structure of ReS$_2$ and possibly its heterostructures."],"url":"http://arxiv.org/abs/2405.12859v1","category":"physics.optics"}
{"created":"2024-05-21 15:13:12","title":"LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language","abstract":"Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.","sentences":["Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses.","Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists.","Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge.","Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves.","We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs.","We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling.","We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression.","Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions.","This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode."],"url":"http://arxiv.org/abs/2405.12856v1","category":"stat.ML"}
{"created":"2024-05-21 14:51:17","title":"Bounds on Distinguishing Separated Wires Using Magnetic Field Measurements","abstract":"Magnetic current imaging (MCI) is useful for non-destructive characterization of microelectronics, including both security analysis and failure analysis, because magnetic fields penetrate the materials that comprise these components to enable through-package imaging of chip activity. Of particular interest are new capabilities offered by emerging magnetic field imagers, such as the Quantum Diamond Microscope, which provide simultaneous wide field-of-view, high spatial resolution vector magnetic field imaging capabilities under ambient conditions. While MCI offers several advantages for non-destructive measurement of microelectronics functional activity, there are many limitations of the technique due to rapid falloff of magnetic fields and loss of high frequency spatial information at large sensor standoff distances. To understand spatial resolution as a function of standoff distance, we consider the problem of using magnetic fields to distinguish (1) between a single wire carrying current $I$ and a pair of wires carrying current $I/2$ in the same direction and (2) between no currents and a pair of wires carrying current $I/2$ in opposite directions. In both cases, we compare performance for a single point measurement, representative of typical magnetometers, to performance for an array of measurements found in emerging magnetic imaging devices. Additionally, we examine the advantage provided by measurement of the full vector magnetic field compared to measurement of a single component. We establish and compare for the first time the theoretical lower bounds on separability based on the wire separation and sensor standoff distance of the magnetic field measurements obtained from traditional and new microelectronics reliability tools.","sentences":["Magnetic current imaging (MCI) is useful for non-destructive characterization of microelectronics, including both security analysis and failure analysis, because magnetic fields penetrate the materials that comprise these components to enable through-package imaging of chip activity.","Of particular interest are new capabilities offered by emerging magnetic field imagers, such as the Quantum Diamond Microscope, which provide simultaneous wide field-of-view, high spatial resolution vector magnetic field imaging capabilities under ambient conditions.","While MCI offers several advantages for non-destructive measurement of microelectronics functional activity, there are many limitations of the technique due to rapid falloff of magnetic fields and loss of high frequency spatial information at large sensor standoff distances.","To understand spatial resolution as a function of standoff distance, we consider the problem of using magnetic fields to distinguish (1) between a single wire carrying current $I$ and a pair of wires carrying current $I/2$ in the same direction and (2) between no currents and a pair of wires carrying current $I/2$ in opposite directions.","In both cases, we compare performance for a single point measurement, representative of typical magnetometers, to performance for an array of measurements found in emerging magnetic imaging devices.","Additionally, we examine the advantage provided by measurement of the full vector magnetic field compared to measurement of a single component.","We establish and compare for the first time the theoretical lower bounds on separability based on the wire separation and sensor standoff distance of the magnetic field measurements obtained from traditional and new microelectronics reliability tools."],"url":"http://arxiv.org/abs/2405.12844v1","category":"physics.ins-det"}
{"created":"2024-05-21 13:53:58","title":"Stochastic Inference of Plate Bending from Heterogeneous Data: Physics-informed Gaussian Processes via Kirchhoff-Love Theory","abstract":"Advancements in machine learning and an abundance of structural monitoring data have inspired the integration of mechanical models with probabilistic models to identify a structure's state and quantify the uncertainty of its physical parameters and response. In this paper, we propose an inference methodology for classical Kirchhoff-Love plates via physics-informed Gaussian Processes (GP). A probabilistic model is formulated as a multi-output GP by placing a GP prior on the deflection and deriving the covariance function using the linear differential operators of the plate governing equations. The posteriors of the flexural rigidity, hyperparameters, and plate response are inferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) sampling from noisy measurements. We demonstrate the applicability with two examples: a simply supported plate subjected to a sinusoidal load and a fixed plate subjected to a uniform load. The results illustrate how the proposed methodology can be employed to perform stochastic inference for plate rigidity and physical quantities by integrating measurements from various sensor types and qualities. Potential applications of the presented methodology are in structural health monitoring and uncertainty quantification of plate-like structures.","sentences":["Advancements in machine learning and an abundance of structural monitoring data have inspired the integration of mechanical models with probabilistic models to identify a structure's state and quantify the uncertainty of its physical parameters and response.","In this paper, we propose an inference methodology for classical Kirchhoff-Love plates via physics-informed Gaussian Processes (GP).","A probabilistic model is formulated as a multi-output GP by placing a GP prior on the deflection and deriving the covariance function using the linear differential operators of the plate governing equations.","The posteriors of the flexural rigidity, hyperparameters, and plate response are inferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) sampling from noisy measurements.","We demonstrate the applicability with two examples: a simply supported plate subjected to a sinusoidal load and a fixed plate subjected to a uniform load.","The results illustrate how the proposed methodology can be employed to perform stochastic inference for plate rigidity and physical quantities by integrating measurements from various sensor types and qualities.","Potential applications of the presented methodology are in structural health monitoring and uncertainty quantification of plate-like structures."],"url":"http://arxiv.org/abs/2405.12802v1","category":"cs.LG"}
{"created":"2024-05-21 13:51:48","title":"Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval","abstract":"A common retrieve-and-rerank paradigm involves retrieving a broad set of relevant candidates using a scalable bi-encoder, followed by expensive but more accurate cross-encoders to a limited candidate set. However, this small subset often leads to error propagation from the bi-encoders, thereby restricting the performance of the overall pipeline. To address these issues, we propose the Comparing Multiple Candidates (CMC) framework, which compares a query and multiple candidate embeddings jointly through shallow self-attention layers. While providing contextualized representations, CMC is scalable enough to handle multiple comparisons simultaneously, where comparing 2K candidates takes only twice as long as comparing 100. Practitioners can use CMC as a lightweight and effective reranker to improve top-1 accuracy. Moreover, when integrated with another retriever, CMC reranking can function as a virtually enhanced retriever. This configuration adds only negligible latency compared to using a single retriever (virtual), while significantly improving recall at K (enhanced).} Through experiments, we demonstrate that CMC, as a virtually enhanced retriever, significantly improves Recall@k (+6.7, +3.5%-p for R@16, R@64) compared to the initial retrieval stage on the ZeSHEL dataset. Meanwhile, we conduct experiments for direct reranking on entity, passage, and dialogue ranking. The results indicate that CMC is not only faster (11x) than cross-encoders but also often more effective, with improved prediction performance in Wikipedia entity linking (+0.7%-p) and DSTC7 dialogue ranking (+3.3%-p). The code and link to datasets are available at https://github.com/yc-song/cmc","sentences":["A common retrieve-and-rerank paradigm involves retrieving a broad set of relevant candidates using a scalable bi-encoder, followed by expensive but more accurate cross-encoders to a limited candidate set.","However, this small subset often leads to error propagation from the bi-encoders, thereby restricting the performance of the overall pipeline.","To address these issues, we propose the Comparing Multiple Candidates (CMC) framework, which compares a query and multiple candidate embeddings jointly through shallow self-attention layers.","While providing contextualized representations, CMC is scalable enough to handle multiple comparisons simultaneously, where comparing 2K candidates takes only twice as long as comparing 100.","Practitioners can use CMC as a lightweight and effective reranker to improve top-1 accuracy.","Moreover, when integrated with another retriever, CMC reranking can function as a virtually enhanced retriever.","This configuration adds only negligible latency compared to using a single retriever (virtual), while significantly improving recall at K (enhanced).}","Through experiments, we demonstrate that CMC, as a virtually enhanced retriever, significantly improves Recall@k (+6.7, +3.5%-p for R@16, R@64) compared to the initial retrieval stage on the ZeSHEL dataset.","Meanwhile, we conduct experiments for direct reranking on entity, passage, and dialogue ranking.","The results indicate that CMC is not only faster (11x) than cross-encoders but also often more effective, with improved prediction performance in Wikipedia entity linking (+0.7%-p) and DSTC7 dialogue ranking (+3.3%-p).","The code and link to datasets are available at https://github.com/yc-song/cmc"],"url":"http://arxiv.org/abs/2405.12801v1","category":"cs.CL"}
{"created":"2024-05-21 13:50:23","title":"Dark-Field X-Ray Microscopy with Structured Illumination for Three-Dimensional Imaging","abstract":"We introduce a structured illumination technique for dark-field x-ray microscopy optimized for three-dimensional imaging of ordered materials at sub-micrometer length scales. Our method utilizes a coded aperture to spatially modulate the incident x-ray beam on the sample, enabling the reconstruction of the sample's 3D structure from images captured at various aperture positions. Unlike common volumetric imaging techniques such as tomography, our approach casts a scanning x-ray silhouette of a coded aperture for depth resolution along the axis of diffraction, eliminating any need for sample rotation or rastering, leading to a highly stable imaging modality. This modification provides robustness against geometric uncertainties during data acquisition, particularly for achieving sub-micrometer resolutions where geometric uncertainties typically limit resolution. We introduce the image reconstruction model and validate our results with experimental data on an isolated twin domain within a bulk single crystal of an iron pnictide obtained using a dark-field x-ray microscope. This timely advancement aligns with the enhanced brightness upgrade of the world's synchrotron radiation facilities, opening unprecedented opportunities in imaging.","sentences":["We introduce a structured illumination technique for dark-field x-ray microscopy optimized for three-dimensional imaging of ordered materials at sub-micrometer length scales.","Our method utilizes a coded aperture to spatially modulate the incident x-ray beam on the sample, enabling the reconstruction of the sample's 3D structure from images captured at various aperture positions.","Unlike common volumetric imaging techniques such as tomography, our approach casts a scanning x-ray silhouette of a coded aperture for depth resolution along the axis of diffraction, eliminating any need for sample rotation or rastering, leading to a highly stable imaging modality.","This modification provides robustness against geometric uncertainties during data acquisition, particularly for achieving sub-micrometer resolutions where geometric uncertainties typically limit resolution.","We introduce the image reconstruction model and validate our results with experimental data on an isolated twin domain within a bulk single crystal of an iron pnictide obtained using a dark-field x-ray microscope.","This timely advancement aligns with the enhanced brightness upgrade of the world's synchrotron radiation facilities, opening unprecedented opportunities in imaging."],"url":"http://arxiv.org/abs/2405.12799v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 12:25:49","title":"Nearest is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks","abstract":"Model quantization is widely used to compress and accelerate deep neural networks. However, recent studies have revealed the feasibility of weaponizing model quantization via implanting quantization-conditioned backdoors (QCBs). These special backdoors stay dormant on released full-precision models but will come into effect after standard quantization. Due to the peculiarity of QCBs, existing defenses have minor effects on reducing their threats or are even infeasible. In this paper, we conduct the first in-depth analysis of QCBs. We reveal that the activation of existing QCBs primarily stems from the nearest rounding operation and is closely related to the norms of neuron-wise truncation errors (i.e., the difference between the continuous full-precision weights and its quantized version). Motivated by these insights, we propose Error-guided Flipped Rounding with Activation Preservation (EFRAP), an effective and practical defense against QCBs. Specifically, EFRAP learns a non-nearest rounding strategy with neuron-wise error norm and layer-wise activation preservation guidance, flipping the rounding strategies of neurons crucial for backdoor effects but with minimal impact on clean accuracy. Extensive evaluations on benchmark datasets demonstrate that our EFRAP can defeat state-of-the-art QCB attacks under various settings. Code is available at https://github.com/AntigoneRandy/QuantBackdoor_EFRAP.","sentences":["Model quantization is widely used to compress and accelerate deep neural networks.","However, recent studies have revealed the feasibility of weaponizing model quantization via implanting quantization-conditioned backdoors (QCBs).","These special backdoors stay dormant on released full-precision models but will come into effect after standard quantization.","Due to the peculiarity of QCBs, existing defenses have minor effects on reducing their threats or are even infeasible.","In this paper, we conduct the first in-depth analysis of QCBs.","We reveal that the activation of existing QCBs primarily stems from the nearest rounding operation and is closely related to the norms of neuron-wise truncation errors (i.e., the difference between the continuous full-precision weights and its quantized version).","Motivated by these insights, we propose Error-guided Flipped Rounding with Activation Preservation (EFRAP), an effective and practical defense against QCBs.","Specifically, EFRAP learns a non-nearest rounding strategy with neuron-wise error norm and layer-wise activation preservation guidance, flipping the rounding strategies of neurons crucial for backdoor effects but with minimal impact on clean accuracy.","Extensive evaluations on benchmark datasets demonstrate that our EFRAP can defeat state-of-the-art QCB attacks under various settings.","Code is available at https://github.com/AntigoneRandy/QuantBackdoor_EFRAP."],"url":"http://arxiv.org/abs/2405.12725v1","category":"cs.CR"}
{"created":"2024-05-21 12:21:26","title":"Preservation under Reduced Products in Continuous Logic","abstract":"We introduce a fragment of continuous first-order logic, analogue of Palyutin formulas (or h-formulas) in classical model theory, which is preserved under reduced products in both directions. We use it to extend classical results on complete theories which are preserved under reduced product and their stability. We also characterize the set of Palyutin sentences, Palyutin theories and other related fragments in terms of their preservation properties, both in the classical setting and the metric one.","sentences":["We introduce a fragment of continuous first-order logic, analogue of Palyutin formulas (or h-formulas) in classical model theory, which is preserved under reduced products in both directions.","We use it to extend classical results on complete theories which are preserved under reduced product and their stability.","We also characterize the set of Palyutin sentences, Palyutin theories and other related fragments in terms of their preservation properties, both in the classical setting and the metric one."],"url":"http://arxiv.org/abs/2405.12720v1","category":"math.LO"}
{"created":"2024-05-21 12:20:19","title":"How to Train a Backdoor-Robust Model on a Poisoned Dataset without Auxiliary Data?","abstract":"Backdoor attacks have attracted wide attention from academia and industry due to their great security threat to deep neural networks (DNN). Most of the existing methods propose to conduct backdoor attacks by poisoning the training dataset with different strategies, so it's critical to identify the poisoned samples and then train a clean model on the unreliable dataset in the context of defending backdoor attacks. Although numerous backdoor countermeasure researches are proposed, their inherent weaknesses render them limited in practical scenarios, such as the requirement of enough clean samples, unstable defense performance under various attack conditions, poor defense performance against adaptive attacks, and so on.Therefore, in this paper, we are committed to overcome the above limitations and propose a more practical backdoor defense method. Concretely, we first explore the inherent relationship between the potential perturbations and the backdoor trigger, and the theoretical analysis and experimental results demonstrate that the poisoned samples perform more robustness to perturbation than the clean ones. Then, based on our key explorations, we introduce AdvrBD, an Adversarial perturbation-based and robust Backdoor Defense framework, which can effectively identify the poisoned samples and train a clean model on the poisoned dataset. Constructively, our AdvrBD eliminates the requirement for any clean samples or knowledge about the poisoned dataset (e.g., poisoning ratio), which significantly improves the practicality in real-world scenarios.","sentences":["Backdoor attacks have attracted wide attention from academia and industry due to their great security threat to deep neural networks (DNN).","Most of the existing methods propose to conduct backdoor attacks by poisoning the training dataset with different strategies, so it's critical to identify the poisoned samples and then train a clean model on the unreliable dataset in the context of defending backdoor attacks.","Although numerous backdoor countermeasure researches are proposed, their inherent weaknesses render them limited in practical scenarios, such as the requirement of enough clean samples, unstable defense performance under various attack conditions, poor defense performance against adaptive attacks, and so on.","Therefore, in this paper, we are committed to overcome the above limitations and propose a more practical backdoor defense method.","Concretely, we first explore the inherent relationship between the potential perturbations and the backdoor trigger, and the theoretical analysis and experimental results demonstrate that the poisoned samples perform more robustness to perturbation than the clean ones.","Then, based on our key explorations, we introduce AdvrBD, an Adversarial perturbation-based and robust Backdoor Defense framework, which can effectively identify the poisoned samples and train a clean model on the poisoned dataset.","Constructively, our AdvrBD eliminates the requirement for any clean samples or knowledge about the poisoned dataset (e.g., poisoning ratio), which significantly improves the practicality in real-world scenarios."],"url":"http://arxiv.org/abs/2405.12719v1","category":"cs.CR"}
{"created":"2024-05-21 11:58:11","title":"Object-Centric Event Logs: Specifications, Comparative Analysis and Refinement","abstract":"Process mining aims to comprehend and enhance business processes by analyzing event logs. Recently, object-centric process mining has gained traction by considering multiple objects interacting with each other in a process. This object-centric approach offers advantages over traditional methods by avoiding dimension reduction issues. However, in contrast to traditional process mining where a standard event log format was quickly agreed upon with XES providing a common platform for further research and industry, various object-centric logging formats have been proposed, each addressing specific challenges such as object relations or dynamic attribute changes. This makes that interoperability of object-centric algorithms remains a challenge, hindering reproducibility and generalizability in research. Additionally, the object-centric process storage paradigm aligns well with a wide range of object-oriented databases storing process data.   This paper introduces a specifications framework from three perspectives originating from process mining (what should be analyzed), object-centric process modeling (how it should be modeled), and database storage (how it should be stored) perspectives in order to compare and evaluate object-centric log formats. By identifying commonalities and discrepancies among these formats, the study delves into unresolved issues and proposes potential solutions. Ultimately, this research contributes to advancing object-centric process mining by facilitating a deeper understanding of event log formats and promoting consistency and compatibility across methodologies.","sentences":["Process mining aims to comprehend and enhance business processes by analyzing event logs.","Recently, object-centric process mining has gained traction by considering multiple objects interacting with each other in a process.","This object-centric approach offers advantages over traditional methods by avoiding dimension reduction issues.","However, in contrast to traditional process mining where a standard event log format was quickly agreed upon with XES providing a common platform for further research and industry, various object-centric logging formats have been proposed, each addressing specific challenges such as object relations or dynamic attribute changes.","This makes that interoperability of object-centric algorithms remains a challenge, hindering reproducibility and generalizability in research.","Additionally, the object-centric process storage paradigm aligns well with a wide range of object-oriented databases storing process data.   ","This paper introduces a specifications framework from three perspectives originating from process mining (what should be analyzed), object-centric process modeling (how it should be modeled), and database storage (how it should be stored) perspectives in order to compare and evaluate object-centric log formats.","By identifying commonalities and discrepancies among these formats, the study delves into unresolved issues and proposes potential solutions.","Ultimately, this research contributes to advancing object-centric process mining by facilitating a deeper understanding of event log formats and promoting consistency and compatibility across methodologies."],"url":"http://arxiv.org/abs/2405.12709v1","category":"cs.DB"}
{"created":"2024-05-21 11:56:01","title":"Multimodal video analysis for crowd anomaly detection using open access tourism cameras","abstract":"In this article, we propose the detection of crowd anomalies through the extraction of information in the form of time series from video format using a multimodal approach. Through pattern recognition algorithms and segmentation, informative measures of the number of people and image occupancy are extracted at regular intervals, which are then analyzed to obtain trends and anomalous behaviors. Specifically, through temporal decomposition and residual analysis, intervals or specific situations of unusual behaviors are identified, which can be used in decision-making and improvement of actions in sectors related to human movement such as tourism or security.   The application of this methodology on the webcam of Turisme Comunitat Valenciana in the town of Morella (Comunitat Valenciana, Spain) has provided excellent results. It is shown to correctly detect specific anomalous situations and unusual overall increases during the previous weekend and during the festivities in October 2023. These results have been obtained while preserving the confidentiality of individuals at all times by using measures that maximize anonymity, without trajectory recording or person recognition.","sentences":["In this article, we propose the detection of crowd anomalies through the extraction of information in the form of time series from video format using a multimodal approach.","Through pattern recognition algorithms and segmentation, informative measures of the number of people and image occupancy are extracted at regular intervals, which are then analyzed to obtain trends and anomalous behaviors.","Specifically, through temporal decomposition and residual analysis, intervals or specific situations of unusual behaviors are identified, which can be used in decision-making and improvement of actions in sectors related to human movement such as tourism or security.   ","The application of this methodology on the webcam of Turisme Comunitat Valenciana in the town of Morella (Comunitat Valenciana, Spain) has provided excellent results.","It is shown to correctly detect specific anomalous situations and unusual overall increases during the previous weekend and during the festivities in October 2023.","These results have been obtained while preserving the confidentiality of individuals at all times by using measures that maximize anonymity, without trajectory recording or person recognition."],"url":"http://arxiv.org/abs/2405.12708v1","category":"cs.CV"}
{"created":"2024-05-21 11:49:07","title":"Getting Wiser from Multiple Data: Probabilistic Updating according to Jeffrey and Pearl","abstract":"In probabilistic updating one transforms a prior distribution in the light of given evidence into a posterior distribution, via what is called conditioning, updating, belief revision or inference. This is the essence of learning, as Bayesian updating. It will be illustrated via a physical model involving (adapted) water flows through pipes with different diameters.   Bayesian updating makes us wiser, in the sense that the posterior distribution makes the evidence more likely than the prior, since it incorporates the evidence. Things are less clear when one wishes to learn from multiple pieces of evidence / data. It turns out that there are (at least) two forms of updating for this, associated with Jeffrey and Pearl. The difference is not always clearly recognised.   This paper provides an introduction and an overview in the setting of discrete probability theory. It starts from an elementary question, involving multiple pieces of evidence, that has been sent to a small group academic specialists. Their answers show considerable differences. This is used as motivation and starting point to introduce the two forms of updating, of Jeffrey and Pearl, for multiple inputs and to elaborate their properties. In the end the account is related to so-called variational free energy (VFE) update in the cognitive theory of predictive processing. It is shown that both Jeffrey and Pearl outperform VFE updating and that VFE updating need not decrease divergence - that is correct errors - as it is supposed to do.","sentences":["In probabilistic updating one transforms a prior distribution in the light of given evidence into a posterior distribution, via what is called conditioning, updating, belief revision or inference.","This is the essence of learning, as Bayesian updating.","It will be illustrated via a physical model involving (adapted) water flows through pipes with different diameters.   ","Bayesian updating makes us wiser, in the sense that the posterior distribution makes the evidence more likely than the prior, since it incorporates the evidence.","Things are less clear when one wishes to learn from multiple pieces of evidence / data.","It turns out that there are (at least) two forms of updating for this, associated with Jeffrey and Pearl.","The difference is not always clearly recognised.   ","This paper provides an introduction and an overview in the setting of discrete probability theory.","It starts from an elementary question, involving multiple pieces of evidence, that has been sent to a small group academic specialists.","Their answers show considerable differences.","This is used as motivation and starting point to introduce the two forms of updating, of Jeffrey and Pearl, for multiple inputs and to elaborate their properties.","In the end the account is related to so-called variational free energy (VFE) update in the cognitive theory of predictive processing.","It is shown that both Jeffrey and Pearl outperform VFE updating and that VFE updating need not decrease divergence - that is correct errors - as it is supposed to do."],"url":"http://arxiv.org/abs/2405.12700v1","category":"cs.LO"}
{"created":"2024-05-21 11:46:51","title":"GeckoGraph: A Visual Language for Polymorphic Types","abstract":"Polymorphic types are an important feature in most strongly typed programming languages. They allow functions to be written in a way that can be used with different data types, while still enforcing the relationship and constraints between the values. However, programmers often find polymorphic types difficult to use and understand and tend to reason using concrete types. We propose GeckoGraph, a graphical notation for types. GeckoGraph aims to accompany traditional text-based type notation and to make reading, understanding, and comparing types easier. We conducted a large-scale human study using GeckoGraph compared to text-based type notation. To our knowledge, this is the largest controlled user study on functional programming ever conducted. The results of the study show that GeckoGraph helps improve programmers' ability to succeed in the programming tasks we designed, especially for novice programmers.","sentences":["Polymorphic types are an important feature in most strongly typed programming languages.","They allow functions to be written in a way that can be used with different data types, while still enforcing the relationship and constraints between the values.","However, programmers often find polymorphic types difficult to use and understand and tend to reason using concrete types.","We propose GeckoGraph, a graphical notation for types.","GeckoGraph aims to accompany traditional text-based type notation and to make reading, understanding, and comparing types easier.","We conducted a large-scale human study using GeckoGraph compared to text-based type notation.","To our knowledge, this is the largest controlled user study on functional programming ever conducted.","The results of the study show that GeckoGraph helps improve programmers' ability to succeed in the programming tasks we designed, especially for novice programmers."],"url":"http://arxiv.org/abs/2405.12699v1","category":"cs.PL"}
{"created":"2024-05-21 11:35:14","title":"Parameter estimation in Comparative Judgement","abstract":"Comparative Judgement is an assessment method where item ratings are estimated based on rankings of subsets of the items. These rankings are typically pairwise, with ratings taken to be the estimated parameters from fitting a Bradley-Terry model. Likelihood penalization is often employed. Adaptive scheduling of the comparisons can increase the efficiency of the assessment. We show that the most commonly used penalty is not the best-performing penalty under adaptive scheduling and can lead to substantial bias in parameter estimates. We demonstrate this using simulated and real data and provide a theoretical explanation for the relative performance of the penalties considered. Further, we propose a superior approach based on bootstrapping. It is shown to produce better parameter estimates for adaptive schedules and to be robust to variations in underlying strength distributions and initial penalization method.","sentences":["Comparative Judgement is an assessment method where item ratings are estimated based on rankings of subsets of the items.","These rankings are typically pairwise, with ratings taken to be the estimated parameters from fitting a Bradley-Terry model.","Likelihood penalization is often employed.","Adaptive scheduling of the comparisons can increase the efficiency of the assessment.","We show that the most commonly used penalty is not the best-performing penalty under adaptive scheduling and can lead to substantial bias in parameter estimates.","We demonstrate this using simulated and real data and provide a theoretical explanation for the relative performance of the penalties considered.","Further, we propose a superior approach based on bootstrapping.","It is shown to produce better parameter estimates for adaptive schedules and to be robust to variations in underlying strength distributions and initial penalization method."],"url":"http://arxiv.org/abs/2405.12694v1","category":"stat.ME"}
{"created":"2024-05-21 11:11:22","title":"Observation of Spin Splitting in Room-Temperature Metallic Antiferromagnet CrSb","abstract":"Recently, unconventional antiferromagnets that enable the splitting of electronic spins have been theoretically proposed and experimentally realized, where the magnetic sublattices containing moments pointing at different directions are connected by a novel set of symmetries. Such spin splitting (SS) is substantial, $k$-dependent, and independent of the spin-orbit coupling strength, making these magnets promising materials for antiferromagnetic spintronics. Here, combined with angle-resolved photoemission spectroscopy (ARPES) and density functional theory (DFT) calculations, we perform a systematic study on CrSb, a metallic spin-split antiferromagnet candidate with $T_N$ = 703 K. Our data reveals the electronic structure of CrSb along both out-of-plane and in-plane momentum directions, which renders anisotropic $k$-dependent SS and agrees well with the calculational results. The magnitude of such SS reaches up to at least 0.8 eV at non-high-symmetry momentum points, which is significantly higher than the largest known SOC-induced SS. This compound expands the choice of materials in the field of antiferromagnetic spintronics and is likely to stimulate subsequent investigations of high-efficiency spintronic devices that are functional at room temperature.","sentences":["Recently, unconventional antiferromagnets that enable the splitting of electronic spins have been theoretically proposed and experimentally realized, where the magnetic sublattices containing moments pointing at different directions are connected by a novel set of symmetries.","Such spin splitting (SS) is substantial, $k$-dependent, and independent of the spin-orbit coupling strength, making these magnets promising materials for antiferromagnetic spintronics.","Here, combined with angle-resolved photoemission spectroscopy (ARPES) and density functional theory (DFT) calculations, we perform a systematic study on CrSb, a metallic spin-split antiferromagnet candidate with $T_N$ = 703 K. Our data reveals the electronic structure of CrSb along both out-of-plane and in-plane momentum directions, which renders anisotropic $k$-dependent SS and agrees well with the calculational results.","The magnitude of such SS reaches up to at least 0.8 eV at non-high-symmetry momentum points, which is significantly higher than the largest known SOC-induced SS.","This compound expands the choice of materials in the field of antiferromagnetic spintronics and is likely to stimulate subsequent investigations of high-efficiency spintronic devices that are functional at room temperature."],"url":"http://arxiv.org/abs/2405.12679v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 11:10:41","title":"Dimuons from neutrino-nucleus collisions in the semi-inclusive DIS approach","abstract":"We present a next-to-leading order perturbative QCD calculation of dimuon production in neutrino-nucleus collisions. This process is typically calculated by assuming it to be proportional to inclusive charm production, which requires an effective acceptance correction to take the experimental cuts on the decay-muon kinematics into account. Here, we instead compute the dimuon production cross section directly as a convolution of semi-inclusive deep inelastic scattering to produce charmed hadrons, and a decay function fitted to $e^+e^-$ data to produce a muon from the charmed hadrons. The presented approach is in a good agreement with available experimental data and will serve as a starting point for higher-order QCD calculations without an external acceptance correction. The uncertainties arising from the decay function and scale dependence are sizeably smaller than those from the nuclear parton distribution functions. We also calculate the effective acceptances within our approach and compare them to those usually used in global fits of parton distribution functions, finding differences of the order of $10\\,\\%$, depending on the kinematics, perturbative order, and applied parton distributions.","sentences":["We present a next-to-leading order perturbative QCD calculation of dimuon production in neutrino-nucleus collisions.","This process is typically calculated by assuming it to be proportional to inclusive charm production, which requires an effective acceptance correction to take the experimental cuts on the decay-muon kinematics into account.","Here, we instead compute the dimuon production cross section directly as a convolution of semi-inclusive deep inelastic scattering to produce charmed hadrons, and a decay function fitted to $e^+e^-$ data to produce a muon from the charmed hadrons.","The presented approach is in a good agreement with available experimental data and will serve as a starting point for higher-order QCD calculations without an external acceptance correction.","The uncertainties arising from the decay function and scale dependence are sizeably smaller than those from the nuclear parton distribution functions.","We also calculate the effective acceptances within our approach and compare them to those usually used in global fits of parton distribution functions, finding differences of the order of $10\\,\\%$, depending on the kinematics, perturbative order, and applied parton distributions."],"url":"http://arxiv.org/abs/2405.12677v1","category":"hep-ph"}
{"created":"2024-05-21 10:24:59","title":"IREE Oriented Green 6G Networks: A Radial Basis Function Based Approach","abstract":"In order to provide design guidelines for energy efficient 6G networks, we propose a novel radial basis function (RBF) based optimization framework to maximize the integrated relative energy efficiency (IREE) metric. Different from the conventional energy efficient optimization schemes, we maximize the transformed utility for any given IREE using spectrum efficiency oriented RBF network and gradually update the IREE metric using proposed Dinkelbach's algorithm. The existence and uniqueness properties of RBF networks are provided, and the convergence conditions of the entire framework are discussed as well. Through some numerical experiments, we show that the proposed IREE outperforms many existing SE or EE oriented designs and find a new Jensen-Shannon (JS) divergence constrained region, which behaves differently from the conventional EE-SE region. Meanwhile, by studying IREE-SE trade-offs under different traffic requirements, we suggest that network operators shall spend more efforts to balance the distributions of traffic demands and network capacities in order to improve the IREE performance, especially when the spatial variations of the traffic distribution are significant.","sentences":["In order to provide design guidelines for energy efficient 6G networks, we propose a novel radial basis function (RBF) based optimization framework to maximize the integrated relative energy efficiency (IREE) metric.","Different from the conventional energy efficient optimization schemes, we maximize the transformed utility for any given IREE using spectrum efficiency oriented RBF network and gradually update the IREE metric using proposed Dinkelbach's algorithm.","The existence and uniqueness properties of RBF networks are provided, and the convergence conditions of the entire framework are discussed as well.","Through some numerical experiments, we show that the proposed IREE outperforms many existing SE or EE oriented designs and find a new Jensen-Shannon (JS) divergence constrained region, which behaves differently from the conventional EE-SE region.","Meanwhile, by studying IREE-SE trade-offs under different traffic requirements, we suggest that network operators shall spend more efforts to balance the distributions of traffic demands and network capacities in order to improve the IREE performance, especially when the spatial variations of the traffic distribution are significant."],"url":"http://arxiv.org/abs/2405.12664v1","category":"cs.NI"}
{"created":"2024-05-21 10:15:30","title":"Comprehensive mm-Wave FMCW Radar Dataset for Vital Sign Monitoring: Embracing Extreme Physiological Scenarios","abstract":"Recent advancements in non-invasive health monitoring technologies underscore the potential of mm-Wave Frequency-Modulated Continuous Wave (FMCW) radar in real-time vital sign detection. This paper introduces a novel dataset, the first of its kind, derived from mm-Wave FMCW radar, meticulously capturing heart rate and respiratory rate under various conditions. Comprising data from ten participants, including scenarios with elevated heart rates and participants with diverse physiological profiles such as asthma and meditation practitioners, this dataset is validated against the Polar H10 sensor, ensuring its reliability for scientific research. This dataset can offer a significant resource for developing and testing algorithms aimed at non-invasive health monitoring, promising to facilitate advancements in remote health monitoring technologies.","sentences":["Recent advancements in non-invasive health monitoring technologies underscore the potential of mm-Wave Frequency-Modulated Continuous Wave (FMCW) radar in real-time vital sign detection.","This paper introduces a novel dataset, the first of its kind, derived from mm-Wave FMCW radar, meticulously capturing heart rate and respiratory rate under various conditions.","Comprising data from ten participants, including scenarios with elevated heart rates and participants with diverse physiological profiles such as asthma and meditation practitioners, this dataset is validated against the Polar H10 sensor, ensuring its reliability for scientific research.","This dataset can offer a significant resource for developing and testing algorithms aimed at non-invasive health monitoring, promising to facilitate advancements in remote health monitoring technologies."],"url":"http://arxiv.org/abs/2405.12659v1","category":"eess.SP"}
{"created":"2024-05-21 09:51:15","title":"Combining Twitter and Mobile Phone Data to Observe Border-Rush: The Turkish-European Border Opening","abstract":"Following Turkey's 2020 decision to revoke border controls, many individuals journeyed towards the Greek, Bulgarian, and Turkish borders. However, the lack of verifiable statistics on irregular migration and discrepancies between media reports and actual migration patterns require further exploration. The objective of this study is to bridge this knowledge gap by harnessing novel data sources, specifically mobile phone and Twitter data, to construct estimators of cross-border mobility and to cultivate a qualitative comprehension of the unfolding events. By employing a migration diplomacy framework, we analyse emergent mobility patterns at the border. Our findings demonstrate the potential of mobile phone data for quantitative metrics and Twitter data for qualitative understanding. We underscore the ethical implications of leveraging Big Data, particularly considering the vulnerability of the population under study. This underscores the imperative for exhaustive research into the socio-political facets of human mobility, with the aim of discerning the potentialities, limitations, and risks inherent in these data sources and their integration. This scholarly endeavour contributes to a more nuanced understanding of migration dynamics and paves the way for the formulation of regulations that preclude misuse and oppressive surveillance, thereby ensuring a more accurate representation of migration realities.","sentences":["Following Turkey's 2020 decision to revoke border controls, many individuals journeyed towards the Greek, Bulgarian, and Turkish borders.","However, the lack of verifiable statistics on irregular migration and discrepancies between media reports and actual migration patterns require further exploration.","The objective of this study is to bridge this knowledge gap by harnessing novel data sources, specifically mobile phone and Twitter data, to construct estimators of cross-border mobility and to cultivate a qualitative comprehension of the unfolding events.","By employing a migration diplomacy framework, we analyse emergent mobility patterns at the border.","Our findings demonstrate the potential of mobile phone data for quantitative metrics and Twitter data for qualitative understanding.","We underscore the ethical implications of leveraging Big Data, particularly considering the vulnerability of the population under study.","This underscores the imperative for exhaustive research into the socio-political facets of human mobility, with the aim of discerning the potentialities, limitations, and risks inherent in these data sources and their integration.","This scholarly endeavour contributes to a more nuanced understanding of migration dynamics and paves the way for the formulation of regulations that preclude misuse and oppressive surveillance, thereby ensuring a more accurate representation of migration realities."],"url":"http://arxiv.org/abs/2405.12642v1","category":"cs.SI"}
{"created":"2024-05-21 09:24:30","title":"Asymptotic Properties of Matthews Correlation Coefficient","abstract":"Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions. The Matthews correlation coefficient (MCC) is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances. Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC. This deficiency often leads to studies merely validating and comparing MCC point estimates, a practice that, while common, overlooks the statistical significance and reliability of results. Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs. Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances. Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field.","sentences":["Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions.","The Matthews correlation coefficient (MCC) is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances.","Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC.","This deficiency often leads to studies merely validating and comparing MCC point estimates, a practice that, while common, overlooks the statistical significance and reliability of results.","Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs.","Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances.","Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field."],"url":"http://arxiv.org/abs/2405.12622v1","category":"stat.ME"}
{"created":"2024-05-21 09:06:47","title":"Investigating stellar wind dynamics of WR135: A pulsating WC8-type star","abstract":"We report high-frequency pulsations in WR135 from short cadence optical photometric and spectroscopic time series surveys. The harmonics up to $6^{th}$ order are detected from the integrated photometric flux variation while the comparatively weaker $8^{th}$ harmonic is detected from the varying strengths of the least blended emission lines. We investigate the atmospheric stratification of the stellar winds of WR135 using a radiative transfer modeling code, CMFGEN, and find the physical conditions that can support the propagation of such pulsations. From our study, we find that the sub-sonic layers of the atmosphere are close to the Eddington limit and are launched by the Fe-opacity. While the outer optically thin super-sonic winds ($\\tau_{ross}$=0.1-0.01) are driven by the He II and C IV opacities. The stratified winds above the sonic point undergo velocity perturbation that can lead to clumps. We find that the optically thin supersonic winds ($\\tau_{ross}$=0.1-0.01) contain dense smaller clumps (f=0.2-0.3) that oscillate under the higher-order harmonics of the pulsation. These clumps grow larger (f=0.1), dominate the outer stellar winds ($\\tau_{ross}$=0.01-0.001), and oscillate under the lower-order harmonics.","sentences":["We report high-frequency pulsations in WR135 from short cadence optical photometric and spectroscopic time series surveys.","The harmonics up to $6^{th}$ order are detected from the integrated photometric flux variation while the comparatively weaker $8^{th}$ harmonic is detected from the varying strengths of the least blended emission lines.","We investigate the atmospheric stratification of the stellar winds of WR135 using a radiative transfer modeling code, CMFGEN, and find the physical conditions that can support the propagation of such pulsations.","From our study, we find that the sub-sonic layers of the atmosphere are close to the Eddington limit and are launched by the Fe-opacity.","While the outer optically thin super-sonic winds ($\\tau_{ross}$=0.1-0.01) are driven by the He II and C IV opacities.","The stratified winds above the sonic point undergo velocity perturbation that can lead to clumps.","We find that the optically thin supersonic winds ($\\tau_{ross}$=0.1-0.01) contain dense smaller clumps (f=0.2-0.3) that oscillate under the higher-order harmonics of the pulsation.","These clumps grow larger (f=0.1), dominate the outer stellar winds ($\\tau_{ross}$=0.01-0.001), and oscillate under the lower-order harmonics."],"url":"http://arxiv.org/abs/2405.12613v1","category":"astro-ph.SR"}
{"created":"2024-05-21 08:50:57","title":"Quantum confinement theory of the heat capacity of thin films","abstract":"A theory and mechanistic understanding of the thermal properties of solids under nanoscale confinement are currently missing. We develop a theoretical quantum confinement description of thin films which predicts a new physical law for the heat capacity. In particular, due to the suppression of vibrational modes caused by the thin film confinement, the vibrational density of states (VDOS) deviates from the Debye quadratic law in frequency and is, instead, cubic in frequency. This leads to a temperature dependence of the heat capacity which is $\\sim T^4$ instead of Debye's $\\sim T^3$ law. Furthermore, the new theory predicts a linear increase of the heat capacity upon increasing the nanometric film thickness. Both dependencies are found in excellent agreement with recent experimental data on NbTiN thin films.","sentences":["A theory and mechanistic understanding of the thermal properties of solids under nanoscale confinement are currently missing.","We develop a theoretical quantum confinement description of thin films which predicts a new physical law for the heat capacity.","In particular, due to the suppression of vibrational modes caused by the thin film confinement, the vibrational density of states (VDOS) deviates from the Debye quadratic law in frequency and is, instead, cubic in frequency.","This leads to a temperature dependence of the heat capacity which is $\\sim T^4$ instead of Debye's $\\sim T^3$ law.","Furthermore, the new theory predicts a linear increase of the heat capacity upon increasing the nanometric film thickness.","Both dependencies are found in excellent agreement with recent experimental data on NbTiN thin films."],"url":"http://arxiv.org/abs/2405.12600v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 08:38:10","title":"Correlated insulators and charge density wave states in chirally twisted triple bilayer graphene","abstract":"Motivated by recent experimental observations of displacement-field-tuned correlated insulators at integer and half-integer fillings in chirally twisted triple bilayer graphene (CTTBG), we study the single-particle and interacting physics of CTTBG. We find that there are two inequivalent stacking orders, {\\it i.e.}, ABABBC and ABABAB, and both exhibit flat bands with nontrivial topology. We then use the Hartree-Fock approximation to calculate the rich phase diagram of CTTBG at all integer and half-integer fillings in both stacking orders and under the vertical displacement field. Under a small displacement field, the groundstates are flavor polarized states for ABABBC stacking order and intervalley coherent states for ABABAB stacking order at all integer and half-integer fillings. A larger displacement field will turn them into layer-polarized states. At half-integer fillings, the groundstates also exhibit charge density wave (CDW) order. For ABABAB stacking, the groundstates are always $2\\times1$ stripe state among a range of displacement fields. For ABABBC stacking, the groundstates are also $2\\times1$ stripe states under a small displacement field and a larger displacement will possibly favor further translation-symmetry-breaking, depending on filling and the direction of the displacement field. We demonstrate that the CDW states observed in the experiment can originate from the strong Coulomb interaction of the flat band electrons.","sentences":["Motivated by recent experimental observations of displacement-field-tuned correlated insulators at integer and half-integer fillings in chirally twisted triple bilayer graphene (CTTBG), we study the single-particle and interacting physics of CTTBG.","We find that there are two inequivalent stacking orders, {\\it i.e.}, ABABBC and ABABAB, and both exhibit flat bands with nontrivial topology.","We then use the Hartree-Fock approximation to calculate the rich phase diagram of CTTBG at all integer and half-integer fillings in both stacking orders and under the vertical displacement field.","Under a small displacement field, the groundstates are flavor polarized states for ABABBC stacking order and intervalley coherent states for ABABAB stacking order at all integer and half-integer fillings.","A larger displacement field will turn them into layer-polarized states.","At half-integer fillings, the groundstates also exhibit charge density wave (CDW) order.","For ABABAB stacking, the groundstates are always $2\\times1$ stripe state among a range of displacement fields.","For ABABBC stacking, the groundstates are also $2\\times1$ stripe states under a small displacement field and a larger displacement will possibly favor further translation-symmetry-breaking, depending on filling and the direction of the displacement field.","We demonstrate that the CDW states observed in the experiment can originate from the strong Coulomb interaction of the flat band electrons."],"url":"http://arxiv.org/abs/2405.12595v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 08:31:11","title":"Invariance of elliptic genus under wall-crossing","abstract":"Wall-crossing formulas for various flavors of elliptic genus can be obtained using master spaces. We give a topological criterion which implies that such wall-crossing formulas are trivial. Applications are given for: GIT quotients, following Thaddeus; moduli of sheaves, following Mochizuki; Donaldson-Thomas and Vafa-Witten theory, following Joyce and Tanaka-Thomas respectively.","sentences":["Wall-crossing formulas for various flavors of elliptic genus can be obtained using master spaces.","We give a topological criterion which implies that such wall-crossing formulas are trivial.","Applications are given for: GIT quotients, following Thaddeus; moduli of sheaves, following Mochizuki; Donaldson-Thomas and Vafa-Witten theory, following Joyce and Tanaka-Thomas respectively."],"url":"http://arxiv.org/abs/2405.12587v1","category":"math.AG"}
{"created":"2024-05-21 08:22:14","title":"Fast Estimation of Relative Transformation Based on Fusion of Odometry and UWB Ranging Data","abstract":"In this paper, we investigate the problem of estimating the 4-DOF (three-dimensional position and orientation) robot-robot relative frame transformation using odometers and distance measurements between robots. Firstly, we apply a two-step estimation method based on maximum likelihood estimation. Specifically, a good initial value is obtained through unconstrained least squares and projection, followed by a more accurate estimate achieved through one-step Gauss-Newton iteration. Additionally, the optimal installation positions of Ultra-Wideband (UWB) are provided, and the minimum operating time under different quantities of UWB devices is determined. Simulation demonstrates that the two-step approach offers faster computation with guaranteed accuracy while effectively addressing the relative transformation estimation problem within limited space constraints. Furthermore, this method can be applied to real-time relative transformation estimation when a specific number of UWB devices are installed.","sentences":["In this paper, we investigate the problem of estimating the 4-DOF (three-dimensional position and orientation) robot-robot relative frame transformation using odometers and distance measurements between robots.","Firstly, we apply a two-step estimation method based on maximum likelihood estimation.","Specifically, a good initial value is obtained through unconstrained least squares and projection, followed by a more accurate estimate achieved through one-step Gauss-Newton iteration.","Additionally, the optimal installation positions of Ultra-Wideband (UWB) are provided, and the minimum operating time under different quantities of UWB devices is determined.","Simulation demonstrates that the two-step approach offers faster computation with guaranteed accuracy while effectively addressing the relative transformation estimation problem within limited space constraints.","Furthermore, this method can be applied to real-time relative transformation estimation when a specific number of UWB devices are installed."],"url":"http://arxiv.org/abs/2405.12577v1","category":"cs.RO"}
{"created":"2024-05-21 08:15:17","title":"iHERO: Interactive Human-oriented Exploration and Supervision Under Scarce Communication","abstract":"Exploration of unknown scenes before human entry is essential for safety and efficiency in numerous scenarios, e.g., subterranean exploration, reconnaissance, search and rescue missions. Fleets of autonomous robots are particularly suitable for this task, via concurrent exploration, multi-sensory perception and autonomous navigation. Communication however among the robots can be severely restricted to only close- range exchange via ad-hoc networks. Although some recent works have addressed the problem of collaborative exploration under restricted communication, the crucial role of the human operator has been mostly neglected. Indeed, the operator may: (i) require timely update regarding the exploration progress and fleet status; (ii) prioritize certain regions; and (iii) dynamically move within the explored area; To facilitate these requests, this work proposes an interactive human-oriented online coordination framework for collaborative exploration and supervision under scarce communication (iHERO). The robots switch smoothly and optimally among fast exploration, intermittent exchange of map and sensory data, and return to the operator for status update. It is ensured that these requests are fulfilled online interactively with a pre-specified latency. Extensive large-scale human-in- the-loop simulations and hardware experiments are performed over numerous challenging scenes, which signify its performance such as explored area and efficiency, and validate its potential applicability to real-world scenarios.","sentences":["Exploration of unknown scenes before human entry is essential for safety and efficiency in numerous scenarios, e.g., subterranean exploration, reconnaissance, search and rescue missions.","Fleets of autonomous robots are particularly suitable for this task, via concurrent exploration, multi-sensory perception and autonomous navigation.","Communication however among the robots can be severely restricted to only close- range exchange via ad-hoc networks.","Although some recent works have addressed the problem of collaborative exploration under restricted communication, the crucial role of the human operator has been mostly neglected.","Indeed, the operator may: (i) require timely update regarding the exploration progress and fleet status; (ii) prioritize certain regions; and (iii) dynamically move within the explored area; To facilitate these requests, this work proposes an interactive human-oriented online coordination framework for collaborative exploration and supervision under scarce communication (iHERO).","The robots switch smoothly and optimally among fast exploration, intermittent exchange of map and sensory data, and return to the operator for status update.","It is ensured that these requests are fulfilled online interactively with a pre-specified latency.","Extensive large-scale human-in- the-loop simulations and hardware experiments are performed over numerous challenging scenes, which signify its performance such as explored area and efficiency, and validate its potential applicability to real-world scenarios."],"url":"http://arxiv.org/abs/2405.12571v1","category":"cs.RO"}
{"created":"2024-05-21 08:06:33","title":"Resilience Analysis of Multi-modal Logistics Service Network Through Robust Optimization with Budget-of-Uncertainty","abstract":"Supply chain resilience analysis aims to identify the critical elements in the supply chain, measure its reliability, and analyze solutions for improving vulnerabilities. While extensive methods like stochastic approaches have been dominant, robust optimization-widely applied in robust planning under uncertainties without specific probability distributions-remains relatively underexplored for this research problem. This paper employs robust optimization with budget-of-uncertainty as a tool to analyze the resilience of multi-modal logistics service networks under time uncertainty. We examine the interactive effects of three critical factors: network size, disruption scale, disruption degree. The computational experiments offer valuable managerial insights for practitioners and researchers.","sentences":["Supply chain resilience analysis aims to identify the critical elements in the supply chain, measure its reliability, and analyze solutions for improving vulnerabilities.","While extensive methods like stochastic approaches have been dominant, robust optimization-widely applied in robust planning under uncertainties without specific probability distributions-remains relatively underexplored for this research problem.","This paper employs robust optimization with budget-of-uncertainty as a tool to analyze the resilience of multi-modal logistics service networks under time uncertainty.","We examine the interactive effects of three critical factors: network size, disruption scale, disruption degree.","The computational experiments offer valuable managerial insights for practitioners and researchers."],"url":"http://arxiv.org/abs/2405.12565v1","category":"q-fin.RM"}
{"created":"2024-05-21 08:05:43","title":"NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments","abstract":"Over the last few decades, numerous LiDAR-inertial odometry (LIO) algorithms have been developed, demonstrating satisfactory performance across diverse environments. Most of these algorithms have predominantly been validated in open outdoor environments, however they often encounter challenges in confined indoor settings. In such indoor environments, reliable point cloud registration becomes problematic due to the rapid changes in LiDAR scans and repetitive structural features like walls and stairs, particularly in multifloor buildings. In this paper, we present NV-LIO, a normal vector based LIO framework, designed for simultaneous localization and mapping (SLAM) in indoor environments with multifloor structures. Our approach extracts the normal vectors from the LiDAR scans and utilizes them for correspondence search to enhance the point cloud registration performance. To ensure robust registration, the distribution of the normal vector directions is analyzed, and situations of degeneracy are examined to adjust the matching uncertainty. Additionally, a viewpoint based loop closure module is implemented to avoid wrong correspondences that are blocked by the walls. The propsed method is validated through public datasets and our own dataset. To contribute to the community, the code will be made public on https://github.com/dhchung/nv_lio.","sentences":["Over the last few decades, numerous LiDAR-inertial odometry (LIO) algorithms have been developed, demonstrating satisfactory performance across diverse environments.","Most of these algorithms have predominantly been validated in open outdoor environments, however they often encounter challenges in confined indoor settings.","In such indoor environments, reliable point cloud registration becomes problematic due to the rapid changes in LiDAR scans and repetitive structural features like walls and stairs, particularly in multifloor buildings.","In this paper, we present NV-LIO, a normal vector based LIO framework, designed for simultaneous localization and mapping (SLAM) in indoor environments with multifloor structures.","Our approach extracts the normal vectors from the LiDAR scans and utilizes them for correspondence search to enhance the point cloud registration performance.","To ensure robust registration, the distribution of the normal vector directions is analyzed, and situations of degeneracy are examined to adjust the matching uncertainty.","Additionally, a viewpoint based loop closure module is implemented to avoid wrong correspondences that are blocked by the walls.","The propsed method is validated through public datasets and our own dataset.","To contribute to the community, the code will be made public on https://github.com/dhchung/nv_lio."],"url":"http://arxiv.org/abs/2405.12563v1","category":"cs.RO"}
{"created":"2024-05-21 08:03:16","title":"Implicit-explicit Crank-Nicolson scheme for Oseen's equation at high Reynolds number","abstract":"In this paper we continue the work on implicit-explicit (IMEX) time discretizations for the incompressible Oseen equations that we started in \\cite{BGG23} (E. Burman, D. Garg, J. Guzm\\`an, {\\emph{Implicit-explicit time discretization for Oseen's equation at high Reynolds number with application to fractional step methods}}, SIAM J. Numer. Anal., 61, 2859--2886, 2023). The pressure velocity coupling and the viscous terms are treated implicitly, while the convection term is treated explicitly using extrapolation. Herein we focus on the implicit-explicit Crank-Nicolson method for time discretization. For the discretization in space we consider finite element methods with stabilization on the gradient jumps. The stabilizing terms ensures inf-sup stability for equal order interpolation and robustness at high Reynolds number. Under suitable Courant conditions we prove stability of the implicit-explicit Crank-Nicolson scheme in this regime. The stabilization allows us to prove error estimates of order $O(h^{k+\\frac12} + \\tau^2)$. Here $h$ is the mesh parameter, $k$ the polynomial order and $\\tau$ the time step. Finally we discuss some fractional step methods that are implied by the IMEX scheme. Numerical examples are reported comparing the different methods when applied to the Navier-Stokes' equations.","sentences":["In this paper we continue the work on implicit-explicit (IMEX) time discretizations for the incompressible Oseen equations that we started in \\cite{BGG23} (E. Burman, D. Garg, J. Guzm\\`an, {\\emph{Implicit-explicit time discretization for Oseen's equation at high Reynolds number with application to fractional step methods}}, SIAM J. Numer.","Anal., 61, 2859--2886, 2023).","The pressure velocity coupling and the viscous terms are treated implicitly, while the convection term is treated explicitly using extrapolation.","Herein we focus on the implicit-explicit Crank-Nicolson method for time discretization.","For the discretization in space we consider finite element methods with stabilization on the gradient jumps.","The stabilizing terms ensures inf-sup stability for equal order interpolation and robustness at high Reynolds number.","Under suitable Courant conditions we prove stability of the implicit-explicit Crank-Nicolson scheme in this regime.","The stabilization allows us to prove error estimates of order $O(h^{k+\\frac12} + \\tau^2)$. Here $h$ is the mesh parameter, $k$ the polynomial order and $\\tau$ the time step.","Finally we discuss some fractional step methods that are implied by the IMEX scheme.","Numerical examples are reported comparing the different methods when applied to the Navier-Stokes' equations."],"url":"http://arxiv.org/abs/2405.12562v1","category":"math.NA"}
{"created":"2024-05-21 07:59:33","title":"Cosmic rays from annihilation of heavy dark matter particles","abstract":"The origin of the ultra high energy cosmic rays via annihilation of heavy stable, fermions \"f\", of the cosmological dark matter (DM) is studied. The particles in question are supposed to be created by the scalaron decays in $R^2$ modified gravity. Novel part of our approach is the assumption that the mass of these carriers of DM is slightly below than a half of the scalaron mass. In such a case the phase space volume becomes tiny. It leads to sufficiently low probability of \"f\" production, so their average cosmological energy density could be equal to the observed energy density of dark matter. Several regions of the universe, where the annihilation could take place, are studied. They include the whole universe under assumption of homogeneous energy density, the high density DM clump in the galactic centre, the cloud of DM in the Galaxy with realistic density distribution, and high density clusters of DM in the Galaxy. Possible resonance annihilation of $f \\bar f$ into energetic light particle is considered. We have shown that the proposed scenario can successfully explain the origin of the ultrahigh energy flux of cosmic rays where the canonical astrophysical mechanisms are not operative.","sentences":["The origin of the ultra high energy cosmic rays via annihilation of heavy stable, fermions \"f\", of the cosmological dark matter (DM) is studied.","The particles in question are supposed to be created by the scalaron decays in $R^2$ modified gravity.","Novel part of our approach is the assumption that the mass of these carriers of DM is slightly below than a half of the scalaron mass.","In such a case the phase space volume becomes tiny.","It leads to sufficiently low probability of \"f\" production, so their average cosmological energy density could be equal to the observed energy density of dark matter.","Several regions of the universe, where the annihilation could take place, are studied.","They include the whole universe under assumption of homogeneous energy density, the high density DM clump in the galactic centre, the cloud of DM in the Galaxy with realistic density distribution, and high density clusters of DM in the Galaxy.","Possible resonance annihilation of $f \\bar f$ into energetic light particle is considered.","We have shown that the proposed scenario can successfully explain the origin of the ultrahigh energy flux of cosmic rays where the canonical astrophysical mechanisms are not operative."],"url":"http://arxiv.org/abs/2405.12560v1","category":"hep-ph"}
{"created":"2024-05-21 07:47:21","title":"Uncertainty quantification by block bootstrap for differentially private stochastic gradient descent","abstract":"Stochastic Gradient Descent (SGD) is a widely used tool in machine learning. In the context of Differential Privacy (DP), SGD has been well studied in the last years in which the focus is mainly on convergence rates and privacy guarantees. While in the non private case, uncertainty quantification (UQ) for SGD by bootstrap has been addressed by several authors, these procedures cannot be transferred to differential privacy due to multiple queries to the private data. In this paper, we propose a novel block bootstrap for SGD under local differential privacy that is computationally tractable and does not require an adjustment of the privacy budget. The method can be easily implemented and is applicable to a broad class of estimation problems. We prove the validity of our approach and illustrate its finite sample properties by means of a simulation study. As a by-product, the new method also provides a simple alternative numerical tool for UQ for non-private SGD.","sentences":["Stochastic Gradient Descent (SGD) is a widely used tool in machine learning.","In the context of Differential Privacy (DP), SGD has been well studied in the last years in which the focus is mainly on convergence rates and privacy guarantees.","While in the non private case, uncertainty quantification (UQ) for SGD by bootstrap has been addressed by several authors, these procedures cannot be transferred to differential privacy due to multiple queries to the private data.","In this paper, we propose a novel block bootstrap for SGD under local differential privacy that is computationally tractable and does not require an adjustment of the privacy budget.","The method can be easily implemented and is applicable to a broad class of estimation problems.","We prove the validity of our approach and illustrate its finite sample properties by means of a simulation study.","As a by-product, the new method also provides a simple alternative numerical tool for UQ for non-private SGD."],"url":"http://arxiv.org/abs/2405.12553v1","category":"stat.ML"}
{"created":"2024-05-21 06:48:26","title":"Dataset and Benchmark for Urdu Natural Scenes Text Detection, Recognition and Visual Question Answering","abstract":"The development of Urdu scene text detection, recognition, and Visual Question Answering (VQA) technologies is crucial for advancing accessibility, information retrieval, and linguistic diversity in digital content, facilitating better understanding and interaction with Urdu-language visual data. This initiative seeks to bridge the gap between textual and visual comprehension. We propose a new multi-task Urdu scene text dataset comprising over 1000 natural scene images, which can be used for text detection, recognition, and VQA tasks. We provide fine-grained annotations for text instances, addressing the limitations of previous datasets for facing arbitrary-shaped texts. By incorporating additional annotation points, this dataset facilitates the development and assessment of methods that can handle diverse text layouts, intricate shapes, and non-standard orientations commonly encountered in real-world scenarios. Besides, the VQA annotations make it the first benchmark for the Urdu Text VQA method, which can prompt the development of Urdu scene text understanding. The proposed dataset is available at: https://github.com/Hiba-MeiRuan/Urdu-VQA-Dataset-/tree/main","sentences":["The development of Urdu scene text detection, recognition, and Visual Question Answering (VQA) technologies is crucial for advancing accessibility, information retrieval, and linguistic diversity in digital content, facilitating better understanding and interaction with Urdu-language visual data.","This initiative seeks to bridge the gap between textual and visual comprehension.","We propose a new multi-task Urdu scene text dataset comprising over 1000 natural scene images, which can be used for text detection, recognition, and VQA tasks.","We provide fine-grained annotations for text instances, addressing the limitations of previous datasets for facing arbitrary-shaped texts.","By incorporating additional annotation points, this dataset facilitates the development and assessment of methods that can handle diverse text layouts, intricate shapes, and non-standard orientations commonly encountered in real-world scenarios.","Besides, the VQA annotations make it the first benchmark for the Urdu Text VQA method, which can prompt the development of Urdu scene text understanding.","The proposed dataset is available at: https://github.com/Hiba-MeiRuan/Urdu-VQA-Dataset-/tree/main"],"url":"http://arxiv.org/abs/2405.12533v1","category":"cs.CV"}
{"created":"2024-05-21 05:39:31","title":"Active Object Detection with Knowledge Aggregation and Distillation from Large Models","abstract":"Accurately detecting active objects undergoing state changes is essential for comprehending human interactions and facilitating decision-making. The existing methods for active object detection (AOD) primarily rely on visual appearance of the objects within input, such as changes in size, shape and relationship with hands. However, these visual changes can be subtle, posing challenges, particularly in scenarios with multiple distracting no-change instances of the same category. We observe that the state changes are often the result of an interaction being performed upon the object, thus propose to use informed priors about object related plausible interactions (including semantics and visual appearance) to provide more reliable cues for AOD. Specifically, we propose a knowledge aggregation procedure to integrate the aforementioned informed priors into oracle queries within the teacher decoder, offering more object affordance commonsense to locate the active object. To streamline the inference process and reduce extra knowledge inputs, we propose a knowledge distillation approach that encourages the student decoder to mimic the detection capabilities of the teacher decoder using the oracle query by replicating its predictions and attention. Our proposed framework achieves state-of-the-art performance on four datasets, namely Ego4D, Epic-Kitchens, MECCANO, and 100DOH, which demonstrates the effectiveness of our approach in improving AOD.","sentences":["Accurately detecting active objects undergoing state changes is essential for comprehending human interactions and facilitating decision-making.","The existing methods for active object detection (AOD) primarily rely on visual appearance of the objects within input, such as changes in size, shape and relationship with hands.","However, these visual changes can be subtle, posing challenges, particularly in scenarios with multiple distracting no-change instances of the same category.","We observe that the state changes are often the result of an interaction being performed upon the object, thus propose to use informed priors about object related plausible interactions (including semantics and visual appearance) to provide more reliable cues for AOD.","Specifically, we propose a knowledge aggregation procedure to integrate the aforementioned informed priors into oracle queries within the teacher decoder, offering more object affordance commonsense to locate the active object.","To streamline the inference process and reduce extra knowledge inputs, we propose a knowledge distillation approach that encourages the student decoder to mimic the detection capabilities of the teacher decoder using the oracle query by replicating its predictions and attention.","Our proposed framework achieves state-of-the-art performance on four datasets, namely Ego4D, Epic-Kitchens, MECCANO, and 100DOH, which demonstrates the effectiveness of our approach in improving AOD."],"url":"http://arxiv.org/abs/2405.12509v1","category":"cs.CV"}
{"created":"2024-05-21 05:31:13","title":"Upper bounds for moments of zeta sums","abstract":"We establish upper bounds for moments of zeta sums using results on shifted moments of the Riemann zeta function under the Riemann hypothesis.","sentences":["We establish upper bounds for moments of zeta sums using results on shifted moments of the Riemann zeta function under the Riemann hypothesis."],"url":"http://arxiv.org/abs/2405.12506v1","category":"math.NT"}
{"created":"2024-05-21 04:16:05","title":"First joint oscillation analysis of Super-Kamiokande atmospheric and T2K accelerator neutrino data","abstract":"The Super-Kamiokande and T2K collaborations present a joint measurement of neutrino oscillation parameters from their atmospheric and beam neutrino data. It uses a common interaction model for events overlapping in neutrino energy and correlated detector systematic uncertainties between the two datasets, which are found to be compatible. Using 3244.4 days of atmospheric data and a beam exposure of $19.7(16.3) \\times 10^{20}$ protons on target in (anti)neutrino mode, the analysis finds a 1.9$\\sigma$ exclusion of CP-conservation (defined as $J_{CP}=0$) and a preference for the normal mass ordering.","sentences":["The Super-Kamiokande and T2K collaborations present a joint measurement of neutrino oscillation parameters from their atmospheric and beam neutrino data.","It uses a common interaction model for events overlapping in neutrino energy and correlated detector systematic uncertainties between the two datasets, which are found to be compatible.","Using 3244.4 days of atmospheric data and a beam exposure of $19.7(16.3)","\\times 10^{20}$ protons on target in (anti)neutrino mode, the analysis finds a 1.9$\\sigma$ exclusion of CP-conservation (defined as $J_{CP}=0$) and a preference for the normal mass ordering."],"url":"http://arxiv.org/abs/2405.12488v1","category":"hep-ex"}
{"created":"2024-05-21 03:48:55","title":"Dynamic Asset Pricing in a Unified Bachelier-Black-Scholes-Merton Model","abstract":"We develop asset pricing under a unified Bachelier and Black-Scholes-Merton (BBSM) market model. We derive option pricing via the Feynman-Kac formula as well as through deflator-driven risk-neutral valuation. We show a necessary condition for the unified model to support a perpetual derivative. We develop discrete binomial pricing under the unified model. Finally, we investigate the term structure of interest rates by considering the pricing of zero-coupon bonds, forward and futures contracts. In all cases, we show that the unified model reduces to standard Black-Scholes-Merton pricing (in the appropriate parameter limit) and derive (also under the appropriate limit) pricing for a Bachelier model. The Bachelier limit of our unified model allows for positive riskless rates.","sentences":["We develop asset pricing under a unified Bachelier and Black-Scholes-Merton (BBSM) market model.","We derive option pricing via the Feynman-Kac formula as well as through deflator-driven risk-neutral valuation.","We show a necessary condition for the unified model to support a perpetual derivative.","We develop discrete binomial pricing under the unified model.","Finally, we investigate the term structure of interest rates by considering the pricing of zero-coupon bonds, forward and futures contracts.","In all cases, we show that the unified model reduces to standard Black-Scholes-Merton pricing (in the appropriate parameter limit) and derive (also under the appropriate limit) pricing for a Bachelier model.","The Bachelier limit of our unified model allows for positive riskless rates."],"url":"http://arxiv.org/abs/2405.12479v1","category":"q-fin.MF"}
{"created":"2024-05-21 02:30:13","title":"A High Compression Ratio Channel Multiplexing Method for Micro-pattern Gaseous Detectors","abstract":"Micro-pattern gas detectors (MPGD) find wide-ranging applications in particle physics experiments, industry, and medical services, owing to their large area, fine spatial resolution, and relatively low material content within the sensitive region. However, the demand for a large number of readout channels poses a bottleneck, limiting the application of MPGD to achieve higher accuracy and more extensive area. This requirement also presents significant challenges regarding system integration, power consumption, cooling, and cost.   Previous studies have shown that, under sparse effective hits, a channel multiplexing method based on position encoded can address this issue.   Nonetheless, improving the compression ratio and addressing the high event rate problem remain key challenges requiring further investigation.   In this research, we have developed two types of high compression ratio multiplexing methods and their mathematical models. It is shown that a maximum of $n \\times (n-1)/2 - (n - 2)/2 + 1$ detector channels can be read out with n electronics channels if n is even. Using these methods, several multiplexing boards were designed and implemented. Considering the real condition of the detectors, we achieved an multiplexing board with 64 readout electronics reading out 1024 detector channels, marking the highest compression ratio in current research. Moreover, these multiplexing circuits were utilized and verified in our cosmic-ray muon imaging facilities, demonstrating their advantage of reducing the required number of front-end electronics cards.","sentences":["Micro-pattern gas detectors (MPGD) find wide-ranging applications in particle physics experiments, industry, and medical services, owing to their large area, fine spatial resolution, and relatively low material content within the sensitive region.","However, the demand for a large number of readout channels poses a bottleneck, limiting the application of MPGD to achieve higher accuracy and more extensive area.","This requirement also presents significant challenges regarding system integration, power consumption, cooling, and cost.   ","Previous studies have shown that, under sparse effective hits, a channel multiplexing method based on position encoded can address this issue.   ","Nonetheless, improving the compression ratio and addressing the high event rate problem remain key challenges requiring further investigation.   ","In this research, we have developed two types of high compression ratio multiplexing methods and their mathematical models.","It is shown that a maximum of $n \\times (n-1)/2 - (n - 2)/2 + 1$ detector channels can be read out with n electronics channels if n is even.","Using these methods, several multiplexing boards were designed and implemented.","Considering the real condition of the detectors, we achieved an multiplexing board with 64 readout electronics reading out 1024 detector channels, marking the highest compression ratio in current research.","Moreover, these multiplexing circuits were utilized and verified in our cosmic-ray muon imaging facilities, demonstrating their advantage of reducing the required number of front-end electronics cards."],"url":"http://arxiv.org/abs/2405.12457v1","category":"physics.ins-det"}
{"created":"2024-05-21 01:57:06","title":"Dirac fermions under rainbow gravity effects in the Bonnor-Melvin-Lambda spacetime","abstract":"In this paper, we study the relativistic energy spectrum for Dirac fermions under rainbow gravity effects in the $(3+1)$-dimensional Bonnor-Melvin-Lambda spacetime, where we work with the curved Dirac equation in cylindrical coordinates. Using the tetrads formalism of General Relativity and considering a first-order approximation for the trigonometric functions, we obtain a Bessel equation. To solve this differential equation, we also consider a region where a hard-wall confining potential is present (i.e., some finite distance where the radial wave function is null). In other words, we define a second boundary condition (Dirichlet boundary condition) to achieve the quantization of the energy. Consequently, we obtain the spectrum for a fermion/antifermion, which is quantized in terms of quantum numbers $n$, $m_j$ and $m_s$, where $n$ is the radial quantum number, $m_j$ is the total magnetic quantum number, $m_s$ is the spin magnetic quantum number, and explicitly depends on the rainbow functions $F(\\xi)$ and $G(\\xi)$, curvature parameter $\\alpha$, cosmological constant $\\Lambda$, fixed radius $r_0$, and on the rest energy $m_0$, and $z$-momentum $p_z$. So, analyzing this spectrum according to the values of $m_j$ and $m_s$, we see that for $m_j>0$ with $m_s=-1/2$ (positive angular momentum and spin down), and for $m_j<0$ with $m_s=+1/2$ (negative angular momentum and spin up), the spectrum is the same. Besides, we graphically analyze in detail the behavior of the spectrum for the three scenarios of rainbow gravity as a function of $\\Lambda$, $r_0$, and $\\alpha$ for three different values of $n$ (ground state and the first two excited states).","sentences":["In this paper, we study the relativistic energy spectrum for Dirac fermions under rainbow gravity effects in the $(3+1)$-dimensional Bonnor-Melvin-Lambda spacetime, where we work with the curved Dirac equation in cylindrical coordinates.","Using the tetrads formalism of General Relativity and considering a first-order approximation for the trigonometric functions, we obtain a Bessel equation.","To solve this differential equation, we also consider a region where a hard-wall confining potential is present (i.e., some finite distance where the radial wave function is null).","In other words, we define a second boundary condition (Dirichlet boundary condition) to achieve the quantization of the energy.","Consequently, we obtain the spectrum for a fermion/antifermion, which is quantized in terms of quantum numbers $n$, $m_j$ and $m_s$, where $n$ is the radial quantum number, $m_j$ is the total magnetic quantum number, $m_s$ is the spin magnetic quantum number, and explicitly depends on the rainbow functions $F(\\xi)$ and $G(\\xi)$, curvature parameter $\\alpha$, cosmological constant $\\Lambda$, fixed radius $r_0$, and on the rest energy $m_0$, and $z$-momentum $p_z$. So, analyzing this spectrum according to the values of $m_j$ and $m_s$, we see that for $m_j>0$ with $m_s=-1/2$ (positive angular momentum and spin down), and for $m_j<0$ with $m_s=+1/2$ (negative angular momentum and spin up), the spectrum is the same.","Besides, we graphically analyze in detail the behavior of the spectrum for the three scenarios of rainbow gravity as a function of $\\Lambda$, $r_0$, and $\\alpha$ for three different values of $n$ (ground state and the first two excited states)."],"url":"http://arxiv.org/abs/2405.12449v1","category":"gr-qc"}
{"created":"2024-05-21 01:56:48","title":"The TESS-Keck Survey. XXII. A sub-Neptune Orbiting TOI-1437","abstract":"Exoplanet discoveries have revealed a dramatic diversity of planet sizes across a vast array of orbital architectures. Sub-Neptunes are of particular interest; due to their absence in our own solar system, we rely on demographics of exoplanets to better understand their bulk composition and formation scenarios. Here, we present the discovery and characterization of TOI-1437 b, a sub-Neptune with a 18.84 day orbit around a near-Solar analog (Mstar = 1.10 +/- 0.10 Msun, Rstar = 1.17 +/- 0.12 Rsun). The planet was detected using photometric data from the Transiting Exoplanet Survey Satellite (TESS) mission and radial velocity follow-up observations were carried out as a part of the TESS-Keck Survey (TKS) using both the HIRES instrument at Keck Observatory and the Levy Spectrograph on the Automated Planet Finder (APF) telescope. A combined analysis of these data reveal a planet radius of Rp = 2.24 +/- 0.23 Rearth and a mass measurement of Mp = 9.6 +/- 3.9 Mearth). TOI-1437 b is one of few (~50) known transiting sub-Neptunes orbiting a solar-mass star that has a radial velocity mass measurement. As the formation pathway of these worlds remains an unanswered question, the precise mass characterization of TOI-1437 b may provide further insight into this class of planet.","sentences":["Exoplanet discoveries have revealed a dramatic diversity of planet sizes across a vast array of orbital architectures.","Sub-Neptunes are of particular interest; due to their absence in our own solar system, we rely on demographics of exoplanets to better understand their bulk composition and formation scenarios.","Here, we present the discovery and characterization of TOI-1437 b, a sub-Neptune with a 18.84 day orbit around a near-Solar analog (Mstar = 1.10 +/- 0.10 Msun, Rstar = 1.17 +/- 0.12 Rsun).","The planet was detected using photometric data from the Transiting Exoplanet Survey Satellite (TESS) mission and radial velocity follow-up observations were carried out as a part of the TESS-Keck Survey (TKS) using both the HIRES instrument at Keck Observatory and the Levy Spectrograph on the Automated Planet Finder (APF) telescope.","A combined analysis of these data reveal a planet radius of Rp = 2.24 +/- 0.23 Rearth and a mass measurement of Mp = 9.6 +/- 3.9 Mearth).","TOI-1437 b is one of few (~50) known transiting sub-Neptunes orbiting a solar-mass star that has a radial velocity mass measurement.","As the formation pathway of these worlds remains an unanswered question, the precise mass characterization of TOI-1437 b may provide further insight into this class of planet."],"url":"http://arxiv.org/abs/2405.12448v1","category":"astro-ph.EP"}
{"created":"2024-05-21 01:50:17","title":"Computer assisted proofs for transverse heteroclinics by the parameterization method","abstract":"This work develops a functional analytic framework for making computer assisted arguments involving transverse heteroclinic connecting orbits between hyperbolic periodic solutions of ordinary differential equations. We exploit a Fourier-Taylor approximation of the local stable/unstable manifold of the periodic orbit, combined with a numerical method for solving two point boundary value problems via Chebyshev series approximations. The a-posteriori analysis developed provides mathematically rigorous bounds on all approximation errors, providing both abstract existence results and quantitative information about the true heteroclinic solution. Example calculations are given for both the dissipative Lorenz system and the Hamiltonian Hill Restricted Four Body Problem.","sentences":["This work develops a functional analytic framework for making computer assisted arguments involving transverse heteroclinic connecting orbits between hyperbolic periodic solutions of ordinary differential equations.","We exploit a Fourier-Taylor approximation of the local stable/unstable manifold of the periodic orbit, combined with a numerical method for solving two point boundary value problems via Chebyshev series approximations.","The a-posteriori analysis developed provides mathematically rigorous bounds on all approximation errors, providing both abstract existence results and quantitative information about the true heteroclinic solution.","Example calculations are given for both the dissipative Lorenz system and the Hamiltonian Hill Restricted Four Body Problem."],"url":"http://arxiv.org/abs/2405.12446v1","category":"math.DS"}
{"created":"2024-05-21 01:44:15","title":"Monte Carlo Simulations of the Blinking-Checkers Model for Polyamorphic Fluids","abstract":"The blinking-checkers model [F. Caupin and M. A. Anisimov, Phys. Rev. Lett, 127,185701 (2021)] is a minimal lattice model which has demonstrated that, in the meanfield approximation, it can reproduce the phenomenon of fluid polyamorphism. This model is a binary lattice-gas, in which each site has three possible states: empty, occupied with particles of type 1, and occupied with particles of type 2. Additionally, the two types of particles may interconvert from one to another. Equilibrium interconversion imposes a constraint that makes this model thermodynamically equivalent to a single-component system. In this work, Monte-Carlo simulations of the blinking-checkers model are performed, demonstrating polyamorphic phase behavior. The locations of the liquid-liquid and liquid-gas critical points are found to be different from the meanfield predictions for this model with the same interaction parameters, as the phase behavior is significantly affected by critical fluctuations. Based on the computed values of the critical exponents of the order parameter, susceptibility, correlation length, and surface tension, we confirm that the blinking-checkers model, for both liquid-gas and liquid-liquid equilibria, belongs to the three-dimensional Ising class of critical-point universality.","sentences":["The blinking-checkers model [F. Caupin and M. A. Anisimov, Phys. Rev. Lett, 127,185701 (2021)] is a minimal lattice model which has demonstrated that, in the meanfield approximation, it can reproduce the phenomenon of fluid polyamorphism.","This model is a binary lattice-gas, in which each site has three possible states: empty, occupied with particles of type 1, and occupied with particles of type 2.","Additionally, the two types of particles may interconvert from one to another.","Equilibrium interconversion imposes a constraint that makes this model thermodynamically equivalent to a single-component system.","In this work, Monte-Carlo simulations of the blinking-checkers model are performed, demonstrating polyamorphic phase behavior.","The locations of the liquid-liquid and liquid-gas critical points are found to be different from the meanfield predictions for this model with the same interaction parameters, as the phase behavior is significantly affected by critical fluctuations.","Based on the computed values of the critical exponents of the order parameter, susceptibility, correlation length, and surface tension, we confirm that the blinking-checkers model, for both liquid-gas and liquid-liquid equilibria, belongs to the three-dimensional Ising class of critical-point universality."],"url":"http://arxiv.org/abs/2405.12445v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-21 01:31:44","title":"No-Regret M${}^{\\natural}$-Concave Function Maximization: Stochastic Bandit Algorithms and NP-Hardness of Adversarial Full-Information Setting","abstract":"M${}^{\\natural}$-concave functions, a.k.a. gross substitute valuation functions, play a fundamental role in many fields, including discrete mathematics and economics. In practice, perfect knowledge of M${}^{\\natural}$-concave functions is often unavailable a priori, and we can optimize them only interactively based on some feedback. Motivated by such situations, we study online M${}^{\\natural}$-concave function maximization problems, which are interactive versions of the problem studied by Murota and Shioura (1999). For the stochastic bandit setting, we present $O(T^{-1/2})$-simple regret and $O(T^{2/3})$-regret algorithms under $T$ times access to unbiased noisy value oracles of M${}^{\\natural}$-concave functions. A key to proving these results is the robustness of the greedy algorithm to local errors in M${}^{\\natural}$-concave function maximization, which is one of our main technical results. While we obtain those positive results for the stochastic setting, another main result of our work is an impossibility in the adversarial setting. We prove that, even with full-information feedback, no algorithms that run in polynomial time per round can achieve $O(T^{1-c})$ regret for any constant $c > 0$ unless $\\mathsf{P} = \\mathsf{NP}$. Our proof is based on a reduction from the matroid intersection problem for three matroids, which would be a novel idea in the context of online learning.","sentences":["M${}^{\\natural}$-concave functions, a.k.a. gross substitute valuation functions, play a fundamental role in many fields, including discrete mathematics and economics.","In practice, perfect knowledge of M${}^{\\natural}$-concave functions is often unavailable a priori, and we can optimize them only interactively based on some feedback.","Motivated by such situations, we study online M${}^{\\natural}$-concave function maximization problems, which are interactive versions of the problem studied by Murota and Shioura (1999).","For the stochastic bandit setting, we present $O(T^{-1/2})$-simple regret and $O(T^{2/3})$-regret algorithms under $T$ times access to unbiased noisy value oracles of M${}^{\\natural}$-concave functions.","A key to proving these results is the robustness of the greedy algorithm to local errors in M${}^{\\natural}$-concave function maximization, which is one of our main technical results.","While we obtain those positive results for the stochastic setting, another main result of our work is an impossibility in the adversarial setting.","We prove that, even with full-information feedback, no algorithms that run in polynomial time per round can achieve $O(T^{1-c})$ regret for any constant $c > 0$ unless $\\mathsf{P} =","\\mathsf{NP}$. Our proof is based on a reduction from the matroid intersection problem for three matroids, which would be a novel idea in the context of online learning."],"url":"http://arxiv.org/abs/2405.12439v1","category":"cs.LG"}
{"created":"2024-05-21 01:30:49","title":"Considerations for Single-Arm Trials to Support Accelerated Approval of Oncology Drugs","abstract":"In the last two decades, single-arm trials (SATs) have been effectively used to study anticancer therapies in well-defined patient populations using durable response rates as an objective and interpretable clinical endpoints. With a growing trend of regulatory accelerated approval (AA) requiring randomized controlled trials (RCTs), some confusions have arisen about the roles of SATs in AA. This paper is intended to elucidate conditions under which an SAT may be considered reasonable for AA. Specifically, the paper describes (1) two necessary conditions for designing an SAT, (2) three sufficient conditions that help either optimize the study design or interpret the study results, (3) four conditions that demonstrate substantial evidence of clinical benefits of the drug, and (4) a plan of a confirmatory RCT to verify the clinical benefits. Some further considerations are discussed to help design a scientifically sound SAT and communicate with regulatory agencies. Conditions presented in this paper may serve as a set of references for sponsors using SATs for regulatory decision.","sentences":["In the last two decades, single-arm trials (SATs) have been effectively used to study anticancer therapies in well-defined patient populations using durable response rates as an objective and interpretable clinical endpoints.","With a growing trend of regulatory accelerated approval (AA) requiring randomized controlled trials (RCTs), some confusions have arisen about the roles of SATs in AA.","This paper is intended to elucidate conditions under which an SAT may be considered reasonable for AA.","Specifically, the paper describes (1) two necessary conditions for designing an SAT, (2) three sufficient conditions that help either optimize the study design or interpret the study results, (3) four conditions that demonstrate substantial evidence of clinical benefits of the drug, and (4) a plan of a confirmatory RCT to verify the clinical benefits.","Some further considerations are discussed to help design a scientifically sound SAT and communicate with regulatory agencies.","Conditions presented in this paper may serve as a set of references for sponsors using SATs for regulatory decision."],"url":"http://arxiv.org/abs/2405.12437v1","category":"stat.AP"}
{"created":"2024-05-21 01:25:00","title":"Avoidance of vincular patterns by Catalan words","abstract":"Let $\\mathcal{C}_n$ denote the set of words $w=w_1\\cdots w_n$ on the alphabet of positive integers satisfying $w_{i+1}\\leq w_i+1$ for $1 \\leq i \\leq n-1$ with $w_1=1$. The members of $\\mathcal{C}_n$ are known as Catalan words and are enumerated by the $n$-th Catalan number $C_n$. The problem of finding the cardinality of various avoidance classes of $\\mathcal{C}_n$ has been an ongoing object of study, and members of $\\mathcal{C}_n$ avoiding one or two classical or a single consecutive pattern have been enumerated. In this paper, we extend these results to vincular patterns and seek to determine the cardinality of each avoidance class corresponding to a pattern of type (1,2) or (2,1). In several instances, a simple explicit formula for this cardinality may be given. In the more difficult cases, we find only a formula for the (ordinary) generating function which enumerates the class in question. We make extensive use of functional equations in establishing our generating function results.","sentences":["Let $\\mathcal{C}_n$ denote the set of words $w=w_1\\cdots w_n$ on the alphabet of positive integers satisfying $w_{i+1}\\leq w_i+1$ for $1 \\leq","i \\leq n-1$ with $w_1=1$. The members of $\\mathcal{C}_n$ are known as Catalan words and are enumerated by the $n$-th Catalan number $C_n$. The problem of finding the cardinality of various avoidance classes of $\\mathcal{C}_n$ has been an ongoing object of study, and members of $\\mathcal{C}_n$ avoiding one or two classical or a single consecutive pattern have been enumerated.","In this paper, we extend these results to vincular patterns and seek to determine the cardinality of each avoidance class corresponding to a pattern of type (1,2) or (2,1).","In several instances, a simple explicit formula for this cardinality may be given.","In the more difficult cases, we find only a formula for the (ordinary) generating function which enumerates the class in question.","We make extensive use of functional equations in establishing our generating function results."],"url":"http://arxiv.org/abs/2405.12435v1","category":"math.CO"}
{"created":"2024-05-21 00:31:01","title":"Continuous wave driving elucidates the desynchronisation dynamics of ultrashort dissipative Raman solitons generated in dispersive Kerr resonators","abstract":"Phase-coherent pulsed driving of passive optical fiber resonators enable the generation of ultrashort dissipative Raman solitons with durations well below 100~fs. The existence and characteristics of such solitons critically depends on the desynchronisation between the pulsed driving source and the resonator roundtrip time, yet the full mechanism through which these dependencies arise remains unclear. Here, we numerically demonstrate that Raman solitons can exist even under conditions of continuous wave driving, and by numerically examining the existence and characteristics of Raman solitons under such conditions, we elucidate the role of desynchronisation in pulse-driven systems. In addition to providing new insights on the existence and characteristics of ultrashort Raman solitons, our analysis yields a qualitative explanation for the range of desynchronisations over which the solitons can exist.","sentences":["Phase-coherent pulsed driving of passive optical fiber resonators enable the generation of ultrashort dissipative Raman solitons with durations well below 100~fs.","The existence and characteristics of such solitons critically depends on the desynchronisation between the pulsed driving source and the resonator roundtrip time, yet the full mechanism through which these dependencies arise remains unclear.","Here, we numerically demonstrate that Raman solitons can exist even under conditions of continuous wave driving, and by numerically examining the existence and characteristics of Raman solitons under such conditions, we elucidate the role of desynchronisation in pulse-driven systems.","In addition to providing new insights on the existence and characteristics of ultrashort Raman solitons, our analysis yields a qualitative explanation for the range of desynchronisations over which the solitons can exist."],"url":"http://arxiv.org/abs/2405.12425v1","category":"physics.optics"}
{"created":"2024-05-21 00:26:11","title":"Rethinking Robustness Assessment: Adversarial Attacks on Learning-based Quadrupedal Locomotion Controllers","abstract":"Legged locomotion has recently achieved remarkable success with the progress of machine learning techniques, especially deep reinforcement learning (RL). Controllers employing neural networks have demonstrated empirical and qualitative robustness against real-world uncertainties, including sensor noise and external perturbations. However, formally investigating the vulnerabilities of these locomotion controllers remains a challenge. This difficulty arises from the requirement to pinpoint vulnerabilities across a long-tailed distribution within a high-dimensional, temporally sequential space. As a first step towards quantitative verification, we propose a computational method that leverages sequential adversarial attacks to identify weaknesses in learned locomotion controllers. Our research demonstrates that, even state-of-the-art robust controllers can fail significantly under well-designed, low-magnitude adversarial sequence. Through experiments in simulation and on the real robot, we validate our approach's effectiveness, and we illustrate how the results it generates can be used to robustify the original policy and offer valuable insights into the safety of these black-box policies.","sentences":["Legged locomotion has recently achieved remarkable success with the progress of machine learning techniques, especially deep reinforcement learning (RL).","Controllers employing neural networks have demonstrated empirical and qualitative robustness against real-world uncertainties, including sensor noise and external perturbations.","However, formally investigating the vulnerabilities of these locomotion controllers remains a challenge.","This difficulty arises from the requirement to pinpoint vulnerabilities across a long-tailed distribution within a high-dimensional, temporally sequential space.","As a first step towards quantitative verification, we propose a computational method that leverages sequential adversarial attacks to identify weaknesses in learned locomotion controllers.","Our research demonstrates that, even state-of-the-art robust controllers can fail significantly under well-designed, low-magnitude adversarial sequence.","Through experiments in simulation and on the real robot, we validate our approach's effectiveness, and we illustrate how the results it generates can be used to robustify the original policy and offer valuable insights into the safety of these black-box policies."],"url":"http://arxiv.org/abs/2405.12424v1","category":"cs.RO"}
{"created":"2024-05-20 23:43:32","title":"An explicit construction of the unitarily invariant quaternionic polynomial spaces on the sphere","abstract":"The decomposition of the polynomials on the quaternionic unit sphere in $\\Hd$ into irreducible modules under the action of the quaternionic unitary (symplectic) group and quaternionic scalar multiplication has been studied by several authors. Typically, these abstract decompositions into ``quaternionic spherical harmonics'' specify the irreducible representations involved and their multiplicities.   The elementary constructive approach taken here gives an orthogonal direct sum of irreducibles, which can be described by some low-dimensional subspaces, to which commuting linear operators $L$ and $R$ are applied. These operators map harmonic polynomials to harmonic polynomials, and zonal polynomials to zonal polynomials. We give explicit formulas for the relevant ``zonal polynomials'' and describe the symmetries, dimensions, and ``complexity'' of the subspaces involved.   Possible applications include the construction and analysis of desirable sets of points in quaternionic space, such as equiangular lines, lattices and spherical designs (cubature rules).","sentences":["The decomposition of the polynomials on the quaternionic unit sphere in $\\Hd$ into irreducible modules under the action of the quaternionic unitary (symplectic) group and quaternionic scalar multiplication has been studied by several authors.","Typically, these abstract decompositions into ``quaternionic spherical harmonics'' specify the irreducible representations involved and their multiplicities.   ","The elementary constructive approach taken here gives an orthogonal direct sum of irreducibles, which can be described by some low-dimensional subspaces, to which commuting linear operators $L$ and $R$ are applied.","These operators map harmonic polynomials to harmonic polynomials, and zonal polynomials to zonal polynomials.","We give explicit formulas for the relevant ``zonal polynomials'' and describe the symmetries, dimensions, and ``complexity'' of the subspaces involved.   ","Possible applications include the construction and analysis of desirable sets of points in quaternionic space, such as equiangular lines, lattices and spherical designs (cubature rules)."],"url":"http://arxiv.org/abs/2405.12416v1","category":"math.RT"}
{"created":"2024-05-20 23:38:06","title":"Targeted Multilingual Adaptation for Low-resource Language Families","abstract":"The \"massively-multilingual\" training of multilingual models is known to limit their utility in any one language, and they perform particularly poorly on low-resource languages. However, there is evidence that low-resource languages can benefit from targeted multilinguality, where the model is trained on closely related languages. To test this approach more rigorously, we systematically study best practices for adapting a pre-trained model to a language family. Focusing on the Uralic family as a test case, we adapt XLM-R under various configurations to model 15 languages; we then evaluate the performance of each experimental setting on two downstream tasks and 11 evaluation languages. Our adapted models significantly outperform mono- and multilingual baselines. Furthermore, a regression analysis of hyperparameter effects reveals that adapted vocabulary size is relatively unimportant for low-resource languages, and that low-resource languages can be aggressively up-sampled during training at little detriment to performance in high-resource languages. These results introduce new best practices for performing language adaptation in a targeted setting.","sentences":["The \"massively-multilingual\" training of multilingual models is known to limit their utility in any one language, and they perform particularly poorly on low-resource languages.","However, there is evidence that low-resource languages can benefit from targeted multilinguality, where the model is trained on closely related languages.","To test this approach more rigorously, we systematically study best practices for adapting a pre-trained model to a language family.","Focusing on the Uralic family as a test case, we adapt XLM-R under various configurations to model 15 languages; we then evaluate the performance of each experimental setting on two downstream tasks and 11 evaluation languages.","Our adapted models significantly outperform mono- and multilingual baselines.","Furthermore, a regression analysis of hyperparameter effects reveals that adapted vocabulary size is relatively unimportant for low-resource languages, and that low-resource languages can be aggressively up-sampled during training at little detriment to performance in high-resource languages.","These results introduce new best practices for performing language adaptation in a targeted setting."],"url":"http://arxiv.org/abs/2405.12413v1","category":"cs.CL"}
{"created":"2024-05-20 23:30:07","title":"On Measuring Calibration of Discrete Probabilistic Neural Networks","abstract":"As machine learning systems become increasingly integrated into real-world applications, accurately representing uncertainty is crucial for enhancing their safety, robustness, and reliability. Training neural networks to fit high-dimensional probability distributions via maximum likelihood has become an effective method for uncertainty quantification. However, such models often exhibit poor calibration, leading to overconfident predictions. Traditional metrics like Expected Calibration Error (ECE) and Negative Log Likelihood (NLL) have limitations, including biases and parametric assumptions. This paper proposes a new approach using conditional kernel mean embeddings to measure calibration discrepancies without these biases and assumptions. Preliminary experiments on synthetic data demonstrate the method's potential, with future work planned for more complex applications.","sentences":["As machine learning systems become increasingly integrated into real-world applications, accurately representing uncertainty is crucial for enhancing their safety, robustness, and reliability.","Training neural networks to fit high-dimensional probability distributions via maximum likelihood has become an effective method for uncertainty quantification.","However, such models often exhibit poor calibration, leading to overconfident predictions.","Traditional metrics like Expected Calibration Error (ECE) and Negative Log Likelihood (NLL) have limitations, including biases and parametric assumptions.","This paper proposes a new approach using conditional kernel mean embeddings to measure calibration discrepancies without these biases and assumptions.","Preliminary experiments on synthetic data demonstrate the method's potential, with future work planned for more complex applications."],"url":"http://arxiv.org/abs/2405.12412v1","category":"cs.LG"}
{"created":"2024-05-20 23:12:55","title":"Flexible Active Safety Motion Control for Robotic Obstacle Avoidance: A CBF-Guided MPC Approach","abstract":"A flexible active safety motion (FASM) control approach is proposed for the avoidance of dynamic obstacles and the reference tracking in robot manipulators. The distinctive feature of the proposed method lies in its utilization of control barrier functions (CBF) to design flexible CBF-guided safety criteria (CBFSC) with dynamically optimized decay rates, thereby offering flexibility and active safety for robot manipulators in dynamic environments. First, discrete-time CBFs are employed to formulate the novel flexible CBFSC with dynamic decay rates for robot manipulators. Following that, the model predictive control (MPC) philosophy is applied, integrating flexible CBFSC as safety constraints into the receding-horizon optimization problem. Significantly, the decay rates of the designed CBFSC are incorporated as decision variables in the optimization problem, facilitating the dynamic enhancement of flexibility during the obstacle avoidance process. In particular, a novel cost function that integrates a penalty term is designed to dynamically adjust the safety margins of the CBFSC. Finally, experiments are conducted in various scenarios using a Universal Robots 5 (UR5) manipulator to validate the effectiveness of the proposed approach.","sentences":["A flexible active safety motion (FASM) control approach is proposed for the avoidance of dynamic obstacles and the reference tracking in robot manipulators.","The distinctive feature of the proposed method lies in its utilization of control barrier functions (CBF) to design flexible CBF-guided safety criteria (CBFSC) with dynamically optimized decay rates, thereby offering flexibility and active safety for robot manipulators in dynamic environments.","First, discrete-time CBFs are employed to formulate the novel flexible CBFSC with dynamic decay rates for robot manipulators.","Following that, the model predictive control (MPC) philosophy is applied, integrating flexible CBFSC as safety constraints into the receding-horizon optimization problem.","Significantly, the decay rates of the designed CBFSC are incorporated as decision variables in the optimization problem, facilitating the dynamic enhancement of flexibility during the obstacle avoidance process.","In particular, a novel cost function that integrates a penalty term is designed to dynamically adjust the safety margins of the CBFSC.","Finally, experiments are conducted in various scenarios using a Universal Robots 5 (UR5) manipulator to validate the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2405.12408v1","category":"cs.RO"}
{"created":"2024-05-20 22:56:40","title":"Unifying Monopole and Center Vortex as the Semiclassical Confinement Mechanism","abstract":"Magnetic excitations play a crucial role in understanding the color confinement of $4$d Yang-Mills theory, and we have the monopole and the center vortex as plausible candidates to explain its mechanism. Under suitable compactified setups of $4$d Yang-Mills theory, we can achieve different weakly-coupled descriptions of confinement phenomena: The monopole mechanism takes place on $\\mathbb{R}^3\\times S^1$ with the double-trace deformation, and the center-vortex mechanism is effective on $\\mathbb{R}^2\\times T^2$ with the 't Hooft flux. We unify these two semiclassical descriptions by showing the explicit relation between the monopole and center vortex.","sentences":["Magnetic excitations play a crucial role in understanding the color confinement of $4$d Yang-Mills theory, and we have the monopole and the center vortex as plausible candidates to explain its mechanism.","Under suitable compactified setups of $4$d Yang-Mills theory, we can achieve different weakly-coupled descriptions of confinement phenomena: The monopole mechanism takes place on $\\mathbb{R}^3\\times S^1$ with the double-trace deformation, and the center-vortex mechanism is effective on $\\mathbb{R}^2\\times T^2$ with the 't Hooft flux.","We unify these two semiclassical descriptions by showing the explicit relation between the monopole and center vortex."],"url":"http://arxiv.org/abs/2405.12402v1","category":"hep-th"}
{"created":"2024-05-20 22:53:28","title":"Probing $|V_{td}|$ in single-top production at $pe^{\\mp}$ colliders","abstract":"We study the consequences for top-quark physics of having electron and positron beams available at the LHeC and FCC-he, as was the case in HERA. We show that the asymmetry between top production in $pe^+$ collisions and antitop production in $pe^-$ reactions is sensitive to $|V_{td}|$. By means of detailed parton-level Monte Carlo simulations of single $t$ and $\\bar{t}$ production and its backgrounds, we parametrize the asymmetry dependence on $|V_{td}|$ and estimate its uncertainties. We thus obtain limits on $|V_{td}|$ that are substantially stronger than current ones, and also smaller than current projections for the HL-LHC. We have $|V_{td}| < 1.6\\times |V_{td}^\\mathrm{PDG}|$ at the LHeC, at 68\\% C.L.\\ with $L_\\mathrm{int}=2$/ab.","sentences":["We study the consequences for top-quark physics of having electron and positron beams available at the LHeC and FCC-he, as was the case in HERA.","We show that the asymmetry between top production in $pe^+$ collisions and antitop production in $pe^-$ reactions is sensitive to $|V_{td}|$. By means of detailed parton-level Monte Carlo simulations of single $t$ and $\\bar{t}$ production and its backgrounds, we parametrize the asymmetry dependence on $|V_{td}|$ and estimate its uncertainties.","We thus obtain limits on $|V_{td}|$ that are substantially stronger than current ones, and also smaller than current projections for the HL-LHC.","We have $|V_{td}| < 1.6\\times |V_{td}^\\mathrm{PDG}|$ at the LHeC, at 68\\% C.L.\\ with $L_\\mathrm{int}=2$/ab."],"url":"http://arxiv.org/abs/2405.12400v1","category":"hep-ph"}
{"created":"2024-05-20 21:43:43","title":"Conformal Counterfactual Inference under Hidden Confounding","abstract":"Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes scenarios. Predicting potential outcomes along with its uncertainty in a counterfactual world poses the foundamental challenge in causal inference. Existing methods that construct confidence intervals for counterfactuals either rely on the assumption of strong ignorability, or need access to un-identifiable lower and upper bounds that characterize the difference between observational and interventional distributions. To overcome these limitations, we first propose a novel approach wTCP-DR based on transductive weighted conformal prediction, which provides confidence intervals for counterfactual outcomes with marginal converage guarantees, even under hidden confounding. With less restrictive assumptions, our approach requires access to a fraction of interventional data (from randomized controlled trials) to account for the covariate shift from observational distributoin to interventional distribution. Theoretical results explicitly demonstrate the conditions under which our algorithm is strictly advantageous to the naive method that only uses interventional data. After ensuring valid intervals on counterfactuals, it is straightforward to construct intervals for individual treatment effects (ITEs). We demonstrate our method across synthetic and real-world data, including recommendation systems, to verify the superiority of our methods compared against state-of-the-art baselines in terms of both coverage and efficiency","sentences":["Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes scenarios.","Predicting potential outcomes along with its uncertainty in a counterfactual world poses the foundamental challenge in causal inference.","Existing methods that construct confidence intervals for counterfactuals either rely on the assumption of strong ignorability, or need access to un-identifiable lower and upper bounds that characterize the difference between observational and interventional distributions.","To overcome these limitations, we first propose a novel approach wTCP-DR based on transductive weighted conformal prediction, which provides confidence intervals for counterfactual outcomes with marginal converage guarantees, even under hidden confounding.","With less restrictive assumptions, our approach requires access to a fraction of interventional data (from randomized controlled trials) to account for the covariate shift from observational distributoin to interventional distribution.","Theoretical results explicitly demonstrate the conditions under which our algorithm is strictly advantageous to the naive method that only uses interventional data.","After ensuring valid intervals on counterfactuals, it is straightforward to construct intervals for individual treatment effects (ITEs).","We demonstrate our method across synthetic and real-world data, including recommendation systems, to verify the superiority of our methods compared against state-of-the-art baselines in terms of both coverage and efficiency"],"url":"http://arxiv.org/abs/2405.12387v1","category":"cs.LG"}
{"created":"2024-05-20 21:26:00","title":"Stochastic Reservoir Computers","abstract":"Reservoir computing is a form of machine learning that utilizes nonlinear dynamical systems to perform complex tasks in a cost-effective manner when compared to typical neural networks. Many recent advancements in reservoir computing, in particular quantum reservoir computing, make use of reservoirs that are inherently stochastic. However, the theoretical justification for using these systems has not yet been well established. In this paper, we investigate the universality of stochastic reservoir computers, in which we use a stochastic system for reservoir computing using the probabilities of each reservoir state as the readout instead of the states themselves. In stochastic reservoir computing, the number of distinct states of the entire reservoir computer can potentially scale exponentially with the size of the reservoir hardware, offering the advantage of compact device size. We prove that classes of stochastic echo state networks, and therefore the class of all stochastic reservoir computers, are universal approximating classes. We also investigate the performance of two practical examples of stochastic reservoir computers in classification and chaotic time series prediction. While shot noise is a limiting factor in the performance of stochastic reservoir computing, we show significantly improved performance compared to a deterministic reservoir computer with similar hardware in cases where the effects of noise are small.","sentences":["Reservoir computing is a form of machine learning that utilizes nonlinear dynamical systems to perform complex tasks in a cost-effective manner when compared to typical neural networks.","Many recent advancements in reservoir computing, in particular quantum reservoir computing, make use of reservoirs that are inherently stochastic.","However, the theoretical justification for using these systems has not yet been well established.","In this paper, we investigate the universality of stochastic reservoir computers, in which we use a stochastic system for reservoir computing using the probabilities of each reservoir state as the readout instead of the states themselves.","In stochastic reservoir computing, the number of distinct states of the entire reservoir computer can potentially scale exponentially with the size of the reservoir hardware, offering the advantage of compact device size.","We prove that classes of stochastic echo state networks, and therefore the class of all stochastic reservoir computers, are universal approximating classes.","We also investigate the performance of two practical examples of stochastic reservoir computers in classification and chaotic time series prediction.","While shot noise is a limiting factor in the performance of stochastic reservoir computing, we show significantly improved performance compared to a deterministic reservoir computer with similar hardware in cases where the effects of noise are small."],"url":"http://arxiv.org/abs/2405.12382v1","category":"cs.LG"}
{"created":"2024-05-20 21:06:30","title":"A silicon photonics waveguide-coupled colloidal quantum dot photodiode sensitive beyond 1.6 um","abstract":"Silicon photonics faces a persistent challenge in extending photodetection capabilities beyond the 1.6 um wavelength range, primarily due to the lack of appropriate epitaxial materials. Colloidal quantum dots (QDs) present a promising solution here, offering distinct advantages such as infrared wavelength tunability, cost-effectiveness, and facile deposition. Their unique properties position them as a potential candidate for enabling photodetection in silicon photonics beyond the conventional telecom wavelength, thereby expanding the potential applications and capabilities within this domain. In this study, we have successfully integrated lead sulfide (PbS) colloidal quantum dot photodiodes (QDPDs) onto silicon waveguides using standard process techniques. The integrated photodiodes exhibit a remarkable responsivity of 1.3 A/W (with an external quantum efficiency of 74.8%) at a wavelength of 2.1 um, a low dark current of only 106 nA and a bandwidth of 1.1 MHz under a -3 V bias. To demonstrate the scalability of our integration approach, we have developed a compact 8-channel spectrometer incorporating an array of QDPDs. This achievement marks a significant step toward realizing a cost-effective photodetector solution for silicon photonics, particularly tailored for a wide range of sensing applications around the 2 um wavelength range.","sentences":["Silicon photonics faces a persistent challenge in extending photodetection capabilities beyond the 1.6 um wavelength range, primarily due to the lack of appropriate epitaxial materials.","Colloidal quantum dots (QDs) present a promising solution here, offering distinct advantages such as infrared wavelength tunability, cost-effectiveness, and facile deposition.","Their unique properties position them as a potential candidate for enabling photodetection in silicon photonics beyond the conventional telecom wavelength, thereby expanding the potential applications and capabilities within this domain.","In this study, we have successfully integrated lead sulfide (PbS) colloidal quantum dot photodiodes (QDPDs) onto silicon waveguides using standard process techniques.","The integrated photodiodes exhibit a remarkable responsivity of 1.3 A/W (with an external quantum efficiency of 74.8%) at a wavelength of 2.1 um, a low dark current of only 106 nA and a bandwidth of 1.1 MHz under a -3 V bias.","To demonstrate the scalability of our integration approach, we have developed a compact 8-channel spectrometer incorporating an array of QDPDs.","This achievement marks a significant step toward realizing a cost-effective photodetector solution for silicon photonics, particularly tailored for a wide range of sensing applications around the 2 um wavelength range."],"url":"http://arxiv.org/abs/2405.12376v1","category":"physics.optics"}
{"created":"2024-05-20 20:21:23","title":"A New Asteroseismic $\\textit{Kepler}$ Benchmark Constrains the Onset of Weakened Magnetic Braking in Mature Sun-Like Stars","abstract":"Stellar spin down is a critical yet poorly understood component of stellar evolution. In particular, results from the Kepler Mission imply that mature age, solar-type stars have inefficient magnetic braking, resulting in a stalled spin down rate. However, a large number of precise asteroseismic ages are needed for mature ($\\geq$ 3Gyr) stars in order to probe the regime where traditional and stalled spin-down models differ. In this paper, we present a new asteroseismic benchmark star for gyrochronology discovered using reprocessed Kepler short cadence data. KIC 11029516 (Papayu) is a bright ($K_{p}$ = 9.6 mag) solar-type star with well-measured rotation period (21.1$\\pm$0.8 days) from spot modulation using 4 years of Kepler long cadence data. We combine asteroseismology and spectroscopy to obtain $T_{eff}=5888\\pm100$ K, $\\rm{[Fe/H]} = 0.30 \\pm 0.06\\,$ dex, $M = 1.24 \\pm 0.05 M_{\\odot}$, $R = 1.34 \\pm 0.02 R_{\\odot}$ and age of 4.0 $\\pm$ 0.4 Gyr, making Papayu one of the most similar stars to the Sun in terms of temperature and radius with an asteroseismic age and a rotation period measured from spot modulation. We find that Papayu sits at the transition of where traditional and weakened spin-down models diverge. A comparison with stars of similar zero-age main-sequence temperatures supports previous findings that weakened spin-down models are required to explain the ages and rotation periods of old solar-type stars.","sentences":["Stellar spin down is a critical yet poorly understood component of stellar evolution.","In particular, results from the Kepler Mission imply that mature age, solar-type stars have inefficient magnetic braking, resulting in a stalled spin down rate.","However, a large number of precise asteroseismic ages are needed for mature ($\\geq$ 3Gyr) stars in order to probe the regime where traditional and stalled spin-down models differ.","In this paper, we present a new asteroseismic benchmark star for gyrochronology discovered using reprocessed Kepler short cadence data.","KIC 11029516 (Papayu) is a bright ($K_{p}$ = 9.6 mag) solar-type star with well-measured rotation period (21.1$\\pm$0.8 days) from spot modulation using 4 years of Kepler long cadence data.","We combine asteroseismology and spectroscopy to obtain $T_{eff}=5888\\pm100$ K, $\\rm{[Fe/H]} = 0.30 \\pm 0.06\\,$ dex, $M = 1.24 \\pm 0.05 M_{\\odot}$, $R = 1.34 \\pm 0.02 R_{\\odot}$ and age of 4.0 $\\pm$ 0.4 Gyr, making Papayu one of the most similar stars to the Sun in terms of temperature and radius with an asteroseismic age and a rotation period measured from spot modulation.","We find that Papayu sits at the transition of where traditional and weakened spin-down models diverge.","A comparison with stars of similar zero-age main-sequence temperatures supports previous findings that weakened spin-down models are required to explain the ages and rotation periods of old solar-type stars."],"url":"http://arxiv.org/abs/2405.12362v1","category":"astro-ph.SR"}
{"created":"2024-05-20 20:14:23","title":"Paired Conditional Generative Adversarial Network for Highly Accelerated Liver 4D MRI","abstract":"Purpose: 4D MRI with high spatiotemporal resolution is desired for image-guided liver radiotherapy. Acquiring densely sampling k-space data is time-consuming. Accelerated acquisition with sparse samples is desirable but often causes degraded image quality or long reconstruction time. We propose the Reconstruct Paired Conditional Generative Adversarial Network (Re-Con-GAN) to shorten the 4D MRI reconstruction time while maintaining the reconstruction quality.   Methods: Patients who underwent free-breathing liver 4D MRI were included in the study. Fully- and retrospectively under-sampled data at 3, 6 and 10 times (3x, 6x and 10x) were first reconstructed using the nuFFT algorithm. Re-Con-GAN then trained input and output in pairs. Three types of networks, ResNet9, UNet and reconstruction swin transformer, were explored as generators. PatchGAN was selected as the discriminator. Re-Con-GAN processed the data (3D+t) as temporal slices (2D+t). A total of 48 patients with 12332 temporal slices were split into training (37 patients with 10721 slices) and test (11 patients with 1611 slices).   Results: Re-Con-GAN consistently achieved comparable/better PSNR, SSIM, and RMSE scores compared to CS/UNet models. The inference time of Re-Con-GAN, UNet and CS are 0.15s, 0.16s, and 120s. The GTV detection task showed that Re-Con-GAN and CS, compared to UNet, better improved the dice score (3x Re-Con-GAN 80.98%; 3x CS 80.74%; 3x UNet 79.88%) of unprocessed under-sampled images (3x 69.61%).   Conclusion: A generative network with adversarial training is proposed with promising and efficient reconstruction results demonstrated on an in-house dataset. The rapid and qualitative reconstruction of 4D liver MR has the potential to facilitate online adaptive MR-guided radiotherapy for liver cancer.","sentences":["Purpose: 4D MRI with high spatiotemporal resolution is desired for image-guided liver radiotherapy.","Acquiring densely sampling k-space data is time-consuming.","Accelerated acquisition with sparse samples is desirable but often causes degraded image quality or long reconstruction time.","We propose the Reconstruct Paired Conditional Generative Adversarial Network (Re-Con-GAN) to shorten the 4D MRI reconstruction time while maintaining the reconstruction quality.   ","Methods: Patients who underwent free-breathing liver 4D MRI were included in the study.","Fully- and retrospectively under-sampled data at 3, 6 and 10 times (3x, 6x and 10x) were first reconstructed using the nuFFT algorithm.","Re-Con-GAN then trained input and output in pairs.","Three types of networks, ResNet9, UNet and reconstruction swin transformer, were explored as generators.","PatchGAN was selected as the discriminator.","Re-Con-GAN processed the data (3D+t) as temporal slices (2D+t).","A total of 48 patients with 12332 temporal slices were split into training (37 patients with 10721 slices) and test (11 patients with 1611 slices).   ","Results: Re-Con-GAN consistently achieved comparable/better PSNR, SSIM, and RMSE scores compared to CS/UNet models.","The inference time of Re-Con-GAN, UNet and CS are 0.15s, 0.16s, and 120s.","The GTV detection task showed that Re-Con-GAN and CS, compared to UNet, better improved the dice score (3x Re-Con-GAN 80.98%; 3x CS 80.74%; 3x UNet 79.88%) of unprocessed under-sampled images (3x 69.61%).   ","Conclusion: A generative network with adversarial training is proposed with promising and efficient reconstruction results demonstrated on an in-house dataset.","The rapid and qualitative reconstruction of 4D liver MR has the potential to facilitate online adaptive MR-guided radiotherapy for liver cancer."],"url":"http://arxiv.org/abs/2405.12357v1","category":"eess.IV"}
{"created":"2024-05-20 19:59:25","title":"A framework for extraction and transformation of documents","abstract":"We present a theoretical framework for the extraction and transformation of text documents. We propose to use a two-phase process where the first phase extracts span-tuples from a document, and the second phase maps the content of the span-tuples into new documents. We base the extraction phase on the framework of document spanners and the transformation phase on the theory of polyregular functions, the class of regular string-to-string functions with polynomial growth.   For supporting practical extract-transform scenarios, we propose an extension of document spanners described by regex formulas from span-tuples to so-called multispan-tuples, where variables are mapped to sets of spans. We prove that this extension, called regex multispanners, has the same desirable properties as standard spanners described by regex formulas. In our framework, an Extract-Transform (ET) program is given by a regex multispanner followed by a polyregular function.   In this paper, we study the expressibility and evaluation problem of ET programs when the transformation function is linear, called linear ET programs. We show that linear ET programs are equally expressive as non-deterministic streaming string transducers under bag semantics. Moreover, we show that linear ET programs are closed under composition. Finally, we present an enumeration algorithm for evaluating every linear ET program over a document with linear time preprocessing and constant delay.","sentences":["We present a theoretical framework for the extraction and transformation of text documents.","We propose to use a two-phase process where the first phase extracts span-tuples from a document, and the second phase maps the content of the span-tuples into new documents.","We base the extraction phase on the framework of document spanners and the transformation phase on the theory of polyregular functions, the class of regular string-to-string functions with polynomial growth.   ","For supporting practical extract-transform scenarios, we propose an extension of document spanners described by regex formulas from span-tuples to so-called multispan-tuples, where variables are mapped to sets of spans.","We prove that this extension, called regex multispanners, has the same desirable properties as standard spanners described by regex formulas.","In our framework, an Extract-Transform (ET) program is given by a regex multispanner followed by a polyregular function.   ","In this paper, we study the expressibility and evaluation problem of ET programs when the transformation function is linear, called linear ET programs.","We show that linear ET programs are equally expressive as non-deterministic streaming string transducers under bag semantics.","Moreover, we show that linear ET programs are closed under composition.","Finally, we present an enumeration algorithm for evaluating every linear ET program over a document with linear time preprocessing and constant delay."],"url":"http://arxiv.org/abs/2405.12350v1","category":"cs.DB"}
{"created":"2024-05-20 19:42:59","title":"Ultrafast Structured Light through Nonlinear Frequency Generation in an Optical Enhancement Cavity","abstract":"The generation of shaped laser beams, or structured light, is of interest in a wide range of fields, from microscopy to fundamental physics. There are several ways to make shaped beams, most commonly using spatial light modulators comprised of pixels of liquid crystals. These methods have limitations on the wavelength, pulse duration, and average power that can be used. Here we present a method to generate shaped light that can be used at any wavelength from the UV to IR, on ultrafast pulses, and a large range of optical powers. By exploiting the frequency difference between higher order modes, a result of the Gouy phase, and cavity mode matching, we can selectively couple into a variety of pure and composite higher order modes. Optical cavities are used as a spatial filter and then combined with sum frequency generation in a nonlinear crystal as the output coupler to the cavity to create ultrafast, frequency comb structured light.","sentences":["The generation of shaped laser beams, or structured light, is of interest in a wide range of fields, from microscopy to fundamental physics.","There are several ways to make shaped beams, most commonly using spatial light modulators comprised of pixels of liquid crystals.","These methods have limitations on the wavelength, pulse duration, and average power that can be used.","Here we present a method to generate shaped light that can be used at any wavelength from the UV to IR, on ultrafast pulses, and a large range of optical powers.","By exploiting the frequency difference between higher order modes, a result of the Gouy phase, and cavity mode matching, we can selectively couple into a variety of pure and composite higher order modes.","Optical cavities are used as a spatial filter and then combined with sum frequency generation in a nonlinear crystal as the output coupler to the cavity to create ultrafast, frequency comb structured light."],"url":"http://arxiv.org/abs/2405.12346v1","category":"physics.optics"}
{"created":"2024-05-20 19:29:48","title":"Probabilistic Eddy Identification with Uncertainty Quantification","abstract":"Mesoscale eddies are critical in ocean circulation and the global climate system. Standard eddy identification methods are usually based on deterministic optimal point estimates of the ocean flow field, which produce a single best estimate without accounting for inherent uncertainties in the data. However, large uncertainty exists in estimating the flow field due to the use of noisy, sparse, and indirect observations, as well as turbulent flow models. When this uncertainty is overlooked, the accuracy of eddy identification is significantly affected. This paper presents a general probabilistic eddy identification framework that adapts existing methods to incorporate uncertainty into the diagnostic. The framework begins by sampling an ensemble of ocean realizations from the probabilistic state estimation, which considers uncertainty. Traditional eddy diagnostics are then applied to individual realizations, and the corresponding eddy statistics are aggregated from these diagnostic results. The framework is applied to a scenario mimicking the Beaufort Gyre marginal ice zone, where large uncertainty appears in estimating the ocean field using Lagrangian data assimilation with sparse ice floe trajectories. The probabilistic eddy identification precisely characterizes the contribution from the turbulent fluctuations and provides additional insights in comparison to its deterministic counterpart. The skills in counting the number of eddies and computing the probability of the eddy are both significantly improved under the probabilistic framework. Notably, large biases appear when estimating the eddy lifetime using deterministic methods. In contrast, probabilistic eddy identification not only provides a closer estimate but also quantifies the uncertainty in inferring such a crucial dynamical quantity.","sentences":["Mesoscale eddies are critical in ocean circulation and the global climate system.","Standard eddy identification methods are usually based on deterministic optimal point estimates of the ocean flow field, which produce a single best estimate without accounting for inherent uncertainties in the data.","However, large uncertainty exists in estimating the flow field due to the use of noisy, sparse, and indirect observations, as well as turbulent flow models.","When this uncertainty is overlooked, the accuracy of eddy identification is significantly affected.","This paper presents a general probabilistic eddy identification framework that adapts existing methods to incorporate uncertainty into the diagnostic.","The framework begins by sampling an ensemble of ocean realizations from the probabilistic state estimation, which considers uncertainty.","Traditional eddy diagnostics are then applied to individual realizations, and the corresponding eddy statistics are aggregated from these diagnostic results.","The framework is applied to a scenario mimicking the Beaufort Gyre marginal ice zone, where large uncertainty appears in estimating the ocean field using Lagrangian data assimilation with sparse ice floe trajectories.","The probabilistic eddy identification precisely characterizes the contribution from the turbulent fluctuations and provides additional insights in comparison to its deterministic counterpart.","The skills in counting the number of eddies and computing the probability of the eddy are both significantly improved under the probabilistic framework.","Notably, large biases appear when estimating the eddy lifetime using deterministic methods.","In contrast, probabilistic eddy identification not only provides a closer estimate but also quantifies the uncertainty in inferring such a crucial dynamical quantity."],"url":"http://arxiv.org/abs/2405.12342v1","category":"math.DS"}
{"created":"2024-05-20 19:24:10","title":"Cascade-based Randomization for Inferring Causal Effects under Diffusion Interference","abstract":"The presence of interference, where the outcome of an individual may depend on the treatment assignment and behavior of neighboring nodes, can lead to biased causal effect estimation. Current approaches to network experiment design focus on limiting interference through cluster-based randomization, in which clusters are identified using graph clustering, and cluster randomization dictates the node assignment to treatment and control. However, cluster-based randomization approaches perform poorly when interference propagates in cascades, whereby the response of individuals to treatment propagates to their multi-hop neighbors. When we have knowledge of the cascade seed nodes, we can leverage this interference structure to mitigate the resulting causal effect estimation bias. With this goal, we propose a cascade-based network experiment design that initiates treatment assignment from the cascade seed node and propagates the assignment to their multi-hop neighbors to limit interference during cascade growth and thereby reduce the overall causal effect estimation error. Our extensive experiments on real-world and synthetic datasets demonstrate that our proposed framework outperforms the existing state-of-the-art approaches in estimating causal effects in network data.","sentences":["The presence of interference, where the outcome of an individual may depend on the treatment assignment and behavior of neighboring nodes, can lead to biased causal effect estimation.","Current approaches to network experiment design focus on limiting interference through cluster-based randomization, in which clusters are identified using graph clustering, and cluster randomization dictates the node assignment to treatment and control.","However, cluster-based randomization approaches perform poorly when interference propagates in cascades, whereby the response of individuals to treatment propagates to their multi-hop neighbors.","When we have knowledge of the cascade seed nodes, we can leverage this interference structure to mitigate the resulting causal effect estimation bias.","With this goal, we propose a cascade-based network experiment design that initiates treatment assignment from the cascade seed node and propagates the assignment to their multi-hop neighbors to limit interference during cascade growth and thereby reduce the overall causal effect estimation error.","Our extensive experiments on real-world and synthetic datasets demonstrate that our proposed framework outperforms the existing state-of-the-art approaches in estimating causal effects in network data."],"url":"http://arxiv.org/abs/2405.12340v1","category":"cs.LG"}
{"created":"2024-05-20 19:08:19","title":"Close Binary Fractions in accreted and in-situ Halo Stars","abstract":"The study of binary stars in the Galactic halo provides crucial insights into the dynamical history and formation processes of the Milky Way. In this work, we aim to investigate the binary fraction in a sample of accreted and in-situ halo stars, focusing on short-period binaries. Utilising data from Gaia DR3, we analysed the radial velocity (RV) uncertainty $\\sigma_{\\mathrm{RV}}$ distribution of a sample of main-sequence stars. We used a novel Bayesian framework to model the dependence in $\\sigma_{\\mathrm{RV}}$ of single and binary systems allowing us to estimate binary fractions $F$ in a sample of bright ($G_{\\mathrm{RVS}}$ < 12) Gaia sources. We selected the samples of in-situ and accreted halo stars based on estimating the 6D phase space information and affiliating the stars to the different samples on an action-angle vs energy ($L_{\\mathrm{z}}-E$) diagram. Our results indicate a higher, though not significant, binary fraction in accreted stars compared to the in-situ halo sample. We further explore binary fractions using cuts in $E$ and $L_z$ and find a higher binary fraction in both high-energy and prograde orbits that might be explained by differences in metallicity. By cross-matching our Gaia sample with APOGEE DR17 catalogue, we confirm the results of previous studies on higher binary fractions in metal-poor stars and find the fractions of accreted and in-situ halo stars consistent with this trend. Our finding provides new insights into binary stars' formation processes and dynamical evolution in the primordial Milky Way Galaxy and its accreted dwarf Galaxies.","sentences":["The study of binary stars in the Galactic halo provides crucial insights into the dynamical history and formation processes of the Milky Way.","In this work, we aim to investigate the binary fraction in a sample of accreted and in-situ halo stars, focusing on short-period binaries.","Utilising data from Gaia DR3, we analysed the radial velocity (RV) uncertainty $\\sigma_{\\mathrm{RV}}$ distribution of a sample of main-sequence stars.","We used a novel Bayesian framework to model the dependence in $\\sigma_{\\mathrm{RV}}$ of single and binary systems allowing us to estimate binary fractions $F$ in a sample of bright ($G_{\\mathrm{RVS}}$ < 12) Gaia sources.","We selected the samples of in-situ and accreted halo stars based on estimating the 6D phase space information and affiliating the stars to the different samples on an action-angle vs energy ($L_{\\mathrm{z}}-E$) diagram.","Our results indicate a higher, though not significant, binary fraction in accreted stars compared to the in-situ halo sample.","We further explore binary fractions using cuts in $E$ and $L_z$ and find a higher binary fraction in both high-energy and prograde orbits that might be explained by differences in metallicity.","By cross-matching our Gaia sample with APOGEE DR17 catalogue, we confirm the results of previous studies on higher binary fractions in metal-poor stars and find the fractions of accreted and in-situ halo stars consistent with this trend.","Our finding provides new insights into binary stars' formation processes and dynamical evolution in the primordial Milky Way Galaxy and its accreted dwarf Galaxies."],"url":"http://arxiv.org/abs/2405.12335v1","category":"astro-ph.GA"}
{"created":"2024-05-20 19:05:32","title":"Efficacy of static analysis tools for software defect detection on open-source projects","abstract":"In software practice, static analysis tools remain an integral part of detecting defects in software and there have been various tools designed to run the analysis in different programming languages like Java, C++, and Python. This paper presents an empirical comparison of popular static analysis tools for identifying software defects using several datasets using Java, C++, and Python code. The study used popular analysis tools such as SonarQube, PMD, Checkstyle, and FindBugs to perform the comparison based on using the datasets. The study also used various evaluation metrics such as Precision, Recall, and F1-score to determine the performance of each analysis tool. The study results show that SonarQube performs considerably well than all other tools in terms of its defect detection across the various three programming languages. These findings remain consistent with other existing studies that also agree on SonarQube being an effective tool for defect detection in software. The study contributes to much insight on static analysis tools with different programming languages and additional information to understand the strengths and weaknesses of each analysis tool. The study also discusses the implications for software development researchers and practitioners, and future directions in this area. Our research approach aim is to provide a recommendation guideline to enable software developers, practitioners, and researchers to make the right choice on static analysis tools to detect errors in their software codes. Also, for researchers to embark on investigating and improving software analysis tools to enhance the quality and reliability of the software systems and its software development processes practice.","sentences":["In software practice, static analysis tools remain an integral part of detecting defects in software and there have been various tools designed to run the analysis in different programming languages like Java, C++, and Python.","This paper presents an empirical comparison of popular static analysis tools for identifying software defects using several datasets using Java, C++, and Python code.","The study used popular analysis tools such as SonarQube, PMD, Checkstyle, and FindBugs to perform the comparison based on using the datasets.","The study also used various evaluation metrics such as Precision, Recall, and F1-score to determine the performance of each analysis tool.","The study results show that SonarQube performs considerably well than all other tools in terms of its defect detection across the various three programming languages.","These findings remain consistent with other existing studies that also agree on SonarQube being an effective tool for defect detection in software.","The study contributes to much insight on static analysis tools with different programming languages and additional information to understand the strengths and weaknesses of each analysis tool.","The study also discusses the implications for software development researchers and practitioners, and future directions in this area.","Our research approach aim is to provide a recommendation guideline to enable software developers, practitioners, and researchers to make the right choice on static analysis tools to detect errors in their software codes.","Also, for researchers to embark on investigating and improving software analysis tools to enhance the quality and reliability of the software systems and its software development processes practice."],"url":"http://arxiv.org/abs/2405.12333v1","category":"cs.SE"}
{"created":"2024-05-20 18:52:33","title":"Diversifying by Intent in Recommender Systems","abstract":"It has become increasingly clear that recommender systems overly focusing on short-term engagement can inadvertently hurt long-term user experience. However, it is challenging to optimize long-term user experience directly as the desired signal is sparse, noisy and manifests over a long horizon. In this work, we show the benefits of incorporating higher-level user understanding, specifically user intents that can persist across multiple interactions or recommendation sessions, for whole-page recommendation toward optimizing long-term user experience. User intent has primarily been investigated within the context of search, but remains largely under-explored for recommender systems. To bridge this gap, we develop a probabilistic intent-based whole-page diversification framework in the final stage of a recommender system. Starting with a prior belief of user intents, the proposed diversification framework sequentially selects items at each position based on these beliefs, and subsequently updates posterior beliefs about the intents. It ensures that different user intents are represented in a page towards optimizing long-term user experience.   We experiment with the intent diversification framework on one of the world's largest content recommendation platforms, serving billions of users daily. Our framework incorporates the user's exploration intent, capturing their propensity to explore new interests and content. Live experiments show that the proposed framework leads to an increase in user retention and overall user enjoyment, validating its effectiveness in facilitating long-term planning. In particular, it enables users to consistently discover and engage with diverse contents that align with their underlying intents over time, thereby leading to an improved long-term user experience.","sentences":["It has become increasingly clear that recommender systems overly focusing on short-term engagement can inadvertently hurt long-term user experience.","However, it is challenging to optimize long-term user experience directly as the desired signal is sparse, noisy and manifests over a long horizon.","In this work, we show the benefits of incorporating higher-level user understanding, specifically user intents that can persist across multiple interactions or recommendation sessions, for whole-page recommendation toward optimizing long-term user experience.","User intent has primarily been investigated within the context of search, but remains largely under-explored for recommender systems.","To bridge this gap, we develop a probabilistic intent-based whole-page diversification framework in the final stage of a recommender system.","Starting with a prior belief of user intents, the proposed diversification framework sequentially selects items at each position based on these beliefs, and subsequently updates posterior beliefs about the intents.","It ensures that different user intents are represented in a page towards optimizing long-term user experience.   ","We experiment with the intent diversification framework on one of the world's largest content recommendation platforms, serving billions of users daily.","Our framework incorporates the user's exploration intent, capturing their propensity to explore new interests and content.","Live experiments show that the proposed framework leads to an increase in user retention and overall user enjoyment, validating its effectiveness in facilitating long-term planning.","In particular, it enables users to consistently discover and engage with diverse contents that align with their underlying intents over time, thereby leading to an improved long-term user experience."],"url":"http://arxiv.org/abs/2405.12327v1","category":"cs.IR"}
{"created":"2024-05-20 18:50:15","title":"A canonical polyadic tensor basis for fast Bayesian estimation of multi-subject fMRI activation patterns","abstract":"Task-evoked functional magnetic resonance imaging studies, such as the Human Connectome Project (HCP), are a powerful tool for exploring how brain activity is influenced by cognitive tasks like memory retention, decision-making, and language processing. A fast Bayesian function-on-scalar model is proposed for estimating population-level activation maps linked to the working memory task. The model is based on the canonical polyadic (CP) tensor decomposition of coefficient maps obtained for each subject. This decomposition effectively yields a tensor basis capable of extracting both common features and subject-specific features from the coefficient maps. These subject-specific features, in turn, are modeled as a function of covariates of interest using a Bayesian model that accounts for the correlation of the CP-extracted features. The dimensionality reduction achieved with the tensor basis allows for a fast MCMC estimation of population-level activation maps. This model is applied to one hundred unrelated subjects from the HCP dataset, yielding significant insights into brain signatures associated with working memory.","sentences":["Task-evoked functional magnetic resonance imaging studies, such as the Human Connectome Project (HCP), are a powerful tool for exploring how brain activity is influenced by cognitive tasks like memory retention, decision-making, and language processing.","A fast Bayesian function-on-scalar model is proposed for estimating population-level activation maps linked to the working memory task.","The model is based on the canonical polyadic (CP) tensor decomposition of coefficient maps obtained for each subject.","This decomposition effectively yields a tensor basis capable of extracting both common features and subject-specific features from the coefficient maps.","These subject-specific features, in turn, are modeled as a function of covariates of interest using a Bayesian model that accounts for the correlation of the CP-extracted features.","The dimensionality reduction achieved with the tensor basis allows for a fast MCMC estimation of population-level activation maps.","This model is applied to one hundred unrelated subjects from the HCP dataset, yielding significant insights into brain signatures associated with working memory."],"url":"http://arxiv.org/abs/2405.12325v1","category":"stat.AP"}
{"created":"2024-05-20 18:29:53","title":"Dynamic Line Rating using Hyper-local Weather Predictions: A Machine Learning Approach","abstract":"Dynamic Line Rating (DLR) systems are crucial for renewable energy integration in transmission networks. However, traditional methods relying on sensor data face challenges due to the impracticality of installing sensors on every pole or span. Additionally, sensor-based approaches may struggle predicting DLR in rapidly changing weather conditions. This paper proposes a novel approach, leveraging machine learning (ML) techniques alongside hyper-local weather forecast data. Unlike conventional methods, which solely rely on sensor data, this approach utilizes ML models trained to predict hyper-local weather parameters on a full network scale. Integrating topographical data enhances prediction accuracy by accounting for landscape features and obstacles around overhead lines. The paper introduces confidence intervals for DLR assessments to mitigate risks associated with uncertainties. A case study from Estonia demonstrates the practical implementation of the proposed methodology, highlighting its effectiveness in real-world scenarios. By addressing limitations of sensor-based approaches, this research contributes to the discourse of renewable energy integration in transmission systems, advancing efficiency and reliability in the power grid.","sentences":["Dynamic Line Rating (DLR) systems are crucial for renewable energy integration in transmission networks.","However, traditional methods relying on sensor data face challenges due to the impracticality of installing sensors on every pole or span.","Additionally, sensor-based approaches may struggle predicting DLR in rapidly changing weather conditions.","This paper proposes a novel approach, leveraging machine learning (ML) techniques alongside hyper-local weather forecast data.","Unlike conventional methods, which solely rely on sensor data, this approach utilizes ML models trained to predict hyper-local weather parameters on a full network scale.","Integrating topographical data enhances prediction accuracy by accounting for landscape features and obstacles around overhead lines.","The paper introduces confidence intervals for DLR assessments to mitigate risks associated with uncertainties.","A case study from Estonia demonstrates the practical implementation of the proposed methodology, highlighting its effectiveness in real-world scenarios.","By addressing limitations of sensor-based approaches, this research contributes to the discourse of renewable energy integration in transmission systems, advancing efficiency and reliability in the power grid."],"url":"http://arxiv.org/abs/2405.12319v1","category":"cs.LG"}
{"created":"2024-05-20 18:29:36","title":"Kernel spectral joint embeddings for high-dimensional noisy datasets using duo-landmark integral operators","abstract":"Integrative analysis of multiple heterogeneous datasets has become standard practice in many research fields, especially in single-cell genomics and medical informatics. Existing approaches oftentimes suffer from limited power in capturing nonlinear structures, insufficient account of noisiness and effects of high-dimensionality, lack of adaptivity to signals and sample sizes imbalance, and their results are sometimes difficult to interpret. To address these limitations, we propose a novel kernel spectral method that achieves joint embeddings of two independently observed high-dimensional noisy datasets. The proposed method automatically captures and leverages possibly shared low-dimensional structures across datasets to enhance embedding quality. The obtained low-dimensional embeddings can be utilized for many downstream tasks such as simultaneous clustering, data visualization, and denoising. The proposed method is justified by rigorous theoretical analysis. Specifically, we show the consistency of our method in recovering the low-dimensional noiseless signals, and characterize the effects of the signal-to-noise ratios on the rates of convergence. Under a joint manifolds model framework, we establish the convergence of ultimate embeddings to the eigenfunctions of some newly introduced integral operators. These operators, referred to as duo-landmark integral operators, are defined by the convolutional kernel maps of some reproducing kernel Hilbert spaces (RKHSs). These RKHSs capture the either partially or entirely shared underlying low-dimensional nonlinear signal structures of the two datasets. Our numerical experiments and analyses of two single-cell omics datasets demonstrate the empirical advantages of the proposed method over existing methods in both embeddings and several downstream tasks.","sentences":["Integrative analysis of multiple heterogeneous datasets has become standard practice in many research fields, especially in single-cell genomics and medical informatics.","Existing approaches oftentimes suffer from limited power in capturing nonlinear structures, insufficient account of noisiness and effects of high-dimensionality, lack of adaptivity to signals and sample sizes imbalance, and their results are sometimes difficult to interpret.","To address these limitations, we propose a novel kernel spectral method that achieves joint embeddings of two independently observed high-dimensional noisy datasets.","The proposed method automatically captures and leverages possibly shared low-dimensional structures across datasets to enhance embedding quality.","The obtained low-dimensional embeddings can be utilized for many downstream tasks such as simultaneous clustering, data visualization, and denoising.","The proposed method is justified by rigorous theoretical analysis.","Specifically, we show the consistency of our method in recovering the low-dimensional noiseless signals, and characterize the effects of the signal-to-noise ratios on the rates of convergence.","Under a joint manifolds model framework, we establish the convergence of ultimate embeddings to the eigenfunctions of some newly introduced integral operators.","These operators, referred to as duo-landmark integral operators, are defined by the convolutional kernel maps of some reproducing kernel Hilbert spaces (RKHSs).","These RKHSs capture the either partially or entirely shared underlying low-dimensional nonlinear signal structures of the two datasets.","Our numerical experiments and analyses of two single-cell omics datasets demonstrate the empirical advantages of the proposed method over existing methods in both embeddings and several downstream tasks."],"url":"http://arxiv.org/abs/2405.12317v1","category":"stat.ML"}
{"created":"2024-05-20 18:25:31","title":"Holographic vacuum energy regularization and corrected entropy of de Sitter space","abstract":"We propose that the spectrum of the surface area of the apparent horizon (AH) of de Sitter (dS) spacetime leads to corrected temperature and entropy of the dS spacetime, offering new insights into its thermodynamic properties. This is done by employing the spectrum of the AH radius, acquired from the Wheeler--DeWitt (WDW) equation, together with the Stefan--Boltzmann law, the time-energy uncertainty relation, and the unified first law of thermodynamics.","sentences":["We propose that the spectrum of the surface area of the apparent horizon (AH) of de Sitter (dS) spacetime leads to corrected temperature and entropy of the dS spacetime, offering new insights into its thermodynamic properties.","This is done by employing the spectrum of the AH radius, acquired from the Wheeler--DeWitt (WDW) equation, together with the Stefan--Boltzmann law, the time-energy uncertainty relation, and the unified first law of thermodynamics."],"url":"http://arxiv.org/abs/2405.12314v1","category":"gr-qc"}
{"created":"2024-05-20 18:15:20","title":"Deep learning-based hyperspectral image reconstruction for quality assessment of agro-product","abstract":"Hyperspectral imaging (HSI) has recently emerged as a promising tool for many agricultural applications; however, the technology cannot be directly used in a real-time system due to the extensive time needed to process large volumes of data. Consequently, the development of a simple, compact, and cost-effective imaging system is not possible with the current HSI systems. Therefore, the overall goal of this study was to reconstruct hyperspectral images from RGB images through deep learning for agricultural applications. Specifically, this study used Hyperspectral Convolutional Neural Network - Dense (HSCNN-D) to reconstruct hyperspectral images from RGB images for predicting soluble solid content (SSC) in sweet potatoes. The algorithm accurately reconstructed the hyperspectral images from RGB images, with the resulting spectra closely matching the ground-truth. The partial least squares regression (PLSR) model based on reconstructed spectra outperformed the model using the full spectral range, demonstrating its potential for SSC prediction in sweet potatoes. These findings highlight the potential of deep learning-based hyperspectral image reconstruction as a low-cost, efficient tool for various agricultural uses.","sentences":["Hyperspectral imaging (HSI) has recently emerged as a promising tool for many agricultural applications; however, the technology cannot be directly used in a real-time system due to the extensive time needed to process large volumes of data.","Consequently, the development of a simple, compact, and cost-effective imaging system is not possible with the current HSI systems.","Therefore, the overall goal of this study was to reconstruct hyperspectral images from RGB images through deep learning for agricultural applications.","Specifically, this study used Hyperspectral Convolutional Neural Network - Dense (HSCNN-D) to reconstruct hyperspectral images from RGB images for predicting soluble solid content (SSC) in sweet potatoes.","The algorithm accurately reconstructed the hyperspectral images from RGB images, with the resulting spectra closely matching the ground-truth.","The partial least squares regression (PLSR) model based on reconstructed spectra outperformed the model using the full spectral range, demonstrating its potential for SSC prediction in sweet potatoes.","These findings highlight the potential of deep learning-based hyperspectral image reconstruction as a low-cost, efficient tool for various agricultural uses."],"url":"http://arxiv.org/abs/2405.12313v1","category":"cs.CV"}
{"created":"2024-05-20 18:14:33","title":"A Principled Approach for a New Bias Measure","abstract":"The widespread use of machine learning and data-driven algorithms for decision making has been steadily increasing over many years. The areas in which this is happening are diverse: healthcare, employment, finance, education, the legal system to name a few; and the associated negative side effects are being increasingly harmful for society. Negative data \\emph{bias} is one of those, which tends to result in harmful consequences for specific groups of people. Any mitigation strategy or effective policy that addresses the negative consequences of bias must start with awareness that bias exists, together with a way to understand and quantify it. However, there is a lack of consensus on how to measure data bias and oftentimes the intended meaning is context dependent and not uniform within the research community. The main contributions of our work are: (1) a general algorithmic framework for defining and efficiently quantifying the bias level of a dataset with respect to a protected group; and (2) the definition of a new bias measure. Our results are experimentally validated using nine publicly available datasets and theoretically analyzed, which provide novel insights about the problem. Based on our approach, we also derive a bias mitigation algorithm that might be useful to policymakers.","sentences":["The widespread use of machine learning and data-driven algorithms for decision making has been steadily increasing over many years.","The areas in which this is happening are diverse: healthcare, employment, finance, education, the legal system to name a few; and the associated negative side effects are being increasingly harmful for society.","Negative data \\emph{bias} is one of those, which tends to result in harmful consequences for specific groups of people.","Any mitigation strategy or effective policy that addresses the negative consequences of bias must start with awareness that bias exists, together with a way to understand and quantify it.","However, there is a lack of consensus on how to measure data bias and oftentimes the intended meaning is context dependent and not uniform within the research community.","The main contributions of our work are: (1) a general algorithmic framework for defining and efficiently quantifying the bias level of a dataset with respect to a protected group; and (2) the definition of a new bias measure.","Our results are experimentally validated using nine publicly available datasets and theoretically analyzed, which provide novel insights about the problem.","Based on our approach, we also derive a bias mitigation algorithm that might be useful to policymakers."],"url":"http://arxiv.org/abs/2405.12312v1","category":"cs.LG"}
{"created":"2024-05-20 18:14:31","title":"Cost-Optimal Microservices Deployment with Cluster Autoscaling and Spot Pricing","abstract":"Microservices architecture has been established as an ideal software architecture for cloud-based software development and deployment, offering many benefits such as agility and efficiency. Microservices are often associated with containers and container orchestration systems for deployment, as containerization provides convenient tools and techniques for resource management, including the automation of orchestration processes. Among the factors that make the cloud suitable for commercial software deployment, transient pricing options like AWS Spot Pricing are particularly attractive as they allow consumers to significantly reduce cloud costs. However, the dynamic nature of resource demand and the abrupt termination of spot VMs make transient pricing challenging. Nonetheless, containerization and container orchestration systems open new avenues to optimize the cost of microservices deployments by leveraging spot pricing on the public cloud while achieving application and business goals.   We propose SpotKube, an open-source, Kubernetes-based, application-aware, genetic algorithm-based solution for cost optimization, which autoscales clusters for microservices-based applications hosted on public clouds with spot pricing options. SpotKube analyzes application characteristics and recommends the optimal configuration for resource allocation to the cluster. It consists of an elastic cluster autoscaler powered by an optimization algorithm that ensures cost-effective microservices deployment while meeting application performance requirements and handling abrupt termination of nodes, thereby minimizing the impact on system availability. We implement and evaluate SpotKube with representative microservices-based applications in a real public cloud setup, demonstrating the effectiveness of our approach against alternative optimization strategies.","sentences":["Microservices architecture has been established as an ideal software architecture for cloud-based software development and deployment, offering many benefits such as agility and efficiency.","Microservices are often associated with containers and container orchestration systems for deployment, as containerization provides convenient tools and techniques for resource management, including the automation of orchestration processes.","Among the factors that make the cloud suitable for commercial software deployment, transient pricing options like AWS Spot Pricing are particularly attractive as they allow consumers to significantly reduce cloud costs.","However, the dynamic nature of resource demand and the abrupt termination of spot VMs make transient pricing challenging.","Nonetheless, containerization and container orchestration systems open new avenues to optimize the cost of microservices deployments by leveraging spot pricing on the public cloud while achieving application and business goals.   ","We propose SpotKube, an open-source, Kubernetes-based, application-aware, genetic algorithm-based solution for cost optimization, which autoscales clusters for microservices-based applications hosted on public clouds with spot pricing options.","SpotKube analyzes application characteristics and recommends the optimal configuration for resource allocation to the cluster.","It consists of an elastic cluster autoscaler powered by an optimization algorithm that ensures cost-effective microservices deployment while meeting application performance requirements and handling abrupt termination of nodes, thereby minimizing the impact on system availability.","We implement and evaluate SpotKube with representative microservices-based applications in a real public cloud setup, demonstrating the effectiveness of our approach against alternative optimization strategies."],"url":"http://arxiv.org/abs/2405.12311v1","category":"cs.DC"}
{"created":"2024-05-20 18:12:36","title":"Continual Deep Reinforcement Learning for Decentralized Satellite Routing","abstract":"This paper introduces a full solution for decentralized routing in Low Earth Orbit satellite constellations based on continual Deep Reinforcement Learning (DRL). This requires addressing multiple challenges, including the partial knowledge at the satellites and their continuous movement, and the time-varying sources of uncertainty in the system, such as traffic, communication links, or communication buffers. We follow a multi-agent approach, where each satellite acts as an independent decision-making agent, while acquiring a limited knowledge of the environment based on the feedback received from the nearby agents. The solution is divided into two phases. First, an offline learning phase relies on decentralized decisions and a global Deep Neural Network (DNN) trained with global experiences. Then, the online phase with local, on-board, and pre-trained DNNs requires continual learning to evolve with the environment, which can be done in two different ways: (1) Model anticipation, where the predictable conditions of the constellation are exploited by each satellite sharing local model with the next satellite; and (2) Federated Learning (FL), where each agent's model is merged first at the cluster level and then aggregated in a global Parameter Server. The results show that, without high congestion, the proposed Multi-Agent DRL framework achieves the same E2E performance as a shortest-path solution, but the latter assumes intensive communication overhead for real-time network-wise knowledge of the system at a centralized node, whereas ours only requires limited feedback exchange among first neighbour satellites. Importantly, our solution adapts well to congestion conditions and exploits less loaded paths. Moreover, the divergence of models over time is easily tackled by the synergy between anticipation, applied in short-term alignment, and FL, utilized for long-term alignment.","sentences":["This paper introduces a full solution for decentralized routing in Low Earth Orbit satellite constellations based on continual Deep Reinforcement Learning (DRL).","This requires addressing multiple challenges, including the partial knowledge at the satellites and their continuous movement, and the time-varying sources of uncertainty in the system, such as traffic, communication links, or communication buffers.","We follow a multi-agent approach, where each satellite acts as an independent decision-making agent, while acquiring a limited knowledge of the environment based on the feedback received from the nearby agents.","The solution is divided into two phases.","First, an offline learning phase relies on decentralized decisions and a global Deep Neural Network (DNN) trained with global experiences.","Then, the online phase with local, on-board, and pre-trained DNNs requires continual learning to evolve with the environment, which can be done in two different ways: (1) Model anticipation, where the predictable conditions of the constellation are exploited by each satellite sharing local model with the next satellite; and (2) Federated Learning (FL), where each agent's model is merged first at the cluster level and then aggregated in a global Parameter Server.","The results show that, without high congestion, the proposed Multi-Agent DRL framework achieves the same E2E performance as a shortest-path solution, but the latter assumes intensive communication overhead for real-time network-wise knowledge of the system at a centralized node, whereas ours only requires limited feedback exchange among first neighbour satellites.","Importantly, our solution adapts well to congestion conditions and exploits less loaded paths.","Moreover, the divergence of models over time is easily tackled by the synergy between anticipation, applied in short-term alignment, and FL, utilized for long-term alignment."],"url":"http://arxiv.org/abs/2405.12308v1","category":"cs.LG"}
{"created":"2024-05-20 18:12:01","title":"Entropy Production by Underdamped Langevin Dynamics","abstract":"Entropy production (EP) is a central quantity in nonequilibrium physics as it monitors energy dissipation, reversibility, and free energy differences during thermodynamic transformations. Estimating EP, however, is challenging both theoretically and experimentally due to limited access to the system dynamics. For overdamped Langevin dynamics and Markov jump processes it was recently proposed that, from thermodynamic uncertainty relations (TUR), short-time cumulant currents can be used to estimate EP without knowledge of the dynamics. Yet, estimation of EP in underdamped Langevin systems remains an active challenge. To address this, we derive a modified TUR that relates the statistics of two specific novel currents -- one cumulant current and one stochastic current -- to a system's EP. These two distinct but related currents are used to constrain EP in the modified TUR. One highlight is that there always exists a family of currents such that the uncertainty relations saturate, even for long-time averages and in nonsteady-state scenarios. Another is that our method only requires limited knowledge of the dynamics -- specifically, the damping-coefficient to mass ratio and the diffusion constant. This uncertainty relation allows estimating EP for both overdamped and underdamped Langevin dynamics. We validate the method numerically, through applications to several underdamped systems, to underscore the flexibility in obtaining EP in nonequilibrium Langevin systems.","sentences":["Entropy production (EP) is a central quantity in nonequilibrium physics as it monitors energy dissipation, reversibility, and free energy differences during thermodynamic transformations.","Estimating EP, however, is challenging both theoretically and experimentally due to limited access to the system dynamics.","For overdamped Langevin dynamics and Markov jump processes it was recently proposed that, from thermodynamic uncertainty relations (TUR), short-time cumulant currents can be used to estimate EP without knowledge of the dynamics.","Yet, estimation of EP in underdamped Langevin systems remains an active challenge.","To address this, we derive a modified TUR that relates the statistics of two specific novel currents -- one cumulant current and one stochastic current -- to a system's EP.","These two distinct but related currents are used to constrain EP in the modified TUR.","One highlight is that there always exists a family of currents such that the uncertainty relations saturate, even for long-time averages and in nonsteady-state scenarios.","Another is that our method only requires limited knowledge of the dynamics -- specifically, the damping-coefficient to mass ratio and the diffusion constant.","This uncertainty relation allows estimating EP for both overdamped and underdamped Langevin dynamics.","We validate the method numerically, through applications to several underdamped systems, to underscore the flexibility in obtaining EP in nonequilibrium Langevin systems."],"url":"http://arxiv.org/abs/2405.12305v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-20 18:11:45","title":"Automatic Hardware Pragma Insertion in High-Level Synthesis: A Non-Linear Programming Approach","abstract":"High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.","sentences":["High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs.","These tools offer benefits such as reduced development time and enhanced performance.","However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps.","Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary.","Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   ","To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers.","Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels.","Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels.","Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results."],"url":"http://arxiv.org/abs/2405.12304v1","category":"cs.AR"}
{"created":"2024-05-20 18:00:00","title":"Axion baryogenesis puts a new spin on the Hubble tension","abstract":"We show that a rotating axion field that makes a transition from a matter-like equation of state to a kination-like equation of state around the epoch of recombination can significantly ameliorate the Hubble tension, i.e., the discrepancy between the determinations of the present-day expansion rate $H_0$ from observations of the cosmic microwave background on one hand and Type Ia supernovae on the other. We consider a specific, UV-complete model of such a rotating axion and find that it can relax the Hubble tension without exacerbating tensions in determinations of other cosmological parameters, in particular the amplitude of matter fluctuations $S_8$. We subsequently demonstrate how this rotating axion model can also generate the baryon asymmetry of our universe, by introducing a coupling of the axion field to right-handed neutrinos. This baryogenesis model predicts heavy neutral leptons that are most naturally within reach of future lepton colliders, but in finely-tuned regions of parameter space may also be accessible at the high-luminosity LHC and the beam dump experiment SHiP.","sentences":["We show that a rotating axion field that makes a transition from a matter-like equation of state to a kination-like equation of state around the epoch of recombination can significantly ameliorate the Hubble tension, i.e., the discrepancy between the determinations of the present-day expansion rate $H_0$ from observations of the cosmic microwave background on one hand and Type Ia supernovae on the other.","We consider a specific, UV-complete model of such a rotating axion and find that it can relax the Hubble tension without exacerbating tensions in determinations of other cosmological parameters, in particular the amplitude of matter fluctuations $S_8$. We subsequently demonstrate how this rotating axion model can also generate the baryon asymmetry of our universe, by introducing a coupling of the axion field to right-handed neutrinos.","This baryogenesis model predicts heavy neutral leptons that are most naturally within reach of future lepton colliders, but in finely-tuned regions of parameter space may also be accessible at the high-luminosity LHC and the beam dump experiment SHiP."],"url":"http://arxiv.org/abs/2405.12268v1","category":"hep-ph"}
{"created":"2024-05-20 17:59:59","title":"CMB-HD as a Probe of Dark Matter on Sub-Galactic Scales","abstract":"We show for the first time that high-resolution CMB lensing observations can probe structure on sub-galactic scales. In particular, a CMB-HD experiment can probe out to k ~ 55 h/Mpc, corresponding to halo masses of about $10^8 M_{\\odot}$. Over the range 0.005 h/Mpc < k < 55 h/Mpc, spanning four orders of magnitude, the total lensing signal-to-noise ratio (SNR) from the temperature, polarization, and lensing power spectra is greater than 1900. CMB-HD gains most of the lensing SNR at small scales from the temperature power spectrum, as opposed to the lensing spectrum. These lensing measurements allow CMB-HD to distinguish between cold dark matter (CDM) and non-CDM models that change the matter power spectrum on sub-galactic scales. We also find that CMB-HD can distinguish between baryonic feedback effects and non-CDM models due to the different way each impacts the lensing signal. The kinetic Sunyaev-Zel'dovich (kSZ) power spectrum further constrains non-CDM models that deviate from CDM on the smallest scales CMB-HD measures. For example, CMB-HD can detect 1 keV warm dark matter (WDM) at 30$\\sigma$, or rule out about 7 keV WDM at 95% CL, in a $\\Lambda$WDM + $N_{\\rm{eff}} + \\sum m_\\nu + m_{\\rm{WDM}} + \\log_{10}T_{\\rm{AGN}} + A_{\\rm{kSZ}} + n_{\\rm{kSZ}}$ model; here $T_{\\rm{AGN}}$ characterizes the strength of the feedback, and $A_{\\rm{kSZ}}$ and $n_{\\rm{kSZ}}$ allow freedom in the amplitude and slope of the kinetic Sunyaev-Zel'dovich power spectrum. We make the CMB-HD Fisher code used here publicly available, and note that it can be modified to use any non-CDM model that changes the matter power spectrum.","sentences":["We show for the first time that high-resolution CMB lensing observations can probe structure on sub-galactic scales.","In particular, a CMB-HD experiment can probe out to k ~ 55 h/Mpc, corresponding to halo masses of about $10^8 M_{\\odot}$. Over the range 0.005 h/Mpc < k < 55 h/Mpc, spanning four orders of magnitude, the total lensing signal-to-noise ratio (SNR) from the temperature, polarization, and lensing power spectra is greater than 1900.","CMB-HD gains most of the lensing SNR at small scales from the temperature power spectrum, as opposed to the lensing spectrum.","These lensing measurements allow CMB-HD to distinguish between cold dark matter (CDM) and non-CDM models that change the matter power spectrum on sub-galactic scales.","We also find that CMB-HD can distinguish between baryonic feedback effects and non-CDM models due to the different way each impacts the lensing signal.","The kinetic Sunyaev-Zel'dovich (kSZ) power spectrum further constrains non-CDM models that deviate from CDM on the smallest scales CMB-HD measures.","For example, CMB-HD can detect 1 keV warm dark matter (WDM) at 30$\\sigma$, or rule out about 7 keV WDM at 95% CL, in a $\\Lambda$WDM + $N_{\\rm{eff}} + \\sum m_\\nu + m_{\\rm{WDM}} + \\log_{10}T_{\\rm{AGN}} + A_{\\rm{kSZ}} + n_{\\rm{kSZ}}$ model; here $T_{\\rm{AGN}}$ characterizes the strength of the feedback, and $A_{\\rm{kSZ}}$ and $n_{\\rm{kSZ}}$ allow freedom in the amplitude and slope of the kinetic Sunyaev-Zel'dovich power spectrum.","We make the CMB-HD Fisher code used here publicly available, and note that it can be modified to use any non-CDM model that changes the matter power spectrum."],"url":"http://arxiv.org/abs/2405.12220v1","category":"astro-ph.CO"}
{"created":"2024-05-20 17:59:59","title":"Images that Sound: Composing Images and Sounds on a Single Canvas","abstract":"Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: https://ificl.github.io/images-that-sound/","sentences":["Spectrograms are 2D representations of sound that look very different from the images found in our visual world.","And natural images, when played as spectrograms, make unnatural sounds.","In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio.","We call these spectrograms images that sound.","Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space.","During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models.","Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt.","Please see our project page for video results: https://ificl.github.io/images-that-sound/"],"url":"http://arxiv.org/abs/2405.12221v1","category":"cs.CV"}
{"created":"2024-05-20 17:59:17","title":"Radiative transfer of 21-cm line through ionised cavities in an expanding universe","abstract":"The optical depth parameterisation is typically used to study the 21-cm signals associated with the properties of the neutral hydrogen (HI) gas and the ionisation morphology during the Epoch of Reionisation (EoR), without solving the radiative transfer equation. To assess the uncertainties resulting from this simplification, we conduct explicit radiative transfer calculations using the cosmological 21-cm radiative transfer (C21LRT) code and examine the imprints of ionisation structures on the 21-cm spectrum. We consider a globally averaged reionisation history and implement fully ionised cavities (HII bubbles) of diameters $d$ ranging from 0.01 Mpc to 10 Mpc at epochs within the emission and the absorption regimes of the 21-cm global signal. The single-ray C21LRT calculations show that the shape of the imprinted spectral features are primarily determined by $d$ and the 21-cm line profile, which is parametrised by the turbulent velocity of the HI gas. It reveals the spectral features tied to the transition from ionised to neutral regions that calculations based on the optical depth parametrisation were unable to capture. We also present analytical approximations of the calculated spectral features of the HII bubbles. The multiple-ray calculations show that the apparent shape of a HII bubble (of $d=5$ Mpc at $z=8$), because of the finite speed of light, differs depending on whether the bubble's ionisation front is stationary or expanding. Our study shows the necessity of properly accounting for the effects of line-continuum interaction, line broadening and cosmological expansion to correctly predict the EoR 21-cm signals.","sentences":["The optical depth parameterisation is typically used to study the 21-cm signals associated with the properties of the neutral hydrogen (HI) gas and the ionisation morphology during the Epoch of Reionisation (EoR), without solving the radiative transfer equation.","To assess the uncertainties resulting from this simplification, we conduct explicit radiative transfer calculations using the cosmological 21-cm radiative transfer (C21LRT) code and examine the imprints of ionisation structures on the 21-cm spectrum.","We consider a globally averaged reionisation history and implement fully ionised cavities (HII bubbles) of diameters $d$ ranging from 0.01 Mpc to 10 Mpc at epochs within the emission and the absorption regimes of the 21-cm global signal.","The single-ray C21LRT calculations show that the shape of the imprinted spectral features are primarily determined by $d$ and the 21-cm line profile, which is parametrised by the turbulent velocity of the HI gas.","It reveals the spectral features tied to the transition from ionised to neutral regions that calculations based on the optical depth parametrisation were unable to capture.","We also present analytical approximations of the calculated spectral features of the HII bubbles.","The multiple-ray calculations show that the apparent shape of a HII bubble (of $d=5$ Mpc at $z=8$), because of the finite speed of light, differs depending on whether the bubble's ionisation front is stationary or expanding.","Our study shows the necessity of properly accounting for the effects of line-continuum interaction, line broadening and cosmological expansion to correctly predict the EoR 21-cm signals."],"url":"http://arxiv.org/abs/2405.12216v1","category":"astro-ph.CO"}
{"created":"2024-05-20 17:58:21","title":"Optimal tail estimates in $\u03b2$-ensembles and applications to last passage percolation","abstract":"Hermite and Laguerre $\\beta$-ensembles are important and well studied models in random matrix theory with special cases $\\beta=1,2,4$ corresponding to eigenvalues of classical random matrix ensembles. It is well known that the largest eigenvalues in these, under appropriate scaling, converge weakly to the Tracy-Widom $\\beta$ distribution whose distribution function $F_{\\beta}$ has asymptotics given by $1-F_{\\beta}(x)=\\exp\\left(-\\frac{2\\beta}{3}(1+o(1))x^{3/2}\\right)$ as $x\\to \\infty$ and $F_{\\beta}(x)=\\exp\\left(-\\frac{\\beta}{24}(1+o(1))|x|^3\\right)$ as $x\\to -\\infty$. Although tail estimates for the largest eigenvalues with correct exponents have been proved for the pre-limiting models, estimates with matching constants had not so far been established for general $\\beta$; even in the exactly solvable cases, some of the bounds were missing. In this paper, we prove upper and lower moderate deviation estimates for both tails with matching constants.   We illustrate the usefulness of these estimates by considering certain questions in planar exponential last passage percolation (LPP), a well-studied model in the KPZ universality class in which certain statistics have same distributions as largest eigenvalues in Laguerre $\\beta$-ensembles (for $\\beta=1,2,4$). Using our estimates in conjunction with a combination of old and new results on the LPP geometry, we obtain three laws of iterated logarithm including one which settles a conjecture of Ledoux (J. Theor. Probab., 2018). We expect that the sharp moderate deviation estimates will find many further applications in LPP problems and beyond.","sentences":["Hermite and Laguerre $\\beta$-ensembles are important and well studied models in random matrix theory with special cases $\\beta=1,2,4$ corresponding to eigenvalues of classical random matrix ensembles.","It is well known that the largest eigenvalues in these, under appropriate scaling, converge weakly to the Tracy-Widom $\\beta$ distribution whose distribution function $F_{\\beta}$ has asymptotics given by $1-F_{\\beta}(x)=\\exp\\left(-\\frac{2\\beta}{3}(1+o(1))x^{3/2}\\right)$ as $x\\to \\infty$ and $F_{\\beta}(x)=\\exp\\left(-\\frac{\\beta}{24}(1+o(1))|x|^3\\right)$ as $x\\to -\\infty$. Although tail estimates for the largest eigenvalues with correct exponents have been proved for the pre-limiting models, estimates with matching constants had not so far been established for general $\\beta$; even in the exactly solvable cases, some of the bounds were missing.","In this paper, we prove upper and lower moderate deviation estimates for both tails with matching constants.   ","We illustrate the usefulness of these estimates by considering certain questions in planar exponential last passage percolation (LPP), a well-studied model in the KPZ universality class in which certain statistics have same distributions as largest eigenvalues in Laguerre $\\beta$-ensembles (for $\\beta=1,2,4$).","Using our estimates in conjunction with a combination of old and new results on the LPP geometry, we obtain three laws of iterated logarithm including one which settles a conjecture of Ledoux (J. Theor.","Probab., 2018).","We expect that the sharp moderate deviation estimates will find many further applications in LPP problems and beyond."],"url":"http://arxiv.org/abs/2405.12215v1","category":"math.PR"}
{"created":"2024-05-20 17:57:01","title":"Octo: An Open-Source Generalist Robot Policy","abstract":"Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.","sentences":["Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly.","However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains.","In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation.","As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date.","It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs.","In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces.","We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models."],"url":"http://arxiv.org/abs/2405.12213v1","category":"cs.RO"}
{"created":"2024-05-20 17:56:22","title":"Forced Measurement of Astronomical Sources at Low Signal to Noise","abstract":"We propose a modified moment matching algorithm to avoid catastrophic failures for sources with a low signal to noise ratio (SNR). The proposed modifications include a method to eliminate non-physical negative pixel values and a forced single iteration with an initial guess derived from co-add measurements when iterative methods are unstable. We correct for all biases in measurements introduced by the method. We find that the proposed modifications allow the algorithm to avoid catastrophic failures in nearly 100\\% of the cases, especially at low signal to noise ratio. Additionally, with a reasonable guess from co-add measurements, the algorithm measures the flux, centroid, size, shape and ellipticity with bias statistically consistent with zero. We show the proposed method allows us to measure sources seven times fainter than traditional methods when applied to images obtained from WIYN-ODI. We also present a scheme to find uncertainties in measurements when using the new method to measure astronomical sources.","sentences":["We propose a modified moment matching algorithm to avoid catastrophic failures for sources with a low signal to noise ratio (SNR).","The proposed modifications include a method to eliminate non-physical negative pixel values and a forced single iteration with an initial guess derived from co-add measurements when iterative methods are unstable.","We correct for all biases in measurements introduced by the method.","We find that the proposed modifications allow the algorithm to avoid catastrophic failures in nearly 100\\% of the cases, especially at low signal to noise ratio.","Additionally, with a reasonable guess from co-add measurements, the algorithm measures the flux, centroid, size, shape and ellipticity with bias statistically consistent with zero.","We show the proposed method allows us to measure sources seven times fainter than traditional methods when applied to images obtained from WIYN-ODI.","We also present a scheme to find uncertainties in measurements when using the new method to measure astronomical sources."],"url":"http://arxiv.org/abs/2405.12212v1","category":"astro-ph.IM"}
{"created":"2024-05-20 17:47:18","title":"Optimistic Query Routing in Clustering-based Approximate Maximum Inner Product Search","abstract":"Clustering-based nearest neighbor search is a simple yet effective method in which data points are partitioned into geometric shards to form an index, and only a few shards are searched during query processing to find an approximate set of top-$k$ vectors. Even though the search efficacy is heavily influenced by the algorithm that identifies the set of shards to probe, it has received little attention in the literature. This work attempts to bridge that gap by studying the problem of routing in clustering-based maximum inner product search (MIPS). We begin by unpacking existing routing protocols and notice the surprising contribution of optimism. We then take a page from the sequential decision making literature and formalize that insight following the principle of ``optimism in the face of uncertainty.'' In particular, we present a new framework that incorporates the moments of the distribution of inner products within each shard to optimistically estimate the maximum inner product. We then present a simple instance of our algorithm that uses only the first two moments to reach the same accuracy as state-of-the-art routers such as \\scann by probing up to $50%$ fewer points on a suite of benchmark MIPS datasets. Our algorithm is also space-efficient: we design a sketch of the second moment whose size is independent of the number of points and in practice requires storing only $O(1)$ additional vectors per shard.","sentences":["Clustering-based nearest neighbor search is a simple yet effective method in which data points are partitioned into geometric shards to form an index, and only a few shards are searched during query processing to find an approximate set of top-$k$ vectors.","Even though the search efficacy is heavily influenced by the algorithm that identifies the set of shards to probe, it has received little attention in the literature.","This work attempts to bridge that gap by studying the problem of routing in clustering-based maximum inner product search (MIPS).","We begin by unpacking existing routing protocols and notice the surprising contribution of optimism.","We then take a page from the sequential decision making literature and formalize that insight following the principle of ``optimism in the face of uncertainty.''","In particular, we present a new framework that incorporates the moments of the distribution of inner products within each shard to optimistically estimate the maximum inner product.","We then present a simple instance of our algorithm that uses only the first two moments to reach the same accuracy as state-of-the-art routers such as \\scann by probing up to $50%$ fewer points on a suite of benchmark MIPS datasets.","Our algorithm is also space-efficient: we design a sketch of the second moment whose size is independent of the number of points and in practice requires storing only $O(1)$ additional vectors per shard."],"url":"http://arxiv.org/abs/2405.12207v1","category":"cs.LG"}
{"created":"2024-05-20 17:45:36","title":"Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models","abstract":"Scientist learn early on how to cite scientific sources to support their claims. Sometimes, however, scientists have challenges determining where a citation should be situated -- or, even worse, fail to cite a source altogether. Automatically detecting sentences that need a citation (i.e., citation worthiness) could solve both of these issues, leading to more robust and well-constructed scientific arguments. Previous researchers have applied machine learning to this task but have used small datasets and models that do not take advantage of recent algorithmic developments such as attention mechanisms in deep learning. We hypothesize that we can develop significantly accurate deep learning architectures that learn from large supervised datasets constructed from open access publications. In this work, we propose a Bidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism and contextual information to detect sentences that need citations. We also produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset, which is orders of magnitude larger than previous datasets. Our experiments show that our architecture achieves state of the art performance on the standard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance ($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer learning across these datasets. We further use interpretable models to illuminate how specific language is used to promote and inhibit citations. We discover that sections and surrounding sentences are crucial for our improved predictions. We further examined purported mispredictions of the model, and uncovered systematic human mistakes in citation behavior and source data. This opens the door for our model to check documents during pre-submission and pre-archival procedures. We make this new dataset, the code, and a web-based tool available to the community.","sentences":["Scientist learn early on how to cite scientific sources to support their claims.","Sometimes, however, scientists have challenges determining where a citation should be situated -- or, even worse, fail to cite a source altogether.","Automatically detecting sentences that need a citation (i.e., citation worthiness) could solve both of these issues, leading to more robust and well-constructed scientific arguments.","Previous researchers have applied machine learning to this task but have used small datasets and models that do not take advantage of recent algorithmic developments such as attention mechanisms in deep learning.","We hypothesize that we can develop significantly accurate deep learning architectures that learn from large supervised datasets constructed from open access publications.","In this work, we propose a Bidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism and contextual information to detect sentences that need citations.","We also produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset, which is orders of magnitude larger than previous datasets.","Our experiments show that our architecture achieves state of the art performance on the standard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance ($F_{1}=0.856$) on the new PMOA-CITE.","Moreover, we show that it can transfer learning across these datasets.","We further use interpretable models to illuminate how specific language is used to promote and inhibit citations.","We discover that sections and surrounding sentences are crucial for our improved predictions.","We further examined purported mispredictions of the model, and uncovered systematic human mistakes in citation behavior and source data.","This opens the door for our model to check documents during pre-submission and pre-archival procedures.","We make this new dataset, the code, and a web-based tool available to the community."],"url":"http://arxiv.org/abs/2405.12206v1","category":"cs.CL"}
{"created":"2024-05-20 17:37:10","title":"Bond percolation games on the $2$-dimensional square lattice, and ergodicity of associated probabilistic cellular automata","abstract":"We consider bond percolation games on the $2$-dimensional square lattice in which each edge (that is either between the sites $(x,y)$ and $(x+1,y)$ or between the sites $(x,y)$ and $(x,y+1)$, for all $(x,y) \\in \\mathbb{Z}^{2}$) has been assigned, independently, a label that reads \"trap\" with probability $p$, \"target\" with probability $q$, and \"open\" with probability $1-p-q$. Once a realization of this labeling is generated, it is revealed in its entirety to the players before the game starts. The game involves a single token, initially placed at the origin, and two players who take turns to make moves. A move involves relocating the token from where it is currently located, say the site $(x,y)$, to any one of $(x+1,y)$ and $(x,y+1)$. A player wins if she is able to move the token along an edge labeled a target, or if she is able to force her opponent to move the token along an edge labeled a trap. The game is said to result in a draw if it continues indefinitely (i.e.\\ with the token always being moved along open edges). We ask the question: for what values of $p$ and $q$ is the probability of draw equal to $0$? By establishing a close connection between the event of draw and the ergodicity of a suitably defined probabilistic cellualar automaton, we are able to show that the probability of draw is $0$ when $p > 0.157175$ and $q=0$, and when $p=q \\geqslant 0.10883$.","sentences":["We consider bond percolation games on the $2$-dimensional square lattice in which each edge (that is either between the sites $(x,y)$ and $(x+1,y)$ or between the sites $(x,y)$ and $(x,y+1)$, for all $(x,y) \\in \\mathbb{Z}^{2}$) has been assigned, independently, a label that reads \"trap\" with probability $p$, \"target\" with probability $q$, and \"open\" with probability $1-p-q$. Once a realization of this labeling is generated, it is revealed in its entirety to the players before the game starts.","The game involves a single token, initially placed at the origin, and two players who take turns to make moves.","A move involves relocating the token from where it is currently located, say the site $(x,y)$, to any one of $(x+1,y)$ and $(x,y+1)$.","A player wins if she is able to move the token along an edge labeled a target, or if she is able to force her opponent to move the token along an edge labeled a trap.","The game is said to result in a draw if it continues indefinitely (i.e.\\ with the token always being moved along open edges).","We ask the question: for what values of $p$ and $q$ is the probability of draw equal to $0$?","By establishing a close connection between the event of draw and the ergodicity of a suitably defined probabilistic cellualar automaton, we are able to show that the probability of draw is $0$ when $p > 0.157175$ and $q=0$, and when $p=q \\geqslant 0.10883$."],"url":"http://arxiv.org/abs/2405.12199v1","category":"math.PR"}
{"created":"2024-05-20 17:22:33","title":"The sign of scalar curvature on K\u00e4hler blowups","abstract":"We show that if $(M,\\omega)$ is a compact K\\\"ahler manifold with positive/negative scalar curvature, then the blowup of $M$ at any point also furnishes a positive/negative scalar curvature K\\\"ahler metric in classes which make the exceptional divisor small. In the case of K\\\"ahler surfaces with positive scalar curvature, this extends a result of N. Hitchin to surfaces and answers a conjecture of C. LeBrun in the affirmative, as a result completing the classification of such surfaces.","sentences":["We show that if $(M,\\omega)$ is a compact K\\\"ahler manifold with positive/negative scalar curvature, then the blowup of $M$ at any point also furnishes a positive/negative scalar curvature K\\\"ahler metric in classes which make the exceptional divisor small.","In the case of K\\\"ahler surfaces with positive scalar curvature, this extends a result of N. Hitchin to surfaces and answers a conjecture of C. LeBrun in the affirmative, as a result completing the classification of such surfaces."],"url":"http://arxiv.org/abs/2405.12189v1","category":"math.DG"}
{"created":"2024-05-20 17:11:37","title":"Robust VAR Capability Curve of DER with Uncertain Renewable Generation","abstract":"Active distribution system with high penetration of inverter based distributed energy resources (DER) can be utilized for VAR-related ancillary services. To utilize the DER flexibility, transmission system operator (TSO) must be presented with the aggregated DER flexibility of distribution system. However, the uncertainty in renewable generation questions the credibility of aggregated capability curve in practice. In this paper, we incorporate the uncertainty into aggregation process to develop a robust capability curve while preserving the real physics (unbalance and lossy nature) of distribution system. Statistical inference method is employed to quantify uncertainty in solar generation and quantified uncertainty is integrated into a chance constrained optimal power flow (OPF). It provides the grid operator with the dispatchable aggregated reactive power capability. The resulting capability curve with the associated probability can be harnessed by the TSO for decision making for both planning and operation.","sentences":["Active distribution system with high penetration of inverter based distributed energy resources (DER) can be utilized for VAR-related ancillary services.","To utilize the DER flexibility, transmission system operator (TSO) must be presented with the aggregated DER flexibility of distribution system.","However, the uncertainty in renewable generation questions the credibility of aggregated capability curve in practice.","In this paper, we incorporate the uncertainty into aggregation process to develop a robust capability curve while preserving the real physics (unbalance and lossy nature) of distribution system.","Statistical inference method is employed to quantify uncertainty in solar generation and quantified uncertainty is integrated into a chance constrained optimal power flow (OPF).","It provides the grid operator with the dispatchable aggregated reactive power capability.","The resulting capability curve with the associated probability can be harnessed by the TSO for decision making for both planning and operation."],"url":"http://arxiv.org/abs/2405.12184v1","category":"eess.SY"}
{"created":"2024-05-20 17:02:18","title":"Establishing Trust in the Beyond-5G Core Network using Trusted Execution Environments","abstract":"The fifth generation (5G) of cellular networks starts a paradigm shift from the traditional monolithic system design to a Service Based Architecture, that fits modern performance requirements and scales efficiently to new services. This paradigm will be the foundation of future cellular core networks beyond 5G. The new architecture splits network functionalities into smaller logical entities that can be disaggregated logically, physically, and geographically. This affords interoperability between the mobile network operators and commercial software and hardware vendors or cloud providers. By making use of commodity services and products, this system construct inherits the vulnerabilities in those underlying technologies, thereby increasing its attack surface and requiring a rigorous security analysis. In this work, we review the security implications introduced in B5G networks, and the security mechanisms that are supported by the 5G standard. We emphasize on the support of Zero Trust Architecture in 5G and its relevance in decentralized deployments. We revisit the definition of trust in modern enterprise network operations and identify important Zero Trust properties that are weakened by the nature of cloud deployments. To that end, we propose a vertical extension of Zero Trust, namely, Zero Trust Execution, to model untrusted execution environments, and we provide an analysis on how to establish trust in Beyond-5G network architectures using Trusted Execution Environments. Our analysis shows how our model architecture handles the increased attack surface and reinforces the Zero Trust Architecture principles in the 5G Core, without any changes to the 5G standard. Finally, we provide experimental results over a 5G testbed using Open5GS and UERANSIM that demonstrate minimal performance overhead, and a monetary cost evaluation.","sentences":["The fifth generation (5G) of cellular networks starts a paradigm shift from the traditional monolithic system design to a Service Based Architecture, that fits modern performance requirements and scales efficiently to new services.","This paradigm will be the foundation of future cellular core networks beyond 5G.","The new architecture splits network functionalities into smaller logical entities that can be disaggregated logically, physically, and geographically.","This affords interoperability between the mobile network operators and commercial software and hardware vendors or cloud providers.","By making use of commodity services and products, this system construct inherits the vulnerabilities in those underlying technologies, thereby increasing its attack surface and requiring a rigorous security analysis.","In this work, we review the security implications introduced in B5G networks, and the security mechanisms that are supported by the 5G standard.","We emphasize on the support of Zero Trust Architecture in 5G and its relevance in decentralized deployments.","We revisit the definition of trust in modern enterprise network operations and identify important Zero Trust properties that are weakened by the nature of cloud deployments.","To that end, we propose a vertical extension of Zero Trust, namely, Zero Trust Execution, to model untrusted execution environments, and we provide an analysis on how to establish trust in Beyond-5G network architectures using Trusted Execution Environments.","Our analysis shows how our model architecture handles the increased attack surface and reinforces the Zero Trust Architecture principles in the 5G Core, without any changes to the 5G standard.","Finally, we provide experimental results over a 5G testbed using Open5GS and UERANSIM that demonstrate minimal performance overhead, and a monetary cost evaluation."],"url":"http://arxiv.org/abs/2405.12177v1","category":"cs.CR"}
{"created":"2024-05-20 16:54:20","title":"Deformation of Residual Intersections","abstract":"It is shown that in a Cohen-Macaulay local ring, the generic linkage of an ideal $I$ is a deformation of the arbitrary linkage of $I$. This fact does not need $I$ to be a Cohen-Macaulay ideal. The same holds for $s$-residual intersections of $I$ when $s$ does not exceed the height of $I$ by one. Under some slight conditions on $I$, one further generalizes this principle to encompass any $s$-residual intersection.","sentences":["It is shown that in a Cohen-Macaulay local ring, the generic linkage of an ideal $I$ is a deformation of the arbitrary linkage of $I$. This fact does not need $I$ to be a Cohen-Macaulay ideal.","The same holds for $s$-residual intersections of $I$ when $s$ does not exceed the height of $I$ by one.","Under some slight conditions on $I$, one further generalizes this principle to encompass any $s$-residual intersection."],"url":"http://arxiv.org/abs/2405.12170v1","category":"math.AC"}
{"created":"2024-05-20 16:42:41","title":"Optimal Eigenvalue Rigidity of Random Regular Graphs","abstract":"Consider the normalized adjacency matrices of random $d$-regular graphs on $N$ vertices with fixed degree $d\\geq 3$, and denote the eigenvalues as $\\lambda_1=d/\\sqrt{d-1}\\geq \\lambda_2\\geq\\lambda_3\\cdots\\geq \\lambda_N$. We prove that the optimal (up to an extra $N^{{\\rm o}_N(1)}$ factor, where ${\\rm o}_N(1)$ can be arbitrarily small) eigenvalue rigidity holds. More precisely, denote $\\gamma_i$ as the classical location of the $i$-th eigenvalue under the Kesten-Mckay law in decreasing order. Then with probability $1-N^{-1+{\\rm o}_N(1)}$,   \\begin{align*}   |\\lambda_i-\\gamma_i|\\leq \\frac{N^{{\\rm o}_N(1)}}{N^{2/3} (\\min\\{i,N-i+1\\})^{1/3}},\\quad \\text{ for all } i\\in \\{2,3,\\cdots,N\\}.   \\end{align*}   In particular, the fluctuations of extreme eigenvalues are bounded by $N^{-2/3+{\\rm o}_N(1)}$. This gives the same order of fluctuation as for the eigenvalues of matrices from the Gaussian Orthogonal Ensemble.","sentences":["Consider the normalized adjacency matrices of random $d$-regular graphs on $N$ vertices with fixed degree $d\\geq 3$, and denote the eigenvalues as $\\lambda_1=d/\\sqrt{d-1}\\geq \\lambda_2\\geq\\lambda_3\\cdots\\geq \\lambda_N$. We prove that the optimal (up to an extra $N^{{\\rm o}_N(1)}$ factor, where ${\\rm o}_N(1)$ can be arbitrarily small) eigenvalue rigidity holds.","More precisely, denote $\\gamma_i$ as the classical location of the $i$-th eigenvalue under the Kesten-Mckay law in decreasing order.","Then with probability $1-N^{-1+{\\rm o}_N(1)}$,   \\begin{align*}   |\\lambda_i-\\gamma_i|\\leq \\frac{N^{{\\rm o}_N(1)}}{N^{2/3} (\\min\\{i,N-i+1\\})^{1/3}},\\quad \\text{ for all } i\\in \\{2,3,\\cdots,N\\}.   ","\\end{align*}   In particular, the fluctuations of extreme eigenvalues are bounded by $N^{-2/3+{\\rm o}_N(1)}$.","This gives the same order of fluctuation as for the eigenvalues of matrices from the Gaussian Orthogonal Ensemble."],"url":"http://arxiv.org/abs/2405.12161v1","category":"math.PR"}
{"created":"2024-05-20 16:38:44","title":"Geant4: a Game Changer in High Energy Physics and Related Applicative Fields","abstract":"Geant4 is an object-oriented toolkit for the simulation of the passage of particles through matter. Its development was initially motivated by the requirements of physics experiments at high energy hadron colliders under construction in the last decade of the 20th century. Since its release in 1998, it has been exploited in many different applicative fields, including space science, nuclear physics, medical physics and archaeology. Its valuable support to scientific discovery is demonstrated by more than 16000 citations received in the past 25 years, including notable citations for main discoveries in different fields. This accomplishment shows that well designed software plays a key role in enabling scientific advancement. In this paper we discuss the key principles and the innovative decisions at the basis of Geant4, which made it a game changer in high energy physics and related fields, and outline some considerations regarding future directions.","sentences":["Geant4 is an object-oriented toolkit for the simulation of the passage of particles through matter.","Its development was initially motivated by the requirements of physics experiments at high energy hadron colliders under construction in the last decade of the 20th century.","Since its release in 1998, it has been exploited in many different applicative fields, including space science, nuclear physics, medical physics and archaeology.","Its valuable support to scientific discovery is demonstrated by more than 16000 citations received in the past 25 years, including notable citations for main discoveries in different fields.","This accomplishment shows that well designed software plays a key role in enabling scientific advancement.","In this paper we discuss the key principles and the innovative decisions at the basis of Geant4, which made it a game changer in high energy physics and related fields, and outline some considerations regarding future directions."],"url":"http://arxiv.org/abs/2405.12159v1","category":"physics.comp-ph"}
{"created":"2024-05-21 17:21:28","title":"Adaptive Variant of Frank-Wolfe Method for Relative Smooth Convex Optimization Problems","abstract":"The paper introduces a new adaptive version of the Frank-Wolfe algorithm for relatively smooth convex functions. It is proposed to use the Bregman divergence other than half the square of the Euclidean norm in the formula for step-size. Algorithm convergence estimates for minimization problems of relatively smooth convex functions with the triangle scaling property are proved. Computational experiments are performed, and conditions are shown in which the obvious advantage of the proposed algorithm over its Euclidean norm analogue is shown. We also found examples of problems for which the proposed variation of the Frank-Wolfe method works better than known accelerated gradient-type methods for relatively smooth convex functions with the triangle scaling property.","sentences":["The paper introduces a new adaptive version of the Frank-Wolfe algorithm for relatively smooth convex functions.","It is proposed to use the Bregman divergence other than half the square of the Euclidean norm in the formula for step-size.","Algorithm convergence estimates for minimization problems of relatively smooth convex functions with the triangle scaling property are proved.","Computational experiments are performed, and conditions are shown in which the obvious advantage of the proposed algorithm over its Euclidean norm analogue is shown.","We also found examples of problems for which the proposed variation of the Frank-Wolfe method works better than known accelerated gradient-type methods for relatively smooth convex functions with the triangle scaling property."],"url":"http://arxiv.org/abs/2405.12948v1","category":"math.OC"}
{"created":"2024-05-21 17:06:06","title":"Circuit QED theory of direct and dual Shapiro steps with finite-size transmission line resonators","abstract":"We investigate the occurrence of direct and dual Shapiro steps for a Josephson junction coupled to a finite-size transmission line resonator. We treat both problems through a circuit QED approach with a large, but finite number of photon modes. For the dual case, we do not assume the (approximate) charge-phase duality, but include the full multi-band dynamics for the Josephson junction. Mean-field equations within such Hamiltonian approach reproduce the result obtained through a dissipative classical equation when the number of transmission line modes is large enough. To account for quantum and thermal fluctuations, we go beyond the mean-field treatment within a truncated Wigner approach. The fluctuations are shown to modify both the direct and the dual steps. We show how the dual steps are very sensitive to these fluctuations and identify the key physical parameters for the junction and the transmission line controlling their robustness, which is essential for applications to close the quantum metrological triangle.","sentences":["We investigate the occurrence of direct and dual Shapiro steps for a Josephson junction coupled to a finite-size transmission line resonator.","We treat both problems through a circuit QED approach with a large, but finite number of photon modes.","For the dual case, we do not assume the (approximate) charge-phase duality, but include the full multi-band dynamics for the Josephson junction.","Mean-field equations within such Hamiltonian approach reproduce the result obtained through a dissipative classical equation when the number of transmission line modes is large enough.","To account for quantum and thermal fluctuations, we go beyond the mean-field treatment within a truncated Wigner approach.","The fluctuations are shown to modify both the direct and the dual steps.","We show how the dual steps are very sensitive to these fluctuations and identify the key physical parameters for the junction and the transmission line controlling their robustness, which is essential for applications to close the quantum metrological triangle."],"url":"http://arxiv.org/abs/2405.12935v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 15:46:13","title":"Approximating TSP Variants Using a Bridge Lemma","abstract":"We give improved approximations for two metric \\textsc{Traveling Salesman Problem} (TSP) variants. In \\textsc{Ordered TSP} (OTSP) we are given a linear ordering on a subset of nodes $o_1, \\ldots, o_k$. The TSP solution must have that $o_{i+1}$ is visited at some point after $o_i$ for each $1 \\leq i < k$. This is the special case of \\textsc{Precedence-Constrained TSP} ($PTSP$) in which the precedence constraints are given by a single chain on a subset of nodes. In \\textsc{$k$-Person TSP Path} (k-TSPP), we are given pairs of nodes $(s_1,t_1), \\ldots, (s_k,t_k)$. The goal is to find an $s_i$-$t_i$ path with minimum total cost such that every node is visited by at least one path.   We obtain a $3/2 + e^{-1} < 1.878$ approximation for OTSP, the first improvement over a trivial $\\alpha+1$ approximation where $\\alpha$ is the current best TSP approximation. We also obtain a $1 + 2 \\cdot e^{-1/2} < 2.214$ approximation for k-TSPP, the first improvement over a trivial $3$-approximation.   These algorithms both use an adaptation of the Bridge Lemma that was initially used to obtain improved \\textsc{Steiner Tree} approximations [Byrka et al., 2013]. Roughly speaking, our variant states that the cost of a cheapest forest rooted at a given set of terminal nodes will decrease by a substantial amount if we randomly sample a set of non-terminal nodes to also become terminals such provided each non-terminal has a constant probability of being sampled. We believe this view of the Bridge Lemma will find further use for improved vehicle routing approximations beyond this paper.","sentences":["We give improved approximations for two metric \\textsc{Traveling Salesman Problem} (TSP) variants.","In \\textsc{Ordered TSP} (OTSP) we are given a linear ordering on a subset of nodes $o_1, \\ldots, o_k$.","The TSP solution must have that $o_{i+1}$ is visited at some point after $o_i$ for each $1 \\leq","i < k$.","This is the special case of \\textsc{Precedence-Constrained TSP} ($PTSP$) in which the precedence constraints are given by a single chain on a subset of nodes.","In \\textsc{$k$-Person TSP Path} (k-TSPP), we are given pairs of nodes $(s_1,t_1), \\ldots, (s_k,t_k)$. The goal is to find an $s_i$-$t_i$ path with minimum total cost such that every node is visited by at least one path.   ","We obtain a $3/2 + e^{-1} < 1.878$ approximation for OTSP, the first improvement over a trivial $\\alpha+1$ approximation where $\\alpha$ is the current best TSP approximation.","We also obtain a $1 + 2 \\cdot e^{-1/2} < 2.214$ approximation for k-TSPP, the first improvement over a trivial $3$-approximation.   ","These algorithms both use an adaptation of the Bridge Lemma that was initially used to obtain improved \\textsc{Steiner","Tree} approximations [Byrka et al., 2013].","Roughly speaking, our variant states that the cost of a cheapest forest rooted at a given set of terminal nodes will decrease by a substantial amount if we randomly sample a set of non-terminal nodes to also become terminals such provided each non-terminal has a constant probability of being sampled.","We believe this view of the Bridge Lemma will find further use for improved vehicle routing approximations beyond this paper."],"url":"http://arxiv.org/abs/2405.12876v1","category":"cs.DS"}
{"created":"2024-05-21 15:11:35","title":"Inconsistency-Aware Cross-Attention for Audio-Visual Fusion in Dimensional Emotion Recognition","abstract":"Leveraging complementary relationships across modalities has recently drawn a lot of attention in multimodal emotion recognition. Most of the existing approaches explored cross-attention to capture the complementary relationships across the modalities. However, the modalities may also exhibit weak complementary relationships, which may deteriorate the cross-attended features, resulting in poor multimodal feature representations. To address this problem, we propose Inconsistency-Aware Cross-Attention (IACA), which can adaptively select the most relevant features on-the-fly based on the strong or weak complementary relationships across audio and visual modalities. Specifically, we design a two-stage gating mechanism that can adaptively select the appropriate relevant features to deal with weak complementary relationships. Extensive experiments are conducted on the challenging Aff-Wild2 dataset to show the robustness of the proposed model.","sentences":["Leveraging complementary relationships across modalities has recently drawn a lot of attention in multimodal emotion recognition.","Most of the existing approaches explored cross-attention to capture the complementary relationships across the modalities.","However, the modalities may also exhibit weak complementary relationships, which may deteriorate the cross-attended features, resulting in poor multimodal feature representations.","To address this problem, we propose Inconsistency-Aware Cross-Attention (IACA), which can adaptively select the most relevant features on-the-fly based on the strong or weak complementary relationships across audio and visual modalities.","Specifically, we design a two-stage gating mechanism that can adaptively select the appropriate relevant features to deal with weak complementary relationships.","Extensive experiments are conducted on the challenging Aff-Wild2 dataset to show the robustness of the proposed model."],"url":"http://arxiv.org/abs/2405.12853v1","category":"cs.CV"}
{"created":"2024-05-21 14:04:56","title":"Optimizing Coded-Apertures for Depth-Resolved Diffraction","abstract":"Coded apertures, traditionally employed in x-ray astronomy for imaging celestial objects, are now being adapted for micro-scale applications, particularly in studying microscopic specimens with synchrotron light diffraction. In this paper, we focus on micro-coded aperture imaging and its capacity to accomplish depth-resolved micro-diffraction analysis within crystalline specimens. We study aperture specifications and scanning parameters by assessing characteristics like size, thickness, and patterns. Numerical experiments assist in assessing their impact on reconstruction quality. Empirical data from a Laue diffraction microscope at a synchrotron undulator beamline supports our findings. Overall, our results offer key insights for optimizing aperture design in advancing micro-scale diffraction imaging at synchrotrons. This study contributes insights to this expanding field and suggests significant advancements, especially when coupled with the enhanced flux anticipated from the global upgrades of synchrotron sources.","sentences":["Coded apertures, traditionally employed in x-ray astronomy for imaging celestial objects, are now being adapted for micro-scale applications, particularly in studying microscopic specimens with synchrotron light diffraction.","In this paper, we focus on micro-coded aperture imaging and its capacity to accomplish depth-resolved micro-diffraction analysis within crystalline specimens.","We study aperture specifications and scanning parameters by assessing characteristics like size, thickness, and patterns.","Numerical experiments assist in assessing their impact on reconstruction quality.","Empirical data from a Laue diffraction microscope at a synchrotron undulator beamline supports our findings.","Overall, our results offer key insights for optimizing aperture design in advancing micro-scale diffraction imaging at synchrotrons.","This study contributes insights to this expanding field and suggests significant advancements, especially when coupled with the enhanced flux anticipated from the global upgrades of synchrotron sources."],"url":"http://arxiv.org/abs/2405.12813v1","category":"eess.SP"}
{"created":"2024-05-21 13:35:46","title":"Enhanced Dissipation via the Malliavin Calculus","abstract":"In this work we investigate the phenomenon of enhanced dissipation using techniques from the Malliavin Calculus. In particular, we construct a concise, elementary argument, that allows us to recover the well-known enhanced dissipation timescale for shear flows, first obtained by Bedrossian and Coti Zelati in 2017, as well as the precise hypoelliptic regularisation in x.","sentences":["In this work we investigate the phenomenon of enhanced dissipation using techniques from the Malliavin Calculus.","In particular, we construct a concise, elementary argument, that allows us to recover the well-known enhanced dissipation timescale for shear flows, first obtained by Bedrossian and Coti Zelati in 2017, as well as the precise hypoelliptic regularisation in x."],"url":"http://arxiv.org/abs/2405.12787v1","category":"math.AP"}
{"created":"2024-05-21 13:28:32","title":"Self-Supervised Modality-Agnostic Pre-Training of Swin Transformers","abstract":"Unsupervised pre-training has emerged as a transformative paradigm, displaying remarkable advancements in various domains. However, the susceptibility to domain shift, where pre-training data distribution differs from fine-tuning, poses a significant obstacle. To address this, we augment the Swin Transformer to learn from different medical imaging modalities, enhancing downstream performance. Our model, dubbed SwinFUSE (Swin Multi-Modal Fusion for UnSupervised Enhancement), offers three key advantages: (i) it learns from both Computed Tomography (CT) and Magnetic Resonance Images (MRI) during pre-training, resulting in complementary feature representations; (ii) a domain-invariance module (DIM) that effectively highlights salient input regions, enhancing adaptability; (iii) exhibits remarkable generalizability, surpassing the confines of tasks it was initially pre-trained on. Our experiments on two publicly available 3D segmentation datasets show a modest 1-2% performance trade-off compared to single-modality models, yet significant out-performance of up to 27% on out-of-distribution modality. This substantial improvement underscores our proposed approach's practical relevance and real-world applicability. Code is available at: https://github.com/devalab/SwinFUSE","sentences":["Unsupervised pre-training has emerged as a transformative paradigm, displaying remarkable advancements in various domains.","However, the susceptibility to domain shift, where pre-training data distribution differs from fine-tuning, poses a significant obstacle.","To address this, we augment the Swin Transformer to learn from different medical imaging modalities, enhancing downstream performance.","Our model, dubbed SwinFUSE (Swin Multi-Modal Fusion for UnSupervised Enhancement), offers three key advantages: (i) it learns from both Computed Tomography (CT) and Magnetic Resonance Images (MRI) during pre-training, resulting in complementary feature representations; (ii) a domain-invariance module (DIM) that effectively highlights salient input regions, enhancing adaptability; (iii) exhibits remarkable generalizability, surpassing the confines of tasks it was initially pre-trained on.","Our experiments on two publicly available 3D segmentation datasets show a modest 1-2% performance trade-off compared to single-modality models, yet significant out-performance of up to 27% on out-of-distribution modality.","This substantial improvement underscores our proposed approach's practical relevance and real-world applicability.","Code is available at: https://github.com/devalab/SwinFUSE"],"url":"http://arxiv.org/abs/2405.12781v1","category":"cs.CV"}
{"created":"2024-05-21 12:36:53","title":"Learning tensor trains from noisy functions with application to quantum simulation","abstract":"Tensor cross interpolation (TCI) is a powerful technique for learning a tensor train (TT) by adaptively sampling a target tensor based on an interpolation formula. However, when the tensor evaluations contain random noise, optimizing the TT is more advantageous than interpolating the noise. Here, we propose a new method that starts with an initial guess of TT and optimizes it using non-linear least-squares by fitting it to measured points obtained from TCI. We use quantics TCI (QTCI) in this method and demonstrate its effectiveness on sine and two-time correlation functions, with each evaluated with random noise. The resulting TT exhibits increased robustness against noise compared to the QTCI method. Furthermore, we employ this optimized TT of the correlation function in quantum simulation based on pseudo-imaginary-time evolution, resulting in ground-state energy with higher accuracy than the QTCI or Monte Carlo methods.","sentences":["Tensor cross interpolation (TCI) is a powerful technique for learning a tensor train (TT) by adaptively sampling a target tensor based on an interpolation formula.","However, when the tensor evaluations contain random noise, optimizing the TT is more advantageous than interpolating the noise.","Here, we propose a new method that starts with an initial guess of TT and optimizes it using non-linear least-squares by fitting it to measured points obtained from TCI.","We use quantics TCI (QTCI) in this method and demonstrate its effectiveness on sine and two-time correlation functions, with each evaluated with random noise.","The resulting TT exhibits increased robustness against noise compared to the QTCI method.","Furthermore, we employ this optimized TT of the correlation function in quantum simulation based on pseudo-imaginary-time evolution, resulting in ground-state energy with higher accuracy than the QTCI or Monte Carlo methods."],"url":"http://arxiv.org/abs/2405.12730v1","category":"quant-ph"}
{"created":"2024-05-21 11:52:14","title":"Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting","abstract":"This work addresses the need for a balanced approach between performance and efficiency in scalable production environments for visually-rich document understanding (VDU) tasks. Currently, there is a reliance on large document foundation models that offer advanced capabilities but come with a heavy computational burden. In this paper, we propose a multimodal early exit (EE) model design that incorporates various training strategies, exit layer types and placements. Our goal is to achieve a Pareto-optimal balance between predictive performance and efficiency for multimodal document image classification. Through a comprehensive set of experiments, we compare our approach with traditional exit policies and showcase an improved performance-efficiency trade-off. Our multimodal EE design preserves the model's predictive capabilities, enhancing both speed and latency. This is achieved through a reduction of over 20% in latency, while fully retaining the baseline accuracy. This research represents the first exploration of multimodal EE design within the VDU community, highlighting as well the effectiveness of calibration in improving confidence scores for exiting at different layers. Overall, our findings contribute to practical VDU applications by enhancing both performance and efficiency.","sentences":["This work addresses the need for a balanced approach between performance and efficiency in scalable production environments for visually-rich document understanding (VDU) tasks.","Currently, there is a reliance on large document foundation models that offer advanced capabilities but come with a heavy computational burden.","In this paper, we propose a multimodal early exit (EE) model design that incorporates various training strategies, exit layer types and placements.","Our goal is to achieve a Pareto-optimal balance between predictive performance and efficiency for multimodal document image classification.","Through a comprehensive set of experiments, we compare our approach with traditional exit policies and showcase an improved performance-efficiency trade-off.","Our multimodal EE design preserves the model's predictive capabilities, enhancing both speed and latency.","This is achieved through a reduction of over 20% in latency, while fully retaining the baseline accuracy.","This research represents the first exploration of multimodal EE design within the VDU community, highlighting as well the effectiveness of calibration in improving confidence scores for exiting at different layers.","Overall, our findings contribute to practical VDU applications by enhancing both performance and efficiency."],"url":"http://arxiv.org/abs/2405.12705v1","category":"cs.CV"}
{"created":"2024-05-21 09:41:56","title":"Multiscale lubrication simulation based on fourier feature networks with trainable frequency","abstract":"Rough surface lubrication simulation is crucial for designing and optimizing tribological performance. Despite the growing application of Physical Information Neural Networks (PINNs) in hydrodynamic lubrication analysis, their use has been primarily limited to smooth surfaces. This is due to traditional PINN methods suffer from spectral bias, favoring to learn low-frequency features and thus failing to analyze rough surfaces with high-frequency signals. To date, no PINN methods have been reported for rough surface lubrication. To overcome these limitations, this work introduces a novel multi-scale lubrication neural network architecture that utilizes a trainable Fourier feature network. By incorporating learnable feature embedding frequencies, this architecture automatically adapts to various frequency components, thereby enhancing the analysis of rough surface characteristics. This method has been tested across multiple surface morphologies, and the results have been compared with those obtained using the finite element method (FEM). The comparative analysis demonstrates that this approach achieves a high consistency with FEM results. Furthermore, this novel architecture surpasses traditional Fourier feature networks with fixed feature embedding frequencies in both accuracy and computational efficiency. Consequently, the multi-scale lubrication neural network model offers a more efficient tool for rough surface lubrication analysis.","sentences":["Rough surface lubrication simulation is crucial for designing and optimizing tribological performance.","Despite the growing application of Physical Information Neural Networks (PINNs) in hydrodynamic lubrication analysis, their use has been primarily limited to smooth surfaces.","This is due to traditional PINN methods suffer from spectral bias, favoring to learn low-frequency features and thus failing to analyze rough surfaces with high-frequency signals.","To date, no PINN methods have been reported for rough surface lubrication.","To overcome these limitations, this work introduces a novel multi-scale lubrication neural network architecture that utilizes a trainable Fourier feature network.","By incorporating learnable feature embedding frequencies, this architecture automatically adapts to various frequency components, thereby enhancing the analysis of rough surface characteristics.","This method has been tested across multiple surface morphologies, and the results have been compared with those obtained using the finite element method (FEM).","The comparative analysis demonstrates that this approach achieves a high consistency with FEM results.","Furthermore, this novel architecture surpasses traditional Fourier feature networks with fixed feature embedding frequencies in both accuracy and computational efficiency.","Consequently, the multi-scale lubrication neural network model offers a more efficient tool for rough surface lubrication analysis."],"url":"http://arxiv.org/abs/2405.12638v1","category":"cs.LG"}
{"created":"2024-05-21 09:41:42","title":"The Discovery and Follow-up of Four Transiting Short-period Sub-Neptunes Orbiting M dwarfs","abstract":"Sub-Neptunes with $2-3R_\\oplus$ are intermediate in size between rocky planets and Neptune-sized planets. The orbital properties and bulk compositions of transiting sub-Neptunes provide clues to the formation and evolution of close-in small planets. In this paper, we present the discovery and follow-up of four sub-Neptunes orbiting M dwarfs (TOI-782, TOI-1448, TOI-2120, and TOI-2406), three of which were newly validated by ground-based follow-up observations and statistical analyses. TOI-782 b, TOI-1448 b, TOI-2120 b, and TOI-2406 b have radii of $R_\\mathrm{p} = 2.740^{+0.082}_{-0.079}\\,R_\\oplus$, $2.769^{+0.073}_{-0.068}\\,R_\\oplus$, $2.120\\pm0.067\\,R_\\oplus$, and $2.830^{+0.068}_{-0.066}\\,R_\\oplus$ and orbital periods of $P = 8.02$, $8.11$, $5.80$, and $3.08$\\,days, respectively. Doppler monitoring with Subaru/InfraRed Doppler instrument led to 2$\\sigma$ upper limits on the masses of $<19.1\\ M_\\oplus$, $<19.5\\ M_\\oplus$, $<6.8\\ M_\\oplus$, and $<15.6\\ M_\\oplus$ for TOI-782 b, TOI-1448 b, TOI-2120 b, and TOI-2406 b, respectively. The mass-radius relationship of these four sub-Neptunes testifies to the existence of volatile material in their interiors. These four sub-Neptunes, which are located above the so-called ``radius valley'', are likely to retain a significant atmosphere and/or an icy mantle on the core, such as a water world. We find that at least three of the four sub-Neptunes (TOI-782 b, TOI-2120 b, and TOI-2406 b) orbiting M dwarfs older than 1 Gyr, are likely to have eccentricities of $e \\sim 0.2-0.3$. The fact that tidal circularization of their orbits is not achieved over 1 Gyr suggests inefficient tidal dissipation in their interiors.","sentences":["Sub-Neptunes with $2-3R_\\oplus$ are intermediate in size between rocky planets and Neptune-sized planets.","The orbital properties and bulk compositions of transiting sub-Neptunes provide clues to the formation and evolution of close-in small planets.","In this paper, we present the discovery and follow-up of four sub-Neptunes orbiting M dwarfs (TOI-782, TOI-1448, TOI-2120, and TOI-2406), three of which were newly validated by ground-based follow-up observations and statistical analyses.","TOI-782 b, TOI-1448 b, TOI-2120 b, and TOI-2406 b have radii of $R_\\mathrm{p} = 2.740^{+0.082}_{-0.079}\\,R_\\oplus$, $2.769^{+0.073}_{-0.068}\\,R_\\oplus$, $2.120\\pm0.067\\,R_\\oplus$, and $2.830^{+0.068}_{-0.066}\\,R_\\oplus$ and orbital periods of $P = 8.02$, $8.11$, $5.80$, and $3.08$\\,days, respectively.","Doppler monitoring with Subaru/InfraRed Doppler instrument led to 2$\\sigma$ upper limits on the masses of $<19.1\\ M_\\oplus$, $<19.5\\ M_\\oplus$, $<6.8\\ M_\\oplus$, and $<15.6\\ M_\\oplus$ for TOI-782 b, TOI-1448 b, TOI-2120 b, and TOI-2406 b, respectively.","The mass-radius relationship of these four sub-Neptunes testifies to the existence of volatile material in their interiors.","These four sub-Neptunes, which are located above the so-called ``radius valley'', are likely to retain a significant atmosphere and/or an icy mantle on the core, such as a water world.","We find that at least three of the four sub-Neptunes (TOI-782 b, TOI-2120 b, and TOI-2406 b) orbiting M dwarfs older than 1 Gyr, are likely to have eccentricities of $e \\sim 0.2-0.3$.","The fact that tidal circularization of their orbits is not achieved over 1 Gyr suggests inefficient tidal dissipation in their interiors."],"url":"http://arxiv.org/abs/2405.12637v1","category":"astro-ph.EP"}
{"created":"2024-05-21 09:25:58","title":"Deep ReLU Neural Network Emulation in High-Frequency Acoustic Scattering","abstract":"We obtain wavenumber-robust error bounds for the deep neural network (DNN) emulation of the solution to the time-harmonic, sound-soft acoustic scattering problem in the exterior of a smooth, convex obstacle in two physical dimensions. The error bounds are based on a boundary reduction of the scattering problem in the unbounded exterior region to its smooth, curved boundary $\\Gamma$ using the so-called combined field integral equation (CFIE), a well-posed, second-kind boundary integral equation (BIE) for the field's Neumann datum on $\\Gamma$. In this setting, the continuity and stability constants of this formulation are explicit in terms of the (non-dimensional) wavenumber $\\kappa$. Using wavenumber-explicit asymptotics of the problem's Neumann datum, we analyze the DNN approximation rate for this problem. We use fully connected NNs of the feed-forward type with Rectified Linear Unit (ReLU) activation. Through a constructive argument we prove the existence of DNNs with an $\\epsilon$-error bound in the $L^\\infty(\\Gamma)$-norm having a small, fixed width and a depth that increases $\\textit{spectrally}$ with the target accuracy $\\epsilon>0$. We show that for fixed $\\epsilon>0$, the depth of these NNs should increase $\\textit{poly-logarithmically}$ with respect to the wavenumber $\\kappa$ whereas the width of the NN remains fixed. Unlike current computational approaches, such as wavenumber-adapted versions of the Galerkin Boundary Element Method (BEM) with shape- and wavenumber-tailored solution $\\textit{ansatz}$ spaces, our DNN approximations do not require any prior analytic information about the scatterer's shape.","sentences":["We obtain wavenumber-robust error bounds for the deep neural network (DNN) emulation of the solution to the time-harmonic, sound-soft acoustic scattering problem in the exterior of a smooth, convex obstacle in two physical dimensions.","The error bounds are based on a boundary reduction of the scattering problem in the unbounded exterior region to its smooth, curved boundary $\\Gamma$ using the so-called combined field integral equation (CFIE), a well-posed, second-kind boundary integral equation (BIE) for the field's Neumann datum on $\\Gamma$. In this setting, the continuity and stability constants of this formulation are explicit in terms of the (non-dimensional) wavenumber $\\kappa$.","Using wavenumber-explicit asymptotics of the problem's Neumann datum, we analyze the DNN approximation rate for this problem.","We use fully connected NNs of the feed-forward type with Rectified Linear Unit (ReLU) activation.","Through a constructive argument we prove the existence of DNNs with an $\\epsilon$-error bound in the $L^\\infty(\\Gamma)$-norm having a small, fixed width and a depth that increases $\\textit{spectrally}$ with the target accuracy $\\epsilon>0$. We show that for fixed $\\epsilon>0$, the depth of these NNs should increase $\\textit{poly-logarithmically}$ with respect to the wavenumber $\\kappa$ whereas the width of the NN remains fixed.","Unlike current computational approaches, such as wavenumber-adapted versions of the Galerkin Boundary Element Method (BEM) with shape- and wavenumber-tailored solution $\\textit{ansatz}$ spaces, our DNN approximations do not require any prior analytic information about the scatterer's shape."],"url":"http://arxiv.org/abs/2405.12624v1","category":"math.NA"}
{"created":"2024-05-21 08:27:35","title":"Is Dataset Quality Still a Concern in Diagnosis Using Large Foundation Model?","abstract":"Recent advancements in pre-trained large foundation models (LFM) have yielded significant breakthroughs across various domains, including natural language processing and computer vision. These models have been particularly impactful in the domain of medical diagnostic tasks. With abundant unlabeled data, an LFM has been developed for fundus images using the Vision Transformer (VIT) and a self-supervised learning framework. This LFM has shown promising performance in fundus disease diagnosis across multiple datasets. On the other hand, deep learning models have long been challenged by dataset quality issues, such as image quality and dataset bias. To investigate the influence of data quality on LFM, we conducted explorations in two fundus diagnosis tasks using datasets of varying quality. Specifically, we explored the following questions: Is LFM more robust to image quality? Is LFM affected by dataset bias? Can fine-tuning techniques alleviate these effects? Our investigation found that LFM exhibits greater resilience to dataset quality issues, including image quality and dataset bias, compared to typical convolutional networks. Furthermore, we discovered that overall fine-tuning is an effective adapter for LFM to mitigate the impact of dataset quality issues.","sentences":["Recent advancements in pre-trained large foundation models (LFM) have yielded significant breakthroughs across various domains, including natural language processing and computer vision.","These models have been particularly impactful in the domain of medical diagnostic tasks.","With abundant unlabeled data, an LFM has been developed for fundus images using the Vision Transformer (VIT) and a self-supervised learning framework.","This LFM has shown promising performance in fundus disease diagnosis across multiple datasets.","On the other hand, deep learning models have long been challenged by dataset quality issues, such as image quality and dataset bias.","To investigate the influence of data quality on LFM, we conducted explorations in two fundus diagnosis tasks using datasets of varying quality.","Specifically, we explored the following questions: Is LFM more robust to image quality?","Is LFM affected by dataset bias?","Can fine-tuning techniques alleviate these effects?","Our investigation found that LFM exhibits greater resilience to dataset quality issues, including image quality and dataset bias, compared to typical convolutional networks.","Furthermore, we discovered that overall fine-tuning is an effective adapter for LFM to mitigate the impact of dataset quality issues."],"url":"http://arxiv.org/abs/2405.12584v1","category":"eess.IV"}
{"created":"2024-05-21 08:26:38","title":"Carbon-aware Software Services","abstract":"The significant carbon footprint of the ICT sector calls for methodologies to contain carbon emissions of running software. This article proposes a novel framework for implementing, configuring and assessing carbon-aware interactive software services. First, we propose a methodology to implement carbon-aware services leveraging the Strategy design pattern to feature alternative service versions with different energy consumption. Then, we devise a bilevel optimisation scheme to configure which version to use at different times of the day, based on forecasts of carbon intensity and service requests, pursuing the two-fold goal of minimising carbon emissions and maintaining average output quality above a desired set-point. Last, an open-source prototype of such optimisation scheme is used to configure a software service implemented as per our methodology and assessed against traditional non-adaptive implementations of the same service. Results show the capability of our framework to control the average quality of output results of carbon-aware services and to reduce carbon emissions from 8% to 50%.","sentences":["The significant carbon footprint of the ICT sector calls for methodologies to contain carbon emissions of running software.","This article proposes a novel framework for implementing, configuring and assessing carbon-aware interactive software services.","First, we propose a methodology to implement carbon-aware services leveraging the Strategy design pattern to feature alternative service versions with different energy consumption.","Then, we devise a bilevel optimisation scheme to configure which version to use at different times of the day, based on forecasts of carbon intensity and service requests, pursuing the two-fold goal of minimising carbon emissions and maintaining average output quality above a desired set-point.","Last, an open-source prototype of such optimisation scheme is used to configure a software service implemented as per our methodology and assessed against traditional non-adaptive implementations of the same service.","Results show the capability of our framework to control the average quality of output results of carbon-aware services and to reduce carbon emissions from 8% to 50%."],"url":"http://arxiv.org/abs/2405.12582v1","category":"cs.SE"}
{"created":"2024-05-21 07:16:20","title":"Orthogonally Initiated Particle Swarm Optimization with Advanced Mutation for Real-Parameter Optimization","abstract":"This article introduces an enhanced particle swarm optimizer (PSO), termed Orthogonal PSO with Mutation (OPSO-m). Initially, it proposes an orthogonal array-based learning approach to cultivate an improved initial swarm for PSO, significantly boosting the adaptability of swarm-based optimization algorithms. The article further presents archive-based self-adaptive learning strategies, dividing the population into regular and elite subgroups. Each subgroup employs distinct learning mechanisms. The regular group utilizes efficient learning schemes derived from three unique archives, which categorize individuals based on their quality levels. Additionally, a mutation strategy is implemented to update the positions of elite individuals. Comparative studies are conducted to assess the effectiveness of these learning strategies in OPSO-m, evaluating its optimization capacity through exploration-exploitation dynamics and population diversity analysis. The proposed OPSO-m model is tested on real-parameter challenges from the CEC 2017 suite in 10, 30, 50, and 100-dimensional search spaces, with its results compared to contemporary state-of-the-art algorithms using a sensitivity metric. OPSO-m exhibits distinguished performance in the precision of solutions, rapidity of convergence, efficiency in search, and robust stability, thus highlighting its superior aptitude for resolving intricate optimization issues.","sentences":["This article introduces an enhanced particle swarm optimizer (PSO), termed Orthogonal PSO with Mutation (OPSO-m).","Initially, it proposes an orthogonal array-based learning approach to cultivate an improved initial swarm for PSO, significantly boosting the adaptability of swarm-based optimization algorithms.","The article further presents archive-based self-adaptive learning strategies, dividing the population into regular and elite subgroups.","Each subgroup employs distinct learning mechanisms.","The regular group utilizes efficient learning schemes derived from three unique archives, which categorize individuals based on their quality levels.","Additionally, a mutation strategy is implemented to update the positions of elite individuals.","Comparative studies are conducted to assess the effectiveness of these learning strategies in OPSO-m, evaluating its optimization capacity through exploration-exploitation dynamics and population diversity analysis.","The proposed OPSO-m model is tested on real-parameter challenges from the CEC 2017 suite in 10, 30, 50, and 100-dimensional search spaces, with its results compared to contemporary state-of-the-art algorithms using a sensitivity metric.","OPSO-m exhibits distinguished performance in the precision of solutions, rapidity of convergence, efficiency in search, and robust stability, thus highlighting its superior aptitude for resolving intricate optimization issues."],"url":"http://arxiv.org/abs/2405.12542v1","category":"cs.NE"}
{"created":"2024-05-21 04:41:16","title":"Why does the elephant random walk behave as the randomized play-the-winner rule?","abstract":"The randomized play-the-winner rule (RPW) is a response-adaptive design proposed by Wei and Durham (1978) for sequentially randomizing patients to treatments in a two-treatment clinical trial so that more patients are assigned to the better treatment as the clinical trial goes on. The elephant random walk (ERW) proposed by Schutz and Trimper (2004) is a non-Markovian discrete-time random walk on $\\mathbb Z$ which has a link to a famous saying that elephants can always remember where they have been. The asymptotic behaviors of RPW rule and ERW have been studied in litterateurs independently, and their asymptotic behaviors are very similar. In this paper, we show that the RPW rule is a biased ERW and link them with the recursive stochastic algorithm. With the help of a recursive stochastic algorithm, we obtain the strong invariance principle of the ERW as well as multi-dimensional ERW with random step sizes. By the strong invariance principle, the central limit theorem and precise law of the iterated logarithm are obtained for both the multi-dimensional ERW, the multi-dimensional ERW with random step sizes and their centers of mass.","sentences":["The randomized play-the-winner rule (RPW) is a response-adaptive design proposed by Wei and Durham (1978) for sequentially randomizing patients to treatments in a two-treatment clinical trial so that more patients are assigned to the better treatment as the clinical trial goes on.","The elephant random walk (ERW) proposed by Schutz and Trimper (2004) is a non-Markovian discrete-time random walk on $\\mathbb Z$ which has a link to a famous saying that elephants can always remember where they have been.","The asymptotic behaviors of RPW rule and ERW have been studied in litterateurs independently, and their asymptotic behaviors are very similar.","In this paper, we show that the RPW rule is a biased ERW and link them with the recursive stochastic algorithm.","With the help of a recursive stochastic algorithm, we obtain the strong invariance principle of the ERW as well as multi-dimensional ERW with random step sizes.","By the strong invariance principle, the central limit theorem and precise law of the iterated logarithm are obtained for both the multi-dimensional ERW, the multi-dimensional ERW with random step sizes and their centers of mass."],"url":"http://arxiv.org/abs/2405.12495v1","category":"math.PR"}
{"created":"2024-05-21 03:28:45","title":"How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing","abstract":"Spectral Graph Neural Networks (GNNs), alternatively known as graph filters, have gained increasing prevalence for heterophily graphs. Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform. In an attempt to avert prohibitive computations, numerous polynomial filters have been proposed. However, polynomials in the majority of these filters are predefined and remain fixed across different graphs, failing to accommodate the varying degrees of heterophily. Addressing this gap, we demystify the intrinsic correlation between the spectral property of desired polynomial bases and the heterophily degrees via thorough theoretical analyses. Subsequently, we develop a novel adaptive heterophily basis wherein the basis vectors mutually form angles reflecting the heterophily degree of the graph. We integrate this heterophily basis with the homophily basis to construct a universal polynomial basis UniBasis, which devises a polynomial filter based graph neural network - UniFilter. It optimizes the convolution and propagation in GNN, thus effectively limiting over-smoothing and alleviating over-squashing. Our extensive experiments, conducted on a diverse range of real-world and synthetic datasets with varying degrees of heterophily, support the superiority of UniFilter. These results not only demonstrate the universality of UniBasis but also highlight its proficiency in graph explanation.","sentences":["Spectral Graph Neural Networks (GNNs), alternatively known as graph filters, have gained increasing prevalence for heterophily graphs.","Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform.","In an attempt to avert prohibitive computations, numerous polynomial filters have been proposed.","However, polynomials in the majority of these filters are predefined and remain fixed across different graphs, failing to accommodate the varying degrees of heterophily.","Addressing this gap, we demystify the intrinsic correlation between the spectral property of desired polynomial bases and the heterophily degrees via thorough theoretical analyses.","Subsequently, we develop a novel adaptive heterophily basis wherein the basis vectors mutually form angles reflecting the heterophily degree of the graph.","We integrate this heterophily basis with the homophily basis to construct a universal polynomial basis UniBasis, which devises a polynomial filter based graph neural network - UniFilter.","It optimizes the convolution and propagation in GNN, thus effectively limiting over-smoothing and alleviating over-squashing.","Our extensive experiments, conducted on a diverse range of real-world and synthetic datasets with varying degrees of heterophily, support the superiority of UniFilter.","These results not only demonstrate the universality of UniBasis but also highlight its proficiency in graph explanation."],"url":"http://arxiv.org/abs/2405.12474v1","category":"cs.LG"}
{"created":"2024-05-21 02:33:17","title":"PLM4Traj: Cognizing Movement Patterns and Travel Purposes from Trajectories with Pre-trained Language Models","abstract":"Spatio-temporal trajectories play a vital role in various spatio-temporal data mining tasks. Developing a versatile trajectory learning approach that can adapt to different tasks while ensuring high accuracy is crucial. This requires effectively extracting movement patterns and travel purposes embedded in trajectories. However, this task is challenging due to limitations in the size and quality of available trajectory datasets. On the other hand, pre-trained language models (PLMs) have shown great success in adapting to different tasks by training on large-scale, high-quality corpus datasets. Given the similarities between trajectories and sentences, there is potential in leveraging PLMs to enhance the development of a versatile and effective trajectory learning method. Nevertheless, vanilla PLMs are not tailored to handle the unique spatio-temporal features present in trajectories and lack the capability to extract movement patterns and travel purposes from them.   To overcome these obstacles, we propose a model called PLM4Traj that effectively utilizes PLMs to model trajectories. PLM4Traj leverages the strengths of PLMs to create a versatile trajectory learning approach while addressing the limitations of vanilla PLMs in modeling trajectories. Firstly, PLM4Traj incorporates a novel trajectory semantic embedder that enables PLMs to process spatio-temporal features in trajectories and extract movement patterns and travel purposes from them. Secondly, PLM4Traj introduces a novel trajectory prompt that integrates movement patterns and travel purposes into PLMs, while also allowing the model to adapt to various tasks. Extensive experiments conducted on two real-world datasets and two representative tasks demonstrate that PLM4Traj successfully achieves its design goals. Codes are available at https://github.com/Zeru19/PLM4Traj.","sentences":["Spatio-temporal trajectories play a vital role in various spatio-temporal data mining tasks.","Developing a versatile trajectory learning approach that can adapt to different tasks while ensuring high accuracy is crucial.","This requires effectively extracting movement patterns and travel purposes embedded in trajectories.","However, this task is challenging due to limitations in the size and quality of available trajectory datasets.","On the other hand, pre-trained language models (PLMs) have shown great success in adapting to different tasks by training on large-scale, high-quality corpus datasets.","Given the similarities between trajectories and sentences, there is potential in leveraging PLMs to enhance the development of a versatile and effective trajectory learning method.","Nevertheless, vanilla PLMs are not tailored to handle the unique spatio-temporal features present in trajectories and lack the capability to extract movement patterns and travel purposes from them.   ","To overcome these obstacles, we propose a model called PLM4Traj that effectively utilizes PLMs to model trajectories.","PLM4Traj leverages the strengths of PLMs to create a versatile trajectory learning approach while addressing the limitations of vanilla PLMs in modeling trajectories.","Firstly, PLM4Traj incorporates a novel trajectory semantic embedder that enables PLMs to process spatio-temporal features in trajectories and extract movement patterns and travel purposes from them.","Secondly, PLM4Traj introduces a novel trajectory prompt that integrates movement patterns and travel purposes into PLMs, while also allowing the model to adapt to various tasks.","Extensive experiments conducted on two real-world datasets and two representative tasks demonstrate that PLM4Traj successfully achieves its design goals.","Codes are available at https://github.com/Zeru19/PLM4Traj."],"url":"http://arxiv.org/abs/2405.12459v1","category":"cs.LG"}
{"created":"2024-05-21 01:55:33","title":"EPL: Empirical Prototype Learning for Deep Face Recognition","abstract":"Prototype learning is widely used in face recognition, which takes the row vectors of coefficient matrix in the last linear layer of the feature extraction model as the prototypes for each class. When the prototypes are updated using the facial sample feature gradients in the model training, they are prone to being pulled away from the class center by the hard samples, resulting in decreased overall model performance. In this paper, we explicitly define prototypes as the expectations of sample features in each class and design the empirical prototypes using the existing samples in the dataset. We then devise a strategy to adaptively update these empirical prototypes during the model training based on the similarity between the sample features and the empirical prototypes. Furthermore, we propose an empirical prototype learning (EPL) method, which utilizes an adaptive margin parameter with respect to sample features. EPL assigns larger margins to the normal samples and smaller margins to the hard samples, allowing the learned empirical prototypes to better reflect the class center dominated by the normal samples and finally pull the hard samples towards the empirical prototypes through the learning. The extensive experiments on MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace demonstrate the effectiveness of EPL. Our code is available at $\\href{https://github.com/WakingHours-GitHub/EPL}{https://github.com/WakingHours-GitHub/EPL}$.","sentences":["Prototype learning is widely used in face recognition, which takes the row vectors of coefficient matrix in the last linear layer of the feature extraction model as the prototypes for each class.","When the prototypes are updated using the facial sample feature gradients in the model training, they are prone to being pulled away from the class center by the hard samples, resulting in decreased overall model performance.","In this paper, we explicitly define prototypes as the expectations of sample features in each class and design the empirical prototypes using the existing samples in the dataset.","We then devise a strategy to adaptively update these empirical prototypes during the model training based on the similarity between the sample features and the empirical prototypes.","Furthermore, we propose an empirical prototype learning (EPL) method, which utilizes an adaptive margin parameter with respect to sample features.","EPL assigns larger margins to the normal samples and smaller margins to the hard samples, allowing the learned empirical prototypes to better reflect the class center dominated by the normal samples and finally pull the hard samples towards the empirical prototypes through the learning.","The extensive experiments on MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace demonstrate the effectiveness of EPL.","Our code is available at $\\href{https://github.com/WakingHours-GitHub/EPL}{https://github.com/WakingHours-GitHub/EPL}$."],"url":"http://arxiv.org/abs/2405.12447v1","category":"cs.CV"}
{"created":"2024-05-21 01:33:36","title":"Subgrid scale modeling of droplet bag breakup in VOF simulations","abstract":"The mesh-dependency of the breakup of liquid films, including their breakup length scales and resulting drop size distributions, has long been an obstacle inhibiting the computational modeling of large-scale spray systems. With the aim of overcoming this barrier, this work presents a framework for the prediction and modeling of subgrid-thickness liquid film formation and breakup within two-phase simulations using the volume of fluid method. A two-plane interface reconstruction is used to capture the development of liquid films as their thickness decreases below the mesh size. The breakup of the film is predicted with a semi-analytical model that incorporates the film geometry captured through the two-plane reconstruction. The framework is validated against experiments of the bag breakup of a liquid drop at $\\text{We}=13.8$ through the comparison of the resulting drop size and velocity distributions. The generated distributions show good agreement with experimental results for drop resolutions as low as 25.6 cells across the initial diameter. The presented framework enables these drop breakup simulations to be performed at a computational cost three orders of magnitude lower than the cost of simulations utilizing adaptive mesh refinement.","sentences":["The mesh-dependency of the breakup of liquid films, including their breakup length scales and resulting drop size distributions, has long been an obstacle inhibiting the computational modeling of large-scale spray systems.","With the aim of overcoming this barrier, this work presents a framework for the prediction and modeling of subgrid-thickness liquid film formation and breakup within two-phase simulations using the volume of fluid method.","A two-plane interface reconstruction is used to capture the development of liquid films as their thickness decreases below the mesh size.","The breakup of the film is predicted with a semi-analytical model that incorporates the film geometry captured through the two-plane reconstruction.","The framework is validated against experiments of the bag breakup of a liquid drop at $\\text{We}=13.8$ through the comparison of the resulting drop size and velocity distributions.","The generated distributions show good agreement with experimental results for drop resolutions as low as 25.6 cells across the initial diameter.","The presented framework enables these drop breakup simulations to be performed at a computational cost three orders of magnitude lower than the cost of simulations utilizing adaptive mesh refinement."],"url":"http://arxiv.org/abs/2405.12441v1","category":"physics.flu-dyn"}
{"created":"2024-05-21 01:19:52","title":"Resolving Word Vagueness with Scenario-guided Adapter for Natural Language Inference","abstract":"Natural Language Inference (NLI) is a crucial task in natural language processing that involves determining the relationship between two sentences, typically referred to as the premise and the hypothesis. However, traditional NLI models solely rely on the semantic information inherent in independent sentences and lack relevant situational visual information, which can hinder a complete understanding of the intended meaning of the sentences due to the ambiguity and vagueness of language. To address this challenge, we propose an innovative ScenaFuse adapter that simultaneously integrates large-scale pre-trained linguistic knowledge and relevant visual information for NLI tasks. Specifically, we first design an image-sentence interaction module to incorporate visuals into the attention mechanism of the pre-trained model, allowing the two modalities to interact comprehensively. Furthermore, we introduce an image-sentence fusion module that can adaptively integrate visual information from images and semantic information from sentences. By incorporating relevant visual information and leveraging linguistic knowledge, our approach bridges the gap between language and vision, leading to improved understanding and inference capabilities in NLI tasks. Extensive benchmark experiments demonstrate that our proposed ScenaFuse, a scenario-guided approach, consistently boosts NLI performance.","sentences":["Natural Language Inference (NLI) is a crucial task in natural language processing that involves determining the relationship between two sentences, typically referred to as the premise and the hypothesis.","However, traditional NLI models solely rely on the semantic information inherent in independent sentences and lack relevant situational visual information, which can hinder a complete understanding of the intended meaning of the sentences due to the ambiguity and vagueness of language.","To address this challenge, we propose an innovative ScenaFuse adapter that simultaneously integrates large-scale pre-trained linguistic knowledge and relevant visual information for NLI tasks.","Specifically, we first design an image-sentence interaction module to incorporate visuals into the attention mechanism of the pre-trained model, allowing the two modalities to interact comprehensively.","Furthermore, we introduce an image-sentence fusion module that can adaptively integrate visual information from images and semantic information from sentences.","By incorporating relevant visual information and leveraging linguistic knowledge, our approach bridges the gap between language and vision, leading to improved understanding and inference capabilities in NLI tasks.","Extensive benchmark experiments demonstrate that our proposed ScenaFuse, a scenario-guided approach, consistently boosts NLI performance."],"url":"http://arxiv.org/abs/2405.12434v1","category":"cs.CL"}
{"created":"2024-05-21 00:57:46","title":"On the Origin of Fast Rotating Stars. I. Photometric calibration and results of AO-assisted BVRI+Halpha imaging of NGC330 with SAMI/SOAR","abstract":"H$\\alpha$ emission is a clear indicator of circumstellar activity in Be stars, historically employed to assess the classical Be star (CBe) population in young open clusters (YOCs). The YOC NGC330 in the Small Magellanic Cloud exhibits a large known fraction of {CBe} stars and was selected for a pilot study to establish a comprehensive methodology for identifying H$\\alpha$ emitters in the Magellanic Clouds, encompassing the entire B-type spectral range. Using the SOAR Adaptative Module Imager (SAMI), we investigated the stellar population of NGC330 using multi-band BVRI+H$\\alpha$ imaging. We identified H$\\alpha$ emitters within the entire V-band range covered by SAMI/SOAR observations ($V\\lesssim22$), comprising the complete B-type stellar population and offering a unique opportunity to explore the Be phenomenon across all spectral sub-classes. The stellar radial distribution shows a clear bimodal pattern between the most massive (B5 or earlier) and the lower-mass main-sequence objects (later than B6) within the cluster. The former is concentrated towards the cluster center (showing a dispersion of $\\sigma=4.26\\pm0.20$ pc), whereas the latter extends across larger radii ($\\sigma=10.83\\pm0.65$ pc), indicating mass stratification within NGC330. The total fraction of emitters is $4.4\\pm0.5\\%$, notably smaller than previous estimates from flux- or seeing-limited observations. However, a higher fraction of H$\\alpha$ emitters is observed among higher-mass stars ($32.8\\pm3.4\\%$) than within lower-mass ($4.4\\pm0.9\\%$). Consequently, the putative CBe population exhibits distinct dynamical characteristics compared to the bulk of the stellar population in NGC330. These findings highlight the significance of the current observations in providing a complete picture of the CBe population in NGC330.","sentences":["H$\\alpha$ emission is a clear indicator of circumstellar activity in Be stars, historically employed to assess the classical Be star (CBe) population in young open clusters (YOCs).","The YOC NGC330 in the Small Magellanic Cloud exhibits a large known fraction of {CBe} stars and was selected for a pilot study to establish a comprehensive methodology for identifying H$\\alpha$ emitters in the Magellanic Clouds, encompassing the entire B-type spectral range.","Using the SOAR Adaptative Module Imager (SAMI), we investigated the stellar population of NGC330 using multi-band BVRI+H$\\alpha$ imaging.","We identified H$\\alpha$ emitters within the entire V-band range covered by SAMI/SOAR observations ($V\\lesssim22$), comprising the complete B-type stellar population and offering a unique opportunity to explore the Be phenomenon across all spectral sub-classes.","The stellar radial distribution shows a clear bimodal pattern between the most massive (B5 or earlier) and the lower-mass main-sequence objects (later than B6) within the cluster.","The former is concentrated towards the cluster center (showing a dispersion of $\\sigma=4.26\\pm0.20$ pc), whereas the latter extends across larger radii ($\\sigma=10.83\\pm0.65$ pc), indicating mass stratification within NGC330.","The total fraction of emitters is $4.4\\pm0.5\\%$, notably smaller than previous estimates from flux- or seeing-limited observations.","However, a higher fraction of H$\\alpha$ emitters is observed among higher-mass stars ($32.8\\pm3.4\\%$) than within lower-mass ($4.4\\pm0.9\\%$).","Consequently, the putative CBe population exhibits distinct dynamical characteristics compared to the bulk of the stellar population in NGC330.","These findings highlight the significance of the current observations in providing a complete picture of the CBe population in NGC330."],"url":"http://arxiv.org/abs/2405.12429v1","category":"astro-ph.GA"}
{"created":"2024-05-21 00:36:34","title":"Deep learning approaches to indoor wireless channel estimation for low-power communication","abstract":"In the rapidly growing development of the Internet of Things (IoT) infrastructure, achieving reliable wireless communication is a challenge. IoT devices operate in diverse environments with common signal interference and fluctuating channel conditions. Accurate channel estimation helps adapt the transmission strategies to current conditions, ensuring reliable communication. Traditional methods, such as Least Squares (LS) and Minimum Mean Squared Error (MMSE) estimation techniques, often struggle to adapt to the diverse and complex environments typical of IoT networks. This research article delves into the potential of Deep Learning (DL) to enhance channel estimation, focusing on the Received Signal Strength Indicator (RSSI) metric - a critical yet challenging aspect due to its susceptibility to noise and environmental factors. This paper presents two Fully Connected Neural Networks (FCNNs)-based Low Power (LP-IoT) channel estimation models, leveraging RSSI for accurate channel estimation in LP-IoT communication. Our Model A exhibits a remarkable 99.02% reduction in Mean Squared Error (MSE), and Model B demonstrates a notable 90.03% MSE reduction compared to the benchmarks set by current studies. Additionally, the comparative studies of our model A with other DL-based techniques show significant efficiency in our estimation models.","sentences":["In the rapidly growing development of the Internet of Things (IoT) infrastructure, achieving reliable wireless communication is a challenge.","IoT devices operate in diverse environments with common signal interference and fluctuating channel conditions.","Accurate channel estimation helps adapt the transmission strategies to current conditions, ensuring reliable communication.","Traditional methods, such as Least Squares (LS) and Minimum Mean Squared Error (MMSE) estimation techniques, often struggle to adapt to the diverse and complex environments typical of IoT networks.","This research article delves into the potential of Deep Learning (DL) to enhance channel estimation, focusing on the Received Signal Strength Indicator (RSSI) metric - a critical yet challenging aspect due to its susceptibility to noise and environmental factors.","This paper presents two Fully Connected Neural Networks (FCNNs)-based Low Power (LP-IoT) channel estimation models, leveraging RSSI for accurate channel estimation in LP-IoT communication.","Our Model A exhibits a remarkable 99.02% reduction in Mean Squared Error (MSE), and Model B demonstrates a notable 90.03% MSE reduction compared to the benchmarks set by current studies.","Additionally, the comparative studies of our model A with other DL-based techniques show significant efficiency in our estimation models."],"url":"http://arxiv.org/abs/2405.12427v1","category":"cs.LG"}
{"created":"2024-05-21 00:01:06","title":"Extraction of Human DNA from Soil: Protocol Adaptations","abstract":"PCR-based analysis of DNA is utilized in a wide variety of fields, including Forensic Science. Aside from the more common ample sources, material analyzed here can refer to specimen excavated from a soil environment, or a sampling of the soil itself to recover DNA leached into the soil from decomposing human remains or from body fluids intermingled with the soil in an outdoor crime scene. The common problematic of these types of sample is the presence of humic acids, which are a component of any soil environment, and when the co-extracted with the DNA, lead to inhibition of enzyme-based procedures including PCR.   While a variety of methods exist for the extraction of DNA from excavated skeletal remains, protocols for extraction of DNA from the soil directly are usually targeting soil microorganism. To address the need for methodology suitable for extraction of human DNA from soil, a selection of three published protocols were adapted for this purpose, to be tested and evaluated using standardized samples. The resulting protocols are presented here.","sentences":["PCR-based analysis of DNA is utilized in a wide variety of fields, including Forensic Science.","Aside from the more common ample sources, material analyzed here can refer to specimen excavated from a soil environment, or a sampling of the soil itself to recover DNA leached into the soil from decomposing human remains or from body fluids intermingled with the soil in an outdoor crime scene.","The common problematic of these types of sample is the presence of humic acids, which are a component of any soil environment, and when the co-extracted with the DNA, lead to inhibition of enzyme-based procedures including PCR.   ","While a variety of methods exist for the extraction of DNA from excavated skeletal remains, protocols for extraction of DNA from the soil directly are usually targeting soil microorganism.","To address the need for methodology suitable for extraction of human DNA from soil, a selection of three published protocols were adapted for this purpose, to be tested and evaluated using standardized samples.","The resulting protocols are presented here."],"url":"http://arxiv.org/abs/2405.12422v1","category":"q-bio.QM"}
{"created":"2024-05-20 23:27:47","title":"HD 110067 c has an aligned orbit","abstract":"Planetary systems in mean motion resonances hold a special place among the planetary population. They allow us to study planet formation in great detail as dissipative processes are thought to have played an important role in their existence. Additionally, planetary masses in bright resonant systems may be independently measured both by radial velocities (RVs) and transit timing variations (TTVs). In principle, they also allow us to quickly determine the inclination of all planets in the system, as for the system to be stable, they are likely all in coplanar orbits. To describe the full dynamical state of the system, we also need the stellar obliquity that provides the orbital alignment of a planet with respect to the spin of their host star and can be measured thanks to the Rossiter-McLaughlin effect. It was recently discovered that HD 110067 harbours a system of six sub-Neptunes in resonant chain orbits. We here analyze an ESPRESSO high-resolution spectroscopic time series of HD 110067 during the transit of planet c. We find the orbit of HD 110067 c to be well aligned with sky projected obliquity $\\lambda =6^{+24}_{-26}$ deg. This result is indicative that the current architecture of the system has been reached through convergent migration without any major disruptive events. Finally, we report transit-timing variation in this system as we find a significant offset of 19 $\\pm$ 4 minutes in the center of the transit compared to the published ephemeris.","sentences":["Planetary systems in mean motion resonances hold a special place among the planetary population.","They allow us to study planet formation in great detail as dissipative processes are thought to have played an important role in their existence.","Additionally, planetary masses in bright resonant systems may be independently measured both by radial velocities (RVs) and transit timing variations (TTVs).","In principle, they also allow us to quickly determine the inclination of all planets in the system, as for the system to be stable, they are likely all in coplanar orbits.","To describe the full dynamical state of the system, we also need the stellar obliquity that provides the orbital alignment of a planet with respect to the spin of their host star and can be measured thanks to the Rossiter-McLaughlin effect.","It was recently discovered that HD 110067 harbours a system of six sub-Neptunes in resonant chain orbits.","We here analyze an ESPRESSO high-resolution spectroscopic time series of HD 110067 during the transit of planet c.","We find the orbit of HD 110067 c to be well aligned with sky projected obliquity $\\lambda =6^{+24}_{-26}$ deg.","This result is indicative that the current architecture of the system has been reached through convergent migration without any major disruptive events.","Finally, we report transit-timing variation in this system as we find a significant offset of 19 $\\pm$ 4 minutes in the center of the transit compared to the published ephemeris."],"url":"http://arxiv.org/abs/2405.12409v1","category":"astro-ph.EP"}
{"created":"2024-05-20 22:04:41","title":"Ultraclean suspended graphene by radiolysis of adsorbed water","abstract":"Access to intrinsic properties of a 2D material is challenging due to the absence of a bulk that would dominate over surface contamination, and this lack of bulk also precludes effective conventional cleaning methods that are almost always sacrificial. Suspended graphene and carbon contaminants represent the most salient challenge. This work has achieved ultraclean graphene, attested by electron energy loss (EEL) spectra unprecedentedly exhibiting fine-structure features expected from bonding and band structure. In the cleaning process in a transmission electron microscope, radicals generated by radiolysis of intentionally adsorbed water remove organic contaminants, which would otherwise be feedstock of the notorious electron irradiation induced carbon deposition. This method can be readily adapted to other experimental settings and other materials, to enable previously inhibited undertakings that rely on the intrinsic properties or ultimate thinness of 2D materials. Importantly, the method is surprisingly simple and robust, easily implementable with common lab equipment.","sentences":["Access to intrinsic properties of a 2D material is challenging due to the absence of a bulk that would dominate over surface contamination, and this lack of bulk also precludes effective conventional cleaning methods that are almost always sacrificial.","Suspended graphene and carbon contaminants represent the most salient challenge.","This work has achieved ultraclean graphene, attested by electron energy loss (EEL) spectra unprecedentedly exhibiting fine-structure features expected from bonding and band structure.","In the cleaning process in a transmission electron microscope, radicals generated by radiolysis of intentionally adsorbed water remove organic contaminants, which would otherwise be feedstock of the notorious electron irradiation induced carbon deposition.","This method can be readily adapted to other experimental settings and other materials, to enable previously inhibited undertakings that rely on the intrinsic properties or ultimate thinness of 2D materials.","Importantly, the method is surprisingly simple and robust, easily implementable with common lab equipment."],"url":"http://arxiv.org/abs/2405.12395v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 21:40:08","title":"SciJava Ops: An Improved Algorithms Framework for Fiji and Beyond","abstract":"Many scientific software platforms provide plugin mechanisms that simplify the integration, deployment, and execution of externally developed functionality. One of the most widely used platforms in the imaging space is Fiji, a popular open-source application for scientific image analysis. Fiji incorporates and builds on the ImageJ and ImageJ2 platforms, which provide a powerful plugin architecture used by thousands of plugins to solve a wide variety of problems. This capability is a major part of Fiji's success, and it has become a widely used biological image analysis tool and a target for new functionality. However, a plugin-based software architecture cannot unify disparate platforms operating on incompatible data structures; interoperability necessitates the creation of adaptation or \"bridge\" layers to translate data and invoke functionality. As a result, while platforms like Fiji enable a high degree of interconnectivity and extensibility, they were not fundamentally designed to integrate across the many data types, programming languages, and architectural differences of various software platforms.To help address this challenge, we present SciJava Ops, a foundational software library for expressing algorithms as plugins in a unified and extensible way. Continuing the evolution of Fiji's SciJava plugin mechanism, SciJava Ops enables users to harness algorithms from various software platforms within a central execution environment. In addition, SciJava Ops automatically adapts data into the most appropriate structure for each algorithm, allowing users to freely and transparently combine algorithms from otherwise incompatible tools. While SciJava Ops is initially distributed as a Fiji update site, the framework does not require Fiji, ImageJ, or ImageJ2, and would be suitable for integration with additional image analysis platforms.","sentences":["Many scientific software platforms provide plugin mechanisms that simplify the integration, deployment, and execution of externally developed functionality.","One of the most widely used platforms in the imaging space is Fiji, a popular open-source application for scientific image analysis.","Fiji incorporates and builds on the ImageJ and ImageJ2 platforms, which provide a powerful plugin architecture used by thousands of plugins to solve a wide variety of problems.","This capability is a major part of Fiji's success, and it has become a widely used biological image analysis tool and a target for new functionality.","However, a plugin-based software architecture cannot unify disparate platforms operating on incompatible data structures; interoperability necessitates the creation of adaptation or \"bridge\" layers to translate data and invoke functionality.","As a result, while platforms like Fiji enable a high degree of interconnectivity and extensibility, they were not fundamentally designed to integrate across the many data types, programming languages, and architectural differences of various software platforms.","To help address this challenge, we present SciJava Ops, a foundational software library for expressing algorithms as plugins in a unified and extensible way.","Continuing the evolution of Fiji's SciJava plugin mechanism, SciJava Ops enables users to harness algorithms from various software platforms within a central execution environment.","In addition, SciJava Ops automatically adapts data into the most appropriate structure for each algorithm, allowing users to freely and transparently combine algorithms from otherwise incompatible tools.","While SciJava Ops is initially distributed as a Fiji update site, the framework does not require Fiji, ImageJ, or ImageJ2, and would be suitable for integration with additional image analysis platforms."],"url":"http://arxiv.org/abs/2405.12385v1","category":"cs.SE"}
{"created":"2024-05-20 21:20:28","title":"Large scale scattering using fast solvers based on neural operators","abstract":"We extend a recently proposed machine-learning-based iterative solver, i.e. the hybrid iterative transferable solver (HINTS), to solve the scattering problem described by the Helmholtz equation in an exterior domain with a complex absorbing boundary condition. The HINTS method combines neural operators (NOs) with standard iterative solvers, e.g. Jacobi and Gauss-Seidel (GS), to achieve better performance by leveraging the spectral bias of neural networks. In HINTS, some iterations of the conventional iterative method are replaced by inferences of the pre-trained NO. In this work, we employ HINTS to solve the scattering problem for both 2D and 3D problems, where the standard iterative solver fails. We consider square and triangular scatterers of various sizes in 2D, and a cube and a model submarine in 3D. We explore and illustrate the extrapolation capability of HINTS in handling diverse geometries of the scatterer, which is achieved by training the NO on non-scattering scenarios and then deploying it in HINTS to solve scattering problems. The accurate results demonstrate that the NO in HINTS method remains effective without retraining or fine-tuning it whenever a new scatterer is given. Taken together, our results highlight the adaptability and versatility of the extended HINTS methodology in addressing diverse scattering problems.","sentences":["We extend a recently proposed machine-learning-based iterative solver, i.e. the hybrid iterative transferable solver (HINTS), to solve the scattering problem described by the Helmholtz equation in an exterior domain with a complex absorbing boundary condition.","The HINTS method combines neural operators (NOs) with standard iterative solvers, e.g. Jacobi and Gauss-Seidel (GS), to achieve better performance by leveraging the spectral bias of neural networks.","In HINTS, some iterations of the conventional iterative method are replaced by inferences of the pre-trained NO.","In this work, we employ HINTS to solve the scattering problem for both 2D and 3D problems, where the standard iterative solver fails.","We consider square and triangular scatterers of various sizes in 2D, and a cube and a model submarine in 3D.","We explore and illustrate the extrapolation capability of HINTS in handling diverse geometries of the scatterer, which is achieved by training the NO on non-scattering scenarios and then deploying it in HINTS to solve scattering problems.","The accurate results demonstrate that the NO in HINTS method remains effective without retraining or fine-tuning it whenever a new scatterer is given.","Taken together, our results highlight the adaptability and versatility of the extended HINTS methodology in addressing diverse scattering problems."],"url":"http://arxiv.org/abs/2405.12380v1","category":"cs.LG"}
{"created":"2024-05-20 21:18:53","title":"Phase-space negativity as a computational resource for quantum kernel methods","abstract":"Quantum kernel methods are a proposal for achieving quantum computational advantage in machine learning. They are based on a hybrid classical-quantum computation where a function called the quantum kernel is estimated by a quantum device while the rest of the computation is performed classically. Quantum advantages may be achieved through this method only if the quantum kernel function cannot be estimated efficiently on a classical computer. In this paper, we provide sufficient conditions for the efficient classical estimation of quantum kernel functions for bosonic systems. Specifically, we show that if the negativity in the phase-space quasi-probability distributions of data-encoding quantum states associated with the quantum kernel scales at most polynomially with the size of the quantum circuit, then the kernel function can be estimated efficiently classically. We consider quantum optical examples involving linear-optical networks with and without adaptive non-Gaussian measurements and investigate the effects of loss on the efficiency of the classical simulation. Our results underpin the role of the negativity in phase-space quasi-probability distributions as an essential resource in quantum machine learning based on kernel methods.","sentences":["Quantum kernel methods are a proposal for achieving quantum computational advantage in machine learning.","They are based on a hybrid classical-quantum computation where a function called the quantum kernel is estimated by a quantum device while the rest of the computation is performed classically.","Quantum advantages may be achieved through this method only if the quantum kernel function cannot be estimated efficiently on a classical computer.","In this paper, we provide sufficient conditions for the efficient classical estimation of quantum kernel functions for bosonic systems.","Specifically, we show that if the negativity in the phase-space quasi-probability distributions of data-encoding quantum states associated with the quantum kernel scales at most polynomially with the size of the quantum circuit, then the kernel function can be estimated efficiently classically.","We consider quantum optical examples involving linear-optical networks with and without adaptive non-Gaussian measurements and investigate the effects of loss on the efficiency of the classical simulation.","Our results underpin the role of the negativity in phase-space quasi-probability distributions as an essential resource in quantum machine learning based on kernel methods."],"url":"http://arxiv.org/abs/2405.12378v1","category":"quant-ph"}
{"created":"2024-05-20 20:38:22","title":"AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field","abstract":"3D Gaussian Splatting (3DGS) has recently advanced radiance field reconstruction by offering superior capabilities for novel view synthesis and real-time rendering speed. However, its strategy of blending optimization and adaptive density control might lead to sub-optimal results; it can sometimes yield noisy geometry and blurry artifacts due to prioritizing optimizing large Gaussians at the cost of adequately densifying smaller ones. To address this, we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided Optimization. The Atomized Proliferation constrains ellipsoid Gaussians of various sizes into more uniform-sized Atom Gaussians. The strategy enhances the representation of areas with fine features by placing greater emphasis on densification in accordance with scene details. In addition, we proposed a Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal Loss. This optimization method effectively smooths flat surfaces while preserving intricate details. Our evaluation shows that AtomGS outperforms existing state-of-the-art methods in rendering quality. Additionally, it achieves competitive accuracy in geometry reconstruction and offers a significant improvement in training speed over other SDF-based methods. More interactive demos can be found in our website (\\href{https://rongliu-leo.github.io/AtomGS/}{https://rongliu-leo.github.io/AtomGS/}).","sentences":["3D Gaussian Splatting (3DGS) has recently advanced radiance field reconstruction by offering superior capabilities for novel view synthesis and real-time rendering speed.","However, its strategy of blending optimization and adaptive density control might lead to sub-optimal results; it can sometimes yield noisy geometry and blurry artifacts due to prioritizing optimizing large Gaussians at the cost of adequately densifying smaller ones.","To address this, we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided Optimization.","The Atomized Proliferation constrains ellipsoid Gaussians of various sizes into more uniform-sized Atom Gaussians.","The strategy enhances the representation of areas with fine features by placing greater emphasis on densification in accordance with scene details.","In addition, we proposed a Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal Loss.","This optimization method effectively smooths flat surfaces while preserving intricate details.","Our evaluation shows that AtomGS outperforms existing state-of-the-art methods in rendering quality.","Additionally, it achieves competitive accuracy in geometry reconstruction and offers a significant improvement in training speed over other SDF-based methods.","More interactive demos can be found in our website (\\href{https://rongliu-leo.github.io/AtomGS/}{https://rongliu-leo.github.io/AtomGS/})."],"url":"http://arxiv.org/abs/2405.12369v1","category":"cs.CV"}
{"created":"2024-05-20 20:27:00","title":"Question-Based Retrieval using Atomic Units for Enterprise RAG","abstract":"Enterprise retrieval augmented generation (RAG) offers a highly flexible framework for combining powerful large language models (LLMs) with internal, possibly temporally changing, documents. In RAG, documents are first chunked. Relevant chunks are then retrieved for a specific user query, which are passed as context to a synthesizer LLM to generate the query response. However, the retrieval step can limit performance, as incorrect chunks can lead the synthesizer LLM to generate a false response. This work proposes a zero-shot adaptation of standard dense retrieval steps for more accurate chunk recall. Specifically, a chunk is first decomposed into atomic statements. A set of synthetic questions are then generated on these atoms (with the chunk as the context). Dense retrieval involves finding the closest set of synthetic questions, and associated chunks, to the user query. It is found that retrieval with the atoms leads to higher recall than retrieval with chunks. Further performance gain is observed with retrieval using the synthetic questions generated over the atoms. Higher recall at the retrieval step enables higher performance of the enterprise LLM using the RAG pipeline.","sentences":["Enterprise retrieval augmented generation (RAG) offers a highly flexible framework for combining powerful large language models (LLMs) with internal, possibly temporally changing, documents.","In RAG, documents are first chunked.","Relevant chunks are then retrieved for a specific user query, which are passed as context to a synthesizer LLM to generate the query response.","However, the retrieval step can limit performance, as incorrect chunks can lead the synthesizer LLM to generate a false response.","This work proposes a zero-shot adaptation of standard dense retrieval steps for more accurate chunk recall.","Specifically, a chunk is first decomposed into atomic statements.","A set of synthetic questions are then generated on these atoms (with the chunk as the context).","Dense retrieval involves finding the closest set of synthetic questions, and associated chunks, to the user query.","It is found that retrieval with the atoms leads to higher recall than retrieval with chunks.","Further performance gain is observed with retrieval using the synthetic questions generated over the atoms.","Higher recall at the retrieval step enables higher performance of the enterprise LLM using the RAG pipeline."],"url":"http://arxiv.org/abs/2405.12363v1","category":"cs.CL"}
{"created":"2024-05-20 20:14:09","title":"Coarse-graining conformational dynamics with multi-dimensional generalized Langevin equation: how, when, and why","abstract":"A data-driven ab initio generalized Langevin equation (AIGLE) approach is developed to learn and simulate high-dimensional, heterogeneous, coarse-grained conformational dynamics. Constrained by the fluctuation-dissipation theorem, the approach can build coarse-grained models in dynamical consistency with all-atom molecular dynamics. We also propose practical criteria for AIGLE to enforce long-term dynamical consistency. Case studies of a toy polymer, with 20 coarse-grained sites, and the alanine dipeptide, with two dihedral angles, elucidate why one should adopt AIGLE or its Markovian limit for modeling coarse-grained conformational dynamics in practice.","sentences":["A data-driven ab initio generalized Langevin equation (AIGLE) approach is developed to learn and simulate high-dimensional, heterogeneous, coarse-grained conformational dynamics.","Constrained by the fluctuation-dissipation theorem, the approach can build coarse-grained models in dynamical consistency with all-atom molecular dynamics.","We also propose practical criteria for AIGLE to enforce long-term dynamical consistency.","Case studies of a toy polymer, with 20 coarse-grained sites, and the alanine dipeptide, with two dihedral angles, elucidate why one should adopt AIGLE or its Markovian limit for modeling coarse-grained conformational dynamics in practice."],"url":"http://arxiv.org/abs/2405.12356v1","category":"physics.bio-ph"}
{"created":"2024-05-20 19:36:34","title":"A Test of the Thermodynamics of Evolution","abstract":"Recent research has extended methods from the fields of thermodynamics and statistical mechanics into other disciplines. Most notably, one recent work creates a unified theoretical framework to understand evolutionary biology, machine learning, and thermodynamics. We present simulations of biological evolution used to test this framework. The test simulates organisms whose behavior is determined by specific parameters that play the role of genes. These genes are passed on to new simulated organisms with the capacity to mutate, allowing adaption of the organisms to the environment. With this simulation, we are able to test the the framework in question. The results of our simulation are consistent with the work being tested, providing evidence for it.","sentences":["Recent research has extended methods from the fields of thermodynamics and statistical mechanics into other disciplines.","Most notably, one recent work creates a unified theoretical framework to understand evolutionary biology, machine learning, and thermodynamics.","We present simulations of biological evolution used to test this framework.","The test simulates organisms whose behavior is determined by specific parameters that play the role of genes.","These genes are passed on to new simulated organisms with the capacity to mutate, allowing adaption of the organisms to the environment.","With this simulation, we are able to test the the framework in question.","The results of our simulation are consistent with the work being tested, providing evidence for it."],"url":"http://arxiv.org/abs/2405.12344v1","category":"q-bio.PE"}
{"created":"2024-05-20 18:52:41","title":"Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation","abstract":"The accurate segmentation of medical images is crucial for diagnosing and treating diseases. Recent studies demonstrate that vision transformer-based methods have significantly improved performance in medical image segmentation, primarily due to their superior ability to establish global relationships among features and adaptability to various inputs. However, these methods struggle with the low signal-to-noise ratio inherent to medical images. Additionally, the effective utilization of channel and spatial information, which are essential for medical image segmentation, is limited by the representation capacity of self-attention. To address these challenges, we propose a multi-dimension transformer with attention-based filtering (MDT-AF), which redesigns the patch embedding and self-attention mechanism for medical image segmentation. MDT-AF incorporates an attention-based feature filtering mechanism into the patch embedding blocks and employs a coarse-to-fine process to mitigate the impact of low signal-to-noise ratio. To better capture complex structures in medical images, MDT-AF extends the self-attention mechanism to incorporate spatial and channel dimensions, enriching feature representation. Moreover, we introduce an interaction mechanism to improve the feature aggregation between spatial and channel dimensions. Experimental results on three public medical image segmentation benchmarks show that MDT-AF achieves state-of-the-art (SOTA) performance.","sentences":["The accurate segmentation of medical images is crucial for diagnosing and treating diseases.","Recent studies demonstrate that vision transformer-based methods have significantly improved performance in medical image segmentation, primarily due to their superior ability to establish global relationships among features and adaptability to various inputs.","However, these methods struggle with the low signal-to-noise ratio inherent to medical images.","Additionally, the effective utilization of channel and spatial information, which are essential for medical image segmentation, is limited by the representation capacity of self-attention.","To address these challenges, we propose a multi-dimension transformer with attention-based filtering (MDT-AF), which redesigns the patch embedding and self-attention mechanism for medical image segmentation.","MDT-AF incorporates an attention-based feature filtering mechanism into the patch embedding blocks and employs a coarse-to-fine process to mitigate the impact of low signal-to-noise ratio.","To better capture complex structures in medical images, MDT-AF extends the self-attention mechanism to incorporate spatial and channel dimensions, enriching feature representation.","Moreover, we introduce an interaction mechanism to improve the feature aggregation between spatial and channel dimensions.","Experimental results on three public medical image segmentation benchmarks show that MDT-AF achieves state-of-the-art (SOTA) performance."],"url":"http://arxiv.org/abs/2405.12328v1","category":"cs.CV"}
{"created":"2024-05-20 18:29:41","title":"Hierarchical SegNet with Channel and Context Attention for Accurate Lung Segmentation in Chest X-ray Images","abstract":"Lung segmentation in chest X-ray images is a critical task in medical image analysis, enabling accurate diagnosis and treatment of various lung diseases. In this paper, we propose a novel approach for lung segmentation by integrating Hierarchical SegNet with a proposed multi-modal attention mechanism. The channel attention mechanism highlights specific feature maps or channels crucial for lung region segmentation, while the context attention mechanism adaptively weighs the importance of different spatial regions. By combining both mechanisms, the proposed mechanism enables the model to better capture complex patterns and relationships between various features, leading to improved segmentation accuracy and better feature representation. Furthermore, an attention gating mechanism is employed to integrate attention information with encoder features, allowing the model to adaptively weigh the importance of different attention features and ignore irrelevant ones. Experimental results demonstrate that our proposed approach achieves state-of-the-art performance in lung segmentation tasks, outperforming existing methods. The proposed approach has the potential to improve the accuracy and efficiency of lung disease diagnosis and treatment, and can be extended to other medical image analysis tasks.","sentences":["Lung segmentation in chest X-ray images is a critical task in medical image analysis, enabling accurate diagnosis and treatment of various lung diseases.","In this paper, we propose a novel approach for lung segmentation by integrating Hierarchical SegNet with a proposed multi-modal attention mechanism.","The channel attention mechanism highlights specific feature maps or channels crucial for lung region segmentation, while the context attention mechanism adaptively weighs the importance of different spatial regions.","By combining both mechanisms, the proposed mechanism enables the model to better capture complex patterns and relationships between various features, leading to improved segmentation accuracy and better feature representation.","Furthermore, an attention gating mechanism is employed to integrate attention information with encoder features, allowing the model to adaptively weigh the importance of different attention features and ignore irrelevant ones.","Experimental results demonstrate that our proposed approach achieves state-of-the-art performance in lung segmentation tasks, outperforming existing methods.","The proposed approach has the potential to improve the accuracy and efficiency of lung disease diagnosis and treatment, and can be extended to other medical image analysis tasks."],"url":"http://arxiv.org/abs/2405.12318v1","category":"eess.IV"}
{"created":"2024-05-20 17:18:26","title":"Brewer-Nash Scrutinised: Mechanised Checking of Policies featuring Write Revocation","abstract":"This paper revisits the Brewer-Nash security policy model inspired by ethical Chinese Wall policies. We draw attention to the fact that write access can be revoked in the Brewer-Nash model. The semantics of write access were underspecified originally, leading to multiple interpretations for which we provide a modern operational semantics. We go on to modernise the analysis of information flow in the Brewer-Nash model, by adopting a more precise definition adapted from Kessler. For our modernised reformulation, we provide full mechanised coverage for all theorems proposed by Brewer & Nash. Most theorems are established automatically using the tool {log} with the exception of a theorem regarding information flow, which combines a lemma in {log} with a theorem mechanised in Coq. Having covered all theorems originally posed by Brewer-Nash, achieving modern precision and mechanisation, we propose this work as a step towards a methodology for automated checking of more complex security policy models.","sentences":["This paper revisits the Brewer-Nash security policy model inspired by ethical Chinese Wall policies.","We draw attention to the fact that write access can be revoked in the Brewer-Nash model.","The semantics of write access were underspecified originally, leading to multiple interpretations for which we provide a modern operational semantics.","We go on to modernise the analysis of information flow in the Brewer-Nash model, by adopting a more precise definition adapted from Kessler.","For our modernised reformulation, we provide full mechanised coverage for all theorems proposed by Brewer & Nash.","Most theorems are established automatically using the tool {log} with the exception of a theorem regarding information flow, which combines a lemma in {log} with a theorem mechanised in Coq.","Having covered all theorems originally posed by Brewer-Nash, achieving modern precision and mechanisation, we propose this work as a step towards a methodology for automated checking of more complex security policy models."],"url":"http://arxiv.org/abs/2405.12187v1","category":"cs.CR"}
{"created":"2024-05-20 16:01:01","title":"DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM","abstract":"Visual Language Tracking (VLT) enhances single object tracking (SOT) by integrating natural language descriptions from a video, for the precise tracking of a specified object. By leveraging high-level semantic information, VLT guides object tracking, alleviating the constraints associated with relying on a visual modality. Nevertheless, most VLT benchmarks are annotated in a single granularity and lack a coherent semantic framework to provide scientific guidance. Moreover, coordinating human annotators for high-quality annotations is laborious and time-consuming. To address these challenges, we introduce DTLLM-VLT, which automatically generates extensive and multi-granularity text to enhance environmental diversity. (1) DTLLM-VLT generates scientific and multi-granularity text descriptions using a cohesive prompt framework. Its succinct and highly adaptable design allows seamless integration into various visual tracking benchmarks. (2) We select three prominent benchmarks to deploy our approach: short-term tracking, long-term tracking, and global instance tracking. We offer four granularity combinations for these benchmarks, considering the extent and density of semantic information, thereby showcasing the practicality and versatility of DTLLM-VLT. (3) We conduct comparative experiments on VLT benchmarks with different text granularities, evaluating and analyzing the impact of diverse text on tracking performance. Conclusionally, this work leverages LLM to provide multi-granularity semantic information for VLT task from efficient and diverse perspectives, enabling fine-grained evaluation of multi-modal trackers. In the future, we believe this work can be extended to more datasets to support vision datasets understanding.","sentences":["Visual Language Tracking (VLT) enhances single object tracking (SOT) by integrating natural language descriptions from a video, for the precise tracking of a specified object.","By leveraging high-level semantic information, VLT guides object tracking, alleviating the constraints associated with relying on a visual modality.","Nevertheless, most VLT benchmarks are annotated in a single granularity and lack a coherent semantic framework to provide scientific guidance.","Moreover, coordinating human annotators for high-quality annotations is laborious and time-consuming.","To address these challenges, we introduce DTLLM-VLT, which automatically generates extensive and multi-granularity text to enhance environmental diversity.","(1) DTLLM-VLT generates scientific and multi-granularity text descriptions using a cohesive prompt framework.","Its succinct and highly adaptable design allows seamless integration into various visual tracking benchmarks.","(2) We select three prominent benchmarks to deploy our approach: short-term tracking, long-term tracking, and global instance tracking.","We offer four granularity combinations for these benchmarks, considering the extent and density of semantic information, thereby showcasing the practicality and versatility of DTLLM-VLT.","(3) We conduct comparative experiments on VLT benchmarks with different text granularities, evaluating and analyzing the impact of diverse text on tracking performance.","Conclusionally, this work leverages LLM to provide multi-granularity semantic information for VLT task from efficient and diverse perspectives, enabling fine-grained evaluation of multi-modal trackers.","In the future, we believe this work can be extended to more datasets to support vision datasets understanding."],"url":"http://arxiv.org/abs/2405.12139v1","category":"cs.CV"}
{"created":"2024-05-20 15:48:32","title":"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning","abstract":"Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.","sentences":["Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models.","In this paper, we analyze the impact of low-rank updating, as implemented in LoRA.","Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge.","Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters.","To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix.","Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA.","We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining.","Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks."],"url":"http://arxiv.org/abs/2405.12130v1","category":"cs.CL"}
{"created":"2024-05-20 15:43:40","title":"Design, Control, and Motion-Planning for a Root-Perching Rotor-Distributed Manipulator","abstract":"Manipulation performance improvement is crucial for aerial robots. For aerial manipulators, the baselink position and attitude errors directly affect the precision at the end effector. To address this stability problem, fixed-body approaches such as perching on the environment using the rotor suction force are useful. Additionally, conventional arm-equipped multirotors, called rotor-concentrated manipulators (RCMs), find it difficult to generate a large wrench at the end effector due to joint torque limitations. Using distributed rotors to each link, the thrust can support each link weight, decreasing the arm joints' torque. Based on this approach, rotor-distributed manipulators (RDMs) can increase feasible wrench and reachability of the end-effector. This paper introduces a minimal configuration of a rotor-distributed manipulator that can perch on surfaces, especially ceilings, using a part of their body. First, we design a minimal rotor-distributed arm considering the flight and end-effector performance. Second, a flight controller is proposed for this minimal RDM along with a perching controller adaptable for various types of aerial robots. Third, we propose a motion planning method based on inverse kinematics (IK), considering specific constraints to the proposed RDMs such as perching force. Finally, we evaluate flight and perching motions and \\revise{confirm} that the proposed manipulator can significantly improve the manipulation performance.","sentences":["Manipulation performance improvement is crucial for aerial robots.","For aerial manipulators, the baselink position and attitude errors directly affect the precision at the end effector.","To address this stability problem, fixed-body approaches such as perching on the environment using the rotor suction force are useful.","Additionally, conventional arm-equipped multirotors, called rotor-concentrated manipulators (RCMs), find it difficult to generate a large wrench at the end effector due to joint torque limitations.","Using distributed rotors to each link, the thrust can support each link weight, decreasing the arm joints' torque.","Based on this approach, rotor-distributed manipulators (RDMs) can increase feasible wrench and reachability of the end-effector.","This paper introduces a minimal configuration of a rotor-distributed manipulator that can perch on surfaces, especially ceilings, using a part of their body.","First, we design a minimal rotor-distributed arm considering the flight and end-effector performance.","Second, a flight controller is proposed for this minimal RDM along with a perching controller adaptable for various types of aerial robots.","Third, we propose a motion planning method based on inverse kinematics (IK), considering specific constraints to the proposed RDMs such as perching force.","Finally, we evaluate flight and perching motions and \\revise{confirm} that the proposed manipulator can significantly improve the manipulation performance."],"url":"http://arxiv.org/abs/2405.12125v1","category":"eess.SY"}
{"created":"2024-05-20 15:39:40","title":"An Active Learning Framework with a Class Balancing Strategy for Time Series Classification","abstract":"Training machine learning models for classification tasks often requires labeling numerous samples, which is costly and time-consuming, especially in time series analysis. This research investigates Active Learning (AL) strategies to reduce the amount of labeled data needed for effective time series classification. Traditional AL techniques cannot control the selection of instances per class for labeling, leading to potential bias in classification performance and instance selection, particularly in imbalanced time series datasets. To address this, we propose a novel class-balancing instance selection algorithm integrated with standard AL strategies. Our approach aims to select more instances from classes with fewer labeled examples, thereby addressing imbalance in time series datasets. We demonstrate the effectiveness of our AL framework in selecting informative data samples for two distinct domains of tactile texture recognition and industrial fault detection. In robotics, our method achieves high-performance texture categorization while significantly reducing labeled training data requirements to 70%. We also evaluate the impact of different sliding window time intervals on robotic texture classification using AL strategies. In synthetic fiber manufacturing, we adapt AL techniques to address the challenge of fault classification, aiming to minimize data annotation cost and time for industries. We also address real-life class imbalances in the multiclass industrial anomalous dataset using our class-balancing instance algorithm integrated with AL strategies. Overall, this thesis highlights the potential of our AL framework across these two distinct domains.","sentences":["Training machine learning models for classification tasks often requires labeling numerous samples, which is costly and time-consuming, especially in time series analysis.","This research investigates Active Learning (AL) strategies to reduce the amount of labeled data needed for effective time series classification.","Traditional AL techniques cannot control the selection of instances per class for labeling, leading to potential bias in classification performance and instance selection, particularly in imbalanced time series datasets.","To address this, we propose a novel class-balancing instance selection algorithm integrated with standard AL strategies.","Our approach aims to select more instances from classes with fewer labeled examples, thereby addressing imbalance in time series datasets.","We demonstrate the effectiveness of our AL framework in selecting informative data samples for two distinct domains of tactile texture recognition and industrial fault detection.","In robotics, our method achieves high-performance texture categorization while significantly reducing labeled training data requirements to 70%.","We also evaluate the impact of different sliding window time intervals on robotic texture classification using AL strategies.","In synthetic fiber manufacturing, we adapt AL techniques to address the challenge of fault classification, aiming to minimize data annotation cost and time for industries.","We also address real-life class imbalances in the multiclass industrial anomalous dataset using our class-balancing instance algorithm integrated with AL strategies.","Overall, this thesis highlights the potential of our AL framework across these two distinct domains."],"url":"http://arxiv.org/abs/2405.12122v1","category":"cs.LG"}
{"created":"2024-05-20 15:38:53","title":"EdgeLoc: A Communication-Adaptive Parallel System for Real-Time Localization in Infrastructure-Assisted Autonomous Driving","abstract":"This paper presents EdgeLoc, an infrastructure-assisted, real-time localization system for autonomous driving that addresses the incompatibility between traditional localization methods and deep learning approaches. The system is built on top of the Robot Operating System (ROS) and combines the real-time performance of traditional methods with the high accuracy of deep learning approaches. The system leverages edge computing capabilities of roadside units (RSUs) for precise localization to enhance on-vehicle localization that is based on the real-time visual odometry. EdgeLoc is a parallel processing system, utilizing a proposed uncertainty-aware pose fusion solution. It achieves communication adaptivity through online learning and addresses fluctuations via window-based detection. Moreover, it achieves optimal latency and maximum improvement by utilizing auto-splitting vehicle-infrastructure collaborative inference, as well as online distribution learning for decision-making. Even with the most basic end-to-end deep neural network for localization estimation, EdgeLoc realizes a 67.75\\% reduction in the localization error for real-time local visual odometry, a 29.95\\% reduction for non-real-time collaborative inference, and a 30.26\\% reduction compared to Kalman filtering. Finally, accuracy-to-latency conversion was experimentally validated, and an overall experiment was conducted on a practical cellular network. The system is open sourced at https://github.com/LoganCome/EdgeAssistedLocalization.","sentences":["This paper presents EdgeLoc, an infrastructure-assisted, real-time localization system for autonomous driving that addresses the incompatibility between traditional localization methods and deep learning approaches.","The system is built on top of the Robot Operating System (ROS) and combines the real-time performance of traditional methods with the high accuracy of deep learning approaches.","The system leverages edge computing capabilities of roadside units (RSUs) for precise localization to enhance on-vehicle localization that is based on the real-time visual odometry.","EdgeLoc is a parallel processing system, utilizing a proposed uncertainty-aware pose fusion solution.","It achieves communication adaptivity through online learning and addresses fluctuations via window-based detection.","Moreover, it achieves optimal latency and maximum improvement by utilizing auto-splitting vehicle-infrastructure collaborative inference, as well as online distribution learning for decision-making.","Even with the most basic end-to-end deep neural network for localization estimation, EdgeLoc realizes a 67.75\\% reduction in the localization error for real-time local visual odometry, a 29.95\\% reduction for non-real-time collaborative inference, and a 30.26\\% reduction compared to Kalman filtering.","Finally, accuracy-to-latency conversion was experimentally validated, and an overall experiment was conducted on a practical cellular network.","The system is open sourced at https://github.com/LoganCome/EdgeAssistedLocalization."],"url":"http://arxiv.org/abs/2405.12120v1","category":"cs.DC"}
{"created":"2024-05-20 15:05:01","title":"Quantum Dissipation at Conical Intersections of Quasienergies","abstract":"We introduce the notion of conical intersections to Floquet theory and work out consequences of the underlying spatio-temporal symmetries for a driven two-level system coupled to an ohmic heat bath. We find that on manifolds with constant quasienergy splitting, the mean energies of the Floquet states vary significantly. The stationary populations behave in the same way, which emphasizes the importance of the mean energies for dissipation. Owing to symmetries, the stationary state may be fully mixed even at zero temperature. In the limit of large driving frequency, such full mixture is found in the whole vicinity of the intersection.","sentences":["We introduce the notion of conical intersections to Floquet theory and work out consequences of the underlying spatio-temporal symmetries for a driven two-level system coupled to an ohmic heat bath.","We find that on manifolds with constant quasienergy splitting, the mean energies of the Floquet states vary significantly.","The stationary populations behave in the same way, which emphasizes the importance of the mean energies for dissipation.","Owing to symmetries, the stationary state may be fully mixed even at zero temperature.","In the limit of large driving frequency, such full mixture is found in the whole vicinity of the intersection."],"url":"http://arxiv.org/abs/2405.12093v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 15:03:42","title":"A Possible Additional Formation Pathway for the Interstellar Diatomic SiS","abstract":"The formation of silicon monosulfide (SiS) in space appears to be a difficult process, but the present work is showing that a previously excluded pathway may contribute to its astronomical abundance. Reaction of the radicals SH + SiH produces SiS with a submerged transition state and generates a stabilizing H$_2$ molecule as a product to dissipate the kinetic energy. Such is a textbook chemical reaction for favorable gas-phase chemistry. While previously proposed mechanisms reacting atomic sulfur and silicon with SiH, SH, and H$_2$S will still be major contributors to the production of SiS, an abundance of SiS in certain regions could be a marker for the presence of SiH where it has previously been unobserved. These quantum chemically-computed reaction profiles imply that the silicon-chalcogen chemistry of molecular clouds, shocked regions, or protoplanetary disks may be richer than previously thought. Quantum chemical spectral data for the intermediate cis- and trans-HSiSH are also provided in order to aid in their potential spectroscopic characterization.","sentences":["The formation of silicon monosulfide (SiS) in space appears to be a difficult process, but the present work is showing that a previously excluded pathway may contribute to its astronomical abundance.","Reaction of the radicals SH + SiH produces SiS with a submerged transition state and generates a stabilizing H$_2$ molecule as a product to dissipate the kinetic energy.","Such is a textbook chemical reaction for favorable gas-phase chemistry.","While previously proposed mechanisms reacting atomic sulfur and silicon with SiH, SH, and H$_2$S will still be major contributors to the production of SiS, an abundance of SiS in certain regions could be a marker for the presence of SiH where it has previously been unobserved.","These quantum chemically-computed reaction profiles imply that the silicon-chalcogen chemistry of molecular clouds, shocked regions, or protoplanetary disks may be richer than previously thought.","Quantum chemical spectral data for the intermediate cis- and trans-HSiSH are also provided in order to aid in their potential spectroscopic characterization."],"url":"http://arxiv.org/abs/2405.12092v1","category":"astro-ph.GA"}
{"created":"2024-05-20 14:55:20","title":"Noise-tolerant learnability of shallow quantum circuits from statistics and the cost of quantum pseudorandomness","abstract":"This work studies the learnability of unknown quantum circuits in the near term. We prove the natural robustness of quantum statistical queries for learning quantum processes and provide an efficient way to benchmark various classes of noise from statistics, which gives us a powerful framework for developing noise-tolerant algorithms. We adapt a learning algorithm for constant-depth quantum circuits to the quantum statistical query setting with a small overhead in the query complexity. We prove average-case lower bounds for learning random quantum circuits of logarithmic and higher depths within diamond distance with statistical queries. Additionally, we show the hardness of the quantum threshold search problem from quantum statistical queries and discuss its implications for the learnability of shallow quantum circuits. Finally, we prove that pseudorandom unitaries (PRUs) cannot be constructed using circuits of constant depth by constructing an efficient distinguisher and proving a new variation of the quantum no-free lunch theorem.","sentences":["This work studies the learnability of unknown quantum circuits in the near term.","We prove the natural robustness of quantum statistical queries for learning quantum processes and provide an efficient way to benchmark various classes of noise from statistics, which gives us a powerful framework for developing noise-tolerant algorithms.","We adapt a learning algorithm for constant-depth quantum circuits to the quantum statistical query setting with a small overhead in the query complexity.","We prove average-case lower bounds for learning random quantum circuits of logarithmic and higher depths within diamond distance with statistical queries.","Additionally, we show the hardness of the quantum threshold search problem from quantum statistical queries and discuss its implications for the learnability of shallow quantum circuits.","Finally, we prove that pseudorandom unitaries (PRUs) cannot be constructed using circuits of constant depth by constructing an efficient distinguisher and proving a new variation of the quantum no-free lunch theorem."],"url":"http://arxiv.org/abs/2405.12085v1","category":"quant-ph"}
{"created":"2024-05-20 14:13:22","title":"Energy-Efficient Federated Edge Learning with Streaming Data: A Lyapunov Optimization Approach","abstract":"Federated learning (FL) has received significant attention in recent years for its advantages in efficient training of machine learning models across distributed clients without disclosing user-sensitive data. Specifically, in federated edge learning (FEEL) systems, the time-varying nature of wireless channels introduces inevitable system dynamics in the communication process, thereby affecting training latency and energy consumption. In this work, we further consider a streaming data scenario where new training data samples are randomly generated over time at edge devices. Our goal is to develop a dynamic scheduling and resource allocation algorithm to address the inherent randomness in data arrivals and resource availability under long-term energy constraints. To achieve this, we formulate a stochastic network optimization problem and use the Lyapunov drift-plus-penalty framework to obtain a dynamic resource management design. Our proposed algorithm makes adaptive decisions on device scheduling, computational capacity adjustment, and allocation of bandwidth and transmit power in every round. We provide convergence analysis for the considered setting with heterogeneous data and time-varying objective functions, which supports the rationale behind our proposed scheduling design. The effectiveness of our scheme is verified through simulation results, demonstrating improved learning performance and energy efficiency as compared to baseline schemes.","sentences":["Federated learning (FL) has received significant attention in recent years for its advantages in efficient training of machine learning models across distributed clients without disclosing user-sensitive data.","Specifically, in federated edge learning (FEEL) systems, the time-varying nature of wireless channels introduces inevitable system dynamics in the communication process, thereby affecting training latency and energy consumption.","In this work, we further consider a streaming data scenario where new training data samples are randomly generated over time at edge devices.","Our goal is to develop a dynamic scheduling and resource allocation algorithm to address the inherent randomness in data arrivals and resource availability under long-term energy constraints.","To achieve this, we formulate a stochastic network optimization problem and use the Lyapunov drift-plus-penalty framework to obtain a dynamic resource management design.","Our proposed algorithm makes adaptive decisions on device scheduling, computational capacity adjustment, and allocation of bandwidth and transmit power in every round.","We provide convergence analysis for the considered setting with heterogeneous data and time-varying objective functions, which supports the rationale behind our proposed scheduling design.","The effectiveness of our scheme is verified through simulation results, demonstrating improved learning performance and energy efficiency as compared to baseline schemes."],"url":"http://arxiv.org/abs/2405.12046v1","category":"cs.LG"}
{"created":"2024-05-20 14:05:35","title":"Adaptive Extraction Network for Multivariate Long Sequence Time-Series Forecasting","abstract":"Models employing CNN architecture have made significant progress in multivariate long sequence time-series forecasting (MLSTF), particularly in modeling local time series characteristics. However, during the MLSTF process, extracting the global time series patterns and understanding the correlations among different variables are highly significant. To address this challenge, we introduce multi-resolution convolution and deformable convolution operations. By enlarging the receptive field using convolution kernels with different dilation factors to capture temporal correlation information across different resolutions, and adaptively adjusting the sampling positions through additional offset vectors, we enhance the network's ability to capture correlated features between variables. Building upon this, we propose ATVCNet, an adaptive temporal-variable convolutional network designed to effectively model the local/global temporal dependencies and inter-variable dependencies of multivariate time series. Specifically, extracting and fusing time series features at different resolutions, captures both local contextual information and global patterns in the time series. The designed inter-variable feature adaptive extraction module captures the correlation among different variables in the time series. We evaluated the performance of ATVCNet across eight real-world datasets. The results indicate that ATVCNet achieved a performance improvement of approximately 63.4% over state-of-the-art MLSTF models.","sentences":["Models employing CNN architecture have made significant progress in multivariate long sequence time-series forecasting (MLSTF), particularly in modeling local time series characteristics.","However, during the MLSTF process, extracting the global time series patterns and understanding the correlations among different variables are highly significant.","To address this challenge, we introduce multi-resolution convolution and deformable convolution operations.","By enlarging the receptive field using convolution kernels with different dilation factors to capture temporal correlation information across different resolutions, and adaptively adjusting the sampling positions through additional offset vectors, we enhance the network's ability to capture correlated features between variables.","Building upon this, we propose ATVCNet, an adaptive temporal-variable convolutional network designed to effectively model the local/global temporal dependencies and inter-variable dependencies of multivariate time series.","Specifically, extracting and fusing time series features at different resolutions, captures both local contextual information and global patterns in the time series.","The designed inter-variable feature adaptive extraction module captures the correlation among different variables in the time series.","We evaluated the performance of ATVCNet across eight real-world datasets.","The results indicate that ATVCNet achieved a performance improvement of approximately 63.4% over state-of-the-art MLSTF models."],"url":"http://arxiv.org/abs/2405.12038v1","category":"cs.LG"}
{"created":"2024-05-20 13:40:52","title":"Continuous Sign Language Recognition with Adapted Conformer via Unsupervised Pretraining","abstract":"Conventional Deep Learning frameworks for continuous sign language recognition (CSLR) are comprised of a single or multi-modal feature extractor, a sequence-learning module, and a decoder for outputting the glosses. The sequence learning module is a crucial part wherein transformers have demonstrated their efficacy in the sequence-to-sequence tasks. Analyzing the research progress in the field of Natural Language Processing and Speech Recognition, a rapid introduction of various transformer variants is observed. However, in the realm of sign language, experimentation in the sequence learning component is limited. In this work, the state-of-the-art Conformer model for Speech Recognition is adapted for CSLR and the proposed model is termed ConSignformer. This marks the first instance of employing Conformer for a vision-based task. ConSignformer has bimodal pipeline of CNN as feature extractor and Conformer for sequence learning. For improved context learning we also introduce Cross-Modal Relative Attention (CMRA). By incorporating CMRA into the model, it becomes more adept at learning and utilizing complex relationships within the data. To further enhance the Conformer model, unsupervised pretraining called Regressional Feature Extraction is conducted on a curated sign language dataset. The pretrained Conformer is then fine-tuned for the downstream recognition task. The experimental results confirm the effectiveness of the adopted pretraining strategy and demonstrate how CMRA contributes to the recognition process. Remarkably, leveraging a Conformer-based backbone, our model achieves state-of-the-art performance on the benchmark datasets: PHOENIX-2014 and PHOENIX-2014T.","sentences":["Conventional Deep Learning frameworks for continuous sign language recognition (CSLR) are comprised of a single or multi-modal feature extractor, a sequence-learning module, and a decoder for outputting the glosses.","The sequence learning module is a crucial part wherein transformers have demonstrated their efficacy in the sequence-to-sequence tasks.","Analyzing the research progress in the field of Natural Language Processing and Speech Recognition, a rapid introduction of various transformer variants is observed.","However, in the realm of sign language, experimentation in the sequence learning component is limited.","In this work, the state-of-the-art Conformer model for Speech Recognition is adapted for CSLR and the proposed model is termed ConSignformer.","This marks the first instance of employing Conformer for a vision-based task.","ConSignformer has bimodal pipeline of CNN as feature extractor and Conformer for sequence learning.","For improved context learning we also introduce Cross-Modal Relative Attention (CMRA).","By incorporating CMRA into the model, it becomes more adept at learning and utilizing complex relationships within the data.","To further enhance the Conformer model, unsupervised pretraining called Regressional Feature Extraction is conducted on a curated sign language dataset.","The pretrained Conformer is then fine-tuned for the downstream recognition task.","The experimental results confirm the effectiveness of the adopted pretraining strategy and demonstrate how CMRA contributes to the recognition process.","Remarkably, leveraging a Conformer-based backbone, our model achieves state-of-the-art performance on the benchmark datasets: PHOENIX-2014 and PHOENIX-2014T."],"url":"http://arxiv.org/abs/2405.12018v1","category":"cs.CV"}
{"created":"2024-05-20 13:19:02","title":"Mamba-in-Mamba: Centralized Mamba-Cross-Scan in Tokenized Mamba Model for Hyperspectral Image Classification","abstract":"Hyperspectral image (HSI) classification is pivotal in the remote sensing (RS) field, particularly with the advancement of deep learning techniques. Sequential models, adapted from the natural language processing (NLP) field such as Recurrent Neural Networks (RNNs) and Transformers, have been tailored to this task, offering a unique viewpoint. However, several challenges persist 1) RNNs struggle with centric feature aggregation and are sensitive to interfering pixels, 2) Transformers require significant computational resources and often underperform with limited HSI training samples, and 3) Current scanning methods for converting images into sequence-data are simplistic and inefficient. In response, this study introduces the innovative Mamba-in-Mamba (MiM) architecture for HSI classification, the first attempt of deploying State Space Model (SSM) in this task. The MiM model includes 1) A novel centralized Mamba-Cross-Scan (MCS) mechanism for transforming images into sequence-data, 2) A Tokenized Mamba (T-Mamba) encoder that incorporates a Gaussian Decay Mask (GDM), a Semantic Token Learner (STL), and a Semantic Token Fuser (STF) for enhanced feature generation and concentration, and 3) A Weighted MCS Fusion (WMF) module coupled with a Multi-Scale Loss Design to improve decoding efficiency. Experimental results from three public HSI datasets with fixed and disjoint training-testing samples demonstrate that our method outperforms existing baselines and state-of-the-art approaches, highlighting its efficacy and potential in HSI applications.","sentences":["Hyperspectral image (HSI) classification is pivotal in the remote sensing (RS) field, particularly with the advancement of deep learning techniques.","Sequential models, adapted from the natural language processing (NLP) field such as Recurrent Neural Networks (RNNs) and Transformers, have been tailored to this task, offering a unique viewpoint.","However, several challenges persist 1) RNNs struggle with centric feature aggregation and are sensitive to interfering pixels, 2) Transformers require significant computational resources and often underperform with limited HSI training samples, and 3) Current scanning methods for converting images into sequence-data are simplistic and inefficient.","In response, this study introduces the innovative Mamba-in-Mamba (MiM) architecture for HSI classification, the first attempt of deploying State Space Model (SSM) in this task.","The MiM model includes 1) A novel centralized Mamba-Cross-Scan (MCS) mechanism for transforming images into sequence-data, 2) A Tokenized Mamba (T-Mamba) encoder that incorporates a Gaussian Decay Mask (GDM), a Semantic Token Learner (STL), and a Semantic Token Fuser (STF) for enhanced feature generation and concentration, and 3) A Weighted MCS Fusion (WMF) module coupled with a Multi-Scale Loss Design to improve decoding efficiency.","Experimental results from three public HSI datasets with fixed and disjoint training-testing samples demonstrate that our method outperforms existing baselines and state-of-the-art approaches, highlighting its efficacy and potential in HSI applications."],"url":"http://arxiv.org/abs/2405.12003v1","category":"cs.CV"}
{"created":"2024-05-20 13:06:04","title":"Max-Min Fairness and PHY-Layer Design of Uplink MIMO Rate-Splitting Multiple Access with Finite Blocklength","abstract":"Rate-Splitting Multiple Access (RSMA) has emerged as a potent and reliable multiple access and interference management technique in wireless communications. While downlink Multiple-Input Multiple-Ouput (MIMO) RSMA has been widely investigated, uplink MIMO RSMA has not been fully explored. In this paper, we investigate the performance of uplink RSMA in short-packet communications with perfect Channel State Information at Transmitter (CSIT) and Channel State Information at Receiver (CSIR). We propose an uplink MIMO RSMA framework and optimize both precoders and combiners with Max-Min Fairness (MMF) metric and Finite Blocklength (FBL) constraints. Due to the high coupling between precoders and combiners, we apply the Alternating Optimization (AO) to decompose the optimization problem into two subproblems. To tackle these subproblems, we propose a Successive Convex Approximation (SCA)-based approach. Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver. Subsequently, the Physical (PHY)-layer of the uplink MIMO RSMA architecture is designed and evaluated using multi-user Link-Level Simulations (LLS), accounting for finite constellation modulation, finite length polar codes, message splitting, adaptive modulation and coding, and Successive Interference Cancellation (SIC) at the receiver. Numerical results demonstrate that applying RSMA in uplink MIMO with FBL constraints not only achieves MMF gains over conventional transmission schemes such as Space Division Multiple Access (SDMA) and Non-orthogonal Multiple Access (NOMA) but also exhibits robustness to network loads. The benefits of splitting messages from multiple users are also illustrated. LLS results confirm the improved max-min throughput benefits of RSMA over SDMA and NOMA.","sentences":["Rate-Splitting Multiple Access (RSMA) has emerged as a potent and reliable multiple access and interference management technique in wireless communications.","While downlink Multiple-Input Multiple-Ouput (MIMO) RSMA has been widely investigated, uplink MIMO RSMA has not been fully explored.","In this paper, we investigate the performance of uplink RSMA in short-packet communications with perfect Channel State Information at Transmitter (CSIT) and Channel State Information at Receiver (CSIR).","We propose an uplink MIMO RSMA framework and optimize both precoders and combiners with Max-Min Fairness (MMF) metric and Finite Blocklength (FBL) constraints.","Due to the high coupling between precoders and combiners, we apply the Alternating Optimization (AO) to decompose the optimization problem into two subproblems.","To tackle these subproblems, we propose a Successive Convex Approximation (SCA)-based approach.","Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver.","Subsequently, the Physical (PHY)-layer of the uplink MIMO RSMA architecture is designed and evaluated using multi-user Link-Level Simulations (LLS), accounting for finite constellation modulation, finite length polar codes, message splitting, adaptive modulation and coding, and Successive Interference Cancellation (SIC) at the receiver.","Numerical results demonstrate that applying RSMA in uplink MIMO with FBL constraints not only achieves MMF gains over conventional transmission schemes such as Space Division Multiple Access (SDMA) and Non-orthogonal Multiple Access (NOMA) but also exhibits robustness to network loads.","The benefits of splitting messages from multiple users are also illustrated.","LLS results confirm the improved max-min throughput benefits of RSMA over SDMA and NOMA."],"url":"http://arxiv.org/abs/2405.11996v1","category":"cs.IT"}
{"created":"2024-05-20 12:54:57","title":"GGAvatar: Geometric Adjustment of Gaussian Head Avatar","abstract":"We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula's limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics.","sentences":["We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations.","GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster.","Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions.","Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula's limitations effectively.","Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics."],"url":"http://arxiv.org/abs/2405.11993v1","category":"cs.CV"}
{"created":"2024-05-20 12:39:28","title":"On Separation Logic, Computational Independence, and Pseudorandomness (Extended Version)","abstract":"Separation logic is a substructural logic which has proved to have numerous and fruitful applications to the verification of programs working on dynamic data structures. Recently, Barthe, Hsu and Liao have proposed a new way of giving semantics to separation logic formulas in which separating conjunction is interpreted in terms of probabilistic independence. The latter is taken in its exact form, i.e., two events are independent if and only if the joint probability is the product of the probabilities of the two events. There is indeed a literature on weaker notions of independence which are computational in nature, i.e. independence holds only against efficient adversaries and modulo a negligible probability of success. The aim of this work is to explore the nature of computational independence in a cryptographic scenario, in view of the aforementioned advances in separation logic. We show on the one hand that the semantics of separation logic can be adapted so as to account for complexity bounded adversaries, and on the other hand that the obtained logical system is useful for writing simple and compact proofs of standard cryptographic results in which the adversary remains hidden. Remarkably, this allows for a fruitful interplay between independence and pseudorandomness, itself a crucial notion in cryptography.","sentences":["Separation logic is a substructural logic which has proved to have numerous and fruitful applications to the verification of programs working on dynamic data structures.","Recently, Barthe, Hsu and Liao have proposed a new way of giving semantics to separation logic formulas in which separating conjunction is interpreted in terms of probabilistic independence.","The latter is taken in its exact form, i.e., two events are independent if and only if the joint probability is the product of the probabilities of the two events.","There is indeed a literature on weaker notions of independence which are computational in nature, i.e. independence holds only against efficient adversaries and modulo a negligible probability of success.","The aim of this work is to explore the nature of computational independence in a cryptographic scenario, in view of the aforementioned advances in separation logic.","We show on the one hand that the semantics of separation logic can be adapted so as to account for complexity bounded adversaries, and on the other hand that the obtained logical system is useful for writing simple and compact proofs of standard cryptographic results in which the adversary remains hidden.","Remarkably, this allows for a fruitful interplay between independence and pseudorandomness, itself a crucial notion in cryptography."],"url":"http://arxiv.org/abs/2405.11987v1","category":"cs.CR"}
{"created":"2024-05-20 12:11:41","title":"Position-Guided Prompt Learning for Anomaly Detection in Chest X-Rays","abstract":"Anomaly detection in chest X-rays is a critical task. Most methods mainly model the distribution of normal images, and then regard significant deviation from normal distribution as anomaly. Recently, CLIP-based methods, pre-trained on a large number of medical images, have shown impressive performance on zero/few-shot downstream tasks. In this paper, we aim to explore the potential of CLIP-based methods for anomaly detection in chest X-rays. Considering the discrepancy between the CLIP pre-training data and the task-specific data, we propose a position-guided prompt learning method. Specifically, inspired by the fact that experts diagnose chest X-rays by carefully examining distinct lung regions, we propose learnable position-guided text and image prompts to adapt the task data to the frozen pre-trained CLIP-based model. To enhance the model's discriminative capability, we propose a novel structure-preserving anomaly synthesis method within chest x-rays during the training process. Extensive experiments on three datasets demonstrate that our proposed method outperforms some state-of-the-art methods. The code of our implementation is available at https://github.com/sunzc-sunny/PPAD.","sentences":["Anomaly detection in chest X-rays is a critical task.","Most methods mainly model the distribution of normal images, and then regard significant deviation from normal distribution as anomaly.","Recently, CLIP-based methods, pre-trained on a large number of medical images, have shown impressive performance on zero/few-shot downstream tasks.","In this paper, we aim to explore the potential of CLIP-based methods for anomaly detection in chest X-rays.","Considering the discrepancy between the CLIP pre-training data and the task-specific data, we propose a position-guided prompt learning method.","Specifically, inspired by the fact that experts diagnose chest X-rays by carefully examining distinct lung regions, we propose learnable position-guided text and image prompts to adapt the task data to the frozen pre-trained CLIP-based model.","To enhance the model's discriminative capability, we propose a novel structure-preserving anomaly synthesis method within chest x-rays during the training process.","Extensive experiments on three datasets demonstrate that our proposed method outperforms some state-of-the-art methods.","The code of our implementation is available at https://github.com/sunzc-sunny/PPAD."],"url":"http://arxiv.org/abs/2405.11976v1","category":"cs.CV"}
{"created":"2024-05-20 11:22:03","title":"PET: Multi-agent Independent PPO-based Automatic ECN Tuning for High-Speed Data Center Networks","abstract":"Explicit Congestion Notification (ECN)-based congestion control schemes have been widely adopted in high-speed data center networks (DCNs), where the ECN marking threshold plays a determinant role in guaranteeing a packet lossless DCN. However, existing approaches either employ static settings with immutable thresholds that cannot be dynamically self-adjusted to adapt to network dynamics, or fail to take into account many-to-one traffic patterns and different requirements of different types of traffic, resulting in relatively poor performance. To address these problems, this paper proposes a novel learning-based automatic ECN tuning scheme, named PET, based on the multi-agent Independent Proximal Policy Optimization (IPPO) algorithm. PET dynamically adjusts ECN thresholds by fully considering pivotal congestion-contributing factors, including queue length, output data rate, output rate of ECN-marked packets, current ECN threshold, the extent of incast, and the ratio of mice and elephant flows. PET adopts the Decentralized Training and Decentralized Execution (DTDE) paradigm and combines offline and online training to accommodate network dynamics. PET is also fair and readily deployable with commodity hardware. Comprehensive experimental results demonstrate that, compared with state-of-the-art static schemes and the learning-based automatic scheme, our PET achieves better performance in terms of flow completion time, convergence rate, queue length variance, and system robustness.","sentences":["Explicit Congestion Notification (ECN)-based congestion control schemes have been widely adopted in high-speed data center networks (DCNs), where the ECN marking threshold plays a determinant role in guaranteeing a packet lossless DCN.","However, existing approaches either employ static settings with immutable thresholds that cannot be dynamically self-adjusted to adapt to network dynamics, or fail to take into account many-to-one traffic patterns and different requirements of different types of traffic, resulting in relatively poor performance.","To address these problems, this paper proposes a novel learning-based automatic ECN tuning scheme, named PET, based on the multi-agent Independent Proximal Policy Optimization (IPPO) algorithm.","PET dynamically adjusts ECN thresholds by fully considering pivotal congestion-contributing factors, including queue length, output data rate, output rate of ECN-marked packets, current ECN threshold, the extent of incast, and the ratio of mice and elephant flows.","PET adopts the Decentralized Training and Decentralized Execution (DTDE) paradigm and combines offline and online training to accommodate network dynamics.","PET is also fair and readily deployable with commodity hardware.","Comprehensive experimental results demonstrate that, compared with state-of-the-art static schemes and the learning-based automatic scheme, our PET achieves better performance in terms of flow completion time, convergence rate, queue length variance, and system robustness."],"url":"http://arxiv.org/abs/2405.11956v1","category":"cs.NI"}
{"created":"2024-05-20 10:54:47","title":"WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles","abstract":"This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists. Large language models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned and employed to create lay summaries from complex scientific texts. The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information. The experiments demonstrated that fine-tuning generally led to the best performance across most evaluated metrics. Few-shot learning notably improved the models' ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed. Out of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance. Determined by the overall score, our approach improved upon the baseline by approx. 5.5 percentage points and was only approx 1.5 percentage points behind the first place.","sentences":["This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists.","Large language models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned and employed to create lay summaries from complex scientific texts.","The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information.","The experiments demonstrated that fine-tuning generally led to the best performance across most evaluated metrics.","Few-shot learning notably improved the models' ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt.","Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed.","Out of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance.","Determined by the overall score, our approach improved upon the baseline by approx.","5.5 percentage points and was only approx 1.5 percentage points behind the first place."],"url":"http://arxiv.org/abs/2405.11950v1","category":"cs.CL"}
{"created":"2024-05-20 09:48:15","title":"ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation","abstract":"Human annotation is a time-consuming task that requires a significant amount of effort. To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct. However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort. To tackle this challenge, we propose Araida, an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections. Araida involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate. Empirical studies demonstrate that Araida is adaptable to different annotation tasks and models. On average, it reduces human correction labor by 11.02% compared to vanilla interactive data annotation methods.","sentences":["Human annotation is a time-consuming task that requires a significant amount of effort.","To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct.","However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort.","To tackle this challenge, we propose Araida, an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections.","Araida involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate.","Empirical studies demonstrate that Araida is adaptable to different annotation tasks and models.","On average, it reduces human correction labor by 11.02% compared to vanilla interactive data annotation methods."],"url":"http://arxiv.org/abs/2405.11912v1","category":"cs.CL"}
{"created":"2024-05-20 09:30:38","title":"Model-Based Qubit Noise Spectroscopy","abstract":"Qubit noise spectroscopy (QNS) is a valuable tool for both the characterization of a qubit's environment and as a precursor to more effective qubit control to improve qubit fidelities. Existing approaches to QNS are what the classical spectrum estimation literature would call \"non-parametric\" approaches, in that a series of probe sequences are used to estimate noise power at a set of points or bands. In contrast, model-based approaches to spectrum estimation assume additional structure in the form of the spectrum and leverage this for improved statistical accuracy or other capabilities, such as superresolution. Here, we derive model-based QNS approaches using inspiration from classical signal processing, primarily though the recently developed Schrodinger wave autoregressive moving-average (SchWARMA) formalism for modeling correlated noise. We show, through both simulation and experimental data, how these model-based QNS approaches maintain the statistical and computational benefits of their classical counterparts, resulting in powerful new estimation approaches. Beyond the direct application of these approaches to QNS and quantum sensing, we anticipate that the flexibility of the underlying models will find utility in adaptive feedback control for quantum systems, in analogy with their role in classical adaptive signal processing and control.","sentences":["Qubit noise spectroscopy (QNS) is a valuable tool for both the characterization of a qubit's environment and as a precursor to more effective qubit control to improve qubit fidelities.","Existing approaches to QNS are what the classical spectrum estimation literature would call \"non-parametric\" approaches, in that a series of probe sequences are used to estimate noise power at a set of points or bands.","In contrast, model-based approaches to spectrum estimation assume additional structure in the form of the spectrum and leverage this for improved statistical accuracy or other capabilities, such as superresolution.","Here, we derive model-based QNS approaches using inspiration from classical signal processing, primarily though the recently developed Schrodinger wave autoregressive moving-average (SchWARMA) formalism for modeling correlated noise.","We show, through both simulation and experimental data, how these model-based QNS approaches maintain the statistical and computational benefits of their classical counterparts, resulting in powerful new estimation approaches.","Beyond the direct application of these approaches to QNS and quantum sensing, we anticipate that the flexibility of the underlying models will find utility in adaptive feedback control for quantum systems, in analogy with their role in classical adaptive signal processing and control."],"url":"http://arxiv.org/abs/2405.11898v1","category":"quant-ph"}
{"created":"2024-05-20 08:42:56","title":"Automorphism groups of prime models, and invariant measures","abstract":"We adapt the notion of a (relatively) definable subset of Aut(M) when M is a saturated model to the case Aut(M/A) when M is atomic and strongly omega-homogeneous over A. We discuss the existence and uniqueness of invariant measures on the Boolean algebra of definable subsets of Aut(M/A). For example when Th(M) is stable we have existence and uniqueness. We also discuss the compatibility of our definability notions with definable Galois cohomology and differential Galois theory.","sentences":["We adapt the notion of a (relatively) definable subset of Aut(M) when M is a saturated model to the case Aut(M/A) when M is atomic and strongly omega-homogeneous over A. We discuss the existence and uniqueness of invariant measures on the Boolean algebra of definable subsets of Aut(M/A).","For example when Th(M) is stable we have existence and uniqueness.","We also discuss the compatibility of our definability notions with definable Galois cohomology and differential Galois theory."],"url":"http://arxiv.org/abs/2405.11878v1","category":"math.LO"}
{"created":"2024-05-20 07:54:03","title":"Evolving Storytelling: Benchmarks and Methods for New Character Customization with Diffusion Models","abstract":"Diffusion-based models for story visualization have shown promise in generating content-coherent images for storytelling tasks. However, how to effectively integrate new characters into existing narratives while maintaining character consistency remains an open problem, particularly with limited data. Two major limitations hinder the progress: (1) the absence of a suitable benchmark due to potential character leakage and inconsistent text labeling, and (2) the challenge of distinguishing between new and old characters, leading to ambiguous results. To address these challenges, we introduce the NewEpisode benchmark, comprising refined datasets designed to evaluate generative models' adaptability in generating new stories with fresh characters using just a single example story. The refined dataset involves refined text prompts and eliminates character leakage. Additionally, to mitigate the character confusion of generated results, we propose EpicEvo, a method that customizes a diffusion-based visual story generation model with a single story featuring the new characters seamlessly integrating them into established character dynamics. EpicEvo introduces a novel adversarial character alignment module to align the generated images progressively in the diffusive process, with exemplar images of new characters, while applying knowledge distillation to prevent forgetting of characters and background details. Our evaluation quantitatively demonstrates that EpicEvo outperforms existing baselines on the NewEpisode benchmark, and qualitative studies confirm its superior customization of visual story generation in diffusion models. In summary, EpicEvo provides an effective way to incorporate new characters using only one example story, unlocking new possibilities for applications such as serialized cartoons.","sentences":["Diffusion-based models for story visualization have shown promise in generating content-coherent images for storytelling tasks.","However, how to effectively integrate new characters into existing narratives while maintaining character consistency remains an open problem, particularly with limited data.","Two major limitations hinder the progress: (1) the absence of a suitable benchmark due to potential character leakage and inconsistent text labeling, and (2) the challenge of distinguishing between new and old characters, leading to ambiguous results.","To address these challenges, we introduce the NewEpisode benchmark, comprising refined datasets designed to evaluate generative models' adaptability in generating new stories with fresh characters using just a single example story.","The refined dataset involves refined text prompts and eliminates character leakage.","Additionally, to mitigate the character confusion of generated results, we propose EpicEvo, a method that customizes a diffusion-based visual story generation model with a single story featuring the new characters seamlessly integrating them into established character dynamics.","EpicEvo introduces a novel adversarial character alignment module to align the generated images progressively in the diffusive process, with exemplar images of new characters, while applying knowledge distillation to prevent forgetting of characters and background details.","Our evaluation quantitatively demonstrates that EpicEvo outperforms existing baselines on the NewEpisode benchmark, and qualitative studies confirm its superior customization of visual story generation in diffusion models.","In summary, EpicEvo provides an effective way to incorporate new characters using only one example story, unlocking new possibilities for applications such as serialized cartoons."],"url":"http://arxiv.org/abs/2405.11852v1","category":"cs.CV"}
{"created":"2024-05-20 06:12:33","title":"FedCAda: Adaptive Client-Side Optimization for Accelerated and Stable Federated Learning","abstract":"Federated learning (FL) has emerged as a prominent approach for collaborative training of machine learning models across distributed clients while preserving data privacy. However, the quest to balance acceleration and stability becomes a significant challenge in FL, especially on the client-side. In this paper, we introduce FedCAda, an innovative federated client adaptive algorithm designed to tackle this challenge. FedCAda leverages the Adam algorithm to adjust the correction process of the first moment estimate $m$ and the second moment estimate $v$ on the client-side and aggregate adaptive algorithm parameters on the server-side, aiming to accelerate convergence speed and communication efficiency while ensuring stability and performance. Additionally, we investigate several algorithms incorporating different adjustment functions. This comparative analysis revealed that due to the limited information contained within client models from other clients during the initial stages of federated learning, more substantial constraints need to be imposed on the parameters of the adaptive algorithm. As federated learning progresses and clients gather more global information, FedCAda gradually diminishes the impact on adaptive parameters. These findings provide insights for enhancing the robustness and efficiency of algorithmic improvements. Through extensive experiments on computer vision (CV) and natural language processing (NLP) datasets, we demonstrate that FedCAda outperforms the state-of-the-art methods in terms of adaptability, convergence, stability, and overall performance. This work contributes to adaptive algorithms for federated learning, encouraging further exploration.","sentences":["Federated learning (FL) has emerged as a prominent approach for collaborative training of machine learning models across distributed clients while preserving data privacy.","However, the quest to balance acceleration and stability becomes a significant challenge in FL, especially on the client-side.","In this paper, we introduce FedCAda, an innovative federated client adaptive algorithm designed to tackle this challenge.","FedCAda leverages the Adam algorithm to adjust the correction process of the first moment estimate $m$ and the second moment estimate $v$ on the client-side and aggregate adaptive algorithm parameters on the server-side, aiming to accelerate convergence speed and communication efficiency while ensuring stability and performance.","Additionally, we investigate several algorithms incorporating different adjustment functions.","This comparative analysis revealed that due to the limited information contained within client models from other clients during the initial stages of federated learning, more substantial constraints need to be imposed on the parameters of the adaptive algorithm.","As federated learning progresses and clients gather more global information, FedCAda gradually diminishes the impact on adaptive parameters.","These findings provide insights for enhancing the robustness and efficiency of algorithmic improvements.","Through extensive experiments on computer vision (CV) and natural language processing (NLP) datasets, we demonstrate that FedCAda outperforms the state-of-the-art methods in terms of adaptability, convergence, stability, and overall performance.","This work contributes to adaptive algorithms for federated learning, encouraging further exploration."],"url":"http://arxiv.org/abs/2405.11811v1","category":"cs.LG"}
{"created":"2024-05-20 06:00:12","title":"Dual-sided Peltier Elements for Rapid Thermal Feedback in Wearables","abstract":"This paper introduces a motor-driven Peltier device designed to deliver immediate thermal sensations within extended reality (XR) environments. The system incorporates eight motor-driven Peltier elements, facilitating swift transitions between warm and cool sensations by rotating preheated or cooled elements to opposite sides. A multi-layer structure, comprising aluminum and silicone layers, ensures user comfort and safety while maintaining optimal temperatures for thermal stimuli. Time-temperature characteristic analysis demonstrates the system's ability to provide warm and cool sensations efficiently, with a dual-sided lifetime of up to 206 seconds at a 2V input. Our system design is adaptable to various body parts and can be synchronized with corresponding visual stimuli to enhance the immersive sensation of virtual object interaction and information delivery.","sentences":["This paper introduces a motor-driven Peltier device designed to deliver immediate thermal sensations within extended reality (XR) environments.","The system incorporates eight motor-driven Peltier elements, facilitating swift transitions between warm and cool sensations by rotating preheated or cooled elements to opposite sides.","A multi-layer structure, comprising aluminum and silicone layers, ensures user comfort and safety while maintaining optimal temperatures for thermal stimuli.","Time-temperature characteristic analysis demonstrates the system's ability to provide warm and cool sensations efficiently, with a dual-sided lifetime of up to 206 seconds at a 2V input.","Our system design is adaptable to various body parts and can be synchronized with corresponding visual stimuli to enhance the immersive sensation of virtual object interaction and information delivery."],"url":"http://arxiv.org/abs/2405.11807v1","category":"cs.HC"}
{"created":"2024-05-20 05:50:25","title":"Learning of Balance Controller Considering Changes in Body State for Musculoskeletal Humanoids","abstract":"The musculoskeletal humanoid is difficult to modelize due to the flexibility and redundancy of its body, whose state can change over time, and so balance control of its legs is challenging. There are some cases where ordinary PID controls may cause instability. In this study, to solve these problems, we propose a method of learning a correlation model among the joint angle, muscle tension, and muscle length of the ankle and the zero moment point to perform balance control. In addition, information on the changing body state is embedded in the model using parametric bias, and the model estimates and adapts to the current body state by learning this information online. This makes it possible to adapt to changes in upper body posture that are not directly taken into account in the model, since it is difficult to learn the complete dynamics of the whole body considering the amount of data and computation. The model can also adapt to changes in body state, such as the change in footwear and change in the joint origin due to recalibration. The effectiveness of this method is verified by a simulation and by using an actual musculoskeletal humanoid, Musashi.","sentences":["The musculoskeletal humanoid is difficult to modelize due to the flexibility and redundancy of its body, whose state can change over time, and so balance control of its legs is challenging.","There are some cases where ordinary PID controls may cause instability.","In this study, to solve these problems, we propose a method of learning a correlation model among the joint angle, muscle tension, and muscle length of the ankle and the zero moment point to perform balance control.","In addition, information on the changing body state is embedded in the model using parametric bias, and the model estimates and adapts to the current body state by learning this information online.","This makes it possible to adapt to changes in upper body posture that are not directly taken into account in the model, since it is difficult to learn the complete dynamics of the whole body considering the amount of data and computation.","The model can also adapt to changes in body state, such as the change in footwear and change in the joint origin due to recalibration.","The effectiveness of this method is verified by a simulation and by using an actual musculoskeletal humanoid, Musashi."],"url":"http://arxiv.org/abs/2405.11803v1","category":"cs.RO"}
{"created":"2024-05-20 05:44:11","title":"Self-Supervised Learning of Visual Servoing for Low-Rigidity Robots Considering Temporal Body Changes","abstract":"In this study, we investigate object grasping by visual servoing in a low-rigidity robot. It is difficult for a low-rigidity robot to handle its own body as intended compared to a rigid robot, and calibration between vision and body takes some time. In addition, the robot must constantly adapt to changes in its body, such as the change in camera position and change in joints due to aging. Therefore, we develop a method for a low-rigidity robot to autonomously learn visual servoing of its body. We also develop a mechanism that can adaptively change its visual servoing according to temporal body changes. We apply our method to a low-rigidity 6-axis arm, MyCobot, and confirm its effectiveness by conducting object grasping experiments based on visual servoing.","sentences":["In this study, we investigate object grasping by visual servoing in a low-rigidity robot.","It is difficult for a low-rigidity robot to handle its own body as intended compared to a rigid robot, and calibration between vision and body takes some time.","In addition, the robot must constantly adapt to changes in its body, such as the change in camera position and change in joints due to aging.","Therefore, we develop a method for a low-rigidity robot to autonomously learn visual servoing of its body.","We also develop a mechanism that can adaptively change its visual servoing according to temporal body changes.","We apply our method to a low-rigidity 6-axis arm, MyCobot, and confirm its effectiveness by conducting object grasping experiments based on visual servoing."],"url":"http://arxiv.org/abs/2405.11798v1","category":"cs.RO"}
{"created":"2024-05-20 05:29:45","title":"Application of time-series quantum generative model to financial data","abstract":"Despite proposing a quantum generative model for time series that successfully learns correlated series with multiple Brownian motions, the model has not been adapted and evaluated for financial problems. In this study, a time-series generative model was applied as a quantum generative model to actual financial data. Future data for two correlated time series were generated and compared with classical methods such as long short-term memory and vector autoregression. Furthermore, numerical experiments were performed to complete missing values. Based on the results, we evaluated the practical applications of the time-series quantum generation model. It was observed that fewer parameter values were required compared with the classical method. In addition, the quantum time-series generation model was feasible for both stationary and nonstationary data. These results suggest that several parameters can be applied to various types of time-series data.","sentences":["Despite proposing a quantum generative model for time series that successfully learns correlated series with multiple Brownian motions, the model has not been adapted and evaluated for financial problems.","In this study, a time-series generative model was applied as a quantum generative model to actual financial data.","Future data for two correlated time series were generated and compared with classical methods such as long short-term memory and vector autoregression.","Furthermore, numerical experiments were performed to complete missing values.","Based on the results, we evaluated the practical applications of the time-series quantum generation model.","It was observed that fewer parameter values were required compared with the classical method.","In addition, the quantum time-series generation model was feasible for both stationary and nonstationary data.","These results suggest that several parameters can be applied to various types of time-series data."],"url":"http://arxiv.org/abs/2405.11795v1","category":"quant-ph"}
{"created":"2024-05-20 05:09:36","title":"Enhanced dissipation and stability of Poiseuille flow for two-dimensional Boussinesq system","abstract":"We investigate the nonlinear stability problem for the two-dimensional Boussinesq system around the Poiseuille flow in a finite channel. The system has the characteristic of Navier-slip boundary condition for the velocity and Dirichlet boundary condition for the temperature, with a small viscosity $\\nu$ and small thermal diffusion $\\mu,$ respectively. More precisely, we prove that if the initial velocity and initial temperature satisfies$$||u_{0}-(1-y^2,0) ||_{H^{\\frac{7}{2}+}}\\leq c_0\\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{2}{3}}$$ and $$ ||\\theta_{0}||_{H^1}+|||D_x|^{\\frac{1}{8}}\\theta_{0}||_{H^1}\\leq c_1\\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{{\\frac{31}{24}}}$$ for some small constants $c_0$ and $c_1$ which are both independent of $\\mu,\\nu$, then we can reach the conclusion that the velocity remains within $O\\left( \\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{2}{3}}\\right) $ of the Poiseuille flow; the temperature remains $O\\left( \\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{31}{24}}\\right) $ of the constant 0, and approaches to 0 as $t\\rightarrow\\infty.$","sentences":["We investigate the nonlinear stability problem for the two-dimensional Boussinesq system around the Poiseuille flow in a finite channel.","The system has the characteristic of Navier-slip boundary condition for the velocity and Dirichlet boundary condition for the temperature, with a small viscosity $\\nu$ and small thermal diffusion $\\mu,$ respectively.","More precisely, we prove that if the initial velocity and initial temperature satisfies$$||u_{0}-(1-y^2,0) ||_{H^{\\frac{7}{2}+}}\\leq c_0\\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{2}{3}}$$ and $$ ||\\theta_{0}||_{H^1}+|||D_x|^{\\frac{1}{8}}\\theta_{0}||_{H^1}\\leq c_1\\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{{\\frac{31}{24}}}$$ for some small constants $c_0$ and $c_1$ which are both independent of $\\mu,\\nu$, then we can reach the conclusion that the velocity remains within $O\\left( \\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{2}{3}}\\right) $ of the Poiseuille flow; the temperature remains $O\\left( \\min\\left\\lbrace \\mu,\\nu\\right\\rbrace ^{\\frac{31}{24}}\\right) $ of the constant 0, and approaches to 0 as $t\\rightarrow\\infty.$"],"url":"http://arxiv.org/abs/2405.11787v1","category":"math.AP"}
{"created":"2024-05-20 03:57:55","title":"An Improved Design for All-Photonic Quantum Repeaters","abstract":"All-photonic quantum repeaters use multi-qubit photonic graph states, called repeater graph states (RGS), instead of matter-based quantum memories, for protection against predominantly loss errors. The RGS comprises tree-graph-encoded logical qubits for error correction at the repeaters and physical {\\em link} qubits to create entanglement between neighboring repeaters. The two methods to generate the RGS are probabilistic stitching -- using linear optical Bell state measurements (fusion) -- of small entangled states prepared via multiplexed-probabilistic linear optical circuits fed with single photons, and a direct deterministic preparation using a small number of quantum-logic-capable solid-state emitters. The resource overhead due to fusions and the circuit depth of the quantum emitter system both increase with the size of the RGS. Therefore engineering a resource-efficient RGS is crucial. We propose a new RGS design, which achieves a higher entanglement rate for all-photonic quantum repeaters using fewer qubits than the previously known RGS would. We accomplish this by boosting the probability of entangling neighboring repeaters with tree-encoded link qubits. We also propose a new adaptive scheme to perform logical BSM on the link qubits for loss-only errors. The adaptive BSM outperforms the previous schemes for logical BSM on tree codes when the qubit loss probability is uniform. It reduces the number of optical modes required to perform logical BSM on link qubits to improve the entanglement rate further.","sentences":["All-photonic quantum repeaters use multi-qubit photonic graph states, called repeater graph states (RGS), instead of matter-based quantum memories, for protection against predominantly loss errors.","The RGS comprises tree-graph-encoded logical qubits for error correction at the repeaters and physical {\\em link} qubits to create entanglement between neighboring repeaters.","The two methods to generate the RGS are probabilistic stitching -- using linear optical Bell state measurements (fusion) -- of small entangled states prepared via multiplexed-probabilistic linear optical circuits fed with single photons, and a direct deterministic preparation using a small number of quantum-logic-capable solid-state emitters.","The resource overhead due to fusions and the circuit depth of the quantum emitter system both increase with the size of the RGS.","Therefore engineering a resource-efficient RGS is crucial.","We propose a new RGS design, which achieves a higher entanglement rate for all-photonic quantum repeaters using fewer qubits than the previously known RGS would.","We accomplish this by boosting the probability of entangling neighboring repeaters with tree-encoded link qubits.","We also propose a new adaptive scheme to perform logical BSM on the link qubits for loss-only errors.","The adaptive BSM outperforms the previous schemes for logical BSM on tree codes when the qubit loss probability is uniform.","It reduces the number of optical modes required to perform logical BSM on link qubits to improve the entanglement rate further."],"url":"http://arxiv.org/abs/2405.11768v1","category":"quant-ph"}
{"created":"2024-05-20 03:33:12","title":"Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning","abstract":"Semi-supervised learning (SSL) has witnessed remarkable progress, resulting in the emergence of numerous method variations. However, practitioners often encounter challenges when attempting to deploy these methods due to their subpar performance. In this paper, we present a novel SSL approach named FineSSL that significantly addresses this limitation by adapting pre-trained foundation models. We identify the aggregated biases and cognitive deviation problems inherent in foundation models, and propose a simple yet effective solution by imposing balanced margin softmax and decoupled label smoothing. Through extensive experiments, we demonstrate that FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces the training cost by over six times, and can seamlessly integrate various fine-tuning and modern SSL algorithms. The source code is available at https://github.com/Gank0078/FineSSL.","sentences":["Semi-supervised learning (SSL) has witnessed remarkable progress, resulting in the emergence of numerous method variations.","However, practitioners often encounter challenges when attempting to deploy these methods due to their subpar performance.","In this paper, we present a novel SSL approach named FineSSL that significantly addresses this limitation by adapting pre-trained foundation models.","We identify the aggregated biases and cognitive deviation problems inherent in foundation models, and propose a simple yet effective solution by imposing balanced margin softmax and decoupled label smoothing.","Through extensive experiments, we demonstrate that FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces the training cost by over six times, and can seamlessly integrate various fine-tuning and modern SSL algorithms.","The source code is available at https://github.com/Gank0078/FineSSL."],"url":"http://arxiv.org/abs/2405.11756v1","category":"cs.LG"}
{"created":"2024-05-20 03:31:43","title":"Versatile Teacher: A Class-aware Teacher-student Framework for Cross-domain Adaptation","abstract":"Addressing the challenge of domain shift between datasets is vital in maintaining model performance. In the context of cross-domain object detection, the teacher-student framework, a widely-used semi-supervised model, has shown significant accuracy improvements. However, existing methods often overlook class differences, treating all classes equally, resulting in suboptimal results. Furthermore, the integration of instance-level alignment with a one-stage detector, essential due to the absence of a Region Proposal Network (RPN), remains unexplored in this framework. In response to these shortcomings, we introduce a novel teacher-student model named Versatile Teacher (VT). VT differs from previous works by considering class-specific detection difficulty and employing a two-step pseudo-label selection mechanism, referred to as Class-aware Pseudo-label Adaptive Selection (CAPS), to generate more reliable pseudo labels. These labels are leveraged as saliency matrices to guide the discriminator for targeted instance-level alignment. Our method demonstrates promising results on three benchmark datasets, and extends the alignment methods for widely-used one-stage detectors, presenting significant potential for practical applications. Code is available at https://github.com/RicardooYoung/VersatileTeacher.","sentences":["Addressing the challenge of domain shift between datasets is vital in maintaining model performance.","In the context of cross-domain object detection, the teacher-student framework, a widely-used semi-supervised model, has shown significant accuracy improvements.","However, existing methods often overlook class differences, treating all classes equally, resulting in suboptimal results.","Furthermore, the integration of instance-level alignment with a one-stage detector, essential due to the absence of a Region Proposal Network (RPN), remains unexplored in this framework.","In response to these shortcomings, we introduce a novel teacher-student model named Versatile Teacher (VT).","VT differs from previous works by considering class-specific detection difficulty and employing a two-step pseudo-label selection mechanism, referred to as Class-aware Pseudo-label Adaptive Selection (CAPS), to generate more reliable pseudo labels.","These labels are leveraged as saliency matrices to guide the discriminator for targeted instance-level alignment.","Our method demonstrates promising results on three benchmark datasets, and extends the alignment methods for widely-used one-stage detectors, presenting significant potential for practical applications.","Code is available at https://github.com/RicardooYoung/VersatileTeacher."],"url":"http://arxiv.org/abs/2405.11754v1","category":"cs.CV"}
{"created":"2024-05-20 03:26:58","title":"Foundation Model for Chemical Process Modeling: Meta-Learning with Physics-Informed Adaptation","abstract":"In this work, we introduce a novel application of foundation models in the domain of nonlinear chemical process modeling. Given the challenges of obtaining accurate first-principles models for real-world chemical processes and the inefficiency of rebuilding and retraining models for new chemical processes, we pose a pivotal question: What if we could develop a single, universal neural network (i.e., foundation model) capable of rapidly adapting to modeling any new chemical process? To address this question, we propose a meta-learning-based approach using Reptile to construct the foundation model, followed by physics-informed adaptation to fine-tune it to new modeling tasks using only a few data samples. To assess the effectiveness of our methodology, we construct a foundation model for various chemical reactions in three classical generic reactors, including continuous stirred tank reactors (CSTRs), batch reactors (BRs), and plug flow reactors (PFRs). Our approach outperforms conventional methods such as data-driven learning, physics-informed learning, transfer learning, and pure meta-learning in a few-shot setting. Furthermore, our method achieves rapid adaptation to new CSTRs, BRs, and PFRs using only a few data samples from the designated tasks. Source code is available at https://github.com/killingbear999/chemical-process-foundation-model.","sentences":["In this work, we introduce a novel application of foundation models in the domain of nonlinear chemical process modeling.","Given the challenges of obtaining accurate first-principles models for real-world chemical processes and the inefficiency of rebuilding and retraining models for new chemical processes, we pose a pivotal question: What if we could develop a single, universal neural network (i.e., foundation model) capable of rapidly adapting to modeling any new chemical process?","To address this question, we propose a meta-learning-based approach using Reptile to construct the foundation model, followed by physics-informed adaptation to fine-tune it to new modeling tasks using only a few data samples.","To assess the effectiveness of our methodology, we construct a foundation model for various chemical reactions in three classical generic reactors, including continuous stirred tank reactors (CSTRs), batch reactors (BRs), and plug flow reactors (PFRs).","Our approach outperforms conventional methods such as data-driven learning, physics-informed learning, transfer learning, and pure meta-learning in a few-shot setting.","Furthermore, our method achieves rapid adaptation to new CSTRs, BRs, and PFRs using only a few data samples from the designated tasks.","Source code is available at https://github.com/killingbear999/chemical-process-foundation-model."],"url":"http://arxiv.org/abs/2405.11752v1","category":"cs.CE"}
{"created":"2024-05-20 00:58:53","title":"Adaptive Batch Normalization Networks for Adversarial Robustness","abstract":"Deep networks are vulnerable to adversarial examples. Adversarial Training (AT) has been a standard foundation of modern adversarial defense approaches due to its remarkable effectiveness. However, AT is extremely time-consuming, refraining it from wide deployment in practical applications. In this paper, we aim at a non-AT defense: How to design a defense method that gets rid of AT but is still robust against strong adversarial attacks? To answer this question, we resort to adaptive Batch Normalization (BN), inspired by the recent advances in test-time domain adaptation. We propose a novel defense accordingly, referred to as the Adaptive Batch Normalization Network (ABNN). ABNN employs a pre-trained substitute model to generate clean BN statistics and sends them to the target model. The target model is exclusively trained on clean data and learns to align the substitute model's BN statistics. Experimental results show that ABNN consistently improves adversarial robustness against both digital and physically realizable attacks on both image and video datasets. Furthermore, ABNN can achieve higher clean data performance and significantly lower training time complexity compared to AT-based approaches.","sentences":["Deep networks are vulnerable to adversarial examples.","Adversarial Training (AT) has been a standard foundation of modern adversarial defense approaches due to its remarkable effectiveness.","However, AT is extremely time-consuming, refraining it from wide deployment in practical applications.","In this paper, we aim at a non-AT defense: How to design a defense method that gets rid of AT but is still robust against strong adversarial attacks?","To answer this question, we resort to adaptive Batch Normalization (BN), inspired by the recent advances in test-time domain adaptation.","We propose a novel defense accordingly, referred to as the Adaptive Batch Normalization Network (ABNN).","ABNN employs a pre-trained substitute model to generate clean BN statistics and sends them to the target model.","The target model is exclusively trained on clean data and learns to align the substitute model's BN statistics.","Experimental results show that ABNN consistently improves adversarial robustness against both digital and physically realizable attacks on both image and video datasets.","Furthermore, ABNN can achieve higher clean data performance and significantly lower training time complexity compared to AT-based approaches."],"url":"http://arxiv.org/abs/2405.11708v1","category":"cs.LG"}
{"created":"2024-05-19 20:13:04","title":"Hyperbolic enhancement of a quantum battery","abstract":"A quantum system which can store energy, and from which one can extract useful work, is known as a quantum battery. Such a device raises interesting issues surrounding how quantum physics can provide certain advantages in the charging, energy storage or discharging of the quantum battery as compared to their classical equivalents. However, the pernicious effect of dissipation degrades the performance of any realistic battery. Here we show how one can circumvent this problem of energy loss by proposing a quantum battery model which benefits from quantum squeezing. Namely, charging the battery quadratically with a short temporal pulse induces a hyperbolic enhancement in the stored energy, such that the dissipation present becomes essentially negligible in comparison. Furthermore, we show that when the driving is strong enough the useful work which can be extracted from the quantum battery, that is the ergotropy, is exactly equal to the stored energy. These impressive properties imply a highly efficient quantum energetic device with abundant amounts of ergotropy. Our theoretical results suggest a possible route to realizing high-performance quantum batteries, which could be realized in a variety of platforms exploiting quantum continuous variables.","sentences":["A quantum system which can store energy, and from which one can extract useful work, is known as a quantum battery.","Such a device raises interesting issues surrounding how quantum physics can provide certain advantages in the charging, energy storage or discharging of the quantum battery as compared to their classical equivalents.","However, the pernicious effect of dissipation degrades the performance of any realistic battery.","Here we show how one can circumvent this problem of energy loss by proposing a quantum battery model which benefits from quantum squeezing.","Namely, charging the battery quadratically with a short temporal pulse induces a hyperbolic enhancement in the stored energy, such that the dissipation present becomes essentially negligible in comparison.","Furthermore, we show that when the driving is strong enough the useful work which can be extracted from the quantum battery, that is the ergotropy, is exactly equal to the stored energy.","These impressive properties imply a highly efficient quantum energetic device with abundant amounts of ergotropy.","Our theoretical results suggest a possible route to realizing high-performance quantum batteries, which could be realized in a variety of platforms exploiting quantum continuous variables."],"url":"http://arxiv.org/abs/2405.11662v1","category":"quant-ph"}
{"created":"2024-05-19 18:57:25","title":"Hummer: Towards Limited Competitive Preference Dataset","abstract":"Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback. However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others. In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets. We then present \\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative pairwise preference datasets with reduced-conflict alignment objectives. \\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment objectives. Furthermore, we develop reward models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to balance diverse alignment objectives effectively. This sampling method positions HummerRM as an ideal model for domain-specific further fine-tuning and reducing vulnerabilities to attacks.","sentences":["Preference datasets are essential for incorporating human preferences into pre-trained language models, playing a key role in the success of Reinforcement Learning from Human Feedback.","However, these datasets often demonstrate conflicting alignment objectives, leading to increased vulnerability to jailbreak attacks and challenges in adapting downstream tasks to prioritize specific alignment objectives without negatively impacting others.","In this work, we introduce a novel statistical metric, Alignment Dimension Conflict, to quantify the degree of conflict within preference datasets.","We then present \\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative pairwise preference datasets with reduced-conflict alignment objectives.","\\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback from GPT-4, marking as the first preference dataset aimed at reducing the competition between alignment objectives.","Furthermore, we develop reward models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to balance diverse alignment objectives effectively.","This sampling method positions HummerRM as an ideal model for domain-specific further fine-tuning and reducing vulnerabilities to attacks."],"url":"http://arxiv.org/abs/2405.11647v2","category":"cs.AI"}
{"created":"2024-05-19 18:29:53","title":"Time depending magnetization of nanoparticles under radiofrequency fields: relaxation time in water for solid-liquid transition","abstract":"In application as hyperthermia and nanowarming, power dissipation arises when the time-dependent magnetization $M(t)$ of an out-of-equilibrium system of nanoparticles lags behind the applied field $H(t)$. The key parameter governing this process is the relaxation time $\\tau$ of the system, which induces a phase shift $\\phi_n$ between $H(t)$ and every nth harmonic component of $M(t)$. In this work, we present an expression for $M(t)$ in terms of $\\tau$ and the equilibrium magnetization, valid for any magnetic system exhibiting odd equilibrium response. From this calculation, we obtain a method for determining the effective $\\tau$ of a MNPs sample directly from the experimental measurement of $M(t)$. Additionally, we demonstrate that the power dissipation (SAR: Specific Absorption Rate) of any magnetic sample under a sinusoidal field can be obtained from the first harmonic component of $M(t)$. As an illustrative application, we explore the variation of $\\tau$ for magnetic MNPs in aqueous suspension during the melting process of the matrix. In this case, the change in $\\tau$ can be understood as a result of the reorientation of the MNPs in the direction of the applied field as the matrix becomes liquid.","sentences":["In application as hyperthermia and nanowarming, power dissipation arises when the time-dependent magnetization $M(t)$ of an out-of-equilibrium system of nanoparticles lags behind the applied field $H(t)$. The key parameter governing this process is the relaxation time $\\tau$ of the system, which induces a phase shift $\\phi_n$ between $H(t)$ and every nth harmonic component of $M(t)$. In this work, we present an expression for $M(t)$ in terms of $\\tau$ and the equilibrium magnetization, valid for any magnetic system exhibiting odd equilibrium response.","From this calculation, we obtain a method for determining the effective $\\tau$ of a MNPs sample directly from the experimental measurement of $M(t)$. Additionally, we demonstrate that the power dissipation (SAR: Specific Absorption Rate) of any magnetic sample under a sinusoidal field can be obtained from the first harmonic component of $M(t)$. As an illustrative application, we explore the variation of $\\tau$ for magnetic MNPs in aqueous suspension during the melting process of the matrix.","In this case, the change in $\\tau$ can be understood as a result of the reorientation of the MNPs in the direction of the applied field as the matrix becomes liquid."],"url":"http://arxiv.org/abs/2405.11641v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-19 17:58:26","title":"Zero-Shot Stance Detection using Contextual Data Generation with LLMs","abstract":"Stance detection, the classification of attitudes expressed in a text towards a specific topic, is vital for applications like fake news detection and opinion mining. However, the scarcity of labeled data remains a challenge for this task. To address this problem, we propose Dynamic Model Adaptation with Contextual Data Generation (DyMoAdapt) that combines Few-Shot Learning and Large Language Models. In this approach, we aim to fine-tune an existing model at test time. We achieve this by generating new topic-specific data using GPT-3. This method could enhance performance by allowing the adaptation of the model to new topics. However, the results did not increase as we expected. Furthermore, we introduce the Multi Generated Topic VAST (MGT-VAST) dataset, which extends VAST using GPT-3. In this dataset, each context is associated with multiple topics, allowing the model to understand the relationship between contexts and various potential topics","sentences":["Stance detection, the classification of attitudes expressed in a text towards a specific topic, is vital for applications like fake news detection and opinion mining.","However, the scarcity of labeled data remains a challenge for this task.","To address this problem, we propose Dynamic Model Adaptation with Contextual Data Generation (DyMoAdapt) that combines Few-Shot Learning and Large Language Models.","In this approach, we aim to fine-tune an existing model at test time.","We achieve this by generating new topic-specific data using GPT-3.","This method could enhance performance by allowing the adaptation of the model to new topics.","However, the results did not increase as we expected.","Furthermore, we introduce the Multi Generated Topic VAST (MGT-VAST) dataset, which extends VAST using GPT-3.","In this dataset, each context is associated with multiple topics, allowing the model to understand the relationship between contexts and various potential topics"],"url":"http://arxiv.org/abs/2405.11637v1","category":"cs.CL"}
{"created":"2024-05-19 17:51:53","title":"Spectral theory of infinite dimensional dissipative Hamiltonian systems","abstract":"The spectral theory for operator pencils and operator differential-algebraic equations is studied. Special focus is laid on singular operator pencils and three different concepts of singular operator pencils are introduced. The concepts are analyzed in detail and examples are presented that illustrate the subtle differences. It is investigated how these concepts are related to uniqueness of the underlying algebraic-differential operator equation, showing that, in general, classical results known from the finite dimensional case of matrix pencils and differential-algebraic equations do not prevail. The results are then studied in the setting of structured operator pencils arising in dissipative differential-algebraic equations. Here, unlike in the general infinite-dimensional case, the uniqueness of solutions is closely related to the singularity of the pencil.","sentences":["The spectral theory for operator pencils and operator differential-algebraic equations is studied.","Special focus is laid on singular operator pencils and three different concepts of singular operator pencils are introduced.","The concepts are analyzed in detail and examples are presented that illustrate the subtle differences.","It is investigated how these concepts are related to uniqueness of the underlying algebraic-differential operator equation, showing that, in general, classical results known from the finite dimensional case of matrix pencils and differential-algebraic equations do not prevail.","The results are then studied in the setting of structured operator pencils arising in dissipative differential-algebraic equations.","Here, unlike in the general infinite-dimensional case, the uniqueness of solutions is closely related to the singularity of the pencil."],"url":"http://arxiv.org/abs/2405.11634v1","category":"math.FA"}
{"created":"2024-05-19 15:45:44","title":"Magnetic field dragging in filamentary molecular clouds","abstract":"Maps of polarized dust emission of molecular clouds reveal the morphology of the magnetic field associated with star-forming regions. In particular, polarization maps of hub-filament systems show the distortion of magnetic field lines induced by gas flows onto and inside filaments. We aim to understand the relation between the curvature of magnetic field lines associated with filaments in hub-filament systems and the properties of the underlying gas flows. We consider steady-state models of gas with finite electrical resistivity flowing across a transverse magnetic field. We derive the relation between the bending of the field lines and the flow parameters represented by the Alfv\\'en Mach number and the magnetic Reynolds number. We find that, on the scale of the filaments, the relevant parameter for a gas of finite electrical resistivity is the magnetic Reynolds number, and we derive the relation between the deflection angle of the field from the initial direction (assumed perpendicular to the filament) and the value of the electrical resistivity, due to either Ohmic dissipation or ambipolar diffusion. Application of this model to specific observations of polarized dust emission in filamentary clouds shows that magnetic Reynolds numbers of a few tens are required to reproduce the data. Despite significant uncertainties in the observations (the flow speed, the geometry and orientation of the filament), and the idealization of the model, the specific cases considered show that ambipolar diffusion can provide the resistivity needed to maintain a steady state flow across magnetic fields of significant strength over realistic time scales.","sentences":["Maps of polarized dust emission of molecular clouds reveal the morphology of the magnetic field associated with star-forming regions.","In particular, polarization maps of hub-filament systems show the distortion of magnetic field lines induced by gas flows onto and inside filaments.","We aim to understand the relation between the curvature of magnetic field lines associated with filaments in hub-filament systems and the properties of the underlying gas flows.","We consider steady-state models of gas with finite electrical resistivity flowing across a transverse magnetic field.","We derive the relation between the bending of the field lines and the flow parameters represented by the Alfv\\'en Mach number and the magnetic Reynolds number.","We find that, on the scale of the filaments, the relevant parameter for a gas of finite electrical resistivity is the magnetic Reynolds number, and we derive the relation between the deflection angle of the field from the initial direction (assumed perpendicular to the filament) and the value of the electrical resistivity, due to either Ohmic dissipation or ambipolar diffusion.","Application of this model to specific observations of polarized dust emission in filamentary clouds shows that magnetic Reynolds numbers of a few tens are required to reproduce the data.","Despite significant uncertainties in the observations (the flow speed, the geometry and orientation of the filament), and the idealization of the model, the specific cases considered show that ambipolar diffusion can provide the resistivity needed to maintain a steady state flow across magnetic fields of significant strength over realistic time scales."],"url":"http://arxiv.org/abs/2405.11589v1","category":"astro-ph.GA"}
{"created":"2024-05-19 15:15:18","title":"Securing Health Data on the Blockchain: A Differential Privacy and Federated Learning Framework","abstract":"This study proposes a framework to enhance privacy in Blockchain-based Internet of Things (BIoT) systems used in the healthcare sector. The framework addresses the challenge of leveraging health data for analytics while protecting patient privacy. To achieve this, the study integrates Differential Privacy (DP) with Federated Learning (FL) to protect sensitive health data collected by IoT nodes. The proposed framework utilizes dynamic personalization and adaptive noise distribution strategies to balance privacy and data utility. Additionally, blockchain technology ensures secure and transparent aggregation and storage of model updates. Experimental results on the SVHN dataset demonstrate that the proposed framework achieves strong privacy guarantees against various attack scenarios while maintaining high accuracy in health analytics tasks. For 15 rounds of federated learning with an epsilon value of 8.0, the model obtains an accuracy of 64.50%. The blockchain integration, utilizing Ethereum, Ganache, Web3.py, and IPFS, exhibits an average transaction latency of around 6 seconds and consistent gas consumption across rounds, validating the practicality and feasibility of the proposed approach.","sentences":["This study proposes a framework to enhance privacy in Blockchain-based Internet of Things (BIoT) systems used in the healthcare sector.","The framework addresses the challenge of leveraging health data for analytics while protecting patient privacy.","To achieve this, the study integrates Differential Privacy (DP) with Federated Learning (FL) to protect sensitive health data collected by IoT nodes.","The proposed framework utilizes dynamic personalization and adaptive noise distribution strategies to balance privacy and data utility.","Additionally, blockchain technology ensures secure and transparent aggregation and storage of model updates.","Experimental results on the SVHN dataset demonstrate that the proposed framework achieves strong privacy guarantees against various attack scenarios while maintaining high accuracy in health analytics tasks.","For 15 rounds of federated learning with an epsilon value of 8.0, the model obtains an accuracy of 64.50%.","The blockchain integration, utilizing Ethereum, Ganache, Web3.py, and IPFS, exhibits an average transaction latency of around 6 seconds and consistent gas consumption across rounds, validating the practicality and feasibility of the proposed approach."],"url":"http://arxiv.org/abs/2405.11580v1","category":"cs.CR"}
{"created":"2024-05-19 14:42:19","title":"Quantile Activation: departing from single point estimation for better generalization across distortions","abstract":"A classifier is, in its essence, a function which takes an input and returns the class of the input and implicitly assumes an underlying distribution. We argue in this article that one has to move away from this basic tenet to obtain generalisation across distributions. Specifically, the class of the sample should depend on the points from its context distribution for better generalisation across distributions.   How does one achieve this? The key idea is to adapt the outputs of each neuron of the network to its context distribution. We propose quantile activation, QACT, which, in simple terms, outputs the relative quantile of the sample in its context distribution, instead of the actual values in traditional networks.   The scope of this article is to validate the proposed activation across several experimental settings, and compare it with conventional techniques. For this, we use the datasets developed to test robustness against distortions CIFAR10C, CIFAR100C, MNISTC, TinyImagenetC, and show that we achieve a significantly higher generalisation across distortions than the conventional classifiers, across different architectures. Although this paper is only a proof of concept, we surprisingly find that this approach outperforms DINOv2(small) at large distortions, even though DINOv2 is trained with a far bigger network on a considerably larger dataset.","sentences":["A classifier is, in its essence, a function which takes an input and returns the class of the input and implicitly assumes an underlying distribution.","We argue in this article that one has to move away from this basic tenet to obtain generalisation across distributions.","Specifically, the class of the sample should depend on the points from its context distribution for better generalisation across distributions.   ","How does one achieve this?","The key idea is to adapt the outputs of each neuron of the network to its context distribution.","We propose quantile activation, QACT, which, in simple terms, outputs the relative quantile of the sample in its context distribution, instead of the actual values in traditional networks.   ","The scope of this article is to validate the proposed activation across several experimental settings, and compare it with conventional techniques.","For this, we use the datasets developed to test robustness against distortions CIFAR10C, CIFAR100C, MNISTC, TinyImagenetC, and show that we achieve a significantly higher generalisation across distortions than the conventional classifiers, across different architectures.","Although this paper is only a proof of concept, we surprisingly find that this approach outperforms DINOv2(small) at large distortions, even though DINOv2 is trained with a far bigger network on a considerably larger dataset."],"url":"http://arxiv.org/abs/2405.11573v1","category":"cs.LG"}
{"created":"2024-05-19 14:31:44","title":"A thermodynamic and analytical description on the quantitative phase-field model with enhanced interface diffusivity","abstract":"Based on the idea of maintaining physical diffuse interface kinetics, enhancing interfacial diffusivity has recently provided a new direction for quantitative phase-field simulation at microstructural length and time scale. Establishing a general relationship between interface diffusivity and width is vital to facilitate the practical application. However, it is still limited by time-consuming numerical corrections, and its relationship with non-dilute thermodynamic properties still needs to be revealed. In this study, we present a new thermodynamic and analytical method for determining interfacial diffusivity enhancement. Unlike previous numerical corrections of partition coefficients and interface temperature, this new method aims to keep several thermodynamic quantities unchanged after enlarging the interface width. These essential quantities are theoretically proven to be diffusion potential jump across the diffuse interface and free energy dissipation by trans-interface diffusion. Since no dilute approximation has been employed in model derivation, the present method is available for binary alloys with arbitrary thermodynamic properties and can be easily extended to describe multicomponent systems. Therefore, the present method is expected to advance the recent quantitative phase-field framework and facilitate its practical applications.","sentences":["Based on the idea of maintaining physical diffuse interface kinetics, enhancing interfacial diffusivity has recently provided a new direction for quantitative phase-field simulation at microstructural length and time scale.","Establishing a general relationship between interface diffusivity and width is vital to facilitate the practical application.","However, it is still limited by time-consuming numerical corrections, and its relationship with non-dilute thermodynamic properties still needs to be revealed.","In this study, we present a new thermodynamic and analytical method for determining interfacial diffusivity enhancement.","Unlike previous numerical corrections of partition coefficients and interface temperature, this new method aims to keep several thermodynamic quantities unchanged after enlarging the interface width.","These essential quantities are theoretically proven to be diffusion potential jump across the diffuse interface and free energy dissipation by trans-interface diffusion.","Since no dilute approximation has been employed in model derivation, the present method is available for binary alloys with arbitrary thermodynamic properties and can be easily extended to describe multicomponent systems.","Therefore, the present method is expected to advance the recent quantitative phase-field framework and facilitate its practical applications."],"url":"http://arxiv.org/abs/2405.11568v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-19 14:31:01","title":"Shortcut to Chemically Accurate Quantum Computing via Density-based Basis-set Correction","abstract":"Quantum computing promises a computational advantage over classical methods in electronic-structure calculations, with expected applications in drug design and materials science. Accessing a quantitative description of chemical systems while minimizing quantum resources, such as the number of qubits, is an essential challenge given the limited capabilities of current quantum processors. We provide a shortcut towards quantum computations at chemical accuracy by approaching the complete-basis-set limit (CBS) through integrating density-functional theory into quantum algorithms via density-based basis-set corrections coupled to basis-sets crafted on-the-fly and specifically adapted to a given system/user-defined qubit budget. The approach self-consistently accelerates the basis-set convergence, improving electronic densities, ground-state energies, and first-order properties such as dipole moments. It can also serve as a classical, \\textit{a posteriori}, energy correction to quantum hardware calculations. The strategy is assessed using GPU-accelerated state-vector emulation up to 32 qubits. We converge the ground-state energies of four systems (He, Be, H$_2$, LiH) within chemical accuracy of the CBS full-configuration-interaction reference, while offering a systematic increase of accuracy beyond a double-zeta quality for various molecules up to the H$_8$ hydrogen chain. We also obtain dissociation curves for H$_2$ and LiH that reach the CBS limit whereas for the challenging simulation of the N$_2$ triple-bond breaking, we achieve a near-triple-zeta quality at the cost of a minimal basis-set. This hybrid strategy allows us to obtain quantitative results that would otherwise require brute-force quantum simulations using far more than 100 logical qubits, thereby opening up opportunities to explore real-world chemistry with reasonable computational resources.","sentences":["Quantum computing promises a computational advantage over classical methods in electronic-structure calculations, with expected applications in drug design and materials science.","Accessing a quantitative description of chemical systems while minimizing quantum resources, such as the number of qubits, is an essential challenge given the limited capabilities of current quantum processors.","We provide a shortcut towards quantum computations at chemical accuracy by approaching the complete-basis-set limit (CBS) through integrating density-functional theory into quantum algorithms via density-based basis-set corrections coupled to basis-sets crafted on-the-fly and specifically adapted to a given system/user-defined qubit budget.","The approach self-consistently accelerates the basis-set convergence, improving electronic densities, ground-state energies, and first-order properties such as dipole moments.","It can also serve as a classical, \\textit{a posteriori}, energy correction to quantum hardware calculations.","The strategy is assessed using GPU-accelerated state-vector emulation up to 32 qubits.","We converge the ground-state energies of four systems (He, Be, H$_2$, LiH) within chemical accuracy of the CBS full-configuration-interaction reference, while offering a systematic increase of accuracy beyond a double-zeta quality for various molecules up to the H$_8$ hydrogen chain.","We also obtain dissociation curves for H$_2$ and LiH that reach the CBS limit whereas for the challenging simulation of the N$_2$ triple-bond breaking, we achieve a near-triple-zeta quality at the cost of a minimal basis-set.","This hybrid strategy allows us to obtain quantitative results that would otherwise require brute-force quantum simulations using far more than 100 logical qubits, thereby opening up opportunities to explore real-world chemistry with reasonable computational resources."],"url":"http://arxiv.org/abs/2405.11567v1","category":"physics.chem-ph"}
{"created":"2024-05-19 14:29:02","title":"User-Centric Association and Feedback Bit Allocation for FDD Cell-Free Massive MIMO","abstract":"In this paper, we introduce a novel approach to user-centric association and feedback bit allocation for the downlink of a cell-free massive MIMO (CF-mMIMO) system, operating under limited feedback constraints. In CF-mMIMO systems employing frequency division duplexing, each access point (AP) relies on channel information provided by its associated user equipments (UEs) for beamforming design. Since the uplink control channel is typically shared among UEs, we take account of each AP's total feedback budget, which is distributed among its associated UEs. By employing the Saleh-Valenzuela multi-resolvable path channel model with different average path gains, we first identify necessary feedback information for each UE, along with an appropriate codebook structure. This structure facilitates adaptive quantization of multiple paths based on their dominance. We then formulate a joint optimization problem addressing user-centric UE-AP association and feedback bit allocation. To address this challenge, we analyze the impact of feedback bit allocation and derive our proposed scheme from the solution of an alternative optimization problem aimed at devising long-term policies, explicitly considering the effects of feedback bit allocation. Numerical results show that our proposed scheme effectively enhances the performance of conventional approaches in CF-mMIMO systems.","sentences":["In this paper, we introduce a novel approach to user-centric association and feedback bit allocation for the downlink of a cell-free massive MIMO (CF-mMIMO) system, operating under limited feedback constraints.","In CF-mMIMO systems employing frequency division duplexing, each access point (AP) relies on channel information provided by its associated user equipments (UEs) for beamforming design.","Since the uplink control channel is typically shared among UEs, we take account of each AP's total feedback budget, which is distributed among its associated UEs.","By employing the Saleh-Valenzuela multi-resolvable path channel model with different average path gains, we first identify necessary feedback information for each UE, along with an appropriate codebook structure.","This structure facilitates adaptive quantization of multiple paths based on their dominance.","We then formulate a joint optimization problem addressing user-centric UE-AP association and feedback bit allocation.","To address this challenge, we analyze the impact of feedback bit allocation and derive our proposed scheme from the solution of an alternative optimization problem aimed at devising long-term policies, explicitly considering the effects of feedback bit allocation.","Numerical results show that our proposed scheme effectively enhances the performance of conventional approaches in CF-mMIMO systems."],"url":"http://arxiv.org/abs/2405.11563v1","category":"cs.IT"}
{"created":"2024-05-19 13:26:33","title":"Adaptive Online Experimental Design for Causal Discovery","abstract":"Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.","sentences":["Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination.","The majority of existing causal discovery methods are developed assuming infinite interventional data.","We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems.","A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case.","We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history.","Given any desired confidence value, the algorithm determines a termination condition and runs until it is met.","We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples.","Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs.","It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples."],"url":"http://arxiv.org/abs/2405.11548v1","category":"cs.LG"}
{"created":"2024-05-19 13:11:18","title":"Verification technology for finger vein biometric","abstract":"Finger vein biometrics is an approach to identifying individuals based on the unique patterns of blood vessels in their fingers, and the technology is advanced in image capture and processing techniques, which is leading to more efficient, accurate, and reliable systems. This article focuses on a verification system that compares the matrices of an efficient finger vein verification system on different databases to test its strength and efficiency. Contrast Limited Adaptive Histogram Equalization (CLAHE) has been examined as an image enhancement and processing method to improve contrast and render details in an image easier to detect. A random forest classifier is deployed with a comparison between two pretrained systems, VGG16 and ResNet50, which are types of convolutional neural networks. VGG-16 and ResNet-50 models are implemented on three different datasets, and fine-tuning these models enabled the harnessing of their powerful capabilities and achieving superior performance on the specific image classification task.","sentences":["Finger vein biometrics is an approach to identifying individuals based on the unique patterns of blood vessels in their fingers, and the technology is advanced in image capture and processing techniques, which is leading to more efficient, accurate, and reliable systems.","This article focuses on a verification system that compares the matrices of an efficient finger vein verification system on different databases to test its strength and efficiency.","Contrast Limited Adaptive Histogram Equalization (CLAHE) has been examined as an image enhancement and processing method to improve contrast and render details in an image easier to detect.","A random forest classifier is deployed with a comparison between two pretrained systems, VGG16 and ResNet50, which are types of convolutional neural networks.","VGG-16 and ResNet-50 models are implemented on three different datasets, and fine-tuning these models enabled the harnessing of their powerful capabilities and achieving superior performance on the specific image classification task."],"url":"http://arxiv.org/abs/2405.11540v1","category":"eess.IV"}
{"created":"2024-05-19 11:36:45","title":"Overcoming Data and Model Heterogeneities in Decentralized Federated Learning via Synthetic Anchors","abstract":"Conventional Federated Learning (FL) involves collaborative training of a global model while maintaining user data privacy. One of its branches, decentralized FL, is a serverless network that allows clients to own and optimize different local models separately, which results in saving management and communication resources. Despite the promising advancements in decentralized FL, it may reduce model generalizability due to lacking a global model. In this scenario, managing data and model heterogeneity among clients becomes a crucial problem, which poses a unique challenge that must be overcome: How can every client's local model learn generalizable representation in a decentralized manner? To address this challenge, we propose a novel Decentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA. Based on the theory of domain adaptation and Knowledge Distillation (KD), we theoretically and empirically show that synthesizing global anchors based on raw data distribution facilitates mutual knowledge transfer. We further design two effective regularization terms for local training: 1) REG loss that regularizes the distribution of the client's latent embedding with the anchors and 2) KD loss that enables clients to learn from others. Through extensive experiments on diverse client data distributions, we showcase the effectiveness of DeSA in enhancing both inter- and intra-domain accuracy of each client.","sentences":["Conventional Federated Learning (FL) involves collaborative training of a global model while maintaining user data privacy.","One of its branches, decentralized FL, is a serverless network that allows clients to own and optimize different local models separately, which results in saving management and communication resources.","Despite the promising advancements in decentralized FL, it may reduce model generalizability due to lacking a global model.","In this scenario, managing data and model heterogeneity among clients becomes a crucial problem, which poses a unique challenge that must be overcome: How can every client's local model learn generalizable representation in a decentralized manner?","To address this challenge, we propose a novel Decentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA.","Based on the theory of domain adaptation and Knowledge Distillation (KD), we theoretically and empirically show that synthesizing global anchors based on raw data distribution facilitates mutual knowledge transfer.","We further design two effective regularization terms for local training: 1) REG loss that regularizes the distribution of the client's latent embedding with the anchors and 2) KD loss that enables clients to learn from others.","Through extensive experiments on diverse client data distributions, we showcase the effectiveness of DeSA in enhancing both inter- and intra-domain accuracy of each client."],"url":"http://arxiv.org/abs/2405.11525v1","category":"cs.LG"}
{"created":"2024-05-19 11:29:52","title":"Diffusion-Based Hierarchical Image Steganography","abstract":"This paper introduces Hierarchical Image Steganography, a novel method that enhances the security and capacity of embedding multiple images into a single container using diffusion models. HIS assigns varying levels of robustness to images based on their importance, ensuring enhanced protection against manipulation. It adaptively exploits the robustness of the Diffusion Model alongside the reversibility of the Flow Model. The integration of Embed-Flow and Enhance-Flow improves embedding efficiency and image recovery quality, respectively, setting HIS apart from conventional multi-image steganography techniques. This innovative structure can autonomously generate a container image, thereby securely and efficiently concealing multiple images and text. Rigorous subjective and objective evaluations underscore our advantage in analytical resistance, robustness, and capacity, illustrating its expansive applicability in content safeguarding and privacy fortification.","sentences":["This paper introduces Hierarchical Image Steganography, a novel method that enhances the security and capacity of embedding multiple images into a single container using diffusion models.","HIS assigns varying levels of robustness to images based on their importance, ensuring enhanced protection against manipulation.","It adaptively exploits the robustness of the Diffusion Model alongside the reversibility of the Flow Model.","The integration of Embed-Flow and Enhance-Flow improves embedding efficiency and image recovery quality, respectively, setting HIS apart from conventional multi-image steganography techniques.","This innovative structure can autonomously generate a container image, thereby securely and efficiently concealing multiple images and text.","Rigorous subjective and objective evaluations underscore our advantage in analytical resistance, robustness, and capacity, illustrating its expansive applicability in content safeguarding and privacy fortification."],"url":"http://arxiv.org/abs/2405.11523v1","category":"cs.CV"}
{"created":"2024-05-19 11:24:21","title":"A comparative study of augmented inverse propensity weighted estimators using outcome-adaptive lasso and other penalized regression methods","abstract":"Confounder selection may be efficiently conducted using penalized regression methods when causal effects are estimated from observational data with many variables. An outcome-adaptive lasso was proposed to build a model for the propensity score that can be employed in conjunction with other variable selection methods for the outcome model to apply the augmented inverse propensity weighted (AIPW) estimator. However, researchers may not know which method is optimal to use for outcome model when applying the AIPW estimator with the outcome-adaptive lasso. This study provided hints on readily implementable penalized regression methods that should be adopted for the outcome model as a counterpart of the outcome-adaptive lasso. We evaluated the bias and variance of the AIPW estimators using the propensity score (PS) model and an outcome model based on penalized regression methods under various conditions by analyzing a clinical trial example and numerical experiments; the estimates and standard errors of the AIPW estimators were almost identical in an example with over 5000 participants. The AIPW estimators using penalized regression methods with the oracle property performed well in terms of bias and variance in numerical experiments with smaller sample sizes. Meanwhile, the bias of the AIPW estimator using the ordinary lasso for the PS and outcome models was considerably larger.","sentences":["Confounder selection may be efficiently conducted using penalized regression methods when causal effects are estimated from observational data with many variables.","An outcome-adaptive lasso was proposed to build a model for the propensity score that can be employed in conjunction with other variable selection methods for the outcome model to apply the augmented inverse propensity weighted (AIPW) estimator.","However, researchers may not know which method is optimal to use for outcome model when applying the AIPW estimator with the outcome-adaptive lasso.","This study provided hints on readily implementable penalized regression methods that should be adopted for the outcome model as a counterpart of the outcome-adaptive lasso.","We evaluated the bias and variance of the AIPW estimators using the propensity score (PS) model and an outcome model based on penalized regression methods under various conditions by analyzing a clinical trial example and numerical experiments; the estimates and standard errors of the AIPW estimators were almost identical in an example with over 5000 participants.","The AIPW estimators using penalized regression methods with the oracle property performed well in terms of bias and variance in numerical experiments with smaller sample sizes.","Meanwhile, the bias of the AIPW estimator using the ordinary lasso for the PS and outcome models was considerably larger."],"url":"http://arxiv.org/abs/2405.11522v1","category":"stat.ME"}
{"created":"2024-05-19 10:50:06","title":"Optimizing Underwater IoT Routing with Multi-Criteria Decision Making and Uncertainty Weights","abstract":"Effective data routing is vital in the Internet of Things (IoT) paradigm, especially in underwater mobile sensor networks where inefficiency can lead to significant resource consumption. This article presents an innovative method designed to enhance network performance and reduce resource usage, while also accurately determining component weights in these networks, ensuring quality service. Building upon previous research on multi-criteria decision-making systems in coastal RPL networks, our method involves key adaptations for underwater environments. It integrates comprehensive network features to identify the optimal parent node for each sensor, employing the fuzzy SWARA decision-making approach under uncertain conditions. This method takes into account various factors including hops, energy, ARSSI rate, delay, ETX, link delivery rate, and depth to determine the most effective parent node assignment. Through simulation, our approach demonstrates marked improvements in network performance compared to existing solutions. These advancements are significant, offering a new direction in enhancing underwater IoT communications and suggesting wider applications for IoT systems facing similar challenges.","sentences":["Effective data routing is vital in the Internet of Things (IoT) paradigm, especially in underwater mobile sensor networks where inefficiency can lead to significant resource consumption.","This article presents an innovative method designed to enhance network performance and reduce resource usage, while also accurately determining component weights in these networks, ensuring quality service.","Building upon previous research on multi-criteria decision-making systems in coastal RPL networks, our method involves key adaptations for underwater environments.","It integrates comprehensive network features to identify the optimal parent node for each sensor, employing the fuzzy SWARA decision-making approach under uncertain conditions.","This method takes into account various factors including hops, energy, ARSSI rate, delay, ETX, link delivery rate, and depth to determine the most effective parent node assignment.","Through simulation, our approach demonstrates marked improvements in network performance compared to existing solutions.","These advancements are significant, offering a new direction in enhancing underwater IoT communications and suggesting wider applications for IoT systems facing similar challenges."],"url":"http://arxiv.org/abs/2405.11513v1","category":"cs.NI"}
{"created":"2024-05-21 17:54:53","title":"Accelerating Resonance Searches via Signature-Oriented Pre-training","abstract":"The search for heavy resonances beyond the Standard Model (BSM) is a key objective at the LHC. While the recent use of advanced deep neural networks for boosted-jet tagging significantly enhances the sensitivity of dedicated searches, it is limited to specific final states, leaving vast potential BSM phase space underexplored. We introduce a novel experimental method, Signature-Oriented Pre-training for Heavy-resonance ObservatioN (Sophon), which leverages deep learning to cover an extensive number of boosted final states. Pre-trained on the comprehensive JetClass-II dataset, the Sophon model learns intricate jet signatures, ensuring the optimal constructions of various jet tagging discriminates and enabling high-performance transfer learning capabilities. We show that the method can not only push widespread model-specific searches to their sensitivity frontier, but also greatly improve model-agnostic approaches, accelerating LHC resonance searches in a broad sense.","sentences":["The search for heavy resonances beyond the Standard Model (BSM) is a key objective at the LHC.","While the recent use of advanced deep neural networks for boosted-jet tagging significantly enhances the sensitivity of dedicated searches, it is limited to specific final states, leaving vast potential BSM phase space underexplored.","We introduce a novel experimental method, Signature-Oriented Pre-training for Heavy-resonance ObservatioN (Sophon), which leverages deep learning to cover an extensive number of boosted final states.","Pre-trained on the comprehensive JetClass-II dataset, the Sophon model learns intricate jet signatures, ensuring the optimal constructions of various jet tagging discriminates and enabling high-performance transfer learning capabilities.","We show that the method can not only push widespread model-specific searches to their sensitivity frontier, but also greatly improve model-agnostic approaches, accelerating LHC resonance searches in a broad sense."],"url":"http://arxiv.org/abs/2405.12972v1","category":"hep-ph"}
{"created":"2024-05-21 17:28:54","title":"A Fueter operator for 3/2-spinors","abstract":"We show the non-compactness of moduli space of solutions of the monopole equations for $3/2$-spinors on a closed 3-manifold is equivalent to the existence of `3/2-Fueter sections' that are solutions of an overdetermined non-linear elliptic differential equation. These are sections of a fiber bundle whose fiber is a special 4-dimensional submanifold of the hyperk\\\"ahler manifold of center-framed charged one $SU(2)$-instantons on $\\mathbf{R}^4$. This fiber bundle does not inherit a hyperk\\\"ahler structure.","sentences":["We show the non-compactness of moduli space of solutions of the monopole equations for $3/2$-spinors on a closed 3-manifold is equivalent to the existence of `3/2-Fueter sections' that are solutions of an overdetermined non-linear elliptic differential equation.","These are sections of a fiber bundle whose fiber is a special 4-dimensional submanifold of the hyperk\\\"ahler manifold of center-framed charged one $SU(2)$-instantons on $\\mathbf{R}^4$. This fiber bundle does not inherit a hyperk\\\"ahler structure."],"url":"http://arxiv.org/abs/2405.12956v1","category":"math.DG"}
{"created":"2024-05-21 17:17:36","title":"The hanging chain problem with respect to a circle","abstract":"Let $\\s^1$ be a circle in Euclidean plane. We consider the problem of finding the shape of a planar curve which is an extremal of the potential energy that measures the distance to $\\s^1$. We describe the shape of these curves distinguishing if the curves lie in the inside or outside of $\\s^1$. We extend the problem for energies that are powers to the distance to $\\s^1$.","sentences":["Let $\\s^1$ be a circle in Euclidean plane.","We consider the problem of finding the shape of a planar curve which is an extremal of the potential energy that measures the distance to $\\s^1$. We describe the shape of these curves distinguishing if the curves lie in the inside or outside of $\\s^1$. We extend the problem for energies that are powers to the distance to $\\s^1$."],"url":"http://arxiv.org/abs/2405.12947v1","category":"math.DG"}
{"created":"2024-05-21 17:08:17","title":"Improved predictions of phenomenological nuclear charge radius formulae with Bayesian optimization approach","abstract":"The model inputs play a key role in the performance of the Bayesian optimization approach. In this paper, we investigate the influence of the inputs on the improved predictions of phenomenological nuclear charge radius formulas using an approach combining those original formulas and the Bayesian neural network (BNN). We find that there is no improvement in predictions after the abnormal odd-even staggering effect of 181,183,185Hg is injected into the BNN, while the original phenomenological formulas themselves possess rich physical information or rigid constraints. It indicates the abundance and intensity of physical inputs affect the performance of the Bayesian optimization approach as well as the robustness of the BNN. We further demonstrate that, by ensuring that the number of neurons in the hidden layer is larger than the number of NN inputs, adding hidden layers into the BNN can significantly improve the predictions of nuclear charge radii formulas within the Bayesian optimization approach.","sentences":["The model inputs play a key role in the performance of the Bayesian optimization approach.","In this paper, we investigate the influence of the inputs on the improved predictions of phenomenological nuclear charge radius formulas using an approach combining those original formulas and the Bayesian neural network (BNN).","We find that there is no improvement in predictions after the abnormal odd-even staggering effect of 181,183,185Hg is injected into the BNN, while the original phenomenological formulas themselves possess rich physical information or rigid constraints.","It indicates the abundance and intensity of physical inputs affect the performance of the Bayesian optimization approach as well as the robustness of the BNN.","We further demonstrate that, by ensuring that the number of neurons in the hidden layer is larger than the number of NN inputs, adding hidden layers into the BNN can significantly improve the predictions of nuclear charge radii formulas within the Bayesian optimization approach."],"url":"http://arxiv.org/abs/2405.12936v1","category":"nucl-th"}
{"created":"2024-05-21 16:14:16","title":"On a time-frequency blurring operator with applications in data augmentation","abstract":"Inspired by the success of recent data augmentation methods for signals which act on time-frequency representations, we introduce an operator which convolves the short-time Fourier transform of a signal with a specified kernel. Analytical properties including boundedness, compactness and positivity are investigated from the perspective of time-frequency analysis. A convolutional neural network and a vision transformer are trained to classify audio signals using spectrograms with different augmentation setups, including the above mentioned time-frequency blurring operator, with results indicating that the operator can significantly improve test performance, especially in the data-starved regime.","sentences":["Inspired by the success of recent data augmentation methods for signals which act on time-frequency representations, we introduce an operator which convolves the short-time Fourier transform of a signal with a specified kernel.","Analytical properties including boundedness, compactness and positivity are investigated from the perspective of time-frequency analysis.","A convolutional neural network and a vision transformer are trained to classify audio signals using spectrograms with different augmentation setups, including the above mentioned time-frequency blurring operator, with results indicating that the operator can significantly improve test performance, especially in the data-starved regime."],"url":"http://arxiv.org/abs/2405.12899v1","category":"math.FA"}
{"created":"2024-05-21 15:48:19","title":"Predictive design of two-dimensional electrides with tunable magnetic, topological, and superconducting properties","abstract":"Two-dimensional materials are of interest for their exotic properties, for example, superconductivity, and highly tunability. Focusing on phonon-mediating superconductivity, one would propose to promote critical temperature by substituting heavy elements by lighter ones, in order to increase Debye temperature. Following recent experimental progress in transition-metal nitrides and theoretically revealing W$_2$N$_3$ as a candidate for high-temperature superconductivity, we investigate the possibility of two-dimensional superconductivity through surface engineering on MoN$_2$. Using density functional theory calculation, we found multigap superconductivity at temperature up to 36K within anisotropic Eliashberg equation in passivated electride-like surfaces. We also demonstrate their possibility to sustain topological superconductivity with strong spin-orbital coupling.","sentences":["Two-dimensional materials are of interest for their exotic properties, for example, superconductivity, and highly tunability.","Focusing on phonon-mediating superconductivity, one would propose to promote critical temperature by substituting heavy elements by lighter ones, in order to increase Debye temperature.","Following recent experimental progress in transition-metal nitrides and theoretically revealing W$_2$N$_3$ as a candidate for high-temperature superconductivity, we investigate the possibility of two-dimensional superconductivity through surface engineering on MoN$_2$. Using density functional theory calculation, we found multigap superconductivity at temperature up to 36K within anisotropic Eliashberg equation in passivated electride-like surfaces.","We also demonstrate their possibility to sustain topological superconductivity with strong spin-orbital coupling."],"url":"http://arxiv.org/abs/2405.12879v1","category":"cond-mat.supr-con"}
{"created":"2024-05-21 15:13:02","title":"Explicit gate construction of block-encoding for Hamiltonians needed for simulating partial differential equations","abstract":"Quantum computation is an emerging technology with important potential for solving certain problems pivotal in various scientific and engineering disciplines. This paper introduces an efficient quantum protocol for the explicit construction of the block-encoding for an important class of Hamiltonians. Using the Schrodingerisation technique -- which converts non-conservative PDEs into conservative ones -- this particular class of Hamiltonians is shown to be sufficient for simulating any linear partial differential equations that have coefficients which are polynomial functions. The class of Hamiltonians consist of discretisations of polynomial products and sums of position and momentum operators. This construction is explicit and leverages minimal one- and two-qubit operations. The explicit construction of this block-encoding forms a fundamental building block for constructing the unitary evolution operator for this Hamiltonian. The proposed algorithm exhibits polynomial scaling with respect to the spatial partitioning size, suggesting an exponential speedup over classical finite-difference methods. This work provides an important foundation for building explicit and efficient quantum circuits for solving partial differential equations.","sentences":["Quantum computation is an emerging technology with important potential for solving certain problems pivotal in various scientific and engineering disciplines.","This paper introduces an efficient quantum protocol for the explicit construction of the block-encoding for an important class of Hamiltonians.","Using the Schrodingerisation technique -- which converts non-conservative PDEs into conservative ones -- this particular class of Hamiltonians is shown to be sufficient for simulating any linear partial differential equations that have coefficients which are polynomial functions.","The class of Hamiltonians consist of discretisations of polynomial products and sums of position and momentum operators.","This construction is explicit and leverages minimal one-","and two-qubit operations.","The explicit construction of this block-encoding forms a fundamental building block for constructing the unitary evolution operator for this Hamiltonian.","The proposed algorithm exhibits polynomial scaling with respect to the spatial partitioning size, suggesting an exponential speedup over classical finite-difference methods.","This work provides an important foundation for building explicit and efficient quantum circuits for solving partial differential equations."],"url":"http://arxiv.org/abs/2405.12855v1","category":"quant-ph"}
{"created":"2024-05-21 14:35:54","title":"Constant sectional curvature surfaces with a semi-symmetric non-metric connection","abstract":"Consider the Euclidean space $\\mathbb{R}^3$ endowed with a canonical semi-symmetric non-metric connection determined by a vector field $\\mathsf{C}\\in\\mathfrak{X}(\\mathbb{R}^3)$. We study surfaces when the sectional curvature with respect to this connection is constant. In case that the surface is cylindrical, we obtain full classification when the rulings are orthogonal or parallel to $\\mathsf{C}$. If the surface is rotational, we prove that the rotation axis is parallel to $\\mathsf{C}$ and we classify all conical rotational surfaces with constant sectional curvature. Finally, for the particular case $\\frac12$ of the sectional curvature, the existence of rotational surfaces orthogonally intersecting the rotation axis is also obtained.","sentences":["Consider the Euclidean space $\\mathbb{R}^3$ endowed with a canonical semi-symmetric non-metric connection determined by a vector field $\\mathsf{C}\\in\\mathfrak{X}(\\mathbb{R}^3)$. We study surfaces when the sectional curvature with respect to this connection is constant.","In case that the surface is cylindrical, we obtain full classification when the rulings are orthogonal or parallel to $\\mathsf{C}$. If the surface is rotational, we prove that the rotation axis is parallel to $\\mathsf{C}$ and we classify all conical rotational surfaces with constant sectional curvature.","Finally, for the particular case $\\frac12$ of the sectional curvature, the existence of rotational surfaces orthogonally intersecting the rotation axis is also obtained."],"url":"http://arxiv.org/abs/2405.12831v1","category":"math.DG"}
{"created":"2024-05-21 14:26:53","title":"Chordal-NMF with Riemannian Multiplicative Update","abstract":"Nonnegative Matrix Factorization (NMF) is the problem of approximating a given nonnegative matrix M through the conic combination of two nonnegative low-rank matrices W and H. Traditionally NMF is tackled by optimizing a specific objective function evaluating the quality of the approximation. This assessment is often done based on the Frobenius norm. In this study, we argue that the Frobenius norm as the \"point-to-point\" distance may not always be appropriate. Due to the nonnegative combination resulting in a polyhedral cone, this conic perspective of NMF may not naturally align with conventional point-to-point distance measures. Hence, a ray-to-ray chordal distance is proposed as an alternative way of measuring the discrepancy between M and WH. This measure is related to the Euclidean distance on the unit sphere, motivating us to employ nonsmooth manifold optimization approaches.   We apply Riemannian optimization technique to solve chordal-NMF by casting it on a manifold. Unlike existing works on Riemannian optimization that require the manifold to be smooth, the nonnegativity in chordal-NMF is a non-differentiable manifold. We propose a Riemannian Multiplicative Update (RMU) that preserves the convergence properties of Riemannian gradient descent without breaking the smoothness condition on the manifold.   We showcase the effectiveness of the Chordal-NMF on synthetic datasets as well as real-world multispectral images.","sentences":["Nonnegative Matrix Factorization (NMF) is the problem of approximating a given nonnegative matrix M through the conic combination of two nonnegative low-rank matrices W and H. Traditionally NMF is tackled by optimizing a specific objective function evaluating the quality of the approximation.","This assessment is often done based on the Frobenius norm.","In this study, we argue that the Frobenius norm as the \"point-to-point\" distance may not always be appropriate.","Due to the nonnegative combination resulting in a polyhedral cone, this conic perspective of NMF may not naturally align with conventional point-to-point distance measures.","Hence, a ray-to-ray chordal distance is proposed as an alternative way of measuring the discrepancy between M and WH.","This measure is related to the Euclidean distance on the unit sphere, motivating us to employ nonsmooth manifold optimization approaches.   ","We apply Riemannian optimization technique to solve chordal-NMF by casting it on a manifold.","Unlike existing works on Riemannian optimization that require the manifold to be smooth, the nonnegativity in chordal-NMF is a non-differentiable manifold.","We propose a Riemannian Multiplicative Update (RMU) that preserves the convergence properties of Riemannian gradient descent without breaking the smoothness condition on the manifold.   ","We showcase the effectiveness of the Chordal-NMF on synthetic datasets as well as real-world multispectral images."],"url":"http://arxiv.org/abs/2405.12823v1","category":"math.OC"}
{"created":"2024-05-21 14:26:51","title":"Open-Path Detection of Organic Vapors via Quantum Infrared Spectroscopy","abstract":"In recent years, quantum Fourier transform infrared (QFTIR) spectroscopy was proposed as an alternative to conventional spectroscopy in the mid-infrared region of the spectrum. In this way, harnessing induced coherence and spectral entanglement offers perspectives for practical organic gasses detection. Still, little research was conducted in order to bring QFTIR spectrometers closer to domestic or in-field usage. In this work, we first use such a spectrometer for open-path detection of multiple interfering organic gasses in ambient air. The accurate identification of mixtures of acetone, methanol and ethanol vapors is demonstrated with a QFTIR spectrometer. We achieved this breakthrough by using a nonlinear Michelson interferometer with 1.7m-long arms in order to increase the absorption length, coupled with analysis techniques from differential optical absorption spectroscopy. The evolution of different gasses' concentrations in ambient air was measured through time. These results constitute the first use-case of a QFTIR spectrometer as a detector of organic gasses, and thus represent an important milestone towards the development of such detectors in practical situations.","sentences":["In recent years, quantum Fourier transform infrared (QFTIR) spectroscopy was proposed as an alternative to conventional spectroscopy in the mid-infrared region of the spectrum.","In this way, harnessing induced coherence and spectral entanglement offers perspectives for practical organic gasses detection.","Still, little research was conducted in order to bring QFTIR spectrometers closer to domestic or in-field usage.","In this work, we first use such a spectrometer for open-path detection of multiple interfering organic gasses in ambient air.","The accurate identification of mixtures of acetone, methanol and ethanol vapors is demonstrated with a QFTIR spectrometer.","We achieved this breakthrough by using a nonlinear Michelson interferometer with 1.7m-long arms in order to increase the absorption length, coupled with analysis techniques from differential optical absorption spectroscopy.","The evolution of different gasses' concentrations in ambient air was measured through time.","These results constitute the first use-case of a QFTIR spectrometer as a detector of organic gasses, and thus represent an important milestone towards the development of such detectors in practical situations."],"url":"http://arxiv.org/abs/2405.12822v1","category":"quant-ph"}
{"created":"2024-05-21 13:00:33","title":"Sachs equations and plane waves II: Isometries and conformal isometries","abstract":"This article describes the symmetries of plane wave spacetimes in dimension four and greater. It begins with a description of the isometric automorphisms, and in particular the homogeneous plane waves. Then the article turns to describing isometries from one plane wave to another. The structure of the isometries is relevant for the problem of classifying vacuum spacetimes by observables, and the article presents an explicit example of a family of vacuum spacetimes encoding the Bernoulli shift, and so not classifiable by observables. Next, the article turns to a description of the conformal isometries. Here it is assumed that the conformal curvature does not vanish identically, the case of Minkowski space being both trivial and very degenerate. The article then classifies all conformal automorphisms and isometries of plane waves of the Rosen and Brinkmann forms.","sentences":["This article describes the symmetries of plane wave spacetimes in dimension four and greater.","It begins with a description of the isometric automorphisms, and in particular the homogeneous plane waves.","Then the article turns to describing isometries from one plane wave to another.","The structure of the isometries is relevant for the problem of classifying vacuum spacetimes by observables, and the article presents an explicit example of a family of vacuum spacetimes encoding the Bernoulli shift, and so not classifiable by observables.","Next, the article turns to a description of the conformal isometries.","Here it is assumed that the conformal curvature does not vanish identically, the case of Minkowski space being both trivial and very degenerate.","The article then classifies all conformal automorphisms and isometries of plane waves of the Rosen and Brinkmann forms."],"url":"http://arxiv.org/abs/2405.12748v1","category":"math-ph"}
{"created":"2024-05-21 12:23:27","title":"The DKP Equation in Presence of a Cusp Potential: Transmission Resonances and Bound States","abstract":"In this article, we solve the Duffin--Kemmer--Petiau (DKP) equation in the presence of the cusp potential for spin--one particles. We derived the scattering solutions and calculated the bound states in terms of the Whittaker functions. We show that transmission resonances are present, as well as the particle--anti-particle bound states.","sentences":["In this article, we solve the Duffin--Kemmer--Petiau (DKP) equation in the presence of the cusp potential for spin--one particles.","We derived the scattering solutions and calculated the bound states in terms of the Whittaker functions.","We show that transmission resonances are present, as well as the particle--anti-particle bound states."],"url":"http://arxiv.org/abs/2405.12722v1","category":"quant-ph"}
{"created":"2024-05-21 12:20:15","title":"Unique continuation from conical boundary points for fractional equations","abstract":"We provide fine asymptotics of solutions of fractional elliptic equations at boundary points where the domain is locally conical; that is, corner type singularities appear. Our method relies on a suitable smoothing of the corner singularity and an approximation scheme, which allow us to provide a Pohozaev type inequality. Then, the asymptotics of solutions at the conical point follow by an Almgren type monotonicity formula, blow-up analysis and Fourier decomposition on eigenspaces of a spherical eigenvalue problem. A strong unique continuation principle follows as a corollary.","sentences":["We provide fine asymptotics of solutions of fractional elliptic equations at boundary points where the domain is locally conical; that is, corner type singularities appear.","Our method relies on a suitable smoothing of the corner singularity and an approximation scheme, which allow us to provide a Pohozaev type inequality.","Then, the asymptotics of solutions at the conical point follow by an Almgren type monotonicity formula, blow-up analysis and Fourier decomposition on eigenspaces of a spherical eigenvalue problem.","A strong unique continuation principle follows as a corollary."],"url":"http://arxiv.org/abs/2405.12718v1","category":"math.AP"}
{"created":"2024-05-21 10:07:10","title":"New regime of the Coulomb blockade in quantum dots","abstract":"We consider how the absence of thermalisation affects the classical Coulomb blockade regime in quantum dots. By solving the quantum kinetic equation in the experimentally accessible regime when the dot has two relevant occupation states, we calculate the current-voltage characteristics for arbitrary coupling to the leads. If the couplings are strongly asymmetric, we have found that the Coulomb staircase practically reduces to the first step independent of the charging energy when the latter is larger than the Fermi energy, while the standard thermalised results are recovered in the opposite case. If the couplings are of the same order, the absence of thermalisation has a new, striking signature - a robust additional peak in the differential conductance.","sentences":["We consider how the absence of thermalisation affects the classical Coulomb blockade regime in quantum dots.","By solving the quantum kinetic equation in the experimentally accessible regime when the dot has two relevant occupation states, we calculate the current-voltage characteristics for arbitrary coupling to the leads.","If the couplings are strongly asymmetric, we have found that the Coulomb staircase practically reduces to the first step independent of the charging energy when the latter is larger than the Fermi energy, while the standard thermalised results are recovered in the opposite case.","If the couplings are of the same order, the absence of thermalisation has a new, striking signature - a robust additional peak in the differential conductance."],"url":"http://arxiv.org/abs/2405.12653v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 09:58:44","title":"Observation of the Tetragonal-Orthorhombic transition in polycrystalline YBa$_2$Cu$_3$O$_{6+\u03b4}$ samples during the oxidation process at constant temperature via thermogravimetry and differential thermometry analysis","abstract":"This work examines the oxygenation process of polycrystalline powder of YBa$_2$Cu$_3$O$_{6+\\delta}$ superconductor material through constant temperature thermogravimetric analysis. Starting from completely deoxygenated samples ($\\delta$ = 0), both the mass evolution in an oxygen atmosphere and a differential temperature value referenced to an inert sample (alumina powder) subjected to the same experimental conditions are recorded. The oxygenation process itself is identified as an exothermic process accompanied by a second also exothermic associated process. This second process is identified as the Tetragonal-Orthorhombic transition present in the material. Based on the obtained results, the structural phase diagram of the material is reconstructed. Issues of metastability of the structural phases, as well as the potential of the equipment used to obtain these results comparable to those obtained in large facilities, are discussed. Reinterpretations of previously discussed works in the literature are inferred.","sentences":["This work examines the oxygenation process of polycrystalline powder of YBa$_2$Cu$_3$O$_{6+\\delta}$ superconductor material through constant temperature thermogravimetric analysis.","Starting from completely deoxygenated samples ($\\delta$ = 0), both the mass evolution in an oxygen atmosphere and a differential temperature value referenced to an inert sample (alumina powder) subjected to the same experimental conditions are recorded.","The oxygenation process itself is identified as an exothermic process accompanied by a second also exothermic associated process.","This second process is identified as the Tetragonal-Orthorhombic transition present in the material.","Based on the obtained results, the structural phase diagram of the material is reconstructed.","Issues of metastability of the structural phases, as well as the potential of the equipment used to obtain these results comparable to those obtained in large facilities, are discussed.","Reinterpretations of previously discussed works in the literature are inferred."],"url":"http://arxiv.org/abs/2405.12650v1","category":"cond-mat.supr-con"}
{"created":"2024-05-21 09:58:02","title":"Bi-flat F-structures as differential bicomplexes and Gauss-Manin connections","abstract":"We show that a bi-flat F-structure $(\\nabla,\\circ,e,\\nabla^*,*,E)$ on a manifold $M$ defines a differential bicomplex $(d_{\\nabla},d_{E\\circ\\nabla^*})$ on forms with value on the tangent sheaf of the manifold. Moreover, the sequence of vector fields defined recursively by $d_{\\nabla}X_{(\\alpha+1)}=d_{L\\nabla^*}X_{(\\alpha)}$ coincide with the coefficients of the formal expansion of the flat local sections of a family of flat connections $\\nabla^{GM}$ associated with the bi-flat structure. In the case of Dubrovin-Frobenius manifold the connection $\\nabla^{GM}$ (for suitable choice of an auxiliary parameter) can be identified with the Levi-Civita connection of the flat pencil of metrics defined by the invariant metric and the intesection form.","sentences":["We show that a bi-flat F-structure $(\\nabla,\\circ,e,\\nabla^*,*,E)$ on a manifold $M$ defines a differential bicomplex $(d_{\\nabla},d_{E\\circ\\nabla^*})$ on forms with value on the tangent sheaf of the manifold.","Moreover, the sequence of vector fields defined recursively by $d_{\\nabla}X_{(\\alpha+1)}=d_{L\\nabla^*}X_{(\\alpha)}$ coincide with the coefficients of the formal expansion of the flat local sections of a family of flat connections $\\nabla^{GM}$ associated with the bi-flat structure.","In the case of Dubrovin-Frobenius manifold the connection $\\nabla^{GM}$ (for suitable choice of an auxiliary parameter) can be identified with the Levi-Civita connection of the flat pencil of metrics defined by the invariant metric and the intesection form."],"url":"http://arxiv.org/abs/2405.12649v1","category":"math.DG"}
{"created":"2024-05-21 09:05:19","title":"P-adic Rankin-Selberg L-functions in universal deformation families and functional equations","abstract":"We construct a $p$-adic Rankin-Selberg $L$-function associated to the product of two families of modular forms, where the first is an ordinary (Hida) family, and the second an arbitrary universal-deformation family (without any ordinarity condition at $p$). This gives a function on a 4-dimensional base space - strictly larger than the ordinary eigenvariety, which is 3-dimensional in this case. We prove our $p$-adic $L$-function interpolates all critical values of the Rankin-Selberg $L$-functions for the classical specialisations of our family, and derive a functional equation for our $p$-adic $L$-function.","sentences":["We construct a $p$-adic Rankin-Selberg $L$-function associated to the product of two families of modular forms, where the first is an ordinary (Hida) family, and the second an arbitrary universal-deformation family (without any ordinarity condition at $p$).","This gives a function on a 4-dimensional base space - strictly larger than the ordinary eigenvariety, which is 3-dimensional in this case.","We prove our $p$-adic $L$-function interpolates all critical values of the Rankin-Selberg $L$-functions for the classical specialisations of our family, and derive a functional equation for our $p$-adic $L$-function."],"url":"http://arxiv.org/abs/2405.12611v1","category":"math.NT"}
{"created":"2024-05-21 04:05:42","title":"Molecule-induced surface second-order nonlinearity in an inversion symmetric microcavity","abstract":"Inversion symmetry eliminates the second-order nonlinear responses in materials commonly used in silicon photonics with electric-dipole approximation. The lack of effective methods to induce the second-order nonlinearity in silicon photonic materials prevents their applications in second-order nonlinear integrated photonics. Here, we experimentally demonstrate a surface second-order nonlinear optics approach for boosting the second harmonic (SH) generation process in a silica microcavity. By leveraging the molecule-induced surface second-order nonlinearity, a record high SH efficiency of about 6.7% W-1 is achieved in a silica microcavity functionalized with a surface asymmetrically-aligned molecular monolayer, which is enhanced of two to four orders of magnitude compared to that before molecule-functionalization. Furthermore, we derive the equations that govern the surface second-order nonlinear process in inversion symmetric microcavities. Our method not only enables high efficiency second-order nonlinear frequency conversions in silica photonics, but also can apply to other inversion symmetric material platforms for integrated photonics.","sentences":["Inversion symmetry eliminates the second-order nonlinear responses in materials commonly used in silicon photonics with electric-dipole approximation.","The lack of effective methods to induce the second-order nonlinearity in silicon photonic materials prevents their applications in second-order nonlinear integrated photonics.","Here, we experimentally demonstrate a surface second-order nonlinear optics approach for boosting the second harmonic (SH) generation process in a silica microcavity.","By leveraging the molecule-induced surface second-order nonlinearity, a record high SH efficiency of about 6.7% W-1 is achieved in a silica microcavity functionalized with a surface asymmetrically-aligned molecular monolayer, which is enhanced of two to four orders of magnitude compared to that before molecule-functionalization.","Furthermore, we derive the equations that govern the surface second-order nonlinear process in inversion symmetric microcavities.","Our method not only enables high efficiency second-order nonlinear frequency conversions in silica photonics, but also can apply to other inversion symmetric material platforms for integrated photonics."],"url":"http://arxiv.org/abs/2405.12483v1","category":"physics.optics"}
{"created":"2024-05-21 02:41:40","title":"A finite element-based physics-informed operator learning framework for spatiotemporal partial differential equations on arbitrary domains","abstract":"We propose a novel finite element-based physics-informed operator learning framework that allows for predicting spatiotemporal dynamics governed by partial differential equations (PDEs). The proposed framework employs a loss function inspired by the finite element method (FEM) with the implicit Euler time integration scheme. A transient thermal conduction problem is considered to benchmark the performance. The proposed operator learning framework takes a temperature field at the current time step as input and predicts a temperature field at the next time step. The Galerkin discretized weak formulation of the heat equation is employed to incorporate physics into the loss function, which is coined finite operator learning (FOL). Upon training, the networks successfully predict the temperature evolution over time for any initial temperature field at high accuracy compared to the FEM solution. The framework is also confirmed to be applicable to a heterogeneous thermal conductivity and arbitrary geometry. The advantages of FOL can be summarized as follows: First, the training is performed in an unsupervised manner, avoiding the need for a large data set prepared from costly simulations or experiments. Instead, random temperature patterns generated by the Gaussian random process and the Fourier series, combined with constant temperature fields, are used as training data to cover possible temperature cases. Second, shape functions and backward difference approximation are exploited for the domain discretization, resulting in a purely algebraic equation. This enhances training efficiency, as one avoids time-consuming automatic differentiation when optimizing weights and biases while accepting possible discretization errors. Finally, thanks to the interpolation power of FEM, any arbitrary geometry can be handled with FOL, which is crucial to addressing various engineering application scenarios.","sentences":["We propose a novel finite element-based physics-informed operator learning framework that allows for predicting spatiotemporal dynamics governed by partial differential equations (PDEs).","The proposed framework employs a loss function inspired by the finite element method (FEM) with the implicit Euler time integration scheme.","A transient thermal conduction problem is considered to benchmark the performance.","The proposed operator learning framework takes a temperature field at the current time step as input and predicts a temperature field at the next time step.","The Galerkin discretized weak formulation of the heat equation is employed to incorporate physics into the loss function, which is coined finite operator learning (FOL).","Upon training, the networks successfully predict the temperature evolution over time for any initial temperature field at high accuracy compared to the FEM solution.","The framework is also confirmed to be applicable to a heterogeneous thermal conductivity and arbitrary geometry.","The advantages of FOL can be summarized as follows:","First, the training is performed in an unsupervised manner, avoiding the need for a large data set prepared from costly simulations or experiments.","Instead, random temperature patterns generated by the Gaussian random process and the Fourier series, combined with constant temperature fields, are used as training data to cover possible temperature cases.","Second, shape functions and backward difference approximation are exploited for the domain discretization, resulting in a purely algebraic equation.","This enhances training efficiency, as one avoids time-consuming automatic differentiation when optimizing weights and biases while accepting possible discretization errors.","Finally, thanks to the interpolation power of FEM, any arbitrary geometry can be handled with FOL, which is crucial to addressing various engineering application scenarios."],"url":"http://arxiv.org/abs/2405.12465v1","category":"cs.LG"}
{"created":"2024-05-21 02:10:44","title":"One-step data-driven generative model via Schr\u00f6dinger Bridge","abstract":"Generating samples from a probability distribution is a fundamental task in machine learning and statistics. This article proposes a novel scheme for sampling from a distribution for which the probability density $\\mu({\\bf x})$ for ${\\bf x}\\in{\\mathbb{R}}^d$ is unknown, but finite independent samples are given. We focus on constructing a Schr\\\"odinger Bridge (SB) diffusion process on finite horizon $t\\in[0,1]$ which induces a probability evolution starting from a fixed point at $t=0$ and ending with the desired target distribution $\\mu({\\bf x})$ at $t=1$. The diffusion process is characterized by a stochastic differential equation whose drift function can be solely estimated from data samples through a simple one-step procedure. Compared to the classical iterative schemes developed for the SB problem, the methodology of this article is quite simple, efficient, and computationally inexpensive as it does not require the training of neural network and thus circumvents many of the challenges in building the network architecture. The performance of our new generative model is evaluated through a series of numerical experiments on multi-modal low-dimensional simulated data and high-dimensional benchmark image data. Experimental results indicate that the synthetic samples generated from our SB Bridge based algorithm are comparable with the samples generated from the state-of-the-art methods in the field. Our formulation opens up new opportunities for developing efficient diffusion models that can be directly applied to large scale real-world data.","sentences":["Generating samples from a probability distribution is a fundamental task in machine learning and statistics.","This article proposes a novel scheme for sampling from a distribution for which the probability density $\\mu({\\bf x})$ for ${\\bf x}\\in{\\mathbb{R}}^d$ is unknown, but finite independent samples are given.","We focus on constructing a Schr\\\"odinger Bridge (SB) diffusion process on finite horizon $t\\in[0,1]$ which induces a probability evolution starting from a fixed point at $t=0$ and ending with the desired target distribution $\\mu({\\bf x})$ at $t=1$. The diffusion process is characterized by a stochastic differential equation whose drift function can be solely estimated from data samples through a simple one-step procedure.","Compared to the classical iterative schemes developed for the SB problem, the methodology of this article is quite simple, efficient, and computationally inexpensive as it does not require the training of neural network and thus circumvents many of the challenges in building the network architecture.","The performance of our new generative model is evaluated through a series of numerical experiments on multi-modal low-dimensional simulated data and high-dimensional benchmark image data.","Experimental results indicate that the synthetic samples generated from our SB Bridge based algorithm are comparable with the samples generated from the state-of-the-art methods in the field.","Our formulation opens up new opportunities for developing efficient diffusion models that can be directly applied to large scale real-world data."],"url":"http://arxiv.org/abs/2405.12453v1","category":"stat.CO"}
{"created":"2024-05-21 01:39:11","title":"FFCL: Forward-Forward Net with Cortical Loops, Training and Inference on Edge Without Backpropagation","abstract":"The Forward-Forward Learning (FFL) algorithm is a recently proposed solution for training neural networks without needing memory-intensive backpropagation. During training, labels accompany input data, classifying them as positive or negative inputs. Each layer learns its response to these inputs independently. In this study, we enhance the FFL with the following contributions: 1) We optimize label processing by segregating label and feature forwarding between layers, enhancing learning performance. 2) By revising label integration, we enhance the inference process, reduce computational complexity, and improve performance. 3) We introduce feedback loops akin to cortical loops in the brain, where information cycles through and returns to earlier neurons, enabling layers to combine complex features from previous layers with lower-level features, enhancing learning efficiency.","sentences":["The Forward-Forward Learning (FFL) algorithm is a recently proposed solution for training neural networks without needing memory-intensive backpropagation.","During training, labels accompany input data, classifying them as positive or negative inputs.","Each layer learns its response to these inputs independently.","In this study, we enhance the FFL with the following contributions: 1) We optimize label processing by segregating label and feature forwarding between layers, enhancing learning performance.","2) By revising label integration, we enhance the inference process, reduce computational complexity, and improve performance.","3) We introduce feedback loops akin to cortical loops in the brain, where information cycles through and returns to earlier neurons, enabling layers to combine complex features from previous layers with lower-level features, enhancing learning efficiency."],"url":"http://arxiv.org/abs/2405.12443v1","category":"cs.LG"}
{"created":"2024-05-21 01:33:02","title":"Kinetic energy and speed powers $v^n$ of a heavy quark inside $S$ wave and $P$ wave heavy-light mesons","abstract":"Based on the instantaneous Bethe-Salpeter equation method, we calculate the average values $\\overline{|\\vec{q}|^n}\\equiv q^n$ and speed powers $\\overline{|\\vec{v}|^n} \\equiv v^n$ ($n=1,2,3,4$) of a heavy quark inside $S$ wave and $P$ wave heavy-light mesons, where $\\vec{q}$ and $\\vec{v}$ are the three dimensional momentum and velocity of the heavy quark, respectively. We obtain the kinetic energy $\\mu^2_{_\\pi}=0.455$ GeV$^2$ for the $B$ meson, which is consistent with the experimental result $0.464\\pm 0.076$ GeV$^2$. For the $B_{s}$, $D$ and $D_{s}$ mesons, the $\\mu^2_{_\\pi}$ are $0.530$ GeV$^2$, $0.317$ GeV$^2$ and $0.369$ GeV$^2$, respectively. And $v^2=0.0185$, $0.0215$, $0.121$, and $0.140$ for $B$, $B_{s}$, $D$, and $D_{s}$. We obtain some relationships, for example, $q^n_{_{0^-}}(mS)\\approx q^n_{_{1^-}}(mS)$, $q^n_{_{0^+}}(mP)\\approx q^n_{_{1^{+'}}}(mP^{'})> q^n_{_{1^+}}(mP)\\approx q^n_{_{2^+}}(mP)$, and $q^n(mS)< q^n(mP)$ ($m=1,2,3$), etc.","sentences":["Based on the instantaneous Bethe-Salpeter equation method, we calculate the average values $\\overline{|\\vec{q}|^n}\\equiv q^n$ and speed powers $\\overline{|\\vec{v}|^n} \\equiv v^n$ ($n=1,2,3,4$) of a heavy quark inside $S$ wave and $P$ wave heavy-light mesons, where $\\vec{q}$ and $\\vec{v}$ are the three dimensional momentum and velocity of the heavy quark, respectively.","We obtain the kinetic energy $\\mu^2_{_\\pi}=0.455$ GeV$^2$ for the $B$ meson, which is consistent with the experimental result $0.464\\pm 0.076$ GeV$^2$. For the $B_{s}$, $D$ and $D_{s}$ mesons, the $\\mu^2_{_\\pi}$ are $0.530$ GeV$^2$, $0.317$ GeV$^2$ and $0.369$ GeV$^2$, respectively.","And $v^2=0.0185$, $0.0215$, $0.121$, and $0.140$ for $B$, $B_{s}$, $D$, and $D_{s}$. We obtain some relationships, for example, $q^n_{_{0^-}}(mS)\\approx","q^n_{_{1^-}}(mS)$, $q^n_{_{0^+}}(mP)\\approx q^n_{_{1^{+'}}}(mP^{'})>","q^n_{_{1^+}}(mP)\\approx q^n_{_{2^+}}(mP)$, and $q^n(mS)< q^n(mP)$ ($m=1,2,3$), etc."],"url":"http://arxiv.org/abs/2405.12440v1","category":"hep-ph"}
{"created":"2024-05-21 01:15:12","title":"Power Measurement Based Channel Estimation for IRS-Enhanced Wireless Coverage","abstract":"In this paper, we study an IRS-assisted coverage enhancement problem for a given region, aiming to optimize the passive reflection of the IRS for improving the average communication performance in the region by accounting for both deterministic and random channels in the environment. To this end, we first derive the closed-form expression of the average received signal power in terms of the deterministic base station (BS)-IRS-user cascaded channels over all user locations, and propose an IRS-aided coverage enhancement framework to facilitate the estimation of such deterministic channels for IRS passive reflection design. Specifically, to avoid the exorbitant overhead of estimating the cascaded channels at all possible user locations, a location selection method is first proposed to select only a set of typical user locations for channel estimation by exploiting the channel spatial correlation in the region. To estimate the deterministic cascaded channels at the selected user locations, conventional IRS channel estimation methods require additional pilot signals, which not only results in high system training overhead but also may not be compatible with the existing communication protocols. To overcome this issue, we further propose a single-layer neural network (NN)-enabled IRS channel estimation method in this paper, based on only the average received signal power measurements at each selected location corresponding to different IRS random training reflections, which can be offline implemented in current wireless systems. Numerical results demonstrate that our proposed scheme can significantly improve the coverage performance of the target region and outperform the existing power-measurement-based IRS reflection designs.","sentences":["In this paper, we study an IRS-assisted coverage enhancement problem for a given region, aiming to optimize the passive reflection of the IRS for improving the average communication performance in the region by accounting for both deterministic and random channels in the environment.","To this end, we first derive the closed-form expression of the average received signal power in terms of the deterministic base station (BS)-IRS-user cascaded channels over all user locations, and propose an IRS-aided coverage enhancement framework to facilitate the estimation of such deterministic channels for IRS passive reflection design.","Specifically, to avoid the exorbitant overhead of estimating the cascaded channels at all possible user locations, a location selection method is first proposed to select only a set of typical user locations for channel estimation by exploiting the channel spatial correlation in the region.","To estimate the deterministic cascaded channels at the selected user locations, conventional IRS channel estimation methods require additional pilot signals, which not only results in high system training overhead but also may not be compatible with the existing communication protocols.","To overcome this issue, we further propose a single-layer neural network (NN)-enabled IRS channel estimation method in this paper, based on only the average received signal power measurements at each selected location corresponding to different IRS random training reflections, which can be offline implemented in current wireless systems.","Numerical results demonstrate that our proposed scheme can significantly improve the coverage performance of the target region and outperform the existing power-measurement-based IRS reflection designs."],"url":"http://arxiv.org/abs/2405.12432v1","category":"cs.IT"}
{"created":"2024-05-20 23:54:28","title":"GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details","abstract":"Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives. Our project page is available at: https://xuan-li.github.io/GarmentDreamerDemo/.","sentences":["Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly.","Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos.","However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model.","In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts.","In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis.","Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details.","We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives.","Our project page is available at: https://xuan-li.github.io/GarmentDreamerDemo/."],"url":"http://arxiv.org/abs/2405.12420v1","category":"cs.CV"}
{"created":"2024-05-20 23:28:38","title":"Local search for valued constraint satisfaction parameterized by treedepth","abstract":"Sometimes local search algorithms cannot efficiently find even local peaks. To understand why, I look at the structure of ascents in fitness landscapes from valued constraint satisfaction problems (VCSPs). Given a VCSP with a constraint graph of treedepth $d$, I prove that from any initial assignment there always exists an ascent of length $2^{d + 1} \\cdot n$ to a local peak. This means that short ascents always exist in fitness landscapes from constraint graphs of logarithmic treedepth, and thus also for all VCSPs of bounded treewidth. But this does not mean that local search algorithms will always find and follow such short ascents in sparse VCSPs. I show that with loglog treedepth, superpolynomial ascents exist; and for polylog treedepth, there are initial assignments from which all ascents are superpolynomial. Together, these results suggest that the study of sparse VCSPs can help us better understand the barriers to efficient local search.","sentences":["Sometimes local search algorithms cannot efficiently find even local peaks.","To understand why, I look at the structure of ascents in fitness landscapes from valued constraint satisfaction problems (VCSPs).","Given a VCSP with a constraint graph of treedepth $d$, I prove that from any initial assignment there always exists an ascent of length $2^{d + 1} \\cdot n$ to a local peak.","This means that short ascents always exist in fitness landscapes from constraint graphs of logarithmic treedepth, and thus also for all VCSPs of bounded treewidth.","But this does not mean that local search algorithms will always find and follow such short ascents in sparse VCSPs.","I show that with loglog treedepth, superpolynomial ascents exist; and for polylog treedepth, there are initial assignments from which all ascents are superpolynomial.","Together, these results suggest that the study of sparse VCSPs can help us better understand the barriers to efficient local search."],"url":"http://arxiv.org/abs/2405.12410v1","category":"cs.DM"}
{"created":"2024-05-20 22:35:34","title":"ASMR: Activation-sharing Multi-resolution Coordinate Networks For Efficient Inference","abstract":"Coordinate network or implicit neural representation (INR) is a fast-emerging method for encoding natural signals (such as images and videos) with the benefits of a compact neural representation. While numerous methods have been proposed to increase the encoding capabilities of an INR, an often overlooked aspect is the inference efficiency, usually measured in multiply-accumulate (MAC) count. This is particularly critical in use cases where inference throughput is greatly limited by hardware constraints. To this end, we propose the Activation-Sharing Multi-Resolution (ASMR) coordinate network that combines multi-resolution coordinate decomposition with hierarchical modulations. Specifically, an ASMR model enables the sharing of activations across grids of the data. This largely decouples its inference cost from its depth which is directly correlated to its reconstruction capability, and renders a near O(1) inference complexity irrespective of the number of layers. Experiments show that ASMR can reduce the MAC of a vanilla SIREN model by up to 500x while achieving an even higher reconstruction quality than its SIREN baseline.","sentences":["Coordinate network or implicit neural representation (INR) is a fast-emerging method for encoding natural signals (such as images and videos) with the benefits of a compact neural representation.","While numerous methods have been proposed to increase the encoding capabilities of an INR, an often overlooked aspect is the inference efficiency, usually measured in multiply-accumulate (MAC) count.","This is particularly critical in use cases where inference throughput is greatly limited by hardware constraints.","To this end, we propose the Activation-Sharing Multi-Resolution (ASMR) coordinate network that combines multi-resolution coordinate decomposition with hierarchical modulations.","Specifically, an ASMR model enables the sharing of activations across grids of the data.","This largely decouples its inference cost from its depth which is directly correlated to its reconstruction capability, and renders a near O(1) inference complexity irrespective of the number of layers.","Experiments show that ASMR can reduce the MAC of a vanilla SIREN model by up to 500x while achieving an even higher reconstruction quality than its SIREN baseline."],"url":"http://arxiv.org/abs/2405.12398v1","category":"cs.LG"}
{"created":"2024-05-20 22:14:18","title":"Integration of the Baker-Campbell-Hausdorff product","abstract":"In an arbitrary complete differential graded Lie algebra, we construct a group operation $\\bullet$ on $L_1$ such that the differential of the product of two elements is the Baker-Campbell-Hausdorff product of their differentials, i.e., $d(x\\bullet y)=dx\\ast dy$. We study some properties of this new structure and some applications, especially in homotopy theory, where this operation can be used to construct a Lie model for the 4-simplex. In particular, this solves, in dimension 4, a problem proposed by Lawrence and Sullivan.","sentences":["In an arbitrary complete differential graded Lie algebra, we construct a group operation $\\bullet$ on $L_1$ such that the differential of the product of two elements is the Baker-Campbell-Hausdorff product of their differentials, i.e., $d(x\\bullet y)=dx\\ast dy$. We study some properties of this new structure and some applications, especially in homotopy theory, where this operation can be used to construct a Lie model for the 4-simplex.","In particular, this solves, in dimension 4, a problem proposed by Lawrence and Sullivan."],"url":"http://arxiv.org/abs/2405.12396v1","category":"math.AT"}
{"created":"2024-05-20 22:00:18","title":"Integral canonical models of exceptional Shimura varieties","abstract":"We prove that Shimura varieties admit integral canonical models for sufficiently large primes. In the case of abelian-type Shimura varieties, this recovers work of Kisin-Kottwitz for sufficiently large primes. We also prove the existence of integral canonical models for images of period maps corresponding to geometric families. We deduce several consequences from this, including an unramified rigid analogue of Borel's extension theorem, a version of Tate semisimplicity, CM lifting theorems, and a weakened version of Tate's isogeny theorem for ordinary points.","sentences":["We prove that Shimura varieties admit integral canonical models for sufficiently large primes.","In the case of abelian-type Shimura varieties, this recovers work of Kisin-Kottwitz for sufficiently large primes.","We also prove the existence of integral canonical models for images of period maps corresponding to geometric families.","We deduce several consequences from this, including an unramified rigid analogue of Borel's extension theorem, a version of Tate semisimplicity, CM lifting theorems, and a weakened version of Tate's isogeny theorem for ordinary points."],"url":"http://arxiv.org/abs/2405.12392v1","category":"math.NT"}
{"created":"2024-05-20 21:45:20","title":"Zagreb Connection indices on polyomino chains and random polyomino chains","abstract":"In this manuscript, we delve into the exploration of the first and second Zagreb connection indices of both polyomino chains and random polyomino chains. Our methodology relies on the utilization of Markov chain theory. Within this framework, the article thoroughly examines precise formulas and investigates extreme values. Leveraging the derived formulas, we further explore and elucidate the long-term behavior exhibited by random polyomino chains.","sentences":["In this manuscript, we delve into the exploration of the first and second Zagreb connection indices of both polyomino chains and random polyomino chains.","Our methodology relies on the utilization of Markov chain theory.","Within this framework, the article thoroughly examines precise formulas and investigates extreme values.","Leveraging the derived formulas, we further explore and elucidate the long-term behavior exhibited by random polyomino chains."],"url":"http://arxiv.org/abs/2405.12389v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-20 21:39:19","title":"Vulnerability Detection with Deep Learning","abstract":"Deep learning has been shown to be a promising tool in detecting software vulnerabilities. In this work, we train neural networks with program slices extracted from the source code of C/C++ programs to detect software vulnerabilities. The program slices capture the syntax and semantic characteristics of vulnerability-related program constructs, including API function call, array usage, pointer usage, and arithmetic expression. To achieve a strong prediction model for both vulnerable code and non-vulnerable code, we compare different types of training data, different optimizers, and different types of neural networks. Our result shows that combining different types of characteristics of source code and using a balanced number of vulnerable program slices and non-vulnerable program slices produce a balanced accuracy in predicting both vulnerable code and non-vulnerable code. Among different neural networks, BGRU with the ADAM optimizer performs the best in detecting software vulnerabilities with an accuracy of 92.49%.","sentences":["Deep learning has been shown to be a promising tool in detecting software vulnerabilities.","In this work, we train neural networks with program slices extracted from the source code of C/C++ programs to detect software vulnerabilities.","The program slices capture the syntax and semantic characteristics of vulnerability-related program constructs, including API function call, array usage, pointer usage, and arithmetic expression.","To achieve a strong prediction model for both vulnerable code and non-vulnerable code, we compare different types of training data, different optimizers, and different types of neural networks.","Our result shows that combining different types of characteristics of source code and using a balanced number of vulnerable program slices and non-vulnerable program slices produce a balanced accuracy in predicting both vulnerable code and non-vulnerable code.","Among different neural networks, BGRU with the ADAM optimizer performs the best in detecting software vulnerabilities with an accuracy of 92.49%."],"url":"http://arxiv.org/abs/2405.12384v1","category":"cs.CR"}
{"created":"2024-05-20 21:10:18","title":"Spatio-temporal Attention-based Hidden Physics-informed Neural Network for Remaining Useful Life Prediction","abstract":"Predicting the Remaining Useful Life (RUL) is essential in Prognostic Health Management (PHM) for industrial systems. Although deep learning approaches have achieved considerable success in predicting RUL, challenges such as low prediction accuracy and interpretability pose significant challenges, hindering their practical implementation. In this work, we introduce a Spatio-temporal Attention-based Hidden Physics-informed Neural Network (STA-HPINN) for RUL prediction, which can utilize the associated physics of the system degradation. The spatio-temporal attention mechanism can extract important features from the input data. With the self-attention mechanism on both the sensor dimension and time step dimension, the proposed model can effectively extract degradation information. The hidden physics-informed neural network is utilized to capture the physics mechanisms that govern the evolution of RUL. With the constraint of physics, the model can achieve higher accuracy and reasonable predictions. The approach is validated on a benchmark dataset, demonstrating exceptional performance when compared to cutting-edge methods, especially in the case of complex conditions.","sentences":["Predicting the Remaining Useful Life (RUL) is essential in Prognostic Health Management (PHM) for industrial systems.","Although deep learning approaches have achieved considerable success in predicting RUL, challenges such as low prediction accuracy and interpretability pose significant challenges, hindering their practical implementation.","In this work, we introduce a Spatio-temporal Attention-based Hidden Physics-informed Neural Network (STA-HPINN) for RUL prediction, which can utilize the associated physics of the system degradation.","The spatio-temporal attention mechanism can extract important features from the input data.","With the self-attention mechanism on both the sensor dimension and time step dimension, the proposed model can effectively extract degradation information.","The hidden physics-informed neural network is utilized to capture the physics mechanisms that govern the evolution of RUL.","With the constraint of physics, the model can achieve higher accuracy and reasonable predictions.","The approach is validated on a benchmark dataset, demonstrating exceptional performance when compared to cutting-edge methods, especially in the case of complex conditions."],"url":"http://arxiv.org/abs/2405.12377v1","category":"eess.SY"}
{"created":"2024-05-20 20:37:22","title":"Mimicking Negative Mass Properties","abstract":"In the present work, one analyzes two systems trying to obtain physical conditions where some properties attributed to negative mass can be mimicked by positive mass particles. The first one is the well-known 1/2-spin system described by the Dirac equation in the presence of an external electromagnetic field. Assuming some physical restrictions, one obtains that the use of $e\\rightarrow-e$ can lead to the same results as using $m\\rightarrow-m$. In particular, for a null dielectric function, it is possible to obtain a negative mass behavior from a positive mass system composed of negatively charged particles. The second system is based on the de Broglie matter wave. The dispersion relation of such a wave can be negative (real or imaginary valued) if one assumes an imaginary wavenumber. The consequence is the emergence of a negative refractive index for positive mass particles. However, this behavior is generally attributed to a negative mass system.","sentences":["In the present work, one analyzes two systems trying to obtain physical conditions where some properties attributed to negative mass can be mimicked by positive mass particles.","The first one is the well-known 1/2-spin system described by the Dirac equation in the presence of an external electromagnetic field.","Assuming some physical restrictions, one obtains that the use of $e\\rightarrow-e$ can lead to the same results as using $m\\rightarrow-m$. In particular, for a null dielectric function, it is possible to obtain a negative mass behavior from a positive mass system composed of negatively charged particles.","The second system is based on the de Broglie matter wave.","The dispersion relation of such a wave can be negative (real or imaginary valued) if one assumes an imaginary wavenumber.","The consequence is the emergence of a negative refractive index for positive mass particles.","However, this behavior is generally attributed to a negative mass system."],"url":"http://arxiv.org/abs/2405.12366v1","category":"quant-ph"}
{"created":"2024-05-20 20:18:01","title":"DCE-Qnet: Deep Network Quantification of Dynamic Contrast Enhanced (DCE) MRI","abstract":"Introduction: Quantification of dynamic contrast-enhanced (DCE)-MRI has the potential to provide valuable clinical information, but robust pharmacokinetic modeling remains a challenge for clinical adoption.   Methods: A 7-layer neural network called DCE-Qnet was trained on simulated DCE-MRI signals derived from the Extended Tofts model with the Parker arterial input function. Network training incorporated B1 inhomogeneities to estimate perfusion (Ktrans, vp, ve), tissue T1 relaxation, proton density and bolus arrival time (BAT). The accuracy was tested in a digital phantom in comparison to a conventional nonlinear least-squares fitting (NLSQ). In vivo testing was conducted in 10 healthy subjects. Regions of interest in the cervix and uterine myometrium were used to calculate the inter-subject variability. The clinical utility was demonstrated on a cervical cancer patient. Test-retest experiments were used to assess reproducibility of the parameter maps in the tumor.   Results: The DCE-Qnet reconstruction outperformed NLSQ in the phantom. The coefficient of variation (CV) in the healthy cervix varied between 5-51% depending on the parameter. Parameter values in the tumor agreed with previous studies despite differences in methodology. The CV in the tumor varied between 1-47%.   Conclusion: The proposed approach provides comprehensive DCE-MRI quantification from a single acquisition. DCE-Qnet eliminates the need for separate T1 scan or BAT processing, leading to a reduction of 10 minutes per scan and more accurate quantification.","sentences":["Introduction: Quantification of dynamic contrast-enhanced (DCE)-MRI has the potential to provide valuable clinical information, but robust pharmacokinetic modeling remains a challenge for clinical adoption.   ","Methods: A 7-layer neural network called DCE-Qnet was trained on simulated DCE-MRI signals derived from the Extended Tofts model with the Parker arterial input function.","Network training incorporated B1 inhomogeneities to estimate perfusion (Ktrans, vp, ve), tissue T1 relaxation, proton density and bolus arrival time (BAT).","The accuracy was tested in a digital phantom in comparison to a conventional nonlinear least-squares fitting (NLSQ).","In vivo testing was conducted in 10 healthy subjects.","Regions of interest in the cervix and uterine myometrium were used to calculate the inter-subject variability.","The clinical utility was demonstrated on a cervical cancer patient.","Test-retest experiments were used to assess reproducibility of the parameter maps in the tumor.   ","Results: The DCE-Qnet reconstruction outperformed NLSQ in the phantom.","The coefficient of variation (CV) in the healthy cervix varied between 5-51% depending on the parameter.","Parameter values in the tumor agreed with previous studies despite differences in methodology.","The CV in the tumor varied between 1-47%.   ","Conclusion: The proposed approach provides comprehensive DCE-MRI quantification from a single acquisition.","DCE-Qnet eliminates the need for separate T1 scan or BAT processing, leading to a reduction of 10 minutes per scan and more accurate quantification."],"url":"http://arxiv.org/abs/2405.12360v1","category":"physics.med-ph"}
{"created":"2024-05-20 20:01:00","title":"Different kinds of accelerated propagation of relativistic electromagnetic plasma wavepackets","abstract":"Relativistic electromagnetic plasma waves are described by a dynamical equation that can be solved not only in terms of plane waves, but for several different accelerating wavepacket solutions. Depending on the spatial and temporal dependence of the plasma frequency, different kinds of accelerating solution can be obtained, for example, in terms of Airy or Weber functions. Also, we show that an arbitrary accelerated wavepacket solution is possible, for example, for a system with a luminal plasma slab.","sentences":["Relativistic electromagnetic plasma waves are described by a dynamical equation that can be solved not only in terms of plane waves, but for several different accelerating wavepacket solutions.","Depending on the spatial and temporal dependence of the plasma frequency, different kinds of accelerating solution can be obtained, for example, in terms of Airy or Weber functions.","Also, we show that an arbitrary accelerated wavepacket solution is possible, for example, for a system with a luminal plasma slab."],"url":"http://arxiv.org/abs/2405.12351v1","category":"physics.plasm-ph"}
{"created":"2024-05-20 19:58:11","title":"Holomorphic projective connections on surfaces and osculating spaces","abstract":"We study complex analytic projective connections on surfaces in projective n-spaces in terms of the \"second\" neighborhood of the surface in the ambient space, and in terms of the osculating behavior of the integral curves. We also investigate the action of a remarkable rational transformation on projective connections, and give the geometrical interpretation of joint invariants of a group closely related to the study of equivalence classes of projective connections on surfaces.","sentences":["We study complex analytic projective connections on surfaces in projective n-spaces in terms of the \"second\" neighborhood of the surface in the ambient space, and in terms of the osculating behavior of the integral curves.","We also investigate the action of a remarkable rational transformation on projective connections, and give the geometrical interpretation of joint invariants of a group closely related to the study of equivalence classes of projective connections on surfaces."],"url":"http://arxiv.org/abs/2405.12349v1","category":"math.DG"}
{"created":"2024-05-20 19:42:20","title":"Existence and uniqueness of solutions in the Lipschitz space of a functional equation and its application to the behavior of the paradise fish","abstract":"In this paper, we examine the solvability of a functional equation in a Lipschitz space. As an application, we use our result to determine the existence and uniqueness of solutions to an equation describing a specific type of choice behavior model for the learning process of the paradise fish. Finally, we present some concrete examples where, using numerical techniques, we obtain approximations to the solution of the functional equation. As the straightforward Picard's iteration can be very expensive, we show that an analytical suboptimal least-squares approximation can be chosen in practice, resulting in very good accuracy.","sentences":["In this paper, we examine the solvability of a functional equation in a Lipschitz space.","As an application, we use our result to determine the existence and uniqueness of solutions to an equation describing a specific type of choice behavior model for the learning process of the paradise fish.","Finally, we present some concrete examples where, using numerical techniques, we obtain approximations to the solution of the functional equation.","As the straightforward Picard's iteration can be very expensive, we show that an analytical suboptimal least-squares approximation can be chosen in practice, resulting in very good accuracy."],"url":"http://arxiv.org/abs/2405.12345v1","category":"math.FA"}
{"created":"2024-05-20 18:08:34","title":"Tensor-Train WENO Scheme for Compressible Flows","abstract":"In this study, we introduce a tensor-train (TT) finite difference WENO method for solving compressible Euler equations. In a step-by-step manner, the tensorization of the governing equations is demonstrated. We also introduce \\emph{LF-cross} and \\emph{WENO-cross} methods to compute numerical fluxes and the WENO reconstruction using the cross interpolation technique. A tensor-train approach is developed for boundary condition types commonly encountered in Computational Fluid Dynamics (CFD). The performance of the proposed WENO-TT solver is investigated in a rich set of numerical experiments. We demonstrate that the WENO-TT method achieves the theoretical $\\text{5}^{\\text{th}}$-order accuracy of the classical WENO scheme in smooth problems while successfully capturing complicated shock structures. In an effort to avoid the growth of TT ranks, we propose a dynamic method to estimate the TT approximation error that governs the ranks and overall truncation error of the WENO-TT scheme. Finally, we show that the traditional WENO scheme can be accelerated up to 1000 times in the TT format, and the memory requirements can be significantly decreased for low-rank problems, demonstrating the potential of tensor-train approach for future CFD application. This paper is the first study that develops a finite difference WENO scheme using the tensor-train approach for compressible flows. It is also the first comprehensive work that provides a detailed perspective into the relationship between rank, truncation error, and the TT approximation error for compressible WENO solvers.","sentences":["In this study, we introduce a tensor-train (TT) finite difference WENO method for solving compressible Euler equations.","In a step-by-step manner, the tensorization of the governing equations is demonstrated.","We also introduce \\emph{LF-cross} and \\emph{WENO-cross} methods to compute numerical fluxes and the WENO reconstruction using the cross interpolation technique.","A tensor-train approach is developed for boundary condition types commonly encountered in Computational Fluid Dynamics (CFD).","The performance of the proposed WENO-TT solver is investigated in a rich set of numerical experiments.","We demonstrate that the WENO-TT method achieves the theoretical $\\text{5}^{\\text{th}}$-order accuracy of the classical WENO scheme in smooth problems while successfully capturing complicated shock structures.","In an effort to avoid the growth of TT ranks, we propose a dynamic method to estimate the TT approximation error that governs the ranks and overall truncation error of the WENO-TT scheme.","Finally, we show that the traditional WENO scheme can be accelerated up to 1000 times in the TT format, and the memory requirements can be significantly decreased for low-rank problems, demonstrating the potential of tensor-train approach for future CFD application.","This paper is the first study that develops a finite difference WENO scheme using the tensor-train approach for compressible flows.","It is also the first comprehensive work that provides a detailed perspective into the relationship between rank, truncation error, and the TT approximation error for compressible WENO solvers."],"url":"http://arxiv.org/abs/2405.12301v1","category":"math.NA"}
{"created":"2024-05-20 18:01:15","title":"Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures. Especially inductive GNNs, which enable the processing of graph-structured data without relying on predefined graph structures, are gaining importance in an increasingly wide variety of applications. As these networks demonstrate proficiency across a range of tasks, they become lucrative targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network. A large effort has been made to develop model-stealing attacks that focus on models trained with images and texts. However, little attention has been paid to GNNs trained on graph data. This paper introduces a novel method for unsupervised model-stealing attacks against inductive GNNs, based on graph contrasting learning and spectral graph augmentations to efficiently extract information from the target model. The proposed attack is thoroughly evaluated on six datasets. The results show that this approach demonstrates a higher level of efficiency compared to existing stealing attacks. More concretely, our attack outperforms the baseline on all benchmarks achieving higher fidelity and downstream accuracy of the stolen model while requiring fewer queries sent to the target model.","sentences":["Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures.","Especially inductive GNNs, which enable the processing of graph-structured data without relying on predefined graph structures, are gaining importance in an increasingly wide variety of applications.","As these networks demonstrate proficiency across a range of tasks, they become lucrative targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network.","A large effort has been made to develop model-stealing attacks that focus on models trained with images and texts.","However, little attention has been paid to GNNs trained on graph data.","This paper introduces a novel method for unsupervised model-stealing attacks against inductive GNNs, based on graph contrasting learning and spectral graph augmentations to efficiently extract information from the target model.","The proposed attack is thoroughly evaluated on six datasets.","The results show that this approach demonstrates a higher level of efficiency compared to existing stealing attacks.","More concretely, our attack outperforms the baseline on all benchmarks achieving higher fidelity and downstream accuracy of the stolen model while requiring fewer queries sent to the target model."],"url":"http://arxiv.org/abs/2405.12295v1","category":"cs.LG"}
{"created":"2024-05-20 18:00:24","title":"Quantum Lissajous Figures via Projection","abstract":"We present a new category of quantum Lissajous states for a 2DHO having commensurate angular frequencies. The states result from the projection of ordinary coherent states onto a degenerate subspace of the 2DHO. In this way, new, non-classical quantum mechanically stationary states arise from the classical but non-stationary coherent states. The connection to Lissajous figures is that our states all have probability densities that are localized along the corresponding classical Lissajous figures. We further emphasize the important interplay between the probability current density and the emergence of quantum interference in the states we examine. In doing so, we are able to present a consistent discussion of a class of states known as vortex states.","sentences":["We present a new category of quantum Lissajous states for a 2DHO having commensurate angular frequencies.","The states result from the projection of ordinary coherent states onto a degenerate subspace of the 2DHO.","In this way, new, non-classical quantum mechanically stationary states arise from the classical but non-stationary coherent states.","The connection to Lissajous figures is that our states all have probability densities that are localized along the corresponding classical Lissajous figures.","We further emphasize the important interplay between the probability current density and the emergence of quantum interference in the states we examine.","In doing so, we are able to present a consistent discussion of a class of states known as vortex states."],"url":"http://arxiv.org/abs/2405.12291v1","category":"quant-ph"}
{"created":"2024-05-20 18:00:20","title":"Hushing black holes: tails in dynamical spacetimes","abstract":"Stationary, asymptotically flat, black hole solutions of the vacuum field equations of General Relativity belong to the Kerr family. But how does one approach this state, dynamically? Linearized fluctuations decay at late times, at fixed spatial position, as a Price power law for generic initial conditions. However, little attention was paid to forced and nonlinear spacetimes, where matter and nonlinearities play a role. We uncover a new, source-driven tail governing waves generated by pointlike matter and nonlinearities, which can dominate over Price's decay.","sentences":["Stationary, asymptotically flat, black hole solutions of the vacuum field equations of General Relativity belong to the Kerr family.","But how does one approach this state, dynamically?","Linearized fluctuations decay at late times, at fixed spatial position, as a Price power law for generic initial conditions.","However, little attention was paid to forced and nonlinear spacetimes, where matter and nonlinearities play a role.","We uncover a new, source-driven tail governing waves generated by pointlike matter and nonlinearities, which can dominate over Price's decay."],"url":"http://arxiv.org/abs/2405.12290v1","category":"gr-qc"}
{"created":"2024-05-20 18:00:02","title":"DESI Dark Energy Time Evolution is Recovered by Cosmologically Coupled Black Holes","abstract":"Recent results from baryon acoustic oscillation measurements by the Dark Energy Spectroscopic Instrument (DESI) have found preliminary evidence that dark energy (DE) evolves with time, as parameterized by a $w_0 w_a$ equation of state. In this study, we point out that the DESI-preferred $w_0w_a$ models are recovered when a $w_0w_a$ model is fit to DE produced by baryon conversion in cosmologically coupled black holes (BHs). This recovery does not require any \\emph{ad hoc} parameter adjustments; it solely depends on the independently measured cosmic star formation rate density. We discuss our result in the context of the missing baryon problem and the anomalously low sum of neutrino masses preferred by DESI. The global evolution of DE is an orthogonal probe of cosmological coupling, complementing constraints on BH mass growth from elliptical galaxies, stellar binaries, globular clusters, the LIGO-Virgo-KAGRA merging population, and X-ray binaries. A DE density that correlates with star formation is a natural outcome of cosmological coupling in BH populations.","sentences":["Recent results from baryon acoustic oscillation measurements by the Dark Energy Spectroscopic Instrument (DESI) have found preliminary evidence that dark energy (DE) evolves with time, as parameterized by a $w_0 w_a$ equation of state.","In this study, we point out that the DESI-preferred $w_0w_a$ models are recovered when a $w_0w_a$ model is fit to DE produced by baryon conversion in cosmologically coupled black holes (BHs).","This recovery does not require any \\emph{ad hoc} parameter adjustments; it solely depends on the independently measured cosmic star formation rate density.","We discuss our result in the context of the missing baryon problem and the anomalously low sum of neutrino masses preferred by DESI.","The global evolution of DE is an orthogonal probe of cosmological coupling, complementing constraints on BH mass growth from elliptical galaxies, stellar binaries, globular clusters, the LIGO-Virgo-KAGRA merging population, and X-ray binaries.","A DE density that correlates with star formation is a natural outcome of cosmological coupling in BH populations."],"url":"http://arxiv.org/abs/2405.12282v1","category":"astro-ph.CO"}
{"created":"2024-05-20 18:00:00","title":"Exploring the directly imaged HD 1160 system through spectroscopic characterisation and high-cadence variability monitoring","abstract":"The time variability and spectra of directly imaged companions provide insight into their physical properties and atmospheric dynamics. We present follow-up R~40 spectrophotometric monitoring of red companion HD 1160 B at 2.8-4.2 $\\mu$m using the double-grating 360{\\deg} vector Apodizing Phase Plate (dgvAPP360) coronagraph and ALES integral field spectrograph on the Large Binocular Telescope Interferometer. We use the recently developed technique of gvAPP-enabled differential spectrophotometry to produce differential light curves for HD 1160 B. We reproduce the previously reported ~3.2 h periodic variability in archival data, but detect no periodic variability in new observations taken the following night with a similar 3.5% level precision, suggesting rapid evolution in the variability of HD 1160 B. We also extract complementary spectra of HD 1160 B for each night. The two are mostly consistent, but the companion appears fainter on the second night between 3.0-3.2 $\\mu$m. Fitting models to these spectra produces different values for physical properties depending on the night considered. We find an effective temperature T$_{\\text{eff}}$ = 2794$^{+115}_{-133}$ K on the first night, consistent with the literature, but a cooler T$_{\\text{eff}}$ = 2279$^{+79}_{-157}$ K on the next. We estimate the mass of HD 1160 B to be 16-81 M$_{\\text{Jup}}$, depending on its age. We also present R = 50,000 high-resolution optical spectroscopy of host star HD 1160 A obtained simultaneously with the PEPSI spectrograph. We reclassify its spectral type to A1 IV-V and measure its projected rotational velocity v sin i = 96$^{+6}_{-4}$ km s$^{-1}$. We thus highlight that gvAPP-enabled differential spectrophotometry can achieve repeatable few percent level precision and does not yet reach a systematic noise floor, suggesting greater precision is achievable with additional data or advanced detrending techniques.","sentences":["The time variability and spectra of directly imaged companions provide insight into their physical properties and atmospheric dynamics.","We present follow-up R~40 spectrophotometric monitoring of red companion HD 1160 B at 2.8-4.2 $\\mu$m using the double-grating 360{\\deg} vector Apodizing Phase Plate (dgvAPP360) coronagraph and ALES integral field spectrograph on the Large Binocular Telescope Interferometer.","We use the recently developed technique of gvAPP-enabled differential spectrophotometry to produce differential light curves for HD 1160","B.","We reproduce the previously reported ~3.2 h periodic variability in archival data, but detect no periodic variability in new observations taken the following night with a similar 3.5% level precision, suggesting rapid evolution in the variability of HD 1160","B. We also extract complementary spectra of HD 1160 B for each night.","The two are mostly consistent, but the companion appears fainter on the second night between 3.0-3.2 $\\mu$m.","Fitting models to these spectra produces different values for physical properties depending on the night considered.","We find an effective temperature T$_{\\text{eff}}$ = 2794$^{+115}_{-133}$ K on the first night, consistent with the literature, but a cooler T$_{\\text{eff}}$ = 2279$^{+79}_{-157}$ K on the next.","We estimate the mass of HD 1160 B to be 16-81 M$_{\\text{Jup}}$, depending on its age.","We also present R = 50,000 high-resolution optical spectroscopy of host star HD 1160","A obtained simultaneously with the PEPSI spectrograph.","We reclassify its spectral type to A1 IV-V and measure its projected rotational velocity v sin i =","96$^{+6}_{-4}$ km s$^{-1}$. We thus highlight that gvAPP-enabled differential spectrophotometry can achieve repeatable few percent level precision and does not yet reach a systematic noise floor, suggesting greater precision is achievable with additional data or advanced detrending techniques."],"url":"http://arxiv.org/abs/2405.12271v1","category":"astro-ph.SR"}
{"created":"2024-05-20 17:59:45","title":"Locational marginal burden: Quantifying the equity of optimal power flow solutions","abstract":"Fair distribution of benefits in electric power systems is a pertinent energy policymaking problem; however, these efforts cannot be easily quantified in power system engineering studies. Therefore, we propose locational marginal burden (LMB) to provide an interface between a well-studied measure of energy pricing equity, energy burden, with an optimal power flow problem (OPF). This is achieved by investigating the intrinsic link between the dual optimal solution of an OPF problem and the electricity prices, which are used to calculate the energy burden. By applying results from the field of differentiable optimization, locational marginal prices (LMPs) associated with an OPF solution can be differentiated with respect to demand. This enables electricity retail prices, and thereby, energy burden itself, to be differentiated, resulting in the proposed LMB. Simulation of a synthetic Hawaii network interfaced with real-world socioeconomic data shows how the LMB provides new insights into how the operation of the electricity network affects the equity of energy prices.","sentences":["Fair distribution of benefits in electric power systems is a pertinent energy policymaking problem; however, these efforts cannot be easily quantified in power system engineering studies.","Therefore, we propose locational marginal burden (LMB) to provide an interface between a well-studied measure of energy pricing equity, energy burden, with an optimal power flow problem (OPF).","This is achieved by investigating the intrinsic link between the dual optimal solution of an OPF problem and the electricity prices, which are used to calculate the energy burden.","By applying results from the field of differentiable optimization, locational marginal prices (LMPs) associated with an OPF solution can be differentiated with respect to demand.","This enables electricity retail prices, and thereby, energy burden itself, to be differentiated, resulting in the proposed LMB.","Simulation of a synthetic Hawaii network interfaced with real-world socioeconomic data shows how the LMB provides new insights into how the operation of the electricity network affects the equity of energy prices."],"url":"http://arxiv.org/abs/2405.12219v1","category":"eess.SY"}
{"created":"2024-05-20 17:55:53","title":"Blow-up solutions of the \"bad\" Boussinesq equation","abstract":"We study blow-up solutions of the ``bad\" Boussinesq equation, and prove that a wide range of asymptotic scenarios can happen. For example, for each $T>0$, $x_{0}\\in \\mathbb{R}$ and $\\delta \\in (0,1)$, we prove that there exist Schwartz class solutions $u(x,t)$ on $\\mathbb{R} \\times [0,T)$ such that $|u(x,t)| \\leq C \\frac{1+x^{2}}{(x-x_{0})^{2}}$ and $u(x_{0},t)\\asymp (T-t)^{-\\delta}$ as $t\\to T$.   We also prove that for any $q\\in \\mathbb{N}$, $T>0$, $x_{0}\\in \\mathbb{R}$, $\\delta \\in (0,\\frac{1}{2})$, there exist Schwartz class solutions $u(x,t)$ on $\\mathbb{R} \\times [0,T)$ such that (i) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x,t)|\\leq C$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}\\leq q$, (ii) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x,t)| \\leq C \\frac{1+|x|}{|x-x_{0}|}$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}= q+1$, (iii) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x_{0},t)| \\asymp (T-t)^{-\\delta}$ as $t\\to T$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}= q+1$.   In particular, when $q=0$, this result establishes the existence of wave-breaking solutions, i.e. solutions that remain bounded but whose $x$-derivative blows up in finite time.","sentences":["We study blow-up solutions of the ``bad\" Boussinesq equation, and prove that a wide range of asymptotic scenarios can happen.","For example, for each $T>0$, $x_{0}\\in \\mathbb{R}$ and $\\delta \\in (0,1)$, we prove that there exist Schwartz class solutions $u(x,t)$ on $\\mathbb{R} \\times","[0,T)$ such that $|u(x,t)| \\leq C \\frac{1+x^{2}}{(x-x_{0})^{2}}$ and $u(x_{0},t)\\asymp (T-t)^{-\\delta}$ as $t\\to T$.   ","We also prove that for any $q\\in \\mathbb{N}$, $T>0$, $x_{0}\\in \\mathbb{R}$, $\\delta \\in (0,\\frac{1}{2})$, there exist Schwartz class solutions $u(x,t)$ on $\\mathbb{R} \\times","[0,T)$ such that (i) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x,t)|\\leq C$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}\\leq q$, (ii) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x,t)| \\leq C \\frac{1+|x|}{|x-x_{0}|}$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}= q+1$, (iii) $|\\partial_{x}^{q_{1}}\\partial_{t}^{q_{2}}u(x_{0},t)| \\asymp (T-t)^{-\\delta}$ as $t\\to T$ for each $q_{1},q_{2}\\in \\mathbb{N}$ such that $q_{1}+2q_{2}= q+1$.   In particular, when $q=0$, this result establishes the existence of wave-breaking solutions, i.e. solutions that remain bounded but whose $x$-derivative blows up in finite time."],"url":"http://arxiv.org/abs/2405.12210v1","category":"math.AP"}
{"created":"2024-05-20 17:41:19","title":"Accelerating Relative Entropy Coding with Space Partitioning","abstract":"Relative entropy coding (REC) algorithms encode a random sample following a target distribution $Q$, using a coding distribution $P$ shared between the sender and receiver. Sadly, general REC algorithms suffer from prohibitive encoding times, at least on the order of $2^{D_{\\text{KL}}[Q||P]}$, and faster algorithms are limited to very specific settings. This work addresses this issue by introducing a REC scheme utilizing space partitioning to reduce runtime in practical scenarios. We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications. Notably, our method successfully handles REC tasks with $D_{\\text{KL}}[Q||P]$ about three times what previous methods can manage and reduces the compression rate by approximately 5-15\\% in VAE-based lossless compression on MNIST and INR-based lossy compression on CIFAR-10 compared to previous methods, significantly improving the practicality of REC for neural compression.","sentences":["Relative entropy coding (REC) algorithms encode a random sample following a target distribution $Q$, using a coding distribution $P$ shared between the sender and receiver.","Sadly, general REC algorithms suffer from prohibitive encoding times, at least on the order of $2^{D_{\\text{KL}}[Q||P]}$, and faster algorithms are limited to very specific settings.","This work addresses this issue by introducing a REC scheme utilizing space partitioning to reduce runtime in practical scenarios.","We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications.","Notably, our method successfully handles REC tasks with $D_{\\text{KL}}[Q||P]$ about three times what previous methods can manage and reduces the compression rate by approximately 5-15\\% in VAE-based lossless compression on MNIST and INR-based lossy compression on CIFAR-10 compared to previous methods, significantly improving the practicality of REC for neural compression."],"url":"http://arxiv.org/abs/2405.12203v1","category":"cs.IT"}
{"created":"2024-05-20 17:32:16","title":"YASTN: Yet another symmetric tensor networks; A Python library for abelian symmetric tensor network calculations","abstract":"We present an open-source tensor network Python library for quantum many-body simulations. At its core is an abelian-symmetric tensor, implemented as a sparse block structure managed by logical layer on top of dense multi-dimensional array backend. This serves as the basis for higher-level tensor networks algorithms, operating on matrix product states and projected entangled pair states, implemented here. Using appropriate backend, such as PyTorch, gives direct access to automatic differentiation (AD) for cost-function gradient calculations and execution on GPUs or other supported accelerators. We show the library performance in simulations with infinite projected entangled-pair states, such as finding the ground states with AD, or simulating thermal states of the Hubbard model via imaginary time evolution. We quantify sources of performance gains in those challenging examples allowed by utilizing symmetries.","sentences":["We present an open-source tensor network Python library for quantum many-body simulations.","At its core is an abelian-symmetric tensor, implemented as a sparse block structure managed by logical layer on top of dense multi-dimensional array backend.","This serves as the basis for higher-level tensor networks algorithms, operating on matrix product states and projected entangled pair states, implemented here.","Using appropriate backend, such as PyTorch, gives direct access to automatic differentiation (AD) for cost-function gradient calculations and execution on GPUs or other supported accelerators.","We show the library performance in simulations with infinite projected entangled-pair states, such as finding the ground states with AD, or simulating thermal states of the Hubbard model via imaginary time evolution.","We quantify sources of performance gains in those challenging examples allowed by utilizing symmetries."],"url":"http://arxiv.org/abs/2405.12196v1","category":"cond-mat.str-el"}
{"created":"2024-05-20 17:20:56","title":"Cosserat elasticity as the weak-field limit of Einstein--Cartan relativity","abstract":"The weak-field limit of Einstein--Cartan (EC) relativity is studied. The equations of EC theory are rewritten such that they formally resemble those of Einstein General Relativity (EGR); this allows ideas from post-Newtonian theory to be imported without essential change. The equations of motion are then written both at first post-Newtonian (1PN) order and at 1.5PN order. EC theory's 1PN equations of motion are found to be those of a micropolar/Cosserat elastic medium, along with a decoupled evolution equation for non-classical, spin-related fields. It seems that a necessary condition for these results to hold is that one chooses the non-classical fields to scale with the speed of light in a certain empirically reasonable way. Finally, the 1.5PN equations give greater insight into the coupling between energy-momentum and spin within slowly moving, weakly gravitating matter. Specifically, the weakly relativistic modifications to Cosserat theory involve a gravitational torque and an augmentation of the gravitational force due to a `dynamic mass moment density' with an accompanying `dynamic mass moment density flux', and new forms of linear momentum density captured by a `dynamic mass density flux' and a `dynamic momentum density'.","sentences":["The weak-field limit of Einstein--Cartan (EC) relativity is studied.","The equations of EC theory are rewritten such that they formally resemble those of Einstein General Relativity (EGR); this allows ideas from post-Newtonian theory to be imported without essential change.","The equations of motion are then written both at first post-Newtonian (1PN) order and at 1.5PN order.","EC theory's 1PN equations of motion are found to be those of a micropolar/Cosserat elastic medium, along with a decoupled evolution equation for non-classical, spin-related fields.","It seems that a necessary condition for these results to hold is that one chooses the non-classical fields to scale with the speed of light in a certain empirically reasonable way.","Finally, the 1.5PN equations give greater insight into the coupling between energy-momentum and spin within slowly moving, weakly gravitating matter.","Specifically, the weakly relativistic modifications to Cosserat theory involve a gravitational torque and an augmentation of the gravitational force due to a `dynamic mass moment density' with an accompanying `dynamic mass moment density flux', and new forms of linear momentum density captured by a `dynamic mass density flux' and a `dynamic momentum density'."],"url":"http://arxiv.org/abs/2405.12188v1","category":"gr-qc"}
{"created":"2024-05-20 17:16:27","title":"Directed Metric Structures arising in Large Language Models","abstract":"Large Language Models are transformer neural networks which are trained to produce a probability distribution on the possible next words to given texts in a corpus, in such a way that the most likely word predicted is the actual word in the training text. In this paper we find what is the mathematical structure defined by such conditional probability distributions of text extensions. Changing the view point from probabilities to -log probabilities we observe that the subtext order is completely encoded in a metric structure defined on the space of texts $\\mathcal{L}$, by -log probabilities. We then construct a metric polyhedron $P(\\mathcal{L})$ and an isometric embedding (called Yoneda embedding) of $\\mathcal{L}$ into $P(\\mathcal{L})$ such that texts map to generators of certain special extremal rays. We explain that $P(\\mathcal{L})$ is a $(\\min,+)$ (tropical) linear span of these extremal ray generators. The generators also satisfy a system of $(\\min+)$ linear equations. We then show that $P(\\mathcal{L})$ is compatible with adding more text and from this we derive an approximation of a text vector as a Boltzmann weighted linear combination of the vectors for words in that text. We then prove a duality theorem showing that texts extensions and text restrictions give isometric polyhedra (even though they look a priory very different). Moreover we prove that $P(\\mathcal{L})$ is the lattice closure of (a version of) the so called, Isbell completion of $\\mathcal{L}$ which turns out to be the $(\\max,+)$ span of the text extremal ray generators. All constructions have interpretations in category theory but we don't use category theory explicitly. The categorical interpretations are briefly explained in an appendix. In the final appendix we describe how the syntax to semantics problem could fit in a general well known mathematical duality.","sentences":["Large Language Models are transformer neural networks which are trained to produce a probability distribution on the possible next words to given texts in a corpus, in such a way that the most likely word predicted is the actual word in the training text.","In this paper we find what is the mathematical structure defined by such conditional probability distributions of text extensions.","Changing the view point from probabilities to -log probabilities we observe that the subtext order is completely encoded in a metric structure defined on the space of texts $\\mathcal{L}$, by -log probabilities.","We then construct a metric polyhedron $P(\\mathcal{L})$ and an isometric embedding (called Yoneda embedding) of $\\mathcal{L}$ into $P(\\mathcal{L})$ such that texts map to generators of certain special extremal rays.","We explain that $P(\\mathcal{L})$ is a $(\\min,+)$ (tropical) linear span of these extremal ray generators.","The generators also satisfy a system of $(\\min+)$ linear equations.","We then show that $P(\\mathcal{L})$ is compatible with adding more text and from this we derive an approximation of a text vector as a Boltzmann weighted linear combination of the vectors for words in that text.","We then prove a duality theorem showing that texts extensions and text restrictions give isometric polyhedra (even though they look a priory very different).","Moreover we prove that $P(\\mathcal{L})$ is the lattice closure of (a version of) the so called, Isbell completion of $\\mathcal{L}$ which turns out to be the $(\\max,+)$ span of the text extremal ray generators.","All constructions have interpretations in category theory but we don't use category theory explicitly.","The categorical interpretations are briefly explained in an appendix.","In the final appendix we describe how the syntax to semantics problem could fit in a general well known mathematical duality."],"url":"http://arxiv.org/abs/2405.12264v1","category":"cs.LG"}
{"created":"2024-05-20 17:07:30","title":"Nearest Neighbors GParareal: Improving Scalability of Gaussian Processes for Parallel-in-Time Solvers","abstract":"With the advent of supercomputers, multi-processor environments and parallel-in-time (PinT) algorithms offer ways to solve initial value problems for ordinary and partial differential equations (ODEs and PDEs) over long time intervals, a task often unfeasible with sequential solvers within realistic time frames. A recent approach, GParareal, combines Gaussian Processes with traditional PinT methodology (Parareal) to achieve faster parallel speed-ups. The method is known to outperform Parareal for low-dimensional ODEs and a limited number of computer cores. Here, we present Nearest Neighbors GParareal (nnGParareal), a novel data-enriched PinT integration algorithm. nnGParareal builds upon GParareal by improving its scalability properties for higher-dimensional systems and increased processor count. Through data reduction, the model complexity is reduced from cubic to log-linear in the sample size, yielding a fast and automated procedure to integrate initial value problems over long time intervals. First, we provide both an upper bound for the error and theoretical details on the speed-up benefits. Then, we empirically illustrate the superior performance of nnGParareal, compared to GParareal and Parareal, on nine different systems with unique features (e.g., stiff, chaotic, high-dimensional, or challenging-to-learn systems).","sentences":["With the advent of supercomputers, multi-processor environments and parallel-in-time (PinT) algorithms offer ways to solve initial value problems for ordinary and partial differential equations (ODEs and PDEs) over long time intervals, a task often unfeasible with sequential solvers within realistic time frames.","A recent approach, GParareal, combines Gaussian Processes with traditional PinT methodology (Parareal) to achieve faster parallel speed-ups.","The method is known to outperform Parareal for low-dimensional ODEs and a limited number of computer cores.","Here, we present Nearest Neighbors GParareal (nnGParareal), a novel data-enriched PinT integration algorithm.","nnGParareal builds upon GParareal by improving its scalability properties for higher-dimensional systems and increased processor count.","Through data reduction, the model complexity is reduced from cubic to log-linear in the sample size, yielding a fast and automated procedure to integrate initial value problems over long time intervals.","First, we provide both an upper bound for the error and theoretical details on the speed-up benefits.","Then, we empirically illustrate the superior performance of nnGParareal, compared to GParareal and Parareal, on nine different systems with unique features (e.g., stiff, chaotic, high-dimensional, or challenging-to-learn systems)."],"url":"http://arxiv.org/abs/2405.12182v1","category":"stat.CO"}
{"created":"2024-05-20 17:06:52","title":"Regularization by rough Kraichnan noise for the generalised SQG equations","abstract":"We consider the generalised Surface Quasi-Geostrophic (gSQG) equations in $\\mathbb R^2$ with parameter $\\beta\\in (0,1)$, an active scalar model interpolating between SQG ($\\beta=1$) and the 2D Euler equations ($\\beta=0$) in vorticity form. Existence of weak $(L^1\\cap L^p)$-valued solutions in the deterministic setting is known, but their uniqueness is open. We show that the addition of a rough Stratonovich transport noise of Kraichnan type regularizes the PDE, providing strong existence and pathwise uniqueness of solutions for initial data $\\theta_0\\in L^1\\cap L^p$, for suitable values $p\\in[2,\\infty]$ related to the regularity degree $\\alpha$ of the noise and the singularity degree $\\beta$ of the velocity field; in particular, we can cover any $\\beta\\in (0,1)$ for suitable $\\alpha$ and $p$ and we can reach a suitable (\"critical\") threshold. The result also holds in the presence of external forcing $f\\in L^1_t (L^1\\cap L^p)$ and solutions are shown to depend continuously on the data of the problem; furthermore, they are well approximated by vanishing viscosity and regular approximations.","sentences":["We consider the generalised Surface Quasi-Geostrophic (gSQG) equations in $\\mathbb R^2$ with parameter $\\beta\\in (0,1)$, an active scalar model interpolating between SQG ($\\beta=1$) and the 2D Euler equations ($\\beta=0$) in vorticity form.","Existence of weak $(L^1\\cap L^p)$-valued solutions in the deterministic setting is known, but their uniqueness is open.","We show that the addition of a rough Stratonovich transport noise of Kraichnan type regularizes the PDE, providing strong existence and pathwise uniqueness of solutions for initial data $\\theta_0\\in L^1\\cap L^p$, for suitable values $p\\in[2,\\infty]$ related to the regularity degree $\\alpha$ of the noise and the singularity degree $\\beta$ of the velocity field; in particular, we can cover any $\\beta\\in (0,1)$ for suitable $\\alpha$ and $p$ and we can reach a suitable (\"critical\") threshold.","The result also holds in the presence of external forcing $f\\in L^1_t (L^1\\cap L^p)$ and solutions are shown to depend continuously on the data of the problem; furthermore, they are well approximated by vanishing viscosity and regular approximations."],"url":"http://arxiv.org/abs/2405.12181v1","category":"math.PR"}
{"created":"2024-05-20 16:57:59","title":"Asymptotic stability of the three-dimensional Couette flow for the Stokes-transport equation","abstract":"In this paper, we investigate the asymptotic stability of the three-dimensional Couette flow in a stratified fluid governed by the Stokes-transport equation. We observe that a similar lift-up effect to the three-dimensional Navier-Stokes equation near Couette flow destabilizes the system. We find that the inviscid damping type decay due to the Couette flow together with the damping structure caused by the decreasing background density stabilizes the system. More precisely, we prove that if the initial density is close to a linearly decreasing function in the Gevrey-$\\frac{1}{s}$ class with $\\frac{1}{2}< s\\leq 1$, namely, $\\|\\varrho_{\\mathrm{in}}(X,Y,Z)-(-Y)\\|_{\\mathcal{G}^{s}}\\leq \\epsilon$, then the perturbed density remains close to $-Y$. Moreover, the associated velocity field converges to Couette flow $(Y, 0, 0)^{\\top}$ with a convergence rate of $\\frac{1}{\\langle t\\rangle^3}$.","sentences":["In this paper, we investigate the asymptotic stability of the three-dimensional Couette flow in a stratified fluid governed by the Stokes-transport equation.","We observe that a similar lift-up effect to the three-dimensional Navier-Stokes equation near Couette flow destabilizes the system.","We find that the inviscid damping type decay due to the Couette flow together with the damping structure caused by the decreasing background density stabilizes the system.","More precisely, we prove that if the initial density is close to a linearly decreasing function in the Gevrey-$\\frac{1}{s}$ class with $\\frac{1}{2}< s\\leq 1$, namely, $\\|\\varrho_{\\mathrm{in}}(X,Y,Z)-(-Y)\\|_{\\mathcal{G}^{s}}\\leq \\epsilon$, then the perturbed density remains close to $-Y$.","Moreover, the associated velocity field converges to Couette flow $(Y, 0, 0)^{\\top}$ with a convergence rate of $\\frac{1}{\\langle t\\rangle^3}$."],"url":"http://arxiv.org/abs/2405.12173v1","category":"math.AP"}
{"created":"2024-05-20 16:52:33","title":"WiDRa -- Enabling Millimeter-Level Differential Ranging Accuracy in Wi-Fi Using Carrier Phase","abstract":"Although Wi-Fi is an ideal technology for many ranging applications, the performance of current methods is limited by the system bandwidth, leading to low accuracy of $\\sim 1$ m. For many applications, measuring differential range, viz., the change in the range between adjacent measurements, is sufficient. Correspondingly, this work proposes WiDRa - a Wi-Fi based Differential Ranging solution that provides differential range estimates by using the sum-carrier-phase information. The proposed method is not limited by system bandwidth and can track range changes even smaller than the carrier wavelength. The proposed method is first theoretically justified, while taking into consideration the various hardware impairments affecting Wi-Fi chips. In the process, methods to isolate the sum-carrier phase from the hardware impairments are proposed. Extensive simulation results show that WiDRa can achieve a differential range estimation root-mean-square-error (RMSE) of $\\approx 1$ mm in channels with a Rician-factor $\\geq 7$ (a $100 \\times$ improvement to existing methods). The proposed methods are also validated on off-the-shelf Wi-Fi hardware to demonstrate feasibility, where they achieve an RMSE of $< 1$ mm in the differential range. Finally, limitations of current investigation and future directions of exploration are suggested, to further tap into the potential of WiDRa.","sentences":["Although Wi-Fi is an ideal technology for many ranging applications, the performance of current methods is limited by the system bandwidth, leading to low accuracy of $\\sim 1$ m. For many applications, measuring differential range, viz., the change in the range between adjacent measurements, is sufficient.","Correspondingly, this work proposes WiDRa - a Wi-Fi based Differential Ranging solution that provides differential range estimates by using the sum-carrier-phase information.","The proposed method is not limited by system bandwidth and can track range changes even smaller than the carrier wavelength.","The proposed method is first theoretically justified, while taking into consideration the various hardware impairments affecting Wi-Fi chips.","In the process, methods to isolate the sum-carrier phase from the hardware impairments are proposed.","Extensive simulation results show that WiDRa can achieve a differential range estimation root-mean-square-error (RMSE) of $\\approx 1$ mm in channels with a Rician-factor $\\geq 7$ (a $100 \\times$ improvement to existing methods).","The proposed methods are also validated on off-the-shelf Wi-Fi hardware to demonstrate feasibility, where they achieve an RMSE of $< 1$ mm in the differential range.","Finally, limitations of current investigation and future directions of exploration are suggested, to further tap into the potential of WiDRa."],"url":"http://arxiv.org/abs/2405.12168v1","category":"cs.IT"}
{"created":"2024-05-20 16:51:19","title":"Asymptotic Stability of the two-dimensional Couette flow for the Stokes-transport equation in a finite channel","abstract":"We study the Stokes-transport system in a two-dimensional channel with horizontally moving boundaries, which serves as a reduced model for oceanography and sedimentation. The density is transported by the velocity field, satisfying the momentum balance between viscosity, pressure, and gravity effects, described by the Stokes equation at any given time. Due to the presence of moving boundaries, stratified densities with the Couette flow constitute one class of steady states. In this paper, we investigate the asymptotic stability of these steady states. We prove that if the stratified density is close to a constant density and the perturbation belongs to the Gevrey-3 class with compact support away from the boundary, then the velocity will converge to the Couette flow as time approaches infinity. More precisely, we prove that the horizontal perturbed velocity decays as $\\frac{1}{\\langle t\\rangle^3}$ and the vertical perturbed velocity decays as $\\frac{1}{\\langle t\\rangle^4}$.","sentences":["We study the Stokes-transport system in a two-dimensional channel with horizontally moving boundaries, which serves as a reduced model for oceanography and sedimentation.","The density is transported by the velocity field, satisfying the momentum balance between viscosity, pressure, and gravity effects, described by the Stokes equation at any given time.","Due to the presence of moving boundaries, stratified densities with the Couette flow constitute one class of steady states.","In this paper, we investigate the asymptotic stability of these steady states.","We prove that if the stratified density is close to a constant density and the perturbation belongs to the Gevrey-3 class with compact support away from the boundary, then the velocity will converge to the Couette flow as time approaches infinity.","More precisely, we prove that the horizontal perturbed velocity decays as $\\frac{1}{\\langle t\\rangle^3}$ and the vertical perturbed velocity decays as $\\frac{1}{\\langle t\\rangle^4}$."],"url":"http://arxiv.org/abs/2405.12166v1","category":"math.AP"}
{"created":"2024-05-20 16:32:37","title":"Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents","abstract":"The efficient representation, transmission, and reconstruction of three-dimensional (3D) contents are becoming increasingly important for sixth-generation (6G) networks that aim to merge virtual and physical worlds for offering immersive communication experiences. Neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) have recently emerged as two promising 3D representation techniques based on radiance field rendering, which are able to provide photorealistic rendering results for complex scenes. Therefore, embracing NeRF and 3D-GS in 6G networks is envisioned to be a prominent solution to support emerging 3D applications with enhanced quality of experience. This paper provides a comprehensive overview on the integration of NeRF and 3D-GS in 6G. First, we review the basics of the radiance field rendering techniques, and highlight their applications and implementation challenges over wireless networks. Next, we consider the over-the-air training of NeRF and 3D-GS models over wireless networks by presenting various learning techniques. We particularly focus on the federated learning design over a hierarchical device-edge-cloud architecture. Then, we discuss three practical rendering architectures of NeRF and 3D-GS models at wireless network edge. We provide model compression approaches to facilitate the transmission of radiance field models, and present rendering acceleration approaches and joint computation and communication designs to enhance the rendering efficiency. In particular, we propose a new semantic communication enabled 3D content transmission design, in which the radiance field models are exploited as the semantic knowledge base to reduce the communication overhead for distributed inference. Furthermore, we present the utilization of radiance field rendering in wireless applications like radio mapping and radio imaging.","sentences":["The efficient representation, transmission, and reconstruction of three-dimensional (3D) contents are becoming increasingly important for sixth-generation (6G) networks that aim to merge virtual and physical worlds for offering immersive communication experiences.","Neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) have recently emerged as two promising 3D representation techniques based on radiance field rendering, which are able to provide photorealistic rendering results for complex scenes.","Therefore, embracing NeRF and 3D-GS in 6G networks is envisioned to be a prominent solution to support emerging 3D applications with enhanced quality of experience.","This paper provides a comprehensive overview on the integration of NeRF and 3D-GS in 6G. First, we review the basics of the radiance field rendering techniques, and highlight their applications and implementation challenges over wireless networks.","Next, we consider the over-the-air training of NeRF and 3D-GS models over wireless networks by presenting various learning techniques.","We particularly focus on the federated learning design over a hierarchical device-edge-cloud architecture.","Then, we discuss three practical rendering architectures of NeRF and 3D-GS models at wireless network edge.","We provide model compression approaches to facilitate the transmission of radiance field models, and present rendering acceleration approaches and joint computation and communication designs to enhance the rendering efficiency.","In particular, we propose a new semantic communication enabled 3D content transmission design, in which the radiance field models are exploited as the semantic knowledge base to reduce the communication overhead for distributed inference.","Furthermore, we present the utilization of radiance field rendering in wireless applications like radio mapping and radio imaging."],"url":"http://arxiv.org/abs/2405.12155v1","category":"cs.IT"}
{"created":"2024-05-20 16:31:01","title":"Reconstruction of unknown nonlinear operators in semilinear elliptic models using optimal inputs","abstract":"Physical models often contain unknown functions and relations. The goal of our work is to answer the question of how one should excite or control a system under consideration in an appropriate way to be able to reconstruct an unknown nonlinear relation. To answer this question, we propose a greedy reconstruction algorithm within an offline-online strategy. We apply this strategy to a two-dimensional semilinear elliptic model. Our identification is based on the application of several space-dependent excitations (also called controls). These specific controls are designed by the algorithm in order to obtain a deeper insight into the underlying physical problem and a more precise reconstruction of the unknown relation. We perform numerical simulations that demonstrate the effectiveness of our approach which is not limited to the current type of equation. Since our algorithm provides not only a way to determine unknown operators by existing data but also protocols for new experiments, it is a holistic concept to tackle the problem of improving physical models.","sentences":["Physical models often contain unknown functions and relations.","The goal of our work is to answer the question of how one should excite or control a system under consideration in an appropriate way to be able to reconstruct an unknown nonlinear relation.","To answer this question, we propose a greedy reconstruction algorithm within an offline-online strategy.","We apply this strategy to a two-dimensional semilinear elliptic model.","Our identification is based on the application of several space-dependent excitations (also called controls).","These specific controls are designed by the algorithm in order to obtain a deeper insight into the underlying physical problem and a more precise reconstruction of the unknown relation.","We perform numerical simulations that demonstrate the effectiveness of our approach which is not limited to the current type of equation.","Since our algorithm provides not only a way to determine unknown operators by existing data but also protocols for new experiments, it is a holistic concept to tackle the problem of improving physical models."],"url":"http://arxiv.org/abs/2405.12153v1","category":"math.OC"}
{"created":"2024-05-20 16:15:23","title":"Comparing sharp and smooth transition of the second slow-roll parameter in single-field inflation","abstract":"In single-field inflation, violation of the slow-roll approximation can lead to growth of curvature perturbation outside the horizon. This violation is characterized by a period with a large negative value of the second slow-roll parameter. At an early time, inflation must satisfy the slow-roll approximation, so the large-scale curvature perturbation can explain the cosmic microwave background fluctuations. At intermediate time, it is viable to have a theory that violates the slow-roll approximation, which implies amplification of the curvature perturbation on small scales. Specifically, we consider ultraslow-roll inflation as the intermediate period. At late time, inflation should go back to the slow roll period so that it can end. This means that there are two transitions of the second slow-roll parameter. In this paper, we compare two different possibilities for the second transition: sharp and smooth transitions. Focusing on effects generated by the relevant cubic self-interaction of the curvature perturbation, we find that the bispectrum and one-loop correction to the power spectrum due to the change of the second slow-roll parameter vanish if and only if the Mukhanov-Sasaki equation for perturbation satisfies a specific condition called Wands duality. We also find in the case of sharp transition that, even though this duality is satisfied in the ultraslow-roll and slow-roll phases, it is severely violated at the transition so that the resultant one-loop correction is extremely large inversely proportional to the duration of the transition.","sentences":["In single-field inflation, violation of the slow-roll approximation can lead to growth of curvature perturbation outside the horizon.","This violation is characterized by a period with a large negative value of the second slow-roll parameter.","At an early time, inflation must satisfy the slow-roll approximation, so the large-scale curvature perturbation can explain the cosmic microwave background fluctuations.","At intermediate time, it is viable to have a theory that violates the slow-roll approximation, which implies amplification of the curvature perturbation on small scales.","Specifically, we consider ultraslow-roll inflation as the intermediate period.","At late time, inflation should go back to the slow roll period so that it can end.","This means that there are two transitions of the second slow-roll parameter.","In this paper, we compare two different possibilities for the second transition: sharp and smooth transitions.","Focusing on effects generated by the relevant cubic self-interaction of the curvature perturbation, we find that the bispectrum and one-loop correction to the power spectrum due to the change of the second slow-roll parameter vanish if and only if the Mukhanov-Sasaki equation for perturbation satisfies a specific condition called Wands duality.","We also find in the case of sharp transition that, even though this duality is satisfied in the ultraslow-roll and slow-roll phases, it is severely violated at the transition so that the resultant one-loop correction is extremely large inversely proportional to the duration of the transition."],"url":"http://arxiv.org/abs/2405.12145v1","category":"astro-ph.CO"}
{"created":"2024-05-20 16:13:07","title":"Alterations of electrocortical activity during hand movements induced by motor cortex glioma","abstract":"Glioma cells can reshape functional neuronal networks by hijacking neuronal synapses, leading to partial or complete neurological dysfunction. These mechanisms have been previously explored for language functions. However, the impact of glioma on sensorimotor functions is still unknown. Therefore, we recruited a control group of patients with unaffected motor cortex and a group of patients with glioma-infiltrated motor cortex, and recorded high-density electrocortical signals during finger movement tasks. The results showed that glioma suppresses task-related synchronization in the high-gamma band and reduces the power across all frequency bands. The resulting atypical motor information transmission model with discrete signaling pathways and delayed responses disrupts the stability of neuronal encoding patterns for finger movement kinematics across various temporal-spatial scales. These findings demonstrate that gliomas functionally invade neural circuits within the motor cortex. This result advances our understanding of motor function processing in chronic disease states, which is important to advance the surgical strategies and neurorehabilitation approaches for patients with malignant gliomas.","sentences":["Glioma cells can reshape functional neuronal networks by hijacking neuronal synapses, leading to partial or complete neurological dysfunction.","These mechanisms have been previously explored for language functions.","However, the impact of glioma on sensorimotor functions is still unknown.","Therefore, we recruited a control group of patients with unaffected motor cortex and a group of patients with glioma-infiltrated motor cortex, and recorded high-density electrocortical signals during finger movement tasks.","The results showed that glioma suppresses task-related synchronization in the high-gamma band and reduces the power across all frequency bands.","The resulting atypical motor information transmission model with discrete signaling pathways and delayed responses disrupts the stability of neuronal encoding patterns for finger movement kinematics across various temporal-spatial scales.","These findings demonstrate that gliomas functionally invade neural circuits within the motor cortex.","This result advances our understanding of motor function processing in chronic disease states, which is important to advance the surgical strategies and neurorehabilitation approaches for patients with malignant gliomas."],"url":"http://arxiv.org/abs/2405.12144v1","category":"q-bio.NC"}
{"created":"2024-05-20 16:00:36","title":"On the equivalence of derivatives for maps between Carnot groups","abstract":"This paper gives an alternate, elementary proof of a result of Magnani: maps between Carnot groups that are continuously differential in horizontal directions in the Euclidean sense are continuously Pansu differentiable. Our proof involves primarily Euclidean arguments. We simultaneously reprove Magnani's mean value estimate for continuously Pansu differentiable maps.","sentences":["This paper gives an alternate, elementary proof of a result of Magnani: maps between Carnot groups that are continuously differential in horizontal directions in the Euclidean sense are continuously Pansu differentiable.","Our proof involves primarily Euclidean arguments.","We simultaneously reprove Magnani's mean value estimate for continuously Pansu differentiable maps."],"url":"http://arxiv.org/abs/2405.12138v1","category":"math.MG"}
{"created":"2024-05-20 15:45:23","title":"Neutron-superfluid vortices and proton-superconductor flux tubes: Development of a minimal model for pulsar glitches","abstract":"We develop a theoretical framework that allows us to explore the coupled motion of neutron-superfluid vortices and proton-superconductor flux tubes in a gravitationally collapsed condensate, which describe neutron stars that form pulsars. Our framework uses the 3D Gross-Pitaevskii-Poisson-Equation (GPPE) for neutron Cooper pairs, the Real-Time-Ginzburg-Landau equation (RTGLE) for proton Cooper pairs, the Maxwell equations for the vector potential ${\\bf A}$, and Newtonian gravity and interactions, both direct and induced by the Poisson equation, between the neutron and proton subsystems. For a pulsar we include a crust potential, characterized by an angle $\\theta$, and frictional drag. By carrying out extensive direct numerical simulations, we obtain a variety of interesting results. We show that a rotating proton superconductor generates a uniform London magnetic field, which changes the field distribution inside flux tubes. In the absence of any direct interaction between the two species, they interact through the gravitational Poisson equation. The presence of attractive (repulsive) density-density interaction leads to the attraction (repulsion) between neutron vortices and proton flux tubes. The inclusion of the current-current interaction and the complete Maxwell equations allows us to quantify the entrainment effect that leads to induced magnetization of neutron vortices. We show that, with a strong external magnetic field ${\\bf B}_{\\rm ext}$, proton flux tubes are anchored to the crust, whereas neutron vortices leave the condensate and lead to abrupt changes of the crust angular momentum ${\\rm J}_c$. The frictional term in the dynamical equation for $\\theta$ yields stick-slip dynamics that leads, in turn, to glitches in the time series of ${\\rm J}_c$. By calculating various statistical properties of this time series, we demonstrate that they display self-organised criticality(SOC).","sentences":["We develop a theoretical framework that allows us to explore the coupled motion of neutron-superfluid vortices and proton-superconductor flux tubes in a gravitationally collapsed condensate, which describe neutron stars that form pulsars.","Our framework uses the 3D Gross-Pitaevskii-Poisson-Equation (GPPE) for neutron Cooper pairs, the Real-Time-Ginzburg-Landau equation (RTGLE) for proton Cooper pairs, the Maxwell equations for the vector potential ${\\bf A}$, and Newtonian gravity and interactions, both direct and induced by the Poisson equation, between the neutron and proton subsystems.","For a pulsar we include a crust potential, characterized by an angle $\\theta$, and frictional drag.","By carrying out extensive direct numerical simulations, we obtain a variety of interesting results.","We show that a rotating proton superconductor generates a uniform London magnetic field, which changes the field distribution inside flux tubes.","In the absence of any direct interaction between the two species, they interact through the gravitational Poisson equation.","The presence of attractive (repulsive) density-density interaction leads to the attraction (repulsion) between neutron vortices and proton flux tubes.","The inclusion of the current-current interaction and the complete Maxwell equations allows us to quantify the entrainment effect that leads to induced magnetization of neutron vortices.","We show that, with a strong external magnetic field ${\\bf B}_{\\rm ext}$, proton flux tubes are anchored to the crust, whereas neutron vortices leave the condensate and lead to abrupt changes of the crust angular momentum ${\\rm J}_c$. The frictional term in the dynamical equation for $\\theta$ yields stick-slip dynamics that leads, in turn, to glitches in the time series of ${\\rm J}_c$. By calculating various statistical properties of this time series, we demonstrate that they display self-organised criticality(SOC)."],"url":"http://arxiv.org/abs/2405.12127v1","category":"astro-ph.HE"}
{"created":"2024-05-20 15:44:07","title":"Alzheimer's Magnetic Resonance Imaging Classification Using Deep and Meta-Learning Models","abstract":"Deep learning, a cutting-edge machine learning approach, outperforms traditional machine learning in identifying intricate structures in complex high-dimensional data, particularly in the domain of healthcare. This study focuses on classifying Magnetic Resonance Imaging (MRI) data for Alzheimer's disease (AD) by leveraging deep learning techniques characterized by state-of-the-art CNNs. Brain imaging techniques such as MRI have enabled the measurement of pathophysiological brain changes related to Alzheimer's disease. Alzheimer's disease is the leading cause of dementia in the elderly, and it is an irreversible brain illness that causes gradual cognitive function disorder. In this paper, we train some benchmark deep models individually for the approach of the solution and later use an ensembling approach to combine the effect of multiple CNNs towards the observation of higher recall and accuracy. Here, the model's effectiveness is evaluated using various methods, including stacking, majority voting, and the combination of models with high recall values. The majority voting performs better than the alternative modelling approach as the majority voting approach typically reduces the variance in the predictions. We report a test accuracy of 90% with a precision score of 0.90 and a recall score of 0.89 in our proposed approach. In future, this study can be extended to incorporate other types of medical data, including signals, images, and other data. The same or alternative datasets can be used with additional classifiers, neural networks, and AI techniques to enhance Alzheimer's detection.","sentences":["Deep learning, a cutting-edge machine learning approach, outperforms traditional machine learning in identifying intricate structures in complex high-dimensional data, particularly in the domain of healthcare.","This study focuses on classifying Magnetic Resonance Imaging (MRI) data for Alzheimer's disease (AD) by leveraging deep learning techniques characterized by state-of-the-art CNNs.","Brain imaging techniques such as MRI have enabled the measurement of pathophysiological brain changes related to Alzheimer's disease.","Alzheimer's disease is the leading cause of dementia in the elderly, and it is an irreversible brain illness that causes gradual cognitive function disorder.","In this paper, we train some benchmark deep models individually for the approach of the solution and later use an ensembling approach to combine the effect of multiple CNNs towards the observation of higher recall and accuracy.","Here, the model's effectiveness is evaluated using various methods, including stacking, majority voting, and the combination of models with high recall values.","The majority voting performs better than the alternative modelling approach as the majority voting approach typically reduces the variance in the predictions.","We report a test accuracy of 90% with a precision score of 0.90 and a recall score of 0.89 in our proposed approach.","In future, this study can be extended to incorporate other types of medical data, including signals, images, and other data.","The same or alternative datasets can be used with additional classifiers, neural networks, and AI techniques to enhance Alzheimer's detection."],"url":"http://arxiv.org/abs/2405.12126v1","category":"cs.CV"}
{"created":"2024-05-20 15:32:39","title":"Asteroseismic measurement of core and envelope rotation rates for 2006 red giant branch stars","abstract":"Tens of thousands of red giant stars in the Kepler data exhibit solar-like oscillations. Their oscillations enable us to study the internal physics from core to surface, such as differential rotation. However, envelope rotation rates have been measured for only a dozen RGB stars so far. The limited sample hinders the theoretical interpretation of angular momentum transport in post-main-sequence phases. We apply a new approach to calculate the asymptotic frequencies of mixed modes, which accounts for the so-called near-degeneracy effects and leads to more proper measurements of envelope rotation rates. By fitting these asymptotic expressions to the observations, we obtain measurements of the properties of g modes and mean core and envelope rotation rates. Among 2495 stars with clear mixed-mode patterns, we found that 800 show doublets and 1206 show triplets, doubling the size of pre-existing catalogues. This led us to discover an over-density of stars that narrowly distribute around a well-defined ridge in the plane showing core rotation rate versus evolution along the RGB. With this work, we also increase the sample of stars with measured envelope rotation rates by two orders of magnitude. We find a decreasing trend between envelope rotation rates and evolution, implying that the envelopes slow down with expansion, as expected. We find 243 stars whose envelope rotation rates are significantly larger than zero. For these stars, the core-to-envelope rotation ratios are around 20 and show a large spread with evolution. Several stars show extremely mild differential rotations, with core-to-surface ratios between 1 and 2. These stars also have very slow core rotation rates, suggesting that they go through a peculiar rotational evolution. We also discovered more stars located below the degeneracy sequence, which will provide the opportunity to study the history of possible stellar mergers.","sentences":["Tens of thousands of red giant stars in the Kepler data exhibit solar-like oscillations.","Their oscillations enable us to study the internal physics from core to surface, such as differential rotation.","However, envelope rotation rates have been measured for only a dozen RGB stars so far.","The limited sample hinders the theoretical interpretation of angular momentum transport in post-main-sequence phases.","We apply a new approach to calculate the asymptotic frequencies of mixed modes, which accounts for the so-called near-degeneracy effects and leads to more proper measurements of envelope rotation rates.","By fitting these asymptotic expressions to the observations, we obtain measurements of the properties of g modes and mean core and envelope rotation rates.","Among 2495 stars with clear mixed-mode patterns, we found that 800 show doublets and 1206 show triplets, doubling the size of pre-existing catalogues.","This led us to discover an over-density of stars that narrowly distribute around a well-defined ridge in the plane showing core rotation rate versus evolution along the RGB.","With this work, we also increase the sample of stars with measured envelope rotation rates by two orders of magnitude.","We find a decreasing trend between envelope rotation rates and evolution, implying that the envelopes slow down with expansion, as expected.","We find 243 stars whose envelope rotation rates are significantly larger than zero.","For these stars, the core-to-envelope rotation ratios are around 20 and show a large spread with evolution.","Several stars show extremely mild differential rotations, with core-to-surface ratios between 1 and 2.","These stars also have very slow core rotation rates, suggesting that they go through a peculiar rotational evolution.","We also discovered more stars located below the degeneracy sequence, which will provide the opportunity to study the history of possible stellar mergers."],"url":"http://arxiv.org/abs/2405.12116v1","category":"astro-ph.SR"}
{"created":"2024-05-20 15:29:26","title":"A New Cross-Space Total Variation Regularization Model for Color Image Restoration with Quaternion Blur Operator","abstract":"The cross-channel deblurring problem in color image processing is difficult to solve due to the complex coupling and structural blurring of color pixels. Until now, there are few efficient algorithms that can reduce color infection in deblurring process. To solve this challenging problem, we present a novel cross-space total variation (CSTV) regularization model for color image deblurring by introducing a quaternion blur operator and a cross-color space regularization functional. The existence and uniqueness of the solution is proved and a new L-curve method is proposed to find a sweet balance of regularization functionals on different color spaces.   The Euler-Lagrange equation is derived to show that CSTV has taken into account the coupling of all color channels and the local smoothing within each color channel. A quaternion operator splitting method is firstly proposed to enhance the ability of color infection reduction of the CSTV regularization model. This strategy also applies to the well-known color deblurring models. Numerical experiments on color image databases illustrate the efficiency and manoeuvrability of the new model and algorithms. The color images restored by them successfully maintain the color and spatial information and are of higher quality in terms of PSNR, SSIM, MSE and CIEde2000 than the restorations of the-state-of-the-art methods.","sentences":["The cross-channel deblurring problem in color image processing is difficult to solve due to the complex coupling and structural blurring of color pixels.","Until now, there are few efficient algorithms that can reduce color infection in deblurring process.","To solve this challenging problem, we present a novel cross-space total variation (CSTV) regularization model for color image deblurring by introducing a quaternion blur operator and a cross-color space regularization functional.","The existence and uniqueness of the solution is proved and a new L-curve method is proposed to find a sweet balance of regularization functionals on different color spaces.   ","The Euler-Lagrange equation is derived to show that CSTV has taken into account the coupling of all color channels and the local smoothing within each color channel.","A quaternion operator splitting method is firstly proposed to enhance the ability of color infection reduction of the CSTV regularization model.","This strategy also applies to the well-known color deblurring models.","Numerical experiments on color image databases illustrate the efficiency and manoeuvrability of the new model and algorithms.","The color images restored by them successfully maintain the color and spatial information and are of higher quality in terms of PSNR, SSIM, MSE and CIEde2000 than the restorations of the-state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.12114v1","category":"cs.CV"}
{"created":"2024-05-20 15:23:38","title":"An effective advection induced by oscillating microstructure in a diffusion equation","abstract":"We consider the homogenisation of a diffusion equation in a porous medium. The microstructure is time-dependent and oscillating on a small time scale. This oscillation causes a novel advection in the homogenised equations. Allowing for a locally varying geometry, the oscillating microstructure demonstrates the ability to generate arbitrary and locally varying advection velocity fields.","sentences":["We consider the homogenisation of a diffusion equation in a porous medium.","The microstructure is time-dependent and oscillating on a small time scale.","This oscillation causes a novel advection in the homogenised equations.","Allowing for a locally varying geometry, the oscillating microstructure demonstrates the ability to generate arbitrary and locally varying advection velocity fields."],"url":"http://arxiv.org/abs/2405.12108v1","category":"math.AP"}
{"created":"2024-05-20 15:13:22","title":"DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction","abstract":"Math world problems correction(MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems. In this paper, leveraging the advancements in large language models (LLMs), we address two key objectives:(1) Distinguishing between mathematical reasoning and error correction; (2) Exploring strategies to enhance the error correction capabilities of LLMs in mathematics to solve MWPC task. We noticed that, in real-time education,assisting students in recognizing their mistakes is more crucial than simply providing correct answers. However, current research tends to prioritize obtaining accurate solutions to math problems rather than correcting potentially incorrect ones. Therefore, we modify the research paradigm, demonstrating that improving mathematical reasoning abilities does not equate to mastery in error correction. Meanwhile, we propose a novel method called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in error correction. In experiments, DOP has shown outstanding performance, highlighting its significant impact. We argue that in mathematical education, the demand for outstanding correctors surpasses that for proficient reasoners. Codes and data are available on https://github.com/ChenhaoEcnuCS/Reason-Correct.","sentences":["Math world problems correction(MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems.","In this paper, leveraging the advancements in large language models (LLMs), we address two key objectives:(1)","Distinguishing between mathematical reasoning and error correction; (2) Exploring strategies to enhance the error correction capabilities of LLMs in mathematics to solve MWPC task.","We noticed that, in real-time education,assisting students in recognizing their mistakes is more crucial than simply providing correct answers.","However, current research tends to prioritize obtaining accurate solutions to math problems rather than correcting potentially incorrect ones.","Therefore, we modify the research paradigm, demonstrating that improving mathematical reasoning abilities does not equate to mastery in error correction.","Meanwhile, we propose a novel method called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in error correction.","In experiments, DOP has shown outstanding performance, highlighting its significant impact.","We argue that in mathematical education, the demand for outstanding correctors surpasses that for proficient reasoners.","Codes and data are available on https://github.com/ChenhaoEcnuCS/Reason-Correct."],"url":"http://arxiv.org/abs/2405.12100v1","category":"cs.CL"}
{"created":"2024-05-20 14:59:01","title":"Product representation of perfect cubes","abstract":"Let $F_{k,d}(n)$ be the maximal size of a set ${A}\\subseteq [n]$ such that the equation \\[a_1a_2\\dots a_k=x^d, \\; a_1<a_2<\\ldots<a_k\\] has no solution with $a_1,a_2,\\ldots,a_k\\in {A}$ and integer $x$. Erd\\H{o}s, S\\'ark\\\"ozy and T. S\\'os studied $F_{k,2}$, and gave bounds when $k=2,3,4,6$ and also in the general case. We study the problem for $d=3$, and provide bounds for $k=2,3,4,6$ and $9$, furthermore, in the general case, as well. In particular, we refute an 18 years old conjecture of Verstra\\\"ete.   We also introduce another function $f_{k,d}$ closely related to $F_{k,d}$: While the original problem requires $a_1, \\ldots , a_k$ to all be distinct, we can relax this and only require that the multiset of the $a_i$'s cannot be partitioned into $d$-tuples where each $d$-tuple consists of $d$ copies of the same number.","sentences":["Let $F_{k,d}(n)$ be the maximal size of a set ${A}\\subseteq [n]$ such that the equation \\[a_1a_2\\dots a_k=x^d, \\; a_1<a_2<\\ldots<a_k\\] has no solution with $a_1,a_2,\\ldots,a_k\\in {A}$ and integer $x$. Erd\\H{o}s, S\\'ark\\\"ozy and T. S\\'os studied $F_{k,2}$, and gave bounds when $k=2,3,4,6$ and also in the general case.","We study the problem for $d=3$, and provide bounds for $k=2,3,4,6$ and $9$, furthermore, in the general case, as well.","In particular, we refute an 18 years old conjecture of Verstra\\\"ete.   ","We also introduce another function $f_{k,d}$ closely related to $F_{k,d}$: While the original problem requires $a_1, \\ldots , a_k$ to all be distinct, we can relax this and only require that the multiset of the $a_i$'s cannot be partitioned into $d$-tuples where each $d$-tuple consists of $d$ copies of the same number."],"url":"http://arxiv.org/abs/2405.12088v1","category":"math.CO"}
{"created":"2024-05-20 14:53:25","title":"Distributional Semantics, Holism, and the Instability of Meaning","abstract":"Current language models are built on the so-called distributional semantic approach to linguistic meaning that has the distributional hypothesis at its core. The distributional hypothesis involves a holistic conception of word meaning: the meaning of a word depends upon its relations to other words in the model. A standard objection to meaning holism is the charge of instability: any change in the meaning properties of a linguistic system (a human speaker, for example) would lead to many changes or possibly a complete change in the entire system. When the systems in question are trying to communicate with each other, it has been argued that instability of this kind makes communication impossible (Fodor and Lepore 1992, 1996, 1999). In this article, we examine whether the instability objection poses a problem for distributional models of meaning. First, we distinguish between distinct forms of instability that these models could exhibit, and we argue that only one such form is relevant for understanding the relation between instability and communication: what we call differential instability. Differential instability is variation in the relative distances between points in a space, rather than variation in the absolute position of those points. We distinguish differential and absolute instability by constructing two of our own models, a toy model constructed from the text of two novels, and a more sophisticated model constructed using the Word2vec algorithm from a combination of Wikipedia and SEP articles. We demonstrate the two forms of instability by showing how these models change as the corpora they are constructed from increase in size.","sentences":["Current language models are built on the so-called distributional semantic approach to linguistic meaning that has the distributional hypothesis at its core.","The distributional hypothesis involves a holistic conception of word meaning: the meaning of a word depends upon its relations to other words in the model.","A standard objection to meaning holism is the charge of instability: any change in the meaning properties of a linguistic system (a human speaker, for example) would lead to many changes or possibly a complete change in the entire system.","When the systems in question are trying to communicate with each other, it has been argued that instability of this kind makes communication impossible (Fodor and Lepore 1992, 1996, 1999).","In this article, we examine whether the instability objection poses a problem for distributional models of meaning.","First, we distinguish between distinct forms of instability that these models could exhibit, and we argue that only one such form is relevant for understanding the relation between instability and communication: what we call differential instability.","Differential instability is variation in the relative distances between points in a space, rather than variation in the absolute position of those points.","We distinguish differential and absolute instability by constructing two of our own models, a toy model constructed from the text of two novels, and a more sophisticated model constructed using the Word2vec algorithm from a combination of Wikipedia and SEP articles.","We demonstrate the two forms of instability by showing how these models change as the corpora they are constructed from increase in size."],"url":"http://arxiv.org/abs/2405.12084v1","category":"cs.CL"}
{"created":"2024-05-20 14:48:53","title":"A deep-water closure model for surface waves on axisymmetric swirling flows","abstract":"We consider the propagation of linear gravity waves on the free surface of steady, axisymmetric flows with purely azimuthal velocity. We propose a two-dimensional set of governing equations for surface waves valid in the deep-water limit. These equations come from a closure condition at the free surface that reduces the three-dimensional Euler equations in the bulk of the fluid to a set of two-dimensional equations applied only at the free surface. Since the closure condition is not obtained rigorously, it is validated numerically through comparisons with full three-dimensional calculations for vortex flows, including for a Lamb--Oseen vortex. The model presented here overcomes three limitations of existing models, namely: it is not restricted to potential base flows; it does not assume the base flow to have a flat free surface; and it does not require the use of infinite-order differential operators (such as $\\tanh(\\nabla)$) in the governing equations. The model can be applied in the case of rapid swirl (large Froude number) where the base free surface is substantially deformed. Since the model contains only derivatives of finite order, it is readily amenable to standard numerical study.","sentences":["We consider the propagation of linear gravity waves on the free surface of steady, axisymmetric flows with purely azimuthal velocity.","We propose a two-dimensional set of governing equations for surface waves valid in the deep-water limit.","These equations come from a closure condition at the free surface that reduces the three-dimensional Euler equations in the bulk of the fluid to a set of two-dimensional equations applied only at the free surface.","Since the closure condition is not obtained rigorously, it is validated numerically through comparisons with full three-dimensional calculations for vortex flows, including for a Lamb--Oseen vortex.","The model presented here overcomes three limitations of existing models, namely: it is not restricted to potential base flows; it does not assume the base flow to have a flat free surface; and it does not require the use of infinite-order differential operators (such as $\\tanh(\\nabla)$) in the governing equations.","The model can be applied in the case of rapid swirl (large Froude number) where the base free surface is substantially deformed.","Since the model contains only derivatives of finite order, it is readily amenable to standard numerical study."],"url":"http://arxiv.org/abs/2405.12078v1","category":"physics.flu-dyn"}
{"created":"2024-05-20 14:41:19","title":"The Projective Wave Theory of Consciousness","abstract":"Neural theories of consciousness face three difficulties: (1) The selection problem: how are those neurons which cause consciousness selected, from all the other neurons which do not? (2) the precision problem: how do neurons hold a detailed internal model of 3D space, as the origin of our spatial conscious experience? and (3) the decoding problem: how are the many distorted neural representations of space in the brain decoded, to give our largely undistorted conscious experience of space? These problems can all be addressed if the brains internal model of local 3D space is held not in neurons, but in a wave excitation (holding a projective transform of Euclidean space), and if the wave is the source of spatial consciousness. Such a wave has not yet been detected in the brain, but there are good reasons why it has not been detected; and there is indirect evidence for a wave, in the mammalian thalamus, and in the central body of the insect brain. The resulting projective wave theory of consciousness gives good agreement with the spatial form of our consciousness. It has a positive Bayesian balance between the complexity of its assumptions and the data it accounts for; this gives a basis to believe it.","sentences":["Neural theories of consciousness face three difficulties: (1) The selection problem: how are those neurons which cause consciousness selected, from all the other neurons which do not?","(2) the precision problem: how do neurons hold a detailed internal model of 3D space, as the origin of our spatial conscious experience?","and (3) the decoding problem: how are the many distorted neural representations of space in the brain decoded, to give our largely undistorted conscious experience of space?","These problems can all be addressed if the brains internal model of local 3D space is held not in neurons, but in a wave excitation (holding a projective transform of Euclidean space), and if the wave is the source of spatial consciousness.","Such a wave has not yet been detected in the brain, but there are good reasons why it has not been detected; and there is indirect evidence for a wave, in the mammalian thalamus, and in the central body of the insect brain.","The resulting projective wave theory of consciousness gives good agreement with the spatial form of our consciousness.","It has a positive Bayesian balance between the complexity of its assumptions and the data it accounts for; this gives a basis to believe it."],"url":"http://arxiv.org/abs/2405.12071v1","category":"q-bio.NC"}
{"created":"2024-05-20 14:39:49","title":"Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping","abstract":"By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects.","sentences":["By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity.","However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios.","We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses.","This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry.","In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors.","To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space.","We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region.","We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks.","To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects."],"url":"http://arxiv.org/abs/2405.12069v2","category":"cs.CV"}
{"created":"2024-05-20 14:26:07","title":"NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo","abstract":"In this work we present a novel multi-view photometric stereo (PS) method. Like many works in 3D reconstruction we are leveraging neural shape representations and learnt renderers. However, our work differs from the state-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal we explicity leverage per-pixel intensity renderings rather than relying mainly on estimated normals.   We model point light attenuation and explicitly raytrace cast shadows in order to best approximate each points incoming radiance. This is used as input to a fully neural material renderer that uses minimal prior assumptions and it is jointly optimised with the surface. Finally, estimated normal and segmentation maps can also incorporated in order to maximise the surface accuracy.   Our method is among the first to outperform the classical approach of DiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged at approx 1.5m distance away with approximate 400x400 resolution. Moreover, we show robustness to poor normals in low light count scenario, achieving 0.27mm Chamfer distance when pixel rendering is used instead of estimated normals.","sentences":["In this work we present a novel multi-view photometric stereo (PS) method.","Like many works in 3D reconstruction we are leveraging neural shape representations and learnt renderers.","However, our work differs from the state-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal we explicity leverage per-pixel intensity renderings rather than relying mainly on estimated normals.   ","We model point light attenuation and explicitly raytrace cast shadows in order to best approximate each points incoming radiance.","This is used as input to a fully neural material renderer that uses minimal prior assumptions and it is jointly optimised with the surface.","Finally, estimated normal and segmentation maps can also incorporated in order to maximise the surface accuracy.   ","Our method is among the first to outperform the classical approach of DiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged at approx 1.5m distance away with approximate 400x400 resolution.","Moreover, we show robustness to poor normals in low light count scenario, achieving 0.27mm Chamfer distance when pixel rendering is used instead of estimated normals."],"url":"http://arxiv.org/abs/2405.12057v1","category":"cs.CV"}
{"created":"2024-05-20 14:15:19","title":"Micro-cosmos model of a nucleon","abstract":"This study explores the age-old quest to construct a geometric model of a quantum particle. While static classical particle models have largely been dismissed, the focus has now shifted to intricate dynamic models that hold the promise of reconciling general relativity with quantum mechanics. We propose that matter particles can be described as radiation confined within dynamically curved spacetime regions, without the need for quantization of space and time, and using standard field equations and natural Planck units. Specifically, we investigate a cyclic or oscillating radiation-dominated micro cosmos undergoing repeated bouncing. Our methodology employs integration, with carefully defined initial conditions. The results include several observable properties characteristic of quantum particles. We calculate the total mass, revealing a compelling inverse proportionality between mass and radius identical with the de Broglie relationship. Applying this model to protons, we discover a profound and surprisingly simple relationship between the proton's radius and mass expressed in Planck units. This enables a definition of the proton radius that aligns remarkably well with the 2018 CODATA value. Furthermore, our analysis demonstrates that the radial density profile of the proton (or nucleon), averaged over a cycle time, increases toward the center. The problem of embedding the micro cosmos within a background spacetime is also described. These results underscore the relevance of general relativity in the domain of nuclear physics. Moreover, the model offers a fresh perspective that can stimulate new ideas in the ongoing quest to unify general relativity with quantum physics.","sentences":["This study explores the age-old quest to construct a geometric model of a quantum particle.","While static classical particle models have largely been dismissed, the focus has now shifted to intricate dynamic models that hold the promise of reconciling general relativity with quantum mechanics.","We propose that matter particles can be described as radiation confined within dynamically curved spacetime regions, without the need for quantization of space and time, and using standard field equations and natural Planck units.","Specifically, we investigate a cyclic or oscillating radiation-dominated micro cosmos undergoing repeated bouncing.","Our methodology employs integration, with carefully defined initial conditions.","The results include several observable properties characteristic of quantum particles.","We calculate the total mass, revealing a compelling inverse proportionality between mass and radius identical with the de Broglie relationship.","Applying this model to protons, we discover a profound and surprisingly simple relationship between the proton's radius and mass expressed in Planck units.","This enables a definition of the proton radius that aligns remarkably well with the 2018 CODATA value.","Furthermore, our analysis demonstrates that the radial density profile of the proton (or nucleon), averaged over a cycle time, increases toward the center.","The problem of embedding the micro cosmos within a background spacetime is also described.","These results underscore the relevance of general relativity in the domain of nuclear physics.","Moreover, the model offers a fresh perspective that can stimulate new ideas in the ongoing quest to unify general relativity with quantum physics."],"url":"http://arxiv.org/abs/2405.12049v1","category":"gr-qc"}
{"created":"2024-05-20 14:14:48","title":"Pointwise well-posedness results for degenerate It\u00f4-SDEs with locally bounded drifts","abstract":"Building on results developed in https://doi.org/10.48550/arXiv.2404.14902, where It\\^{o}-SDEs with possibly degenerate and discontinuous dispersion coefficient and measurable drift were analyzed with respect to a given (sub-)invariant measure, we develop here additional elliptic regularity results for PDEs and consider the same equations with some further regularity assumptions on the coefficients to provide a pointwise analysis for every starting point in Euclidean space, $d\\ge 2$. Our main result is (weak) well-posedness, i.e. weak existence and uniqueness in law, which we obtain under our main assumption for any locally bounded drift and arbitrary starting point among all solutions that spend zero time at the points of degeneracy of the dispersion coefficient. The points of degeneracy form a $d$-dimensional Lebesgue measure zero set, but may be hit by the weak solutions. Weak existence for arbitrary starting point is obtained under broader assumptions. In particular, in that case the drift does not need to be locally bounded.","sentences":["Building on results developed in https://doi.org/10.48550/arXiv.2404.14902, where It\\^{o}-SDEs with possibly degenerate and discontinuous dispersion coefficient and measurable drift were analyzed with respect to a given (sub-)invariant measure, we develop here additional elliptic regularity results for PDEs and consider the same equations with some further regularity assumptions on the coefficients to provide a pointwise analysis for every starting point in Euclidean space, $d\\ge 2$.","Our main result is (weak) well-posedness, i.e. weak existence and uniqueness in law, which we obtain under our main assumption for any locally bounded drift and arbitrary starting point among all solutions that spend zero time at the points of degeneracy of the dispersion coefficient.","The points of degeneracy form a $d$-dimensional Lebesgue measure zero set, but may be hit by the weak solutions.","Weak existence for arbitrary starting point is obtained under broader assumptions.","In particular, in that case the drift does not need to be locally bounded."],"url":"http://arxiv.org/abs/2405.12048v1","category":"math.PR"}
{"created":"2024-05-20 14:05:04","title":"Constraints and Time Evolution in Generic $f$(Riemann) Gravity","abstract":"We give a detailed canonical analysis of the $n$-dimensional $f$(Riemann) gravity, correcting the earlier results in the literature. We also write the field equations in the Fischer-Marsden form which is amenable to identifying the non-stationary energy on a spacelike hypersurface. We give pure $R^{2}$ theory as an example.","sentences":["We give a detailed canonical analysis of the $n$-dimensional $f$(Riemann) gravity, correcting the earlier results in the literature.","We also write the field equations in the Fischer-Marsden form which is amenable to identifying the non-stationary energy on a spacelike hypersurface.","We give pure $R^{2}$ theory as an example."],"url":"http://arxiv.org/abs/2405.12037v1","category":"gr-qc"}
{"created":"2024-05-20 13:47:45","title":"Linearized gravity and soft graviton theorem in de Sitter spacetime","abstract":"We study the linearized gravity theory in the Newman-Unti gauge in the near horizon region of the de Sitter spacetime. The linearized Einstein equation involves the cosmological constant. The near horizon symmetry consists of near horizon supertranslation and near horizon superrotation. We compute the near horizon supertranslation charge and find the proper near horizon fall-off conditions which uncover a soft graviton theorem from the Ward identity of the near horizon supertranslation.","sentences":["We study the linearized gravity theory in the Newman-Unti gauge in the near horizon region of the de Sitter spacetime.","The linearized Einstein equation involves the cosmological constant.","The near horizon symmetry consists of near horizon supertranslation and near horizon superrotation.","We compute the near horizon supertranslation charge and find the proper near horizon fall-off conditions which uncover a soft graviton theorem from the Ward identity of the near horizon supertranslation."],"url":"http://arxiv.org/abs/2405.12027v1","category":"hep-th"}
{"created":"2024-05-20 13:42:54","title":"Lattice physics approaches for neural networks","abstract":"Modern neuroscience has evolved into a frontier field that draws on numerous disciplines, resulting in the flourishing of novel conceptual frames primarily inspired by physics and complex systems science. Contributing in this direction, we recently introduced a mathematical framework to describe the spatiotemporal interactions of systems of neurons using lattice field theory, the reference paradigm for theoretical particle physics. In this note, we provide a concise summary of the basics of the theory, aiming to be intuitive to the interdisciplinary neuroscience community. We contextualize our methods, illustrating how to readily connect the parameters of our formulation to experimental variables using well-known renormalization procedures. This synopsis yields the key concepts needed to describe neural networks using lattice physics. Such classes of methods are attention-worthy in an era of blistering improvements in numerical computations, as they can facilitate relating the observation of neural activity to generative models underpinned by physical principles.","sentences":["Modern neuroscience has evolved into a frontier field that draws on numerous disciplines, resulting in the flourishing of novel conceptual frames primarily inspired by physics and complex systems science.","Contributing in this direction, we recently introduced a mathematical framework to describe the spatiotemporal interactions of systems of neurons using lattice field theory, the reference paradigm for theoretical particle physics.","In this note, we provide a concise summary of the basics of the theory, aiming to be intuitive to the interdisciplinary neuroscience community.","We contextualize our methods, illustrating how to readily connect the parameters of our formulation to experimental variables using well-known renormalization procedures.","This synopsis yields the key concepts needed to describe neural networks using lattice physics.","Such classes of methods are attention-worthy in an era of blistering improvements in numerical computations, as they can facilitate relating the observation of neural activity to generative models underpinned by physical principles."],"url":"http://arxiv.org/abs/2405.12022v1","category":"q-bio.NC"}
{"created":"2024-05-20 13:39:58","title":"Strategy-Proof Auctions through Conformal Prediction","abstract":"Auctions are key for maximizing sellers' revenue and ensuring truthful bidding among buyers. Recently, an approach known as differentiable economics based on deep learning shows promise in learning optimal auction mechanisms for multiple items and participants. However, this approach has no guarantee of strategy-proofness at test time. Strategy-proofness is crucial as it ensures that buyers are incentivized to bid their true valuations, leading to optimal and fair auction outcomes without the risk of manipulation. Building on conformal prediction, we introduce a novel approach to achieve strategy-proofness with rigorous statistical guarantees. The key novelties of our method are: (i) the formulation of a regret prediction model, used to quantify at test time violations of strategy-proofness; and (ii) an auction acceptance rule that leverages the predicted regret to ensure that for a new auction, the data-driven mechanism meets the strategy-proofness requirement with high probability (e.g., 99\\%). Numerical experiments demonstrate the necessity for rigorous guarantees, the validity of our theoretical results, and the applicability of our proposed method.","sentences":["Auctions are key for maximizing sellers' revenue and ensuring truthful bidding among buyers.","Recently, an approach known as differentiable economics based on deep learning shows promise in learning optimal auction mechanisms for multiple items and participants.","However, this approach has no guarantee of strategy-proofness at test time.","Strategy-proofness is crucial as it ensures that buyers are incentivized to bid their true valuations, leading to optimal and fair auction outcomes without the risk of manipulation.","Building on conformal prediction, we introduce a novel approach to achieve strategy-proofness with rigorous statistical guarantees.","The key novelties of our method are: (i) the formulation of a regret prediction model, used to quantify at test time violations of strategy-proofness; and (ii) an auction acceptance rule that leverages the predicted regret to ensure that for a new auction, the data-driven mechanism meets the strategy-proofness requirement with high probability (e.g., 99\\%).","Numerical experiments demonstrate the necessity for rigorous guarantees, the validity of our theoretical results, and the applicability of our proposed method."],"url":"http://arxiv.org/abs/2405.12016v2","category":"cs.GT"}
{"created":"2024-05-20 13:32:51","title":"Linear Singer-Hopf conjecture","abstract":"If $X$ is a closed $2n$-dimensional aspherical manifold, i.e., the universal cover of $X$ is contractible, then the Singer-Hopf conjecture predicts that $(-1)^n\\chi(X)\\geq 0$. We prove this conjecture when $X$ is a complex projective manifold whose fundamental group admits an almost faithful linear representation over any field. In fact, we prove a much stronger statement that if $X$ is a complex projective manifold with large fundamental group and $\\pi_1(X)$ admits an almost faithful linear representation, then $\\chi(X, {P})\\geq 0$ for any perverse sheaf ${P}$ on $X$.   To prove the main result, we introduce a vanishing cycle functor of multivalued one-forms. Then using techniques from non-abelian Hodge theories in both archimedean and non-archimedean settings, we deduce the desired positivity from the geometry of pure and mixed period maps.","sentences":["If $X$ is a closed $2n$-dimensional aspherical manifold, i.e., the universal cover of $X$ is contractible, then the Singer-Hopf conjecture predicts that $(-1)^n\\chi(X)\\geq 0$.","We prove this conjecture when $X$ is a complex projective manifold whose fundamental group admits an almost faithful linear representation over any field.","In fact, we prove a much stronger statement that if $X$ is a complex projective manifold with large fundamental group and $\\pi_1(X)$ admits an almost faithful linear representation, then $\\chi(X, {P})\\geq 0$ for any perverse sheaf ${P}$ on $X$.   To prove the main result, we introduce a vanishing cycle functor of multivalued one-forms.","Then using techniques from non-abelian Hodge theories in both archimedean and non-archimedean settings, we deduce the desired positivity from the geometry of pure and mixed period maps."],"url":"http://arxiv.org/abs/2405.12012v1","category":"math.AG"}
{"created":"2024-05-20 13:24:35","title":"Depth Reconstruction with Neural Signed Distance Fields in Structured Light Systems","abstract":"We introduce a novel depth estimation technique for multi-frame structured light setups using neural implicit representations of 3D space. Our approach employs a neural signed distance field (SDF), trained through self-supervised differentiable rendering. Unlike passive vision, where joint estimation of radiance and geometry fields is necessary, we capitalize on known radiance fields from projected patterns in structured light systems. This enables isolated optimization of the geometry field, ensuring convergence and network efficacy with fixed device positioning. To enhance geometric fidelity, we incorporate an additional color loss based on object surfaces during training. Real-world experiments demonstrate our method's superiority in geometric performance for few-shot scenarios, while achieving comparable results with increased pattern availability.","sentences":["We introduce a novel depth estimation technique for multi-frame structured light setups using neural implicit representations of 3D space.","Our approach employs a neural signed distance field (SDF), trained through self-supervised differentiable rendering.","Unlike passive vision, where joint estimation of radiance and geometry fields is necessary, we capitalize on known radiance fields from projected patterns in structured light systems.","This enables isolated optimization of the geometry field, ensuring convergence and network efficacy with fixed device positioning.","To enhance geometric fidelity, we incorporate an additional color loss based on object surfaces during training.","Real-world experiments demonstrate our method's superiority in geometric performance for few-shot scenarios, while achieving comparable results with increased pattern availability."],"url":"http://arxiv.org/abs/2405.12006v1","category":"cs.CV"}
{"created":"2024-05-20 13:13:50","title":"Lifetime Characterization of Extreme Wave Localizations in Crossing Seas","abstract":"Rogue waves (RWs) can form on the ocean surface due to quasi-four wave resonant interaction or superposition principle. Both mechanisms have been acutely studied. The first of the two is known as the nonlinear focusing mechanism and leads to an increased probability of rogue waves when wave conditions are favourable, i.e., when unidirectionality and high narrowband energy of the wave field are satisfied. This work delves into the dynamics of extreme wave focusing in crossing seas, revealing a distinct type of nonlinear RWs, characterized by a decisive longevity compared to those generated by the dispersive focusing mechanism. In fact, through fully nonlinear hydrodynamic numerical simulations, we show that the interactions between two crossing unidirectional wave beams can trigger fully localized and robust development of RWs. These coherent structures, characterized by a typical spectral broadening then spreading in the form of dual bimodality and recurrent wave group focusing, not only defy the weakening expectation of quasi-four wave resonant interaction in directionally spread wave fields, but also differ from classical focusing mechanisms already mentioned. This has been determined following a rigorous lifespan-based statistical analysis of extreme wave events in our fully nonlinear simulations. Utilizing the coupled nonlinear Schr\\\"odinger framework, we also show that such intrinsic focusing dynamics can also be captured by weakly nonlinear wave evolution equations. This opens new research avenues for further explorations of these complex and intriguing wave phenomena in hydrodynamics as well as other nonlinear and dispersive multi-wave systems.","sentences":["Rogue waves (RWs) can form on the ocean surface due to quasi-four wave resonant interaction or superposition principle.","Both mechanisms have been acutely studied.","The first of the two is known as the nonlinear focusing mechanism and leads to an increased probability of rogue waves when wave conditions are favourable, i.e., when unidirectionality and high narrowband energy of the wave field are satisfied.","This work delves into the dynamics of extreme wave focusing in crossing seas, revealing a distinct type of nonlinear RWs, characterized by a decisive longevity compared to those generated by the dispersive focusing mechanism.","In fact, through fully nonlinear hydrodynamic numerical simulations, we show that the interactions between two crossing unidirectional wave beams can trigger fully localized and robust development of RWs.","These coherent structures, characterized by a typical spectral broadening then spreading in the form of dual bimodality and recurrent wave group focusing, not only defy the weakening expectation of quasi-four wave resonant interaction in directionally spread wave fields, but also differ from classical focusing mechanisms already mentioned.","This has been determined following a rigorous lifespan-based statistical analysis of extreme wave events in our fully nonlinear simulations.","Utilizing the coupled nonlinear Schr\\\"odinger framework, we also show that such intrinsic focusing dynamics can also be captured by weakly nonlinear wave evolution equations.","This opens new research avenues for further explorations of these complex and intriguing wave phenomena in hydrodynamics as well as other nonlinear and dispersive multi-wave systems."],"url":"http://arxiv.org/abs/2405.12000v1","category":"physics.flu-dyn"}
{"created":"2024-05-20 12:48:48","title":"Wrinkling of differentially growing bilayers with similar film and substrate moduli","abstract":"The study of growth-induced surface wrinkling in constrained bilayers comprising a thin film attached to a thick substrate is a canonical model for understanding pattern formation in many biological systems. While the bilayer model has received much prior attention, the nonlinear behaviour for arrangements with similar film and substrate properties, or substrate growth that outpaces film growth, remains poorly understood. This paper therefore focuses on these cases in which the substrate's elasticity dominates surface wrinkling. We study the critical states, and the initial and advanced post-critical behaviour of growing bilayers with film-to-substrate modulus ratios in the region of $2.5$--$50$, and cases where the substrate grows faster than the film. Based on nonlinear elasticity, we formulate analytical models for linear buckling analyses and asymptotic projections around the critical point, and use finite element (FE) models coupled to continuation and branch-switching algorithms to uncover the deep post-critical regime. It is shown that a rapidly growing substrate may change the critical mode from film-governed sinusoidal wrinkling to substrate-governed Biot wrinkling depending on the stiffness ratio and growth ratio. We present a phase change diagram of the post-critical modal landscape split into sinusoidal wrinkling, period doubling, period quadrupling, and creasing regimes in terms of the stiffness ratio and growth ratio. While the post-critical regime of film- and substrate-dominated bilayers (either in terms of dominant elasticity or growth rate) is governed by sinusoidal wrinkling and Biot creasing, respectively, the intermediate regions allow for period doubling and quadrupling bifurcations. Finally, we demonstrate the existence of multi-stability in the advanced post-buckling regimes for growing bilayers where growth in the substrate surpasses that of the film.","sentences":["The study of growth-induced surface wrinkling in constrained bilayers comprising a thin film attached to a thick substrate is a canonical model for understanding pattern formation in many biological systems.","While the bilayer model has received much prior attention, the nonlinear behaviour for arrangements with similar film and substrate properties, or substrate growth that outpaces film growth, remains poorly understood.","This paper therefore focuses on these cases in which the substrate's elasticity dominates surface wrinkling.","We study the critical states, and the initial and advanced post-critical behaviour of growing bilayers with film-to-substrate modulus ratios in the region of $2.5$--$50$, and cases where the substrate grows faster than the film.","Based on nonlinear elasticity, we formulate analytical models for linear buckling analyses and asymptotic projections around the critical point, and use finite element (FE) models coupled to continuation and branch-switching algorithms to uncover the deep post-critical regime.","It is shown that a rapidly growing substrate may change the critical mode from film-governed sinusoidal wrinkling to substrate-governed Biot wrinkling depending on the stiffness ratio and growth ratio.","We present a phase change diagram of the post-critical modal landscape split into sinusoidal wrinkling, period doubling, period quadrupling, and creasing regimes in terms of the stiffness ratio and growth ratio.","While the post-critical regime of film- and substrate-dominated bilayers (either in terms of dominant elasticity or growth rate) is governed by sinusoidal wrinkling and Biot creasing, respectively, the intermediate regions allow for period doubling and quadrupling bifurcations.","Finally, we demonstrate the existence of multi-stability in the advanced post-buckling regimes for growing bilayers where growth in the substrate surpasses that of the film."],"url":"http://arxiv.org/abs/2405.11989v1","category":"cond-mat.soft"}
{"created":"2024-05-20 12:34:17","title":"A fully discrete evolving surface finite element method for the Cahn-Hilliard equation with a regular potential","abstract":"We study two fully discrete evolving surface finite element schemes for the Cahn-Hilliard equation on an evolving surface, given a smooth potential with polynomial growth. In particular we establish optimal order error bounds for a (fully implicit) backward Euler time-discretisation, and an implicit-explicit time-discretisation, with isoparametric surface finite elements discretising space.","sentences":["We study two fully discrete evolving surface finite element schemes for the Cahn-Hilliard equation on an evolving surface, given a smooth potential with polynomial growth.","In particular we establish optimal order error bounds for a (fully implicit) backward Euler time-discretisation, and an implicit-explicit time-discretisation, with isoparametric surface finite elements discretising space."],"url":"http://arxiv.org/abs/2405.11984v1","category":"math.NA"}
{"created":"2024-05-20 12:20:33","title":"Non-equilibrium orbital edge magnetization","abstract":"Uncompensated non-equilibrium orbital magnetization may arise at sample edges in the presence of charge current. The value of the effect scales as the product of the current density and the electron mean free path without any additional smallness. This non-relativistic phenomenon originates in a lack of inversion symmetry of the electron wave functions in a vicinity of sample interfaces. In a conducting layer, where $z$ direction is chosen perpendicular to the surface, and the current flows in $x$ direction, the non-equilibrium orbital magnetization points in $y$ direction. In a top-bottom symmetric layer, the orbital magnetization has an opposite sign near the top and bottom interfaces thus mimicking the symmetry of the spin-Hall effect but can exceed the latter by orders of magnitude.","sentences":["Uncompensated non-equilibrium orbital magnetization may arise at sample edges in the presence of charge current.","The value of the effect scales as the product of the current density and the electron mean free path without any additional smallness.","This non-relativistic phenomenon originates in a lack of inversion symmetry of the electron wave functions in a vicinity of sample interfaces.","In a conducting layer, where $z$ direction is chosen perpendicular to the surface, and the current flows in $x$ direction, the non-equilibrium orbital magnetization points in $y$ direction.","In a top-bottom symmetric layer, the orbital magnetization has an opposite sign near the top and bottom interfaces thus mimicking the symmetry of the spin-Hall effect but can exceed the latter by orders of magnitude."],"url":"http://arxiv.org/abs/2405.11979v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 11:48:13","title":"Sobolev regularity theory for stochastic reaction-diffusion-advection equations with spatially homogeneous colored noises and variable-order nonlocal operators","abstract":"This article investigates the existence, uniqueness, and regularity of solutions to nonlinear stochastic reaction-diffusion-advection equations (SRDAEs) with spatially homogeneous colored noises and variable-order nonlocal operators in mixed norm $L_q(L_p)$-spaces. We introduce a new condition (strongly reinforced Dalang's condition) on colored noise, which facilitates a deeper understanding of the complicated relation between nonlinearities and stochastic forces. Additionally, we establish the space-time H\\\"older type regularity of solutions.","sentences":["This article investigates the existence, uniqueness, and regularity of solutions to nonlinear stochastic reaction-diffusion-advection equations (SRDAEs) with spatially homogeneous colored noises and variable-order nonlocal operators in mixed norm $L_q(L_p)$-spaces.","We introduce a new condition (strongly reinforced Dalang's condition) on colored noise, which facilitates a deeper understanding of the complicated relation between nonlinearities and stochastic forces.","Additionally, we establish the space-time H\\\"older type regularity of solutions."],"url":"http://arxiv.org/abs/2405.11969v1","category":"math.PR"}
{"created":"2024-05-20 11:37:24","title":"Subspace embedding with random Khatri-Rao products and its application to eigensolvers","abstract":"Various iterative eigenvalue solvers have been developed to compute parts of the spectrum for a large sparse matrix, including the power method, Krylov subspace methods, contour integral methods, and preconditioned solvers such as the so called LOBPCG method. All of these solvers rely on random matrices to determine, e.g., starting vectors that have, with high probability, a non-negligible overlap with the eigenvectors of interest. For this purpose, a safe and common choice are unstructured Gaussian random matrices. In this work, we investigate the use of random Khatri-Rao products in eigenvalue solvers. On the one hand, we establish a novel subspace embedding property that provides theoretical justification for the use of such structured random matrices. On the other hand, we highlight the potential algorithmic benefits when solving eigenvalue problems with Kronecker product structure, as they arise frequently from the discretization of eigenvalue problems for differential operators on tensor product domains. In particular, we consider the use of random Khatri-Rao products within a contour integral method and LOBPCG. Numerical experiments indicate that the gains for the contour integral method strongly depend on the ability to efficiently and accurately solve (shifted) matrix equations with low-rank right-hand side. The flexibility of LOBPCG to directly employ preconditioners makes it easier to benefit from Khatri-Rao product structure, at the expense of having less theoretical justification.","sentences":["Various iterative eigenvalue solvers have been developed to compute parts of the spectrum for a large sparse matrix, including the power method, Krylov subspace methods, contour integral methods, and preconditioned solvers such as the so called LOBPCG method.","All of these solvers rely on random matrices to determine, e.g., starting vectors that have, with high probability, a non-negligible overlap with the eigenvectors of interest.","For this purpose, a safe and common choice are unstructured Gaussian random matrices.","In this work, we investigate the use of random Khatri-Rao products in eigenvalue solvers.","On the one hand, we establish a novel subspace embedding property that provides theoretical justification for the use of such structured random matrices.","On the other hand, we highlight the potential algorithmic benefits when solving eigenvalue problems with Kronecker product structure, as they arise frequently from the discretization of eigenvalue problems for differential operators on tensor product domains.","In particular, we consider the use of random Khatri-Rao products within a contour integral method and LOBPCG.","Numerical experiments indicate that the gains for the contour integral method strongly depend on the ability to efficiently and accurately solve (shifted) matrix equations with low-rank right-hand side.","The flexibility of LOBPCG to directly employ preconditioners makes it easier to benefit from Khatri-Rao product structure, at the expense of having less theoretical justification."],"url":"http://arxiv.org/abs/2405.11962v1","category":"math.NA"}
{"created":"2024-05-20 11:21:23","title":"Shallow Recurrent Decoder for Reduced Order Modeling of Plasma Dynamics","abstract":"Reduced order models are becoming increasingly important for rendering complex and multiscale spatio-temporal dynamics computationally tractable. The computational efficiency of such surrogate models is especially important for design, exhaustive exploration and physical understanding. Plasma simulations, in particular those applied to the study of ${\\bf E}\\times {\\bf B}$ plasma discharges and technologies, such as Hall thrusters, require substantial computational resources in order to resolve the multidimentional dynamics that span across wide spatial and temporal scales. Although high-fidelity computational tools are available to simulate such systems over limited conditions and in highly simplified geometries, simulations of full-size systems and/or extensive parametric studies over many geometric configurations and under different physical conditions are computationally intractable with conventional numerical tools. Thus, scientific studies and industrially oriented modeling of plasma systems, including the important ${\\bf E}\\times {\\bf B}$ technologies, stand to significantly benefit from reduced order modeling algorithms. We develop a model reduction scheme based upon a {\\em Shallow REcurrent Decoder} (SHRED) architecture. The scheme uses a neural network for encoding limited sensor measurements in time (sequence-to-sequence encoding) to full state-space reconstructions via a decoder network. Based upon the theory of separation of variables, the SHRED architecture is capable of (i) reconstructing full spatio-temporal fields with as little as three point sensors, even the fields that are not measured with sensor feeds but that are in dynamic coupling with the measured field, and (ii) forecasting the future state of the system using neural network roll-outs from the trained time encoding model.","sentences":["Reduced order models are becoming increasingly important for rendering complex and multiscale spatio-temporal dynamics computationally tractable.","The computational efficiency of such surrogate models is especially important for design, exhaustive exploration and physical understanding.","Plasma simulations, in particular those applied to the study of ${\\bf E}\\times {\\bf B}$ plasma discharges and technologies, such as Hall thrusters, require substantial computational resources in order to resolve the multidimentional dynamics that span across wide spatial and temporal scales.","Although high-fidelity computational tools are available to simulate such systems over limited conditions and in highly simplified geometries, simulations of full-size systems and/or extensive parametric studies over many geometric configurations and under different physical conditions are computationally intractable with conventional numerical tools.","Thus, scientific studies and industrially oriented modeling of plasma systems, including the important ${\\bf E}\\times {\\bf B}$ technologies, stand to significantly benefit from reduced order modeling algorithms.","We develop a model reduction scheme based upon a {\\em Shallow REcurrent Decoder} (SHRED) architecture.","The scheme uses a neural network for encoding limited sensor measurements in time (sequence-to-sequence encoding) to full state-space reconstructions via a decoder network.","Based upon the theory of separation of variables, the SHRED architecture is capable of (i) reconstructing full spatio-temporal fields with as little as three point sensors, even the fields that are not measured with sensor feeds but that are in dynamic coupling with the measured field, and (ii) forecasting the future state of the system using neural network roll-outs from the trained time encoding model."],"url":"http://arxiv.org/abs/2405.11955v1","category":"physics.plasm-ph"}
{"created":"2024-05-20 11:12:33","title":"A gluing construction of constant scalar curvature K\u00e4hler metrics of Poincar\u00e9 type","abstract":"Given a compact K\\\"ahler manifold with no non-trivial holomorphic vector field, assume it admits a constant scalar curvature K\\\"ahler metric. Fix finitely many points, we show the existence of constant scalar curvature K\\\"ahler metrics of Poincar\\'e type on the complement of these points in the compact manifold.","sentences":["Given a compact K\\\"ahler manifold with no non-trivial holomorphic vector field, assume it admits a constant scalar curvature K\\\"ahler metric.","Fix finitely many points, we show the existence of constant scalar curvature K\\\"ahler metrics of Poincar\\'e type on the complement of these points in the compact manifold."],"url":"http://arxiv.org/abs/2405.11952v1","category":"math.DG"}
{"created":"2024-05-20 10:29:21","title":"On the equivalence of two spinodal decomposition criteria with a case study of Fe${}_{15}$Co${}_{15}$Ni${}_{35}$Cu${}_{35}$ multicomponent alloy","abstract":"Spinodal decomposition in multicomponent alloys has attracted increasing attention due to its beneficial effect on their mechanical and functional properties and potential applications. Both based on the Cahn-Hillard equation, the reference element method (REM) and the projection matrix method (PMM) are the two main methods to predict the occurrence of spinodal decomposition in multicomponent alloys. In this work, it is mathematically proven that the two methods are equivalent, and therefore the advanced results based on one method can be applied to the other. Based on these methods, the $Fe{}_{15}$Co${}_{15}$Ni${}_{35}$Cu${}_{35}$ multicomponent alloy is designed as a case study. Experimental results confirm the spinodal decomposition in the heat-treated alloy, and its strength and ductility are simultaneously enhanced. This work can be the pavement for further theoretical and experimental studies on the spinodal decomposition in multicomponent alloys.","sentences":["Spinodal decomposition in multicomponent alloys has attracted increasing attention due to its beneficial effect on their mechanical and functional properties and potential applications.","Both based on the Cahn-Hillard equation, the reference element method (REM) and the projection matrix method (PMM) are the two main methods to predict the occurrence of spinodal decomposition in multicomponent alloys.","In this work, it is mathematically proven that the two methods are equivalent, and therefore the advanced results based on one method can be applied to the other.","Based on these methods, the $Fe{}_{15}$Co${}_{15}$Ni${}_{35}$Cu${}_{35}$ multicomponent alloy is designed as a case study.","Experimental results confirm the spinodal decomposition in the heat-treated alloy, and its strength and ductility are simultaneously enhanced.","This work can be the pavement for further theoretical and experimental studies on the spinodal decomposition in multicomponent alloys."],"url":"http://arxiv.org/abs/2405.11940v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 10:25:50","title":"Convergence of the area functional on spaces with lower Ricci bounds and applications","abstract":"The goal of this note is to prove convergence results w.r.t. the area functional on metric measure spaces with a lower Ricci curvature bound. The paper can be divided in two parts. In the first part, we show that the heat flow provides good approximation properties for the area functional on $RCD(K,\\infty)$ spaces, implying that in this setting the area formula for functions of bounded variation holds and that the area functional coincides with its relaxation. Moreover, we obtain partial regularity and uniqueness results for functions whose epigraphs are perimeter minimizing. In the second part of the paper, we consider Ricci limit spaces (and also finite dimensional $RCD(K,N)$ spaces for some results) and we show that, thanks to the previously obtained properties, minimizers of the area functional can be approximated with minimizers along the converging sequence of manifolds. As a first application, we show that minimizers of the area functional on non-collapsed Ricci limit spaces are locally Lipschitz and satisfy a-priori gradient estimates. Secondly, we obtain a Bernstein-type result for area minimizers with sublinear growth at infinity.","sentences":["The goal of this note is to prove convergence results w.r.t.","the area functional on metric measure spaces with a lower Ricci curvature bound.","The paper can be divided in two parts.","In the first part, we show that the heat flow provides good approximation properties for the area functional on $RCD(K,\\infty)$ spaces, implying that in this setting the area formula for functions of bounded variation holds and that the area functional coincides with its relaxation.","Moreover, we obtain partial regularity and uniqueness results for functions whose epigraphs are perimeter minimizing.","In the second part of the paper, we consider Ricci limit spaces (and also finite dimensional $RCD(K,N)$ spaces for some results) and we show that, thanks to the previously obtained properties, minimizers of the area functional can be approximated with minimizers along the converging sequence of manifolds.","As a first application, we show that minimizers of the area functional on non-collapsed Ricci limit spaces are locally Lipschitz and satisfy a-priori gradient estimates.","Secondly, we obtain a Bernstein-type result for area minimizers with sublinear growth at infinity."],"url":"http://arxiv.org/abs/2405.11938v1","category":"math.DG"}
{"created":"2024-05-20 10:16:26","title":"Nonequilbrium physics of generative diffusion models","abstract":"Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interest from industrial application, but a complete picture about inherent mechanisms is still lacking. In this paper, we provide a transparent physics analysis of the diffusion models, deriving the fluctuation theorem, entropy production, Franz-Parisi potential to understand the intrinsic phase transitions discovered recently. Our analysis is rooted in non-equlibrium physics and concepts from equilibrium physics, i.e., treating both forward and backward dynamics as a Langevin dynamics, and treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder studied in spin glass theory. This unified principle is expected to guide machine learning practitioners to design better algorithms and theoretical physicists to link the machine learning to non-equilibrium thermodynamics.","sentences":["Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interest from industrial application, but a complete picture about inherent mechanisms is still lacking.","In this paper, we provide a transparent physics analysis of the diffusion models, deriving the fluctuation theorem, entropy production, Franz-Parisi potential to understand the intrinsic phase transitions discovered recently.","Our analysis is rooted in non-equlibrium physics and concepts from equilibrium physics, i.e., treating both forward and backward dynamics as a Langevin dynamics, and treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder studied in spin glass theory.","This unified principle is expected to guide machine learning practitioners to design better algorithms and theoretical physicists to link the machine learning to non-equilibrium thermodynamics."],"url":"http://arxiv.org/abs/2405.11932v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-20 10:06:19","title":"Response time in a pair of processor sharing queues with Join-the-Shortest-Queue scheduling","abstract":"Join-the-Shortest-Queue (JSQ) is the scheduling policy of choice for many network providers, cloud servers and traffic management systems, where individual queues are served under processor sharing (PS) queueing discipline. A numerical solution for the response time distribution in two parallel PS queues with JSQ scheduling is derived for the first time. Using the generating function method, two partial differential equations (PDEs) are obtained corresponding to conditional response times, where the conditioning is on a particular traced task joining the first or the second queue. These PDEs are functional equations that contain partial generating functions and their partial derivatives, and therefore cannot be solved by commonly used techniques. We are able to solve these PDEs numerically with good accuracy and perform the deconditioning with respect to the queue-length probabilities by evaluating a certain complex integral. Numerical results for the density and the first four moments compare well against regenerative simulation with 500,000 regeneration cycles.","sentences":["Join-the-Shortest-Queue (JSQ) is the scheduling policy of choice for many network providers, cloud servers and traffic management systems, where individual queues are served under processor sharing (PS) queueing discipline.","A numerical solution for the response time distribution in two parallel PS queues with JSQ scheduling is derived for the first time.","Using the generating function method, two partial differential equations (PDEs) are obtained corresponding to conditional response times, where the conditioning is on a particular traced task joining the first or the second queue.","These PDEs are functional equations that contain partial generating functions and their partial derivatives, and therefore cannot be solved by commonly used techniques.","We are able to solve these PDEs numerically with good accuracy and perform the deconditioning with respect to the queue-length probabilities by evaluating a certain complex integral.","Numerical results for the density and the first four moments compare well against regenerative simulation with 500,000 regeneration cycles."],"url":"http://arxiv.org/abs/2405.11927v1","category":"cs.PF"}
{"created":"2024-05-20 10:03:26","title":"The Dirichlet problem for Monge-Amp\u00e8re type equations on Riemannian manifolds","abstract":"In this paper, we study the Dirichlet problem for Monge-Amp\\`ere type equations for p-plurisubharmonic functions on Riemannian manifolds. The a priori estimates up to the second order derivatives of solutions are established. The existence of a solution then follows by the continuity method.","sentences":["In this paper, we study the Dirichlet problem for Monge-Amp\\`ere type equations for p-plurisubharmonic functions on Riemannian manifolds.","The a priori estimates up to the second order derivatives of solutions are established.","The existence of a solution then follows by the continuity method."],"url":"http://arxiv.org/abs/2405.11925v2","category":"math.AP"}
{"created":"2024-05-20 09:58:03","title":"MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections","abstract":"3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis. However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints. To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting. The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space. We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane. All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects. Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results. Project page: https://mirror-gaussian.github.io/.","sentences":["3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis.","However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints.","To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting.","The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space.","We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane.","All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework.","MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects.","Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results.","Project page: https://mirror-gaussian.github.io/."],"url":"http://arxiv.org/abs/2405.11921v1","category":"cs.CV"}
{"created":"2024-05-20 09:42:44","title":"Ensemble and Mixture-of-Experts DeepONets For Operator Learning","abstract":"We present a novel deep operator network (DeepONet) architecture for operator learning, the ensemble DeepONet, that allows for enriching the trunk network of a single DeepONet with multiple distinct trunk networks. This trunk enrichment allows for greater expressivity and generalization capabilities over a range of operator learning problems. We also present a spatial mixture-of-experts (MoE) DeepONet trunk network architecture that utilizes a partition-of-unity (PoU) approximation to promote spatial locality and model sparsity in the operator learning problem. We first prove that both the ensemble and PoU-MoE DeepONets are universal approximators. We then demonstrate that ensemble DeepONets containing a trunk ensemble of a standard trunk, the PoU-MoE trunk, and/or a proper orthogonal decomposition (POD) trunk can achieve 2-4x lower relative $\\ell_2$ errors than standard DeepONets and POD-DeepONets on both standard and challenging new operator learning problems involving partial differential equations (PDEs) in two and three dimensions. Our new PoU-MoE formulation provides a natural way to incorporate spatial locality and model sparsity into any neural network architecture, while our new ensemble DeepONet provides a powerful and general framework for incorporating basis enrichment in scientific machine learning architectures for operator learning.","sentences":["We present a novel deep operator network (DeepONet) architecture for operator learning, the ensemble DeepONet, that allows for enriching the trunk network of a single DeepONet with multiple distinct trunk networks.","This trunk enrichment allows for greater expressivity and generalization capabilities over a range of operator learning problems.","We also present a spatial mixture-of-experts (MoE) DeepONet trunk network architecture that utilizes a partition-of-unity (PoU) approximation to promote spatial locality and model sparsity in the operator learning problem.","We first prove that both the ensemble and PoU-MoE DeepONets are universal approximators.","We then demonstrate that ensemble DeepONets containing a trunk ensemble of a standard trunk, the PoU-MoE trunk, and/or a proper orthogonal decomposition (POD) trunk can achieve 2-4x lower relative $\\ell_2$ errors than standard DeepONets and POD-DeepONets on both standard and challenging new operator learning problems involving partial differential equations (PDEs) in two and three dimensions.","Our new PoU-MoE formulation provides a natural way to incorporate spatial locality and model sparsity into any neural network architecture, while our new ensemble DeepONet provides a powerful and general framework for incorporating basis enrichment in scientific machine learning architectures for operator learning."],"url":"http://arxiv.org/abs/2405.11907v2","category":"cs.LG"}
{"created":"2024-05-20 09:40:06","title":"Pole structure of $P_\u03c8^N(4312)^+$ via machine learning and uniformized S-matrix","abstract":"We probed the pole structure of the $P_\\psi^{N}(4312)^{+}$ using a trained deep neural network. The training dataset was generated using uniformized independent S-matrix poles to ensure that the obtained interpretation is as model-independent as possible. To prevent possible ambiguity in the interpretation of the pole structure, we included the contribution from the off-diagonal element of the S-matrix. Five out of the six neural networks we trained favor $P_\\psi^{N}(4312)^{+}$ as possibly having a three-pole structure, with one pole on each of the unphysical sheets - a first in its report. The two poles can be associated to a pole-shadow pair which is a characteristic of a true resonance. On the other hand, the last pole is most likely associated with the coupled-channel effect. The combined effect of these poles produced a peak below the $\\Sigma^{+}_C\\bar{D}^0$ which mimic the line shape of a hadronic molecule.","sentences":["We probed the pole structure of the $P_\\psi^{N}(4312)^{+}$ using a trained deep neural network.","The training dataset was generated using uniformized independent S-matrix poles to ensure that the obtained interpretation is as model-independent as possible.","To prevent possible ambiguity in the interpretation of the pole structure, we included the contribution from the off-diagonal element of the S-matrix.","Five out of the six neural networks we trained favor $P_\\psi^{N}(4312)^{+}$ as possibly having a three-pole structure, with one pole on each of the unphysical sheets - a first in its report.","The two poles can be associated to a pole-shadow pair which is a characteristic of a true resonance.","On the other hand, the last pole is most likely associated with the coupled-channel effect.","The combined effect of these poles produced a peak below the $\\Sigma^{+}_C\\bar{D}^0$ which mimic the line shape of a hadronic molecule."],"url":"http://arxiv.org/abs/2405.11906v1","category":"hep-ph"}
{"created":"2024-05-20 09:33:27","title":"A comprehensive overview of deep learning techniques for 3D point cloud classification and semantic segmentation","abstract":"Point cloud analysis has a wide range of applications in many areas such as computer vision, robotic manipulation, and autonomous driving. While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unordered, irregular and noisy 3D points. To stimulate future research, this paper analyzes recent progress in deep learning methods employed for point cloud processing and presents challenges and potential directions to advance this field. It serves as a comprehensive review on two major tasks in 3D point cloud processing-- namely, 3D shape classification and semantic segmentation.","sentences":["Point cloud analysis has a wide range of applications in many areas such as computer vision, robotic manipulation, and autonomous driving.","While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unordered, irregular and noisy 3D points.","To stimulate future research, this paper analyzes recent progress in deep learning methods employed for point cloud processing and presents challenges and potential directions to advance this field.","It serves as a comprehensive review on two major tasks in 3D point cloud processing-- namely, 3D shape classification and semantic segmentation."],"url":"http://arxiv.org/abs/2405.11903v1","category":"cs.CV"}
{"created":"2024-05-21 17:54:06","title":"BiomedParse: a biomedical foundation model for image parsing of everything everywhere all at once","abstract":"Biomedical image analysis is fundamental for biomedical discovery in cell biology, pathology, radiology, and many other biomedical domains. Holistic image analysis comprises interdependent subtasks such as segmentation, detection, and recognition of relevant objects. Here, we propose BiomedParse, a biomedical foundation model for imaging parsing that can jointly conduct segmentation, detection, and recognition for 82 object types across 9 imaging modalities. Through joint learning, we can improve accuracy for individual tasks and enable novel applications such as segmenting all relevant objects in an image through a text prompt, rather than requiring users to laboriously specify the bounding box for each object. We leveraged readily available natural-language labels or descriptions accompanying those datasets and use GPT-4 to harmonize the noisy, unstructured text information with established biomedical object ontologies. We created a large dataset comprising over six million triples of image, segmentation mask, and textual description. On image segmentation, we showed that BiomedParse is broadly applicable, outperforming state-of-the-art methods on 102,855 test image-mask-label triples across 9 imaging modalities (everything). On object detection, which aims to locate a specific object of interest, BiomedParse again attained state-of-the-art performance, especially on objects with irregular shapes (everywhere). On object recognition, which aims to identify all objects in a given image along with their semantic types, we showed that BiomedParse can simultaneously segment and label all biomedical objects in an image (all at once). In summary, BiomedParse is an all-in-one tool for biomedical image analysis by jointly solving segmentation, detection, and recognition for all major biomedical image modalities, paving the path for efficient and accurate image-based biomedical discovery.","sentences":["Biomedical image analysis is fundamental for biomedical discovery in cell biology, pathology, radiology, and many other biomedical domains.","Holistic image analysis comprises interdependent subtasks such as segmentation, detection, and recognition of relevant objects.","Here, we propose BiomedParse, a biomedical foundation model for imaging parsing that can jointly conduct segmentation, detection, and recognition for 82 object types across 9 imaging modalities.","Through joint learning, we can improve accuracy for individual tasks and enable novel applications such as segmenting all relevant objects in an image through a text prompt, rather than requiring users to laboriously specify the bounding box for each object.","We leveraged readily available natural-language labels or descriptions accompanying those datasets and use GPT-4 to harmonize the noisy, unstructured text information with established biomedical object ontologies.","We created a large dataset comprising over six million triples of image, segmentation mask, and textual description.","On image segmentation, we showed that BiomedParse is broadly applicable, outperforming state-of-the-art methods on 102,855 test image-mask-label triples across 9 imaging modalities (everything).","On object detection, which aims to locate a specific object of interest, BiomedParse again attained state-of-the-art performance, especially on objects with irregular shapes (everywhere).","On object recognition, which aims to identify all objects in a given image along with their semantic types, we showed that BiomedParse can simultaneously segment and label all biomedical objects in an image (all at once).","In summary, BiomedParse is an all-in-one tool for biomedical image analysis by jointly solving segmentation, detection, and recognition for all major biomedical image modalities, paving the path for efficient and accurate image-based biomedical discovery."],"url":"http://arxiv.org/abs/2405.12971v1","category":"cs.CV"}
{"created":"2024-05-21 17:44:48","title":"Comprehensive Multimodal Deep Learning Survival Prediction Enabled by a Transformer Architecture: A Multicenter Study in Glioblastoma","abstract":"Background: This research aims to improve glioblastoma survival prediction by integrating MR images, clinical and molecular-pathologic data in a transformer-based deep learning model, addressing data heterogeneity and performance generalizability. Method: We propose and evaluate a transformer-based non-linear and non-proportional survival prediction model. The model employs self-supervised learning techniques to effectively encode the high-dimensional MRI input for integration with non-imaging data using cross-attention. To demonstrate model generalizability, the model is assessed with the time-dependent concordance index (Cdt) in two training setups using three independent public test sets: UPenn-GBM, UCSF-PDGM, and RHUH-GBM, each comprising 378, 366, and 36 cases, respectively. Results: The proposed transformer model achieved promising performance for imaging as well as non-imaging data, effectively integrating both modalities for enhanced performance (UPenn-GBM test-set, imaging Cdt 0.645, multimodal Cdt 0.707) while outperforming state-of-the-art late-fusion 3D-CNN-based models. Consistent performance was observed across the three independent multicenter test sets with Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM, first external test set) and 0.618 (RHUH-GBM, second external test set). The model achieved significant discrimination between patients with favorable and unfavorable survival for all three datasets (logrank p 1.9\\times{10}^{-8}, 9.7\\times{10}^{-3}, and 1.2\\times{10}^{-2}). Conclusions: The proposed transformer-based survival prediction model integrates complementary information from diverse input modalities, contributing to improved glioblastoma survival prediction compared to state-of-the-art methods. Consistent performance was observed across institutions supporting model generalizability.","sentences":["Background:","This research aims to improve glioblastoma survival prediction by integrating MR images, clinical and molecular-pathologic data in a transformer-based deep learning model, addressing data heterogeneity and performance generalizability.","Method: We propose and evaluate a transformer-based non-linear and non-proportional survival prediction model.","The model employs self-supervised learning techniques to effectively encode the high-dimensional MRI input for integration with non-imaging data using cross-attention.","To demonstrate model generalizability, the model is assessed with the time-dependent concordance index (Cdt) in two training setups using three independent public test sets: UPenn-GBM, UCSF-PDGM, and RHUH-GBM, each comprising 378, 366, and 36 cases, respectively.","Results:","The proposed transformer model achieved promising performance for imaging as well as non-imaging data, effectively integrating both modalities for enhanced performance (UPenn-GBM test-set, imaging Cdt 0.645, multimodal Cdt 0.707) while outperforming state-of-the-art late-fusion 3D-CNN-based models.","Consistent performance was observed across the three independent multicenter test sets with Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM, first external test set) and 0.618 (RHUH-GBM, second external test set).","The model achieved significant discrimination between patients with favorable and unfavorable survival for all three datasets (logrank p 1.9\\times{10}^{-8}, 9.7\\times{10}^{-3}, and 1.2\\times{10}^{-2}).","Conclusions: The proposed transformer-based survival prediction model integrates complementary information from diverse input modalities, contributing to improved glioblastoma survival prediction compared to state-of-the-art methods.","Consistent performance was observed across institutions supporting model generalizability."],"url":"http://arxiv.org/abs/2405.12963v1","category":"eess.IV"}
{"created":"2024-05-21 17:31:10","title":"Online Learning of Halfspaces with Massart Noise","abstract":"We study the task of online learning in the presence of Massart noise. Instead of assuming that the online adversary chooses an arbitrary sequence of labels, we assume that the context $\\mathbf{x}$ is selected adversarially but the label $y$ presented to the learner disagrees with the ground-truth label of $\\mathbf{x}$ with unknown probability at most $\\eta$. We study the fundamental class of $\\gamma$-margin linear classifiers and present a computationally efficient algorithm that achieves mistake bound $\\eta T + o(T)$. Our mistake bound is qualitatively tight for efficient algorithms: it is known that even in the offline setting achieving classification error better than $\\eta$ requires super-polynomial time in the SQ model.   We extend our online learning model to a $k$-arm contextual bandit setting where the rewards -- instead of satisfying commonly used realizability assumptions -- are consistent (in expectation) with some linear ranking function with weight vector $\\mathbf{w}^\\ast$. Given a list of contexts $\\mathbf{x}_1,\\ldots \\mathbf{x}_k$, if $\\mathbf{w}^*\\cdot \\mathbf{x}_i > \\mathbf{w}^* \\cdot \\mathbf{x}_j$, the expected reward of action $i$ must be larger than that of $j$ by at least $\\Delta$. We use our Massart online learner to design an efficient bandit algorithm that obtains expected reward at least $(1-1/k)~ \\Delta T - o(T)$ bigger than choosing a random action at every round.","sentences":["We study the task of online learning in the presence of Massart noise.","Instead of assuming that the online adversary chooses an arbitrary sequence of labels, we assume that the context $\\mathbf{x}$ is selected adversarially but the label $y$ presented to the learner disagrees with the ground-truth label of $\\mathbf{x}$ with unknown probability at most $\\eta$. We study the fundamental class of $\\gamma$-margin linear classifiers and present a computationally efficient algorithm that achieves mistake bound $\\eta T + o(T)$. Our mistake bound is qualitatively tight for efficient algorithms: it is known that even in the offline setting achieving classification error better than $\\eta$ requires super-polynomial time in the SQ model.   ","We extend our online learning model to a $k$-arm contextual bandit setting where the rewards -- instead of satisfying commonly used realizability assumptions -- are consistent (in expectation) with some linear ranking function with weight vector $\\mathbf{w}^\\ast$. Given a list of contexts $\\mathbf{x}_1,\\ldots \\mathbf{x}_k$, if $\\mathbf{w}^*\\cdot \\mathbf{x}_i > \\mathbf{w}^*","\\cdot \\mathbf{x}_j$, the expected reward of action $i$ must be larger than that of $j$ by at least $\\Delta$. We use our Massart online learner to design an efficient bandit algorithm that obtains expected reward at least $(1-1/k)~ \\Delta T - o(T)$ bigger than choosing a random action at every round."],"url":"http://arxiv.org/abs/2405.12958v1","category":"cs.LG"}
{"created":"2024-05-21 17:17:34","title":"Tutorly: Turning Programming Videos Into Apprenticeship Learning Environments with LLMs","abstract":"Online programming videos, including tutorials and streamcasts, are widely popular and contain a wealth of expert knowledge. However, effectively utilizing these resources to achieve targeted learning goals can be challenging. Unlike direct tutoring, video content lacks tailored guidance based on individual learning paces, personalized feedback, and interactive engagement necessary for support and monitoring. Our work transforms programming videos into one-on-one tutoring experiences using the cognitive apprenticeship framework. Tutorly, developed as a JupyterLab Plugin, allows learners to (1) set personalized learning goals, (2) engage in learning-by-doing through a conversational LLM-based mentor agent, (3) receive guidance and feedback based on a student model that steers the mentor moves. In a within-subject study with 16 participants learning exploratory data analysis from a streamcast, Tutorly significantly improved their performance from 61.9% to 76.6% based on a post-test questionnaire. Tutorly demonstrates the potential for enhancing programming video learning experiences with LLM and learner modeling.","sentences":["Online programming videos, including tutorials and streamcasts, are widely popular and contain a wealth of expert knowledge.","However, effectively utilizing these resources to achieve targeted learning goals can be challenging.","Unlike direct tutoring, video content lacks tailored guidance based on individual learning paces, personalized feedback, and interactive engagement necessary for support and monitoring.","Our work transforms programming videos into one-on-one tutoring experiences using the cognitive apprenticeship framework.","Tutorly, developed as a JupyterLab Plugin, allows learners to (1) set personalized learning goals, (2) engage in learning-by-doing through a conversational LLM-based mentor agent, (3) receive guidance and feedback based on a student model that steers the mentor moves.","In a within-subject study with 16 participants learning exploratory data analysis from a streamcast, Tutorly significantly improved their performance from 61.9% to 76.6% based on a post-test questionnaire.","Tutorly demonstrates the potential for enhancing programming video learning experiences with LLM and learner modeling."],"url":"http://arxiv.org/abs/2405.12946v1","category":"cs.HC"}
{"created":"2024-05-21 16:02:06","title":"Retrievable Domain-Sensitive Feature Memory for Multi-Domain Recommendation","abstract":"With the increase in the business scale and number of domains in online advertising, multi-domain ad recommendation has become a mainstream solution in the industry. The core of multi-domain recommendation is effectively modeling the commonalities and distinctions among domains. Existing works are dedicated to designing model architectures for implicit multi-domain modeling while overlooking an in-depth investigation from a more fundamental perspective of feature distributions. This paper focuses on features with significant differences across various domains in both distributions and effects on model predictions. We refer to these features as domain-sensitive features, which serve as carriers of domain distinctions and are crucial for multi-domain modeling. Experiments demonstrate that existing multi-domain modeling methods may neglect domain-sensitive features, indicating insufficient learning of domain distinctions. To avoid this neglect, we propose a domain-sensitive feature attribution method to identify features that best reflect domain distinctions from the feature set. Further, we design a memory architecture that extracts domain-specific information from domain-sensitive features for the model to retrieve and integrate, thereby enhancing the awareness of domain distinctions. Extensive offline and online experiments demonstrate the superiority of our method in capturing domain distinctions and improving multi-domain recommendation performance.","sentences":["With the increase in the business scale and number of domains in online advertising, multi-domain ad recommendation has become a mainstream solution in the industry.","The core of multi-domain recommendation is effectively modeling the commonalities and distinctions among domains.","Existing works are dedicated to designing model architectures for implicit multi-domain modeling while overlooking an in-depth investigation from a more fundamental perspective of feature distributions.","This paper focuses on features with significant differences across various domains in both distributions and effects on model predictions.","We refer to these features as domain-sensitive features, which serve as carriers of domain distinctions and are crucial for multi-domain modeling.","Experiments demonstrate that existing multi-domain modeling methods may neglect domain-sensitive features, indicating insufficient learning of domain distinctions.","To avoid this neglect, we propose a domain-sensitive feature attribution method to identify features that best reflect domain distinctions from the feature set.","Further, we design a memory architecture that extracts domain-specific information from domain-sensitive features for the model to retrieve and integrate, thereby enhancing the awareness of domain distinctions.","Extensive offline and online experiments demonstrate the superiority of our method in capturing domain distinctions and improving multi-domain recommendation performance."],"url":"http://arxiv.org/abs/2405.12892v1","category":"cs.IR"}
{"created":"2024-05-21 14:37:53","title":"Effect of Synthetic Jets Actuator Parameters on Deep Reinforcement Learning-Based Flow Control Performance in a Square Cylinder","abstract":"We utilize deep reinforcement learning (DRL) algorithms to precisely control the mass flow rates of synthetic jets located on the upper and lower surfaces of a square cylinder, achieving effective active flow control. Through DRL-based active flow control (AFC) technology, we significantly reduce the lift and drag coefficients of the square cylinder at Re=100 and Re=500, while completely suppressing vortex shedding in the wake flow field.Additionally, we conduct a sensitivity analysis of the position and width parameters of the synthetic jets regarding flow control performance. Our observations indicate that positioning the synthetic jets near the trailing edge corners of the square cylinder, as opposed to the leading edge corners, completely suppresses vortex shedding, resulting in more stable controlled flow coefficients, C_D and C_L. When the synthetic jets are positioned at the trailing edge corners, flow control reduces the mean drag coefficient by 14.4% and the standard deviation of the lift coefficient by 86.1% for the baseline flow at Re=100. For the baseline flow at Re=500, flow control reduces the mean drag coefficient by 51.4% and the standard deviation of the lift coefficient by 90.5%. At both Reynolds numbers, vortex shedding in the wake flow field is completely suppressed. Furthermore, using narrower synthetic jets results in a lower reduction rate of the standard deviations of C_D and C_L, while increasing the mean and standard deviation of the mass flow rate of the jets used for flow control. This study provides guidance on optimizing the width and position of synthetic jets for DRL-based active flow control.","sentences":["We utilize deep reinforcement learning (DRL) algorithms to precisely control the mass flow rates of synthetic jets located on the upper and lower surfaces of a square cylinder, achieving effective active flow control.","Through DRL-based active flow control (AFC) technology, we significantly reduce the lift and drag coefficients of the square cylinder at Re=100 and Re=500, while completely suppressing vortex shedding in the wake flow field.","Additionally, we conduct a sensitivity analysis of the position and width parameters of the synthetic jets regarding flow control performance.","Our observations indicate that positioning the synthetic jets near the trailing edge corners of the square cylinder, as opposed to the leading edge corners, completely suppresses vortex shedding, resulting in more stable controlled flow coefficients, C_D and C_L. When the synthetic jets are positioned at the trailing edge corners, flow control reduces the mean drag coefficient by 14.4% and the standard deviation of the lift coefficient by 86.1% for the baseline flow at Re=100.","For the baseline flow at Re=500, flow control reduces the mean drag coefficient by 51.4% and the standard deviation of the lift coefficient by 90.5%.","At both Reynolds numbers, vortex shedding in the wake flow field is completely suppressed.","Furthermore, using narrower synthetic jets results in a lower reduction rate of the standard deviations of C_D and C_L, while increasing the mean and standard deviation of the mass flow rate of the jets used for flow control.","This study provides guidance on optimizing the width and position of synthetic jets for DRL-based active flow control."],"url":"http://arxiv.org/abs/2405.12834v1","category":"physics.flu-dyn"}
{"created":"2024-05-21 13:09:04","title":"BIMM: Brain Inspired Masked Modeling for Video Representation Learning","abstract":"The visual pathway of human brain includes two sub-pathways, ie, the ventral pathway and the dorsal pathway, which focus on object identification and dynamic information modeling, respectively. Both pathways comprise multi-layer structures, with each layer responsible for processing different aspects of visual information. Inspired by visual information processing mechanism of the human brain, we propose the Brain Inspired Masked Modeling (BIMM) framework, aiming to learn comprehensive representations from videos. Specifically, our approach consists of ventral and dorsal branches, which learn image and video representations, respectively. Both branches employ the Vision Transformer (ViT) as their backbone and are trained using masked modeling method. To achieve the goals of different visual cortices in the brain, we segment the encoder of each branch into three intermediate blocks and reconstruct progressive prediction targets with light weight decoders. Furthermore, drawing inspiration from the information-sharing mechanism in the visual pathways, we propose a partial parameter sharing strategy between the branches during training. Extensive experiments demonstrate that BIMM achieves superior performance compared to the state-of-the-art methods.","sentences":["The visual pathway of human brain includes two sub-pathways, ie, the ventral pathway and the dorsal pathway, which focus on object identification and dynamic information modeling, respectively.","Both pathways comprise multi-layer structures, with each layer responsible for processing different aspects of visual information.","Inspired by visual information processing mechanism of the human brain, we propose the Brain Inspired Masked Modeling (BIMM) framework, aiming to learn comprehensive representations from videos.","Specifically, our approach consists of ventral and dorsal branches, which learn image and video representations, respectively.","Both branches employ the Vision Transformer (ViT) as their backbone and are trained using masked modeling method.","To achieve the goals of different visual cortices in the brain, we segment the encoder of each branch into three intermediate blocks and reconstruct progressive prediction targets with light weight decoders.","Furthermore, drawing inspiration from the information-sharing mechanism in the visual pathways, we propose a partial parameter sharing strategy between the branches during training.","Extensive experiments demonstrate that BIMM achieves superior performance compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.12757v1","category":"cs.CV"}
{"created":"2024-05-21 13:06:55","title":"Parallel Algorithm for Optimal Threshold Labeling of Ordinal Regression Methods","abstract":"Ordinal regression (OR) is classification of ordinal data in which the underlying categorical target variable has a natural ordinal relation for the underlying explanatory variable. For $K$-class OR tasks, threshold methods learn a one-dimensional transformation (1DT) of the explanatory variable so that 1DT values for observations of the explanatory variable preserve the order of label values $1,\\ldots,K$ for corresponding observations of the target variable well, and then assign a label prediction to the learned 1DT through threshold labeling, namely, according to the rank of an interval to which the 1DT belongs among intervals on the real line separated by $(K-1)$ threshold parameters. In this study, we propose a parallelizable algorithm to find the optimal threshold labeling, which was developed in previous research, and derive sufficient conditions for that algorithm to successfully output the optimal threshold labeling. In a numerical experiment we performed, the computation time taken for the whole learning process of a threshold method with the optimal threshold labeling could be reduced to approximately 60\\,\\% by using the proposed algorithm with parallel processing compared to using an existing algorithm based on dynamic programming.","sentences":["Ordinal regression (OR) is classification of ordinal data in which the underlying categorical target variable has a natural ordinal relation for the underlying explanatory variable.","For $K$-class OR tasks, threshold methods learn a one-dimensional transformation (1DT) of the explanatory variable so that 1DT values for observations of the explanatory variable preserve the order of label values $1,\\ldots,K$ for corresponding observations of the target variable well, and then assign a label prediction to the learned 1DT through threshold labeling, namely, according to the rank of an interval to which the 1DT belongs among intervals on the real line separated by $(K-1)$ threshold parameters.","In this study, we propose a parallelizable algorithm to find the optimal threshold labeling, which was developed in previous research, and derive sufficient conditions for that algorithm to successfully output the optimal threshold labeling.","In a numerical experiment we performed, the computation time taken for the whole learning process of a threshold method with the optimal threshold labeling could be reduced to approximately 60\\,\\% by using the proposed algorithm with parallel processing compared to using an existing algorithm based on dynamic programming."],"url":"http://arxiv.org/abs/2405.12756v1","category":"cs.LG"}
{"created":"2024-05-21 13:03:06","title":"A Stealthy Backdoor Attack for Without-Label-Sharing Split Learning","abstract":"As a novel privacy-preserving paradigm aimed at reducing client computational costs and achieving data utility, split learning has garnered extensive attention and proliferated widespread applications across various fields, including smart health and smart transportation, among others. While recent studies have primarily concentrated on addressing privacy leakage concerns in split learning, such as inference attacks and data reconstruction, the exploration of security issues (e.g., backdoor attacks) within the framework of split learning has been comparatively limited. Nonetheless, the security vulnerability within the context of split learning is highly posing a threat and can give rise to grave security implications, such as the illegal impersonation in the face recognition model. Therefore, in this paper, we propose a stealthy backdoor attack strategy (namely SBAT) tailored to the without-label-sharing split learning architecture, which unveils the inherent security vulnerability of split learning. We posit the existence of a potential attacker on the server side aiming to introduce a backdoor into the training model, while exploring two scenarios: one with known client network architecture and the other with unknown architecture. Diverging from traditional backdoor attack methods that manipulate the training data and labels, we constructively conduct the backdoor attack by injecting the trigger embedding into the server network. Specifically, our SBAT achieves a higher level of attack stealthiness by refraining from modifying any intermediate parameters (e.g., gradients) during training and instead executing all malicious operations post-training.","sentences":["As a novel privacy-preserving paradigm aimed at reducing client computational costs and achieving data utility, split learning has garnered extensive attention and proliferated widespread applications across various fields, including smart health and smart transportation, among others.","While recent studies have primarily concentrated on addressing privacy leakage concerns in split learning, such as inference attacks and data reconstruction, the exploration of security issues (e.g., backdoor attacks) within the framework of split learning has been comparatively limited.","Nonetheless, the security vulnerability within the context of split learning is highly posing a threat and can give rise to grave security implications, such as the illegal impersonation in the face recognition model.","Therefore, in this paper, we propose a stealthy backdoor attack strategy (namely SBAT) tailored to the without-label-sharing split learning architecture, which unveils the inherent security vulnerability of split learning.","We posit the existence of a potential attacker on the server side aiming to introduce a backdoor into the training model, while exploring two scenarios: one with known client network architecture and the other with unknown architecture.","Diverging from traditional backdoor attack methods that manipulate the training data and labels, we constructively conduct the backdoor attack by injecting the trigger embedding into the server network.","Specifically, our SBAT achieves a higher level of attack stealthiness by refraining from modifying any intermediate parameters (e.g., gradients) during training and instead executing all malicious operations post-training."],"url":"http://arxiv.org/abs/2405.12751v1","category":"cs.CR"}
{"created":"2024-05-21 11:54:16","title":"Disentangled Representation with Cross Experts Covariance Loss for Multi-Domain Recommendation","abstract":"Multi-domain learning (MDL) has emerged as a prominent research area aimed at enhancing the quality of personalized services. The key challenge in MDL lies in striking a balance between learning commonalities across domains while preserving the distinct characteristics of each domain. However, this gives rise to a challenging dilemma. On one hand, a model needs to leverage domain-specific modules, such as experts or embeddings, to preserve the uniqueness of each domain. On the other hand, due to the long-tailed distributions observed in real-world domains, some tail domains may lack sufficient samples to fully learn their corresponding modules. Unfortunately, existing approaches have not adequately addressed this dilemma. To address this issue, we propose a novel model called Crocodile, which stands for Cross-experts Covariance Loss for Disentangled Learning. Crocodile adopts a multi-embedding paradigm to facilitate model learning and employs a Covariance Loss on these embeddings to disentangle them. This disentanglement enables the model to capture diverse user interests across domains effectively. Additionally, we introduce a novel gating mechanism to further enhance the capabilities of Crocodile. Through empirical analysis, we demonstrate that our proposed method successfully resolves these two challenges and outperforms all state-of-the-art methods on publicly available datasets. We firmly believe that the analytical perspectives and design concept of disentanglement presented in our work can pave the way for future research in the field of MDL.","sentences":["Multi-domain learning (MDL) has emerged as a prominent research area aimed at enhancing the quality of personalized services.","The key challenge in MDL lies in striking a balance between learning commonalities across domains while preserving the distinct characteristics of each domain.","However, this gives rise to a challenging dilemma.","On one hand, a model needs to leverage domain-specific modules, such as experts or embeddings, to preserve the uniqueness of each domain.","On the other hand, due to the long-tailed distributions observed in real-world domains, some tail domains may lack sufficient samples to fully learn their corresponding modules.","Unfortunately, existing approaches have not adequately addressed this dilemma.","To address this issue, we propose a novel model called Crocodile, which stands for Cross-experts Covariance Loss for Disentangled Learning.","Crocodile adopts a multi-embedding paradigm to facilitate model learning and employs a Covariance Loss on these embeddings to disentangle them.","This disentanglement enables the model to capture diverse user interests across domains effectively.","Additionally, we introduce a novel gating mechanism to further enhance the capabilities of Crocodile.","Through empirical analysis, we demonstrate that our proposed method successfully resolves these two challenges and outperforms all state-of-the-art methods on publicly available datasets.","We firmly believe that the analytical perspectives and design concept of disentanglement presented in our work can pave the way for future research in the field of MDL."],"url":"http://arxiv.org/abs/2405.12706v1","category":"cs.IR"}
{"created":"2024-05-21 11:10:00","title":"Experimental investigation of trans-scale displacement responses of wrinkle defects in fiber reinforced composite laminates","abstract":"Wrinkle defects were found widely exist in the field of industrial products, i.e. wind turbine blades and filament-wound composite pressure vessels. The magnitude of wrinkle wavelength varies from several millimeters to over one hundred millimeters. Locating the wrinkle defects and measuring their responses are very important to the assessment of the structures that containing wrinkle defects. A meso-mechanical modeling is presented based on the homogenization method to obtain the effective stiffness of a graded wrinkle. The finite element simulation predicts the trans-scale response of out-of-plane displacement of wrinkled laminates, where the maximum displacement ranges from nanoscale to millimeter scale. Such trans-scale effect requires different measurement approaches to observe the displacement responses. Here we employed Shearography (Speckle Pattern Shearing Interferometry) and fringe projection profilometry (FPP) method respectively according to the different magnitude of displacement. In FPP method, a displacement extraction algorithm was presented to obtain the out-of-plane displacement. The measurement sensitivity and accuracy of Shearography and FPP are compared, which provides a quantitative reference for industrial non-destructive test.","sentences":["Wrinkle defects were found widely exist in the field of industrial products, i.e. wind turbine blades and filament-wound composite pressure vessels.","The magnitude of wrinkle wavelength varies from several millimeters to over one hundred millimeters.","Locating the wrinkle defects and measuring their responses are very important to the assessment of the structures that containing wrinkle defects.","A meso-mechanical modeling is presented based on the homogenization method to obtain the effective stiffness of a graded wrinkle.","The finite element simulation predicts the trans-scale response of out-of-plane displacement of wrinkled laminates, where the maximum displacement ranges from nanoscale to millimeter scale.","Such trans-scale effect requires different measurement approaches to observe the displacement responses.","Here we employed Shearography (Speckle Pattern Shearing Interferometry) and fringe projection profilometry (FPP) method respectively according to the different magnitude of displacement.","In FPP method, a displacement extraction algorithm was presented to obtain the out-of-plane displacement.","The measurement sensitivity and accuracy of Shearography and FPP are compared, which provides a quantitative reference for industrial non-destructive test."],"url":"http://arxiv.org/abs/2405.12676v1","category":"cs.CV"}
{"created":"2024-05-21 09:33:49","title":"Efficient Learned Wavelet Image and Video Coding","abstract":"Learned wavelet image and video coding approaches provide an explainable framework with a latent space corresponding to a wavelet decomposition. The wavelet image coder iWave++ achieves state-of-the-art performance and has been employed for various compression tasks, including lossy as well as lossless image, video, and medical data compression. However, the approaches suffer from slow decoding speed due to the autoregressive context model used in iWave++. In this paper, we show how a parallelized context model can be integrated into the iWave++ framework. Our experimental results demonstrate a speedup factor of over 350 and 240 for image and video compression, respectively. At the same time, the rate-distortion performance in terms of Bj{\\o}ntegaard delta bitrate is slightly worse by 1.5\\% for image coding and 1\\% for video coding. In addition, we analyze the learned wavelet decomposition by visualizing its subband impulse responses.","sentences":["Learned wavelet image and video coding approaches provide an explainable framework with a latent space corresponding to a wavelet decomposition.","The wavelet image coder iWave++ achieves state-of-the-art performance and has been employed for various compression tasks, including lossy as well as lossless image, video, and medical data compression.","However, the approaches suffer from slow decoding speed due to the autoregressive context model used in iWave++.","In this paper, we show how a parallelized context model can be integrated into the iWave++ framework.","Our experimental results demonstrate a speedup factor of over 350 and 240 for image and video compression, respectively.","At the same time, the rate-distortion performance in terms of Bj{\\o}ntegaard delta bitrate is slightly worse by 1.5\\% for image coding and 1\\% for video coding.","In addition, we analyze the learned wavelet decomposition by visualizing its subband impulse responses."],"url":"http://arxiv.org/abs/2405.12631v1","category":"eess.IV"}
{"created":"2024-05-21 08:11:42","title":"TypeII-CsiNet: CSI Feedback with TypeII Codebook","abstract":"The latest TypeII codebook selects partial strongest angular-delay ports for the feedback of downlink channel state information (CSI), whereas its performance is limited due to the deficiency of utilizing the correlations among the port coefficients. To tackle this issue, we propose a tailored autoencoder named TypeII-CsiNet to effectively integrate the TypeII codebook with deep learning, wherein three novel designs are developed for sufficiently boosting the sum rate performance. Firstly, a dedicated pre-processing module is designed to sort the selected ports for reserving the correlations of their corresponding coefficients. Secondly, a position-filling layer is developed in the decoder to fill the feedback coefficients into their ports in the recovered CSI matrix, so that the corresponding angular-delay-domain structure is adequately leveraged to enhance the reconstruction accuracy. Thirdly, a two-stage loss function is proposed to improve the sum rate performance while avoiding the trapping in local optimums during model training. Simulation results verify that our proposed TypeII-CsiNet outperforms the TypeII codebook and existing deep learning benchmarks.","sentences":["The latest TypeII codebook selects partial strongest angular-delay ports for the feedback of downlink channel state information (CSI), whereas its performance is limited due to the deficiency of utilizing the correlations among the port coefficients.","To tackle this issue, we propose a tailored autoencoder named TypeII-CsiNet to effectively integrate the TypeII codebook with deep learning, wherein three novel designs are developed for sufficiently boosting the sum rate performance.","Firstly, a dedicated pre-processing module is designed to sort the selected ports for reserving the correlations of their corresponding coefficients.","Secondly, a position-filling layer is developed in the decoder to fill the feedback coefficients into their ports in the recovered CSI matrix, so that the corresponding angular-delay-domain structure is adequately leveraged to enhance the reconstruction accuracy.","Thirdly, a two-stage loss function is proposed to improve the sum rate performance while avoiding the trapping in local optimums during model training.","Simulation results verify that our proposed TypeII-CsiNet outperforms the TypeII codebook and existing deep learning benchmarks."],"url":"http://arxiv.org/abs/2405.12569v1","category":"eess.SP"}
{"created":"2024-05-21 04:21:35","title":"Customize Your Own Paired Data via Few-shot Way","abstract":"Existing solutions to image editing tasks suffer from several issues. Though achieving remarkably satisfying generated results, some supervised methods require huge amounts of paired training data, which greatly limits their usages. The other unsupervised methods take full advantage of large-scale pre-trained priors, thus being strictly restricted to the domains where the priors are trained on and behaving badly in out-of-distribution cases. The task we focus on is how to enable the users to customize their desired effects through only few image pairs. In our proposed framework, a novel few-shot learning mechanism based on the directional transformations among samples is introduced and expands the learnable space exponentially. Adopting a diffusion model pipeline, we redesign the condition calculating modules in our model and apply several technical improvements. Experimental results demonstrate the capabilities of our method in various cases.","sentences":["Existing solutions to image editing tasks suffer from several issues.","Though achieving remarkably satisfying generated results, some supervised methods require huge amounts of paired training data, which greatly limits their usages.","The other unsupervised methods take full advantage of large-scale pre-trained priors, thus being strictly restricted to the domains where the priors are trained on and behaving badly in out-of-distribution cases.","The task we focus on is how to enable the users to customize their desired effects through only few image pairs.","In our proposed framework, a novel few-shot learning mechanism based on the directional transformations among samples is introduced and expands the learnable space exponentially.","Adopting a diffusion model pipeline, we redesign the condition calculating modules in our model and apply several technical improvements.","Experimental results demonstrate the capabilities of our method in various cases."],"url":"http://arxiv.org/abs/2405.12490v1","category":"cs.CV"}
{"created":"2024-05-21 02:36:37","title":"Physics-based Scene Layout Generation from Human Motion","abstract":"Creating scenes for captured motions that achieve realistic human-scene interaction is crucial for 3D animation in movies or video games. As character motion is often captured in a blue-screened studio without real furniture or objects in place, there may be a discrepancy between the planned motion and the captured one. This gives rise to the need for automatic scene layout generation to relieve the burdens of selecting and positioning furniture and objects. Previous approaches cannot avoid artifacts like penetration and floating due to the lack of physical constraints. Furthermore, some heavily rely on specific data to learn the contact affordances, restricting the generalization ability to different motions. In this work, we present a physics-based approach that simultaneously optimizes a scene layout generator and simulates a moving human in a physics simulator. To attain plausible and realistic interaction motions, our method explicitly introduces physical constraints. To automatically recover and generate the scene layout, we minimize the motion tracking errors to identify the objects that can afford interaction. We use reinforcement learning to perform a dual-optimization of both the character motion imitation controller and the scene layout generator. To facilitate the optimization, we reshape the tracking rewards and devise pose prior guidance obtained from our estimated pseudo-contact labels. We evaluate our method using motions from SAMP and PROX, and demonstrate physically plausible scene layout reconstruction compared with the previous kinematics-based method.","sentences":["Creating scenes for captured motions that achieve realistic human-scene interaction is crucial for 3D animation in movies or video games.","As character motion is often captured in a blue-screened studio without real furniture or objects in place, there may be a discrepancy between the planned motion and the captured one.","This gives rise to the need for automatic scene layout generation to relieve the burdens of selecting and positioning furniture and objects.","Previous approaches cannot avoid artifacts like penetration and floating due to the lack of physical constraints.","Furthermore, some heavily rely on specific data to learn the contact affordances, restricting the generalization ability to different motions.","In this work, we present a physics-based approach that simultaneously optimizes a scene layout generator and simulates a moving human in a physics simulator.","To attain plausible and realistic interaction motions, our method explicitly introduces physical constraints.","To automatically recover and generate the scene layout, we minimize the motion tracking errors to identify the objects that can afford interaction.","We use reinforcement learning to perform a dual-optimization of both the character motion imitation controller and the scene layout generator.","To facilitate the optimization, we reshape the tracking rewards and devise pose prior guidance obtained from our estimated pseudo-contact labels.","We evaluate our method using motions from SAMP and PROX, and demonstrate physically plausible scene layout reconstruction compared with the previous kinematics-based method."],"url":"http://arxiv.org/abs/2405.12460v1","category":"cs.CV"}
{"created":"2024-05-21 02:16:16","title":"Mutual Information Analysis in Multimodal Learning Systems","abstract":"In recent years, there has been a significant increase in applications of multimodal signal processing and analysis, largely driven by the increased availability of multimodal datasets and the rapid progress in multimodal learning systems. Well-known examples include autonomous vehicles, audiovisual generative systems, vision-language systems, and so on. Such systems integrate multiple signal modalities: text, speech, images, video, LiDAR, etc., to perform various tasks. A key issue for understanding such systems is the relationship between various modalities and how it impacts task performance. In this paper, we employ the concept of mutual information (MI) to gain insight into this issue. Taking advantage of the recent progress in entropy modeling and estimation, we develop a system called InfoMeter to estimate MI between modalities in a multimodal learning system. We then apply InfoMeter to analyze a multimodal 3D object detection system over a large-scale dataset for autonomous driving. Our experiments on this system suggest that a lower MI between modalities is beneficial for detection accuracy. This new insight may facilitate improvements in the development of future multimodal learning systems.","sentences":["In recent years, there has been a significant increase in applications of multimodal signal processing and analysis, largely driven by the increased availability of multimodal datasets and the rapid progress in multimodal learning systems.","Well-known examples include autonomous vehicles, audiovisual generative systems, vision-language systems, and so on.","Such systems integrate multiple signal modalities: text, speech, images, video, LiDAR, etc., to perform various tasks.","A key issue for understanding such systems is the relationship between various modalities and how it impacts task performance.","In this paper, we employ the concept of mutual information (MI) to gain insight into this issue.","Taking advantage of the recent progress in entropy modeling and estimation, we develop a system called InfoMeter to estimate MI between modalities in a multimodal learning system.","We then apply InfoMeter to analyze a multimodal 3D object detection system over a large-scale dataset for autonomous driving.","Our experiments on this system suggest that a lower MI between modalities is beneficial for detection accuracy.","This new insight may facilitate improvements in the development of future multimodal learning systems."],"url":"http://arxiv.org/abs/2405.12456v1","category":"eess.IV"}
{"created":"2024-05-20 23:53:42","title":"GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised Point Cloud Learning in 3D","abstract":"We introduce a pioneering approach to self-supervised learning for point clouds, employing a geometrically informed mask selection strategy called GeoMask3D (GM3D) to boost the efficiency of Masked Auto Encoders (MAE). Unlike the conventional method of random masking, our technique utilizes a teacher-student model to focus on intricate areas within the data, guiding the model's focus toward regions with higher geometric complexity. This strategy is grounded in the hypothesis that concentrating on harder patches yields a more robust feature representation, as evidenced by the improved performance on downstream tasks. Our method also presents a complete-to-partial feature-level knowledge distillation technique designed to guide the prediction of geometric complexity utilizing a comprehensive context from feature-level information. Extensive experiments confirm our method's superiority over State-Of-The-Art (SOTA) baselines, demonstrating marked improvements in classification, and few-shot tasks.","sentences":["We introduce a pioneering approach to self-supervised learning for point clouds, employing a geometrically informed mask selection strategy called GeoMask3D (GM3D) to boost the efficiency of Masked Auto Encoders (MAE).","Unlike the conventional method of random masking, our technique utilizes a teacher-student model to focus on intricate areas within the data, guiding the model's focus toward regions with higher geometric complexity.","This strategy is grounded in the hypothesis that concentrating on harder patches yields a more robust feature representation, as evidenced by the improved performance on downstream tasks.","Our method also presents a complete-to-partial feature-level knowledge distillation technique designed to guide the prediction of geometric complexity utilizing a comprehensive context from feature-level information.","Extensive experiments confirm our method's superiority over State-Of-The-Art (SOTA) baselines, demonstrating marked improvements in classification, and few-shot tasks."],"url":"http://arxiv.org/abs/2405.12419v1","category":"cs.CV"}
{"created":"2024-05-20 23:50:55","title":"Learning models on rooted regular trees with majority update policy: convergence and phase transition","abstract":"We study a learning model in which an agent is stationed at each vertex of $\\mathbb{T}_{m}$, the rooted tree in which each vertex has $m$ children. At any time-step $t \\in \\mathbb{N}_{0}$, they are allowed to select one of two available technologies: $B$ and $R$. Let the technology chosen by the agent at vertex $v\\in\\mathbb{T}_{m}$, at time-step $t$, be $C_{t}(v)$. Let $\\{C_{0}(v):v\\in\\mathbb{T}_{m}\\}$ be i.i.d., where $C_{0}(v)=B$ with probability $\\pi_{0}$. During epoch $t$, the agent at $v$ performs an experiment that results in success with probability $p_{B}$ if $C_{t}(v)=B$, and with probability $p_{R}$ if $C_{t}(v)=R$. If the children of $v$ are $v_{1},\\ldots,v_{m}$, the agent at $v$ updates their technology to $C_{t+1}(v)=B$ if the number of successes among all $v_{i}$ with $C_{t}(v_{i})=B$ exceeds, strictly, the number of successes among all $v_{j}$ with $C_{t}(v_{j})=R$. If these numbers are equal, then the agent at $v$ sets $C_{t+1}(v)=B$ with probability $1/2$. Else, $C_{t+1}(v)=R$. We show that $\\{C_{t}(v):v\\in\\mathbb{T}_{m}\\}$ is i.i.d., where $C_{t}(v)=B$ with probability $\\pi_{t}$, and $\\{\\pi_{t}\\}_{t \\in \\mathbb{N}_{0}}$ converges to a fixed point $\\pi$ of a function $g_{m}$. For $m \\geqslant 3$, there exists a $p(m) \\in (0,1)$ such that $g_{m}$ has a unique fixed point, $1/2$, when $p \\leqslant p(m)$, and three distinct fixed points, of the form $\\alpha$, $1/2$ and $1-\\alpha$, when $p > p(m)$. When $m=3$, $p_{B}=1$ and $p_{R} \\in [0,1)$, we show that $g_{3}$ has a unique fixed point, $1$, when $p_{R} < \\sqrt{3}-1$, two distinct fixed points, one of which is $1$, when $p_{R} = \\sqrt{3}-1$, and three distinct fixed points, one of which is $1$, when $p_{R} > \\sqrt{3}-1$. When $g_{m}$ has multiple fixed points, we also specify which of these fixed points $\\pi$ equals, depending on $\\pi_{0}$. For $m=2$, we describe the behaviour of $g_{3}$ for all $p_{B}$ and $p_{R}$.","sentences":["We study a learning model in which an agent is stationed at each vertex of $\\mathbb{T}_{m}$, the rooted tree in which each vertex has $m$ children.","At any time-step $t \\in \\mathbb{N}_{0}$, they are allowed to select one of two available technologies: $B$ and $R$. Let the technology chosen by the agent at vertex $v\\in\\mathbb{T}_{m}$, at time-step $t$, be $C_{t}(v)$. Let $\\{C_{0}(v):v\\in\\mathbb{T}_{m}\\}$ be i.i.d., where $C_{0}(v)=B$ with probability $\\pi_{0}$. During epoch $t$, the agent at $v$ performs an experiment that results in success with probability $p_{B}$ if $C_{t}(v)=B$, and with probability $p_{R}$ if $C_{t}(v)=R$. If the children of $v$ are $v_{1},\\ldots,v_{m}$, the agent at $v$ updates their technology to $C_{t+1}(v)=B$ if the number of successes among all $v_{i}$ with $C_{t}(v_{i})=B$ exceeds, strictly, the number of successes among all $v_{j}$ with $C_{t}(v_{j})=R$. If these numbers are equal, then the agent at $v$ sets $C_{t+1}(v)=B$ with probability $1/2$. Else, $C_{t+1}(v)=R$. We show that $\\{C_{t}(v):v\\in\\mathbb{T}_{m}\\}$ is i.i.d., where $C_{t}(v)=B$ with probability $\\pi_{t}$, and $\\{\\pi_{t}\\}_{t \\in \\mathbb{N}_{0}}$ converges to a fixed point $\\pi$ of a function $g_{m}$. For $m \\geqslant 3$, there exists a $p(m) \\in (0,1)$ such that $g_{m}$ has a unique fixed point, $1/2$, when $p \\leqslant p(m)$, and three distinct fixed points, of the form $\\alpha$, $1/2$ and $1-\\alpha$, when $p > p(m)$. When $m=3$, $p_{B}=1$ and $p_{R} \\in [0,1)$, we show that $g_{3}$ has a unique fixed point, $1$, when $p_{R} < \\sqrt{3}-1$, two distinct fixed points, one of which is $1$, when $p_{R} = \\sqrt{3}-1$, and three distinct fixed points, one of which is $1$, when $p_{R} > \\sqrt{3}-1$. When $g_{m}$ has multiple fixed points, we also specify which of these fixed points $\\pi$ equals, depending on $\\pi_{0}$. For $m=2$, we describe the behaviour of $g_{3}$ for all $p_{B}$ and $p_{R}$."],"url":"http://arxiv.org/abs/2405.12418v1","category":"math.PR"}
{"created":"2024-05-20 22:56:31","title":"Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints","abstract":"We consider solving large scale nonconvex optimisation problems with nonnegativity constraints. Such problems arise frequently in machine learning, such as nonnegative least-squares, nonnegative matrix factorisation, as well as problems with sparsity-inducing regularisation. In such settings, first-order methods, despite their simplicity, can be prohibitively slow on ill-conditioned problems or become trapped near saddle regions, while most second-order alternatives involve non-trivially challenging subproblems. The two-metric projection framework, initially proposed by Bertsekas (1982), alleviates these issues and achieves the best of both worlds by combining projected gradient steps at the boundary of the feasible region with Newton steps in the interior in such a way that feasibility can be maintained by simple projection onto the nonnegative orthant. We develop extensions of the two-metric projection framework, which by inexactly solving the subproblems as well as employing non-positive curvature directions, are suitable for large scale and nonconvex settings. We obtain state-of-the-art convergence rates for various classes of non-convex problems and demonstrate competitive practical performance on a variety of problems.","sentences":["We consider solving large scale nonconvex optimisation problems with nonnegativity constraints.","Such problems arise frequently in machine learning, such as nonnegative least-squares, nonnegative matrix factorisation, as well as problems with sparsity-inducing regularisation.","In such settings, first-order methods, despite their simplicity, can be prohibitively slow on ill-conditioned problems or become trapped near saddle regions, while most second-order alternatives involve non-trivially challenging subproblems.","The two-metric projection framework, initially proposed by Bertsekas (1982), alleviates these issues and achieves the best of both worlds by combining projected gradient steps at the boundary of the feasible region with Newton steps in the interior in such a way that feasibility can be maintained by simple projection onto the nonnegative orthant.","We develop extensions of the two-metric projection framework, which by inexactly solving the subproblems as well as employing non-positive curvature directions, are suitable for large scale and nonconvex settings.","We obtain state-of-the-art convergence rates for various classes of non-convex problems and demonstrate competitive practical performance on a variety of problems."],"url":"http://arxiv.org/abs/2405.12401v1","category":"math.OC"}
{"created":"2024-05-20 21:42:42","title":"Particle swarm optimization with Applications to Maximum Likelihood Estimation and Penalized Negative Binomial Regression","abstract":"General purpose optimization routines such as nlminb, optim (R) or nlmixed (SAS) are frequently used to estimate model parameters in nonstandard distributions. This paper presents Particle Swarm Optimization (PSO), as an alternative to many of the current algorithms used in statistics. We find that PSO can not only reproduce the same results as the above routines, it can also produce results that are more optimal or when others cannot converge. In the latter case, it can also identify the source of the problem or problems. We highlight advantages of using PSO using four examples, where: (1) some parameters in a generalized distribution are unidentified using PSO when it is not apparent or computationally manifested using routines in R or SAS; (2) PSO can produce estimation results for the log-binomial regressions when current routines may not; (3) PSO provides flexibility in the link function for binomial regression with LASSO penalty, which is unsupported by standard packages like GLM and GENMOD in Stata and SAS, respectively, and (4) PSO provides superior MLE estimates for an EE-IW distribution compared with those from the traditional statistical methods that rely on moments.","sentences":["General purpose optimization routines such as nlminb, optim (R) or nlmixed (SAS) are frequently used to estimate model parameters in nonstandard distributions.","This paper presents Particle Swarm Optimization (PSO), as an alternative to many of the current algorithms used in statistics.","We find that PSO can not only reproduce the same results as the above routines, it can also produce results that are more optimal or when others cannot converge.","In the latter case, it can also identify the source of the problem or problems.","We highlight advantages of using PSO using four examples, where: (1) some parameters in a generalized distribution are unidentified using PSO when it is not apparent or computationally manifested using routines in R or SAS; (2) PSO can produce estimation results for the log-binomial regressions when current routines may not; (3) PSO provides flexibility in the link function for binomial regression with LASSO penalty, which is unsupported by standard packages like GLM and GENMOD in Stata and SAS, respectively, and (4) PSO provides superior MLE estimates for an EE-IW distribution compared with those from the traditional statistical methods that rely on moments."],"url":"http://arxiv.org/abs/2405.12386v1","category":"stat.ML"}
{"created":"2024-05-20 20:56:01","title":"DispaRisk: Assessing and Interpreting Disparity Risks in Datasets","abstract":"Machine Learning algorithms (ML) impact virtually every aspect of human lives and have found use across diverse sectors, including healthcare, finance, and education. Often, ML algorithms have been found to exacerbate societal biases presented in datasets, leading to adversarial impacts on subsets/groups of individuals, in many cases minority groups. To effectively mitigate these untoward effects, it is crucial that disparities/biases are identified and assessed early in a ML pipeline. This proactive approach facilitates timely interventions to prevent bias amplification and reduce complexity at later stages of model development. In this paper, we introduce DispaRisk, a novel framework designed to proactively assess the potential risks of disparities in datasets during the initial stages of the ML pipeline. We evaluate DispaRisk's effectiveness by benchmarking it with commonly used datasets in fairness research. Our findings demonstrate the capabilities of DispaRisk to identify datasets with a high-risk of discrimination, model families prone to biases, and characteristics that heighten discrimination susceptibility in a ML pipeline. The code for our experiments is available in the following repository: https://github.com/jovasque156/disparisk","sentences":["Machine Learning algorithms (ML) impact virtually every aspect of human lives and have found use across diverse sectors, including healthcare, finance, and education.","Often, ML algorithms have been found to exacerbate societal biases presented in datasets, leading to adversarial impacts on subsets/groups of individuals, in many cases minority groups.","To effectively mitigate these untoward effects, it is crucial that disparities/biases are identified and assessed early in a ML pipeline.","This proactive approach facilitates timely interventions to prevent bias amplification and reduce complexity at later stages of model development.","In this paper, we introduce DispaRisk, a novel framework designed to proactively assess the potential risks of disparities in datasets during the initial stages of the ML pipeline.","We evaluate DispaRisk's effectiveness by benchmarking it with commonly used datasets in fairness research.","Our findings demonstrate the capabilities of DispaRisk to identify datasets with a high-risk of discrimination, model families prone to biases, and characteristics that heighten discrimination susceptibility in a ML pipeline.","The code for our experiments is available in the following repository: https://github.com/jovasque156/disparisk"],"url":"http://arxiv.org/abs/2405.12372v1","category":"cs.LG"}
{"created":"2024-05-20 20:06:54","title":"Investigating the Impact of Choice on Deep Reinforcement Learning for Space Controls","abstract":"For many space applications, traditional control methods are often used during operation. However, as the number of space assets continues to grow, autonomous operation can enable rapid development of control methods for different space related tasks. One method of developing autonomous control is Reinforcement Learning (RL), which has become increasingly popular after demonstrating promising performance and success across many complex tasks. While it is common for RL agents to learn bounded continuous control values, this may not be realistic or practical for many space tasks that traditionally prefer an on/off approach for control. This paper analyzes using discrete action spaces, where the agent must choose from a predefined list of actions. The experiments explore how the number of choices provided to the agents affects their measured performance during and after training. This analysis is conducted for an inspection task, where the agent must circumnavigate an object to inspect points on its surface, and a docking task, where the agent must move into proximity of another spacecraft and \"dock\" with a low relative speed. A common objective of both tasks, and most space tasks in general, is to minimize fuel usage, which motivates the agent to regularly choose an action that uses no fuel. Our results show that a limited number of discrete choices leads to optimal performance for the inspection task, while continuous control leads to optimal performance for the docking task.","sentences":["For many space applications, traditional control methods are often used during operation.","However, as the number of space assets continues to grow, autonomous operation can enable rapid development of control methods for different space related tasks.","One method of developing autonomous control is Reinforcement Learning (RL), which has become increasingly popular after demonstrating promising performance and success across many complex tasks.","While it is common for RL agents to learn bounded continuous control values, this may not be realistic or practical for many space tasks that traditionally prefer an on/off approach for control.","This paper analyzes using discrete action spaces, where the agent must choose from a predefined list of actions.","The experiments explore how the number of choices provided to the agents affects their measured performance during and after training.","This analysis is conducted for an inspection task, where the agent must circumnavigate an object to inspect points on its surface, and a docking task, where the agent must move into proximity of another spacecraft and \"dock\" with a low relative speed.","A common objective of both tasks, and most space tasks in general, is to minimize fuel usage, which motivates the agent to regularly choose an action that uses no fuel.","Our results show that a limited number of discrete choices leads to optimal performance for the inspection task, while continuous control leads to optimal performance for the docking task."],"url":"http://arxiv.org/abs/2405.12355v1","category":"cs.LG"}
{"created":"2024-05-20 18:13:15","title":"Accurate Learning of Equivariant Quantum Systems from a Single Ground State","abstract":"Predicting properties across system parameters is an important task in quantum physics, with applications ranging from molecular dynamics to variational quantum algorithms. Recently, provably efficient algorithms to solve this task for ground states within a gapped phase were developed. Here we dramatically improve the efficiency of these algorithms by showing how to learn properties of all ground states for systems with periodic boundary conditions from a single ground state sample. We prove that the prediction error tends to zero in the thermodynamic limit and numerically verify the results.","sentences":["Predicting properties across system parameters is an important task in quantum physics, with applications ranging from molecular dynamics to variational quantum algorithms.","Recently, provably efficient algorithms to solve this task for ground states within a gapped phase were developed.","Here we dramatically improve the efficiency of these algorithms by showing how to learn properties of all ground states for systems with periodic boundary conditions from a single ground state sample.","We prove that the prediction error tends to zero in the thermodynamic limit and numerically verify the results."],"url":"http://arxiv.org/abs/2405.12309v1","category":"quant-ph"}
{"created":"2024-05-20 18:08:34","title":"Integration of Scanning Probe Microscope with High-Performance Computing: fixed-policy and reward-driven workflows implementation","abstract":"The rapid development of computation power and machine learning algorithms has paved the way for automating scientific discovery with a scanning probe microscope (SPM). The key elements towards operationalization of automated SPM are the interface to enable SPM control from Python codes, availability of high computing power, and development of workflows for scientific discovery. Here we build a Python interface library that enables controlling an SPM from either a local computer or a remote high-performance computer (HPC), which satisfies the high computation power need of machine learning algorithms in autonomous workflows. We further introduce a general platform to abstract the operations of SPM in scientific discovery into fixed-policy or reward-driven workflows. Our work provides a full infrastructure to build automated SPM workflows for both routine operations and autonomous scientific discovery with machine learning.","sentences":["The rapid development of computation power and machine learning algorithms has paved the way for automating scientific discovery with a scanning probe microscope (SPM).","The key elements towards operationalization of automated SPM are the interface to enable SPM control from Python codes, availability of high computing power, and development of workflows for scientific discovery.","Here we build a Python interface library that enables controlling an SPM from either a local computer or a remote high-performance computer (HPC), which satisfies the high computation power need of machine learning algorithms in autonomous workflows.","We further introduce a general platform to abstract the operations of SPM in scientific discovery into fixed-policy or reward-driven workflows.","Our work provides a full infrastructure to build automated SPM workflows for both routine operations and autonomous scientific discovery with machine learning."],"url":"http://arxiv.org/abs/2405.12300v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 17:37:10","title":"Multi-View Attentive Contextualization for Multi-View 3D Object Detection","abstract":"We present Multi-View Attentive Contextualization (MvACon), a simple yet effective method for improving 2D-to-3D feature lifting in query-based multi-view 3D (MV3D) object detection. Despite remarkable progress witnessed in the field of query-based MV3D object detection, prior art often suffers from either the lack of exploiting high-resolution 2D features in dense attention-based lifting, due to high computational costs, or from insufficiently dense grounding of 3D queries to multi-scale 2D features in sparse attention-based lifting. Our proposed MvACon hits the two birds with one stone using a representationally dense yet computationally sparse attentive feature contextualization scheme that is agnostic to specific 2D-to-3D feature lifting approaches. In experiments, the proposed MvACon is thoroughly tested on the nuScenes benchmark, using both the BEVFormer and its recent 3D deformable attention (DFA3D) variant, as well as the PETR, showing consistent detection performance improvement, especially in enhancing performance in location, orientation, and velocity prediction. It is also tested on the Waymo-mini benchmark using BEVFormer with similar improvement. We qualitatively and quantitatively show that global cluster-based contexts effectively encode dense scene-level contexts for MV3D object detection. The promising results of our proposed MvACon reinforces the adage in computer vision -- ``(contextualized) feature matters\".","sentences":["We present Multi-View Attentive Contextualization (MvACon), a simple yet effective method for improving 2D-to-3D feature lifting in query-based multi-view 3D (MV3D) object detection.","Despite remarkable progress witnessed in the field of query-based MV3D object detection, prior art often suffers from either the lack of exploiting high-resolution 2D features in dense attention-based lifting, due to high computational costs, or from insufficiently dense grounding of 3D queries to multi-scale 2D features in sparse attention-based lifting.","Our proposed MvACon hits the two birds with one stone using a representationally dense yet computationally sparse attentive feature contextualization scheme that is agnostic to specific 2D-to-3D feature lifting approaches.","In experiments, the proposed MvACon is thoroughly tested on the nuScenes benchmark, using both the BEVFormer and its recent 3D deformable attention (DFA3D) variant, as well as the PETR, showing consistent detection performance improvement, especially in enhancing performance in location, orientation, and velocity prediction.","It is also tested on the Waymo-mini benchmark using BEVFormer with similar improvement.","We qualitatively and quantitatively show that global cluster-based contexts effectively encode dense scene-level contexts for MV3D object detection.","The promising results of our proposed MvACon reinforces the adage in computer vision -- ``(contextualized) feature matters\"."],"url":"http://arxiv.org/abs/2405.12200v1","category":"cs.CV"}
{"created":"2024-05-20 17:20:41","title":"SEL-CIE: Knowledge-Guided Self-Supervised Learning Framework for CIE-XYZ Reconstruction from Non-Linear sRGB Images","abstract":"Modern cameras typically offer two types of image states: a minimally processed linear raw RGB image representing the raw sensor data, and a highly-processed non-linear image state, such as the sRGB state. The CIE-XYZ color space is a device-independent linear space used as part of the camera pipeline and can be helpful for computer vision tasks, such as image deblurring, dehazing, and color recognition tasks in medical applications, where color accuracy is important. However, images are usually saved in non-linear states, and achieving CIE-XYZ color images using conventional methods is not always possible. To tackle this issue, classical methodologies have been developed that focus on reversing the acquisition pipeline. More recently, supervised learning has been employed, using paired CIE-XYZ and sRGB representations of identical images. However, obtaining a large-scale dataset of CIE-XYZ and sRGB pairs can be challenging. To overcome this limitation and mitigate the reliance on large amounts of paired data, self-supervised learning (SSL) can be utilized as a substitute for relying solely on paired data. This paper proposes a framework for using SSL methods alongside paired data to reconstruct CIE-XYZ images and re-render sRGB images, outperforming existing approaches. The proposed framework is applied to the sRGB2XYZ dataset.","sentences":["Modern cameras typically offer two types of image states: a minimally processed linear raw RGB image representing the raw sensor data, and a highly-processed non-linear image state, such as the sRGB state.","The CIE-XYZ color space is a device-independent linear space used as part of the camera pipeline and can be helpful for computer vision tasks, such as image deblurring, dehazing, and color recognition tasks in medical applications, where color accuracy is important.","However, images are usually saved in non-linear states, and achieving CIE-XYZ color images using conventional methods is not always possible.","To tackle this issue, classical methodologies have been developed that focus on reversing the acquisition pipeline.","More recently, supervised learning has been employed, using paired CIE-XYZ and sRGB representations of identical images.","However, obtaining a large-scale dataset of CIE-XYZ and sRGB pairs can be challenging.","To overcome this limitation and mitigate the reliance on large amounts of paired data, self-supervised learning (SSL) can be utilized as a substitute for relying solely on paired data.","This paper proposes a framework for using SSL methods alongside paired data to reconstruct CIE-XYZ images and re-render sRGB images, outperforming existing approaches.","The proposed framework is applied to the sRGB2XYZ dataset."],"url":"http://arxiv.org/abs/2405.12265v1","category":"eess.IV"}
{"created":"2024-05-20 16:09:45","title":"Machine learning for predicting ultralow thermal conductivity and high ZT in complex thermoelectric materials","abstract":"Efficient and precise calculations of thermal transport properties and figure of merit, alongside a deep comprehension of thermal transport mechanisms, are essential for the practical utilization of advanced thermoelectric materials. In this study, we explore the microscopic processes governing thermal transport in the distinguished crystalline material Tl$_9$SbTe$_6$ by integrating a unified thermal transport theory with machine learning-assisted self-consistent phonon calculations. Leveraging machine learning potentials, we expedite the analysis of phonon energy shifts, higher-order scattering mechanisms, and thermal conductivity arising from various contributing factors like population and coherence channels. Our finding unveils an exceptionally low thermal conductivity of 0.31 W m$^{-1}$ K$^{-1}$ at room temperature, a result that closely correlates with experimental observations. Notably, we observe that the off-diagonal terms of heat flux operators play a significant role in shaping the overall lattice thermal conductivity of Tl$_9$SbTe$_6$, where the ultralow thermal conductivity resembles that of glass due to limited group velocities. Furthermore, we achieve a maximum $ZT$ value of 3.17 in the $c$-axis orientation for \\textit{p}-type Tl$_9$SbTe$_6$ at 600 K, and an optimal $ZT$ value of 2.26 in the $a$-axis and $b$-axis direction for \\textit{n}-type Tl$_9$SbTe$_6$ at 500 K. The crystalline Tl$_9$SbTe$_6$ not only showcases remarkable thermal insulation but also demonstrates impressive electrical properties owing to the dual-degeneracy phenomenon within its valence band. These results not only elucidate the underlying reasons for the exceptional thermoelectric performance of Tl$_9$SbTe$_6$ but also suggest potential avenues for further experimental exploration.","sentences":["Efficient and precise calculations of thermal transport properties and figure of merit, alongside a deep comprehension of thermal transport mechanisms, are essential for the practical utilization of advanced thermoelectric materials.","In this study, we explore the microscopic processes governing thermal transport in the distinguished crystalline material Tl$_9$SbTe$_6$ by integrating a unified thermal transport theory with machine learning-assisted self-consistent phonon calculations.","Leveraging machine learning potentials, we expedite the analysis of phonon energy shifts, higher-order scattering mechanisms, and thermal conductivity arising from various contributing factors like population and coherence channels.","Our finding unveils an exceptionally low thermal conductivity of 0.31 W m$^{-1}$ K$^{-1}$ at room temperature, a result that closely correlates with experimental observations.","Notably, we observe that the off-diagonal terms of heat flux operators play a significant role in shaping the overall lattice thermal conductivity of Tl$_9$SbTe$_6$, where the ultralow thermal conductivity resembles that of glass due to limited group velocities.","Furthermore, we achieve a maximum $ZT$ value of 3.17 in the $c$-axis orientation for \\textit{p}-type Tl$_9$SbTe$_6$ at 600 K, and an optimal $ZT$ value of 2.26 in the $a$-axis and $b$-axis direction for \\textit{n}-type Tl$_9$SbTe$_6$ at 500 K. The crystalline Tl$_9$SbTe$_6$ not only showcases remarkable thermal insulation but also demonstrates impressive electrical properties owing to the dual-degeneracy phenomenon within its valence band.","These results not only elucidate the underlying reasons for the exceptional thermoelectric performance of Tl$_9$SbTe$_6$ but also suggest potential avenues for further experimental exploration."],"url":"http://arxiv.org/abs/2405.12143v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 15:48:40","title":"SkyCURTAINs: Model agnostic search for Stellar Streams with Gaia data","abstract":"We present SkyCURTAINs, a data driven and model agnostic method to search for stellar streams in the Milky Way galaxy using data from the Gaia telescope. SkyCURTAINs is a weakly supervised machine learning algorithm that builds a background enriched template in the signal region by leveraging the correlation of the source's characterising features with their proper motion in the sky. This allows for a more representative template of the background in the signal region, and reduces the false positives in the search for stellar streams. The minimal model assumptions in the SkyCURTAINs method allow for a flexible and efficient search for various kinds of anomalies such as streams, globular clusters, or dwarf galaxies directly from the data. We test the performance of SkyCURTAINs on the GD-1 stream and show that it is able to recover the stream with a purity of 75.4% which is an improvement of over 10% over existing machine learning based methods while retaining a signal efficiency of 37.9%.","sentences":["We present SkyCURTAINs, a data driven and model agnostic method to search for stellar streams in the Milky Way galaxy using data from the Gaia telescope.","SkyCURTAINs is a weakly supervised machine learning algorithm that builds a background enriched template in the signal region by leveraging the correlation of the source's characterising features with their proper motion in the sky.","This allows for a more representative template of the background in the signal region, and reduces the false positives in the search for stellar streams.","The minimal model assumptions in the SkyCURTAINs method allow for a flexible and efficient search for various kinds of anomalies such as streams, globular clusters, or dwarf galaxies directly from the data.","We test the performance of SkyCURTAINs on the GD-1 stream and show that it is able to recover the stream with a purity of 75.4% which is an improvement of over 10% over existing machine learning based methods while retaining a signal efficiency of 37.9%."],"url":"http://arxiv.org/abs/2405.12131v1","category":"astro-ph.GA"}
{"created":"2024-05-20 15:25:18","title":"Linguistic Structure from a Bottleneck on Sequential Information Processing","abstract":"Human language is a unique form of communication in the natural world, distinguished by its structured nature. Most fundamentally, it is systematic, meaning that signals can be broken down into component parts that are individually meaningful -- roughly, words -- which are combined in a regular way to form sentences. Furthermore, the way in which these parts are combined maintains a kind of locality: words are usually concatenated together, and they form contiguous phrases, keeping related parts of sentences close to each other. We address the challenge of understanding how these basic properties of language arise from broader principles of efficient communication under information processing constraints. Here we show that natural-language-like systematicity arises from minimization of excess entropy, a measure of statistical complexity that represents the minimum amount of information necessary for predicting the future of a sequence based on its past. In simulations, we show that codes that minimize excess entropy factorize their source distributions into approximately independent components, and then express those components systematically and locally. Next, in a series of massively cross-linguistic corpus studies, we show that human languages are structured to have low excess entropy at the level of phonology, morphology, syntax, and semantics. Our result suggests that human language performs a sequential generalization of Independent Components Analysis on the statistical distribution over meanings that need to be expressed. It establishes a link between the statistical and algebraic structure of human language, and reinforces the idea that the structure of human language may have evolved to minimize cognitive load while maximizing communicative expressiveness.","sentences":["Human language is a unique form of communication in the natural world, distinguished by its structured nature.","Most fundamentally, it is systematic, meaning that signals can be broken down into component parts that are individually meaningful -- roughly, words -- which are combined in a regular way to form sentences.","Furthermore, the way in which these parts are combined maintains a kind of locality: words are usually concatenated together, and they form contiguous phrases, keeping related parts of sentences close to each other.","We address the challenge of understanding how these basic properties of language arise from broader principles of efficient communication under information processing constraints.","Here we show that natural-language-like systematicity arises from minimization of excess entropy, a measure of statistical complexity that represents the minimum amount of information necessary for predicting the future of a sequence based on its past.","In simulations, we show that codes that minimize excess entropy factorize their source distributions into approximately independent components, and then express those components systematically and locally.","Next, in a series of massively cross-linguistic corpus studies, we show that human languages are structured to have low excess entropy at the level of phonology, morphology, syntax, and semantics.","Our result suggests that human language performs a sequential generalization of Independent Components Analysis on the statistical distribution over meanings that need to be expressed.","It establishes a link between the statistical and algebraic structure of human language, and reinforces the idea that the structure of human language may have evolved to minimize cognitive load while maximizing communicative expressiveness."],"url":"http://arxiv.org/abs/2405.12109v1","category":"cs.CL"}
{"created":"2024-05-20 15:21:48","title":"Sheet Music Transformer ++: End-to-End Full-Page Optical Music Recognition for Pianoform Sheet Music","abstract":"Optical Music Recognition is a field that has progressed significantly, bringing accurate systems that transcribe effectively music scores into digital formats. Despite this, there are still several limitations that hinder OMR from achieving its full potential. Specifically, state of the art OMR still depends on multi-stage pipelines for performing full-page transcription, as well as it has only been demonstrated in monophonic cases, leaving behind very relevant engravings. In this work, we present the Sheet Music Transformer++, an end-to-end model that is able to transcribe full-page polyphonic music scores without the need of a previous Layout Analysis step. This is done thanks to an extensive curriculum learning-based pretraining with synthetic data generation. We conduct several experiments on a full-page extension of a public polyphonic transcription dataset. The experimental outcomes confirm that the model is competent at transcribing full-page pianoform scores, marking a noteworthy milestone in end-to-end OMR transcription.","sentences":["Optical Music Recognition is a field that has progressed significantly, bringing accurate systems that transcribe effectively music scores into digital formats.","Despite this, there are still several limitations that hinder OMR from achieving its full potential.","Specifically, state of the art OMR still depends on multi-stage pipelines for performing full-page transcription, as well as it has only been demonstrated in monophonic cases, leaving behind very relevant engravings.","In this work, we present the Sheet Music Transformer++, an end-to-end model that is able to transcribe full-page polyphonic music scores without the need of a previous Layout Analysis step.","This is done thanks to an extensive curriculum learning-based pretraining with synthetic data generation.","We conduct several experiments on a full-page extension of a public polyphonic transcription dataset.","The experimental outcomes confirm that the model is competent at transcribing full-page pianoform scores, marking a noteworthy milestone in end-to-end OMR transcription."],"url":"http://arxiv.org/abs/2405.12105v2","category":"cs.CV"}
{"created":"2024-05-20 15:13:03","title":"Chemical control of self-assembly by the electrosolvation force","abstract":"Self-assembly of matter in solution generally relies on attractive interactions that overcome entropy and drive the formation of higher-order molecular and particulate structures. Such interactions play key roles in a variety of contexts, e.g., crystallisation, biomolecular folding and condensation, pathological protein aggregation, pharmaceuticals and fine chemicals. The electrosolvation force entails a new conceptual paradigm in the known palette of interactions that drive the spontaneous accretion and organisation of matter. However, an understanding of the underlying physical chemistry, and therefore the ability to exert control over and tune the interaction, remains incomplete. Here we demonstrate that this force arises from the structure of the interfacial electrolyte. Neutral molecules such as a different solvent, osmolytes or surfactants, can - even at very low concentrations in the medium - disrupt or reinforce pre-existing interfacial solvent structure, thereby furnishing unanticipated chemical tuning of the ability of matter to self-assemble. The observations further present unexpected mechanistic elements that may explain the impact of co-solvents and osmolytes on protein structure, stability and pathological protein condensation. Our findings shed new light on microscopic mechanisms that drive the emergence of order and structure from molecular to macroscopic scales in the solution phase.","sentences":["Self-assembly of matter in solution generally relies on attractive interactions that overcome entropy and drive the formation of higher-order molecular and particulate structures.","Such interactions play key roles in a variety of contexts, e.g., crystallisation, biomolecular folding and condensation, pathological protein aggregation, pharmaceuticals and fine chemicals.","The electrosolvation force entails a new conceptual paradigm in the known palette of interactions that drive the spontaneous accretion and organisation of matter.","However, an understanding of the underlying physical chemistry, and therefore the ability to exert control over and tune the interaction, remains incomplete.","Here we demonstrate that this force arises from the structure of the interfacial electrolyte.","Neutral molecules such as a different solvent, osmolytes or surfactants, can - even at very low concentrations in the medium - disrupt or reinforce pre-existing interfacial solvent structure, thereby furnishing unanticipated chemical tuning of the ability of matter to self-assemble.","The observations further present unexpected mechanistic elements that may explain the impact of co-solvents and osmolytes on protein structure, stability and pathological protein condensation.","Our findings shed new light on microscopic mechanisms that drive the emergence of order and structure from molecular to macroscopic scales in the solution phase."],"url":"http://arxiv.org/abs/2405.12099v1","category":"cond-mat.soft"}
{"created":"2024-05-21 15:46:52","title":"Epitaxial RuO$_2$ and IrO$_2$ films by pulsed laser deposition on TiO$_2$(110)","abstract":"We present a systematic growth study of epitaxial RuO$_2$(110) and IrO$_2$(110) on TiO$_2$(110) substrates by pulsed laser deposition. We describe the main challenges encountered in the growth process, such as a deteriorating material flux due to laser induced target metallization or the delicate balance of under- vs over-oxidation of the 'stubborn' Ru and Ir metals. We identify growth temperatures and oxygen partial pressures of 700 K, $1\\times 10^{-3}$ mbar for RuO$_2$ and 770 K, $5\\times 10^{-4}$ mbar for IrO$_2$ to optimally balance between metal oxidation and particle mobility during nucleation. In contrast to IrO$_2$, RuO$_2$ exhibits layer-by-layer growth up to 5 unit cells if grown at high deposition rates. At low deposition rates, the large lattice mismatch between film and substrate fosters initial 3D island growth and cluster formation. In analogy to reports for RuO$_2$ based on physical vapor deposition, we find these islands to eventually merge and growth to continue in a step flow mode, resulting in highly crystalline, flat, stoichiometric films of RuO$_2$(110) (up to 30 nm thickness) and IrO$_2$(110) (up to 13 nm thickness) with well defined line defects.","sentences":["We present a systematic growth study of epitaxial RuO$_2$(110) and IrO$_2$(110) on TiO$_2$(110) substrates by pulsed laser deposition.","We describe the main challenges encountered in the growth process, such as a deteriorating material flux due to laser induced target metallization or the delicate balance of under- vs over-oxidation of the 'stubborn' Ru and Ir metals.","We identify growth temperatures and oxygen partial pressures of 700 K, $1\\times 10^{-3}$ mbar for RuO$_2$ and 770 K, $5\\times 10^{-4}$ mbar for IrO$_2$ to optimally balance between metal oxidation and particle mobility during nucleation.","In contrast to IrO$_2$, RuO$_2$ exhibits layer-by-layer growth up to 5 unit cells if grown at high deposition rates.","At low deposition rates, the large lattice mismatch between film and substrate fosters initial 3D island growth and cluster formation.","In analogy to reports for RuO$_2$ based on physical vapor deposition, we find these islands to eventually merge and growth to continue in a step flow mode, resulting in highly crystalline, flat, stoichiometric films of RuO$_2$(110) (up to 30 nm thickness) and IrO$_2$(110) (up to 13 nm thickness) with well defined line defects."],"url":"http://arxiv.org/abs/2405.12878v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 14:52:46","title":"Quantum computing and the stable set problem","abstract":"Given an undirected graph, the stable set problem asks to determine the cardinality of the largest subset of pairwise non-adjacent vertices. This value is called the stability number of the graph, and its computation is an NP-hard problem. In this paper, we focus on solving the stable set problem using the D-wave quantum annealer. By formulating the problem as a quadratic unconstrained binary optimization problem with the penalty method, we show that the optimal value of this formulation is equal to the stability number of the graph for certain penalty parameter values. However, D-Wave's quantum annealer is not an exact solver, so the solutions may be far from the optimum and may not even represent stable sets. To address this, we introduce a post-processing procedure that identifies samples that could lead to improved solutions and extracts stable sets from them. Additionally, we propose a so-called simple CH-partitioning method to handle larger instances that cannot be embedded on D-Wave's quantum processing unit. Finally, we investigate how different penalty parameter values affect the solutions' quality. Extensive computational results show that the post-processing procedure significantly improves the solution quality, while the simple CH-partitioning method successfully extends our approach to medium-size instances.","sentences":["Given an undirected graph, the stable set problem asks to determine the cardinality of the largest subset of pairwise non-adjacent vertices.","This value is called the stability number of the graph, and its computation is an NP-hard problem.","In this paper, we focus on solving the stable set problem using the D-wave quantum annealer.","By formulating the problem as a quadratic unconstrained binary optimization problem with the penalty method, we show that the optimal value of this formulation is equal to the stability number of the graph for certain penalty parameter values.","However, D-Wave's quantum annealer is not an exact solver, so the solutions may be far from the optimum and may not even represent stable sets.","To address this, we introduce a post-processing procedure that identifies samples that could lead to improved solutions and extracts stable sets from them.","Additionally, we propose a so-called simple CH-partitioning method to handle larger instances that cannot be embedded on D-Wave's quantum processing unit.","Finally, we investigate how different penalty parameter values affect the solutions' quality.","Extensive computational results show that the post-processing procedure significantly improves the solution quality, while the simple CH-partitioning method successfully extends our approach to medium-size instances."],"url":"http://arxiv.org/abs/2405.12845v1","category":"math.OC"}
{"created":"2024-05-21 14:25:58","title":"Weak and Strong Nestings of BIBDs","abstract":"We study two types of nestings of balanced incomplete block designs (BIBDs). In both types of nesting, we wish to add a point (the nested point) to every block of a $(v,k,\\lambda)$-BIBD in such a way that we end up with a partial $(w,k+1,\\lambda+1)$-BIBD for some $w \\geq v$. In the case where $w > v$, we are introducing $w-v$ new points. This is called a weak nesting. A strong nesting satisfies the stronger property that no pair containing a new point occurs more than once in the partial $(w,k+1,\\lambda+1)$-BIBD. In both cases, the goal is to minimize $w$. We prove lower bounds on $w$ as a function of $v$, $k$ and $\\lambda$ and we find infinite classes of $(v,2,1)$- and $(v,3,2)$-BIBDs that have optimal nestings.","sentences":["We study two types of nestings of balanced incomplete block designs (BIBDs).","In both types of nesting, we wish to add a point (the nested point) to every block of a $(v,k,\\lambda)$-BIBD in such a way that we end up with a partial $(w,k+1,\\lambda+1)$-BIBD for some $w \\geq v$.","In the case where $w > v$, we are introducing $w-v$ new points.","This is called a weak nesting.","A strong nesting satisfies the stronger property that no pair containing a new point occurs more than once in the partial $(w,k+1,\\lambda+1)$-BIBD.","In both cases, the goal is to minimize $w$. We prove lower bounds on $w$ as a function of $v$, $k$ and $\\lambda$ and we find infinite classes of $(v,2,1)$- and $(v,3,2)$-BIBDs that have optimal nestings."],"url":"http://arxiv.org/abs/2405.12820v1","category":"math.CO"}
{"created":"2024-05-21 11:40:10","title":"A phonon-driven mechanism for an emergent and reversible chirality in crystals","abstract":"We demonstrate from first-principles calculations applied to the K$_{3}$NiO$_{2}$ crystal that a structural phase transition from an achiral to a chiral phase can be mediated by a soft zone boundary phonon and that it can be controlled by pressure and epitaxial strain. Calculations in the presence of an electric field hint at a rich physics dominated by the competition between chirality, polar, and achiral distortions. Induced by the interaction between chiral, polar, and axial phonons, an optimal parameter window for the permanent reversal of chirality is observed.","sentences":["We demonstrate from first-principles calculations applied to the K$_{3}$NiO$_{2}$ crystal that a structural phase transition from an achiral to a chiral phase can be mediated by a soft zone boundary phonon and that it can be controlled by pressure and epitaxial strain.","Calculations in the presence of an electric field hint at a rich physics dominated by the competition between chirality, polar, and achiral distortions.","Induced by the interaction between chiral, polar, and axial phonons, an optimal parameter window for the permanent reversal of chirality is observed."],"url":"http://arxiv.org/abs/2405.12696v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 09:12:16","title":"Towards Using Fast Embedded Model Predictive Control for Human-Aware Predictive Robot Navigation","abstract":"Predictive planning is a key capability for robots to efficiently and safely navigate populated environments. Particularly in densely crowded scenes, with uncertain human motion predictions, predictive path planning, and control can become expensive to compute in real time due to the curse of dimensionality. With the goal of achieving pro-active and legible robot motion in shared environments, in this paper we present HuMAN-MPC, a computationally efficient algorithm for Human Motion Aware Navigation using fast embedded Model Predictive Control. The approach consists of a novel model predictive control (MPC) formulation that leverages a fast state-of-the-art optimization backend based on a sequential quadratic programming real-time iteration scheme while also providing feasibility monitoring. Our experiments, in simulation and on a fully integrated ROS-based platform, show that the approach achieves great scalability with fast computation times without penalizing path quality and efficiency of the resulting avoidance behavior.","sentences":["Predictive planning is a key capability for robots to efficiently and safely navigate populated environments.","Particularly in densely crowded scenes, with uncertain human motion predictions, predictive path planning, and control can become expensive to compute in real time due to the curse of dimensionality.","With the goal of achieving pro-active and legible robot motion in shared environments, in this paper we present HuMAN-MPC, a computationally efficient algorithm for Human Motion Aware Navigation using fast embedded Model Predictive Control.","The approach consists of a novel model predictive control (MPC) formulation that leverages a fast state-of-the-art optimization backend based on a sequential quadratic programming real-time iteration scheme while also providing feasibility monitoring.","Our experiments, in simulation and on a fully integrated ROS-based platform, show that the approach achieves great scalability with fast computation times without penalizing path quality and efficiency of the resulting avoidance behavior."],"url":"http://arxiv.org/abs/2405.12616v1","category":"cs.RO"}
{"created":"2024-05-20 23:42:17","title":"Distribution Steering for Discrete-Time Uncertain Ensemble Systems","abstract":"Ensemble systems appear frequently in many engineering applications and, as a result, they have become an important research topic in control theory. These systems are best characterized by the evolution of their underlying state distribution. Despite the work to date, few results exist dealing with the problem of directly modifying (i.e., \"steering\") the distribution of an ensemble system. In addition, in most of the existing results, the distribution of the states of an ensemble of discrete-time systems is assumed to be Gaussian. However, in case the system parameters are uncertain, it is not always realistic to assume that the distribution of the system follows a Gaussian distribution, thus complicating the solution of the overall problem. In this paper, we address the general distribution steering problem for first-order discrete-time ensemble systems, where the distributions of the system parameters and the states are arbitrary with finite first few moments. Both linear and nonlinear system dynamics are considered using the method of power moments to transform the original infinite-dimensional problem into a finite-dimensional one. We also propose a control law for the ensuing moment system, which allows us to obtain the power moments of the desired control inputs. Finally, we solve the inverse problem to obtain the feasible control inputs from their corresponding power moments. We provide numerical results to validate our theoretical developments. These include cases where the parameter distribution is uniform, Gaussian, non-Gaussian, and multi-modal, respectively.","sentences":["Ensemble systems appear frequently in many engineering applications and, as a result, they have become an important research topic in control theory.","These systems are best characterized by the evolution of their underlying state distribution.","Despite the work to date, few results exist dealing with the problem of directly modifying (i.e., \"steering\") the distribution of an ensemble system.","In addition, in most of the existing results, the distribution of the states of an ensemble of discrete-time systems is assumed to be Gaussian.","However, in case the system parameters are uncertain, it is not always realistic to assume that the distribution of the system follows a Gaussian distribution, thus complicating the solution of the overall problem.","In this paper, we address the general distribution steering problem for first-order discrete-time ensemble systems, where the distributions of the system parameters and the states are arbitrary with finite first few moments.","Both linear and nonlinear system dynamics are considered using the method of power moments to transform the original infinite-dimensional problem into a finite-dimensional one.","We also propose a control law for the ensuing moment system, which allows us to obtain the power moments of the desired control inputs.","Finally, we solve the inverse problem to obtain the feasible control inputs from their corresponding power moments.","We provide numerical results to validate our theoretical developments.","These include cases where the parameter distribution is uniform, Gaussian, non-Gaussian, and multi-modal, respectively."],"url":"http://arxiv.org/abs/2405.12415v1","category":"math.OC"}
{"created":"2024-05-20 19:10:28","title":"Interoperable Provenance Authentication of Broadcast Media using Open Standards-based Metadata, Watermarking and Cryptography","abstract":"The spread of false and misleading information is receiving significant attention from legislative and regulatory bodies. Consumers place trust in specific sources of information, so a scalable, interoperable method for determining the provenance and authenticity of information is needed. In this paper we analyze the posting of broadcast news content to a social media platform, the role of open standards, the interplay of cryptographic metadata and watermarks when validating provenance, and likely success and failure scenarios. We conclude that the open standards for cryptographically authenticated metadata developed by the Coalition for Provenance and Authenticity (C2PA) and for audio and video watermarking developed by the Advanced Television Systems Committee (ATSC) are well suited to address broadcast provenance. We suggest methods for using these standards for optimal success.","sentences":["The spread of false and misleading information is receiving significant attention from legislative and regulatory bodies.","Consumers place trust in specific sources of information, so a scalable, interoperable method for determining the provenance and authenticity of information is needed.","In this paper we analyze the posting of broadcast news content to a social media platform, the role of open standards, the interplay of cryptographic metadata and watermarks when validating provenance, and likely success and failure scenarios.","We conclude that the open standards for cryptographically authenticated metadata developed by the Coalition for Provenance and Authenticity (C2PA) and for audio and video watermarking developed by the Advanced Television Systems Committee (ATSC) are well suited to address broadcast provenance.","We suggest methods for using these standards for optimal success."],"url":"http://arxiv.org/abs/2405.12336v1","category":"cs.CR"}
{"created":"2024-05-20 18:12:14","title":"GW with hybrid functionals for large molecular systems","abstract":"A low-cost approach for stochastically sampling static exchange during TDHF-type propagation is presented. This enables the use of an excellent hybrid DFT starting point for stochastic GW quasiparticle energy calculations. Generalized Kohn-Sham molecular orbitals and energies, rather than those of a local-DFT calculation, are used for building the Green's function and effective Coulomb interaction. The use of an optimally tuned hybrid diminishes the starting point dependency in one-shot stochastic GW, effectively avoiding the need for self-consistent GW iterations.","sentences":["A low-cost approach for stochastically sampling static exchange during TDHF-type propagation is presented.","This enables the use of an excellent hybrid DFT starting point for stochastic GW quasiparticle energy calculations.","Generalized Kohn-Sham molecular orbitals and energies, rather than those of a local-DFT calculation, are used for building the Green's function and effective Coulomb interaction.","The use of an optimally tuned hybrid diminishes the starting point dependency in one-shot stochastic GW, effectively avoiding the need for self-consistent GW iterations."],"url":"http://arxiv.org/abs/2405.12306v1","category":"physics.chem-ph"}
{"created":"2024-05-20 18:00:09","title":"Improved model of the Supernova Refsdal cluster MACS J1149.5+2223 thanks to VLT/MUSE","abstract":"We present new VLT/MUSE observations of the Hubble Frontier Field (HFF) galaxy cluster MACS J1149.5+2223, lensing the well-known supernova \"Refsdal\" into multiple images, which enabled the first cosmological applications with a strongly lensed supernova. Thanks to these data, targeting a northern region of the cluster and thus complementing our previous MUSE program on the cluster core, we release a new catalog containing 162 secure spectroscopic redshifts. We confirm 22 cluster members, which were previously only photometrically selected, and detect ten additional ones, resulting in a total of 308 secure members, of which 63% are spectroscopically confirmed. We further identify 17 new spectroscopic multiple images belonging to 6 different background sources. By exploiting MUSE data, in combination with the deep HFF images, we develop an improved total mass model of MACS J1149.5+2223. This model includes 308 total mass components for the member galaxies and requires four additional mass profiles, one of which is associated with a cluster galaxy overdensity identified in the North, representing the DM mass distribution on larger scales. The values of the resulting 34 free parameters are optimized based on the observed positions of 106 multiple images from 34 different families, that cover the redshift range between 1.240 and 5.983. Our final model has a multiple image position rms value of 0.39\", which is well in agreement with that of other cluster lens models. With this refined mass model, we pave the way towards even better strong-lensing analyses that will exploit the deep and high resolution observations with HST and JWST on a pixel level in the region of the supernova Refsdal host. This will increase the number of observables by around two orders of magnitudes, thus offering us the opportunity of carrying out more precise and accurate cosmographic measurements.","sentences":["We present new VLT/MUSE observations of the Hubble Frontier Field (HFF) galaxy cluster MACS J1149.5","+2223, lensing the well-known supernova \"Refsdal\" into multiple images, which enabled the first cosmological applications with a strongly lensed supernova.","Thanks to these data, targeting a northern region of the cluster and thus complementing our previous MUSE program on the cluster core, we release a new catalog containing 162 secure spectroscopic redshifts.","We confirm 22 cluster members, which were previously only photometrically selected, and detect ten additional ones, resulting in a total of 308 secure members, of which 63% are spectroscopically confirmed.","We further identify 17 new spectroscopic multiple images belonging to 6 different background sources.","By exploiting MUSE data, in combination with the deep HFF images, we develop an improved total mass model of MACS J1149.5+2223.","This model includes 308 total mass components for the member galaxies and requires four additional mass profiles, one of which is associated with a cluster galaxy overdensity identified in the North, representing the DM mass distribution on larger scales.","The values of the resulting 34 free parameters are optimized based on the observed positions of 106 multiple images from 34 different families, that cover the redshift range between 1.240 and 5.983.","Our final model has a multiple image position rms value of 0.39\", which is well in agreement with that of other cluster lens models.","With this refined mass model, we pave the way towards even better strong-lensing analyses that will exploit the deep and high resolution observations with HST and JWST on a pixel level in the region of the supernova Refsdal host.","This will increase the number of observables by around two orders of magnitudes, thus offering us the opportunity of carrying out more precise and accurate cosmographic measurements."],"url":"http://arxiv.org/abs/2405.12287v1","category":"astro-ph.CO"}
{"created":"2024-05-20 18:00:07","title":"Massive Star Cluster Formation II. Runaway Stars as Fossils of Sub-Cluster Mergers","abstract":"Two main mechanisms have classically been proposed for the formation of runaway stars. The binary supernova scenario (BSS) suggests that a massive star in a binary explodes as a supernova, ejecting its companion. The dynamical ejection scenario suggests that a star is ejected through a strong dynamical encounter between multiple stars. We propose a third mechanism for the formation of runaway stars: the sub-cluster ejection scenario (SCES), where an infalling sub-cluster of stars is ejected out of the cluster by a slingshot interaction with the contracting gravitational potential of the assembling cluster. We demonstrate the SCES in a star-by-star simulation of a young massive cluster forming from a $10^6\\rm~M_\\odot$ gas cloud using the Torch framework. This star cluster forms hierarchically through a sequence of sub-cluster mergers, determined by the initial turbulent, spherical initial conditions of the gas. We find that these mergers drive the formation of runaway stars in our model. Late-forming sub-clusters fall into the central potential, where they are ejected on a slingshot trajectory, forming groups of runaway stars that are distributed highly anisotropically. Runaways formed by the same SCES share similar ages, velocities, and ejection directions. Surveying observations, we identify several SCES candidate groups with anisotropic ejection directions. The SCES is capable of producing runaway binaries: two wide dynamical binaries in infalling sub-clusters were tightened through ejection. This allows for another velocity kick via subsequent BSS, which is a promising avenue for producing hypervelocity stars unbound to the Galaxy. The SCES occurs when sub-cluster formation is resolved. We expect non-spherical initial gas distributions to increase runaway star numbers. The observation of groups of runaway stars formed via SCES thus reveals the assembly history of their natal clusters.","sentences":["Two main mechanisms have classically been proposed for the formation of runaway stars.","The binary supernova scenario (BSS) suggests that a massive star in a binary explodes as a supernova, ejecting its companion.","The dynamical ejection scenario suggests that a star is ejected through a strong dynamical encounter between multiple stars.","We propose a third mechanism for the formation of runaway stars: the sub-cluster ejection scenario (SCES), where an infalling sub-cluster of stars is ejected out of the cluster by a slingshot interaction with the contracting gravitational potential of the assembling cluster.","We demonstrate the SCES in a star-by-star simulation of a young massive cluster forming from a $10^6\\rm~M_\\odot$ gas cloud using the Torch framework.","This star cluster forms hierarchically through a sequence of sub-cluster mergers, determined by the initial turbulent, spherical initial conditions of the gas.","We find that these mergers drive the formation of runaway stars in our model.","Late-forming sub-clusters fall into the central potential, where they are ejected on a slingshot trajectory, forming groups of runaway stars that are distributed highly anisotropically.","Runaways formed by the same SCES share similar ages, velocities, and ejection directions.","Surveying observations, we identify several SCES candidate groups with anisotropic ejection directions.","The SCES is capable of producing runaway binaries: two wide dynamical binaries in infalling sub-clusters were tightened through ejection.","This allows for another velocity kick via subsequent BSS, which is a promising avenue for producing hypervelocity stars unbound to the Galaxy.","The SCES occurs when sub-cluster formation is resolved.","We expect non-spherical initial gas distributions to increase runaway star numbers.","The observation of groups of runaway stars formed via SCES thus reveals the assembly history of their natal clusters."],"url":"http://arxiv.org/abs/2405.12286v1","category":"astro-ph.GA"}
{"created":"2024-05-20 17:59:30","title":"Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo","abstract":"We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization.","sentences":["We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes.","Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters.","2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis.","3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization.","Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene.","Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost.","Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization."],"url":"http://arxiv.org/abs/2405.12218v1","category":"cs.CV"}
{"created":"2024-05-20 17:33:36","title":"Near-horizon properties of trajectories with finite force relevant for Ba\u00f1ados-Silk-West effect","abstract":"According to the Banados-SIlk-West (BSW) effect, two particles moving towards a black hole, can collide near the horizon with an unbounded energy in the center of mass frame. This requires one of particles to have fine-tuned parameters in such a way that the time component of generalized momentum is zero $X=0$. Thus the existence of such trjectories is a necessary condition for the BSW effect. However, it is insufficient since the forward-in-time condition requires $X>0$ outside the horizon. We examine this condition for different types of partricles and horizons and find configurations for which the BSW effect is possible. In doing so, we take into account a finite force of unspesified nature exerted on particles. It includes relationships between numbers characterizing the rate with which four-velocity, acceleration and metric functions change near the horizon. For some aforementioned relations, parameters of a system control the sign of $X$, in other cases they are required for $X$ to be real quantity. In the simplest case of free particles the BSW effect for the Kerr or Kerr-Newman black hole is impossible if a fine-tuned particle has a negative energy, so in this sense combination of the Penrose process and the BSW effect is forbidden.","sentences":["According to the Banados-SIlk-West (BSW) effect, two particles moving towards a black hole, can collide near the horizon with an unbounded energy in the center of mass frame.","This requires one of particles to have fine-tuned parameters in such a way that the time component of generalized momentum is zero $X=0$.","Thus the existence of such trjectories is a necessary condition for the BSW effect.","However, it is insufficient since the forward-in-time condition requires $X>0$ outside the horizon.","We examine this condition for different types of partricles and horizons and find configurations for which the BSW effect is possible.","In doing so, we take into account a finite force of unspesified nature exerted on particles.","It includes relationships between numbers characterizing the rate with which four-velocity, acceleration and metric functions change near the horizon.","For some aforementioned relations, parameters of a system control the sign of $X$, in other cases they are required for $X$ to be real quantity.","In the simplest case of free particles the BSW effect for the Kerr or Kerr-Newman black hole is impossible if a fine-tuned particle has a negative energy, so in this sense combination of the Penrose process and the BSW effect is forbidden."],"url":"http://arxiv.org/abs/2405.12198v1","category":"gr-qc"}
{"created":"2024-05-20 17:26:43","title":"The Narrow Depth and Breadth of Corporate Responsible AI Research","abstract":"The transformative potential of AI presents remarkable opportunities, but also significant risks, underscoring the importance of responsible AI development and deployment. Despite a growing emphasis on this area, there is limited understanding of industry's engagement in responsible AI research, i.e., the critical examination of AI's ethical, social, and legal dimensions. To address this gap, we analyzed over 6 million peer-reviewed articles and 32 million patent citations using multiple methods across five distinct datasets to quantify industry's engagement. Our findings reveal that the majority of AI firms show limited or no engagement in this critical subfield of AI. We show a stark disparity between industry's dominant presence in conventional AI research and its limited engagement in responsible AI. Leading AI firms exhibit significantly lower output in responsible AI research compared to their conventional AI research and the contributions of leading academic institutions. Our linguistic analysis documents a narrower scope of responsible AI research within industry, with a lack of diversity in key topics addressed. Our large-scale patent citation analysis uncovers a pronounced disconnect between responsible AI research and the commercialization of AI technologies, suggesting that industry patents rarely build upon insights generated by the responsible AI literature. This gap highlights the potential for AI development to diverge from a socially optimal path, risking unintended consequences due to insufficient consideration of ethical and societal implications. Our results highlight the urgent need for industry to publicly engage in responsible AI research to absorb academic knowledge, cultivate public trust, and proactively mitigate AI-induced societal harms.","sentences":["The transformative potential of AI presents remarkable opportunities, but also significant risks, underscoring the importance of responsible AI development and deployment.","Despite a growing emphasis on this area, there is limited understanding of industry's engagement in responsible AI research, i.e., the critical examination of AI's ethical, social, and legal dimensions.","To address this gap, we analyzed over 6 million peer-reviewed articles and 32 million patent citations using multiple methods across five distinct datasets to quantify industry's engagement.","Our findings reveal that the majority of AI firms show limited or no engagement in this critical subfield of AI.","We show a stark disparity between industry's dominant presence in conventional AI research and its limited engagement in responsible AI.","Leading AI firms exhibit significantly lower output in responsible AI research compared to their conventional AI research and the contributions of leading academic institutions.","Our linguistic analysis documents a narrower scope of responsible AI research within industry, with a lack of diversity in key topics addressed.","Our large-scale patent citation analysis uncovers a pronounced disconnect between responsible AI research and the commercialization of AI technologies, suggesting that industry patents rarely build upon insights generated by the responsible AI literature.","This gap highlights the potential for AI development to diverge from a socially optimal path, risking unintended consequences due to insufficient consideration of ethical and societal implications.","Our results highlight the urgent need for industry to publicly engage in responsible AI research to absorb academic knowledge, cultivate public trust, and proactively mitigate AI-induced societal harms."],"url":"http://arxiv.org/abs/2405.12193v1","category":"cs.CY"}
{"created":"2024-05-20 16:31:54","title":"Risk, utility and sensitivity to large losses","abstract":"Risk and utility functionals are fundamental building blocks in economics and finance. In this paper we investigate under which conditions a risk or utility functional is sensitive to the accumulation of losses in the sense that any sufficiently large multiple of a position that exposes an agent to future losses has positive risk or negative utility. We call this property sensitivity to large losses and provide necessary and sufficient conditions thereof that are easy to check for a very large class of risk and utility functionals. In particular, our results do not rely on convexity and can therefore also be applied to most examples discussed in the recent literature, including (non-convex) star-shaped risk measures or S-shaped utility functions encountered in prospect theory. As expected, Value at Risk generally fails to be sensitive to large losses. More surprisingly, this is also true of Expected Shortfall. By contrast, expected utility functionals as well as (optimized) certainty equivalents are proved to be sensitive to large losses for many standard choices of concave and nonconcave utility functions, including $S$-shaped utility functions. We also show that Value at Risk and Expected Shortfall become sensitive to large losses if they are either properly adjusted or if the property is suitably localized.","sentences":["Risk and utility functionals are fundamental building blocks in economics and finance.","In this paper we investigate under which conditions a risk or utility functional is sensitive to the accumulation of losses in the sense that any sufficiently large multiple of a position that exposes an agent to future losses has positive risk or negative utility.","We call this property sensitivity to large losses and provide necessary and sufficient conditions thereof that are easy to check for a very large class of risk and utility functionals.","In particular, our results do not rely on convexity and can therefore also be applied to most examples discussed in the recent literature, including (non-convex) star-shaped risk measures or S-shaped utility functions encountered in prospect theory.","As expected, Value at Risk generally fails to be sensitive to large losses.","More surprisingly, this is also true of Expected Shortfall.","By contrast, expected utility functionals as well as (optimized) certainty equivalents are proved to be sensitive to large losses for many standard choices of concave and nonconcave utility functions, including $S$-shaped utility functions.","We also show that Value at Risk and Expected Shortfall become sensitive to large losses if they are either properly adjusted or if the property is suitably localized."],"url":"http://arxiv.org/abs/2405.12154v1","category":"q-fin.RM"}
{"created":"2024-05-20 16:27:25","title":"A Nearly Quadratic Improvement for Memory Reallocation","abstract":"In the Memory Reallocation Problem a set of items of various sizes must be dynamically assigned to non-overlapping contiguous chunks of memory. It is guaranteed that the sum of the sizes of all items present at any time is at most a $(1-\\varepsilon)$-fraction of the total size of memory (i.e., the load-factor is at most $1-\\varepsilon$). The allocator receives insert and delete requests online, and can re-arrange existing items to handle the requests, but at a reallocation cost defined to be the sum of the sizes of items moved divided by the size of the item being inserted/deleted.   The folklore algorithm for Memory Reallocation achieves a cost of $O(\\varepsilon^{-1})$ per update. In recent work at FOCS'23, Kuszmaul showed that, in the special case where each item is promised to be smaller than an $\\varepsilon^4$-fraction of memory, it is possible to achieve expected update cost $O(\\log\\varepsilon^{-1})$. Kuszmaul conjectures, however, that for larger items the folklore algorithm is optimal.   In this work we disprove Kuszmaul's conjecture, giving an allocator that achieves expected update cost $O(\\varepsilon^{-1/2} \\operatorname*{polylog} \\varepsilon^{-1})$ on any input sequence. We also give the first non-trivial lower bound for the Memory Reallocation Problem: we demonstrate an input sequence on which any resizable allocator (even offline) must incur amortized update cost at least $\\Omega(\\log\\varepsilon^{-1})$.   Finally, we analyze the Memory Reallocation Problem on a stochastic sequence of inserts and deletes, with random sizes in $[\\delta, 2 \\delta]$ for some $\\delta$. We show that, in this simplified setting, it is possible to achieve $O(\\log\\varepsilon^{-1})$ expected update cost, even in the ``large item'' parameter regime ($\\delta > \\varepsilon^4$).","sentences":["In the Memory Reallocation Problem a set of items of various sizes must be dynamically assigned to non-overlapping contiguous chunks of memory.","It is guaranteed that the sum of the sizes of all items present at any time is at most a $(1-\\varepsilon)$-fraction of the total size of memory (i.e., the load-factor is at most $1-\\varepsilon$).","The allocator receives insert and delete requests online, and can re-arrange existing items to handle the requests, but at a reallocation cost defined to be the sum of the sizes of items moved divided by the size of the item being inserted/deleted.   ","The folklore algorithm for Memory Reallocation achieves a cost of $O(\\varepsilon^{-1})$ per update.","In recent work at FOCS'23, Kuszmaul showed that, in the special case where each item is promised to be smaller than an $\\varepsilon^4$-fraction of memory, it is possible to achieve expected update cost $O(\\log\\varepsilon^{-1})$. Kuszmaul conjectures, however, that for larger items the folklore algorithm is optimal.   ","In this work we disprove Kuszmaul's conjecture, giving an allocator that achieves expected update cost $O(\\varepsilon^{-1/2} \\operatorname*{polylog} \\varepsilon^{-1})$ on any input sequence.","We also give the first non-trivial lower bound for the Memory Reallocation Problem: we demonstrate an input sequence on which any resizable allocator (even offline) must incur amortized update cost at least $\\Omega(\\log\\varepsilon^{-1})$.   Finally, we analyze the Memory Reallocation Problem on a stochastic sequence of inserts and deletes, with random sizes in $[\\delta, 2 \\delta]$ for some $\\delta$. We show that, in this simplified setting, it is possible to achieve $O(\\log\\varepsilon^{-1})$ expected update cost, even in the ``large item'' parameter regime ($\\delta > \\varepsilon^4$)."],"url":"http://arxiv.org/abs/2405.12152v1","category":"cs.DS"}
{"created":"2024-05-20 15:53:50","title":"Two-dimensional signal-dependent parabolic-elliptic Keller-Segel system and its means field derivation","abstract":"In this paper, the well-posedness of two-dimensional signal-dependent Keller-Segel system and its mean field derivation from a interacting particle system on the whole space are investigated. The signal dependence effect is reflected by the fact that the diffusion coefficient in the particle system depends nonlinearly on the interactions between the individuals. Therefore, the mathematical challenge in studying the well-posedness of this system lies in the possible degeneracy and the aggregation effect when the concentration of signal becomes unbounded. The well-established method on bounded domain, to obtain the appropriate estimates for the signal concentration, is invalid for the whole space case. Motivated by the entropy minimization method and Onofri's inequality, which has been successfully applied for parabolic-parabolic Keller-Segel system, we establish a complete entropy estimate benefited from linear diffusion term, which plays important role in obtaining the Lp estimates for the solution. Furthermore, the upper bound for the concentration of signal is obtained. Based on estimates we obtained for the density of cells, the rigorous mean-field derivation is proved by introducing an intermediate particle system with a mollified interaction potential with logarithmic scaling. By using this mollification, we obtain the convergence of the particle trajectories in expectation, which implies the weak propagation of chaos. Additionally, under a regularity assumption of the cell-density, we derive the strong L1 convergence for the propagation of chaos by using relative entropy method.","sentences":["In this paper, the well-posedness of two-dimensional signal-dependent Keller-Segel system and its mean field derivation from a interacting particle system on the whole space are investigated.","The signal dependence effect is reflected by the fact that the diffusion coefficient in the particle system depends nonlinearly on the interactions between the individuals.","Therefore, the mathematical challenge in studying the well-posedness of this system lies in the possible degeneracy and the aggregation effect when the concentration of signal becomes unbounded.","The well-established method on bounded domain, to obtain the appropriate estimates for the signal concentration, is invalid for the whole space case.","Motivated by the entropy minimization method and Onofri's inequality, which has been successfully applied for parabolic-parabolic Keller-Segel system, we establish a complete entropy estimate benefited from linear diffusion term, which plays important role in obtaining the Lp estimates for the solution.","Furthermore, the upper bound for the concentration of signal is obtained.","Based on estimates we obtained for the density of cells, the rigorous mean-field derivation is proved by introducing an intermediate particle system with a mollified interaction potential with logarithmic scaling.","By using this mollification, we obtain the convergence of the particle trajectories in expectation, which implies the weak propagation of chaos.","Additionally, under a regularity assumption of the cell-density, we derive the strong L1 convergence for the propagation of chaos by using relative entropy method."],"url":"http://arxiv.org/abs/2405.12134v1","category":"math.AP"}
{"created":"2024-05-20 15:31:32","title":"Clap: a Rust eDSL for PlonKish Proof Systems with a Semantics-preserving Optimizing Compiler","abstract":"Writing Plonkish constraint systems by hand is tedious and error-prone; as a result, several libraries and DSL's have emerged over the years to facilitate this task as well as techniques to directly analyze constraint systems. However, standalone languages require developers to use a foreign toolchain and leave gaps between the application and its circuits. On the other hand, Rust-embedded DSL like Halo2 or Boojum lack in modularity; furthermore, it is usually impossible to tease apart the circuit from the proof system, making it hard to reuse circuits and even to compare performance of different proof systems on the same circuits.   In this paper we introduce Clap, the first Rust eDSL to propose a prover-agnostic circuit format that enables extensibility, automatic optimizations, and formal guarantees for the resulting constraint system. Clap generates Plonkish constraint systems and witness generators that are sound and complete with respect to each other, leaving no room for subtle bugs due to under- or over-constraining. A model of this equivalence is proved in the Agda proof assistant for a subset of Clap's Rust implementation that is expressive enough to capture the compositional properties of our format. In order to increase the reuse of circuits, a number of optimizations are carried out automatically, sparing the developer from over-specifying low-level constraint system details in their circuit descriptions. We test the expressivity and efficiency of Clap on an implementation of the Poseidon2 hash function that produces a constraint system that is competitive in terms of size with hand-optimized Boojum circuits.","sentences":["Writing Plonkish constraint systems by hand is tedious and error-prone; as a result, several libraries and DSL's have emerged over the years to facilitate this task as well as techniques to directly analyze constraint systems.","However, standalone languages require developers to use a foreign toolchain and leave gaps between the application and its circuits.","On the other hand, Rust-embedded DSL like Halo2 or Boojum lack in modularity; furthermore, it is usually impossible to tease apart the circuit from the proof system, making it hard to reuse circuits and even to compare performance of different proof systems on the same circuits.   ","In this paper we introduce Clap, the first Rust eDSL to propose a prover-agnostic circuit format that enables extensibility, automatic optimizations, and formal guarantees for the resulting constraint system.","Clap generates Plonkish constraint systems and witness generators that are sound and complete with respect to each other, leaving no room for subtle bugs due to under- or over-constraining.","A model of this equivalence is proved in the Agda proof assistant for a subset of Clap's Rust implementation that is expressive enough to capture the compositional properties of our format.","In order to increase the reuse of circuits, a number of optimizations are carried out automatically, sparing the developer from over-specifying low-level constraint system details in their circuit descriptions.","We test the expressivity and efficiency of Clap on an implementation of the Poseidon2 hash function that produces a constraint system that is competitive in terms of size with hand-optimized Boojum circuits."],"url":"http://arxiv.org/abs/2405.12115v1","category":"cs.CR"}
{"created":"2024-05-20 15:05:47","title":"Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?","abstract":"Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL), yet it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power. Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences. As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba in offline RL (dubbed DeMa) from the aspect of data structures and network architectures with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements due to the fact that DeMa's focus on sequences diminishes approximately exponentially. Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) For the components of DeMa, we identify that the hidden attention mechanism is key to its success, which can also work well with other residual structures and does not require position embedding. Extensive evaluations from eight Atari games demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous state-of-the-art methods, outdoing Decision Transformer (DT) by 80\\% with 30\\% fewer parameters, and exceeds DT in MuJoCo with only a quarter of the parameters.","sentences":["Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL), yet it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power.","Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences.","As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba in offline RL (dubbed DeMa) from the aspect of data structures and network architectures with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements due to the fact that DeMa's focus on sequences diminishes approximately exponentially.","Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa.","(2) For the components of DeMa, we identify that the hidden attention mechanism is key to its success, which can also work well with other residual structures and does not require position embedding.","Extensive evaluations from eight Atari games demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous state-of-the-art methods, outdoing Decision Transformer (DT) by 80\\% with 30\\% fewer parameters, and exceeds DT in MuJoCo with only a quarter of the parameters."],"url":"http://arxiv.org/abs/2405.12094v1","category":"cs.LG"}
{"created":"2024-05-20 14:57:16","title":"Channel Balance Interpolation in the Lightning Network via Machine Learning","abstract":"The Bitcoin Lightning Network is a Layer 2 payment protocol that addresses Bitcoin's scalability by facilitating quick and cost effective transactions through payment channels. This research explores the feasibility of using machine learning models to interpolate channel balances within the network, which can be used for optimizing the network's pathfinding algorithms. While there has been much exploration in balance probing and multipath payment protocols, predicting channel balances using solely node and channel features remains an uncharted area. This paper evaluates the performance of several machine learning models against two heuristic baselines and investigates the predictive capabilities of various features. Our model performs favorably in experimental evaluation, outperforming by 10% against an equal split baseline where both edges are assigned half of the channel capacity.","sentences":["The Bitcoin Lightning Network is a Layer 2 payment protocol that addresses Bitcoin's scalability by facilitating quick and cost effective transactions through payment channels.","This research explores the feasibility of using machine learning models to interpolate channel balances within the network, which can be used for optimizing the network's pathfinding algorithms.","While there has been much exploration in balance probing and multipath payment protocols, predicting channel balances using solely node and channel features remains an uncharted area.","This paper evaluates the performance of several machine learning models against two heuristic baselines and investigates the predictive capabilities of various features.","Our model performs favorably in experimental evaluation, outperforming by 10% against an equal split baseline where both edges are assigned half of the channel capacity."],"url":"http://arxiv.org/abs/2405.12087v1","category":"cs.LG"}
{"created":"2024-05-20 14:37:12","title":"QuanEstimation.jl: An open-source Julia framework for quantum parameter estimation","abstract":"As the main theoretical support of quantum metrology, quantum parameter estimation must follow the steps of quantum metrology towards the applied science and industry. Hence, optimal scheme design will soon be a crucial and core task for quantum parameter estimation. To efficiently accomplish this task, software packages aimed at computer-aided design are in high demand. In response to this need, we hereby introduce QuanEstimation.jl, an open-source Julia framework for scheme evaluation and design in quantum parameter estimation. It can be used either as an independent package or as the computational core of the recently developed hybrid-language (Python-Julia) package QuanEstimation [Phys. Rev. Res. 4, 043057 (2022)]. Utilizing this framework, the scheme evaluation and design in quantum parameter estimation can be readily performed, especially when quantum noises exist.","sentences":["As the main theoretical support of quantum metrology, quantum parameter estimation must follow the steps of quantum metrology towards the applied science and industry.","Hence, optimal scheme design will soon be a crucial and core task for quantum parameter estimation.","To efficiently accomplish this task, software packages aimed at computer-aided design are in high demand.","In response to this need, we hereby introduce QuanEstimation.jl, an open-source Julia framework for scheme evaluation and design in quantum parameter estimation.","It can be used either as an independent package or as the computational core of the recently developed hybrid-language (Python-Julia) package QuanEstimation [Phys. Rev. Res. 4, 043057 (2022)].","Utilizing this framework, the scheme evaluation and design in quantum parameter estimation can be readily performed, especially when quantum noises exist."],"url":"http://arxiv.org/abs/2405.12066v1","category":"quant-ph"}
{"created":"2024-05-20 14:34:09","title":"Approximating Multi-Dimensional and Multiband Signals","abstract":"We study the problem of representing a discrete tensor that comes from finite uniform samplings of a multi-dimensional and multiband analog signal. Particularly, we consider two typical cases in which the shape of the subbands is cubic or parallelepipedic. For the cubic case, by examining the spectrum of its corresponding time- and band-limited operators, we obtain a low-dimensional optimal dictionary to represent the original tensor. We further prove that the optimal dictionary can be approximated by the famous \\ac{dpss} with certain modulation, leading to an efficient constructing method. For the parallelepipedic case, we show that there also exists a low-dimensional dictionary to represent the original tensor. We present rigorous proof that the numbers of atoms in both dictionaries are approximately equal to the dot of the total number of samplings and the total volume of the subbands. Our derivations are mainly focused on the \\ac{2d} scenarios but can be naturally extended to high dimensions.","sentences":["We study the problem of representing a discrete tensor that comes from finite uniform samplings of a multi-dimensional and multiband analog signal.","Particularly, we consider two typical cases in which the shape of the subbands is cubic or parallelepipedic.","For the cubic case, by examining the spectrum of its corresponding time- and band-limited operators, we obtain a low-dimensional optimal dictionary to represent the original tensor.","We further prove that the optimal dictionary can be approximated by the famous \\ac{dpss} with certain modulation, leading to an efficient constructing method.","For the parallelepipedic case, we show that there also exists a low-dimensional dictionary to represent the original tensor.","We present rigorous proof that the numbers of atoms in both dictionaries are approximately equal to the dot of the total number of samplings and the total volume of the subbands.","Our derivations are mainly focused on the \\ac{2d} scenarios but can be naturally extended to high dimensions."],"url":"http://arxiv.org/abs/2405.12064v1","category":"eess.SP"}
{"created":"2024-05-20 14:08:32","title":"Earthquakes and the wealth of nations: The cases of Chile and New Zealand","abstract":"The consequences of natural disasters, such as earthquakes, are evident: death, coordination problems, destruction of infrastructure, and displacement of population. However, according to empirical research, the impact of a natural disaster on economic activity is mixed. Natural disasters could have significant economic effects, especially in developing economies. This is particularly important for highly seismic countries such as Chile and New Zealand. This paper contributes to the literature on natural disasters and economic development by analyzing the cases of two affected regions within these countries in the wake of major earthquakes experienced during 2010-2011: Maule (Chile) and Canterbury (New Zealand). We examine the impact of natural disasters on GDP per capita by applying the synthetic control method. Using the synthetic approach, we assess the effects of these two earthquakes by building counterfactuals to compare their recovery trajectories. We find that Chile and New Zealand experienced opposite economic effects. The Canterbury region grew 10% more in three years than its synthetic counterfactual without the earthquake, while the Maule region declined by 5%. We build synthetic controls at a regional and economic-sector level, looking at aggregated and sectoral effects. The difference in institutions, such as property rights and the large amount of government spending given for reconstruction after the New Zealand earthquake relative to Chile's, help to explain the difference in outcomes.","sentences":["The consequences of natural disasters, such as earthquakes, are evident: death, coordination problems, destruction of infrastructure, and displacement of population.","However, according to empirical research, the impact of a natural disaster on economic activity is mixed.","Natural disasters could have significant economic effects, especially in developing economies.","This is particularly important for highly seismic countries such as Chile and New Zealand.","This paper contributes to the literature on natural disasters and economic development by analyzing the cases of two affected regions within these countries in the wake of major earthquakes experienced during 2010-2011:","Maule (Chile) and Canterbury (New Zealand).","We examine the impact of natural disasters on GDP per capita by applying the synthetic control method.","Using the synthetic approach, we assess the effects of these two earthquakes by building counterfactuals to compare their recovery trajectories.","We find that Chile and New Zealand experienced opposite economic effects.","The Canterbury region grew 10% more in three years than its synthetic counterfactual without the earthquake, while the Maule region declined by 5%.","We build synthetic controls at a regional and economic-sector level, looking at aggregated and sectoral effects.","The difference in institutions, such as property rights and the large amount of government spending given for reconstruction after the New Zealand earthquake relative to Chile's, help to explain the difference in outcomes."],"url":"http://arxiv.org/abs/2405.12041v1","category":"econ.GN"}
{"created":"2024-05-20 14:06:45","title":"Randomized Gradient Descents on Riemannian Manifolds: Almost Sure Convergence to Global Minima in and beyond Quantum Optimization","abstract":"We analyze the convergence properties of gradient descent algorithms on Riemannian manifolds. We study randomization of the tangent space directions of Riemannian gradient flows for minimizing smooth cost functions (of Morse--Bott type) to obtain convergence to local optima. We prove that through randomly projecting Riemannian gradients according to the Haar measure, convergence to local optima can be obtained almost surely despite the existence of saddle points. As an application we consider ground state preparation through quantum optimization over the unitary group. In this setting one can efficiently approximate the Haar-random projections by implementing unitary 2-designs on quantum computers. We prove that the respective algorithm almost surely converges to the global minimum that corresponds to the ground state of a desired Hamiltonian. Finally, we discuss the time required by the algorithm to pass a saddle point in a simple two-dimensional setting.","sentences":["We analyze the convergence properties of gradient descent algorithms on Riemannian manifolds.","We study randomization of the tangent space directions of Riemannian gradient flows for minimizing smooth cost functions (of Morse--Bott type) to obtain convergence to local optima.","We prove that through randomly projecting Riemannian gradients according to the Haar measure, convergence to local optima can be obtained almost surely despite the existence of saddle points.","As an application we consider ground state preparation through quantum optimization over the unitary group.","In this setting one can efficiently approximate the Haar-random projections by implementing unitary 2-designs on quantum computers.","We prove that the respective algorithm almost surely converges to the global minimum that corresponds to the ground state of a desired Hamiltonian.","Finally, we discuss the time required by the algorithm to pass a saddle point in a simple two-dimensional setting."],"url":"http://arxiv.org/abs/2405.12039v1","category":"math.OC"}
{"created":"2024-05-20 13:51:12","title":"The Case for DeepSOH: Addressing Path Dependency for Remaining Useful Life","abstract":"The battery state of health (SOH) based on capacity fade and resistance increase is not sufficient for predicting Remaining Useful life (RUL). The electrochemical community blames the path-dependency of the battery degradation mechanisms for our inability to forecast the degradation. The control community knows that the path-dependency is addressed by full state estimation. We show that even the electrode-specific SOH (eSOH) estimation is not enough to fully define the degradation states by simulating infinite possible degradation trajectories and remaining useful lives (RUL) from a unique eSOH. We finally define the deepSOH states that capture the individual contributions of all the common degradation mechanisms, namely, SEI, plating, and mechanical fracture to the loss of lithium inventory. We show that the addition of cell expansion measurement may allow us to estimate the deepSOH and predict the remaining useful life.","sentences":["The battery state of health (SOH) based on capacity fade and resistance increase is not sufficient for predicting Remaining Useful life (RUL).","The electrochemical community blames the path-dependency of the battery degradation mechanisms for our inability to forecast the degradation.","The control community knows that the path-dependency is addressed by full state estimation.","We show that even the electrode-specific SOH (eSOH) estimation is not enough to fully define the degradation states by simulating infinite possible degradation trajectories and remaining useful lives (RUL) from a unique eSOH.","We finally define the deepSOH states that capture the individual contributions of all the common degradation mechanisms, namely, SEI, plating, and mechanical fracture to the loss of lithium inventory.","We show that the addition of cell expansion measurement may allow us to estimate the deepSOH and predict the remaining useful life."],"url":"http://arxiv.org/abs/2405.12028v1","category":"eess.SY"}
{"created":"2024-05-20 13:24:59","title":"The efficiency of black hole formation via collisions in stellar systems: An analysis of data from simulations and observations","abstract":"This paper explores the theoretical relation between star clusters and black holes within, focusing on the potential role of Nuclear Star Clusters (NSCs), Globular Clusters (GCs), and Ultra Compact Dwarf Galaxies (UCDs) as environments that lead to black hole formation through stellar collisions. The study aims to identify optimal conditions for stellar collisions in different stellar systems leading to the formation of very massive stars that subsequently collapse into black holes. Data from numerical simulations and observations of diverse stellar systems are analyzed, encompassing various initial conditions, initial mass functions, and stellar evolution scenarios. We compute a critical mass, determined by the interplay of collision time, system age, and initial properties of the star cluster. The efficiency of black hole formation ($\\epsilon_{\\mathrm{BH}}$) is defined as the ratio of initial stellar mass divided by critical mass. The study finds out that stellar systems with a ratio of initial stellar mass over critical mass above 1 exhibit high efficiencies of black hole formation, ranging from $30-100\\%$. While there is some scatter, potentially attributed to complex system histories and the presence of gas, the results highlight the potential for achieving high efficiencies through a purely collisional channel in black hole formation. In conclusion, this theoretical exploration elucidates the connection between star clusters and black hole formation. The study underscores the significance of UCDs, GCs, and NSCs as environments conducive to stellar collisions leading to black hole formation. The defined black hole formation efficiency ($\\epsilon_{\\mathrm{BH}}$) is shown to be influenced by the ratio of initial stellar mass to critical mass.","sentences":["This paper explores the theoretical relation between star clusters and black holes within, focusing on the potential role of Nuclear Star Clusters (NSCs), Globular Clusters (GCs), and Ultra Compact Dwarf Galaxies (UCDs) as environments that lead to black hole formation through stellar collisions.","The study aims to identify optimal conditions for stellar collisions in different stellar systems leading to the formation of very massive stars that subsequently collapse into black holes.","Data from numerical simulations and observations of diverse stellar systems are analyzed, encompassing various initial conditions, initial mass functions, and stellar evolution scenarios.","We compute a critical mass, determined by the interplay of collision time, system age, and initial properties of the star cluster.","The efficiency of black hole formation ($\\epsilon_{\\mathrm{BH}}$) is defined as the ratio of initial stellar mass divided by critical mass.","The study finds out that stellar systems with a ratio of initial stellar mass over critical mass above 1 exhibit high efficiencies of black hole formation, ranging from $30-100\\%$. While there is some scatter, potentially attributed to complex system histories and the presence of gas, the results highlight the potential for achieving high efficiencies through a purely collisional channel in black hole formation.","In conclusion, this theoretical exploration elucidates the connection between star clusters and black hole formation.","The study underscores the significance of UCDs, GCs, and NSCs as environments conducive to stellar collisions leading to black hole formation.","The defined black hole formation efficiency ($\\epsilon_{\\mathrm{BH}}$) is shown to be influenced by the ratio of initial stellar mass to critical mass."],"url":"http://arxiv.org/abs/2405.12008v1","category":"astro-ph.GA"}
{"created":"2024-05-20 13:11:57","title":"Multi-Agent Optimization and Learning: A Non-Expansive Operators Perspective","abstract":"Multi-agent systems are increasingly widespread in a range of application domains, with optimization and learning underpinning many of the tasks that arise in this context. Different approaches have been proposed to enable the cooperative solution of these optimization and learning problems, including first- and second-order methods, and dual (or Lagrangian) methods, all of which rely on consensus and message-passing. In this article we discuss these algorithms through the lens of non-expansive operator theory, providing a unifying perspective. We highlight the insights that this viewpoint delivers, and discuss how it can spark future original research.","sentences":["Multi-agent systems are increasingly widespread in a range of application domains, with optimization and learning underpinning many of the tasks that arise in this context.","Different approaches have been proposed to enable the cooperative solution of these optimization and learning problems, including first- and second-order methods, and dual (or Lagrangian) methods, all of which rely on consensus and message-passing.","In this article we discuss these algorithms through the lens of non-expansive operator theory, providing a unifying perspective.","We highlight the insights that this viewpoint delivers, and discuss how it can spark future original research."],"url":"http://arxiv.org/abs/2405.11999v1","category":"math.OC"}
{"created":"2024-05-20 12:36:20","title":"Scheduling Jobs with Work-Inefficient Parallel Solutions","abstract":"This paper introduces the \\emph{serial-parallel decision problem}. Consider an online scheduler that receives a series of tasks, where each task has both a parallel and a serial implementation. The parallel implementation has the advantage that it can make progress concurrently on multiple processors, but the disadvantage that it is (potentially) work-inefficient. As tasks arrive, the scheduler must decide for each task which implementation to use.   We begin by studying \\emph{total awake time}. We give a simple \\emph{decide-on-arrival} scheduler that achieves a competitive ratio of $3$ for total awake time -- this scheduler makes serial/parallel decisions immediately when jobs arrive. Our second result is an \\emph{parallel-work-oblivious} scheduler that achieves a competitive ratio of $6$ for total awake time -- this scheduler makes all of its decisions based only on the size of each serial job and without needing to know anything about the parallel implementations. Finally, we prove a lower bound showing that, if a scheduler wishes to achieve a competitive ratio of $O(1)$, it can have at most one of the two aforementioned properties (decide-on-arrival or parallel-work-oblivious). We also prove lower bounds of the form $1 + \\Omega(1)$ on the optimal competitive ratio for any scheduler.   Next, we turn our attention to optimizing \\emph{mean response time}. Here, we show that it is possible to achieve an $O(1)$ competitive ratio with $O(1)$ speed augmentation. This is the most technically involved of our results. We also prove that, in this setting, it is not possible for a parallel-work-oblivious scheduler to do well.   In addition to these results, we present tight bounds on the optimal competitive ratio if we allow for arrival dependencies between tasks (e.g., tasks are components of a single parallel program), and we give an in-depth discussion of the remaining open questions.","sentences":["This paper introduces the \\emph{serial-parallel decision problem}.","Consider an online scheduler that receives a series of tasks, where each task has both a parallel and a serial implementation.","The parallel implementation has the advantage that it can make progress concurrently on multiple processors, but the disadvantage that it is (potentially) work-inefficient.","As tasks arrive, the scheduler must decide for each task which implementation to use.   ","We begin by studying \\emph{total awake time}.","We give a simple \\emph{decide-on-arrival} scheduler that achieves a competitive ratio of $3$ for total awake time -- this scheduler makes serial/parallel decisions immediately when jobs arrive.","Our second result is an \\emph{parallel-work-oblivious} scheduler that achieves a competitive ratio of $6$ for total awake time -- this scheduler makes all of its decisions based only on the size of each serial job and without needing to know anything about the parallel implementations.","Finally, we prove a lower bound showing that, if a scheduler wishes to achieve a competitive ratio of $O(1)$, it can have at most one of the two aforementioned properties (decide-on-arrival or parallel-work-oblivious).","We also prove lower bounds of the form $1 + \\Omega(1)$ on the optimal competitive ratio for any scheduler.   ","Next, we turn our attention to optimizing \\emph{mean response time}.","Here, we show that it is possible to achieve an $O(1)$ competitive ratio with $O(1)$ speed augmentation.","This is the most technically involved of our results.","We also prove that, in this setting, it is not possible for a parallel-work-oblivious scheduler to do well.   ","In addition to these results, we present tight bounds on the optimal competitive ratio if we allow for arrival dependencies between tasks (e.g., tasks are components of a single parallel program), and we give an in-depth discussion of the remaining open questions."],"url":"http://arxiv.org/abs/2405.11986v1","category":"cs.DS"}
{"created":"2024-05-20 12:22:39","title":"Tutorial on Silicon Photonics Integrated Platform Fiber Edge Coupling","abstract":"Photonic integrated circuits (PICs) play a crucial role in almost every aspect of modern life, such as data storage, telecommunications, medical diagnostics, green energy, autonomous driving, agriculture, and high-performance computing. To fully harness their benefits, an efficient coupling mechanism is required to successfully launch light into waveguides from fibers. This study introduces low-loss coupling strategies and their implementation for a silicon nitride integrated platform. Here we present an overview of coupling technologies, optimized designs, and a tutorial on manufacturing techniques for inverted tapers, which enable effective coupling for both transverse-magnetic and transverse-electric modes. The optimized coupling losses for the UHNA-7 fiber and the inverted taper Si3N4 coupler reached -0.81 dB at 1550 nm per connection for single-mode waveguides with 220x1200 nm cross section. The measured coupling losses in the inverted taper coupler with a standard single-mode fiber were -3.28 dB at 1550 nm per connection for the same platform.","sentences":["Photonic integrated circuits (PICs) play a crucial role in almost every aspect of modern life, such as data storage, telecommunications, medical diagnostics, green energy, autonomous driving, agriculture, and high-performance computing.","To fully harness their benefits, an efficient coupling mechanism is required to successfully launch light into waveguides from fibers.","This study introduces low-loss coupling strategies and their implementation for a silicon nitride integrated platform.","Here we present an overview of coupling technologies, optimized designs, and a tutorial on manufacturing techniques for inverted tapers, which enable effective coupling for both transverse-magnetic and transverse-electric modes.","The optimized coupling losses for the UHNA-7 fiber and the inverted taper Si3N4 coupler reached -0.81 dB at 1550 nm per connection for single-mode waveguides with 220x1200 nm cross section.","The measured coupling losses in the inverted taper coupler with a standard single-mode fiber were -3.28 dB at 1550 nm per connection for the same platform."],"url":"http://arxiv.org/abs/2405.11980v1","category":"physics.optics"}
{"created":"2024-05-20 12:04:05","title":"Structured singular values and their application in computing eigenvalue backward errors of the Rosenbrock system matrix","abstract":"The structured singular values (aka the {\\mu}-values) are essential in analyzing the stability of control systems and in the structured eigenvalue perturbation theory of matrices and matrix polynomials. In this paper, we study the {\\mu}-value of a matrix under block-diagonal structured perturbations (full blocks but possibly rectangular). We provide an explicit expression for the {\\mu}-value and also obtain a computable upper bound in terms of minimizing the largest singular value of a parameter-dependent matrix. This upper bound equals the {\\mu}-value when the perturbation matrix has no more than three blocks on the diagonal. We then apply the {\\mu}-value results in computing eigenvalue backward errors of a Rosenbrock system matrix corresponding to a rational matrix function when some or all blocks of the Rosenbrock system matrix are subject to perturbation. The results are illustrated through numerical experiments.","sentences":["The structured singular values (aka the {\\mu}-values) are essential in analyzing the stability of control systems and in the structured eigenvalue perturbation theory of matrices and matrix polynomials.","In this paper, we study the {\\mu}-value of a matrix under block-diagonal structured perturbations (full blocks but possibly rectangular).","We provide an explicit expression for the {\\mu}-value and also obtain a computable upper bound in terms of minimizing the largest singular value of a parameter-dependent matrix.","This upper bound equals the {\\mu}-value when the perturbation matrix has no more than three blocks on the diagonal.","We then apply the {\\mu}-value results in computing eigenvalue backward errors of a Rosenbrock system matrix corresponding to a rational matrix function when some or all blocks of the Rosenbrock system matrix are subject to perturbation.","The results are illustrated through numerical experiments."],"url":"http://arxiv.org/abs/2405.11974v1","category":"math.OC"}
{"created":"2024-05-20 11:37:29","title":"A Simulation Tool for V2G Enabled Demand Response Based on Model Predictive Control","abstract":"Integrating electric vehicles (EVs) into the power grid can revolutionize energy management strategies, offering both challenges and opportunities for creating a more sustainable and resilient grid. In this context, model predictive control (MPC) emerges as a powerful tool for addressing the complexities of Grid-to-vehicle (G2V) and vehicle-to-grid (V2G) enabled demand response management. By leveraging advanced optimization techniques, MPC algorithms can anticipate future grid conditions and dynamically adjust EV charging and discharging schedules to balance supply and demand while minimizing operational costs and maximizing flexibility. However, no standard tools exist to evaluate novel energy management strategies based on MPC approaches. Our research focuses on harnessing the potential of MPC in G2V and V2G applications, by providing a simulation tool that allows to maximize EV flexibility and support demand response initiatives while mitigating the impact on EV battery health. In this paper, we propose an open-source MPC controller for G2V and V2G-enabled demand response management. The proposed approach is capable of tackling the uncertainties inherent in demand response operations. Through extensive simulation and analysis, we demonstrate the efficacy of our approach in maximizing the benefits of G2V and V2G while assessing the impact on the longevity and reliability of EV batteries. Specifically, our controller enables Charge Point Operators (CPOs) to optimize EV charging and discharging schedules in real-time, taking into account fluctuating energy prices, grid constraints, and EV user preferences.","sentences":["Integrating electric vehicles (EVs) into the power grid can revolutionize energy management strategies, offering both challenges and opportunities for creating a more sustainable and resilient grid.","In this context, model predictive control (MPC) emerges as a powerful tool for addressing the complexities of Grid-to-vehicle (G2V) and vehicle-to-grid (V2G) enabled demand response management.","By leveraging advanced optimization techniques, MPC algorithms can anticipate future grid conditions and dynamically adjust EV charging and discharging schedules to balance supply and demand while minimizing operational costs and maximizing flexibility.","However, no standard tools exist to evaluate novel energy management strategies based on MPC approaches.","Our research focuses on harnessing the potential of MPC in G2V and V2G applications, by providing a simulation tool that allows to maximize EV flexibility and support demand response initiatives while mitigating the impact on EV battery health.","In this paper, we propose an open-source MPC controller for G2V and V2G-enabled demand response management.","The proposed approach is capable of tackling the uncertainties inherent in demand response operations.","Through extensive simulation and analysis, we demonstrate the efficacy of our approach in maximizing the benefits of G2V and V2G while assessing the impact on the longevity and reliability of EV batteries.","Specifically, our controller enables Charge Point Operators (CPOs) to optimize EV charging and discharging schedules in real-time, taking into account fluctuating energy prices, grid constraints, and EV user preferences."],"url":"http://arxiv.org/abs/2405.11963v1","category":"eess.SY"}
{"created":"2024-05-20 11:36:47","title":"Gait controllability of length-changing slender microswimmers","abstract":"Controllability results of four models of two-link microscale swimmers that are able to change the length of their links are obtained. The problems are formulated in the framework of Geometric Control Theory, within which the notions of fiber, total, and gait controllability are presented, together with sufficient conditions for the latter two. The dynamics of a general two-link swimmer is described by resorting to Resistive Force Theory and different mechanisms to produce a length-change in the links, namely, active deformation, a sliding hinge, growth at the tip, and telescopic links. Total controllability is proved via gait controllability in all four cases, and illustrated with the aid of numerical simulations.","sentences":["Controllability results of four models of two-link microscale swimmers that are able to change the length of their links are obtained.","The problems are formulated in the framework of Geometric Control Theory, within which the notions of fiber, total, and gait controllability are presented, together with sufficient conditions for the latter two.","The dynamics of a general two-link swimmer is described by resorting to Resistive Force Theory and different mechanisms to produce a length-change in the links, namely, active deformation, a sliding hinge, growth at the tip, and telescopic links.","Total controllability is proved via gait controllability in all four cases, and illustrated with the aid of numerical simulations."],"url":"http://arxiv.org/abs/2405.11961v1","category":"math.OC"}
{"created":"2024-05-20 11:36:37","title":"Dynamic classifier auditing by unsupervised anomaly detection methods: an application in packaging industry predictive maintenance","abstract":"Predictive maintenance in manufacturing industry applications is a challenging research field. Packaging machines are widely used in a large number of logistic companies' warehouses and must be working uninterruptedly. Traditionally, preventive maintenance strategies have been carried out to improve the performance of these machines. However, this kind of policies does not take into account the information provided by the sensors implemented in the machines. This paper presents an expert system for the automatic estimation of work orders to implement predictive maintenance policies for packaging machines. The key idea is that, from a set of alarms related to sensors implemented in the machine, the expert system should take a maintenance action while optimizing the response time. The work order estimator will act as a classifier, yielding a binary decision of whether a machine must undergo a maintenance action by a technician or not, followed by an unsupervised anomaly detection-based filtering stage to audit the classifier's output. The methods used for anomaly detection were: One-Class Support Vector Machine (OCSVM), Minimum Covariance Determinant (MCD) and a majority (hard) voting ensemble of them. All anomaly detection methods improve the performance of the baseline classifer but the best performance in terms of F1 score was obtained by the majority voting ensemble.","sentences":["Predictive maintenance in manufacturing industry applications is a challenging research field.","Packaging machines are widely used in a large number of logistic companies' warehouses and must be working uninterruptedly.","Traditionally, preventive maintenance strategies have been carried out to improve the performance of these machines.","However, this kind of policies does not take into account the information provided by the sensors implemented in the machines.","This paper presents an expert system for the automatic estimation of work orders to implement predictive maintenance policies for packaging machines.","The key idea is that, from a set of alarms related to sensors implemented in the machine, the expert system should take a maintenance action while optimizing the response time.","The work order estimator will act as a classifier, yielding a binary decision of whether a machine must undergo a maintenance action by a technician or not, followed by an unsupervised anomaly detection-based filtering stage to audit the classifier's output.","The methods used for anomaly detection were: One-Class Support Vector Machine (OCSVM), Minimum Covariance Determinant (MCD) and a majority (hard) voting ensemble of them.","All anomaly detection methods improve the performance of the baseline classifer but the best performance in terms of F1 score was obtained by the majority voting ensemble."],"url":"http://arxiv.org/abs/2405.11960v1","category":"cs.CE"}
{"created":"2024-05-20 10:53:27","title":"The extremal values of the ratio of differences of power mean, arithmetic mean, and geometric mean","abstract":"In the paper the maximum and the minimum of the ratio of the difference of the arithmetic mean and the geometric mean, and the difference of the power mean and the geometric mean of $n$ variables, is studied. A new optimization argument was used which reduces $n$ variable optimization problem to a single variable. All possible cases of the choice of the power mean and the choice of the number of variables of the means is studied. The obtained results generalize and complete the earlier results which were either for specific intervals of power means or for small number of variables of the means. Some of the results are formulated as the best constant inequalities involving interpolation of the arithmetic mean and the geometric mean.","sentences":["In the paper the maximum and the minimum of the ratio of the difference of the arithmetic mean and the geometric mean, and the difference of the power mean and the geometric mean of $n$ variables, is studied.","A new optimization argument was used which reduces $n$ variable optimization problem to a single variable.","All possible cases of the choice of the power mean and the choice of the number of variables of the means is studied.","The obtained results generalize and complete the earlier results which were either for specific intervals of power means or for small number of variables of the means.","Some of the results are formulated as the best constant inequalities involving interpolation of the arithmetic mean and the geometric mean."],"url":"http://arxiv.org/abs/2405.11947v1","category":"math.CA"}
{"created":"2024-05-20 10:36:22","title":"Infinite-Dimensional System Signature: System Signature Under the Repairable Principle","abstract":"Investigation of the reliability of technical systems is one of the application areas of stochastic processes. The reliability of a technical system is based on two main elements. The first is the connection type of the system, and the second is the distribution of the working times of the components consisting of the system. In this study, system signatures and their reliability will be calculated under the repairable principle of parallel and serial systems consisting of two components. Although there are a limited number of studies in the literature for repairable systems, there is no study on creating the signature of repairable systems. In technical systems where there is no repair principle, although the system signature has limited components, the technical systems working under the repairable principle, have infinite components of the system signature. While creating the system signature, the probability that the working time of the component that is in the state of the system failure is greater than the repair time was defined as the parameter {\\xi}. In the application part of the study, under the principle of repair, the system signature, and the reliability of the system were be successfully calculated.","sentences":["Investigation of the reliability of technical systems is one of the application areas of stochastic processes.","The reliability of a technical system is based on two main elements.","The first is the connection type of the system, and the second is the distribution of the working times of the components consisting of the system.","In this study, system signatures and their reliability will be calculated under the repairable principle of parallel and serial systems consisting of two components.","Although there are a limited number of studies in the literature for repairable systems, there is no study on creating the signature of repairable systems.","In technical systems where there is no repair principle, although the system signature has limited components, the technical systems working under the repairable principle, have infinite components of the system signature.","While creating the system signature, the probability that the working time of the component that is in the state of the system failure is greater than the repair time was defined as the parameter {\\xi}.","In the application part of the study, under the principle of repair, the system signature, and the reliability of the system were be successfully calculated."],"url":"http://arxiv.org/abs/2405.12257v1","category":"math.OC"}
{"created":"2024-05-20 10:28:41","title":"Optimal balanced-norm error estimate of the LDG method for reaction-diffusion problems II: the two-dimensional case with layer-upwind flux","abstract":"A singularly perturbed reaction-diffusion problem posed on the unit square in $\\mathbb{R}^2$ is solved numerically by a local discontinuous Galerkin (LDG) finite element method. Typical solutions of this class of problem exhibit boundary layers along the sides of the domain; these layers generally cause difficulties for numerical methods. Our LDG method handles the boundary layers by using a Shishkin mesh and also introducing the new concept of a ``layer-upwind flux\" -- a discrete flux whose values are chosen on the fine mesh (which lies inside the boundary layers) in the direction where the layer weakens. On the coarse mesh, one can use a standard central flux. No penalty terms are needed with these fluxes, unlike many other variants of the LDG method. Our choice of discrete flux makes it feasible to derive an optimal-order error analysis in a balanced norm; this norm is stronger than the usual energy norm and is a more appropriate measure for errors in computed solutions for singularly perturbed reaction-diffusion problems. It will be proved that the LDG method is usually convergent of order $O((N^{-1}\\ln N)^{k+1})$ in the balanced norm, where $N$ is the number of mesh intervals in each coordinate direction and tensor-product piecewise polynomials of degree~$k$ in each coordinate variable are used in the LDG method. This result is the first of its kind for the LDG method applied to this class of problem and is optimal for convergence on a Shishkin mesh. Its sharpness is confirmed by numerical experiments.","sentences":["A singularly perturbed reaction-diffusion problem posed on the unit square in $\\mathbb{R}^2$ is solved numerically by a local discontinuous Galerkin (LDG) finite element method.","Typical solutions of this class of problem exhibit boundary layers along the sides of the domain; these layers generally cause difficulties for numerical methods.","Our LDG method handles the boundary layers by using a Shishkin mesh and also introducing the new concept of a ``layer-upwind flux\" -- a discrete flux whose values are chosen on the fine mesh (which lies inside the boundary layers) in the direction where the layer weakens.","On the coarse mesh, one can use a standard central flux.","No penalty terms are needed with these fluxes, unlike many other variants of the LDG method.","Our choice of discrete flux makes it feasible to derive an optimal-order error analysis in a balanced norm; this norm is stronger than the usual energy norm and is a more appropriate measure for errors in computed solutions for singularly perturbed reaction-diffusion problems.","It will be proved that the LDG method is usually convergent of order $O((N^{-1}\\ln N)^{k+1})$ in the balanced norm, where $N$ is the number of mesh intervals in each coordinate direction and tensor-product piecewise polynomials of degree~$k$ in each coordinate variable are used in the LDG method.","This result is the first of its kind for the LDG method applied to this class of problem and is optimal for convergence on a Shishkin mesh.","Its sharpness is confirmed by numerical experiments."],"url":"http://arxiv.org/abs/2405.11939v1","category":"math.NA"}
{"created":"2024-05-20 09:58:27","title":"Effective Clustering on Large Attributed Bipartite Graphs","abstract":"Attributed bipartite graphs (ABGs) are an expressive data model for describing the interactions between two sets of heterogeneous nodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs. Partitioning the target node set in such graphs into k disjoint clusters (referred to as k-ABGC) finds widespread use in various domains, including social network analysis, recommendation systems, information retrieval, and bioinformatics. However, the majority of existing solutions towards k-ABGC either overlook attribute information or fail to capture bipartite graph structures accurately, engendering severely compromised result quality. The severity of these issues is accentuated in real ABGs, which often encompass millions of nodes and a sheer volume of attribute data, rendering effective k-ABGC over such graphs highly challenging.   In this paper, we propose TPO, an effective and efficient approach to k-ABGC that achieves superb clustering performance on multiple real datasets. TPO obtains high clustering quality through two major contributions: (i) a novel formulation and transformation of the k-ABGC problem based on multi-scale attribute affinity specialized for capturing attribute affinities between nodes with the consideration of their multi-hop connections in ABGs, and (ii) a highly efficient solver that includes a suite of carefully-crafted optimizations for sidestepping explicit affinity matrix construction and facilitating faster convergence. Extensive experiments, comparing TPO against 19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO measured against ground-truth labels. Moreover, compared to the state of the arts, TPO is often more than 40x faster over both small and large ABGs.","sentences":["Attributed bipartite graphs (ABGs) are an expressive data model for describing the interactions between two sets of heterogeneous nodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs.","Partitioning the target node set in such graphs into k disjoint clusters (referred to as k-ABGC) finds widespread use in various domains, including social network analysis, recommendation systems, information retrieval, and bioinformatics.","However, the majority of existing solutions towards k-ABGC either overlook attribute information or fail to capture bipartite graph structures accurately, engendering severely compromised result quality.","The severity of these issues is accentuated in real ABGs, which often encompass millions of nodes and a sheer volume of attribute data, rendering effective k-ABGC over such graphs highly challenging.   ","In this paper, we propose TPO, an effective and efficient approach to k-ABGC that achieves superb clustering performance on multiple real datasets.","TPO obtains high clustering quality through two major contributions: (i) a novel formulation and transformation of the k-ABGC problem based on multi-scale attribute affinity specialized for capturing attribute affinities between nodes with the consideration of their multi-hop connections in ABGs, and (ii) a highly efficient solver that includes a suite of carefully-crafted optimizations for sidestepping explicit affinity matrix construction and facilitating faster convergence.","Extensive experiments, comparing TPO against 19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO measured against ground-truth labels.","Moreover, compared to the state of the arts, TPO is often more than 40x faster over both small and large ABGs."],"url":"http://arxiv.org/abs/2405.11922v1","category":"cs.SI"}
{"created":"2024-05-20 09:55:20","title":"A Competitive Showcase of Quantum versus Classical Algorithms in Energy Coalition Formation","abstract":"The formation of energy communities is pivotal for advancing decentralized and sustainable energy management. Within this context, Coalition Structure Generation (CSG) emerges as a promising framework. The complexity of CSG grows rapidly with the number of agents, making classical solvers impractical for even moderate sizes (number of agents>30). Therefore, the development of advanced computational methods is essential. Motivated by this challenge, this study conducts a benchmark comparing classical solvers with quantum annealing on Dwave hardware and the Quantum Approximation Optimization Algorithm (QAOA) on both simulator and IBMQ hardware to address energy community formation. Our classical solvers include Tabu search, simulated annealing, and an exact classical solver. Our findings reveal that Dwave surpasses QAOA on hardware in terms of solution quality. Remarkably, QAOA demonstrates comparable runtime scaling with Dwave, albeit with a significantly larger prefactor. Notably, Dwave exhibits competitive performance compared to the classical solvers, achieving solutions of equal quality with more favorable runtime scaling.","sentences":["The formation of energy communities is pivotal for advancing decentralized and sustainable energy management.","Within this context, Coalition Structure Generation (CSG) emerges as a promising framework.","The complexity of CSG grows rapidly with the number of agents, making classical solvers impractical for even moderate sizes (number of agents>30).","Therefore, the development of advanced computational methods is essential.","Motivated by this challenge, this study conducts a benchmark comparing classical solvers with quantum annealing on Dwave hardware and the Quantum Approximation Optimization Algorithm (QAOA) on both simulator and IBMQ hardware to address energy community formation.","Our classical solvers include Tabu search, simulated annealing, and an exact classical solver.","Our findings reveal that Dwave surpasses QAOA on hardware in terms of solution quality.","Remarkably, QAOA demonstrates comparable runtime scaling with Dwave, albeit with a significantly larger prefactor.","Notably, Dwave exhibits competitive performance compared to the classical solvers, achieving solutions of equal quality with more favorable runtime scaling."],"url":"http://arxiv.org/abs/2405.11917v1","category":"quant-ph"}
{"created":"2024-05-20 09:51:56","title":"Two new calibration techniques of lumped-parameter mathematical models for the cardiovascular system","abstract":"Cardiocirculatory mathematical models serve as valuable tools for investigating physiological and pathological conditions of the circulatory system. To investigate the clinical condition of an individual, cardiocirculatory models need to be personalized by means of calibration methods. In this study we propose a new calibration method for a lumped-parameter cardiocirculatory model. This calibration method utilizes the correlation matrix between parameters and model outputs to calibrate the latter according to data. We test this calibration method and its combination with L-BFGS-B (Limited memory Broyden - Fletcher - Goldfarb - Shanno with Bound constraints) comparing them with the performances of L-BFGS-B alone. We show that the correlation matrix calibration method and the combined one effectively reduce the loss function of the associated optimization problem. In the case of in silico generated data, we show that the two new calibration methods are robust with respect to the initial guess of parameters and to the presence of noise in the data. Notably, the correlation matrix calibration method achieves the best results in estimating the parameters in the case of noisy data and it is faster than the combined calibration method and L-BFGS-B. Finally, we present real test case where the two new calibration methods yield results comparable to those obtained using L-BFGS-B in terms of minimizing the loss function and estimating the clinical data. This highlights the effectiveness of the new calibration methods for clinical applications.","sentences":["Cardiocirculatory mathematical models serve as valuable tools for investigating physiological and pathological conditions of the circulatory system.","To investigate the clinical condition of an individual, cardiocirculatory models need to be personalized by means of calibration methods.","In this study we propose a new calibration method for a lumped-parameter cardiocirculatory model.","This calibration method utilizes the correlation matrix between parameters and model outputs to calibrate the latter according to data.","We test this calibration method and its combination with L-BFGS-B (Limited memory Broyden - Fletcher - Goldfarb - Shanno with Bound constraints) comparing them with the performances of L-BFGS-B alone.","We show that the correlation matrix calibration method and the combined one effectively reduce the loss function of the associated optimization problem.","In the case of in silico generated data, we show that the two new calibration methods are robust with respect to the initial guess of parameters and to the presence of noise in the data.","Notably, the correlation matrix calibration method achieves the best results in estimating the parameters in the case of noisy data and it is faster than the combined calibration method and L-BFGS-B. Finally, we present real test case where the two new calibration methods yield results comparable to those obtained using L-BFGS-B in terms of minimizing the loss function and estimating the clinical data.","This highlights the effectiveness of the new calibration methods for clinical applications."],"url":"http://arxiv.org/abs/2405.11915v1","category":"math.NA"}
{"created":"2024-05-20 09:41:39","title":"Stability criteria of nonlinear generalized proportional fractional delayed systems","abstract":"This work deals with the finite time stability of generalized proportional fractional systems with time delay. First, based on the generalized proportional Gr\\\"onwall inequality, we derive an explicit criterion that enables the system trajectories to stay within a priori given sets during a pre-specified time interval, in terms of the Mittag-Leffler function. Then, we investigate the finite time stability of nonlinear nonhomogeneous delayed systems by means of an approach based on H\\\"older's and Jensen's inequalities. Numerical applications are presented to illustrate the validity and feasibility of the developed results.","sentences":["This work deals with the finite time stability of generalized proportional fractional systems with time delay.","First, based on the generalized proportional Gr\\\"onwall inequality, we derive an explicit criterion that enables the system trajectories to stay within a priori given sets during a pre-specified time interval, in terms of the Mittag-Leffler function.","Then, we investigate the finite time stability of nonlinear nonhomogeneous delayed systems by means of an approach based on H\\\"older's and Jensen's inequalities.","Numerical applications are presented to illustrate the validity and feasibility of the developed results."],"url":"http://arxiv.org/abs/2405.12256v1","category":"math.OC"}
{"created":"2024-05-20 09:28:23","title":"Sparse Attention-driven Quality Prediction for Production Process Optimization in Digital Twins","abstract":"In the process industry, optimizing production lines for long-term efficiency requires real-time monitoring and analysis of operation states to fine-tune production line parameters. However, the complexity in operational logic and the intricate coupling of production process parameters make it difficult to develop an accurate mathematical model for the entire process, thus hindering the deployment of efficient optimization mechanisms. In view of these difficulties, we propose to deploy a digital twin of the production line by digitally abstracting its physical layout and operational logic. By iteratively mapping the real-world data reflecting equipment operation status and product quality inspection in the digital twin, we adopt a quality prediction model for production process based on self-attention-enabled temporal convolutional neural networks. This model enables the data-driven state evolution of the digital twin. The digital twin takes a role of aggregating the information of actual operating conditions and the results of quality-sensitive analysis, which facilitates the optimization of process production quality with virtual-reality evolution under multi-dimensional constraints. Leveraging the digital twin model as an information-flow carrier, we extract temporal features from key process indicators and establish a production process quality prediction model based on the proposed composite neural network. Our operation experiments on a specific tobacco shredding line demonstrate that the proposed digital twin-based production process optimization method fosters seamless integration between virtual and real production lines. This integration achieves an average operating status prediction accuracy of over 98\\% and near-optimal production process control.","sentences":["In the process industry, optimizing production lines for long-term efficiency requires real-time monitoring and analysis of operation states to fine-tune production line parameters.","However, the complexity in operational logic and the intricate coupling of production process parameters make it difficult to develop an accurate mathematical model for the entire process, thus hindering the deployment of efficient optimization mechanisms.","In view of these difficulties, we propose to deploy a digital twin of the production line by digitally abstracting its physical layout and operational logic.","By iteratively mapping the real-world data reflecting equipment operation status and product quality inspection in the digital twin, we adopt a quality prediction model for production process based on self-attention-enabled temporal convolutional neural networks.","This model enables the data-driven state evolution of the digital twin.","The digital twin takes a role of aggregating the information of actual operating conditions and the results of quality-sensitive analysis, which facilitates the optimization of process production quality with virtual-reality evolution under multi-dimensional constraints.","Leveraging the digital twin model as an information-flow carrier, we extract temporal features from key process indicators and establish a production process quality prediction model based on the proposed composite neural network.","Our operation experiments on a specific tobacco shredding line demonstrate that the proposed digital twin-based production process optimization method fosters seamless integration between virtual and real production lines.","This integration achieves an average operating status prediction accuracy of over 98\\% and near-optimal production process control."],"url":"http://arxiv.org/abs/2405.11895v1","category":"cs.LG"}
{"created":"2024-05-20 09:13:47","title":"Lipschitz Continuous Allocations for Optimization Games","abstract":"In cooperative game theory, the primary focus is the equitable allocation of payoffs or costs among agents. However, in the practical applications of cooperative games, accurately representing games is challenging. In such cases, using an allocation method sensitive to small perturbations in the game can lead to various problems, including dissatisfaction among agents and the potential for manipulation by agents seeking to maximize their own benefits. Therefore, the allocation method must be robust against game perturbations.   In this study, we explore optimization games, in which the value of the characteristic function is provided as the optimal value of an optimization problem. To assess the robustness of the allocation methods, we use the Lipschitz constant, which quantifies the extent of change in the allocation vector in response to a unit perturbation in the weight vector of the underlying problem. Thereafter, we provide an algorithm for the matching game that returns an allocation belonging to the $\\left(\\frac{1}{2}-\\epsilon\\right)$-approximate core with Lipschitz constant $O(\\epsilon^{-1})$. Additionally, we provide an algorithm for a minimum spanning tree game that returns an allocation belonging to the $4$-approximate core with a constant Lipschitz constant.   The Shapley value is a popular allocation that satisfies several desirable properties. Therefore, we investigate the robustness of the Shapley value. We demonstrate that the Lipschitz constant of the Shapley value for the minimum spanning tree is constant, whereas that for the matching game is $\\Omega(\\log n)$, where $n$ denotes the number of vertices.","sentences":["In cooperative game theory, the primary focus is the equitable allocation of payoffs or costs among agents.","However, in the practical applications of cooperative games, accurately representing games is challenging.","In such cases, using an allocation method sensitive to small perturbations in the game can lead to various problems, including dissatisfaction among agents and the potential for manipulation by agents seeking to maximize their own benefits.","Therefore, the allocation method must be robust against game perturbations.   ","In this study, we explore optimization games, in which the value of the characteristic function is provided as the optimal value of an optimization problem.","To assess the robustness of the allocation methods, we use the Lipschitz constant, which quantifies the extent of change in the allocation vector in response to a unit perturbation in the weight vector of the underlying problem.","Thereafter, we provide an algorithm for the matching game that returns an allocation belonging to the $\\left(\\frac{1}{2}-\\epsilon\\right)$-approximate core with Lipschitz constant $O(\\epsilon^{-1})$. Additionally, we provide an algorithm for a minimum spanning tree game that returns an allocation belonging to the $4$-approximate core with a constant Lipschitz constant.   ","The Shapley value is a popular allocation that satisfies several desirable properties.","Therefore, we investigate the robustness of the Shapley value.","We demonstrate that the Lipschitz constant of the Shapley value for the minimum spanning tree is constant, whereas that for the matching game is $\\Omega(\\log n)$, where $n$ denotes the number of vertices."],"url":"http://arxiv.org/abs/2405.11889v1","category":"cs.GT"}
{"created":"2024-05-20 09:09:15","title":"Thermodynamic Circuits II: Nonequilibrium conductance matrix for a thermoelectric converter","abstract":"Starting from a linear flux-force relation of a thermoelectric material in local equilibrium, we derive non-linear relations between the currents and forces of a thermoelectric converter (TEC) driven far from equilibrium. Our investigation focuses on a one-dimensional TEC of finite thickness. Building on the achievements of the first paper of this series, we unveil the conservation laws governing physical currents (ie linearly dependent currents). Using the latter allows to put forward two relevant bases of physical and fundamental currents. For these bases, we introduce the concept of non equilibrium conductance matrix to describe the current-force relations of a TEC. This concept will be at the core of the third paper of this series in which we illustrate the serial and parallel association on composite TECs. Finally, we determine in a unified way the optimal working points of the TEC in its two operating modes: the electric generator and the heat pump.","sentences":["Starting from a linear flux-force relation of a thermoelectric material in local equilibrium, we derive non-linear relations between the currents and forces of a thermoelectric converter (TEC) driven far from equilibrium.","Our investigation focuses on a one-dimensional TEC of finite thickness.","Building on the achievements of the first paper of this series, we unveil the conservation laws governing physical currents (ie linearly dependent currents).","Using the latter allows to put forward two relevant bases of physical and fundamental currents.","For these bases, we introduce the concept of non equilibrium conductance matrix to describe the current-force relations of a TEC.","This concept will be at the core of the third paper of this series in which we illustrate the serial and parallel association on composite TECs.","Finally, we determine in a unified way the optimal working points of the TEC in its two operating modes: the electric generator and the heat pump."],"url":"http://arxiv.org/abs/2405.11886v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-20 08:31:17","title":"A study of the reconnection of antiparallel vortices in the infinitely thin case and in the finite thickness case","abstract":"The simplest case is the reconnection of a pair of antiparallel line vortices, e.g., condensation trails of an aircraft. The vortices first undergo long wave deformation (Crow waves), and then reconnect to form coherent structures. Although the behavior of the vortices before and after the reconnection can be clearly observed, what happens during the reconnection still needs to be explained. One of the challenges is related to the fact that the vortices have finite thickness, and therefore, the time and the point of the reconnection cannot be determined. Moreover, the smallest scale of coherent structures that can be observed also depends on the vortex thickness. In this paper, we consider an infinitely thin vortex approximation to study the reconnection process. We show that, in this case, the behavior after the reconnection is quasi-periodic, with the quasi-period being independent of the angle between the vortices at the time of the reconnection. We also show that, in the Fourier transform of the trajectory of the reconnection point, the frequencies that correspond to squares of integers are dominating in a similar way as in the evolution of a polygonal vortex under the localized induction approximation. At the end, we compare the results with a solution of the Navier-Stokes equations for the reconnection of a pair of antiparallel vortices with finite thickness. We use the fluid impulse to determine the reconnection time, the reconnection point, and the quasi-period for this case.","sentences":["The simplest case is the reconnection of a pair of antiparallel line vortices, e.g., condensation trails of an aircraft.","The vortices first undergo long wave deformation (Crow waves), and then reconnect to form coherent structures.","Although the behavior of the vortices before and after the reconnection can be clearly observed, what happens during the reconnection still needs to be explained.","One of the challenges is related to the fact that the vortices have finite thickness, and therefore, the time and the point of the reconnection cannot be determined.","Moreover, the smallest scale of coherent structures that can be observed also depends on the vortex thickness.","In this paper, we consider an infinitely thin vortex approximation to study the reconnection process.","We show that, in this case, the behavior after the reconnection is quasi-periodic, with the quasi-period being independent of the angle between the vortices at the time of the reconnection.","We also show that, in the Fourier transform of the trajectory of the reconnection point, the frequencies that correspond to squares of integers are dominating in a similar way as in the evolution of a polygonal vortex under the localized induction approximation.","At the end, we compare the results with a solution of the Navier-Stokes equations for the reconnection of a pair of antiparallel vortices with finite thickness.","We use the fluid impulse to determine the reconnection time, the reconnection point, and the quasi-period for this case."],"url":"http://arxiv.org/abs/2405.11875v1","category":"math.AP"}
{"created":"2024-05-20 08:30:13","title":"xFinder: Robust and Pinpoint Answer Extraction for Large Language Models","abstract":"The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance. Particularly, the emergence of subjective or non-subjective cheating phenomena, such as test set leakage and prompt format overfitting, poses significant challenges to the reliable evaluation of LLMs. Since evaluation frameworks often utilize Regular Expression (RegEx) for answer extraction, some models may adjust their responses to comply with specific formats that are easily extractable by RegEx. Nevertheless, the key answer extraction module based on RegEx frequently suffers from extraction errors. This paper conducts a comprehensive analysis of the entire LLM evaluation chain, demonstrating that optimizing the key answer extraction module can improve extraction accuracy, reduce LLMs' reliance on specific answer formats, and enhance the reliability of LLM evaluation. To address these issues, we propose xFinder, a model specifically designed for key answer extraction. As part of this process, we create a specialized dataset, the Key Answer Finder (KAF) dataset, to ensure effective model training and evaluation. Through generalization testing and evaluation in real-world scenarios, the results demonstrate that the smallest xFinder model with only 500 million parameters achieves an average answer extraction accuracy of 93.42%. In contrast, RegEx accuracy in the best evaluation framework is 74.38%. xFinder exhibits stronger robustness and higher accuracy compared to existing evaluation frameworks. All resources for xFinder are available at \\url{https://github.com/IAAR-Shanghai/xFinder}.","sentences":["The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance.","Particularly, the emergence of subjective or non-subjective cheating phenomena, such as test set leakage and prompt format overfitting, poses significant challenges to the reliable evaluation of LLMs.","Since evaluation frameworks often utilize Regular Expression (RegEx) for answer extraction, some models may adjust their responses to comply with specific formats that are easily extractable by RegEx.","Nevertheless, the key answer extraction module based on RegEx frequently suffers from extraction errors.","This paper conducts a comprehensive analysis of the entire LLM evaluation chain, demonstrating that optimizing the key answer extraction module can improve extraction accuracy, reduce LLMs' reliance on specific answer formats, and enhance the reliability of LLM evaluation.","To address these issues, we propose xFinder, a model specifically designed for key answer extraction.","As part of this process, we create a specialized dataset, the Key Answer Finder (KAF) dataset, to ensure effective model training and evaluation.","Through generalization testing and evaluation in real-world scenarios, the results demonstrate that the smallest xFinder model with only 500 million parameters achieves an average answer extraction accuracy of 93.42%.","In contrast, RegEx accuracy in the best evaluation framework is 74.38%.","xFinder exhibits stronger robustness and higher accuracy compared to existing evaluation frameworks.","All resources for xFinder are available at \\url{https://github.com/IAAR-Shanghai/xFinder}."],"url":"http://arxiv.org/abs/2405.11874v1","category":"cs.CL"}
{"created":"2024-05-20 08:19:08","title":"Depth Prompting for Sensor-Agnostic Depth Estimation","abstract":"Dense depth maps have been used as a key element of visual perception tasks. There have been tremendous efforts to enhance the depth quality, ranging from optimization-based to learning-based methods. Despite the remarkable progress for a long time, their applicability in the real world is limited due to systematic measurement biases such as density, sensing pattern, and scan range. It is well-known that the biases make it difficult for these methods to achieve their generalization. We observe that learning a joint representation for input modalities (e.g., images and depth), which most recent methods adopt, is sensitive to the biases. In this work, we disentangle those modalities to mitigate the biases with prompt engineering. For this, we design a novel depth prompt module to allow the desirable feature representation according to new depth distributions from either sensor types or scene configurations. Our depth prompt can be embedded into foundation models for monocular depth estimation. Through this embedding process, our method helps the pretrained model to be free from restraint of depth scan range and to provide absolute scale depth maps. We demonstrate the effectiveness of our method through extensive evaluations. Source code is publicly available at https://github.com/JinhwiPark/DepthPrompting .","sentences":["Dense depth maps have been used as a key element of visual perception tasks.","There have been tremendous efforts to enhance the depth quality, ranging from optimization-based to learning-based methods.","Despite the remarkable progress for a long time, their applicability in the real world is limited due to systematic measurement biases such as density, sensing pattern, and scan range.","It is well-known that the biases make it difficult for these methods to achieve their generalization.","We observe that learning a joint representation for input modalities (e.g., images and depth), which most recent methods adopt, is sensitive to the biases.","In this work, we disentangle those modalities to mitigate the biases with prompt engineering.","For this, we design a novel depth prompt module to allow the desirable feature representation according to new depth distributions from either sensor types or scene configurations.","Our depth prompt can be embedded into foundation models for monocular depth estimation.","Through this embedding process, our method helps the pretrained model to be free from restraint of depth scan range and to provide absolute scale depth maps.","We demonstrate the effectiveness of our method through extensive evaluations.","Source code is publicly available at https://github.com/JinhwiPark/DepthPrompting ."],"url":"http://arxiv.org/abs/2405.11867v1","category":"cs.CV"}
{"created":"2024-05-20 08:01:47","title":"Godbillon-Vey type functional for almost contact manifolds","abstract":"Many contact metric manifolds are critical points of curvature functionals restricted to spaces of associated metrics. The Godbillon-Vey functional has never been considered in a variational context in contact geometry. Recently we extended this functional from foliations to arbitrary plane fields on a 3-dimensional manifold, so, the following question arises: can one use the Godbillon-Vey functional to find optimal almost contact manifolds? In the paper, we introduce a Godbillon-Vey type functional for a 3-dimensional almost contact manifold and find its Euler-Lagrange equations for all variations preserving the Reeb vector field. We construct critical (for our functional) 3-dimensional almost contact manifolds having a double-twisted product structure, these solutions belong to the class $C_{5}\\oplus C_{12}$ according to Chinea-Gonzalez classification.","sentences":["Many contact metric manifolds are critical points of curvature functionals restricted to spaces of associated metrics.","The Godbillon-Vey functional has never been considered in a variational context in contact geometry.","Recently we extended this functional from foliations to arbitrary plane fields on a 3-dimensional manifold, so, the following question arises: can one use the Godbillon-Vey functional to find optimal almost contact manifolds?","In the paper, we introduce a Godbillon-Vey type functional for a 3-dimensional almost contact manifold and find its Euler-Lagrange equations for all variations preserving the Reeb vector field.","We construct critical (for our functional) 3-dimensional almost contact manifolds having a double-twisted product structure, these solutions belong to the class $C_{5}\\oplus C_{12}$ according to Chinea-Gonzalez classification."],"url":"http://arxiv.org/abs/2405.11857v1","category":"math.DG"}
{"created":"2024-05-20 08:00:25","title":"Modeling and simulation of a mechanism for suppressing the flipping problem of a jumping robot","abstract":"In order to solve the problem of stable jumping of micro robot, we design a special mechanism: elastic passive joint (EPJ). EPJ can assist in achieving smooth jumping through the opening-closing process when the robot jumps. First, we introduce the composition and operation principle of EPJ, and perform a dynamic modeling of the robot's jumping process. Then, in order to verify the effectiveness of EPJ in controlling the robot's smooth jump, we design a simulation experiment based on MATLAB. Through comparative experiments, it was proved that EPJ can greatly adjust the angular velocity of the robot and increase the jump distance of the robot. Finally, we analyze each parameter in EPJ and performs parameter optimization. After optimization, EPJ achieves a completely flip-free jump of the robot, laying an important foundation for improving the mobility of micro-robot.","sentences":["In order to solve the problem of stable jumping of micro robot, we design a special mechanism: elastic passive joint (EPJ).","EPJ can assist in achieving smooth jumping through the opening-closing process when the robot jumps.","First, we introduce the composition and operation principle of EPJ, and perform a dynamic modeling of the robot's jumping process.","Then, in order to verify the effectiveness of EPJ in controlling the robot's smooth jump, we design a simulation experiment based on MATLAB.","Through comparative experiments, it was proved that EPJ can greatly adjust the angular velocity of the robot and increase the jump distance of the robot.","Finally, we analyze each parameter in EPJ and performs parameter optimization.","After optimization, EPJ achieves a completely flip-free jump of the robot, laying an important foundation for improving the mobility of micro-robot."],"url":"http://arxiv.org/abs/2405.11856v1","category":"cs.RO"}
{"created":"2024-05-20 07:53:41","title":"Rethinking Overlooked Aspects in Vision-Language Models","abstract":"Recent advancements in large vision-language models (LVLMs), such as GPT4-V and LLaVA, have been substantial. LLaVA's modular architecture, in particular, offers a blend of simplicity and efficiency. Recent works mainly focus on introducing more pre-training and instruction tuning data to improve model's performance. This paper delves into the often-neglected aspects of data efficiency during pre-training and the selection process for instruction tuning datasets. Our research indicates that merely increasing the size of pre-training data does not guarantee improved performance and may, in fact, lead to its degradation. Furthermore, we have established a pipeline to pinpoint the most efficient instruction tuning (SFT) dataset, implying that not all SFT data utilized in existing studies are necessary. The primary objective of this paper is not to introduce a state-of-the-art model, but rather to serve as a roadmap for future research, aiming to optimize data usage during pre-training and fine-tuning processes to enhance the performance of vision-language models.","sentences":["Recent advancements in large vision-language models (LVLMs), such as GPT4-V and LLaVA, have been substantial.","LLaVA's modular architecture, in particular, offers a blend of simplicity and efficiency.","Recent works mainly focus on introducing more pre-training and instruction tuning data to improve model's performance.","This paper delves into the often-neglected aspects of data efficiency during pre-training and the selection process for instruction tuning datasets.","Our research indicates that merely increasing the size of pre-training data does not guarantee improved performance and may, in fact, lead to its degradation.","Furthermore, we have established a pipeline to pinpoint the most efficient instruction tuning (SFT) dataset, implying that not all SFT data utilized in existing studies are necessary.","The primary objective of this paper is not to introduce a state-of-the-art model, but rather to serve as a roadmap for future research, aiming to optimize data usage during pre-training and fine-tuning processes to enhance the performance of vision-language models."],"url":"http://arxiv.org/abs/2405.11850v1","category":"cs.CV"}
{"created":"2024-05-20 06:58:47","title":"SSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model","abstract":"Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities. However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency. Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities. Given these advantages, we explore the potential of SSM-based models in audio tasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning. SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively. We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification. Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7% faster in batch inference speed and 95.4% more memory-efficient than SSAST for the tiny model size with an input token size of 22k. These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications.","sentences":["Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities.","However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency.","Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities.","Given these advantages, we explore the potential of SSM-based models in audio tasks.","In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning.","SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively.","We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets.","We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification.","Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks.","Notably, SSAMBA is approximately 92.7% faster in batch inference speed and 95.4% more memory-efficient than SSAST for the tiny model size with an input token size of 22k.","These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications."],"url":"http://arxiv.org/abs/2405.11831v1","category":"eess.AS"}
{"created":"2024-05-20 06:25:59","title":"A Rate-Distortion Analysis for Composite Sources Under Subsource-Dependent Fidelity Criteria","abstract":"A composite source, consisting of multiple subsources and a memoryless switch, outputs one symbol at a time from the subsource selected by the switch. If some data should be encoded more accurately than other data from an information source, the composite source model is suitable because in this model different distortion constraints can be put on the subsources. In this context, we propose subsource-dependent fidelity criteria for composite sources and use them to formulate a rate-distortion problem. We solve the problem and obtain a single-letter expression for the rate-distortion function. Further rate-distortion analysis characterizes the performance of classify-then-compress (CTC) coding, which is frequently used in practice when subsource-dependent fidelity criteria are considered. Our analysis shows that CTC coding generally has performance loss relative to optimal coding, even if the classification is perfect. We also identify the cause of the performance loss, that is, class labels have to be reproduced in CTC coding. Last but not least, we show that the performance loss is negligible for asymptotically small distortion if CTC coding is appropriately designed and some mild conditions are satisfied.","sentences":["A composite source, consisting of multiple subsources and a memoryless switch, outputs one symbol at a time from the subsource selected by the switch.","If some data should be encoded more accurately than other data from an information source, the composite source model is suitable because in this model different distortion constraints can be put on the subsources.","In this context, we propose subsource-dependent fidelity criteria for composite sources and use them to formulate a rate-distortion problem.","We solve the problem and obtain a single-letter expression for the rate-distortion function.","Further rate-distortion analysis characterizes the performance of classify-then-compress (CTC) coding, which is frequently used in practice when subsource-dependent fidelity criteria are considered.","Our analysis shows that CTC coding generally has performance loss relative to optimal coding, even if the classification is perfect.","We also identify the cause of the performance loss, that is, class labels have to be reproduced in CTC coding.","Last but not least, we show that the performance loss is negligible for asymptotically small distortion if CTC coding is appropriately designed and some mild conditions are satisfied."],"url":"http://arxiv.org/abs/2405.11818v1","category":"cs.IT"}
{"created":"2024-05-20 06:15:25","title":"Nonlinear Lindblad Master Equation and Postselected Skin Effect","abstract":"We introduce a non-linear Lindblad master equation to describe the postselection dynamics of open quantum systems described by the Lindblad master equation, which continuously interpolates between the Lindblad master equation and the dynamical equation governed by an effective non-Hermitian Hamiltonian. Within the framework of the non-linear Lindblad master equation, we study a prototypical model and demonstrate the existence of the postselected skin effect with the distribution of a steady state characterized by the accumulation of particles on one side, as long as some quantum jumping terms are discarded by postselction processes. Moreover, we show that the trajectory-averaged entanglement entropy can reflect the different influences from the environment and postselection, and unveil it exhibiting a special distribution with algebraic growth in the short chain and saturation in the long chain induced by the postselected skin effect.","sentences":["We introduce a non-linear Lindblad master equation to describe the postselection dynamics of open quantum systems described by the Lindblad master equation, which continuously interpolates between the Lindblad master equation and the dynamical equation governed by an effective non-Hermitian Hamiltonian.","Within the framework of the non-linear Lindblad master equation, we study a prototypical model and demonstrate the existence of the postselected skin effect with the distribution of a steady state characterized by the accumulation of particles on one side, as long as some quantum jumping terms are discarded by postselction processes.","Moreover, we show that the trajectory-averaged entanglement entropy can reflect the different influences from the environment and postselection, and unveil it exhibiting a special distribution with algebraic growth in the short chain and saturation in the long chain induced by the postselected skin effect."],"url":"http://arxiv.org/abs/2405.11812v1","category":"quant-ph"}
{"created":"2024-05-20 05:46:41","title":"LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering","abstract":"Graph clustering is a fundamental problem in machine learning. Deep learning methods achieve the state-of-the-art results in recent years, but they still cannot work without predefined cluster numbers. Such limitation motivates us to pose a more challenging problem of graph clustering with unknown cluster number. We propose to address this problem from a fresh perspective of graph information theory (i.e., structural information). In the literature, structural information has not yet been introduced to deep clustering, and its classic definition falls short of discrete formulation and modeling node features. In this work, we first formulate a differentiable structural information (DSI) in the continuous realm, accompanied by several theoretical results. By minimizing DSI, we construct the optimal partitioning tree where densely connected nodes in the graph tend to have the same assignment, revealing the cluster structure. DSI is also theoretically presented as a new graph clustering objective, not requiring the predefined cluster number. Furthermore, we design a neural LSEnet in the Lorentz model of hyperbolic space, where we integrate node features to structural information via manifold-valued graph convolution. Extensive empirical results on real graphs show the superiority of our approach.","sentences":["Graph clustering is a fundamental problem in machine learning.","Deep learning methods achieve the state-of-the-art results in recent years, but they still cannot work without predefined cluster numbers.","Such limitation motivates us to pose a more challenging problem of graph clustering with unknown cluster number.","We propose to address this problem from a fresh perspective of graph information theory (i.e., structural information).","In the literature, structural information has not yet been introduced to deep clustering, and its classic definition falls short of discrete formulation and modeling node features.","In this work, we first formulate a differentiable structural information (DSI) in the continuous realm, accompanied by several theoretical results.","By minimizing DSI, we construct the optimal partitioning tree where densely connected nodes in the graph tend to have the same assignment, revealing the cluster structure.","DSI is also theoretically presented as a new graph clustering objective, not requiring the predefined cluster number.","Furthermore, we design a neural LSEnet in the Lorentz model of hyperbolic space, where we integrate node features to structural information via manifold-valued graph convolution.","Extensive empirical results on real graphs show the superiority of our approach."],"url":"http://arxiv.org/abs/2405.11801v1","category":"cs.LG"}
{"created":"2024-05-20 05:34:13","title":"Constructing vortex functions and basis states of Chern insulators: ideal condition, inequality from index theorem, and coherent-like states on von Neumann lattice","abstract":"In the field of fractional Chern insulators, a great deal of effort has been devoted to characterizing Chern bands that exhibit properties similar to the Landau levels. Among them, the concept of the vortex function, which generalizes the complex coordinate used for the symmetric-gauge Landau-level basis, allows for a concise description. In this paper, we develop a theory of constructing the vortex function and basis states of Chern insulators. In the first half, we consider the optimization process of the vortex function, which minimizes an indicator that measures the difference from the ideal Chern insulators. In particular, we focus on the sublattice position dependence of the vortex function or the quantum geometric tensor. In the second half, we construct two types of basis sets for a given vortex function: radially localized basis set and coherent-like basis set. The former basis set is defined as the eigenstates of an analogy of the angular momentum operator. Remarkably, one can always find exact zero mode(s) for this operator, which is explained by the celebrated Atiyah-Singer index theorem. As a byproduct, we propose an inequality rooted in the band topology. We also discuss the subtle differences between our formalism and the previous works about the momentum-space Landau level. The latter basis set generalizes the concept of coherent states on von Neumann lattice. While this basis set is not orthogonal, it is useful to compare the LLL and the given Chern insulator directly in the Brillouin zone.","sentences":["In the field of fractional Chern insulators, a great deal of effort has been devoted to characterizing Chern bands that exhibit properties similar to the Landau levels.","Among them, the concept of the vortex function, which generalizes the complex coordinate used for the symmetric-gauge Landau-level basis, allows for a concise description.","In this paper, we develop a theory of constructing the vortex function and basis states of Chern insulators.","In the first half, we consider the optimization process of the vortex function, which minimizes an indicator that measures the difference from the ideal Chern insulators.","In particular, we focus on the sublattice position dependence of the vortex function or the quantum geometric tensor.","In the second half, we construct two types of basis sets for a given vortex function: radially localized basis set and coherent-like basis set.","The former basis set is defined as the eigenstates of an analogy of the angular momentum operator.","Remarkably, one can always find exact zero mode(s) for this operator, which is explained by the celebrated Atiyah-Singer index theorem.","As a byproduct, we propose an inequality rooted in the band topology.","We also discuss the subtle differences between our formalism and the previous works about the momentum-space Landau level.","The latter basis set generalizes the concept of coherent states on von Neumann lattice.","While this basis set is not orthogonal, it is useful to compare the LLL and the given Chern insulator directly in the Brillouin zone."],"url":"http://arxiv.org/abs/2405.11796v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-20 05:18:47","title":"Source Localization by Multidimensional Steered Response Power Mapping with Sparse Bayesian Learning","abstract":"We propose an advance Steered Response Power (SRP) method for localizing multiple sources. While conventional SRP performs well in adverse conditions, it remains to struggle in scenarios with closely neighboring sources, resulting in ambiguous SRP maps. We address this issue by applying sparsity optimization in SRP to obtain high-resolution maps. Our approach represents SRP maps as multidimensional matrices to preserve time-frequency information and further improve performance in unfavorable conditions. We use multi-dictionary Sparse Bayesian Learning to localize sources without needing prior knowledge of their quantity. We validate our method through practical experiments with a 16-channel planar microphone array and compare against three other SRP and sparsity-based methods. Our multidimensional SRP approach outperforms conventional SRP and the current state-of-the-art sparse SRP methods for localizing closely spaced sources in a reverberant room.","sentences":["We propose an advance Steered Response Power (SRP) method for localizing multiple sources.","While conventional SRP performs well in adverse conditions, it remains to struggle in scenarios with closely neighboring sources, resulting in ambiguous SRP maps.","We address this issue by applying sparsity optimization in SRP to obtain high-resolution maps.","Our approach represents SRP maps as multidimensional matrices to preserve time-frequency information and further improve performance in unfavorable conditions.","We use multi-dictionary Sparse Bayesian Learning to localize sources without needing prior knowledge of their quantity.","We validate our method through practical experiments with a 16-channel planar microphone array and compare against three other SRP and sparsity-based methods.","Our multidimensional SRP approach outperforms conventional SRP and the current state-of-the-art sparse SRP methods for localizing closely spaced sources in a reverberant room."],"url":"http://arxiv.org/abs/2405.11792v1","category":"eess.AS"}
{"created":"2024-05-20 05:08:55","title":"Guided Multi-objective Generative AI to Enhance Structure-based Drug Design","abstract":"Generative AI has the potential to revolutionize drug discovery. Yet, despite recent advances in machine learning, existing models cannot generate molecules that satisfy all desired physicochemical properties. Herein, we describe IDOLpro, a novel generative chemistry AI combining deep diffusion with multi-objective optimization for structure-based drug design. The latent variables of the diffusion model are guided by differentiable scoring functions to explore uncharted chemical space and generate novel ligands in silico, optimizing a plurality of target physicochemical properties. We demonstrate its effectiveness by generating ligands with optimized binding affinity and synthetic accessibility on two benchmark sets. IDOLpro produces ligands with binding affinities over 10% higher than the next best state-of-the-art on each test set. On a test set of experimental complexes, IDOLpro is the first to surpass the performance of experimentally observed ligands. IDOLpro can accommodate other scoring functions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead optimization for drug discovery.","sentences":["Generative AI has the potential to revolutionize drug discovery.","Yet, despite recent advances in machine learning, existing models cannot generate molecules that satisfy all desired physicochemical properties.","Herein, we describe IDOLpro, a novel generative chemistry AI combining deep diffusion with multi-objective optimization for structure-based drug design.","The latent variables of the diffusion model are guided by differentiable scoring functions to explore uncharted chemical space and generate novel ligands in silico, optimizing a plurality of target physicochemical properties.","We demonstrate its effectiveness by generating ligands with optimized binding affinity and synthetic accessibility on two benchmark sets.","IDOLpro produces ligands with binding affinities over 10% higher than the next best state-of-the-art on each test set.","On a test set of experimental complexes, IDOLpro is the first to surpass the performance of experimentally observed ligands.","IDOLpro can accommodate other scoring functions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead optimization for drug discovery."],"url":"http://arxiv.org/abs/2405.11785v1","category":"physics.chem-ph"}
{"created":"2024-05-20 04:55:32","title":"Formulation and evaluation of ocean dynamics problems as optimization problems for quantum annealing machines","abstract":"Recent advancements in quantum computing suggest the potential to revolutionize computational algorithms across various scientific domains including oceanography and atmospheric science. The field is still relatively young and quantum computation is so different from classical computation that suitable frameworks to represent oceanic and atmospheric dynamics are yet to be explored. Quantum annealing, one of the major paradigms, focuses on combinatorial optimization tasks. In this paper, we solve the classical Stommel problem by quantum annealing (QA) and simulated annealing (SA), a classical counterpart of quantum annealing. We cast the linear partial differential equation into an optimization problem by the least-squares method and discretize the cost function in two ways: finite difference and truncated basis expansion. In either case, SA successfully reproduces the expected solution when appropriate parameters are chosen, demonstrating that annealing has the potential. In contrast, QA using the D-Wave quantum annealing machine fails to obtain good solutions for some cases owing to hardware limitations; in particular, the highly limited connectivity graph of the machine limits the size of the solvable problems, at least under currently available algorithms. Either expanding the connectivity graph or improving the graph embedding algorithms would probably be necessary for quantum annealing machines to be usable for oceanic and atmospheric dynamics problems. While this finding emphasizes the need for hardware improvements and enhancements in graph embedding algorithms for practical applications of quantum annealers, the results from simulated annealing suggest its potential to address practical geophysical dynamics problems. As quantum calculation continues to evolve, addressing these challenges may lead to transformative advancements in ocean and atmosphere modeling.","sentences":["Recent advancements in quantum computing suggest the potential to revolutionize computational algorithms across various scientific domains including oceanography and atmospheric science.","The field is still relatively young and quantum computation is so different from classical computation that suitable frameworks to represent oceanic and atmospheric dynamics are yet to be explored.","Quantum annealing, one of the major paradigms, focuses on combinatorial optimization tasks.","In this paper, we solve the classical Stommel problem by quantum annealing (QA) and simulated annealing (SA), a classical counterpart of quantum annealing.","We cast the linear partial differential equation into an optimization problem by the least-squares method and discretize the cost function in two ways: finite difference and truncated basis expansion.","In either case, SA successfully reproduces the expected solution when appropriate parameters are chosen, demonstrating that annealing has the potential.","In contrast, QA using the D-Wave quantum annealing machine fails to obtain good solutions for some cases owing to hardware limitations; in particular, the highly limited connectivity graph of the machine limits the size of the solvable problems, at least under currently available algorithms.","Either expanding the connectivity graph or improving the graph embedding algorithms would probably be necessary for quantum annealing machines to be usable for oceanic and atmospheric dynamics problems.","While this finding emphasizes the need for hardware improvements and enhancements in graph embedding algorithms for practical applications of quantum annealers, the results from simulated annealing suggest its potential to address practical geophysical dynamics problems.","As quantum calculation continues to evolve, addressing these challenges may lead to transformative advancements in ocean and atmosphere modeling."],"url":"http://arxiv.org/abs/2405.11782v1","category":"quant-ph"}
{"created":"2024-05-20 04:46:14","title":"General bounds on the quality of Bayesian coresets","abstract":"Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data. But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings -- i.e., exponential family models, or models with strong log-concavity and smoothness assumptions. This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets. The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work. The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods. The upper bounds are used to analyze the performance of recent subsample-optimize methods. The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions.","sentences":["Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data.","But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings -- i.e., exponential family models, or models with strong log-concavity and smoothness assumptions.","This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets.","The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work.","The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods.","The upper bounds are used to analyze the performance of recent subsample-optimize methods.","The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions."],"url":"http://arxiv.org/abs/2405.11780v1","category":"stat.ML"}
{"created":"2024-05-20 04:20:25","title":"CDM-MPC: An Integrated Dynamic Planning and Control Framework for Bipedal Robots Jumping","abstract":"Performing acrobatic maneuvers like dynamic jumping in bipedal robots presents significant challenges in terms of actuation, motion planning, and control. Traditional approaches to these tasks often simplify dynamics to enhance computational efficiency, potentially overlooking critical factors such as the control of centroidal angular momentum (CAM) and the variability of centroidal composite rigid body inertia (CCRBI). This paper introduces a novel integrated dynamic planning and control framework, termed centroidal dynamics model-based model predictive control (CDM-MPC), designed for robust jumping control that fully considers centroidal momentum and non-constant CCRBI. The framework comprises an optimization-based kinodynamic motion planner and an MPC controller for real-time trajectory tracking and replanning. Additionally, a centroidal momentum-based inverse kinematics (IK) solver and a landing heuristic controller are developed to ensure stability during high-impact landings. The efficacy of the CDM-MPC framework is validated through extensive testing on the full-sized humanoid robot KUAVO in both simulations and experiments.","sentences":["Performing acrobatic maneuvers like dynamic jumping in bipedal robots presents significant challenges in terms of actuation, motion planning, and control.","Traditional approaches to these tasks often simplify dynamics to enhance computational efficiency, potentially overlooking critical factors such as the control of centroidal angular momentum (CAM) and the variability of centroidal composite rigid body inertia (CCRBI).","This paper introduces a novel integrated dynamic planning and control framework, termed centroidal dynamics model-based model predictive control (CDM-MPC), designed for robust jumping control that fully considers centroidal momentum and non-constant CCRBI.","The framework comprises an optimization-based kinodynamic motion planner and an MPC controller for real-time trajectory tracking and replanning.","Additionally, a centroidal momentum-based inverse kinematics (IK) solver and a landing heuristic controller are developed to ensure stability during high-impact landings.","The efficacy of the CDM-MPC framework is validated through extensive testing on the full-sized humanoid robot KUAVO in both simulations and experiments."],"url":"http://arxiv.org/abs/2405.11773v1","category":"cs.RO"}
{"created":"2024-05-20 03:04:56","title":"The statistical and dynamic modeling of the first part of the 2013-2014 Euromaidan protests in Ukraine: The Revolution of Dignity and preceding times","abstract":"Ukraine's tug-of-war between Russia and the West has had significant and lasting consequences for the country. In 2013, Viktor Yanukovych, the Ukrainian president aligned with Russia, opted against signing an association agreement with the European Union. This agreement aimed to facilitate trade and travel between the EU and Ukraine. This decision sparked widespread protests that coalesced in Kyiv's Maidan Square, eventually becoming known as the Euromaidan protests. In this study, we analyze the protest data from 2013, sourced from Ukraine's Center for Social and Labor Research. Despite the dataset's limitations and occasional inconsistencies, we demonstrate the extraction of valuable insights and the construction of a descriptive model from such data. Our investigation reveals a pre-existing state of self-excitation within the system even before the onset of the Euromaidan protests. This self-excitation intensified during the Euromaidan protests. A statistical analysis indicates that the government's utilization of force correlates with increased future protests, exacerbating rather than quelling the protest movement. Furthermore, we introduce the implementation of Hawkes process models to comprehend the spatiotemporal dynamics of the protest activity. Our findings highlight that, while protest activities spread across the entire country, the driving force behind the dynamics of these protests was the level of activity in Kyiv. Furthermore, in contrast to prior research that emphasized geographical proximity as a key predictor of event propagation, our study illustrates that the political alignment among oblasts, which are the distinct municipalities comprising Ukraine, had a more profound impact than mere geographic distance. This underscores the significance of social and cultural factors in molding the trajectory of political movements.","sentences":["Ukraine's tug-of-war between Russia and the West has had significant and lasting consequences for the country.","In 2013, Viktor Yanukovych, the Ukrainian president aligned with Russia, opted against signing an association agreement with the European Union.","This agreement aimed to facilitate trade and travel between the EU and Ukraine.","This decision sparked widespread protests that coalesced in Kyiv's Maidan Square, eventually becoming known as the Euromaidan protests.","In this study, we analyze the protest data from 2013, sourced from Ukraine's Center for Social and Labor Research.","Despite the dataset's limitations and occasional inconsistencies, we demonstrate the extraction of valuable insights and the construction of a descriptive model from such data.","Our investigation reveals a pre-existing state of self-excitation within the system even before the onset of the Euromaidan protests.","This self-excitation intensified during the Euromaidan protests.","A statistical analysis indicates that the government's utilization of force correlates with increased future protests, exacerbating rather than quelling the protest movement.","Furthermore, we introduce the implementation of Hawkes process models to comprehend the spatiotemporal dynamics of the protest activity.","Our findings highlight that, while protest activities spread across the entire country, the driving force behind the dynamics of these protests was the level of activity in Kyiv.","Furthermore, in contrast to prior research that emphasized geographical proximity as a key predictor of event propagation, our study illustrates that the political alignment among oblasts, which are the distinct municipalities comprising Ukraine, had a more profound impact than mere geographic distance.","This underscores the significance of social and cultural factors in molding the trajectory of political movements."],"url":"http://arxiv.org/abs/2405.12253v1","category":"physics.soc-ph"}
{"created":"2024-05-21 17:47:52","title":"A novel forward-looking ultrasound catheter for treating vascular occlusions","abstract":"Thrombotic and chronic occlusions of large blood vessels are a major cause of mortality and morbidity, and so there is a need for improved treatments in many clinical circumstances. Endovascular ultrasound approaches have been shown to hold considerable potential to treat large vessel thrombotic occlusions. Here, we report the development of a novel forward-looking therapeutic ultrasound catheter approach. The design concept centers on the use of a radially polarized hollow cylindrical transducer situated at the distal tip. This approach enables a compact configuration where the central lumen can accommodate a guidewire during navigation as well as provide a route to release cavitation seeds and therapeutic agents adjacent to the site of occlusion. PZT-5H transducers with outer/inner diameters of 1.35/0.73 mm were evaluated using simulations and experiments for their capacity to project forward-looking ultrasound. A length of 2.5 mm operating in the 3rd harmonic of the length mode resonance (1.85 MHz) was selected. Catheters (1.55 mm outer diameter) were designed and fabricated with a liner, outer jacket, and braiding, where the transducers were incorporated with air-backing and a front-face matching layer. Forward-looking pressures of 2.5 MPa (peak negative) at 0.5 mm were achieved. Proof of principle vessel phantom experiments were performed demonstrating the ability to eject microbubbles in proximity to an occlusion and stimulate inertial cavitation. This approach holds potential for treating thrombotic and chronic total occlusions.","sentences":["Thrombotic and chronic occlusions of large blood vessels are a major cause of mortality and morbidity, and so there is a need for improved treatments in many clinical circumstances.","Endovascular ultrasound approaches have been shown to hold considerable potential to treat large vessel thrombotic occlusions.","Here, we report the development of a novel forward-looking therapeutic ultrasound catheter approach.","The design concept centers on the use of a radially polarized hollow cylindrical transducer situated at the distal tip.","This approach enables a compact configuration where the central lumen can accommodate a guidewire during navigation as well as provide a route to release cavitation seeds and therapeutic agents adjacent to the site of occlusion.","PZT-5H transducers with outer/inner diameters of 1.35/0.73 mm were evaluated using simulations and experiments for their capacity to project forward-looking ultrasound.","A length of 2.5 mm operating in the 3rd harmonic of the length mode resonance (1.85 MHz) was selected.","Catheters (1.55 mm outer diameter) were designed and fabricated with a liner, outer jacket, and braiding, where the transducers were incorporated with air-backing and a front-face matching layer.","Forward-looking pressures of 2.5 MPa (peak negative) at 0.5 mm were achieved.","Proof of principle vessel phantom experiments were performed demonstrating the ability to eject microbubbles in proximity to an occlusion and stimulate inertial cavitation.","This approach holds potential for treating thrombotic and chronic total occlusions."],"url":"http://arxiv.org/abs/2405.12966v1","category":"physics.med-ph"}
{"created":"2024-05-21 16:39:26","title":"Commutative codensity monads and probability bimeasures","abstract":"Several well-studied probability monads have been expressed as codensity monads over small categories of stochastic maps, giving a limit description of spaces of probability measures. In this paper we show how properties of probability monads such as commutativity and affineness can arise from their codensity presentation. First we show that their codensity presentation is closely related to another characterisation of probability monads as terminal endofunctors admitting certain maps into the Giry monad, which allows us to generalise a result by Van Breugel on the Kantorovich monad. We then provide sufficient conditions for a codensity monad to lift to $\\bf{MonCat}$, and give a characterisation of exactly pointwise monoidal codensity monads; codensity monads that satisfy a strengthening of these conditions. We show that the Radon monad is exactly pointwise monoidal, and hence give a description of the tensor product of free algebras of the Radon monad in terms of Day convolution. Finally we show that the Giry monad is not exactly pointwise monoidal due to the existence of probability bimeasures that do not extend to measures, although its restriction to standard Borel spaces is. We introduce the notion of a $*$-monad and its Kleisli monoidal op-multicategory to describe the categorical structure that organises the spaces of probability polymeasures on measurable spaces.","sentences":["Several well-studied probability monads have been expressed as codensity monads over small categories of stochastic maps, giving a limit description of spaces of probability measures.","In this paper we show how properties of probability monads such as commutativity and affineness can arise from their codensity presentation.","First we show that their codensity presentation is closely related to another characterisation of probability monads as terminal endofunctors admitting certain maps into the Giry monad, which allows us to generalise a result by Van Breugel on the Kantorovich monad.","We then provide sufficient conditions for a codensity monad to lift to $\\bf{MonCat}$, and give a characterisation of exactly pointwise monoidal codensity monads; codensity monads that satisfy a strengthening of these conditions.","We show that the Radon monad is exactly pointwise monoidal, and hence give a description of the tensor product of free algebras of the Radon monad in terms of Day convolution.","Finally we show that the Giry monad is not exactly pointwise monoidal due to the existence of probability bimeasures that do not extend to measures, although its restriction to standard Borel spaces is.","We introduce the notion of a $*$-monad and its Kleisli monoidal op-multicategory to describe the categorical structure that organises the spaces of probability polymeasures on measurable spaces."],"url":"http://arxiv.org/abs/2405.12917v1","category":"math.CT"}
{"created":"2024-05-21 16:06:27","title":"Giant spatial anisotropy of magnon lifetime in altermagnets","abstract":"Altermagnets are a new class of magnetic materials with zero net magnetization (like antiferromagnets) but spin-split electronic bands (like ferromagnets) over a fraction of reciprocal space. As in antiferromagnets, magnons in altermagnets come in two flavours, that either add one or remove one unit of spin to the $S=0$ ground state. However, in altermagnets these two magnon modes are non-degenerate along some directions in reciprocal space. Here we show that the lifetime of altermagnetic magnons has a very strong dependence on both flavour and direction. Strikingly, coupling to Stoner modes leads to a complete suppression of magnon propagation along selected spatial directions. This giant anisotropy will impact electronic, spin, and energy transport properties and may be exploited in spintronic applications.","sentences":["Altermagnets are a new class of magnetic materials with zero net magnetization (like antiferromagnets) but spin-split electronic bands (like ferromagnets) over a fraction of reciprocal space.","As in antiferromagnets, magnons in altermagnets come in two flavours, that either add one or remove one unit of spin to the $S=0$ ground state.","However, in altermagnets these two magnon modes are non-degenerate along some directions in reciprocal space.","Here we show that the lifetime of altermagnetic magnons has a very strong dependence on both flavour and direction.","Strikingly, coupling to Stoner modes leads to a complete suppression of magnon propagation along selected spatial directions.","This giant anisotropy will impact electronic, spin, and energy transport properties and may be exploited in spintronic applications."],"url":"http://arxiv.org/abs/2405.12896v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 15:42:32","title":"Catalog of Broad H\u03b1 and H\\b{eta} Active Galactic Nuclei in MaNGA","abstract":"Broad H$\\alpha$ and H$\\beta$ emission lines (FWHM > 1,000 km s$^{-1}$) are incredibly efficient tracers of the high-velocity clouds encircling Active Galactic Nuclei (AGN). As a result, we search for these broad line AGN in the Sloan Digital Sky Survey's Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) catalog. We identify 301 broad-line H$\\alpha$ galaxies and 801 broad-line H$\\beta$ galaxies in the catalog. In total, we detect 1,042 unique broad-line galaxies with luminosities between 10$^{37}$ - 10$^{43}$ erg s$^{-1}$ and inferred supermassive black hole masses between 10$^{5}$ - 10$^{10}$ $M_{\\odot}$; 60 feature both broad H$\\alpha$ and broad H$\\beta$ emission. We also determine that the broad line region radius ranges between 0.01 - 46 light days, with a median radius of 0.1 light days (0.02 pc) for our broad H$\\beta$ sample. In addition, we find that both samples feature a higher fraction of galaxy mergers (44% for the broad H$\\alpha$ sample and 43% for the broad H$\\beta$ sample), compared to the full MaNGA galaxy sample (26%), which suggests that merger-driven fueling is strongly active in our sample.","sentences":["Broad H$\\alpha$ and H$\\beta$ emission lines (FWHM > 1,000 km s$^{-1}$) are incredibly efficient tracers of the high-velocity clouds encircling Active Galactic Nuclei (AGN).","As a result, we search for these broad line AGN in the Sloan Digital Sky Survey's Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) catalog.","We identify 301 broad-line H$\\alpha$ galaxies and 801 broad-line H$\\beta$ galaxies in the catalog.","In total, we detect 1,042 unique broad-line galaxies with luminosities between 10$^{37}$ - 10$^{43}$ erg s$^{-1}$ and inferred supermassive black hole masses between 10$^{5}$ - 10$^{10}$ $M_{\\odot}$; 60 feature both broad H$\\alpha$ and broad H$\\beta$ emission.","We also determine that the broad line region radius ranges between 0.01 - 46 light days, with a median radius of 0.1 light days (0.02 pc) for our broad H$\\beta$ sample.","In addition, we find that both samples feature a higher fraction of galaxy mergers (44% for the broad H$\\alpha$ sample and 43% for the broad H$\\beta$ sample), compared to the full MaNGA galaxy sample (26%), which suggests that merger-driven fueling is strongly active in our sample."],"url":"http://arxiv.org/abs/2405.12873v1","category":"astro-ph.GA"}
{"created":"2024-05-21 15:34:51","title":"Spin dependent bandgap renormalization and state filling effect in Bi$_2$Se$_3$ observed by ultrafast Kerr rotation","abstract":"We investigate the ultrafast spin dynamics of the prototypical topological insulator $\\mathrm{Bi_{2}Se_{3}}$ using time-resolved Kerr-rotation (polarization-change) measurements across near-infrared wavelengths. The Kerr-rotation angle $\\Delta \\theta_{K}$ of $\\mathrm{Bi_{2}Se_{3}}$ was found to significantly depend on photon energy around a resonance transition ($\\sim 1.0\\ \\mathrm{eV}$) of bulk states, as well as the ellipticity of the pump light, in the presence of spin excitation. The observed photon-energy dependence of $\\Delta \\theta_{K}$ can be well simulated by assuming spin-dependent refractive-index changes in the presence of band-gap renormalization and state-filling effect upon photoexcitation. Our study delivers comprehensive insights into the opto-spintronic properties of bulk $\\mathrm{Bi_{2}Se_{3}}$ and the fundamental physical processes underlying polarization changes. These findings are expected to be crucial in developing ultrafast magneto-optical memory devices, which can perform read-and-write operations in the Terahertz regime.","sentences":["We investigate the ultrafast spin dynamics of the prototypical topological insulator $\\mathrm{Bi_{2}Se_{3}}$ using time-resolved Kerr-rotation (polarization-change) measurements across near-infrared wavelengths.","The Kerr-rotation angle $\\Delta \\theta_{K}$ of $\\mathrm{Bi_{2}Se_{3}}$ was found to significantly depend on photon energy around a resonance transition ($\\sim 1.0\\ \\mathrm{eV}$) of bulk states, as well as the ellipticity of the pump light, in the presence of spin excitation.","The observed photon-energy dependence of $\\Delta \\theta_{K}$ can be well simulated by assuming spin-dependent refractive-index changes in the presence of band-gap renormalization and state-filling effect upon photoexcitation.","Our study delivers comprehensive insights into the opto-spintronic properties of bulk $\\mathrm{Bi_{2}Se_{3}}$ and the fundamental physical processes underlying polarization changes.","These findings are expected to be crucial in developing ultrafast magneto-optical memory devices, which can perform read-and-write operations in the Terahertz regime."],"url":"http://arxiv.org/abs/2405.12869v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 15:33:04","title":"Revisiting short-plateau SN 2018gj","abstract":"We present an alternative model of unusual type-IIP SN 2018gj. Despite the short plateau and early gamma-rays escape seeming to favor low-mass ejecta, our hydrodynamic model requires a large ejected mass (about 23 Msun). The high ejecta velocity, we find from hydrogen lines in early spectra, is among crucial constraints on the hydrodynamic model. We recover the wind density that rules out a notable contribution of the circumstellar interaction to the bolometric luminosity. The early radioactive gamma-rays escape is found to be due to the high velocity of Ni-56, whereas the asymmetry of the H-alpha emission is attributed to the asymmetry of the Ni-56 ejecta. The available sample of type-IIP supernovae studied hydrodynamically in a uniform way indicates that the asymmetry of the Ni-56 ejecta is probably their intrinsic property. Hydrogen lines in the early spectra of SN 2018gi and SN 2020jfo are found to imply a clumpy structure of the outer ejecta. With two already known similar cases of SN 2008in and SN 2012A we speculate that the clumpiness of the outer ejecta is inherent to type-IIP supernovae related to the red supergiant explosion.","sentences":["We present an alternative model of unusual type-IIP SN 2018gj.","Despite the short plateau and early gamma-rays escape seeming to favor low-mass ejecta, our hydrodynamic model requires a large ejected mass (about 23 Msun).","The high ejecta velocity, we find from hydrogen lines in early spectra, is among crucial constraints on the hydrodynamic model.","We recover the wind density that rules out a notable contribution of the circumstellar interaction to the bolometric luminosity.","The early radioactive gamma-rays escape is found to be due to the high velocity of Ni-56, whereas the asymmetry of the H-alpha emission is attributed to the asymmetry of the Ni-56 ejecta.","The available sample of type-IIP supernovae studied hydrodynamically in a uniform way indicates that the asymmetry of the Ni-56 ejecta is probably their intrinsic property.","Hydrogen lines in the early spectra of SN 2018gi and SN 2020jfo are found to imply a clumpy structure of the outer ejecta.","With two already known similar cases of SN 2008in and SN 2012A we speculate that the clumpiness of the outer ejecta is inherent to type-IIP supernovae related to the red supergiant explosion."],"url":"http://arxiv.org/abs/2405.12867v1","category":"astro-ph.HE"}
{"created":"2024-05-21 14:33:46","title":"Pick-and-place transfer of arbitrary-metal electrodes for van der Waals device fabrication","abstract":"Van der Waals electrode integration is a promising strategy to create near-perfect interfaces between metals and two-dimensional materials, with advantages such as eliminating Fermi-level pinning and reducing contact resistance. However, the lack of a simple, generalizable pick-and-place transfer technology has greatly hampered the wide use of this technique. We demonstrate the pick-and-place transfer of pre-fabricated electrodes from reusable polished hydrogenated diamond substrates without the use of any surface treatments or sacrificial layers. The technique enables transfer of large-scale arbitrary metal electrodes, as demonstrated by successful transfer of eight different elemental metals with work functions ranging from 4.22 to 5.65 eV. The mechanical transfer of metal electrodes from diamond onto van der Waals materials creates atomically smooth interfaces with no interstitial impurities or disorder, as observed with cross-sectional high-resolution transmission electron microscopy and energy-dispersive X-ray spectroscopy. As a demonstration of its device application, we use the diamond-transfer technique to create metal contacts to monolayer transition metal dichalcogenide semiconductors with high-work-function Pd, low-work-function Ti, and semi metal Bi to create n- and p-type field-effect transistors with low Schottky barrier heights. We also extend this technology to other applications such as ambipolar transistor and optoelectronics, paving the way for new device architectures and high-performance devices.","sentences":["Van der Waals electrode integration is a promising strategy to create near-perfect interfaces between metals and two-dimensional materials, with advantages such as eliminating Fermi-level pinning and reducing contact resistance.","However, the lack of a simple, generalizable pick-and-place transfer technology has greatly hampered the wide use of this technique.","We demonstrate the pick-and-place transfer of pre-fabricated electrodes from reusable polished hydrogenated diamond substrates without the use of any surface treatments or sacrificial layers.","The technique enables transfer of large-scale arbitrary metal electrodes, as demonstrated by successful transfer of eight different elemental metals with work functions ranging from 4.22 to 5.65 eV. The mechanical transfer of metal electrodes from diamond onto van der Waals materials creates atomically smooth interfaces with no interstitial impurities or disorder, as observed with cross-sectional high-resolution transmission electron microscopy and energy-dispersive X-ray spectroscopy.","As a demonstration of its device application, we use the diamond-transfer technique to create metal contacts to monolayer transition metal dichalcogenide semiconductors with high-work-function Pd, low-work-function Ti, and semi metal Bi to create n- and p-type field-effect transistors with low Schottky barrier heights.","We also extend this technology to other applications such as ambipolar transistor and optoelectronics, paving the way for new device architectures and high-performance devices."],"url":"http://arxiv.org/abs/2405.12830v1","category":"physics.app-ph"}
{"created":"2024-05-21 14:04:03","title":"Revisiting radiative decays of single-charm baryons in the high-precision era of hadron spectroscopy","abstract":"In this work, we systematically study the radiative decays of single-charm baryons, which is one aspect of their spectroscopy behavior. In order to promote the accuracy of calculations, we adopt the numerical spatial wave functions of the single-charm baryons as the theoretical input, which are obtained by the Gaussian Expansion Method for getting the mass spectrum of the single-charm baryons. The present study of hadron spectroscopy is entering the era of high precision. We believe that the present work on the radiative decays of single-charm baryons can provide valuable information for further exploration of single-charm baryons.","sentences":["In this work, we systematically study the radiative decays of single-charm baryons, which is one aspect of their spectroscopy behavior.","In order to promote the accuracy of calculations, we adopt the numerical spatial wave functions of the single-charm baryons as the theoretical input, which are obtained by the Gaussian Expansion Method for getting the mass spectrum of the single-charm baryons.","The present study of hadron spectroscopy is entering the era of high precision.","We believe that the present work on the radiative decays of single-charm baryons can provide valuable information for further exploration of single-charm baryons."],"url":"http://arxiv.org/abs/2405.12812v1","category":"hep-ph"}
{"created":"2024-05-21 13:59:54","title":"Heterogeneous antiferroelectric ordering in NaNbO3-SrSnO3 ceramics revealed by direct superstructure imaging","abstract":"NaNbO3-based antiferroelectric materials offer a promising pathway towards greener and more cost-effective energy storage devices. However, their intrinsic structural instabilities often lead to reduced energy density that compromises their performance and longevity. In this brief communication, we demonstrate how Dark-Field X-ray Microscopy, when carried out on the characteristically weak 1/4{843}pc superstructure reflection, can map the antiferroelectric phase and its strain heterogeneity in typically small, deeply-embedded grains of a NaNbO3 and 0.95NaNbO3-0.05SrSnO3 ceramics, representative of different phase transition behavior. Our results clearly evidences the stabilizing effect of SrSnO3 on the antiferroelectric phase by enhancing the degree of mesostructural order. In doing so, our method establishes a new platform for exploring the impact of disorder on the long-range strain heterogeneity within antiferroelectrics and other materials with modulated crystal structures.","sentences":["NaNbO3-based antiferroelectric materials offer a promising pathway towards greener and more cost-effective energy storage devices.","However, their intrinsic structural instabilities often lead to reduced energy density that compromises their performance and longevity.","In this brief communication, we demonstrate how Dark-Field X-ray Microscopy, when carried out on the characteristically weak 1/4{843}pc superstructure reflection, can map the antiferroelectric phase and its strain heterogeneity in typically small, deeply-embedded grains of a NaNbO3 and 0.95NaNbO3-0.05SrSnO3 ceramics, representative of different phase transition behavior.","Our results clearly evidences the stabilizing effect of SrSnO3 on the antiferroelectric phase by enhancing the degree of mesostructural order.","In doing so, our method establishes a new platform for exploring the impact of disorder on the long-range strain heterogeneity within antiferroelectrics and other materials with modulated crystal structures."],"url":"http://arxiv.org/abs/2405.12810v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 13:27:27","title":"Excitation of narrow x-ray transitions in thin-film cavities by focused pulses","abstract":"A method to compute the excitation of narrow transitions at hard x-ray energies by short focused x-ray pulses is developed. In particular, the effect of thin-film cavities on the pulse propagation is incorporated via a semi-analytical algorithm requiring the numerical evaluation of only one two- dimensional Fourier transform. We investigate various limiting cases to confirm the reliability of the algorithm. As an application, we show how a focused x-ray pulse propagates in cavity structures utilized in previous theoretical studies and experiments with collimated beams.","sentences":["A method to compute the excitation of narrow transitions at hard x-ray energies by short focused x-ray pulses is developed.","In particular, the effect of thin-film cavities on the pulse propagation is incorporated via a semi-analytical algorithm requiring the numerical evaluation of only one two- dimensional Fourier transform.","We investigate various limiting cases to confirm the reliability of the algorithm.","As an application, we show how a focused x-ray pulse propagates in cavity structures utilized in previous theoretical studies and experiments with collimated beams."],"url":"http://arxiv.org/abs/2405.12780v1","category":"quant-ph"}
{"created":"2024-05-21 13:11:21","title":"Mixing and $C\\!P$ violation in beauty and charm at LHCb","abstract":"These proceedings discuss recent measurements by the LHCb experiment on mixing and $C\\!P$ violation with beauty and charm mesons, as presented at the Moriond QCD 2024 conference. All discussed measurements show agreement with the Standard Model.","sentences":["These proceedings discuss recent measurements by the LHCb experiment on mixing and $C\\!P$ violation with beauty and charm mesons, as presented at the Moriond QCD 2024 conference.","All discussed measurements show agreement with the Standard Model."],"url":"http://arxiv.org/abs/2405.12760v1","category":"hep-ex"}
{"created":"2024-05-21 11:52:01","title":"Demystifying 5G NR Downlink Synchronization for Tactical Networks","abstract":"5G NR is touted to be an attractive candidate for tactical networks owing to its versatility, scalability, and low cost. However, tactical networks need to be stealthy, where an adversary is not able to detect or intercept the tactical communication. In this paper, we investigate the stealthiness of 5G NR by looking at the probability with which an adversary that monitors the downlink synchronization signals can detect the presence of the network. We simulate a single-cell single-eavesdropper scenario and evaluate the probability with which the eavesdropper can detect the synchronization signal block when using either a correlator or an energy detector. We show that this probability is close to $ 100 $% suggesting that 5G out-of-the-box is not suitable for a tactical network. We then propose methods that lower this value without affecting the performance of a legitimate tactical UE.","sentences":["5G NR is touted to be an attractive candidate for tactical networks owing to its versatility, scalability, and low cost.","However, tactical networks need to be stealthy, where an adversary is not able to detect or intercept the tactical communication.","In this paper, we investigate the stealthiness of 5G NR by looking at the probability with which an adversary that monitors the downlink synchronization signals can detect the presence of the network.","We simulate a single-cell single-eavesdropper scenario and evaluate the probability with which the eavesdropper can detect the synchronization signal block when using either a correlator or an energy detector.","We show that this probability is close to $ 100 $% suggesting that 5G out-of-the-box is not suitable for a tactical network.","We then propose methods that lower this value without affecting the performance of a legitimate tactical UE."],"url":"http://arxiv.org/abs/2405.12704v1","category":"eess.SP"}
{"created":"2024-05-21 11:33:44","title":"Prompt fission neutron spectra of 240Pu(n, F)","abstract":"Variation of fission neutron spectra of 240Pu (sf) and 240Pu (n, F) for E up to 20 MeV predicted.Features of angle-integrated prompt fission neutron spectra of 240Pu (n, F) stem from simultaneous analysis of data for 238U (n, F), 239Pu (n, F) and available data on 240Pu (n, F) prompt fission neutron spectra.The data on average energies of 240Pu (n, F) prompt fission neutron spectra support the approach pursued in case of 238U (n, F) and 239Pu (n, F). Soft influence of exclusive neutron spectra on prompt fission neutron spectra observed in case of 240Pu (n, F) and 240Pu (n, xnf) at E = 7-8 MeV. The largest relative amplitude of exclusive neutron spectra 240Pu (n, xnf) is envisaged at E = 6-6.25 MeV. Prompt fission neutron spectra of 240Pu (n, F) are harder than those of 238U (n,F), but softer than prompt fission neutron spectra for 239Pu(n, F). 240Pu (n, F) prompt fission neutron spectra 240Pu shape is rather close to that of 239Pu(n, F), though the contribution of pre-fission neutrons is relatively higher. Exclusive neutron spectra (n, xnf) are consistent with (n,F) cross sections of 237-240Pu(n, F), as well as neutron emissive spectra of 239Pu(n, xn) at E = 14 MeV. We predict the exclusive pre-fission neutron spectra, exclusive neutron spectra of (n,xn) reactions, average total kinetic energy TKE of fission fragments and products, partials of of average prompt fission neutron number and observed prompt fission neutron spectra.","sentences":["Variation of fission neutron spectra of 240Pu (sf) and 240Pu (n, F) for E up to 20 MeV predicted.","Features of angle-integrated prompt fission neutron spectra of 240Pu (n, F) stem from simultaneous analysis of data for 238U (n, F), 239Pu (n, F) and available data on 240Pu (n, F) prompt fission neutron spectra.","The data on average energies of 240Pu (n, F) prompt fission neutron spectra support the approach pursued in case of 238U (n, F) and 239Pu (n, F).","Soft influence of exclusive neutron spectra on prompt fission neutron spectra observed in case of 240Pu (n, F) and 240Pu (n, xnf) at E = 7-8 MeV. The largest relative amplitude of exclusive neutron spectra 240Pu (n, xnf) is envisaged at E = 6-6.25 MeV.","Prompt fission neutron spectra of 240Pu (n, F) are harder than those of 238U (n,F), but softer than prompt fission neutron spectra for 239Pu(n, F).","240Pu (n, F) prompt fission neutron spectra 240Pu shape is rather close to that of 239Pu(n, F), though the contribution of pre-fission neutrons is relatively higher.","Exclusive neutron spectra (n, xnf) are consistent with (n,F) cross sections of 237-240Pu(n, F), as well as neutron emissive spectra of 239Pu(n, xn) at E = 14 MeV.","We predict the exclusive pre-fission neutron spectra, exclusive neutron spectra of (n,xn) reactions, average total kinetic energy TKE of fission fragments and products, partials of of average prompt fission neutron number and observed prompt fission neutron spectra."],"url":"http://arxiv.org/abs/2405.12693v1","category":"nucl-th"}
{"created":"2024-05-21 11:22:16","title":"Large band-splitting in $g$-wave type altermagnet CrSb","abstract":"Altermagnetism (AM), a newly discovered magnetic state, ingeniously integrates the properties of ferromagnetism and antiferromagnetism, representing a significant breakthrough in the field of magnetic materials. Despite experimental verification of some typical AM materials, such as MnTe and MnTe$_2$, the pursuit of AM materials that feature larger spin splitting and higher transition temperature is still essential. Here, our research focuses on CrSb, which possesses N{\\'e}el temperature of up to 700K and giant spin splitting near the Fermi level ($E_F$). Utilizing high-resolution angle-resolved photoemission spectroscopy and density functional theory calculations, we meticulously map the three-dimensional electronic structure of CrSb. Our photoemission spectroscopic results on both (0001) and (10$\\overline{1}$0) cleavages of CrSb collaboratively reveal unprecedented details on AM-induced band splitting, and subsequently pin down its unique bulk $g$-wave symmetry through quantitative analysis of the angular and photon-energy dependence of spin splitting. Moreover, the observed spin splitting reaches the magnitude of 0.93~eV near $E_F$, the most substantial among all confirmed AM materials. This study not only validates the nature of CrSb as a prototype $g$-wave like AM material but also underscores its pivotal role in pioneering applications in spintronics.","sentences":["Altermagnetism (AM), a newly discovered magnetic state, ingeniously integrates the properties of ferromagnetism and antiferromagnetism, representing a significant breakthrough in the field of magnetic materials.","Despite experimental verification of some typical AM materials, such as MnTe and MnTe$_2$, the pursuit of AM materials that feature larger spin splitting and higher transition temperature is still essential.","Here, our research focuses on CrSb, which possesses N{\\'e}el temperature of up to 700K and giant spin splitting near the Fermi level ($E_F$).","Utilizing high-resolution angle-resolved photoemission spectroscopy and density functional theory calculations, we meticulously map the three-dimensional electronic structure of CrSb.","Our photoemission spectroscopic results on both (0001) and (10$\\overline{1}$0) cleavages of CrSb collaboratively reveal unprecedented details on AM-induced band splitting, and subsequently pin down its unique bulk $g$-wave symmetry through quantitative analysis of the angular and photon-energy dependence of spin splitting.","Moreover, the observed spin splitting reaches the magnitude of 0.93~eV near $E_F$, the most substantial among all confirmed AM materials.","This study not only validates the nature of CrSb as a prototype $g$-wave like AM material but also underscores its pivotal role in pioneering applications in spintronics."],"url":"http://arxiv.org/abs/2405.12687v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 10:26:40","title":"Age of massive galaxies at redshift 8","abstract":"Recent James Webb Space Telescope (JWST) data analyses have shown that massive red galaxies existed at redshifts $z>6$, a discovery that is difficult to understand in the context of standard cosmology ($\\Lambda $CDM). Here we analyze these observations more deeply by fitting a stellar population model to the optical and near-infrared photometric data. These fits include a main stellar population in addition to a residual younger population and with the same extinction for both (a lower extinction for the younger population is unphysical). Extra stellar populations or the inclusion of an AGN component do not significantly improve the fits. These galaxies are being viewed at very high redshifts, with an average $\\langle z\\rangle \\approx 8.2$, when the $\\Lambda$CDM Universe was only $\\approx 600$ Myr old. This result conflicts with the inferred ages of these galaxies, however, which were on average between 0.9 and 2.4 Gyr old within 95% CL. Given the sequence of star formation and galaxy assembly in the standard model, these galaxies should instead be even younger than 290 Myr on average, for which our analysis assigns a probability of only $<3\\times 10^{-4}$ ($\\gtrsim 3.6\\sigma $ tension). This outcome may indicate the need to consider non-standard cosmologies. Nevertheless, our conclusions result from several approximations in stellar astrophysics and extinction, so they should be taken with a grain of salt. Further research is necessary to corroborate the possible existence of galaxies older than the $\\Lambda $CDM universe at their observed redshifts.","sentences":["Recent James Webb Space Telescope (JWST) data analyses have shown that massive red galaxies existed at redshifts $z>6$, a discovery that is difficult to understand in the context of standard cosmology ($\\Lambda $CDM).","Here we analyze these observations more deeply by fitting a stellar population model to the optical and near-infrared photometric data.","These fits include a main stellar population in addition to a residual younger population and with the same extinction for both (a lower extinction for the younger population is unphysical).","Extra stellar populations or the inclusion of an AGN component do not significantly improve the fits.","These galaxies are being viewed at very high redshifts, with an average $\\langle z\\rangle \\approx 8.2$, when the $\\Lambda$CDM Universe was only $\\approx 600$ Myr old.","This result conflicts with the inferred ages of these galaxies, however, which were on average between 0.9 and 2.4 Gyr old within 95% CL.","Given the sequence of star formation and galaxy assembly in the standard model, these galaxies should instead be even younger than 290 Myr on average, for which our analysis assigns a probability of only $<3\\times 10^{-4}$ ($\\gtrsim 3.6\\sigma $ tension).","This outcome may indicate the need to consider non-standard cosmologies.","Nevertheless, our conclusions result from several approximations in stellar astrophysics and extinction, so they should be taken with a grain of salt.","Further research is necessary to corroborate the possible existence of galaxies older than the $\\Lambda $CDM universe at their observed redshifts."],"url":"http://arxiv.org/abs/2405.12665v1","category":"astro-ph.CO"}
{"created":"2024-05-21 09:14:40","title":"A modified expression for the Hamiltonian expectation value exploiting the short-range behavior of the wave function","abstract":"The expectation value of the Hamiltonian using a model wave function is widely used to estimate the eigenvalues of electronic Hamiltonians. We explore here a modified formula for models based on long-range interaction. It scales differently the singlet and triplet component of the repulsion between electrons not present in the model (its short-range part). The scaling factors depend uniquely on the parameter used in defining the model interaction, and are constructed using only exact properties. We show results for the ground states and low-lying excited states of Harmonium with two to six electrons. We obtain important improvements for the estimation of the exact energy, not only over the model energy, but also over the expectation value of the Hamiltonian.","sentences":["The expectation value of the Hamiltonian using a model wave function is widely used to estimate the eigenvalues of electronic Hamiltonians.","We explore here a modified formula for models based on long-range interaction.","It scales differently the singlet and triplet component of the repulsion between electrons not present in the model (its short-range part).","The scaling factors depend uniquely on the parameter used in defining the model interaction, and are constructed using only exact properties.","We show results for the ground states and low-lying excited states of Harmonium with two to six electrons.","We obtain important improvements for the estimation of the exact energy, not only over the model energy, but also over the expectation value of the Hamiltonian."],"url":"http://arxiv.org/abs/2405.12618v1","category":"physics.chem-ph"}
{"created":"2024-05-21 08:37:28","title":"Statistical Qubit Freezing Extending Physical Limit of Quantum Annealers","abstract":"Adiabatic quantum annealers encounter scalability challenges due to exponentially fast diminishing energy gaps between ground and excited states with qubit-count increase. This introduces errors in identifying ground states compounded by a thermal noise. We propose a novel algorithmic scheme called statistical qubit freezing (SQF) that selectively fixes the state of statistically deterministic qubit in the annealing Hamiltonian model of the given problem. Applying freezing repeatedly, SQF significantly enhances the spectral gap between of an adiabatic process, as an example, by up to 60\\% compared to traditional annealing methods in the standard D-Wave's quantum Ising machine solution, effectively overcoming the fundamental limitations.","sentences":["Adiabatic quantum annealers encounter scalability challenges due to exponentially fast diminishing energy gaps between ground and excited states with qubit-count increase.","This introduces errors in identifying ground states compounded by a thermal noise.","We propose a novel algorithmic scheme called statistical qubit freezing (SQF) that selectively fixes the state of statistically deterministic qubit in the annealing Hamiltonian model of the given problem.","Applying freezing repeatedly, SQF significantly enhances the spectral gap between of an adiabatic process, as an example, by up to 60\\% compared to traditional annealing methods in the standard D-Wave's quantum Ising machine solution, effectively overcoming the fundamental limitations."],"url":"http://arxiv.org/abs/2405.12594v1","category":"quant-ph"}
{"created":"2024-05-21 08:24:38","title":"Spectral analysis for noisy Hawkes processes inference","abstract":"Classic estimation methods for Hawkes processes rely on the assumption that observed event times are indeed a realisation of a Hawkes process, without considering any potential perturbation of the model. However, in practice, observations are often altered by some noise, the form of which depends on the context.It is then required to model the alteration mechanism in order to infer accurately such a noisy Hawkes process. While several models exist, we consider, in this work, the observations to be the indistinguishable union of event times coming from a Hawkes process and from an independent Poisson process. Since standard inference methods (such as maximum likelihood or Expectation-Maximisation) are either unworkable or numerically prohibitive in this context, we propose an estimation procedure based on the spectral analysis of second order properties of the noisy Hawkes process. Novel results include sufficient conditions for identifiability of the ensuing statistical model with exponential interaction functions for both univariate and bivariate processes. Although we mainly focus on the exponential scenario, other types of kernels are investigated and discussed. A new estimator based on maximising the spectral log-likelihood is then described, and its behaviour is numerically illustrated on synthetic data. Besides being free from knowing the source of each observed time (Hawkes or Poisson process), the proposed estimator is shown to perform accurately in estimating both processes.","sentences":["Classic estimation methods for Hawkes processes rely on the assumption that observed event times are indeed a realisation of a Hawkes process, without considering any potential perturbation of the model.","However, in practice, observations are often altered by some noise, the form of which depends on the context.","It is then required to model the alteration mechanism in order to infer accurately such a noisy Hawkes process.","While several models exist, we consider, in this work, the observations to be the indistinguishable union of event times coming from a Hawkes process and from an independent Poisson process.","Since standard inference methods (such as maximum likelihood or Expectation-Maximisation) are either unworkable or numerically prohibitive in this context, we propose an estimation procedure based on the spectral analysis of second order properties of the noisy Hawkes process.","Novel results include sufficient conditions for identifiability of the ensuing statistical model with exponential interaction functions for both univariate and bivariate processes.","Although we mainly focus on the exponential scenario, other types of kernels are investigated and discussed.","A new estimator based on maximising the spectral log-likelihood is then described, and its behaviour is numerically illustrated on synthetic data.","Besides being free from knowing the source of each observed time (Hawkes or Poisson process), the proposed estimator is shown to perform accurately in estimating both processes."],"url":"http://arxiv.org/abs/2405.12581v1","category":"stat.ME"}
{"created":"2024-05-21 07:51:01","title":"Online Signature Recognition: A Biologically Inspired Feature Vector Splitting Approach","abstract":"This research introduces an innovative approach to explore the cognitive and biologically inspired underpinnings of feature vector splitting for analyzing the significance of different attributes in e-security biometric signature recognition applications. Departing from traditional methods of concatenating features into an extended set, we employ multiple splitting strategies, aligning with cognitive principles, to preserve control over the relative importance of each feature subset. Our methodology is applied to three diverse databases (MCYT100, MCYT300,and SVC) using two classifiers (vector quantization and dynamic time warping with one and five training samples). Experimentation demonstrates that the fusion of pressure data with spatial coordinates (x and y) consistently enhances performance. However, the inclusion of pen-tip angles in the same feature set yields mixed results, with performance improvements observed in select cases. This work delves into the cognitive aspects of feature fusion,shedding light on the cognitive relevance of feature vector splitting in e-security biometric applications.","sentences":["This research introduces an innovative approach to explore the cognitive and biologically inspired underpinnings of feature vector splitting for analyzing the significance of different attributes in e-security biometric signature recognition applications.","Departing from traditional methods of concatenating features into an extended set, we employ multiple splitting strategies, aligning with cognitive principles, to preserve control over the relative importance of each feature subset.","Our methodology is applied to three diverse databases (MCYT100, MCYT300,and SVC) using two classifiers (vector quantization and dynamic time warping with one and five training samples).","Experimentation demonstrates that the fusion of pressure data with spatial coordinates (x and y) consistently enhances performance.","However, the inclusion of pen-tip angles in the same feature set yields mixed results, with performance improvements observed in select cases.","This work delves into the cognitive aspects of feature fusion,shedding light on the cognitive relevance of feature vector splitting in e-security biometric applications."],"url":"http://arxiv.org/abs/2405.12556v1","category":"cs.CV"}
{"created":"2024-05-21 07:24:33","title":"An explicit log-free zero density estimate for the Riemann zeta-function","abstract":"We will provide the first explicit log-free zero-density estimate for $\\zeta(s)$ of the form $N(\\sigma,T)\\le AT^{B(1-\\sigma)}$. In particular, this estimate becomes the sharpest known explicit zero-density estimate uniformly for $\\sigma\\in[\\alpha_0,1]$, with $0.985\\le \\alpha_0\\le 0.992$ and $3\\cdot 10^{12}<T\\le \\exp(6.7\\cdot 10^{12})$.","sentences":["We will provide the first explicit log-free zero-density estimate for $\\zeta(s)$ of the form $N(\\sigma,T)\\le AT^{B(1-\\sigma)}$. In particular, this estimate becomes the sharpest known explicit zero-density estimate uniformly for $\\sigma\\in[\\alpha_0,1]$, with $0.985\\le \\alpha_0\\le 0.992$ and $3\\cdot 10^{12}<T\\le \\exp(6.7\\cdot 10^{12})$."],"url":"http://arxiv.org/abs/2405.12545v1","category":"math.NT"}
{"created":"2024-05-21 05:45:38","title":"Electric field induction in quark-gluon plasma due to thermoelectric effects","abstract":"Relativistic heavy-ion collisions produce quark-gluon plasma (QGP), which is locally thermalized. QGP with higher thermal conductivity advances towards global thermalization. Being made of electrically charged particles (quarks), QGP exhibits interesting thermoelectric phenomena during its evolution, resulting in the induction of an electric field in the medium. For the first time, we estimate the induced electric field in the QGP due to thermoelectric effect. This can be seen even in the QGP produced in the head-on collisions. The Seebeck coefficient is essential in determining the induced field. However, spectator current can produce a magnetic field in peripheral heavy-ion collisions. This breaks the isotropy of the thermoelectric coefficient matrix and introduces magneto-Seebeck and Nernst coefficients that contribute to the induced electric field in peripheral collisions. We have taken care of the temperature evolution of QGP with different hydrodynamic cooling rates to calculate the transport coefficients. The induced electric field is estimated with the cooling rate obtained from Gubser hydrodynamic flow. We estimated the space-time profile of the induced field and found that it is zero at the center and increases as we go away from the center. At the early time of evolution, the electric field can reach a maximum value of $eE \\approx 1~m_\\pi^2$, and the strength decreases in time. Moreover, we estimated the transport coefficients in the presence of the external time-varying magnetic field. The effect of the intensity and decay parameter of the magnetic field on the induced electric field is also explored.","sentences":["Relativistic heavy-ion collisions produce quark-gluon plasma (QGP), which is locally thermalized.","QGP with higher thermal conductivity advances towards global thermalization.","Being made of electrically charged particles (quarks), QGP exhibits interesting thermoelectric phenomena during its evolution, resulting in the induction of an electric field in the medium.","For the first time, we estimate the induced electric field in the QGP due to thermoelectric effect.","This can be seen even in the QGP produced in the head-on collisions.","The Seebeck coefficient is essential in determining the induced field.","However, spectator current can produce a magnetic field in peripheral heavy-ion collisions.","This breaks the isotropy of the thermoelectric coefficient matrix and introduces magneto-Seebeck and Nernst coefficients that contribute to the induced electric field in peripheral collisions.","We have taken care of the temperature evolution of QGP with different hydrodynamic cooling rates to calculate the transport coefficients.","The induced electric field is estimated with the cooling rate obtained from Gubser hydrodynamic flow.","We estimated the space-time profile of the induced field and found that it is zero at the center and increases as we go away from the center.","At the early time of evolution, the electric field can reach a maximum value of $eE \\approx 1~m_\\pi^2$, and the strength decreases in time.","Moreover, we estimated the transport coefficients in the presence of the external time-varying magnetic field.","The effect of the intensity and decay parameter of the magnetic field on the induced electric field is also explored."],"url":"http://arxiv.org/abs/2405.12510v1","category":"hep-ph"}
{"created":"2024-05-21 05:12:44","title":"The origin of large emission line widths in massive galaxies at redshifts $z\\sim 3-4$","abstract":"We present a sample of 22 massive galaxies with stellar masses $>10^{10} M_{\\odot}$ at $3<z<4$ with deep H and K-band high resolution spectra (R=3500-3000) from Keck/MOSFIRE and VLT/KMOS near-infrared spectrographs. We find a large fraction have strong [OIII]5007 and H$\\beta$ emission lines with large line widths ($\\sigma$ 100 -- 450 km/s). We measure the sizes of our galaxies from Hubble Space Telescope images and consider the potential kinematic scaling relations of our sample; and rule out an explanation for these broad lines in terms of galaxy-wide kinematics. Based on consideration of the [OIII]5007 $/$ H$\\beta$ flux ratios, their location in the Mass--Excitation diagram, and the derived bolometric luminosities, we conclude that Active Galactic Nuclei (AGN) and their Narrow Line Regions most likely give rise to this emission. At redshifts $3<z<4$, we find significantly high AGN fractions in massive galaxies, ranging from 60--70\\% for the mass range $10<\\log(M_{\\star}/M_{\\odot})<11$, with a lower limit 30\\% for all galaxies within that redshift range when we apply our most stringent AGN criteria. We also find a considerably lower AGN fraction in massive quiescent galaxies, ranging from 20-30\\%. These fractions of AGN point to the period between $3<z<4$ being a time of heightened activity for the development of supermassive black holes in the massive end of the galaxy population and provide evidence for their role in the emergence of the first massive quenched galaxies at this epoch.","sentences":["We present a sample of 22 massive galaxies with stellar masses $>10^{10} M_{\\odot}$ at $3<z<4$ with deep H and K-band high resolution spectra (R=3500-3000) from Keck/MOSFIRE and VLT/KMOS near-infrared spectrographs.","We find a large fraction have strong [OIII]5007 and H$\\beta$ emission lines with large line widths ($\\sigma$ 100 -- 450 km/s).","We measure the sizes of our galaxies from Hubble Space Telescope images and consider the potential kinematic scaling relations of our sample; and rule out an explanation for these broad lines in terms of galaxy-wide kinematics.","Based on consideration of the [OIII]5007 $/$ H$\\beta$ flux ratios, their location in the Mass--Excitation diagram, and the derived bolometric luminosities, we conclude that Active Galactic Nuclei (AGN) and their Narrow Line Regions most likely give rise to this emission.","At redshifts $3<z<4$, we find significantly high AGN fractions in massive galaxies, ranging from 60--70\\% for the mass range $10<\\log(M_{\\star}/M_{\\odot})<11$, with a lower limit 30\\% for all galaxies within that redshift range when we apply our most stringent AGN criteria.","We also find a considerably lower AGN fraction in massive quiescent galaxies, ranging from 20-30\\%.","These fractions of AGN point to the period between $3<z<4$ being a time of heightened activity for the development of supermassive black holes in the massive end of the galaxy population and provide evidence for their role in the emergence of the first massive quenched galaxies at this epoch."],"url":"http://arxiv.org/abs/2405.12501v1","category":"astro-ph.GA"}
{"created":"2024-05-21 04:26:35","title":"The influence of the Sun and Moon on the observation of very high energy gamma-ray sources using EAS arrays","abstract":"With great advance of ground-based extensive air shower array, such as LHAASO and HAWC, many very high energy (VHE) gamma-ray sources have been discovered and are been monitored regardless of the day and the night. Hence, the Sun and Moon would have some compact on the observation of gamma-ray sources, which have not been taken into account in previous analysis. In this paper, the influence of the Sun and Moon on the observation of very high energy gamma-ray sources when they are near the line of sight of the Sun or Moon is estimated. The tracks of all the known VHE sources are scanned and several VHE sources are found to be very close to the line of sight of the Sun or Moon during some period. The absorption of very high energy gamma-ray by sunlight is estimated with detailed method and some usefully conclusions are achieved. The main influence is the block of the Sun and Moon on gamma-ray and their shadow on the cosmic ray background. The influence is investigated considering the detector angular resolution and some strategy on data analysis are proposed to avoid the underestimation of the gamma-ray emission.","sentences":["With great advance of ground-based extensive air shower array, such as LHAASO and HAWC, many very high energy (VHE) gamma-ray sources have been discovered and are been monitored regardless of the day and the night.","Hence, the Sun and Moon would have some compact on the observation of gamma-ray sources, which have not been taken into account in previous analysis.","In this paper, the influence of the Sun and Moon on the observation of very high energy gamma-ray sources when they are near the line of sight of the Sun or Moon is estimated.","The tracks of all the known VHE sources are scanned and several VHE sources are found to be very close to the line of sight of the Sun or Moon during some period.","The absorption of very high energy gamma-ray by sunlight is estimated with detailed method and some usefully conclusions are achieved.","The main influence is the block of the Sun and Moon on gamma-ray and their shadow on the cosmic ray background.","The influence is investigated considering the detector angular resolution and some strategy on data analysis are proposed to avoid the underestimation of the gamma-ray emission."],"url":"http://arxiv.org/abs/2405.12492v1","category":"astro-ph.IM"}
{"created":"2024-05-21 02:12:50","title":"A Compact Readout Electronics Based on Current Amplifier for Micromegas Detector in Muon Image","abstract":"Muon imaging technology is an innovative imaging technique that can be applied in volcano imaging, heavy nuclear material detection, and archaeological research. The Micromegas detector is a promising choice for muon imaging due to its high spatial resolution and large area. However, the large number of readout channels poses a challenge for electronics. In this paper, a compact front-end electronics (FEE) for reading Micromegas detectors is presented. The electronics use a current-based readout chip, ADAS1128, which integrates 128 current amplifiers for multi-channel charge measurement. After noise and calibration tests, detector energy resolution tests were performed using 5.9 keV X-rays. The X-ray full energy peak and Ar escape peak can be observed; the energy resolution of the full energy peak is about 20.23%@5.9 keV. Additionally, a muon image system was set up to verify the imaging capacity of Micromegas detectors. Test results show that the spatial resolution of the system is better than 200 {\\mu}m. The muon image system can reconstruct the boundaries for objects with a size of 2 cm.","sentences":["Muon imaging technology is an innovative imaging technique that can be applied in volcano imaging, heavy nuclear material detection, and archaeological research.","The Micromegas detector is a promising choice for muon imaging due to its high spatial resolution and large area.","However, the large number of readout channels poses a challenge for electronics.","In this paper, a compact front-end electronics (FEE) for reading Micromegas detectors is presented.","The electronics use a current-based readout chip, ADAS1128, which integrates 128 current amplifiers for multi-channel charge measurement.","After noise and calibration tests, detector energy resolution tests were performed using 5.9 keV X-rays.","The X-ray full energy peak and Ar escape peak can be observed; the energy resolution of the full energy peak is about 20.23%@5.9","keV.","Additionally, a muon image system was set up to verify the imaging capacity of Micromegas detectors.","Test results show that the spatial resolution of the system is better than 200 {\\mu}m.","The muon image system can reconstruct the boundaries for objects with a size of 2 cm."],"url":"http://arxiv.org/abs/2405.12454v1","category":"physics.ins-det"}
{"created":"2024-05-21 01:11:02","title":"Studying magnetic reconnection with synchrotron polarization statistics","abstract":"Magnetic reconnection is a fundamental process for releasing magnetic energy in space physics and astrophysics. At present, the usual way to investigate the reconnection process is through analytical studies or first-principles numerical simulations. This paper is the first to understand the turbulent magnetic reconnection process by exploring the nature of magnetic turbulence. From the perspective of radio synchrotron polarization statistics, we study how to recover the properties of the turbulent magnetic field by considering the line of sight along different directions of the reconnection layer. We find that polarization intensity statistics can reveal the spectral properties of reconnection turbulence. This work opens up a new way of understanding turbulent magnetic reconnection.","sentences":["Magnetic reconnection is a fundamental process for releasing magnetic energy in space physics and astrophysics.","At present, the usual way to investigate the reconnection process is through analytical studies or first-principles numerical simulations.","This paper is the first to understand the turbulent magnetic reconnection process by exploring the nature of magnetic turbulence.","From the perspective of radio synchrotron polarization statistics, we study how to recover the properties of the turbulent magnetic field by considering the line of sight along different directions of the reconnection layer.","We find that polarization intensity statistics can reveal the spectral properties of reconnection turbulence.","This work opens up a new way of understanding turbulent magnetic reconnection."],"url":"http://arxiv.org/abs/2405.12430v1","category":"astro-ph.HE"}
{"created":"2024-05-21 00:37:51","title":"Tunable Surface Plasmon-Polaritons Interaction in All-Metal Pyramidal Metasurfaces: Unveiling Principles and Significance for Biosensing Applications","abstract":"The strong coupling of plasmonic resonance modes in conductive pyramidal nanoparticles leads to an increase in the density of free charges on the surface. By ensuring plasmonic coupling in the pyramidal nanoparticle lattice, the achieved field intensity is potentiated. At the same time, a strong coupling between resonant modes is guaranteed, which results in the formation of new hybrid modes. In this manuscript, we demonstrated a tunable double anticrossing interaction that results from the interaction between two Localized Surface Plasmon Resonance (LSPR) modes and a Surface Plasmon Polariton (SPP) wave. The tuning is done as a function of the variation of the angle of incidence of the input electric field. From the double anticrossing, an increase in field intensity in a blue-shifted LSPR mode located in the red wavelength region is observed. This demonstrates that at certain angles of incidence, the intensity field obtained is strongly favored, which would be beneficial for applications such as Surface Enhancement Raman Spectroscopy (SERS). Nanoparticle-based lattices have been widely used for biosensor applications. However, one of the major limitations of this type of device is the low tolerance to high concentrations of biomolecules, which significantly affects their performance. According to the studies carried out for this manuscript, it was demonstrated that the implemented geometry allows for the observation of an LSPR mode, which is responsible for the control and synchronization of other perceived resonances. This mode remains almost invariant when subjected to structural variations or changes in the angle of incidence of the electric field. These characteristics eliminate the limitation mentioned above, allowing for sensitivities 10^3 times higher than those achieved in conventional systems based on LSPR used to detect P. brasiliensis antigen.","sentences":["The strong coupling of plasmonic resonance modes in conductive pyramidal nanoparticles leads to an increase in the density of free charges on the surface.","By ensuring plasmonic coupling in the pyramidal nanoparticle lattice, the achieved field intensity is potentiated.","At the same time, a strong coupling between resonant modes is guaranteed, which results in the formation of new hybrid modes.","In this manuscript, we demonstrated a tunable double anticrossing interaction that results from the interaction between two Localized Surface Plasmon Resonance (LSPR) modes and a Surface Plasmon Polariton (SPP) wave.","The tuning is done as a function of the variation of the angle of incidence of the input electric field.","From the double anticrossing, an increase in field intensity in a blue-shifted LSPR mode located in the red wavelength region is observed.","This demonstrates that at certain angles of incidence, the intensity field obtained is strongly favored, which would be beneficial for applications such as Surface Enhancement Raman Spectroscopy (SERS).","Nanoparticle-based lattices have been widely used for biosensor applications.","However, one of the major limitations of this type of device is the low tolerance to high concentrations of biomolecules, which significantly affects their performance.","According to the studies carried out for this manuscript, it was demonstrated that the implemented geometry allows for the observation of an LSPR mode, which is responsible for the control and synchronization of other perceived resonances.","This mode remains almost invariant when subjected to structural variations or changes in the angle of incidence of the electric field.","These characteristics eliminate the limitation mentioned above, allowing for sensitivities 10^3 times higher than those achieved in conventional systems based on LSPR used to detect P. brasiliensis antigen."],"url":"http://arxiv.org/abs/2405.12428v1","category":"physics.optics"}
{"created":"2024-05-21 00:34:35","title":"Inferring Message Flows From System Communication Traces","abstract":"This paper proposes a novel method for automatically inferring message flow specifications from the communication traces of a system-on-chip (SoC) design that captures messages exchanged among the components during a system execution. The inferred message flows characterize the communication and coordination of components in a system design for realizing various system functions, and they are essential for SoC validation and debugging. The proposed method relieves the burden of manual development and maintenance of such specifications on human designers. Our method also uses a new accuracy metric, \\emph{acceptance ratio}, to evaluate the quality of the mined specifications instead of the specification size often used in the previous work, enabling more accurate specifications to be mined. Furthermore, this paper introduces the concept of essential causalities to enhance the accuracy of the message flow mining and accelerate the mining process. The effectiveness of the proposed method is evaluated on both synthetic traces and traces generated from executing several system models in GEM5. In both cases, the proposed method achieves superior accuracies compared to a previous approach. Additionally, this paper includes some practical use cases.","sentences":["This paper proposes a novel method for automatically inferring message flow specifications from the communication traces of a system-on-chip (SoC) design that captures messages exchanged among the components during a system execution.","The inferred message flows characterize the communication and coordination of components in a system design for realizing various system functions, and they are essential for SoC validation and debugging.","The proposed method relieves the burden of manual development and maintenance of such specifications on human designers.","Our method also uses a new accuracy metric, \\emph{acceptance ratio}, to evaluate the quality of the mined specifications instead of the specification size often used in the previous work, enabling more accurate specifications to be mined.","Furthermore, this paper introduces the concept of essential causalities to enhance the accuracy of the message flow mining and accelerate the mining process.","The effectiveness of the proposed method is evaluated on both synthetic traces and traces generated from executing several system models in GEM5.","In both cases, the proposed method achieves superior accuracies compared to a previous approach.","Additionally, this paper includes some practical use cases."],"url":"http://arxiv.org/abs/2405.12426v1","category":"cs.LO"}
{"created":"2024-05-20 23:46:57","title":"Cryogenic growth of tantalum thin films for low-loss superconducting circuits","abstract":"Motivated by recent advancements highlighting Ta as a promising material in low-loss superconducting circuits and showing long coherence times in superconducting qubits, we have explored the effect of cryogenic temperatures on the growth of Ta and its integration in superconducting circuits. Cryogenic growth of Ta using a low temperature molecular beam epitaxy (MBE) system is found to stabilize single phase $\\alpha$-Ta on several different substrates, which include Al$\\mathrm{_2}$O$\\mathrm{_3}$(0001), Si(001), Si(111), SiN${_x}$, and GaAs(001). The substrates are actively cooled down to cryogenic temperatures and remain < 20 K during the Ta deposition. X-ray $\\theta$-2$\\theta$ diffraction after warming to room temperature indicates the formation of polycrystalline $\\alpha$-Ta. The 50 nm $\\alpha$-Ta films grown on Al$\\mathrm{_2}$O$\\mathrm{_3}$(0001) at a substrate manipulator temperature of 7 K have a room temperature resistivity ($\\mathrm{\\rho _{300 K}}$) of 13.4 $\\mathrm{\\mu \\Omega}$cm, a residual resistivity ratio (RRR) of 17.3 and a superconducting transition temperature (T$_C$) of 4.14 K, which are comparable to bulk values. In addition, atomic force microscopy (AFM) indicates that the film grown at 7 K with an RMS roughness of 0.45 nm was significantly smoother than the one grown at room temperature. Similar properties are found for films grown on other substrates. Results for films grown at higher substrate manipulator temperatures show higher $\\mathrm{\\rho _{300 K}}$, lower RRR and Tc, and increased $\\beta$-Ta content. Coplanar waveguide resonators with a gap width of 3 $\\mathrm{\\mu}$m fabricated from cryogenically grown Ta on Si(111) and Al$\\mathrm{_2}$O$\\mathrm{_3}$(0001) show low power Q$_i$ of 1.9 million and 0.7 million, respectively, indicating polycrystalline $\\alpha$-Ta films may be promising for superconducting qubit applications even though they are not fully epitaxial.","sentences":["Motivated by recent advancements highlighting Ta as a promising material in low-loss superconducting circuits and showing long coherence times in superconducting qubits, we have explored the effect of cryogenic temperatures on the growth of Ta and its integration in superconducting circuits.","Cryogenic growth of Ta using a low temperature molecular beam epitaxy (MBE) system is found to stabilize single phase $\\alpha$-Ta on several different substrates, which include Al$\\mathrm{_2}$O$\\mathrm{_3}$(0001), Si(001), Si(111), SiN${_x}$, and GaAs(001).","The substrates are actively cooled down to cryogenic temperatures and remain < 20 K during the Ta deposition.","X-ray $\\theta$-2$\\theta$ diffraction after warming to room temperature indicates the formation of polycrystalline $\\alpha$-Ta.","The 50 nm $\\alpha$-Ta films grown on Al$\\mathrm{_2}$O$\\mathrm{_3}$(0001) at a substrate manipulator temperature of 7 K have a room temperature resistivity ($\\mathrm{\\rho _{300 K}}$) of 13.4 $\\mathrm{\\mu \\Omega}$cm, a residual resistivity ratio (RRR) of 17.3 and a superconducting transition temperature (T$_C$) of 4.14 K, which are comparable to bulk values.","In addition, atomic force microscopy (AFM) indicates that the film grown at 7 K with an RMS roughness of 0.45 nm was significantly smoother than the one grown at room temperature.","Similar properties are found for films grown on other substrates.","Results for films grown at higher substrate manipulator temperatures show higher $\\mathrm{\\rho _{300 K}}$, lower RRR and Tc, and increased $\\beta$-Ta content.","Coplanar waveguide resonators with a gap width of 3 $\\mathrm{\\mu}$m fabricated from cryogenically grown Ta on Si(111) and Al$\\mathrm{_2}$O$\\mathrm{_3}$(0001) show low power Q$_i$ of 1.9 million and 0.7 million, respectively, indicating polycrystalline $\\alpha$-Ta films may be promising for superconducting qubit applications even though they are not fully epitaxial."],"url":"http://arxiv.org/abs/2405.12417v1","category":"cond-mat.supr-con"}
{"created":"2024-05-20 23:39:30","title":"The Power of Two in Token Systems","abstract":"In economies without monetary transfers, token systems serve as an alternative to sustain cooperation, alleviate free riding, and increase efficiency. This paper studies whether a token-based economy can be effective in marketplaces with thin exogenous supply. We consider a marketplace in which at each time period one agent requests a service, one agent provides the service, and one token (artificial currency) is used to pay for service provision. The number of tokens each agent has represents the difference between the amount of service provisions and service requests by the agent. We are interested in the behavior of this economy when very few agents are available to provide the requested service. Since balancing the number of tokens across agents is key to sustain cooperation, the agent with the minimum amount of tokens is selected to provide service among the available agents. When exactly one random agent is available to provide service, we show that the token distribution is unstable. However, already when just two random agents are available to provide service, the token distribution is stable, in the sense that agents' token balance is unlikely to deviate much from their initial endowment, and agents return to their initial endowment in finite expected time. Our results mirror the power of two choices paradigm in load balancing problems. Supported by numerical simulations using kidney exchange data, our findings suggest that token systems may generate efficient outcomes in kidney exchange marketplaces by sustaining cooperation between hospitals.","sentences":["In economies without monetary transfers, token systems serve as an alternative to sustain cooperation, alleviate free riding, and increase efficiency.","This paper studies whether a token-based economy can be effective in marketplaces with thin exogenous supply.","We consider a marketplace in which at each time period one agent requests a service, one agent provides the service, and one token (artificial currency) is used to pay for service provision.","The number of tokens each agent has represents the difference between the amount of service provisions and service requests by the agent.","We are interested in the behavior of this economy when very few agents are available to provide the requested service.","Since balancing the number of tokens across agents is key to sustain cooperation, the agent with the minimum amount of tokens is selected to provide service among the available agents.","When exactly one random agent is available to provide service, we show that the token distribution is unstable.","However, already when just two random agents are available to provide service, the token distribution is stable, in the sense that agents' token balance is unlikely to deviate much from their initial endowment, and agents return to their initial endowment in finite expected time.","Our results mirror the power of two choices paradigm in load balancing problems.","Supported by numerical simulations using kidney exchange data, our findings suggest that token systems may generate efficient outcomes in kidney exchange marketplaces by sustaining cooperation between hospitals."],"url":"http://arxiv.org/abs/2405.12414v1","category":"cs.GT"}
{"created":"2024-05-20 23:29:58","title":"Decomposing causality into its synergistic, unique, and redundant components","abstract":"Causality lies at the heart of scientific inquiry, serving as the fundamental basis for understanding interactions among variables in physical systems. Despite its central role, current methods for causal inference face significant challenges due to nonlinear dependencies, stochastic interactions, self-causation, collider effects, and influences from exogenous factors, among others. While existing methods can effectively address some of these challenges, no single approach has successfully integrated all these aspects. Here, we address these challenges with SURD: Synergistic-Unique-Redundant Decomposition of causality. SURD quantifies causality as the increments of redundant, unique, and synergistic information gained about future events from past observations. The formulation is non-intrusive and applicable to both computational and experimental investigations, even when samples are scarce. We benchmark SURD in scenarios that pose significant challenges for causal inference and demonstrate that it offers a more reliable quantification of causality compared to previous methods.","sentences":["Causality lies at the heart of scientific inquiry, serving as the fundamental basis for understanding interactions among variables in physical systems.","Despite its central role, current methods for causal inference face significant challenges due to nonlinear dependencies, stochastic interactions, self-causation, collider effects, and influences from exogenous factors, among others.","While existing methods can effectively address some of these challenges, no single approach has successfully integrated all these aspects.","Here, we address these challenges with SURD: Synergistic-Unique-Redundant Decomposition of causality.","SURD quantifies causality as the increments of redundant, unique, and synergistic information gained about future events from past observations.","The formulation is non-intrusive and applicable to both computational and experimental investigations, even when samples are scarce.","We benchmark SURD in scenarios that pose significant challenges for causal inference and demonstrate that it offers a more reliable quantification of causality compared to previous methods."],"url":"http://arxiv.org/abs/2405.12411v1","category":"physics.data-an"}
{"created":"2024-05-20 22:56:40","title":"Searching for gravitational wave optical counterparts with the Zwicky Transient Facility: summary of O4a","abstract":"During the first half of the fourth observing run (O4a) of the International Gravitational Wave Network (IGWN), the Zwicky Transient Facility (ZTF) conducted a systematic search for kilonova (KN) counterparts to binary neutron star (BNS) and neutron star-black hole (NSBH) merger candidates. Here, we present a comprehensive study of the five high-significance (FAR < 1 per year) BNS and NSBH candidates in O4a. Our follow-up campaigns relied on both target-of-opportunity observations (ToO) and re-weighting of the nominal survey schedule to maximize coverage. We describe the toolkit we have been developing, Fritz, an instance of SkyPortal, instrumental in coordinating and managing our telescope scheduling, candidate vetting, and follow-up observations through a user-friendly interface. ZTF covered a total of 2841 deg$^2$ within the skymaps of the high-significance GW events, reaching a median depth of g~20.2 mag. We circulated 15 candidates, but found no viable KN counterpart to any of the GW events. Based on the ZTF non-detections of the high-significance events in O4a, we used a Bayesian approach, nimbus, to quantify the posterior probability of KN model parameters that are consistent with our non-detections. Our analysis favors KNe with initial absolute magnitude fainter than -16 mag. The joint posterior probability of a GW170817-like KN associated with all our O4a follow-ups was 64%. Additionally, we use a survey simulation software, simsurvey, to determine that our combined filtered efficiency to detect a GW170817-like KN is 36%, when considering the 5 confirmed astrophysical events in O3 (1 BNS and 4 NSBH), along with our O4a follow-ups. Following Kasliwal et al. (2020), we derived joint constraints on the underlying KN luminosity function based on our O3 and O4a follow-ups, determining that no more than 76% of KNe fading at 1 mag/day can peak at a magnitude brighter than -17.5 mag.","sentences":["During the first half of the fourth observing run (O4a) of the International Gravitational Wave Network (IGWN), the Zwicky Transient Facility (ZTF) conducted a systematic search for kilonova (KN) counterparts to binary neutron star (BNS) and neutron star-black hole (NSBH) merger candidates.","Here, we present a comprehensive study of the five high-significance (FAR < 1 per year)","BNS and NSBH candidates in O4a.","Our follow-up campaigns relied on both target-of-opportunity observations (ToO) and re-weighting of the nominal survey schedule to maximize coverage.","We describe the toolkit we have been developing, Fritz, an instance of SkyPortal, instrumental in coordinating and managing our telescope scheduling, candidate vetting, and follow-up observations through a user-friendly interface.","ZTF covered a total of 2841 deg$^2$ within the skymaps of the high-significance GW events, reaching a median depth of g~20.2 mag.","We circulated 15 candidates, but found no viable KN counterpart to any of the GW events.","Based on the ZTF non-detections of the high-significance events in O4a, we used a Bayesian approach, nimbus, to quantify the posterior probability of KN model parameters that are consistent with our non-detections.","Our analysis favors KNe with initial absolute magnitude fainter than -16 mag.","The joint posterior probability of a GW170817-like KN associated with all our O4a follow-ups was 64%.","Additionally, we use a survey simulation software, simsurvey, to determine that our combined filtered efficiency to detect a GW170817-like KN is 36%, when considering the 5 confirmed astrophysical events in O3 (1 BNS and 4 NSBH), along with our O4a follow-ups.","Following Kasliwal et al. (2020), we derived joint constraints on the underlying KN luminosity function based on our O3 and O4a follow-ups, determining that no more than 76% of KNe fading at 1 mag/day can peak at a magnitude brighter than -17.5 mag."],"url":"http://arxiv.org/abs/2405.12403v1","category":"astro-ph.HE"}
{"created":"2024-05-20 22:15:59","title":"Time-reversed information flow through a wormhole in scalar-tensor gravity","abstract":"This Letter aims to advance novel properties of a class of Closed Timelike Curves recently discovered in scalar-tensor gravity [Universe 9, 467 (2023)]. Therein, it was shown that a wormhole acts a gateway between two $\\textit{time-mirrored}$ worlds, where the two asymptotically flat sheets in the Kruskal-Szekeres diagram are glued antipodally along $\\textit{three}$ directions -- time $t$ and the polar and azimuth angles $(\\theta,\\,\\varphi)$ of the 2-sphere -- to form a wormhole throat. This contrasts with the standard embedding diagram which typically glues the sheets only along the $\\theta$ and $\\varphi$ directions. Crucially, due to the `gluing' along the $t$ direction, the wormhole becomes a portal connecting the two spacetime sheets with $\\textit{opposite}$ physical time flows, enabling the emergence of closed timelike loops which straddle the throat. We shall point out that this portal $\\textit{mathematically}$ permits the possibility of backward propagation of information $\\textit{against}$ time. This feature is ubiquitous for wormholes in scalar-tensor theories. In addition, we formulate the Feynman sum for transition amplitudes of microscopic particles in the proximity of a wormhole throat in which we account for timelike paths that experience time reversal.","sentences":["This Letter aims to advance novel properties of a class of Closed Timelike Curves recently discovered in scalar-tensor gravity","[Universe 9, 467 (2023)].","Therein, it was shown that a wormhole acts a gateway between two $\\textit{time-mirrored}$ worlds, where the two asymptotically flat sheets in the Kruskal-Szekeres diagram are glued antipodally along $\\textit{three}$ directions -- time $t$ and the polar and azimuth angles $(\\theta,\\,\\varphi)$ of the 2-sphere -- to form a wormhole throat.","This contrasts with the standard embedding diagram which typically glues the sheets only along the $\\theta$ and $\\varphi$ directions.","Crucially, due to the `gluing' along the $t$ direction, the wormhole becomes a portal connecting the two spacetime sheets with $\\textit{opposite}$ physical time flows, enabling the emergence of closed timelike loops which straddle the throat.","We shall point out that this portal $\\textit{mathematically}$ permits the possibility of backward propagation of information $\\textit{against}$ time.","This feature is ubiquitous for wormholes in scalar-tensor theories.","In addition, we formulate the Feynman sum for transition amplitudes of microscopic particles in the proximity of a wormhole throat in which we account for timelike paths that experience time reversal."],"url":"http://arxiv.org/abs/2405.12397v1","category":"gr-qc"}
{"created":"2024-05-20 22:01:57","title":"The Fuzzy Onion: An Initial Study","abstract":"In our previous contribution, we introduced a matrix formulation of a three-dimensional quantum space named the fuzzy onion. The novel part of the construction is the radial derivative term, which has been defined to recover the correct continuum limit. Here, we describe a numerical simulation of the scalar field theory in this space and test some physical properties of the model with emphasis on the interaction between neighbouring layers.","sentences":["In our previous contribution, we introduced a matrix formulation of a three-dimensional quantum space named the fuzzy onion.","The novel part of the construction is the radial derivative term, which has been defined to recover the correct continuum limit.","Here, we describe a numerical simulation of the scalar field theory in this space and test some physical properties of the model with emphasis on the interaction between neighbouring layers."],"url":"http://arxiv.org/abs/2405.12393v1","category":"hep-th"}
{"created":"2024-05-20 21:05:40","title":"Beyond Lithium-Ion Batteries: Are Effective Electrodes Possible for Alkaline and Other Alkali Elements? Exploring Ion Intercalation in Surface-Modified Few-Layer Graphene and Examining Layer Quantity and Stages","abstract":"In the quest for better energy storage solutions, the role of designing effective electrodes is crucial. Previous research has shown that using materials like single-side fluorinated graphene can improve the stability of ion insertion in few-layer graphene (FLG), which is vital as we move beyond lithium-ion batteries. Alternatives such as sodium and potassium, which are more abundant on Earth, appear promising, but thorough studies on how these ions insert into electrodes in stages are still needed. Our research focuses on the initial three alkali (Li, Na, K) and alkaline (Be, Mg, Ca) earth metals. Using Density Functional Theory (DFT) with advanced calculations, we've investigated how these ions interact with modified graphene at various stages of insertion. This method provides more precise electrical data and has helped us understand the complex interactions involved. Specifically, we found a new site for ion insertion that is energetically favorable. We also explored how modifying the graphene surface affects ions of different sizes and charges and examined how the number of graphene layers influences these interactions. Our discoveries are crucial for developing new materials that could replace lithium-ion batteries and provide a foundation for adjusting electrical properties in battery design through ion staging and surface modifications.","sentences":["In the quest for better energy storage solutions, the role of designing effective electrodes is crucial.","Previous research has shown that using materials like single-side fluorinated graphene can improve the stability of ion insertion in few-layer graphene (FLG), which is vital as we move beyond lithium-ion batteries.","Alternatives such as sodium and potassium, which are more abundant on Earth, appear promising, but thorough studies on how these ions insert into electrodes in stages are still needed.","Our research focuses on the initial three alkali (Li, Na, K) and alkaline (Be, Mg, Ca) earth metals.","Using Density Functional Theory (DFT) with advanced calculations, we've investigated how these ions interact with modified graphene at various stages of insertion.","This method provides more precise electrical data and has helped us understand the complex interactions involved.","Specifically, we found a new site for ion insertion that is energetically favorable.","We also explored how modifying the graphene surface affects ions of different sizes and charges and examined how the number of graphene layers influences these interactions.","Our discoveries are crucial for developing new materials that could replace lithium-ion batteries and provide a foundation for adjusting electrical properties in battery design through ion staging and surface modifications."],"url":"http://arxiv.org/abs/2405.12375v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-20 20:56:50","title":"Identification of soft modes across the commensurate-to-incommensurate charge density wave transition in 1$T$-TaSe$_2$","abstract":"1$T$-TaSe$_2$ is a prototypical charge density wave (CDW) material for which electron-phonon coupling and associated lattice distortion play an important role in driving and stabilizing the CDW phase. Here, we investigate the lattice dynamics of bulk 1$T$-TaSe$_2$ using angle-resolved ultralow wavenumber Raman spectroscopy down to 10 cm$^{-1}$. Our high-resolution spectra allow us to identify at least 27 Raman-active modes in the commensurate (CCDW) phase. Contrary to other layered materials, we do not find evidence of interlayer breathing or shear modes, suggestive of $AA$ stacking in the bulk, or sufficiently weak interlayer coupling. Polarization dependence of the mode intensities allows the assignment of their symmetry, which is supported by first-principles calculations of the phonons for the bulk structure using density functional theory. A detailed temperature dependence in the range $T$ = 80 - 500 K allows us to identify soft modes associated with the CDW superlattice. Upon entering the incommensurate (ICCDW) phase above 473 K, we observe a dramatic loss of resolution of all modes, and significant linewidth broadening associated with a reduced phonon lifetime as the charge-order becomes incommensurate with the lattice.","sentences":["1$T$-TaSe$_2$ is a prototypical charge density wave (CDW) material for which electron-phonon coupling and associated lattice distortion play an important role in driving and stabilizing the CDW phase.","Here, we investigate the lattice dynamics of bulk 1$T$-TaSe$_2$ using angle-resolved ultralow wavenumber Raman spectroscopy down to 10 cm$^{-1}$. Our high-resolution spectra allow us to identify at least 27 Raman-active modes in the commensurate (CCDW) phase.","Contrary to other layered materials, we do not find evidence of interlayer breathing or shear modes, suggestive of $AA$ stacking in the bulk, or sufficiently weak interlayer coupling.","Polarization dependence of the mode intensities allows the assignment of their symmetry, which is supported by first-principles calculations of the phonons for the bulk structure using density functional theory.","A detailed temperature dependence in the range $T$ = 80 - 500 K allows us to identify soft modes associated with the CDW superlattice.","Upon entering the incommensurate (ICCDW) phase above 473 K, we observe a dramatic loss of resolution of all modes, and significant linewidth broadening associated with a reduced phonon lifetime as the charge-order becomes incommensurate with the lattice."],"url":"http://arxiv.org/abs/2405.12373v1","category":"cond-mat.str-el"}
{"created":"2024-05-20 20:45:49","title":"Swift J1727.8-1613 has the Largest Resolved Continuous Jet Ever Seen in an X-ray Binary","abstract":"Multi-wavelength polarimetry and radio observations of Swift J1727.8-1613 at the beginning of its recent 2023 outburst suggested the presence of a bright compact jet aligned in the north-south direction, which could not be confirmed without high angular resolution images. Using the Very Long Baseline Array and the Long Baseline Array, we imaged Swift J1727.8-1613, during the hard/hard-intermediate state, revealing a bright core and a large, two-sided, asymmetrical, resolved jet. The jet extends in the north-south direction, at a position angle of $-0.60\\pm0.07\\deg$ East of North. At 8.4 GHz, the entire resolved jet structure is $\\sim110 (d/2.7\\,\\text{kpc})/\\sin i$ AU long, with the southern approaching jet extending $\\sim80 (d/2.7\\,\\text{kpc})/\\sin i$ AU from the core, where $d$ is the distance to the source and $i$ is the inclination of the jet axis to the line of sight. These images reveal the most resolved continuous X-ray binary jet, and possibly the most physically extended continuous X-ray binary jet ever observed. Based on the brightness ratio of the approaching and receding jets, we put a lower limit on the intrinsic jet speed of $\\beta\\geq0.27$ and an upper limit on the jet inclination of $i\\leq74\\deg$. In our first observation we also detected a rapidly fading discrete jet knot $66.89\\pm0.04$ mas south of the core, with a proper motion of $0.66\\pm0.05$ mas hour$^{-1}$, which we interpret as the result of a downstream internal shock or a jet-ISM interaction, as opposed to a transient relativistic jet launched at the beginning of the outburst.","sentences":["Multi-wavelength polarimetry and radio observations of Swift J1727.8-1613 at the beginning of its recent 2023 outburst suggested the presence of a bright compact jet aligned in the north-south direction, which could not be confirmed without high angular resolution images.","Using the Very Long Baseline Array and the Long Baseline Array, we imaged Swift J1727.8-1613, during the hard/hard-intermediate state, revealing a bright core and a large, two-sided, asymmetrical, resolved jet.","The jet extends in the north-south direction, at a position angle of $-0.60\\pm0.07\\deg$ East of North.","At 8.4 GHz, the entire resolved jet structure is $\\sim110 (d/2.7\\,\\text{kpc})/\\sin i$ AU long, with the southern approaching jet extending $\\sim80 (d/2.7\\,\\text{kpc})/\\sin i$ AU from the core, where $d$ is the distance to the source and $i$ is the inclination of the jet axis to the line of sight.","These images reveal the most resolved continuous X-ray binary jet, and possibly the most physically extended continuous X-ray binary jet ever observed.","Based on the brightness ratio of the approaching and receding jets, we put a lower limit on the intrinsic jet speed of $\\beta\\geq0.27$ and an upper limit on the jet inclination of $i\\leq74\\deg$. In our first observation we also detected a rapidly fading discrete jet knot $66.89\\pm0.04$ mas south of the core, with a proper motion of $0.66\\pm0.05$ mas hour$^{-1}$, which we interpret as the result of a downstream internal shock or a jet-ISM interaction, as opposed to a transient relativistic jet launched at the beginning of the outburst."],"url":"http://arxiv.org/abs/2405.12370v1","category":"astro-ph.HE"}
{"created":"2024-05-20 20:14:32","title":"Using Color Refinement to Boost Enumeration and Counting for Acyclic CQs of Binary Schemas","abstract":"We present an index structure, called the color-index, to boost the evaluation of acyclic conjunctive queries (ACQs) over binary schemas. The color-index is based on the color refinement algorithm, a widely used subroutine for graph isomorphism testing algorithms. Given a database $D$, we use a suitable version of the color refinement algorithm to produce a stable coloring of $D$, an assignment from the active domain of $D$ to a set of colors $C_D$. The main ingredient of the color-index is a particular database $D_c$ whose active domain is $C_D$ and whose size is at most $|D|$. Using the color-index, we can evaluate any free-connex ACQ $Q$ over $D$ with preprocessing time $O(|Q| \\cdot |D_c|)$ and constant delay enumeration. Furthermore, we can also count the number of results of $Q$ over $D$ in time $O(|Q| \\cdot |D_c|)$. Given that $|D_c|$ could be much smaller than $|D|$ (even constant-size for some families of databases), the color-index is the first index structure for evaluating free-connex ACQs that allows efficient enumeration and counting with performance that may be strictly smaller than the database size.","sentences":["We present an index structure, called the color-index, to boost the evaluation of acyclic conjunctive queries (ACQs) over binary schemas.","The color-index is based on the color refinement algorithm, a widely used subroutine for graph isomorphism testing algorithms.","Given a database $D$, we use a suitable version of the color refinement algorithm to produce a stable coloring of $D$, an assignment from the active domain of $D$ to a set of colors $C_D$. The main ingredient of the color-index is a particular database $D_c$ whose active domain is $C_D$ and whose size is at most $|D|$. Using the color-index, we can evaluate any free-connex ACQ $Q$ over $D$ with preprocessing time $O(|Q| \\cdot |D_c|)$ and constant delay enumeration.","Furthermore, we can also count the number of results of $Q$ over $D$ in time $O(|Q| \\cdot |D_c|)$. Given that $|D_c|$ could be much smaller than $|D|$ (even constant-size for some families of databases), the color-index is the first index structure for evaluating free-connex ACQs that allows efficient enumeration and counting with performance that may be strictly smaller than the database size."],"url":"http://arxiv.org/abs/2405.12358v1","category":"cs.DB"}
{"created":"2024-05-20 19:22:24","title":"Nano-gap electrode dielectrophoresis for tether-free trapping and interferometric-scattering detection of single 20 nm particles","abstract":"Accurate detection and characterization of nanoparticles within confined spaces is crucial for applications ranging from nanofluidics to biotechnology. We present a novel approach that combines interferometric scattering (iSCAT) detection with trapping by dielectrophoresis (DEP) to achieve label-free detection of nanoparticles that are trapped and/or actuated between nano-gap electrodes. DEP utilizes the interaction between the induced dipole of the particle and the applied electric field to create a trapping potential. We demonstrate our method by trapping and label-free detection of down to 20 nm polystyrene nanoparticles. Additionally, we demonstrate that the signal-to-noise ratio of our detection can be boosted up to 20-fold by periodic actuation of the nanoparticle in the trap. This is done by a digital lock-in detection scheme on the modulated scattering signal. Our method holds promise for various applications, including assembly of nanoparticles, single-particle property analysis, and nanofluidic devices.","sentences":["Accurate detection and characterization of nanoparticles within confined spaces is crucial for applications ranging from nanofluidics to biotechnology.","We present a novel approach that combines interferometric scattering (iSCAT) detection with trapping by dielectrophoresis (DEP) to achieve label-free detection of nanoparticles that are trapped and/or actuated between nano-gap electrodes.","DEP utilizes the interaction between the induced dipole of the particle and the applied electric field to create a trapping potential.","We demonstrate our method by trapping and label-free detection of down to 20 nm polystyrene nanoparticles.","Additionally, we demonstrate that the signal-to-noise ratio of our detection can be boosted up to 20-fold by periodic actuation of the nanoparticle in the trap.","This is done by a digital lock-in detection scheme on the modulated scattering signal.","Our method holds promise for various applications, including assembly of nanoparticles, single-particle property analysis, and nanofluidic devices."],"url":"http://arxiv.org/abs/2405.12338v1","category":"physics.optics"}
{"created":"2024-05-20 19:14:16","title":"Circular orbits and chaos bound in slow-rotating curved acoustic black holes","abstract":"In recent years, the incorporation of acoustic black holes into Schwarzschild spacetime has enabled the simultaneous existence of event and acoustic horizons, as derived from the Gross-Pitaevskii theory. This paper investigates the dynamics of a particle-like vortex in free fall as it approaches the horizon of a curved slowly-rotating acoustic black hole, specifically within the context of the Lens-Thirring spacetime. We analyze the circular orbits of vortices around the acoustic black hole and delve into the chaotic motion of the vortex at the unstable equilibrium in the vicinity of the horizon. Furthermore, we explore whether a universal bound exists for the Lyapunov exponent characterising this chaotic motion, drawing parallels to the known bounds for the particle motion in general relativity. We anticipate that future experiments will be able to corroborate these findings.","sentences":["In recent years, the incorporation of acoustic black holes into Schwarzschild spacetime has enabled the simultaneous existence of event and acoustic horizons, as derived from the Gross-Pitaevskii theory.","This paper investigates the dynamics of a particle-like vortex in free fall as it approaches the horizon of a curved slowly-rotating acoustic black hole, specifically within the context of the Lens-Thirring spacetime.","We analyze the circular orbits of vortices around the acoustic black hole and delve into the chaotic motion of the vortex at the unstable equilibrium in the vicinity of the horizon.","Furthermore, we explore whether a universal bound exists for the Lyapunov exponent characterising this chaotic motion, drawing parallels to the known bounds for the particle motion in general relativity.","We anticipate that future experiments will be able to corroborate these findings."],"url":"http://arxiv.org/abs/2405.12337v1","category":"hep-th"}
{"created":"2024-05-20 19:00:29","title":"Orbital angular momentum enhanced laser absorption and neutron generation","abstract":"We experimentally demonstrate enhanced absorption of near relativistic optical vortex beams in $\\mathrm{D_2O}$ plasmas to generate a record fast-neutron yield of $1.45 \\times 10^6$ n/s/sr. Beams with a topological charge of 5 were shown to deliver up to a 3.3 times enhancement of fast-neutron yield over a Gaussian focused beam of the same energy but having two orders of magnitude higher intensity. This result was achieved with laser energies of 16 mJ and a pulse duration of 67 fs. The Orbital Angular Momentum (OAM) beam-target interactions in our experiment were also investigated through Particle-in-Cell (PIC) simulations. Electron density rippling resulting in enhanced plasma wave excitation on the critical surface and significantly enhanced resonance absorption is observed.","sentences":["We experimentally demonstrate enhanced absorption of near relativistic optical vortex beams in $\\mathrm{D_2O}$ plasmas to generate a record fast-neutron yield of $1.45 \\times 10^6$ n/s/sr.","Beams with a topological charge of 5 were shown to deliver up to a 3.3 times enhancement of fast-neutron yield over a Gaussian focused beam of the same energy but having two orders of magnitude higher intensity.","This result was achieved with laser energies of 16 mJ and a pulse duration of 67 fs.","The Orbital Angular Momentum (OAM) beam-target interactions in our experiment were also investigated through Particle-in-Cell (PIC) simulations.","Electron density rippling resulting in enhanced plasma wave excitation on the critical surface and significantly enhanced resonance absorption is observed."],"url":"http://arxiv.org/abs/2405.12330v1","category":"physics.plasm-ph"}
{"created":"2024-05-20 18:31:59","title":"Chiral and deconfinement properties of the QCD crossover have a different volume and baryochemical potential dependence","abstract":"The crossover from hadronic to quark matter is understood to be both a deconfinement as well as a chiral symmetry restoring transition. Here, we study observables related to both aspects using lattice simulations: the Polyakov loop and its derivatives and the chiral condensate and its derivatives. At zero baryochemical potential, and infinite volume, the chiral and deconfinement crossover temperatures almost agree. However, chiral and deconfinement related observables have a qualitatively different chemical potential and volume dependence. In general, deconfinement related observables have a milder volume dependence. Furthermore, while the deconfinement transition appears to get broader with increasing $\\mu_B$, the width as well as the strength of the chiral transition is approximately constant. Our results are based on simulations at zero and imaginary chemical potentials using 4stout-improved staggered fermions with $N_\\tau=12$ time-slices and physical quark masses.","sentences":["The crossover from hadronic to quark matter is understood to be both a deconfinement as well as a chiral symmetry restoring transition.","Here, we study observables related to both aspects using lattice simulations: the Polyakov loop and its derivatives and the chiral condensate and its derivatives.","At zero baryochemical potential, and infinite volume, the chiral and deconfinement crossover temperatures almost agree.","However, chiral and deconfinement related observables have a qualitatively different chemical potential and volume dependence.","In general, deconfinement related observables have a milder volume dependence.","Furthermore, while the deconfinement transition appears to get broader with increasing $\\mu_B$, the width as well as the strength of the chiral transition is approximately constant.","Our results are based on simulations at zero and imaginary chemical potentials using 4stout-improved staggered fermions with $N_\\tau=12$ time-slices and physical quark masses."],"url":"http://arxiv.org/abs/2405.12320v1","category":"hep-lat"}
