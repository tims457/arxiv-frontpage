{"created":"2024-03-26 17:58:29","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models","abstract":"SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns or increasing traffic density. Further, SLEDGE can support 500m long routes, a capability not found in existing data-driven simulators like nuPlan. It presents new challenges for planning algorithms, evidenced by failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and dense traffic generated by our model. Compared to nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it a more accessible option and helping with democratizing future research in this field.","sentences":["SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs.","Its core component is a learned model that is able to generate agent bounding boxes and lane graphs.","The model's outputs serve as an initial state for traffic simulation.","The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial.","Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE).","It encodes agents and the lane graph into distinct channels in a rasterized latent map.","This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer.","Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns or increasing traffic density.","Further, SLEDGE can support 500m long routes, a capability not found in existing data-driven simulators like nuPlan.","It presents new challenges for planning algorithms, evidenced by failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and dense traffic generated by our model.","Compared to nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it a more accessible option and helping with democratizing future research in this field."],"url":"http://arxiv.org/abs/2403.17933v1","category":"cs.RO"}
{"created":"2024-03-26 17:57:57","title":"MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution","abstract":"In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.","sentences":["In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities.","Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level.","To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors.","Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents.","This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues.","In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2.","MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines.","Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method.","We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc."],"url":"http://arxiv.org/abs/2403.17927v1","category":"cs.SE"}
{"created":"2024-03-26 17:57:20","title":"FastCAR: Fast Classification And Regression Multi-Task Learning via Task Consolidation for Modelling a Continuous Property Variable of Object Classes","abstract":"FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL) for a classification and a regression task, despite task heterogeneity with only subtle correlation. It addresses object classification and continuous property variable regression, a crucial use case in science and engineering. FastCAR involves a labeling transformation approach that can be used with a single-task regression network architecture. FastCAR outperforms traditional MTL model families, parametrized in the landscape of architecture and loss weighting schemes, when learning of both tasks are collectively considered (classification accuracy of 99.54%, regression mean absolute percentage error of 2.3%). The experiments performed used an Advanced Steel Property dataset contributed by us. The dataset comprises 4536 images of 224x224 pixels, annotated with object classes and hardness properties that take continuous values. With the labeling transformation and single-task regression network architecture, FastCAR achieves reduced latency and time efficiency.","sentences":["FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL) for a classification and a regression task, despite task heterogeneity with only subtle correlation.","It addresses object classification and continuous property variable regression, a crucial use case in science and engineering.","FastCAR involves a labeling transformation approach that can be used with a single-task regression network architecture.","FastCAR outperforms traditional MTL model families, parametrized in the landscape of architecture and loss weighting schemes, when learning of both tasks are collectively considered (classification accuracy of 99.54%, regression mean absolute percentage error of 2.3%).","The experiments performed used an Advanced Steel Property dataset contributed by us.","The dataset comprises 4536 images of 224x224 pixels, annotated with object classes and hardness properties that take continuous values.","With the labeling transformation and single-task regression network architecture, FastCAR achieves reduced latency and time efficiency."],"url":"http://arxiv.org/abs/2403.17926v1","category":"cs.CV"}
{"created":"2024-03-26 17:57:05","title":"AID: Attention Interpolation of Text-to-Image Diffusion","abstract":"Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at https://github.com/QY-H00/attention-interpolation-diffusion.","sentences":["Conditional diffusion models can create unseen images in various settings, aiding image interpolation.","Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood.","Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity.","To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID).","Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness.","We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process.","This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation.","Our approach demonstrates effectiveness for conceptual and spatial interpolation.","Code and demo are available at https://github.com/QY-H00/attention-interpolation-diffusion."],"url":"http://arxiv.org/abs/2403.17924v1","category":"cs.CV"}
{"created":"2024-03-26 17:55:02","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning","abstract":"The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.","sentences":["The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training.","Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings.","Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers.","Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA.","We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization.","Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench scores.","On large models, specifically LLaMA-2-70B, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains."],"url":"http://arxiv.org/abs/2403.17919v1","category":"cs.LG"}
{"created":"2024-03-26 17:54:15","title":"AgentStudio: A Toolkit for Building General Virtual Agents","abstract":"Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence. Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities. To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development. This includes environment setups, data collection, agent evaluation, and visualization. The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces. This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings. To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both created with our graphical interfaces. Furthermore, we present several actionable insights derived from AgentStudio, e.g., general visual grounding, open-ended tool creation, learning from videos, etc. We have open-sourced the environments, datasets, benchmarks, and interfaces to promote research towards developing general virtual agents for the future.","sentences":["Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence.","Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities.","To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development.","This includes environment setups, data collection, agent evaluation, and visualization.","The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces.","This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings.","To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both created with our graphical interfaces.","Furthermore, we present several actionable insights derived from AgentStudio, e.g., general visual grounding, open-ended tool creation, learning from videos, etc.","We have open-sourced the environments, datasets, benchmarks, and interfaces to promote research towards developing general virtual agents for the future."],"url":"http://arxiv.org/abs/2403.17918v1","category":"cs.AI"}
{"created":"2024-03-26 17:54:05","title":"Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes","abstract":"This paper presents two algorithms for multi-agent dynamic coverage in spatiotemporal environments, where the coverage algorithms are informed by the method of data assimilation. In particular, we show that by considering the information assimilation algorithm, here a Numerical Gaussian Process Kalman Filter, the influence of measurements taken at one position on the uncertainty of the estimate at another location can be computed. We use this relationship to propose new coverage algorithms. Furthermore, we show that the controllers naturally extend to the multi-agent context, allowing for a distributed-control central-information paradigm for multi-agent coverage. Finally, we demonstrate the algorithms through a realistic simulation of a team of UAVs collecting wind data over a region in Austria.","sentences":["This paper presents two algorithms for multi-agent dynamic coverage in spatiotemporal environments, where the coverage algorithms are informed by the method of data assimilation.","In particular, we show that by considering the information assimilation algorithm, here a Numerical Gaussian Process Kalman Filter, the influence of measurements taken at one position on the uncertainty of the estimate at another location can be computed.","We use this relationship to propose new coverage algorithms.","Furthermore, we show that the controllers naturally extend to the multi-agent context, allowing for a distributed-control central-information paradigm for multi-agent coverage.","Finally, we demonstrate the algorithms through a realistic simulation of a team of UAVs collecting wind data over a region in Austria."],"url":"http://arxiv.org/abs/2403.17917v1","category":"eess.SY"}
{"created":"2024-03-26 17:53:27","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","abstract":"The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predictions obtained by different CAVs and generates the final prediction. Through extensive experiments and ablation studies, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction tasks. In particular, CMP reduces the average prediction error by 17.2\\% with fewer missing detections compared with the no cooperation setting. Our work marks a significant step forward in the cooperative capabilities of CAVs, showcasing enhanced performance in complex scenarios.","sentences":["The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs).","Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction.","Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities.","Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules.","Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations.","We also propose a prediction aggregation module, which unifies the predictions obtained by different CAVs and generates the final prediction.","Through extensive experiments and ablation studies, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction tasks.","In particular, CMP reduces the average prediction error by 17.2\\% with fewer missing detections compared with the no cooperation setting.","Our work marks a significant step forward in the cooperative capabilities of CAVs, showcasing enhanced performance in complex scenarios."],"url":"http://arxiv.org/abs/2403.17916v1","category":"cs.RO"}
{"created":"2024-03-26 17:51:06","title":"Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports","abstract":"A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we need to understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into BERT. To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels. The effectiveness of our framework is evaluated with the data collected by National Transportation Safety Board (NTSB). It has been shown that fine-level prediction accuracy is highly improved, and the regularization term can be beneficial to the rare event identification problem.","sentences":["A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety.","To better use those reports, we need to understand the most important events or impact factors according to the accident reports.","However, the increasing number of accident reports requires large efforts from domain experts to label those reports.","In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically.","This article argues that we can identify the events more accurately by leveraging the event taxonomy.","More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information.","We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into BERT.","To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels.","The effectiveness of our framework is evaluated with the data collected by National Transportation Safety Board (NTSB).","It has been shown that fine-level prediction accuracy is highly improved, and the regularization term can be beneficial to the rare event identification problem."],"url":"http://arxiv.org/abs/2403.17914v1","category":"cs.AI"}
{"created":"2024-03-26 17:51:05","title":"Enhancing Indoor and Outdoor THz Communications with Beyond Diagonal-IRS: Optimization and Performance Analysis","abstract":"This work investigates the application of Beyond Diagonal Intelligent Reflective Surface (BD-IRS) to enhance THz downlink communication systems, operating in a hybrid: reflective and transmissive mode, to simultaneously provide services to indoor and outdoor users. We propose an optimization framework that jointly optimizes the beamforming vectors and phase shifts in the hybrid reflective/transmissive mode, aiming to maximize the system sum rate. To tackle the challenges in solving the joint design problem, we employ the conjugate gradient method and propose an iterative algorithm that successively optimizes the hybrid beamforming vectors and the phase shifts. Through comprehensive numerical simulations, our findings demonstrate a significant improvement in rate when compared to existing benchmark schemes, including time- and frequency-divided approaches, by approximately $30.5\\%$ and $70.28\\%$ respectively. This underscores the significant influence of IRS elements on system performance relative to that of base station antennas, highlighting their pivotal role in advancing the communication system efficacy.","sentences":["This work investigates the application of Beyond Diagonal Intelligent Reflective Surface (BD-IRS) to enhance THz downlink communication systems, operating in a hybrid: reflective and transmissive mode, to simultaneously provide services to indoor and outdoor users.","We propose an optimization framework that jointly optimizes the beamforming vectors and phase shifts in the hybrid reflective/transmissive mode, aiming to maximize the system sum rate.","To tackle the challenges in solving the joint design problem, we employ the conjugate gradient method and propose an iterative algorithm that successively optimizes the hybrid beamforming vectors and the phase shifts.","Through comprehensive numerical simulations, our findings demonstrate a significant improvement in rate when compared to existing benchmark schemes, including time- and frequency-divided approaches, by approximately $30.5\\%$ and $70.28\\%$ respectively.","This underscores the significant influence of IRS elements on system performance relative to that of base station antennas, highlighting their pivotal role in advancing the communication system efficacy."],"url":"http://arxiv.org/abs/2403.17913v1","category":"eess.SP"}
{"created":"2024-03-26 17:22:29","title":"Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels","abstract":"One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types. This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers. Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance. To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection. Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods.","sentences":["One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types.","This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers.","Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance.","To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection.","Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods."],"url":"http://arxiv.org/abs/2403.17891v1","category":"cs.LG"}
{"created":"2024-03-26 17:21:54","title":"Large scale paired antibody language models","abstract":"Antibodies are proteins produced by the immune system that can identify and neutralise a wide variety of antigens with high specificity and affinity, and constitute the most successful class of biotherapeutics. With the advent of next-generation sequencing, billions of antibody sequences have been collected in recent years, though their application in the design of better therapeutics has been constrained by the sheer volume and complexity of the data. To address this challenge, we present IgBert and IgT5, the best performing antibody-specific language models developed to date which can consistently handle both paired and unpaired variable region sequences as input. These models are trained comprehensively using the more than two billion unpaired sequences and two million paired sequences of light and heavy chains present in the Observed Antibody Space dataset. We show that our models outperform existing antibody and protein language models on a diverse range of design and regression tasks relevant to antibody engineering. This advancement marks a significant leap forward in leveraging machine learning, large scale data sets and high-performance computing for enhancing antibody design for therapeutic development.","sentences":["Antibodies are proteins produced by the immune system that can identify and neutralise a wide variety of antigens with high specificity and affinity, and constitute the most successful class of biotherapeutics.","With the advent of next-generation sequencing, billions of antibody sequences have been collected in recent years, though their application in the design of better therapeutics has been constrained by the sheer volume and complexity of the data.","To address this challenge, we present IgBert and IgT5, the best performing antibody-specific language models developed to date which can consistently handle both paired and unpaired variable region sequences as input.","These models are trained comprehensively using the more than two billion unpaired sequences and two million paired sequences of light and heavy chains present in the Observed Antibody Space dataset.","We show that our models outperform existing antibody and protein language models on a diverse range of design and regression tasks relevant to antibody engineering.","This advancement marks a significant leap forward in leveraging machine learning, large scale data sets and high-performance computing for enhancing antibody design for therapeutic development."],"url":"http://arxiv.org/abs/2403.17889v1","category":"q-bio.BM"}
{"created":"2024-03-26 17:12:34","title":"Double polytropic cosmic acceleration from the Murnaghan equation of state","abstract":"We consider a double polytropic cosmological fluid and demonstrate that, when one constituent resembles a bare cosmological constant while the other emulates a generalized Chaplygin gas, a good description of the Universe's large-scale dynamics is obtained. In particular, our double polytropic reduces to the Murnaghan equation of state, whose applications are already well established in solid state physics and classical thermodynamics. Intriguingly, our model approximates the conventional $\\Lambda$CDM paradigm while reproducing the collective effects of logotropic and generalized Chaplygin fluids across different regimes. To check the goodness of our fluid description, we analyze first order density perturbations, refining our model through various orders of approximation, utilizing $\\sigma_8$ data alongside other cosmological data sets. Encouraging results suggest that our model, based on the Murnaghan equation of state, outperforms the standard cosmological background within specific approximate regimes and, on the whole, surpasses the standard phenomenological reconstruction of dark energy.","sentences":["We consider a double polytropic cosmological fluid and demonstrate that, when one constituent resembles a bare cosmological constant while the other emulates a generalized Chaplygin gas, a good description of the Universe's large-scale dynamics is obtained.","In particular, our double polytropic reduces to the Murnaghan equation of state, whose applications are already well established in solid state physics and classical thermodynamics.","Intriguingly, our model approximates the conventional $\\Lambda$CDM paradigm while reproducing the collective effects of logotropic and generalized Chaplygin fluids across different regimes.","To check the goodness of our fluid description, we analyze first order density perturbations, refining our model through various orders of approximation, utilizing $\\sigma_8$ data alongside other cosmological data sets.","Encouraging results suggest that our model, based on the Murnaghan equation of state, outperforms the standard cosmological background within specific approximate regimes and, on the whole, surpasses the standard phenomenological reconstruction of dark energy."],"url":"http://arxiv.org/abs/2403.17880v1","category":"gr-qc"}
{"created":"2024-03-26 17:10:15","title":"Empowering Data Mesh with Federated Learning","abstract":"The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations. To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh. To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture.","sentences":["The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making.","However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing.","A new data paradigm, Data Mesh, is proposed to overcome these challenges.","Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products.","Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture.","In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations.","To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh.","To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture."],"url":"http://arxiv.org/abs/2403.17878v2","category":"cs.LG"}
{"created":"2024-03-26 17:02:42","title":"Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach","abstract":"Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health. In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users. This addition aims to bridge the gap between LLM capabilities and user perceptions, promoting the ethically responsible development and use of LLM-based technology.","sentences":["Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations.","Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users.","In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health.","In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust.","To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users.","This addition aims to bridge the gap between LLM capabilities and user perceptions, promoting the ethically responsible development and use of LLM-based technology."],"url":"http://arxiv.org/abs/2403.17873v1","category":"cs.AI"}
{"created":"2024-03-26 16:48:13","title":"ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages","abstract":"Question answering (QA) and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, large language models. At the same time, many benchmark datasets have become available for QA and MRC tasks. However, most existing large-scale benchmark datasets have been created predominantly using synchronous document collections like Wikipedia or the Web. Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models. To further contribute to advancing QA and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a large-scale dataset with 485K question-answer pairs created based on the historical newspaper collection Chronicling America. Our dataset is constructed from a subset of the Chronicling America newspaper collection spanning 120 years. One of the significant challenges for utilizing digitized historical newspaper collections is the low quality of OCR text. Therefore, to enable realistic testing of QA models, our dataset can be used in three different ways: answering questions from raw and noisy content, answering questions from cleaner, corrected version of the content, as well as answering questions from scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA spans the longest time period among available QA datasets make it quite a unique and useful resource.","sentences":["Question answering (QA) and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, large language models.","At the same time, many benchmark datasets have become available for QA and MRC tasks.","However, most existing large-scale benchmark datasets have been created predominantly using synchronous document collections like Wikipedia or the Web.","Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models.","To further contribute to advancing QA and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a large-scale dataset with 485K question-answer pairs created based on the historical newspaper collection Chronicling America.","Our dataset is constructed from a subset of the Chronicling America newspaper collection spanning 120 years.","One of the significant challenges for utilizing digitized historical newspaper collections is the low quality of OCR text.","Therefore, to enable realistic testing of QA models, our dataset can be used in three different ways: answering questions from raw and noisy content, answering questions from cleaner, corrected version of the content, as well as answering questions from scanned images of newspaper pages.","This and the fact that ChroniclingAmericaQA spans the longest time period among available QA datasets make it quite a unique and useful resource."],"url":"http://arxiv.org/abs/2403.17859v1","category":"cs.CL"}
{"created":"2024-03-26 16:36:50","title":"Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections","abstract":"Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change. These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties. To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes. Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness. One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands. Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics. In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to drastically geographical elevation changes and uneven distribution through the year. Thus, it is crucial to track and predict the rainfall to make the most use of it and to prevent the floods. However, climate models have limited resolution and require intensive computational power for local-scale use. Therefore, we proposed a deep convolutional neural network with skip connections, attention blocks, and auxiliary data concatenation, in order to downscale the low-resolution precipitation data into high-resolution one. Eventually, we compare with other climate downscaling methods and show better performance in metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation, structural similarity index (SSIM), and forecast indicators.","sentences":["Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change.","These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties.","To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes.","Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness.","One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands.","Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics.","In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to drastically geographical elevation changes and uneven distribution through the year.","Thus, it is crucial to track and predict the rainfall to make the most use of it and to prevent the floods.","However, climate models have limited resolution and require intensive computational power for local-scale use.","Therefore, we proposed a deep convolutional neural network with skip connections, attention blocks, and auxiliary data concatenation, in order to downscale the low-resolution precipitation data into high-resolution one.","Eventually, we compare with other climate downscaling methods and show better performance in metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation, structural similarity index (SSIM), and forecast indicators."],"url":"http://arxiv.org/abs/2403.17847v1","category":"cs.LG"}
{"created":"2024-03-26 16:36:43","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation","abstract":"Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps. In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-storage environments. We provide code and trial video data at http://hovsg.github.io/.","sentences":["Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features.","While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation.","In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation.","Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features.","Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph.","HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps.","In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-storage environments.","We provide code and trial video data at http://hovsg.github.io/."],"url":"http://arxiv.org/abs/2403.17846v1","category":"cs.RO"}
{"created":"2024-03-26 16:27:37","title":"ReMamber: Referring Image Segmentation with Mamba Twister","abstract":"Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve the state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough analyses of ReMamber and discuss other fusion designs using Mamba. These provide valuable perspectives for future research.","sentences":["Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks.","However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies.","Fortunately, Mamba addresses this with efficient linear complexity in processing.","However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data.","In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block.","The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism.","We achieve the state-of-the-art on three challenging benchmarks.","Moreover, we conduct thorough analyses of ReMamber and discuss other fusion designs using Mamba.","These provide valuable perspectives for future research."],"url":"http://arxiv.org/abs/2403.17839v1","category":"cs.CV"}
{"created":"2024-03-26 16:24:42","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","abstract":"High Dynamic Range (HDR) content (i.e., images and videos) has a broad range of applications. However, capturing HDR content from real-world scenes is expensive and time-consuming. Therefore, the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community. A major challenge in this research problem is the lack of datasets, which capture diverse scene conditions (e.g., lighting, shadows, weather, locations, landscapes, objects, humans, buildings) and various image features (e.g., color, contrast, saturation, hue, luminance, brightness, radiance). To address this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game. We perform thorough evaluation of the proposed dataset, which demonstrates significant qualitative and quantitative improvements of the state-of-the-art HDR image reconstruction methods. Furthermore, we demonstrate the effectiveness of the proposed dataset and its impact on additional computer vision tasks including 3D human pose estimation, human body part segmentation, and holistic scene segmentation. The dataset, data collection pipeline, and evaluation code are available at: https://github.com/HrishavBakulBarua/GTA-HDR.","sentences":["High Dynamic Range (HDR) content (i.e., images and videos) has a broad range of applications.","However, capturing HDR content from real-world scenes is expensive and time-consuming.","Therefore, the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community.","A major challenge in this research problem is the lack of datasets, which capture diverse scene conditions (e.g., lighting, shadows, weather, locations, landscapes, objects, humans, buildings) and various image features (e.g., color, contrast, saturation, hue, luminance, brightness, radiance).","To address this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game.","We perform thorough evaluation of the proposed dataset, which demonstrates significant qualitative and quantitative improvements of the state-of-the-art HDR image reconstruction methods.","Furthermore, we demonstrate the effectiveness of the proposed dataset and its impact on additional computer vision tasks including 3D human pose estimation, human body part segmentation, and holistic scene segmentation.","The dataset, data collection pipeline, and evaluation code are available at: https://github.com/HrishavBakulBarua/GTA-HDR."],"url":"http://arxiv.org/abs/2403.17837v1","category":"cs.CV"}
{"created":"2024-03-26 16:13:55","title":"Learning the Optimal Power Flow: Environment Design Matters","abstract":"To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first recommendations regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field.","sentences":["To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach.","However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment.","In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice.","In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance.","Further, we derive some first recommendations regarding the choice of these design decisions.","The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field."],"url":"http://arxiv.org/abs/2403.17831v1","category":"cs.LG"}
{"created":"2024-03-26 16:10:21","title":"Assessment of Multimodal Large Language Models in Alignment with Human Values","abstract":"Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations. To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations. Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle. We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives. Based on the evaluation results, we summarize over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field.","sentences":["Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh).","However, in terms of Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations.","To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations.","Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle.","We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives.","Based on the evaluation results, we summarize over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field."],"url":"http://arxiv.org/abs/2403.17830v1","category":"cs.CV"}
{"created":"2024-03-26 16:06:42","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions","abstract":"Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance. Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose. Given a grasping motion from this stage, multiple different actions can be prompted in the interaction phase. For textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions. Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions. Moreover, we demonstrate the practicality of our framework by utilizing a hand pose estimate from an off-the-shelf pose estimator for guidance, and then sampling multiple different actions in the interaction stage.","sentences":["Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful.","Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets.","We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object.","The method introduces three techniques that enable effective learning from limited data.","First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each.","In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized.","Second, we propose a compact representation that tightly couples hand and object poses.","Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance.","Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose.","Given a grasping motion from this stage, multiple different actions can be prompted in the interaction phase.","For textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions.","Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions.","Moreover, we demonstrate the practicality of our framework by utilizing a hand pose estimate from an off-the-shelf pose estimator for guidance, and then sampling multiple different actions in the interaction stage."],"url":"http://arxiv.org/abs/2403.17827v1","category":"cs.CV"}
{"created":"2024-03-26 16:06:33","title":"On the Computational Complexity of Stackelberg Planning and Meta-Operator Verification: Technical Report","abstract":"Stackelberg planning is a recently introduced single-turn two-player adversarial planning model, where two players are acting in a joint classical planning task, the objective of the first player being hampering the second player from achieving its goal. This places the Stackelberg planning problem somewhere between classical planning and general combinatorial two-player games. But, where exactly? All investigations of Stackelberg planning so far focused on practical aspects. We close this gap by conducting the first theoretical complexity analysis of Stackelberg planning. We show that in general Stackelberg planning is actually no harder than classical planning. Under a polynomial plan-length restriction, however, Stackelberg planning is a level higher up in the polynomial complexity hierarchy, suggesting that compilations into classical planning come with a worst-case exponential plan-length increase. In attempts to identify tractable fragments, we further study its complexity under various planning task restrictions, showing that Stackelberg planning remains intractable where classical planning is not. We finally inspect the complexity of meta-operator verification, a problem that has been recently connected to Stackelberg planning.","sentences":["Stackelberg planning is a recently introduced single-turn two-player adversarial planning model, where two players are acting in a joint classical planning task, the objective of the first player being hampering the second player from achieving its goal.","This places the Stackelberg planning problem somewhere between classical planning and general combinatorial two-player games.","But, where exactly?","All investigations of Stackelberg planning so far focused on practical aspects.","We close this gap by conducting the first theoretical complexity analysis of Stackelberg planning.","We show that in general Stackelberg planning is actually no harder than classical planning.","Under a polynomial plan-length restriction, however, Stackelberg planning is a level higher up in the polynomial complexity hierarchy, suggesting that compilations into classical planning come with a worst-case exponential plan-length increase.","In attempts to identify tractable fragments, we further study its complexity under various planning task restrictions, showing that Stackelberg planning remains intractable where classical planning is not.","We finally inspect the complexity of meta-operator verification, a problem that has been recently connected to Stackelberg planning."],"url":"http://arxiv.org/abs/2403.17826v1","category":"cs.AI"}
{"created":"2024-03-26 15:54:48","title":"Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)","abstract":"Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks.   In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes. We explore various roles that LLMs can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management.","sentences":["Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications.","To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks.   ","In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes.","We explore various roles that LLMs can play in this context while identifying some of the challenges to address.","The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management."],"url":"http://arxiv.org/abs/2403.17819v1","category":"cs.NI"}
{"created":"2024-03-26 15:53:53","title":"CSSTs: A Dynamic Data Structure for Partial Orders in Concurrent Execution Analysis","abstract":"Dynamic analyses are a standard approach to analyzing and testing concurrent programs. Such techniques observe program traces and analyze them to infer the presence or absence of bugs. At its core, each analysis maintains a partial order $P$ that represents order dependencies between events of the analyzed trace $\\sigma$. Naturally, the scalability of the analysis largely depends on how efficiently it maintains $P$. The standard data structure for this task has thus far been vector clocks. These, however, are slow for analyses that follow a non-streaming style, costing $O(n)$ for inserting (and propagating) each new ordering in $P$, where $n$ is the size of $\\sigma$, while they cannot handle the deletion of existing orderings.   In this paper we develop collective sparse segment trees (CSSTs), a simple but elegant data structure for generically maintaining a partial order $P$. CSSTs thrive when the width $k$ of $P$ is much smaller than the size $n$ of its domain, allowing inserting, deleting, and querying for orderings in $P$ to run in $O(logn)$ time. For a concurrent trace, $k$ is bounded by the number of its threads, and is normally orders of magnitude smaller than its size $n$, making CSSTs fitting for this setting. Our experimental results confirm that CSSTs are the best data structure currently to handle a range of dynamic analyses from existing literature.","sentences":["Dynamic analyses are a standard approach to analyzing and testing concurrent programs.","Such techniques observe program traces and analyze them to infer the presence or absence of bugs.","At its core, each analysis maintains a partial order $P$ that represents order dependencies between events of the analyzed trace $\\sigma$. Naturally, the scalability of the analysis largely depends on how efficiently it maintains $P$.","The standard data structure for this task has thus far been vector clocks.","These, however, are slow for analyses that follow a non-streaming style, costing $O(n)$ for inserting (and propagating) each new ordering in $P$, where $n$ is the size of $\\sigma$, while they cannot handle the deletion of existing orderings.   ","In this paper we develop collective sparse segment trees (CSSTs), a simple but elegant data structure for generically maintaining a partial order $P$. CSSTs thrive when the width $k$ of $P$ is much smaller than the size $n$ of its domain, allowing inserting, deleting, and querying for orderings in $P$ to run in $O(logn)$ time.","For a concurrent trace, $k$ is bounded by the number of its threads, and is normally orders of magnitude smaller than its size $n$, making CSSTs fitting for this setting.","Our experimental results confirm that CSSTs are the best data structure currently to handle a range of dynamic analyses from existing literature."],"url":"http://arxiv.org/abs/2403.17818v1","category":"cs.PL"}
{"created":"2024-03-26 15:52:36","title":"D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time Series Forecasting","abstract":"In time series forecasting, effectively disentangling intricate temporal patterns is crucial. While recent works endeavor to combine decomposition techniques with deep learning, multiple frequencies may still be mixed in the decomposed components, e.g., trend and seasonal. Furthermore, frequency domain analysis methods, e.g., Fourier and wavelet transforms, have limitations in resolution in the time domain and adaptability. In this paper, we propose D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for time series forecasting. Specifically, a multi-component decomposing (MCD) block is introduced to decompose the series into components with different frequency ranges, corresponding to the \"shallow\" aspect. A decomposition-reconstruction-decomposition (D-R-D) module is proposed to progressively extract the information of frequencies mixed in the components, corresponding to the \"deep\" aspect. After that, an interaction and fusion (IF) module is used to further analyze the components. Extensive experiments on seven real-world datasets demonstrate that D-PAD achieves the state-of-the-art performance, outperforming the best baseline by an average of 9.48% and 7.15% in MSE and MAE, respectively.","sentences":["In time series forecasting, effectively disentangling intricate temporal patterns is crucial.","While recent works endeavor to combine decomposition techniques with deep learning, multiple frequencies may still be mixed in the decomposed components, e.g., trend and seasonal.","Furthermore, frequency domain analysis methods, e.g., Fourier and wavelet transforms, have limitations in resolution in the time domain and adaptability.","In this paper, we propose D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for time series forecasting.","Specifically, a multi-component decomposing (MCD) block is introduced to decompose the series into components with different frequency ranges, corresponding to the \"shallow\" aspect.","A decomposition-reconstruction-decomposition (D-R-D) module is proposed to progressively extract the information of frequencies mixed in the components, corresponding to the \"deep\" aspect.","After that, an interaction and fusion (IF) module is used to further analyze the components.","Extensive experiments on seven real-world datasets demonstrate that D-PAD achieves the state-of-the-art performance, outperforming the best baseline by an average of 9.48% and 7.15% in MSE and MAE, respectively."],"url":"http://arxiv.org/abs/2403.17814v1","category":"cs.AI"}
{"created":"2024-03-26 15:39:59","title":"Steering Feedback in Dynamic Driving Simulators: The Influence of Steering Wheel Vibration and Vehicle Motion Frequency","abstract":"The validity of the subjective evaluation of steering feedback in driving simulators is crucial for modern vehicle development. Although there are established objective steering characteristics for the assessment of both stationary and dynamic feedback behaviour, factors such as steering wheel vibrations and vehicle body motion, particularly in high-frequency ranges, present challenges in simulator fidelity. This work investigates the influence of steering wheel vibration and vehicle body motion frequency content on the subjective evaluation of steering feedback during closed-loop driving in a dynamic driving simulator. A controlled subject study with 30 participants consisting of a back-to-back comparison of a reference vehicle with an electrical power steering system and three variants of its virtual representation on a dynamic driving simulator was performed. Subjective evaluation focused on the representation of road feedback in comparison to the reference vehicle. The statistical analysis of subjective results show that there is a significant influence of the frequency content of both steering wheel torque and vehicle motion on the subjective evaluation of steering feedback in a dynamic driving simulator. The results suggest an influence of frequency content on the subjective evaluation quality of steering feedback characteristics that are not associated with the dynamic feedback behaviour in the context of established performance indicators.","sentences":["The validity of the subjective evaluation of steering feedback in driving simulators is crucial for modern vehicle development.","Although there are established objective steering characteristics for the assessment of both stationary and dynamic feedback behaviour, factors such as steering wheel vibrations and vehicle body motion, particularly in high-frequency ranges, present challenges in simulator fidelity.","This work investigates the influence of steering wheel vibration and vehicle body motion frequency content on the subjective evaluation of steering feedback during closed-loop driving in a dynamic driving simulator.","A controlled subject study with 30 participants consisting of a back-to-back comparison of a reference vehicle with an electrical power steering system and three variants of its virtual representation on a dynamic driving simulator was performed.","Subjective evaluation focused on the representation of road feedback in comparison to the reference vehicle.","The statistical analysis of subjective results show that there is a significant influence of the frequency content of both steering wheel torque and vehicle motion on the subjective evaluation of steering feedback in a dynamic driving simulator.","The results suggest an influence of frequency content on the subjective evaluation quality of steering feedback characteristics that are not associated with the dynamic feedback behaviour in the context of established performance indicators."],"url":"http://arxiv.org/abs/2403.17800v1","category":"eess.SY"}
{"created":"2024-03-26 15:20:49","title":"Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications","abstract":"The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications. These sophisticated multimodal models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications. This paper investigates the applicability and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision Transformer (ViT) models in addressing critical security challenges. We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations. Our results highlight a significant divergence in performance, with Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models. The ViT models, on the other hand, demonstrate exceptional accuracy, achieving near-perfect performance on both tasks. This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications but also emphasizes the unmatched efficacy of fine-tuned ViT models for precise and dependable tasks.","sentences":["The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications.","These sophisticated multimodal models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications.","This paper investigates the applicability and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision Transformer (ViT) models in addressing critical security challenges.","We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations.","Our results highlight a significant divergence in performance, with Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models.","The ViT models, on the other hand, demonstrate exceptional accuracy, achieving near-perfect performance on both tasks.","This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications but also emphasizes the unmatched efficacy of fine-tuned ViT models for precise and dependable tasks."],"url":"http://arxiv.org/abs/2403.17787v1","category":"cs.AI"}
{"created":"2024-03-26 15:16:14","title":"SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings","abstract":"Crafting effective captions for figures is important. Readers heavily depend on these captions to grasp the figure's message. However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing. This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition. SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, OCR mention, key takeaways, and visual properties reference. Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them. A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing. Participants' feedback further offers valuable design insights for future systems aiming to enhance caption writing.","sentences":["Crafting effective captions for figures is important.","Readers heavily depend on these captions to grasp the figure's message.","However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing.","This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition.","SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, OCR mention, key takeaways, and visual properties reference.","Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them.","A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing.","Participants' feedback further offers valuable design insights for future systems aiming to enhance caption writing."],"url":"http://arxiv.org/abs/2403.17784v1","category":"cs.HC"}
{"created":"2024-03-26 15:12:46","title":"Optical Flow Based Detection and Tracking of Moving Objects for Autonomous Vehicles","abstract":"Accurate velocity estimation of surrounding moving objects and their trajectories are critical elements of perception systems in Automated/Autonomous Vehicles (AVs) with a direct impact on their safety. These are non-trivial problems due to the diverse types and sizes of such objects and their dynamic and random behaviour. Recent point cloud based solutions often use Iterative Closest Point (ICP) techniques, which are known to have certain limitations. For example, their computational costs are high due to their iterative nature, and their estimation error often deteriorates as the relative velocities of the target objects increase (>2 m/sec). Motivated by such shortcomings, this paper first proposes a novel Detection and Tracking of Moving Objects (DATMO) for AVs based on an optical flow technique, which is proven to be computationally efficient and highly accurate for such problems. \\textcolor{black}{This is achieved by representing the driving scenario as a vector field and applying vector calculus theories to ensure spatiotemporal continuity.} We also report the results of a comprehensive performance evaluation of the proposed DATMO technique, carried out in this study using synthetic and real-world data. The results of this study demonstrate the superiority of the proposed technique, compared to the DATMO techniques in the literature, in terms of estimation accuracy and processing time in a wide range of relative velocities of moving objects. Finally, we evaluate and discuss the sensitivity of the estimation error of the proposed DATMO technique to various system and environmental parameters, as well as the relative velocities of the moving objects.","sentences":["Accurate velocity estimation of surrounding moving objects and their trajectories are critical elements of perception systems in Automated/Autonomous Vehicles (AVs) with a direct impact on their safety.","These are non-trivial problems due to the diverse types and sizes of such objects and their dynamic and random behaviour.","Recent point cloud based solutions often use Iterative Closest Point (ICP) techniques, which are known to have certain limitations.","For example, their computational costs are high due to their iterative nature, and their estimation error often deteriorates as the relative velocities of the target objects increase (>2 m/sec).","Motivated by such shortcomings, this paper first proposes a novel Detection and Tracking of Moving Objects (DATMO) for AVs based on an optical flow technique, which is proven to be computationally efficient and highly accurate for such problems.","\\textcolor{black}{This is achieved by representing the driving scenario as a vector field and applying vector calculus theories to ensure spatiotemporal continuity.","}","We also report the results of a comprehensive performance evaluation of the proposed DATMO technique, carried out in this study using synthetic and real-world data.","The results of this study demonstrate the superiority of the proposed technique, compared to the DATMO techniques in the literature, in terms of estimation accuracy and processing time in a wide range of relative velocities of moving objects.","Finally, we evaluate and discuss the sensitivity of the estimation error of the proposed DATMO technique to various system and environmental parameters, as well as the relative velocities of the moving objects."],"url":"http://arxiv.org/abs/2403.17779v1","category":"cs.RO"}
{"created":"2024-03-26 15:11:18","title":"Towards a FAIR Documentation of Workflows and Models in Applied Mathematics","abstract":"Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics. The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows. MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template. Central to these workflows are mathematical models. MaRDI addresses them with the MathModDB ontology, offering a structured formal model description. Here, we showcase the interaction between MaRDMO and the MathModDB Knowledge Graph through an algebraic modeling workflow from the Digital Humanities. This demonstration underscores the versatility of both services beyond their original numerical domain.","sentences":["Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics.","The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows.","MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template.","Central to these workflows are mathematical models.","MaRDI addresses them with the MathModDB ontology, offering a structured formal model description.","Here, we showcase the interaction between MaRDMO and the MathModDB Knowledge Graph through an algebraic modeling workflow from the Digital Humanities.","This demonstration underscores the versatility of both services beyond their original numerical domain."],"url":"http://arxiv.org/abs/2403.17778v1","category":"cs.AI"}
{"created":"2024-03-26 14:57:30","title":"Spectra of correlators in the relaxation time approximation of kinetic theory","abstract":"The relaxation time approximation (RTA) of the kinetic Boltzmann equation is likely the simplest window into the microscopic properties of collective real-time transport. Within this framework, we analytically compute all retarded two-point Green's functions of the energy-momentum tensor and a conserved $U(1)$ current in thermal states with classical massless particles (a `CFT') at non-zero density, and in the absence and presence of broken translational symmetry. This is done in $2+1$ and $3+1$ dimensions. RTA allows a full explicit analysis of the analytic structure of different correlators (poles versus branch cuts) and the transport properties that they imply (the thermoelectric conductivities, and the hydrodynamic, quasihydrodynamic and gapped mode dispersion relations). Our inherently weakly coupled analysis thereby also enables a direct comparison with previously known strongly coupled results in holographic CFTs dual to the Einstein-Maxwell-axion theories.","sentences":["The relaxation time approximation (RTA) of the kinetic Boltzmann equation is likely the simplest window into the microscopic properties of collective real-time transport.","Within this framework, we analytically compute all retarded two-point Green's functions of the energy-momentum tensor and a conserved $U(1)$ current in thermal states with classical massless particles (a `CFT') at non-zero density, and in the absence and presence of broken translational symmetry.","This is done in $2+1$ and $3+1$ dimensions.","RTA allows a full explicit analysis of the analytic structure of different correlators (poles versus branch cuts) and the transport properties that they imply (the thermoelectric conductivities, and the hydrodynamic, quasihydrodynamic and gapped mode dispersion relations).","Our inherently weakly coupled analysis thereby also enables a direct comparison with previously known strongly coupled results in holographic CFTs dual to the Einstein-Maxwell-axion theories."],"url":"http://arxiv.org/abs/2403.17769v1","category":"hep-th"}
{"created":"2024-03-26 14:54:48","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation","abstract":"Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related to this work are available at https://dongqi.me/projects/SciNews.","sentences":["Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public.","The automated generation of such narratives enhances the accessibility of scholarly insights.","In this paper, we present a new corpus to facilitate this paradigm development.","Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines.","To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts.","We benchmark our dataset employing state-of-the-art text generation models.","The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports.","The dataset and code related to this work are available at https://dongqi.me/projects/SciNews."],"url":"http://arxiv.org/abs/2403.17768v1","category":"cs.CL"}
{"created":"2024-03-26 14:44:51","title":"DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection","abstract":"In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant. Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable. This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase. DataCook operates by \"cooking\" the raw data before distribution, enabling the development of models that perform normally on this processed data. However, during the deployment phase, the original test data must be also \"cooked\" through DataCook to ensure normal model performance. This process grants copyright holders control over authorization during the deployment phase. The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to standard adversarial examples (Adv) that aim to confuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations, ensuring that the data processed by DataCook remains easily understandable. We conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D data and the high-resolution variants. The outcomes indicate that DataCook effectively meets its objectives, preventing models trained on AntiAdv from analyzing unauthorized data effectively, without compromising the validity and accuracy of the data in legitimate scenarios. Code and data are available at https://github.com/MedMNIST/DataCook.","sentences":["In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant.","Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable.","This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase.","DataCook operates by \"cooking\" the raw data before distribution, enabling the development of models that perform normally on this processed data.","However, during the deployment phase, the original test data must be also \"cooked\" through DataCook to ensure normal model performance.","This process grants copyright holders control over authorization during the deployment phase.","The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to standard adversarial examples (Adv) that aim to confuse models.","Similar to Adv, AntiAdv introduces imperceptible perturbations, ensuring that the data processed by DataCook remains easily understandable.","We conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D data and the high-resolution variants.","The outcomes indicate that DataCook effectively meets its objectives, preventing models trained on AntiAdv from analyzing unauthorized data effectively, without compromising the validity and accuracy of the data in legitimate scenarios.","Code and data are available at https://github.com/MedMNIST/DataCook."],"url":"http://arxiv.org/abs/2403.17755v1","category":"cs.AI"}
{"created":"2024-03-26 14:44:00","title":"Optimal Euclidean Tree Covers","abstract":"A $(1+\\varepsilon)\\textit{-stretch tree cover}$ of a metric space is a collection of trees, where every pair of points has a $(1+\\varepsilon)$-stretch path in one of the trees. The celebrated $\\textit{Dumbbell Theorem}$ [Arya et~al. STOC'95] states that any set of $n$ points in $d$-dimensional Euclidean space admits a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d} \\cdot \\log(1/\\varepsilon))$ trees, where the $O_d$ notation suppresses terms that depend solely on the dimension~$d$. The running time of their construction is $O_d(n \\log n \\cdot \\frac{\\log(1/\\varepsilon)}{\\varepsilon^{d}} + n \\cdot \\varepsilon^{-2d})$. Since the same point may occur in multiple levels of the tree, the $\\textit{maximum degree}$ of a point in the tree cover may be as large as $\\Omega(\\log \\Phi)$, where $\\Phi$ is the aspect ratio of the input point set.   In this work we present a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d+1} \\cdot \\log(1/\\varepsilon))$ trees, which is optimal (up to the $\\log(1/\\varepsilon)$ factor). Moreover, the maximum degree of points in any tree is an $\\textit{absolute constant}$ for any $d$. As a direct corollary, we obtain an optimal {routing scheme} in low-dimensional Euclidean spaces. We also present a $(1+\\varepsilon)$-stretch $\\textit{Steiner}$ tree cover (that may use Steiner points) with $O_d(\\varepsilon^{(-d+1)/{2}} \\cdot \\log(1/\\varepsilon))$ trees, which too is optimal. The running time of our two constructions is linear in the number of edges in the respective tree covers, ignoring an additive $O_d(n \\log n)$ term; this improves over the running time underlying the Dumbbell Theorem.","sentences":["A $(1+\\varepsilon)\\textit{-stretch tree cover}$ of a metric space is a collection of trees, where every pair of points has a $(1+\\varepsilon)$-stretch path in one of the trees.","The celebrated $\\textit{Dumbbell Theorem}$","[Arya et~al. STOC'95] states that any set of $n$ points in $d$-dimensional Euclidean space admits a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d} \\cdot \\log(1/\\varepsilon))$ trees, where the $O_d$ notation suppresses terms that depend solely on the dimension~$d$.","The running time of their construction is $O_d(n \\log n \\cdot \\frac{\\log(1/\\varepsilon)}{\\varepsilon^{d}} + n \\cdot \\varepsilon^{-2d})$. Since the same point may occur in multiple levels of the tree, the $\\textit{maximum degree}$ of a point in the tree cover may be as large as $\\Omega(\\log \\Phi)$, where $\\Phi$ is the aspect ratio of the input point set.   ","In this work we present a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d+1} \\cdot \\log(1/\\varepsilon))$ trees, which is optimal (up to the $\\log(1/\\varepsilon)$ factor).","Moreover, the maximum degree of points in any tree is an $\\textit{absolute constant}$ for any $d$. As a direct corollary, we obtain an optimal {routing scheme} in low-dimensional Euclidean spaces.","We also present a $(1+\\varepsilon)$-stretch $\\textit{Steiner}$ tree cover (that may use Steiner points) with $O_d(\\varepsilon^{(-d+1)/{2}} \\cdot \\log(1/\\varepsilon))$ trees, which too is optimal.","The running time of our two constructions is linear in the number of edges in the respective tree covers, ignoring an additive $O_d(n \\log n)$ term; this improves over the running time underlying the Dumbbell Theorem."],"url":"http://arxiv.org/abs/2403.17754v1","category":"cs.CG"}
{"created":"2024-03-26 14:43:48","title":"Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS Systems","abstract":"Reconfigurable intelligent surface (RIS)-assisted index modulation system schemes are considered a promising technology for sixth-generation (6G) wireless communication systems, which can enhance various system capabilities such as coverage and reliability. However, obtaining perfect channel state information (CSI) is challenging due to the lack of a radio frequency chain in RIS. In this paper, we investigate the RIS-assisted full-duplex (FD) two-way space shift keying (SSK) system under imperfect CSI, where the signal emissions are augmented by deploying RISs in the vicinity of two FD users. The maximum likelihood detector is utilized to recover the transmit antenna index. With this in mind, we derive closed-form average bit error probability (ABEP) expression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide the upper bound and asymptotic ABEP expressions in the presence of channel estimation errors. To gain more insights, we also derive the outage probability and provide the throughput of the proposed scheme with imperfect CSI. The correctness of the analytical derivation results is confirmed via Monte Carlo simulations. It is demonstrated that increasing the number of elements of RIS can significantly improve the ABEP performance of the FD system over the half-duplex (HD) system. Furthermore, in the high SNR region, the ABEP performance of the FD system is better than that of the HD system.","sentences":["Reconfigurable intelligent surface (RIS)-assisted index modulation system schemes are considered a promising technology for sixth-generation (6G) wireless communication systems, which can enhance various system capabilities such as coverage and reliability.","However, obtaining perfect channel state information (CSI) is challenging due to the lack of a radio frequency chain in RIS.","In this paper, we investigate the RIS-assisted full-duplex (FD) two-way space shift keying (SSK) system under imperfect CSI, where the signal emissions are augmented by deploying RISs in the vicinity of two FD users.","The maximum likelihood detector is utilized to recover the transmit antenna index.","With this in mind, we derive closed-form average bit error probability (ABEP) expression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide the upper bound and asymptotic ABEP expressions in the presence of channel estimation errors.","To gain more insights, we also derive the outage probability and provide the throughput of the proposed scheme with imperfect CSI.","The correctness of the analytical derivation results is confirmed via Monte Carlo simulations.","It is demonstrated that increasing the number of elements of RIS can significantly improve the ABEP performance of the FD system over the half-duplex (HD) system.","Furthermore, in the high SNR region, the ABEP performance of the FD system is better than that of the HD system."],"url":"http://arxiv.org/abs/2403.17751v1","category":"cs.IT"}
{"created":"2024-03-26 14:40:10","title":"UCxn: Typologically Informed Annotation of Constructions Atop Universal Dependencies","abstract":"The Universal Dependencies (UD) project has created an invaluable collection of treebanks with contributions in over 140 languages. However, the UD annotations do not tell the full story. Grammatical constructions that convey meaning through a particular combination of several morphosyntactic elements -- for example, interrogative sentences with special markers and/or word orders -- are not labeled holistically. We argue for (i) augmenting UD annotations with a 'UCxn' annotation layer for such meaning-bearing grammatical constructions, and (ii) approaching this in a typologically informed way so that morphosyntactic strategies can be compared across languages. As a case study, we consider five construction families in ten languages, identifying instances of each construction in UD treebanks through the use of morphosyntactic patterns. In addition to findings regarding these particular constructions, our study yields important insights on methodology for describing and identifying constructions in language-general and language-particular ways, and lays the foundation for future constructional enrichment of UD treebanks.","sentences":["The Universal Dependencies (UD) project has created an invaluable collection of treebanks with contributions in over 140 languages.","However, the UD annotations do not tell the full story.","Grammatical constructions that convey meaning through a particular combination of several morphosyntactic elements -- for example, interrogative sentences with special markers and/or word orders -- are not labeled holistically.","We argue for (i) augmenting UD annotations with a 'UCxn' annotation layer for such meaning-bearing grammatical constructions, and (ii) approaching this in a typologically informed way so that morphosyntactic strategies can be compared across languages.","As a case study, we consider five construction families in ten languages, identifying instances of each construction in UD treebanks through the use of morphosyntactic patterns.","In addition to findings regarding these particular constructions, our study yields important insights on methodology for describing and identifying constructions in language-general and language-particular ways, and lays the foundation for future constructional enrichment of UD treebanks."],"url":"http://arxiv.org/abs/2403.17748v1","category":"cs.CL"}
{"created":"2024-03-26 14:30:23","title":"Using Stratified Sampling to Improve LIME Image Explanations","abstract":"We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator. Experiments show the efficacy of the proposed approach.","sentences":["We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling.","Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data.","We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past.","We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator.","Experiments show the efficacy of the proposed approach."],"url":"http://arxiv.org/abs/2403.17742v1","category":"cs.AI"}
{"created":"2024-03-26 14:29:34","title":"All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction","abstract":"Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important interactions via the observed data. In the experiments, we evaluate our model under three cold-start settings on three real-world datasets. The experimental results show that HIRE outperforms other baselines by a large margin. Furthermore, we visualize the inferred interactions of HIRE to confirm the contribution of our model.","sentences":["Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied.","Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items.","However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task.","Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE).","HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network.","Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important interactions via the observed data.","In the experiments, we evaluate our model under three cold-start settings on three real-world datasets.","The experimental results show that HIRE outperforms other baselines by a large margin.","Furthermore, we visualize the inferred interactions of HIRE to confirm the contribution of our model."],"url":"http://arxiv.org/abs/2403.17740v1","category":"cs.IR"}
{"created":"2024-03-26 14:24:01","title":"Out-of-distribution Rumor Detection via Test-Time Adaptation","abstract":"Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Existing methods for rumor detection have achieved good performance, as they have collected enough corpus from the same data distribution for model training. However, significant distribution shifts between the training data and real-world test data occur due to differences in news topics, social media platforms, languages and the variance in propagation scale caused by news popularity. This leads to a substantial decline in the performance of these existing methods in Out-Of-Distribution (OOD) situations. To address this problem, we propose a simple and efficient method named Test-time Adaptation for Rumor Detection under distribution shifts (TARD). This method models the propagation of news in the form of a propagation graph, and builds propagation graph test-time adaptation framework, enhancing the model's adaptability and robustness when facing OOD problems. Extensive experiments conducted on two group datasets collected from real-world social platforms demonstrate that our framework outperforms the state-of-the-art methods in performance.","sentences":["Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge.","Existing methods for rumor detection have achieved good performance, as they have collected enough corpus from the same data distribution for model training.","However, significant distribution shifts between the training data and real-world test data occur due to differences in news topics, social media platforms, languages and the variance in propagation scale caused by news popularity.","This leads to a substantial decline in the performance of these existing methods in Out-Of-Distribution (OOD) situations.","To address this problem, we propose a simple and efficient method named Test-time Adaptation for Rumor Detection under distribution shifts (TARD).","This method models the propagation of news in the form of a propagation graph, and builds propagation graph test-time adaptation framework, enhancing the model's adaptability and robustness when facing OOD problems.","Extensive experiments conducted on two group datasets collected from real-world social platforms demonstrate that our framework outperforms the state-of-the-art methods in performance."],"url":"http://arxiv.org/abs/2403.17735v1","category":"cs.AI"}
{"created":"2024-03-26 14:21:49","title":"Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation scans using Linked Denoising Diffusion Probabilistic Models","abstract":"The rapid advancement of Artificial Intelligence (AI) in biomedical imaging and radiotherapy is hindered by the limited availability of large imaging data repositories. With recent research and improvements in denoising diffusion probabilistic models (DDPM), high quality synthetic medical scans are now possible. Despite this, there is currently no way of generating multiple related images, such as a corresponding ground truth which can be used to train models, so synthetic scans are often manually annotated before use. This research introduces a novel architecture that is able to generate multiple, related PET-CT-tumour mask pairs using paired networks and conditional encoders. Our approach includes innovative, time step-controlled mechanisms and a `noise-seeding' strategy to improve DDPM sampling consistency. While our model requires a modified perceptual loss function to ensure accurate feature alignment we show generation of clearly aligned synthetic images and improvement in segmentation accuracy with generated images.","sentences":["The rapid advancement of Artificial Intelligence (AI) in biomedical imaging and radiotherapy is hindered by the limited availability of large imaging data repositories.","With recent research and improvements in denoising diffusion probabilistic models (DDPM), high quality synthetic medical scans are now possible.","Despite this, there is currently no way of generating multiple related images, such as a corresponding ground truth which can be used to train models, so synthetic scans are often manually annotated before use.","This research introduces a novel architecture that is able to generate multiple, related PET-CT-tumour mask pairs using paired networks and conditional encoders.","Our approach includes innovative, time step-controlled mechanisms and a `noise-seeding' strategy to improve DDPM sampling consistency.","While our model requires a modified perceptual loss function to ensure accurate feature alignment we show generation of clearly aligned synthetic images and improvement in segmentation accuracy with generated images."],"url":"http://arxiv.org/abs/2403.17734v1","category":"eess.IV"}
{"created":"2024-03-26 14:14:30","title":"Tiny Models are the Computational Saver for Large Models","abstract":"This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively. Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the model's backbone. Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression. This finding will help the research community in exploring new compression methods to address the escalating computational demands posed by rapidly evolving AI models. Our evaluation of this approach in ImageNet-1k classification demonstrates its potential to reduce the number of compute operations by up to 90%, with only negligible losses in performance, across various modern vision models. The code of this work will be available.","sentences":["This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively.","Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources.","Most existing early exit designs are implemented by attaching additional network branches to the model's backbone.","Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance.","Employing them as the first exit can remarkably enhance computational efficiency.","By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression.","This finding will help the research community in exploring new compression methods to address the escalating computational demands posed by rapidly evolving AI models.","Our evaluation of this approach in ImageNet-1k classification demonstrates its potential to reduce the number of compute operations by up to 90%, with only negligible losses in performance, across various modern vision models.","The code of this work will be available."],"url":"http://arxiv.org/abs/2403.17726v1","category":"cs.AI"}
{"created":"2024-03-26 14:12:36","title":"Magnonic inverse-design processor","abstract":"Artificial Intelligence (AI) technology has revolutionized our everyday lives and research. The concept of inverse design, which involves defining a functionality by a human and then using an algorithm to search for the device's design, opened new perspectives for information processing. A specialized AI-driven processor capable of solving an inverse problem in real-time offers a compelling alternative to the time and energy-intensive CMOS computations. Here, we report on a magnon-based processor that uses a complex reconfigurable medium to process data in the gigahertz range, catering to the demands of 5G and 6G telecommunication. Demonstrating its versatility, the processor solves inverse problems using two algorithms to realize RF notch filters and demultiplexers. The processor also exhibits potential for binary, reservoir, and neuromorphic computing paradigms.","sentences":["Artificial Intelligence (AI) technology has revolutionized our everyday lives and research.","The concept of inverse design, which involves defining a functionality by a human and then using an algorithm to search for the device's design, opened new perspectives for information processing.","A specialized AI-driven processor capable of solving an inverse problem in real-time offers a compelling alternative to the time and energy-intensive CMOS computations.","Here, we report on a magnon-based processor that uses a complex reconfigurable medium to process data in the gigahertz range, catering to the demands of 5G and 6G telecommunication.","Demonstrating its versatility, the processor solves inverse problems using two algorithms to realize RF notch filters and demultiplexers.","The processor also exhibits potential for binary, reservoir, and neuromorphic computing paradigms."],"url":"http://arxiv.org/abs/2403.17724v1","category":"physics.app-ph"}
{"created":"2024-03-26 13:58:47","title":"Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark","abstract":"The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity. Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas. However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets. In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images. Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes. Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively. The code and data will be made available soon.","sentences":["The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity.","Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas.","However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets.","In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images.","Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes.","Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively.","The code and data will be made available soon."],"url":"http://arxiv.org/abs/2403.17712v1","category":"cs.CV"}
{"created":"2024-03-26 13:58:00","title":"Optimization-based Prompt Injection Attack to LLM-as-a-Judge","abstract":"LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. Through extensive experiments, we showcase the capability of JudgeDeceiver in altering decision outcomes across various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.","sentences":["LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs).","Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment.","However, the robustness of these systems against prompt injection attacks remains an open question.","In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge.","Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations.","Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems.","Through extensive experiments, we showcase the capability of JudgeDeceiver in altering decision outcomes across various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack."],"url":"http://arxiv.org/abs/2403.17710v1","category":"cs.CR"}
{"created":"2024-03-26 13:54:52","title":"Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","abstract":"With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge. Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment. At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos. However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.   Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos. The dataset provides details on the viewport and gaze attention locations of users. Besides, we present some statistics samples extracted from the dataset. For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution. Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos. That's why we name the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.   The dataset is available on website: https://dianvrlab.github.io/Panonut360/.","sentences":["With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge.","Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment.","At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos.","However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.   ","Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos.","The dataset provides details on the viewport and gaze attention locations of users.","Besides, we present some statistics samples extracted from the dataset.","For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution.","Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos.","That's why we name the dataset Panonut, a saliency weighting shaped like a donut.","Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.   ","The dataset is available on website: https://dianvrlab.github.io/Panonut360/."],"url":"http://arxiv.org/abs/2403.17708v1","category":"cs.CV"}
{"created":"2024-03-26 13:51:10","title":"Effect of light-assisted tunable interaction on the position response function of cold atoms","abstract":"The position response of a particle subjected to a perturbation is of general interest in physics. We study the modification of the position response function of an ensemble of cold atoms in a magneto-optical trap in the presence of tunable light-assisted interactions. We subject the cold atoms to an intense laser light tuned near the photoassociation resonance and observe the position response of the atoms subjected to a sudden displacement. Surprisingly, we observe that the entire cold atomic cloud undergoes collective oscillations. We use a generalised quantum Langevin approach to theoretically analyse the results of the experiments and find good agreement.","sentences":["The position response of a particle subjected to a perturbation is of general interest in physics.","We study the modification of the position response function of an ensemble of cold atoms in a magneto-optical trap in the presence of tunable light-assisted interactions.","We subject the cold atoms to an intense laser light tuned near the photoassociation resonance and observe the position response of the atoms subjected to a sudden displacement.","Surprisingly, we observe that the entire cold atomic cloud undergoes collective oscillations.","We use a generalised quantum Langevin approach to theoretically analyse the results of the experiments and find good agreement."],"url":"http://arxiv.org/abs/2403.17707v1","category":"physics.atom-ph"}
{"created":"2024-03-26 13:50:34","title":"Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement","abstract":"Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics. Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data. In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed \"Topic Refinement\". This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined. By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically. This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by various models. Our comprehensive evaluation across three unique datasets has shown that our topic refinement approach significantly enhances the semantic coherence of topics.","sentences":["Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics.","Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data.","In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed \"Topic Refinement\".","This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined.","By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically.","This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by various models.","Our comprehensive evaluation across three unique datasets has shown that our topic refinement approach significantly enhances the semantic coherence of topics."],"url":"http://arxiv.org/abs/2403.17706v1","category":"cs.CL"}
{"created":"2024-03-26 13:49:48","title":"Prioritize Team Actions: Multi-Agent Temporal Logic Task Planning with Ordering Constraints","abstract":"In this paper, we investigate the problem of linear temporal logic (LTL) path planning for multi-agent systems, introducing the new concept of \\emph{ordering constraints}. Specifically, we consider a generic objective function that is defined for the path of each individual agent. The primary objective is to find a global plan for the team of agents, ensuring they collectively meet the specified LTL requirements. Simultaneously, we aim to maintain a pre-determined order in the values of the objective function for each agent, which we refer to as the ordering constraints. This new requirement stems from scenarios like security-aware planning, where relative orders outweigh absolute values in importance. We present an efficient algorithm to solve this problem, supported by proofs of correctness that demonstrate the optimality of our solution. Additionally, we provide a case study in security-aware path planning to illustrate the practicality and effectiveness of our proposed approach.","sentences":["In this paper, we investigate the problem of linear temporal logic (LTL) path planning for multi-agent systems, introducing the new concept of \\emph{ordering constraints}.","Specifically, we consider a generic objective function that is defined for the path of each individual agent.","The primary objective is to find a global plan for the team of agents, ensuring they collectively meet the specified LTL requirements.","Simultaneously, we aim to maintain a pre-determined order in the values of the objective function for each agent, which we refer to as the ordering constraints.","This new requirement stems from scenarios like security-aware planning, where relative orders outweigh absolute values in importance.","We present an efficient algorithm to solve this problem, supported by proofs of correctness that demonstrate the optimality of our solution.","Additionally, we provide a case study in security-aware path planning to illustrate the practicality and effectiveness of our proposed approach."],"url":"http://arxiv.org/abs/2403.17704v1","category":"eess.SY"}
{"created":"2024-03-26 17:57:20","title":"Testing the $\\mathbf\u039b$CDM Cosmological Model with Forthcoming Measurements of the Cosmic Microwave Background with SPT-3G","abstract":"We forecast constraints on cosmological parameters enabled by three surveys conducted with SPT-3G, the third-generation camera on the South Pole Telescope. The surveys cover separate regions of 1500, 2650, and 6000 ${\\rm deg}^{2}$ to different depths, in total observing 25% of the sky. These regions will be measured to white noise levels of roughly 2.5, 9, and 12 $\\mu{\\rm K-arcmin}$, respectively, in CMB temperature units at 150 GHz by the end of 2024. The survey also includes measurements at 95 and 220 GHz, which have noise levels a factor of ~1.2 and 3.5 times higher than 150 GHz, respectively, with each band having a polarization noise level ~$\\sqrt{\\text{2}}$ times higher than the temperature noise. We use a novel approach to obtain the covariance matrices for jointly and optimally estimated gravitational lensing potential bandpowers and unlensed CMB temperature and polarization bandpowers. We demonstrate the ability to test the $\\Lambda{\\rm CDM}$ model via the consistency of cosmological parameters constrained independently from SPT-3G and Planck data, and consider the improvement in constraints on $\\Lambda{\\rm CDM}$ extension parameters from a joint analysis of SPT-3G and Planck data. The $\\Lambda{\\rm CDM}$ cosmological parameters are typically constrained with uncertainties up to ~2 times smaller with SPT-3G data, compared to Planck, with the two data sets measuring significantly different angular scales and polarization levels, providing additional tests of the standard cosmological model.","sentences":["We forecast constraints on cosmological parameters enabled by three surveys conducted with SPT-3G, the third-generation camera on the South Pole Telescope.","The surveys cover separate regions of 1500, 2650, and 6000 ${\\rm deg}^{2}$ to different depths, in total observing 25% of the sky.","These regions will be measured to white noise levels of roughly 2.5, 9, and 12 $\\mu{\\rm K-arcmin}$, respectively, in CMB temperature units at 150 GHz by the end of 2024.","The survey also includes measurements at 95 and 220 GHz, which have noise levels a factor of ~1.2 and 3.5 times higher than 150 GHz, respectively, with each band having a polarization noise level ~$\\sqrt{\\text{2}}$ times higher than the temperature noise.","We use a novel approach to obtain the covariance matrices for jointly and optimally estimated gravitational lensing potential bandpowers and unlensed CMB temperature and polarization bandpowers.","We demonstrate the ability to test the $\\Lambda{\\rm CDM}$ model via the consistency of cosmological parameters constrained independently from SPT-3G and Planck data, and consider the improvement in constraints on $\\Lambda{\\rm CDM}$ extension parameters from a joint analysis of SPT-3G and Planck data.","The $\\Lambda{\\rm CDM}$ cosmological parameters are typically constrained with uncertainties up to ~2 times smaller with SPT-3G data, compared to Planck, with the two data sets measuring significantly different angular scales and polarization levels, providing additional tests of the standard cosmological model."],"url":"http://arxiv.org/abs/2403.17925v1","category":"astro-ph.CO"}
{"created":"2024-03-26 17:47:00","title":"Beyond chromatic threshold via $(p,q)$-theorem, and sharp blow-up phenomenon","abstract":"We establish a novel connection between the well-known chromatic threshold problem in extremal combinatorics and the celebrated $(p,q)$-theorem in discrete geometry. In particular, for a graph $G$ with bounded clique number and a natural density condition, we prove a $(p,q)$-theorem for an abstract convexity space associated with $G$. Our result strengthens those of Thomassen and Nikiforov on the chromatic threshold of cliques. Our $(p,q)$-theorem can also be viewed as a $\\chi$-boundedness result for (what we call) ultra maximal $K_r$-free graphs.   We further show that the graphs under study are blow-ups of constant size graphs, improving a result of Oberkampf and Schacht on homomorphism threshold of cliques. Our result unravels the cause underpinning such a blow-up phenomenon, differentiating the chromatic and homomorphism threshold problems for cliques. Our result implies that for the homomorphism threshold problem, rather than the minimum degree condition usually considered in the literature, the decisive factor is a clique density condition on co-neighborhoods of vertices. More precisely, we show that if an $n$-vertex $K_{r}$-free graph $G$ satisfies that the common neighborhood of every pair of non-adjacent vertices induces a subgraph with $K_{r-2}$-density at least $\\varepsilon>0$, then $G$ must be a blow-up of some $K_r$-free graph $F$ on at most $2^{O(\\frac{r}{\\varepsilon}\\log\\frac{1}{\\varepsilon})}$ vertices. Furthermore, this single exponential bound is optimal. We construct examples with no $K_r$-free homomorphic image of size smaller than $2^{\\Omega_r(\\frac{1}{\\varepsilon})}$.","sentences":["We establish a novel connection between the well-known chromatic threshold problem in extremal combinatorics and the celebrated $(p,q)$-theorem in discrete geometry.","In particular, for a graph $G$ with bounded clique number and a natural density condition, we prove a $(p,q)$-theorem for an abstract convexity space associated with $G$. Our result strengthens those of Thomassen and Nikiforov on the chromatic threshold of cliques.","Our $(p,q)$-theorem can also be viewed as a $\\chi$-boundedness result for (what we call) ultra maximal $K_r$-free graphs.   ","We further show that the graphs under study are blow-ups of constant size graphs, improving a result of Oberkampf and Schacht on homomorphism threshold of cliques.","Our result unravels the cause underpinning such a blow-up phenomenon, differentiating the chromatic and homomorphism threshold problems for cliques.","Our result implies that for the homomorphism threshold problem, rather than the minimum degree condition usually considered in the literature, the decisive factor is a clique density condition on co-neighborhoods of vertices.","More precisely, we show that if an $n$-vertex $K_{r}$-free graph $G$ satisfies that the common neighborhood of every pair of non-adjacent vertices induces a subgraph with $K_{r-2}$-density at least $\\varepsilon>0$, then $G$ must be a blow-up of some $K_r$-free graph $F$ on at most $2^{O(\\frac{r}{\\varepsilon}\\log\\frac{1}{\\varepsilon})}$ vertices.","Furthermore, this single exponential bound is optimal.","We construct examples with no $K_r$-free homomorphic image of size smaller than $2^{\\Omega_r(\\frac{1}{\\varepsilon})}$."],"url":"http://arxiv.org/abs/2403.17910v1","category":"math.CO"}
{"created":"2024-03-26 17:46:25","title":"ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing Change Detection","abstract":"Deep learning has shown remarkable success in remote sensing change detection (CD), aiming to identify semantic change regions between co-registered satellite image pairs acquired at distinct time stamps. However, existing convolutional neural network and transformer-based frameworks often struggle to accurately segment semantic change regions. Moreover, transformers-based methods with standard self-attention suffer from quadratic computational complexity with respect to the image resolution, making them less practical for CD tasks with limited training data. To address these issues, we propose an efficient change detection framework, ELGC-Net, which leverages rich contextual information to precisely estimate change regions while reducing the model size. Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The focus of our design is the introduction of an Efficient Local-Global Context Aggregator module within the encoder, capturing enhanced global context and local spatial information through a novel pooled-transpose (PT) attention and depthwise convolution, respectively. The PT attention employs pooling operations for robust feature extraction and minimizes computational cost with transposed attention. Extensive experiments on three challenging CD datasets demonstrate that ELGC-Net outperforms existing methods. Compared to the recent transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in intersection over union metric on the LEVIR-CD dataset, while significantly reducing trainable parameters. Our proposed ELGC-Net sets a new state-of-the-art performance in remote sensing change detection benchmarks. Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly reduced computational complexity, suitable for resource-constrained settings, while achieving comparable performance. Project url https://github.com/techmn/elgcnet.","sentences":["Deep learning has shown remarkable success in remote sensing change detection (CD), aiming to identify semantic change regions between co-registered satellite image pairs acquired at distinct time stamps.","However, existing convolutional neural network and transformer-based frameworks often struggle to accurately segment semantic change regions.","Moreover, transformers-based methods with standard self-attention suffer from quadratic computational complexity with respect to the image resolution, making them less practical for CD tasks with limited training data.","To address these issues, we propose an efficient change detection framework, ELGC-Net, which leverages rich contextual information to precisely estimate change regions while reducing the model size.","Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder.","The focus of our design is the introduction of an Efficient Local-Global Context Aggregator module within the encoder, capturing enhanced global context and local spatial information through a novel pooled-transpose (PT) attention and depthwise convolution, respectively.","The PT attention employs pooling operations for robust feature extraction and minimizes computational cost with transposed attention.","Extensive experiments on three challenging CD datasets demonstrate that ELGC-Net outperforms existing methods.","Compared to the recent transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in intersection over union metric on the LEVIR-CD dataset, while significantly reducing trainable parameters.","Our proposed ELGC-Net sets a new state-of-the-art performance in remote sensing change detection benchmarks.","Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly reduced computational complexity, suitable for resource-constrained settings, while achieving comparable performance.","Project url https://github.com/techmn/elgcnet."],"url":"http://arxiv.org/abs/2403.17909v1","category":"cs.CV"}
{"created":"2024-03-26 17:45:48","title":"Multi-Agent Resilient Consensus under Intermittent Faulty and Malicious Transmissions (Extended Version)","abstract":"In this work, we consider the consensus problem in which legitimate agents share their values over an undirected communication network in the presence of malicious or faulty agents. Different from the previous works, we characterize the conditions that generalize to several scenarios such as intermittent faulty or malicious transmissions, based on trust observations. As the standard trust aggregation approach based on a constant threshold fails to distinguish intermittent malicious/faulty activity, we propose a new detection algorithm utilizing time-varying thresholds and the random trust values available to legitimate agents. Under these conditions, legitimate agents almost surely determine their trusted neighborhood correctly with geometrically decaying misclassification probabilities. We further prove that the consensus process converges almost surely even in the presence of malicious agents. We also derive the probabilistic bounds on the deviation from the nominal consensus value that would have been achieved with no malicious agents in the system. Numerical results verify the convergence among agents and exemplify the deviation under different scenarios.","sentences":["In this work, we consider the consensus problem in which legitimate agents share their values over an undirected communication network in the presence of malicious or faulty agents.","Different from the previous works, we characterize the conditions that generalize to several scenarios such as intermittent faulty or malicious transmissions, based on trust observations.","As the standard trust aggregation approach based on a constant threshold fails to distinguish intermittent malicious/faulty activity, we propose a new detection algorithm utilizing time-varying thresholds and the random trust values available to legitimate agents.","Under these conditions, legitimate agents almost surely determine their trusted neighborhood correctly with geometrically decaying misclassification probabilities.","We further prove that the consensus process converges almost surely even in the presence of malicious agents.","We also derive the probabilistic bounds on the deviation from the nominal consensus value that would have been achieved with no malicious agents in the system.","Numerical results verify the convergence among agents and exemplify the deviation under different scenarios."],"url":"http://arxiv.org/abs/2403.17907v1","category":"eess.SY"}
{"created":"2024-03-26 17:27:20","title":"Density of group languages in shift spaces","abstract":"We study the density of group languages (i.e. rational languages recognized by morphisms onto finite groups) inside shift spaces. The density of a rational language can be understood as the frequency of some \"pattern\" in the shift space, for example a pattern like \"words with an even number of a given letter.\" In this paper, we handle density of group languages via ergodicity of skew products between the shift space and the recognizing group. We consider both the cases of shifts of finite type (with a suitable notion of irreducibility), and of minimal shifts. In the latter case, our main result is a closed formula for the density which holds whenever the skew product has minimal closed invariant subsets which are ergodic under the product of the original measure and the uniform probability measure on the group. The formula is derived in part from a characterization of minimal closed invariant subsets for skew products relying on notions of cocycles and coboundaries. In the case where the whole skew product itself is ergodic under the product measure, then the density is completely determined by the cardinality of the image of the language inside the recognizing group. We provide sufficient conditions for the skew product to have minimal closed invariant subsets that are ergodic under the product measure. Finally, we investigate the link between minimal closed invariant subsets, return words and bifix codes.","sentences":["We study the density of group languages (i.e. rational languages recognized by morphisms onto finite groups) inside shift spaces.","The density of a rational language can be understood as the frequency of some \"pattern\" in the shift space, for example a pattern like \"words with an even number of a given letter.\"","In this paper, we handle density of group languages via ergodicity of skew products between the shift space and the recognizing group.","We consider both the cases of shifts of finite type (with a suitable notion of irreducibility), and of minimal shifts.","In the latter case, our main result is a closed formula for the density which holds whenever the skew product has minimal closed invariant subsets which are ergodic under the product of the original measure and the uniform probability measure on the group.","The formula is derived in part from a characterization of minimal closed invariant subsets for skew products relying on notions of cocycles and coboundaries.","In the case where the whole skew product itself is ergodic under the product measure, then the density is completely determined by the cardinality of the image of the language inside the recognizing group.","We provide sufficient conditions for the skew product to have minimal closed invariant subsets that are ergodic under the product measure.","Finally, we investigate the link between minimal closed invariant subsets, return words and bifix codes."],"url":"http://arxiv.org/abs/2403.17892v1","category":"math.DS"}
{"created":"2024-03-26 17:12:50","title":"On the properties of distance covariance for categorical data: Robustness, sure screening, and approximate null distributions","abstract":"Pearson's Chi-squared test, though widely used for detecting association between categorical variables, exhibits low statistical power in large sparse contingency tables. To address this limitation, two novel permutation tests have been recently developed: the distance covariance permutation test and the U-statistic permutation test. Both leverage the distance covariance functional but employ different estimators. In this work, we explore key statistical properties of the distance covariance for categorical variables. Firstly, we show that unlike Chi-squared, the distance covariance functional is B-robust for any number of categories (fixed or diverging). Second, we establish the strong consistency of distance covariance screening under mild conditions, and simulations confirm its advantage over Chi-squared screening, especially for large sparse tables. Finally, we derive an approximate null distribution for a bias-corrected distance correlation estimate, demonstrating its effectiveness through simulations.","sentences":["Pearson's Chi-squared test, though widely used for detecting association between categorical variables, exhibits low statistical power in large sparse contingency tables.","To address this limitation, two novel permutation tests have been recently developed: the distance covariance permutation test and the U-statistic permutation test.","Both leverage the distance covariance functional but employ different estimators.","In this work, we explore key statistical properties of the distance covariance for categorical variables.","Firstly, we show that unlike Chi-squared, the distance covariance functional is B-robust for any number of categories (fixed or diverging).","Second, we establish the strong consistency of distance covariance screening under mild conditions, and simulations confirm its advantage over Chi-squared screening, especially for large sparse tables.","Finally, we derive an approximate null distribution for a bias-corrected distance correlation estimate, demonstrating its effectiveness through simulations."],"url":"http://arxiv.org/abs/2403.17882v1","category":"stat.ME"}
{"created":"2024-03-26 17:06:56","title":"MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation","abstract":"Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers. Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news recommendation focuses on major, resource-rich languages, and English in particular. Moreover, nearly all news recommendation efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages. Accordingly, the existing body of work on news recommendation suffers from a lack of publicly available multilingual benchmarks that would catalyze development of news recommenders effective in multilingual settings and for low-resource languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual news recommendation dataset derived from the English MIND dataset using machine translation, covering a set of 14 linguistically and geographically diverse languages, with digital footprints of varying sizes. Using xMIND, we systematically benchmark several state-of-the-art content-based neural news recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer scenarios, considering both monolingual and bilingual news consumption patterns. Our findings reveal that (i) current NNRs, even when based on a multilingual language model, suffer from substantial performance losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT training has limited benefits, particularly when combined with a bilingual news consumption. Our findings thus warrant a broader research effort in multilingual and cross-lingual news recommendation. The xMIND dataset is available at https://github.com/andreeaiana/xMIND.","sentences":["Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers.","Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news recommendation focuses on major, resource-rich languages, and English in particular.","Moreover, nearly all news recommendation efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages.","Accordingly, the existing body of work on news recommendation suffers from a lack of publicly available multilingual benchmarks that would catalyze development of news recommenders effective in multilingual settings and for low-resource languages.","Aiming to fill this gap, we introduce xMIND, an open, multilingual news recommendation dataset derived from the English MIND dataset using machine translation, covering a set of 14 linguistically and geographically diverse languages, with digital footprints of varying sizes.","Using xMIND, we systematically benchmark several state-of-the-art content-based neural news recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer scenarios, considering both monolingual and bilingual news consumption patterns.","Our findings reveal that (i) current NNRs, even when based on a multilingual language model, suffer from substantial performance losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT training has limited benefits, particularly when combined with a bilingual news consumption.","Our findings thus warrant a broader research effort in multilingual and cross-lingual news recommendation.","The xMIND dataset is available at https://github.com/andreeaiana/xMIND."],"url":"http://arxiv.org/abs/2403.17876v1","category":"cs.IR"}
{"created":"2024-03-26 17:03:55","title":"The Solution to an Impulse Control Problem Motivated by Optimal Harvesting","abstract":"We consider a stochastic impulse control problem that is motivated by applications such as the optimal exploitation of a natural resource. In particular, we consider a stochastic system whose uncontrolled state dynamics are modelled by a non-explosive positive linear diffusion. The control that can be applied to this system takes the form of one-sided impulsive action. The objective of the control problem is to maximise a discounted performance criterion that rewards the effect of control action but involves a fixed cost at each time of a control intervention. We derive the complete solution to this problem under general assumptions. It turns out that the solution can take four qualitatively different forms, several of which have not been observed in the literature. In two of the four cases, there exist only $\\varepsilon$-optimal control strategies. We also show that the boundary classification of 0 may play a critical role in the solution of the problem. Furthermore, we develop a way for establishing the strong solution to a stochastic impulse control problem's optimally controlled SDE.","sentences":["We consider a stochastic impulse control problem that is motivated by applications such as the optimal exploitation of a natural resource.","In particular, we consider a stochastic system whose uncontrolled state dynamics are modelled by a non-explosive positive linear diffusion.","The control that can be applied to this system takes the form of one-sided impulsive action.","The objective of the control problem is to maximise a discounted performance criterion that rewards the effect of control action but involves a fixed cost at each time of a control intervention.","We derive the complete solution to this problem under general assumptions.","It turns out that the solution can take four qualitatively different forms, several of which have not been observed in the literature.","In two of the four cases, there exist only $\\varepsilon$-optimal control strategies.","We also show that the boundary classification of 0 may play a critical role in the solution of the problem.","Furthermore, we develop a way for establishing the strong solution to a stochastic impulse control problem's optimally controlled SDE."],"url":"http://arxiv.org/abs/2403.17875v1","category":"math.OC"}
{"created":"2024-03-26 16:57:01","title":"Sample complexity of quantum hypothesis testing","abstract":"Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing. In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. As a counterpart of the quantum Stein's lemma, we also find that the sample complexity of asymmetric binary quantum hypothesis testing depends logarithmically on the inverse type~II error probability and inversely on the quantum relative entropy. Finally, we provide lower and upper bounds on the sample complexity of multiple quantum hypothesis testing, with it remaining an intriguing open question to improve these bounds.","sentences":["Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state.","In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability.","By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing.","In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity.","As a counterpart of the quantum Stein's lemma, we also find that the sample complexity of asymmetric binary quantum hypothesis testing depends logarithmically on the inverse type~II error probability and inversely on the quantum relative entropy.","Finally, we provide lower and upper bounds on the sample complexity of multiple quantum hypothesis testing, with it remaining an intriguing open question to improve these bounds."],"url":"http://arxiv.org/abs/2403.17868v1","category":"quant-ph"}
{"created":"2024-03-26 16:53:32","title":"A Floquet analysis perspective of driven light-matter interaction models","abstract":"In this paper, we analyze the harmonically driven Jaynes-Cummings and Lipkin-Meshkov-Glick models using both numerical integration of time-dependent Hamiltonians and Floquet theory. For a separation of time-scales between the drive and intrinsic Rabi oscillations in the former model, the driving results in an effective periodic reversal of time. The corresponding Floquet Hamiltonian is a Wannier-Stark model, which can be analytically solved. Despite the chaotic nature of the driven Lipkin-Meshkov-Glick model, moderate system sizes can display qualitatively different behaviors under varying system parameters. Ergodicity arises in systems that are neither adiabatic nor diabatic, owing to repeated multi-level Landau-Zener transitions. Chaotic behavior, observed in slow driving, manifests as random jumps in the magnetization, suggesting potential utility as a random number generator. Furthermore, we discuss both models in terms of what we call Floquet Fock state lattices.","sentences":["In this paper, we analyze the harmonically driven Jaynes-Cummings and Lipkin-Meshkov-Glick models using both numerical integration of time-dependent Hamiltonians and Floquet theory.","For a separation of time-scales between the drive and intrinsic Rabi oscillations in the former model, the driving results in an effective periodic reversal of time.","The corresponding Floquet Hamiltonian is a Wannier-Stark model, which can be analytically solved.","Despite the chaotic nature of the driven Lipkin-Meshkov-Glick model, moderate system sizes can display qualitatively different behaviors under varying system parameters.","Ergodicity arises in systems that are neither adiabatic nor diabatic, owing to repeated multi-level Landau-Zener transitions.","Chaotic behavior, observed in slow driving, manifests as random jumps in the magnetization, suggesting potential utility as a random number generator.","Furthermore, we discuss both models in terms of what we call Floquet Fock state lattices."],"url":"http://arxiv.org/abs/2403.17866v1","category":"quant-ph"}
{"created":"2024-03-26 16:49:31","title":"Stealthy Deactivation of Safety Filters","abstract":"Safety filters ensure that only safe control actions are executed. We propose a simple and stealthy false-data injection attack for deactivating such safety filters; in particular, we focus on deactivating safety filters that are based on control-barrier functions. The attack injects false sensor measurements to bias state estimates to the interior of a safety region, which makes the safety filter accept unsafe control actions. To detect such attacks, we also propose a detector that detects biases manufactured by the proposed attack policy, which complements conventional detectors when safety filters are used. The proposed attack policy and detector are illustrated on a double integrator example.","sentences":["Safety filters ensure that only safe control actions are executed.","We propose a simple and stealthy false-data injection attack for deactivating such safety filters; in particular, we focus on deactivating safety filters that are based on control-barrier functions.","The attack injects false sensor measurements to bias state estimates to the interior of a safety region, which makes the safety filter accept unsafe control actions.","To detect such attacks, we also propose a detector that detects biases manufactured by the proposed attack policy, which complements conventional detectors when safety filters are used.","The proposed attack policy and detector are illustrated on a double integrator example."],"url":"http://arxiv.org/abs/2403.17861v1","category":"cs.SY"}
{"created":"2024-03-26 16:49:25","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications","abstract":"Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable.","sentences":["Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data.","Existing work has mainly focused on mitigation of such errors using either humans or an automated approach.","In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks.","We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure.","For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set.","We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy.","Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable."],"url":"http://arxiv.org/abs/2403.17860v1","category":"cs.CL"}
{"created":"2024-03-26 17:39:36","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians","abstract":"The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations. While demonstrating the potential for real-time rendering, 3D-GS encounters rendering bottlenecks in large scenes with complex details due to an excessive number of Gaussian primitives located within the viewing frustum. This limitation is particularly noticeable in zoom-out views and can lead to inconsistent rendering speeds in scenes with varying details. Moreover, it often struggles to capture the corresponding level of details at different scales with its heuristic density control operation. Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results. Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results.","sentences":["The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations.","While demonstrating the potential for real-time rendering, 3D-GS encounters rendering bottlenecks in large scenes with complex details due to an excessive number of Gaussian primitives located within the viewing frustum.","This limitation is particularly noticeable in zoom-out views and can lead to inconsistent rendering speeds in scenes with varying details.","Moreover, it often struggles to capture the corresponding level of details at different scales with its heuristic density control operation.","Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results.","Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results."],"url":"http://arxiv.org/abs/2403.17898v1","category":"cs.CV"}
{"created":"2024-03-26 17:34:41","title":"Dynamical evolution of the Uranian satellite system I. From the 5/3 Ariel-Umbriel mean motion resonance to the present","abstract":"Mutual gravitational interactions between the five major Uranian satellites raise small quasi-periodic fluctuations on their orbital elements. At the same time, tidal interactions between the satellites and the planet induce a slow outward drift of the orbits, while damping the eccentricities and the inclinations. In this paper, we revisit the current and near past evolution of this system using a N-body integrator, including spin evolution and tidal dissipation with the weak friction model. We update the secular eigenmodes of the system and show that it is unlikely that any of the main satellites were recently captured into a high obliquity Cassini state. We rather expect that the Uranian satellites are in a low obliquity Cassini state and compute their values. We also show that the current eccentricities of the satellites are not forced, and estimate the free eccentricities and inclinations. We constrain the quality factor of Uranus to be $Q_U = (8.6 \\pm 2.9)\\times10^3$, and that of the satellites to be $Q_S \\sim 500$. We find that the system most likely encountered the 5/3 mean motion resonance between Ariel and Umbriel in the past, at about ($0.7\\pm0.2$) Gyr ago. We additionally determine the eccentricities and inclinations of all satellites just after the resonance passage that comply with the current system. We finally show that, from the crossing of the 5/3 MMR to the present, the evolution of the system is mostly peaceful and dominated by tides raised on Uranus by the satellites.","sentences":["Mutual gravitational interactions between the five major Uranian satellites raise small quasi-periodic fluctuations on their orbital elements.","At the same time, tidal interactions between the satellites and the planet induce a slow outward drift of the orbits, while damping the eccentricities and the inclinations.","In this paper, we revisit the current and near past evolution of this system using a N-body integrator, including spin evolution and tidal dissipation with the weak friction model.","We update the secular eigenmodes of the system and show that it is unlikely that any of the main satellites were recently captured into a high obliquity Cassini state.","We rather expect that the Uranian satellites are in a low obliquity Cassini state and compute their values.","We also show that the current eccentricities of the satellites are not forced, and estimate the free eccentricities and inclinations.","We constrain the quality factor of Uranus to be $Q_U = (8.6 \\pm 2.9)\\times10^3$, and that of the satellites to be $Q_S \\sim 500$.","We find that the system most likely encountered the 5/3 mean motion resonance between Ariel and Umbriel in the past, at about ($0.7\\pm0.2$) Gyr ago.","We additionally determine the eccentricities and inclinations of all satellites just after the resonance passage that comply with the current system.","We finally show that, from the crossing of the 5/3 MMR to the present, the evolution of the system is mostly peaceful and dominated by tides raised on Uranus by the satellites."],"url":"http://arxiv.org/abs/2403.17896v1","category":"astro-ph.EP"}
{"created":"2024-03-26 17:20:04","title":"The Unreasonable Ineffectiveness of the Deeper Layers","abstract":"We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.","sentences":["We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed.","To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning.","In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU.","From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand.","From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge."],"url":"http://arxiv.org/abs/2403.17887v1","category":"cs.CL"}
{"created":"2024-03-26 17:19:23","title":"Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation","abstract":"As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient yet performant approach for multi-task EO modelling.","sentences":["As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources.","We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data.","We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility.","We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training).","We evaluate NEC on two EO tasks: scene classification and semantic segmentation.","Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data.","Even at 99.7% compression, performance drops by only 5% on the scene classification task.","Overall, NEC is a data-efficient yet performant approach for multi-task EO modelling."],"url":"http://arxiv.org/abs/2403.17886v1","category":"cs.LG"}
{"created":"2024-03-26 16:20:54","title":"Implementing photometric stereo for scanning helium microscopy (SHeM) to reconstruct true-to-size 3D surfaces","abstract":"Scanning Helium Microscopy (SHeM) offers a combination of spatial and angular resolution via a pinhole-collimated beam of thermal energy, neutral helium-4 atoms for non-destructive imaging. This thesis introduces a novel 3D imaging mode, \"heliometric stereo\", enabling true-to-size 3D surface reconstruction using an adapted photometric stereo algorithm. Stereolithography (SLA) 3D printed plastics are explored for SHeM pinhole plates due to limitations in traditional machining. FormLabs \"Clear Resin\" via SLA printing proves ideal for rapid prototyping of vacuum components, with a developed baking protocol ensuring vacuum compatibility. The study indicates re-wetting of such plastics is a surface process over weeks.   Developing 3D image reconstruction for both single and multi-detector setups required a real-space point tracking method. The point tracking method facilitates facet angle measurement in various materials, including technological and biological crystals. It has since become integral to SHeM imaging protocols for sample manipulator debugging.   The thesis also details a multi-detector SHeM instrument, referred to as B-SHeM. While primarily designed to perform heliometric stereo reconstructions, the instrument also enables the range of novel SHeM experiments such as mixed-species beams to investigate inelastic scattering.   The heliometric stereo methods implemented in the work have motivated the development of a GPU accelerated version of the in-house Monte-Carlo based ray tracing framework, which is the de-facto standard for SHeM image analysis. GPU parallelisation was explored as a method for decreasing simulation time and enabling previously inaccessible simulations involving complex scattering distributions and high resolution, realistic sample geometries. Preliminary testing on an analogous problem yielded a potential performance increase of up to 380 times.","sentences":["Scanning Helium Microscopy (SHeM) offers a combination of spatial and angular resolution via a pinhole-collimated beam of thermal energy, neutral helium-4 atoms for non-destructive imaging.","This thesis introduces a novel 3D imaging mode, \"heliometric stereo\", enabling true-to-size 3D surface reconstruction using an adapted photometric stereo algorithm.","Stereolithography (SLA) 3D printed plastics are explored for SHeM pinhole plates due to limitations in traditional machining.","FormLabs \"Clear Resin\" via SLA printing proves ideal for rapid prototyping of vacuum components, with a developed baking protocol ensuring vacuum compatibility.","The study indicates re-wetting of such plastics is a surface process over weeks.   ","Developing 3D image reconstruction for both single and multi-detector setups required a real-space point tracking method.","The point tracking method facilitates facet angle measurement in various materials, including technological and biological crystals.","It has since become integral to SHeM imaging protocols for sample manipulator debugging.   ","The thesis also details a multi-detector SHeM instrument, referred to as B-SHeM.","While primarily designed to perform heliometric stereo reconstructions, the instrument also enables the range of novel SHeM experiments such as mixed-species beams to investigate inelastic scattering.   ","The heliometric stereo methods implemented in the work have motivated the development of a GPU accelerated version of the in-house Monte-Carlo based ray tracing framework, which is the de-facto standard for SHeM image analysis.","GPU parallelisation was explored as a method for decreasing simulation time and enabling previously inaccessible simulations involving complex scattering distributions and high resolution, realistic sample geometries.","Preliminary testing on an analogous problem yielded a potential performance increase of up to 380 times."],"url":"http://arxiv.org/abs/2403.17835v1","category":"physics.app-ph"}
{"created":"2024-03-26 15:42:04","title":"Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving","abstract":"The automated generation of diverse and complex training scenarios has been an important ingredient in many complex learning tasks. Especially in real-world application domains, such as autonomous driving, auto-curriculum generation is considered vital for obtaining robust and general policies. However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered as a tedious and time-consuming task, especially in more complex simulation environments. In our work, we introduce MATS-Gym, a Multi-Agent Traffic Scenario framework to train agents in CARLA, a high-fidelity driving simulator. MATS-Gym is a multi-agent training framework for autonomous driving that uses partial scenario specifications to generate traffic scenarios with variable numbers of agents. This paper unifies various existing approaches to traffic scenario description into a single training framework and demonstrates how it can be integrated with techniques from unsupervised environment design to automate the generation of adaptive auto-curricula. The code is available at https://github.com/AutonomousDrivingExaminer/mats-gym.","sentences":["The automated generation of diverse and complex training scenarios has been an important ingredient in many complex learning tasks.","Especially in real-world application domains, such as autonomous driving, auto-curriculum generation is considered vital for obtaining robust and general policies.","However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered as a tedious and time-consuming task, especially in more complex simulation environments.","In our work, we introduce MATS-Gym, a Multi-Agent Traffic Scenario framework to train agents in CARLA, a high-fidelity driving simulator.","MATS-Gym is a multi-agent training framework for autonomous driving that uses partial scenario specifications to generate traffic scenarios with variable numbers of agents.","This paper unifies various existing approaches to traffic scenario description into a single training framework and demonstrates how it can be integrated with techniques from unsupervised environment design to automate the generation of adaptive auto-curricula.","The code is available at https://github.com/AutonomousDrivingExaminer/mats-gym."],"url":"http://arxiv.org/abs/2403.17805v1","category":"cs.RO"}
{"created":"2024-03-26 15:40:05","title":"Towards 3D Vision with Low-Cost Single-Photon Cameras","abstract":"We present a method for reconstructing 3D shape of arbitrary Lambertian objects based on measurements by miniature, energy-efficient, low-cost single-photon cameras. These cameras, operating as time resolved image sensors, illuminate the scene with a very fast pulse of diffuse light and record the shape of that pulse as it returns back from the scene at a high temporal resolution. We propose to model this image formation process, account for its non-idealities, and adapt neural rendering to reconstruct 3D geometry from a set of spatially distributed sensors with known poses. We show that our approach can successfully recover complex 3D shapes from simulated data. We further demonstrate 3D object reconstruction from real-world captures, utilizing measurements from a commodity proximity sensor. Our work draws a connection between image-based modeling and active range scanning and is a step towards 3D vision with single-photon cameras.","sentences":["We present a method for reconstructing 3D shape of arbitrary Lambertian objects based on measurements by miniature, energy-efficient, low-cost single-photon cameras.","These cameras, operating as time resolved image sensors, illuminate the scene with a very fast pulse of diffuse light and record the shape of that pulse as it returns back from the scene at a high temporal resolution.","We propose to model this image formation process, account for its non-idealities, and adapt neural rendering to reconstruct 3D geometry from a set of spatially distributed sensors with known poses.","We show that our approach can successfully recover complex 3D shapes from simulated data.","We further demonstrate 3D object reconstruction from real-world captures, utilizing measurements from a commodity proximity sensor.","Our work draws a connection between image-based modeling and active range scanning and is a step towards 3D vision with single-photon cameras."],"url":"http://arxiv.org/abs/2403.17801v1","category":"cs.CV"}
{"created":"2024-03-26 15:17:55","title":"Neural Distributed Controllers with Port-Hamiltonian Structures","abstract":"Controlling large-scale cyber-physical systems necessitates optimal distributed policies, relying solely on local real-time data and limited communication with neighboring agents. However, finding optimal controllers remains challenging, even in seemingly simple scenarios. Parameterizing these policies using Neural Networks (NNs) can deliver good performance, but their sensitivity to small input changes can destabilize the closed-loop system. This paper addresses this issue for a network of nonlinear dissipative systems. Specifically, we leverage well-established port-Hamiltonian structures to characterize deep distributed control policies with closed-loop stability guarantees and a finite $\\mathcal{L}_2$ gain, regardless of specific NN parameters. This eliminates the need to constrain the parameters during optimization and enables training with standard methods like stochastic gradient descent. A numerical study on the consensus control of Kuramoto oscillators demonstrates the effectiveness of the proposed controllers.","sentences":["Controlling large-scale cyber-physical systems necessitates optimal distributed policies, relying solely on local real-time data and limited communication with neighboring agents.","However, finding optimal controllers remains challenging, even in seemingly simple scenarios.","Parameterizing these policies using Neural Networks (NNs) can deliver good performance, but their sensitivity to small input changes can destabilize the closed-loop system.","This paper addresses this issue for a network of nonlinear dissipative systems.","Specifically, we leverage well-established port-Hamiltonian structures to characterize deep distributed control policies with closed-loop stability guarantees and a finite $\\mathcal{L}_2$ gain, regardless of specific NN parameters.","This eliminates the need to constrain the parameters during optimization and enables training with standard methods like stochastic gradient descent.","A numerical study on the consensus control of Kuramoto oscillators demonstrates the effectiveness of the proposed controllers."],"url":"http://arxiv.org/abs/2403.17785v1","category":"eess.SY"}
{"created":"2024-03-26 15:15:15","title":"GenesisTex: Adapting Image Denoising Diffusion to Texture Space","abstract":"We present GenesisTex, a novel method for synthesizing textures for 3D geometries from text descriptions. GenesisTex adapts the pretrained image diffusion model to texture space by texture space sampling. Specifically, we maintain a latent texture map for each viewpoint, which is updated with predicted noise on the rendering of the corresponding viewpoint. The sampled latent texture maps are then decoded into a final texture map. During the sampling process, we focus on both global and local consistency across multiple viewpoints: global consistency is achieved through the integration of style consistency mechanisms within the noise prediction network, and low-level consistency is achieved by dynamically aligning latent textures. Finally, we apply reference-based inpainting and img2img on denser views for texture refinement. Our approach overcomes the limitations of slow optimization in distillation-based methods and instability in inpainting-based methods. Experiments on meshes from various sources demonstrate that our method surpasses the baseline methods quantitatively and qualitatively.","sentences":["We present GenesisTex, a novel method for synthesizing textures for 3D geometries from text descriptions.","GenesisTex adapts the pretrained image diffusion model to texture space by texture space sampling.","Specifically, we maintain a latent texture map for each viewpoint, which is updated with predicted noise on the rendering of the corresponding viewpoint.","The sampled latent texture maps are then decoded into a final texture map.","During the sampling process, we focus on both global and local consistency across multiple viewpoints: global consistency is achieved through the integration of style consistency mechanisms within the noise prediction network, and low-level consistency is achieved by dynamically aligning latent textures.","Finally, we apply reference-based inpainting and img2img on denser views for texture refinement.","Our approach overcomes the limitations of slow optimization in distillation-based methods and instability in inpainting-based methods.","Experiments on meshes from various sources demonstrate that our method surpasses the baseline methods quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2403.17782v1","category":"cs.CV"}
{"created":"2024-03-26 15:09:55","title":"Deconvolution from two order statistics","abstract":"Economic data are often contaminated by measurement errors and truncated by ranking. This paper shows that the classical measurement error model with independent and additive measurement errors is identified nonparametrically using only two order statistics of repeated measurements. The identification result confirms a hypothesis by Athey and Haile (2002) for a symmetric ascending auction model with unobserved heterogeneity. Extensions allow for heterogeneous measurement errors, broadening the applicability to additional empirical settings, including asymmetric auctions and wage offer models. We adapt an existing simulated sieve estimator and illustrate its performance in finite samples.","sentences":["Economic data are often contaminated by measurement errors and truncated by ranking.","This paper shows that the classical measurement error model with independent and additive measurement errors is identified nonparametrically using only two order statistics of repeated measurements.","The identification result confirms a hypothesis by Athey and Haile (2002) for a symmetric ascending auction model with unobserved heterogeneity.","Extensions allow for heterogeneous measurement errors, broadening the applicability to additional empirical settings, including asymmetric auctions and wage offer models.","We adapt an existing simulated sieve estimator and illustrate its performance in finite samples."],"url":"http://arxiv.org/abs/2403.17777v1","category":"econ.EM"}
{"created":"2024-03-26 14:52:27","title":"Can patient-specific acquisition protocol improve performance on defect detection task in myocardial perfusion SPECT?","abstract":"Myocardial perfusion imaging using single-photon emission computed tomography (SPECT), or myocardial perfusion SPECT (MPS) is a widely used clinical imaging modality for the diagnosis of coronary artery disease. Current clinical protocols for acquiring and reconstructing MPS images are similar for most patients. However, for patients with outlier anatomical characteristics, such as large breasts, images acquired using conventional protocols are often sub-optimal in quality, leading to degraded diagnostic accuracy. Solutions to improve image quality for these patients outside of increased dose or total acquisition time remain challenging. Thus, there is an important need for new methodologies to improve image quality for such patients. One approach to improving this performance is adapting the image acquisition protocol specific to each patient. For this study, we first designed and implemented a personalized patient-specific protocol-optimization strategy, which we term precision SPECT (PRESPECT). This strategy integrates ideal observer theory with the constraints of tomographic reconstruction to optimize the acquisition time for each projection view, such that MPS defect detection performance is maximized. We performed a clinically realistic simulation study on patients with outlier anatomies on the task of detecting perfusion defects on various realizations of low-dose scans by an anthropomorphic channelized Hotelling observer. Our results show that using PRESPECT led to improved performance on the defect detection task for the considered patients. These results provide evidence that personalization of MPS acquisition protocol has the potential to improve defect detection performance, motivating further research to design optimal patient-specific acquisition and reconstruction protocols for MPS, as well as developing similar approaches for other medical imaging modalities.","sentences":["Myocardial perfusion imaging using single-photon emission computed tomography (SPECT), or myocardial perfusion SPECT (MPS) is a widely used clinical imaging modality for the diagnosis of coronary artery disease.","Current clinical protocols for acquiring and reconstructing MPS images are similar for most patients.","However, for patients with outlier anatomical characteristics, such as large breasts, images acquired using conventional protocols are often sub-optimal in quality, leading to degraded diagnostic accuracy.","Solutions to improve image quality for these patients outside of increased dose or total acquisition time remain challenging.","Thus, there is an important need for new methodologies to improve image quality for such patients.","One approach to improving this performance is adapting the image acquisition protocol specific to each patient.","For this study, we first designed and implemented a personalized patient-specific protocol-optimization strategy, which we term precision SPECT (PRESPECT).","This strategy integrates ideal observer theory with the constraints of tomographic reconstruction to optimize the acquisition time for each projection view, such that MPS defect detection performance is maximized.","We performed a clinically realistic simulation study on patients with outlier anatomies on the task of detecting perfusion defects on various realizations of low-dose scans by an anthropomorphic channelized Hotelling observer.","Our results show that using PRESPECT led to improved performance on the defect detection task for the considered patients.","These results provide evidence that personalization of MPS acquisition protocol has the potential to improve defect detection performance, motivating further research to design optimal patient-specific acquisition and reconstruction protocols for MPS, as well as developing similar approaches for other medical imaging modalities."],"url":"http://arxiv.org/abs/2403.17764v1","category":"physics.med-ph"}
{"created":"2024-03-26 14:18:43","title":"EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention","abstract":"To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler's formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.Furthermore, a phase contrastive learning task is proposed to improve the anisotropy of contextual representations in EulerFormer. Our theoretical framework possesses a high degree of completeness and generality. It is more robust to semantic variations and possesses moresuperior theoretical properties in principle. Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach.","sentences":["To capture user preference, transformer models have been widely applied to model sequential user behavior data.","The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence.","Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations.","In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference.","However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling.","To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference.","The EulerFormer involves two key technical improvements.","First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler's formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.","Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.","Furthermore, a phase contrastive learning task is proposed to improve the anisotropy of contextual representations in EulerFormer.","Our theoretical framework possesses a high degree of completeness and generality.","It is more robust to semantic variations and possesses moresuperior theoretical properties in principle.","Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach."],"url":"http://arxiv.org/abs/2403.17729v1","category":"cs.IR"}
{"created":"2024-03-26 14:17:01","title":"Masked Autoencoders are PDE Learners","abstract":"Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.","sentences":["Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability.","PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations.","As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs.","Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks.","In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations.","We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale."],"url":"http://arxiv.org/abs/2403.17728v1","category":"cs.LG"}
{"created":"2024-03-26 13:35:10","title":"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition","abstract":"We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the need for special tokens. We evaluate PlainMamba on a variety of visual recognition tasks including image classification, semantic segmentation, object detection, and instance segmentation. Our method achieves performance gains over previous non-hierarchical models and is competitive with hierarchical alternatives. For tasks requiring high-resolution inputs, in particular, PlainMamba requires much less computing while maintaining high performance. Code and models are available at https://github.com/ChenhongyiYang/PlainMamba","sentences":["We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition.","The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images.","In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information.","Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers.","The architecture is further simplified by removing the need for special tokens.","We evaluate PlainMamba on a variety of visual recognition tasks including image classification, semantic segmentation, object detection, and instance segmentation.","Our method achieves performance gains over previous non-hierarchical models and is competitive with hierarchical alternatives.","For tasks requiring high-resolution inputs, in particular, PlainMamba requires much less computing while maintaining high performance.","Code and models are available at https://github.com/ChenhongyiYang/PlainMamba"],"url":"http://arxiv.org/abs/2403.17695v1","category":"cs.CV"}
{"created":"2024-03-26 13:32:32","title":"Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes","abstract":"The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Sc\\`enes \\`a faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works. We propose a data-driven approach to identify the genericity of works created by GenAI, employing \"data-driven bias\" to assess the genericity of expressive compositions. This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset. The potential implications of measuring expressive genericity for copyright law are profound. Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals. More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI.","sentences":["The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains.","This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement.","To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models.","Copyright law distinguishes between original expressions and generic ones (Sc\\`enes \\`a faire), protecting the former and permitting reproduction of the latter.","However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works.","GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works.","We propose a data-driven approach to identify the genericity of works created by GenAI, employing \"data-driven bias\" to assess the genericity of expressive compositions.","This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset.","The potential implications of measuring expressive genericity for copyright law are profound.","Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals.","More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI."],"url":"http://arxiv.org/abs/2403.17691v1","category":"cs.CV"}
{"created":"2024-03-26 13:02:43","title":"How Private is DP-SGD?","abstract":"We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.","sentences":["We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ.","While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis.","On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available.","This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version.","Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD."],"url":"http://arxiv.org/abs/2403.17673v1","category":"cs.LG"}
{"created":"2024-03-26 12:27:32","title":"DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition","abstract":"End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to adapt swiftly to domain-specific entities for the NEC task. A series of experiments conducted on the AISHELL-1 and Homophone datasets confirm the effectiveness of our modeling approach. DANCER outperforms a strong baseline, the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate (CER) reduction of about 7% relatively on AISHELL-1 for named entities. More notably, when tested on Homophone that contain named entities of high phonetic confusion, DANCER offers a more pronounced CER reduction of 46% relatively over PED-NEC for named entities.","sentences":["End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks.","A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance.","However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially.","In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription.","To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to adapt swiftly to domain-specific entities for the NEC task.","A series of experiments conducted on the AISHELL-1 and Homophone datasets confirm the effectiveness of our modeling approach.","DANCER outperforms a strong baseline, the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate (CER) reduction of about 7% relatively on AISHELL-1 for named entities.","More notably, when tested on Homophone that contain named entities of high phonetic confusion, DANCER offers a more pronounced CER reduction of 46% relatively over PED-NEC for named entities."],"url":"http://arxiv.org/abs/2403.17645v1","category":"cs.CL"}
{"created":"2024-03-26 12:08:58","title":"Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems","abstract":"Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurations. This adaptive approach selectively masks input tokens, transforming the recommendation task into an inference challenge based on varying token subsets, thereby enhancing the agent's ability to infer across diverse trajectory lengths. Furthermore, we incorporate a multi-scale segmented retention mechanism that facilitates efficient modeling of long sequences, significantly enhancing computational efficiency. Our experimental analysis, conducted on both online simulator and offline datasets, clearly demonstrates the advantages of our proposed method.","sentences":["Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services.","Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework.","Recent advancements in offline RLRS provide a solution for how to address these two challenges.","However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs.","Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences.","In this study, we introduce a new offline RLRS method to deal with the above problems.","We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurations.","This adaptive approach selectively masks input tokens, transforming the recommendation task into an inference challenge based on varying token subsets, thereby enhancing the agent's ability to infer across diverse trajectory lengths.","Furthermore, we incorporate a multi-scale segmented retention mechanism that facilitates efficient modeling of long sequences, significantly enhancing computational efficiency.","Our experimental analysis, conducted on both online simulator and offline datasets, clearly demonstrates the advantages of our proposed method."],"url":"http://arxiv.org/abs/2403.17634v1","category":"cs.IR"}
{"created":"2024-03-26 12:08:14","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps","abstract":"In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.","sentences":["In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets.","We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations.","We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D).","UADA3D does not depend on pre-trained source models or teacher-student architectures.","Instead, it uses an adversarial approach to directly learn domain-invariant features.","We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains.","Our code is open-source and will be available soon."],"url":"http://arxiv.org/abs/2403.17633v1","category":"cs.CV"}
{"created":"2024-03-26 11:51:58","title":"Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System","abstract":"Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete.","sentences":["Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories.","However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data.","In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots.","Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm.","Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem.","Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system.","This enables us to refine our estimates of the tree traits if an area is revisited later during a mission.","We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests.","Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete."],"url":"http://arxiv.org/abs/2403.17622v1","category":"cs.RO"}
{"created":"2024-03-26 11:09:27","title":"Ultrafast Adaptive Primary Frequency Tuning and Secondary Frequency Identification for S/S WPT system","abstract":"Magnetic resonance wireless power transfer (WPT) technology is increasingly being adopted across diverse applications. However, its effectiveness can be significantly compromised by parameter shifts within the resonance network, owing to its high system quality factor. Such shifts are inherent and challenging to mitigate during the manufacturing process. In response, this article introduces a rapid frequency tuning approach. Leveraging switch-controlled capacitors (SCC) to adjust the resonance network and the primary side's operating frequency, alongside a current zero-crossing detection (ZCD) circuit for voltage-current phase determination, this method circumvents the need for intricate knowledge of WPT system parameters. Moreover, it obviates the necessity for inter-side communication for real-time identification of the secondary side resonance frequency. The swift response of SCC and two-step perturb-and-observe algorithm mitigate output disturbances, thereby expediting the frequency tuning process. Experimental validation on a 200W Series-Series compensated WPT (SS-WPT) system demonstrates that the proposed method achieves frequency recognition accuracy within 0.7kHz in less than 1ms, increasing system efficiency up to 9%.","sentences":["Magnetic resonance wireless power transfer (WPT) technology is increasingly being adopted across diverse applications.","However, its effectiveness can be significantly compromised by parameter shifts within the resonance network, owing to its high system quality factor.","Such shifts are inherent and challenging to mitigate during the manufacturing process.","In response, this article introduces a rapid frequency tuning approach.","Leveraging switch-controlled capacitors (SCC) to adjust the resonance network and the primary side's operating frequency, alongside a current zero-crossing detection (ZCD) circuit for voltage-current phase determination, this method circumvents the need for intricate knowledge of WPT system parameters.","Moreover, it obviates the necessity for inter-side communication for real-time identification of the secondary side resonance frequency.","The swift response of SCC and two-step perturb-and-observe algorithm mitigate output disturbances, thereby expediting the frequency tuning process.","Experimental validation on a 200W Series-Series compensated WPT (SS-WPT) system demonstrates that the proposed method achieves frequency recognition accuracy within 0.7kHz in less than 1ms, increasing system efficiency up to 9%."],"url":"http://arxiv.org/abs/2403.17598v1","category":"eess.SY"}
{"created":"2024-03-26 10:54:07","title":"Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models","abstract":"With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the training set. This novel capability enhances model performance in the few-shot setting and enables model usability in the absence of training data. The two memory networks employ the same flexible memory interactive strategy, which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers. Our approach is tested across 11 datasets under the three task settings. Remarkably, in the zero-shot scenario, it outperforms existing methods by over 3\\% and even shows superior results against methods utilizing external training data. Additionally, our method exhibits robust performance against natural distribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.","sentences":["With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research.","The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation.","Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms.","In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings.","Specifically, we propose the dual memory networks that comprise dynamic and static memory components.","The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the training set.","This novel capability enhances model performance in the few-shot setting and enables model usability in the absence of training data.","The two memory networks employ the same flexible memory interactive strategy, which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers.","Our approach is tested across 11 datasets under the three task settings.","Remarkably, in the zero-shot scenario, it outperforms existing methods by over 3\\% and even shows superior results against methods utilizing external training data.","Additionally, our method exhibits robust performance against natural distribution shifts.","Codes are available at \\url{https://github.com/YBZh/DMN}."],"url":"http://arxiv.org/abs/2403.17589v1","category":"cs.CV"}
{"created":"2024-03-26 10:45:11","title":"Towards a Zero-Data, Controllable, Adaptive Dialog System","abstract":"Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms. Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data.","sentences":["Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree.","The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users.","However, the need for additional training data hinders deployment in new domains.","To address this, we explore approaches to generate this data directly from dialog trees.","We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU.","We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms.","Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data."],"url":"http://arxiv.org/abs/2403.17582v1","category":"cs.CL"}
{"created":"2024-03-26 10:37:54","title":"Flat band fine-tuning and its photonic applications","abstract":"Flat bands - single-particle energy bands - in tight-binding networks have attracted attention due to the presence of macroscopic degeneracies and their extreme sensitivity to perturbations. This makes them natural candidates for emerging exotic phases and unconventional orders. The challenging part however is to construct flat band networks, whose existence relies on symmetries and fine-tuning. In this review we consider the recently proposed systematic ways to construct flat band networks based on symmetries or fine-tuning. We then discuss how the fine-tuning constructions can be further extended, adapted or exploited in presence of perturbations, both single-particle and many-body. This strategy has lead to the discovery of non-perturbative metal-insulator transitions, fractal phases, nonlinear and quantum caging and many-body nonergodic quantum models. We discuss what implications these results may have for the design of fine-tuned nanophotonic systems including photonic crystals, nanocavities, and metasurfaces.","sentences":["Flat bands - single-particle energy bands - in tight-binding networks have attracted attention due to the presence of macroscopic degeneracies and their extreme sensitivity to perturbations.","This makes them natural candidates for emerging exotic phases and unconventional orders.","The challenging part however is to construct flat band networks, whose existence relies on symmetries and fine-tuning.","In this review we consider the recently proposed systematic ways to construct flat band networks based on symmetries or fine-tuning.","We then discuss how the fine-tuning constructions can be further extended, adapted or exploited in presence of perturbations, both single-particle and many-body.","This strategy has lead to the discovery of non-perturbative metal-insulator transitions, fractal phases, nonlinear and quantum caging and many-body nonergodic quantum models.","We discuss what implications these results may have for the design of fine-tuned nanophotonic systems including photonic crystals, nanocavities, and metasurfaces."],"url":"http://arxiv.org/abs/2403.17578v1","category":"physics.optics"}
{"created":"2024-03-26 10:36:08","title":"Channel-Adaptive Pilot Design for FDD-MIMO Systems Utilizing Gaussian Mixture Models","abstract":"In this work, we propose to utilize Gaussian mixture models (GMMs) to design pilots for downlink (DL) channel estimation in frequency division duplex (FDD) systems. The GMM captures prior information during training that is leveraged to design a codebook of pilot matrices in an initial offline phase. Once shared with the mobile terminal (MT), the GMM is utilized to determine a feedback index at the MT in the online phase. This index selects a pilot matrix from a codebook, eliminating the need for online pilot optimization. The GMM is further used for DL channel estimation at the MT via observation-dependent linear minimum mean square error (LMMSE) filters, parametrized by the GMM. The analytic representation of the GMM allows adaptation to any signal-to-noise ratio (SNR) level and pilot configuration without re-training. With extensive simulations, we demonstrate the superior performance of the proposed GMM-based pilot scheme compared to state-of-the-art approaches.","sentences":["In this work, we propose to utilize Gaussian mixture models (GMMs) to design pilots for downlink (DL) channel estimation in frequency division duplex (FDD) systems.","The GMM captures prior information during training that is leveraged to design a codebook of pilot matrices in an initial offline phase.","Once shared with the mobile terminal (MT), the GMM is utilized to determine a feedback index at the MT in the online phase.","This index selects a pilot matrix from a codebook, eliminating the need for online pilot optimization.","The GMM is further used for DL channel estimation at the MT via observation-dependent linear minimum mean square error (LMMSE) filters, parametrized by the GMM.","The analytic representation of the GMM allows adaptation to any signal-to-noise ratio (SNR) level and pilot configuration without re-training.","With extensive simulations, we demonstrate the superior performance of the proposed GMM-based pilot scheme compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.17577v1","category":"eess.SP"}
{"created":"2024-03-26 10:10:56","title":"Deep functional multiple index models with an application to SER","abstract":"Speech Emotion Recognition (SER) plays a crucial role in advancing human-computer interaction and speech processing capabilities. We introduce a novel deep-learning architecture designed specifically for the functional data model known as the multiple-index functional model. Our key innovation lies in integrating adaptive basis layers and an automated data transformation search within the deep learning framework. Simulations for this new model show good performances. This allows us to extract features tailored for chunk-level SER, based on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the effectiveness of our approach on the benchmark IEMOCAP database, achieving good performance compared to existing methods.","sentences":["Speech Emotion Recognition (SER) plays a crucial role in advancing human-computer interaction and speech processing capabilities.","We introduce a novel deep-learning architecture designed specifically for the functional data model known as the multiple-index functional model.","Our key innovation lies in integrating adaptive basis layers and an automated data transformation search within the deep learning framework.","Simulations for this new model show good performances.","This allows us to extract features tailored for chunk-level SER, based on Mel Frequency Cepstral Coefficients (MFCCs).","We demonstrate the effectiveness of our approach on the benchmark IEMOCAP database, achieving good performance compared to existing methods."],"url":"http://arxiv.org/abs/2403.17562v1","category":"cs.SD"}
{"created":"2024-03-26 09:39:21","title":"BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat","abstract":"Creating new air combat tactics and discovering novel maneuvers can require numerous hours of expert pilots' time. Additionally, for each different combat scenario, the same strategies may not work since small changes in equipment performance may drastically change the air combat outcome. For this reason, we created a reinforcement learning environment to help investigate potential air combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR Gym. This type of air combat is important since long-range missiles are often the first weapon to be used in aerial combat. Some existing environments provide high-fidelity simulations but are either not open source or are not adapted to the BVR air combat domain. Other environments are open source but use less accurate simulation models. Our work provides a high-fidelity environment based on the open-source flight dynamics simulator JSBSim and is adapted to the BVR air combat domain. This article describes the building blocks of the environment and some use cases.","sentences":["Creating new air combat tactics and discovering novel maneuvers can require numerous hours of expert pilots' time.","Additionally, for each different combat scenario, the same strategies may not work since small changes in equipment performance may drastically change the air combat outcome.","For this reason, we created a reinforcement learning environment to help investigate potential air combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR Gym.","This type of air combat is important since long-range missiles are often the first weapon to be used in aerial combat.","Some existing environments provide high-fidelity simulations but are either not open source or are not adapted to the BVR air combat domain.","Other environments are open source but use less accurate simulation models.","Our work provides a high-fidelity environment based on the open-source flight dynamics simulator JSBSim and is adapted to the BVR air combat domain.","This article describes the building blocks of the environment and some use cases."],"url":"http://arxiv.org/abs/2403.17533v1","category":"cs.LG"}
{"created":"2024-03-26 09:31:55","title":"Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications","abstract":"Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods. However, the potential benefits from the recent ``exponential'' growth of language models with billions of parameters have not yet been fully explored. In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model. By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model's size to 5.7 billion parameters. We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach. Furthermore, we also have confirmed a positive correlation between the size of the model and its performance. It was particularly noteworthy that languages with fewer resources or those with less linguistic similarity to English benefited more from the parameter increase. Our model is available at https://huggingface.co/pkshatech/m-ST5.","sentences":["Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods.","However, the potential benefits from the recent ``exponential'' growth of language models with billions of parameters have not yet been fully explored.","In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model.","By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model's size to 5.7 billion parameters.","We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach.","Furthermore, we also have confirmed a positive correlation between the size of the model and its performance.","It was particularly noteworthy that languages with fewer resources or those with less linguistic similarity to English benefited more from the parameter increase.","Our model is available at https://huggingface.co/pkshatech/m-ST5."],"url":"http://arxiv.org/abs/2403.17528v1","category":"cs.CL"}
{"created":"2024-03-26 09:25:25","title":"Simplified functional flow equation","abstract":"We propose to adapt the precise definition of the flowing effective action in order to obtain a functional flow equation with simple properties close to physical intuition. The simplified flow equation is invariant under local gauge transformations and suitable for both euclidean and Minkowski signature and analytic continuation. The cutoff always removes fluctuations close to zeros of the inverse full propagator. A formulation of the simplified flow equation in terms of renormalized scale invariant fields permits direct access to scaling solutions and associated fixed points. Corrections to the simplified flow equation involve a field-dependent modification of the cutoff for which we discuss a systematic expansion. Truncated solutions for a scalar field theory in four dimensions suggest a new fixed point with a field-dependent coefficient of the kinetic term.","sentences":["We propose to adapt the precise definition of the flowing effective action in order to obtain a functional flow equation with simple properties close to physical intuition.","The simplified flow equation is invariant under local gauge transformations and suitable for both euclidean and Minkowski signature and analytic continuation.","The cutoff always removes fluctuations close to zeros of the inverse full propagator.","A formulation of the simplified flow equation in terms of renormalized scale invariant fields permits direct access to scaling solutions and associated fixed points.","Corrections to the simplified flow equation involve a field-dependent modification of the cutoff for which we discuss a systematic expansion.","Truncated solutions for a scalar field theory in four dimensions suggest a new fixed point with a field-dependent coefficient of the kinetic term."],"url":"http://arxiv.org/abs/2403.17523v1","category":"hep-th"}
{"created":"2024-03-26 09:20:46","title":"Active drive towards elastic spinodals","abstract":"Active renewable matter, a distinctive feature of adaptive living materials, can exhibit highly unusual mechanical responses by actively navigating the space of material parameters. In particular, it can self-drive towards elastic spinodals where the presence of inhomogeneous floppy modes, makes the matter elastically degenerate. The main effect of the implied marginality is stress localization leading to the emergence of force chains which can be actively assembled and disassembled. In this Letter we formalize the concept of spinodal states for general elastic solids and show how such extreme mechanical regimes can be actively navigated.","sentences":["Active renewable matter, a distinctive feature of adaptive living materials, can exhibit highly unusual mechanical responses by actively navigating the space of material parameters.","In particular, it can self-drive towards elastic spinodals where the presence of inhomogeneous floppy modes, makes the matter elastically degenerate.","The main effect of the implied marginality is stress localization leading to the emergence of force chains which can be actively assembled and disassembled.","In this Letter we formalize the concept of spinodal states for general elastic solids and show how such extreme mechanical regimes can be actively navigated."],"url":"http://arxiv.org/abs/2403.17517v1","category":"cond-mat.soft"}
{"created":"2024-03-26 09:14:25","title":"A unified framework for coarse grained molecular dynamics of proteins","abstract":"Understanding protein dynamics is crucial for elucidating their biological functions. While all-atom molecular dynamics (MD) simulations provide detailed information, coarse-grained (CG) MD simulations capture the essential collective motions of proteins at significantly lower computational cost. In this article, we present a unified framework for coarse-grained molecular dynamics simulation of proteins. Our approach utilizes a tree-structured representation of collective variables, enabling reconstruction of protein Cartesian coordinates with high fidelity. The force field is constructed using a deep neural network trained on trajectories generated from conventional all-atom MD simulations. We demonstrate the framework's effectiveness using the 168-amino protein target T1027 from CASP14. Statistical distributions of the collective variables and time series of root mean square deviation (RMSD) obtained from our coarse-grained simulations closely resemble those from all-atom MD simulations. This method is not only useful for studying the movements of complex proteins, but also has the potential to be adapted for simulating other biomolecules like DNA, RNA, and even electrolytes in batteries.","sentences":["Understanding protein dynamics is crucial for elucidating their biological functions.","While all-atom molecular dynamics (MD) simulations provide detailed information, coarse-grained (CG) MD simulations capture the essential collective motions of proteins at significantly lower computational cost.","In this article, we present a unified framework for coarse-grained molecular dynamics simulation of proteins.","Our approach utilizes a tree-structured representation of collective variables, enabling reconstruction of protein Cartesian coordinates with high fidelity.","The force field is constructed using a deep neural network trained on trajectories generated from conventional all-atom MD simulations.","We demonstrate the framework's effectiveness using the 168-amino protein target T1027 from CASP14.","Statistical distributions of the collective variables and time series of root mean square deviation (RMSD) obtained from our coarse-grained simulations closely resemble those from all-atom MD simulations.","This method is not only useful for studying the movements of complex proteins, but also has the potential to be adapted for simulating other biomolecules like DNA, RNA, and even electrolytes in batteries."],"url":"http://arxiv.org/abs/2403.17513v1","category":"physics.chem-ph"}
{"created":"2024-03-26 09:05:16","title":"Computing conservative probabilities of rare events with surrogates","abstract":"This article provides a critical review of the main methods used to produce conservative estimators of probabilities of rare events, or critical failures, for reliability and certification studies in the broadest sense. These probabilities must theoretically be calculated from simulations of (certified) numerical models, but which typically suffer from prohibitive computational costs. This occurs frequently, for instance, for complex and critical industrial systems. We focus therefore in adapting the common use of surrogates to replace these numerical models, the aim being to offer a high level of confidence in the results. We suggest avenues of research to improve the guarantees currently reachable.","sentences":["This article provides a critical review of the main methods used to produce conservative estimators of probabilities of rare events, or critical failures, for reliability and certification studies in the broadest sense.","These probabilities must theoretically be calculated from simulations of (certified) numerical models, but which typically suffer from prohibitive computational costs.","This occurs frequently, for instance, for complex and critical industrial systems.","We focus therefore in adapting the common use of surrogates to replace these numerical models, the aim being to offer a high level of confidence in the results.","We suggest avenues of research to improve the guarantees currently reachable."],"url":"http://arxiv.org/abs/2403.17505v1","category":"math.ST"}
{"created":"2024-03-26 08:59:36","title":"Time-dependent nuclear energy-density functional theory toolkit for neutron star crust: Dynamics of a nucleus in a neutron superfluid","abstract":"We present a new numerical tool designed to probe the dense layers of neutron star crusts. It is based on the Time-Dependent Hartree-Fock-Bogoliubov theory with generalized Skyrme nuclear energy density functionals, such as the Brussels-Montreal ones. We use it to study the time evolution of a nucleus accelerating through superfluid neutron medium in the inner crust of a neutron star. We extract an effective mass in the low velocity limit. We observe a threshold velocity and specify mechanisms of dissipation: phonon emission, Cooper pairs breaking, and vortex rings creation. The microscopic effects we study have impact on neutron star. Moreover, the mechanisms, we described, are general and apply also to other fermionic superfluid mixtures like liquid helium, or ultracold gases.","sentences":["We present a new numerical tool designed to probe the dense layers of neutron star crusts.","It is based on the Time-Dependent Hartree-Fock-Bogoliubov theory with generalized Skyrme nuclear energy density functionals, such as the Brussels-Montreal ones.","We use it to study the time evolution of a nucleus accelerating through superfluid neutron medium in the inner crust of a neutron star.","We extract an effective mass in the low velocity limit.","We observe a threshold velocity and specify mechanisms of dissipation: phonon emission, Cooper pairs breaking, and vortex rings creation.","The microscopic effects we study have impact on neutron star.","Moreover, the mechanisms, we described, are general and apply also to other fermionic superfluid mixtures like liquid helium, or ultracold gases."],"url":"http://arxiv.org/abs/2403.17499v1","category":"nucl-th"}
{"created":"2024-03-26 08:58:28","title":"Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies","abstract":"In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other). In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations. The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players' assumed efforts during the interaction. We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions. And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly. However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement -- which invites further research in the direction of cost-sharing in collaborative interactions.","sentences":["In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other).","In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations.","The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players' assumed efforts during the interaction.","We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions.","And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly.","However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement -- which invites further research in the direction of cost-sharing in collaborative interactions."],"url":"http://arxiv.org/abs/2403.17497v1","category":"cs.CL"}
{"created":"2024-03-26 08:33:44","title":"Adaptive Bayesian Structure Learning of DAGs With Non-conjugate Prior","abstract":"Directed Acyclic Graphs (DAGs) are solid structures used to describe and infer the dependencies among variables in multivariate scenarios. Having a thorough comprehension of the accurate DAG-generating model is crucial for causal discovery and estimation. Our work suggests utilizing a non-conjugate prior for Gaussian DAG structure learning to enhance the posterior probability. We employ the idea of using the Bessel function to address the computational burden, providing faster MCMC computation compared to the use of conjugate priors. In addition, our proposal exhibits a greater rate of adaptation when compared to the conjugate prior, specifically for the inclusion of nodes in the DAG-generating model. Simulation studies demonstrate the superior accuracy of DAG learning, and we obtain the same maximum a posteriori and median probability model estimate for the AML data, using the non-conjugate prior.","sentences":["Directed Acyclic Graphs (DAGs) are solid structures used to describe and infer the dependencies among variables in multivariate scenarios.","Having a thorough comprehension of the accurate DAG-generating model is crucial for causal discovery and estimation.","Our work suggests utilizing a non-conjugate prior for Gaussian DAG structure learning to enhance the posterior probability.","We employ the idea of using the Bessel function to address the computational burden, providing faster MCMC computation compared to the use of conjugate priors.","In addition, our proposal exhibits a greater rate of adaptation when compared to the conjugate prior, specifically for the inclusion of nodes in the DAG-generating model.","Simulation studies demonstrate the superior accuracy of DAG learning, and we obtain the same maximum a posteriori and median probability model estimate for the AML data, using the non-conjugate prior."],"url":"http://arxiv.org/abs/2403.17489v1","category":"stat.ME"}
{"created":"2024-03-26 08:32:39","title":"KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning","abstract":"Previous work on multimodal sentence embedding has proposed multimodal contrastive learning and achieved promising results. However, by taking the rest of the batch as negative samples without reviewing when forming contrastive pairs, those studies encountered many suspicious and noisy negative examples, significantly affecting the methods' overall performance. In this work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning of Sentence Embeddings), a novel approach that enhances the discrimination and generalizability of multimodal representation and inherits the knowledge from the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the contrastive objective. Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin Supervised Contrastive Learning for Multimodal sentence embeddings), that enhances the discriminative representation by strengthening the margin within the angular space while capturing varying semantics within the negative. Experimental results on widely used Semantic Textual Similarity (STS) benchmarks demonstrate the effectiveness of our approach.","sentences":["Previous work on multimodal sentence embedding has proposed multimodal contrastive learning and achieved promising results.","However, by taking the rest of the batch as negative samples without reviewing when forming contrastive pairs, those studies encountered many suspicious and noisy negative examples, significantly affecting the methods' overall performance.","In this work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning of Sentence Embeddings), a novel approach that enhances the discrimination and generalizability of multimodal representation and inherits the knowledge from the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the contrastive objective.","Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin Supervised Contrastive Learning for Multimodal sentence embeddings), that enhances the discriminative representation by strengthening the margin within the angular space while capturing varying semantics within the negative.","Experimental results on widely used Semantic Textual Similarity (STS) benchmarks demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.17486v1","category":"cs.CL"}
{"created":"2024-03-26 07:57:04","title":"Geometric planted matchings beyond the Gaussian model","abstract":"We consider the problem of recovering an unknown matching between a set of $n$ randomly placed points in $\\mathbb{R}^d$ and random perturbations of these points. This can be seen as a model for particle tracking and more generally, entity resolution. We use matchings in random geometric graphs to derive minimax lower bounds for this problem that hold under great generality. Using these results we show that for a broad class of distributions, the order of the number of mistakes made by an estimator that minimizes the sum of squared Euclidean distances is minimax optimal when $d$ is fixed and is optimal up to $n^{o(1)}$ factors when $d = o(\\log n)$. In the high-dimensional regime we consider a setup where both initial positions and perturbations have independent sub-Gaussian coordinates. In this setup we give sufficient conditions under which the same estimator makes no mistakes with high probability. We prove an analogous result for an adapted version of this estimator that incorporates information on the covariance matrix of the perturbations.","sentences":["We consider the problem of recovering an unknown matching between a set of $n$ randomly placed points in $\\mathbb{R}^d$ and random perturbations of these points.","This can be seen as a model for particle tracking and more generally, entity resolution.","We use matchings in random geometric graphs to derive minimax lower bounds for this problem that hold under great generality.","Using these results we show that for a broad class of distributions, the order of the number of mistakes made by an estimator that minimizes the sum of squared Euclidean distances is minimax optimal when $d$ is fixed and is optimal up to $n^{o(1)}$ factors when $d = o(\\log n)$. In the high-dimensional regime we consider a setup where both initial positions and perturbations have independent sub-Gaussian coordinates.","In this setup we give sufficient conditions under which the same estimator makes no mistakes with high probability.","We prove an analogous result for an adapted version of this estimator that incorporates information on the covariance matrix of the perturbations."],"url":"http://arxiv.org/abs/2403.17469v1","category":"math.ST"}
{"created":"2024-03-26 07:26:27","title":"Adaptive Line-Of-Sight guidance law based on vector fields path following for underactuated unmanned surface vehicle","abstract":"The focus of this paper is to develop a methodology that enables an unmanned surface vehicle (USV) to efficiently track a planned path. The introduction of a vector field-based adaptive line-of-sight guidance law (VFALOS) for accurate trajectory tracking and minimizing the overshoot response time during USV tracking of curved paths improves the overall line-of-sight (LOS) guidance method. These improvements contribute to faster convergence to the desired path, reduce oscillations, and can mitigate the effects of persistent external disturbances. It is shown that the proposed guidance law exhibits k-exponential stability when converging to the desired path consisting of straight and curved lines. The results in the paper show that the proposed method effectively improves the accuracy of the USV tracking the desired path while ensuring the safety of the USV work.","sentences":["The focus of this paper is to develop a methodology that enables an unmanned surface vehicle (USV) to efficiently track a planned path.","The introduction of a vector field-based adaptive line-of-sight guidance law (VFALOS) for accurate trajectory tracking and minimizing the overshoot response time during USV tracking of curved paths improves the overall line-of-sight (LOS) guidance method.","These improvements contribute to faster convergence to the desired path, reduce oscillations, and can mitigate the effects of persistent external disturbances.","It is shown that the proposed guidance law exhibits k-exponential stability when converging to the desired path consisting of straight and curved lines.","The results in the paper show that the proposed method effectively improves the accuracy of the USV tracking the desired path while ensuring the safety of the USV work."],"url":"http://arxiv.org/abs/2403.17448v1","category":"cs.RO"}
{"created":"2024-03-26 07:19:06","title":"Adaptive LiDAR-Radar Fusion for Outdoor Odometry Across Dense Smoke Conditions","abstract":"Robust odometry estimation in perceptually degraded environments represents a key challenge in the field of robotics. In this paper, we propose a LiDAR-radar fusion method for robust odometry for adverse environment with LiDAR degeneracy. By comparing the LiDAR point cloud with the radar static point cloud obtained through preprocessing module, it is possible to identify instances of LiDAR degeneracy to overcome perceptual limits. We demonstrate the effectiveness of our method in challenging conditions such as dense smoke, showcasing its ability to reliably estimate odometry and identify/remove dynamic points prone to LiDAR degeneracy.","sentences":["Robust odometry estimation in perceptually degraded environments represents a key challenge in the field of robotics.","In this paper, we propose a LiDAR-radar fusion method for robust odometry for adverse environment with LiDAR degeneracy.","By comparing the LiDAR point cloud with the radar static point cloud obtained through preprocessing module, it is possible to identify instances of LiDAR degeneracy to overcome perceptual limits.","We demonstrate the effectiveness of our method in challenging conditions such as dense smoke, showcasing its ability to reliably estimate odometry and identify/remove dynamic points prone to LiDAR degeneracy."],"url":"http://arxiv.org/abs/2403.17441v1","category":"cs.RO"}
{"created":"2024-03-26 07:05:06","title":"Particle identification with machine learning from incomplete data in the ALICE experiment","abstract":"The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.","sentences":["The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions.","Such studies require accurate particle identification (PID).","ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts.","Acmuch better performance can be achieved with machine learning (ML) methods.","Our solution uses multiple neural networks (NN) serving as binary classifiers.","Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples.","We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data."],"url":"http://arxiv.org/abs/2403.17436v1","category":"hep-ex"}
{"created":"2024-03-26 06:40:03","title":"Test-time Adaptation Meets Image Enhancement: Improving Accuracy via Uncertainty-aware Logit Switching","abstract":"Deep neural networks have achieved remarkable success in a variety of computer vision applications. However, there is a problem of degrading accuracy when the data distribution shifts between training and testing. As a solution of this problem, Test-time Adaptation~(TTA) has been well studied because of its practicality. Although TTA methods increase accuracy under distribution shift by updating the model at test time, using high-uncertainty predictions is known to degrade accuracy. Since the input image is the root of the distribution shift, we incorporate a new perspective on enhancing the input image into TTA methods to reduce the prediction's uncertainty. We hypothesize that enhancing the input image reduces prediction's uncertainty and increase the accuracy of TTA methods. On the basis of our hypothesis, we propose a novel method: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the classification model is combined with the image enhancement model that transforms input images into recognition-friendly ones, and these models are updated by existing TTA methods. Furthermore, we found that the prediction from the enhanced image does not always have lower uncertainty than the prediction from the original image. Thus, we propose logit switching, which compares the uncertainty measure of these predictions and outputs the lower one. In our experiments, we evaluate TECA with various TTA methods and show that TECA reduces prediction's uncertainty and increases accuracy of TTA methods despite having no hyperparameters and little parameter overhead.","sentences":["Deep neural networks have achieved remarkable success in a variety of computer vision applications.","However, there is a problem of degrading accuracy when the data distribution shifts between training and testing.","As a solution of this problem, Test-time Adaptation~(TTA) has been well studied because of its practicality.","Although TTA methods increase accuracy under distribution shift by updating the model at test time, using high-uncertainty predictions is known to degrade accuracy.","Since the input image is the root of the distribution shift, we incorporate a new perspective on enhancing the input image into TTA methods to reduce the prediction's uncertainty.","We hypothesize that enhancing the input image reduces prediction's uncertainty and increase the accuracy of TTA methods.","On the basis of our hypothesis, we propose a novel method: Test-time Enhancer and Classifier Adaptation~(TECA).","In TECA, the classification model is combined with the image enhancement model that transforms input images into recognition-friendly ones, and these models are updated by existing TTA methods.","Furthermore, we found that the prediction from the enhanced image does not always have lower uncertainty than the prediction from the original image.","Thus, we propose logit switching, which compares the uncertainty measure of these predictions and outputs the lower one.","In our experiments, we evaluate TECA with various TTA methods and show that TECA reduces prediction's uncertainty and increases accuracy of TTA methods despite having no hyperparameters and little parameter overhead."],"url":"http://arxiv.org/abs/2403.17423v1","category":"cs.CV"}
{"created":"2024-03-26 06:14:19","title":"AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations","abstract":"Collaborative filtering methods based on graph neural networks (GNNs) have witnessed significant success in recommender systems (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via message-passing mechanisms. However, these GNN-based RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized recommendations. While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of GNN representations and subsequent recommendation performance. Up to now, the over-correlation issue remains unexplored in RS. Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge. To this end, this paper aims to address the aforementioned gap by undertaking a comprehensive study of the over-correlation issue in graph collaborative filtering models. Firstly, we present empirical evidence to demonstrate the widespread prevalence of over-correlation in these models. Subsequently, we dive into a theoretical analysis which establishes a pivotal connection between the over-correlation and over-smoothing issues. Leveraging these insights, we introduce the Adaptive Feature De-correlation Graph Collaborative Filtering (AFDGCF) framework, which dynamically applies correlation penalties to the feature dimensions of the representation matrix, effectively alleviating both over-correlation and over-smoothing issues. The efficacy of the proposed framework is corroborated through extensive experiments conducted with four representative graph collaborative filtering models across four publicly available datasets.","sentences":["Collaborative filtering methods based on graph neural networks (GNNs) have witnessed significant success in recommender systems (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via message-passing mechanisms.","However, these GNN-based RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized recommendations.","While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of GNN representations and subsequent recommendation performance.","Up to now, the over-correlation issue remains unexplored in RS.","Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge.","To this end, this paper aims to address the aforementioned gap by undertaking a comprehensive study of the over-correlation issue in graph collaborative filtering models.","Firstly, we present empirical evidence to demonstrate the widespread prevalence of over-correlation in these models.","Subsequently, we dive into a theoretical analysis which establishes a pivotal connection between the over-correlation and over-smoothing issues.","Leveraging these insights, we introduce the Adaptive Feature De-correlation Graph Collaborative Filtering (AFDGCF) framework, which dynamically applies correlation penalties to the feature dimensions of the representation matrix, effectively alleviating both over-correlation and over-smoothing issues.","The efficacy of the proposed framework is corroborated through extensive experiments conducted with four representative graph collaborative filtering models across four publicly available datasets."],"url":"http://arxiv.org/abs/2403.17416v1","category":"cs.IR"}
{"created":"2024-03-26 05:46:10","title":"Vortex nucleations in spinor Bose condensates under localized synthetic magnetic fields","abstract":"Gauge fields are ubiquitous in modern quantum physics. In superfluids, quantized vortices can be induced by gauge fields. Here we demonstrate the first experimental observation of vortex nucleations in spinor Bose-Einstein Condensates under radially-localized synthetic magnetic fields. The associated gauge potentials $\\vec{A}$ are azimuthal and created by light-induced spin-orbital-angular-momentum coupling, generating circulating azimuthal velocity fields $\\propto \\vec{p}-\\vec{A}$ even when the canonical momentum $\\vec{p}= 0$. A sufficiently large azimuthal velocity peaked near the condensate center results in a dynamically unstable localized excitation that initiates vortex nucleations. This excitation appears as a spontaneously-formed vortex-antivortex pair near the cloud center. Following the initially developed instability, the dynamics is governed by the asymmetry and dissipation, where the atomic orbital angular momentum evolves and can reach the value of the ground state. Our system exhibits dynamical and Landau instabilities and agrees reasonably with time-dependent Gross-Pitaevskii simulations.","sentences":["Gauge fields are ubiquitous in modern quantum physics.","In superfluids, quantized vortices can be induced by gauge fields.","Here we demonstrate the first experimental observation of vortex nucleations in spinor Bose-Einstein Condensates under radially-localized synthetic magnetic fields.","The associated gauge potentials $\\vec{A}$ are azimuthal and created by light-induced spin-orbital-angular-momentum coupling, generating circulating azimuthal velocity fields $\\propto \\vec{p}-\\vec{A}$ even when the canonical momentum $\\vec{p}= 0$.","A sufficiently large azimuthal velocity peaked near the condensate center results in a dynamically unstable localized excitation that initiates vortex nucleations.","This excitation appears as a spontaneously-formed vortex-antivortex pair near the cloud center.","Following the initially developed instability, the dynamics is governed by the asymmetry and dissipation, where the atomic orbital angular momentum evolves and can reach the value of the ground state.","Our system exhibits dynamical and Landau instabilities and agrees reasonably with time-dependent Gross-Pitaevskii simulations."],"url":"http://arxiv.org/abs/2403.17403v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-26 05:23:12","title":"Natural-artificial hybrid swarm: Cyborg-insect group navigation in unknown obstructed soft terrain","abstract":"Navigating multi-robot systems in complex terrains has always been a challenging task. This is due to the inherent limitations of traditional robots in collision avoidance, adaptation to unknown environments, and sustained energy efficiency. In order to overcome these limitations, this research proposes a solution by integrating living insects with miniature electronic controllers to enable robotic-like programmable control, and proposing a novel control algorithm for swarming. Although these creatures, called cyborg insects, have the ability to instinctively avoid collisions with neighbors and obstacles while adapting to complex terrains, there is a lack of literature on the control of multi-cyborg systems. This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects' inherent individual variability in their reactions to control input. In response to this issue, we propose a novel swarm navigation algorithm addressing these challenges. The effectiveness of the algorithm is demonstrated through an experimental validation in which a cyborg swarm was successfully navigated through an unknown sandy field with obstacles and hills. This research contributes to the domain of swarm robotics and showcases the potential of integrating biological organisms with robotics and control theory to create more intelligent autonomous systems with real-world applications.","sentences":["Navigating multi-robot systems in complex terrains has always been a challenging task.","This is due to the inherent limitations of traditional robots in collision avoidance, adaptation to unknown environments, and sustained energy efficiency.","In order to overcome these limitations, this research proposes a solution by integrating living insects with miniature electronic controllers to enable robotic-like programmable control, and proposing a novel control algorithm for swarming.","Although these creatures, called cyborg insects, have the ability to instinctively avoid collisions with neighbors and obstacles while adapting to complex terrains, there is a lack of literature on the control of multi-cyborg systems.","This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects' inherent individual variability in their reactions to control input.","In response to this issue, we propose a novel swarm navigation algorithm addressing these challenges.","The effectiveness of the algorithm is demonstrated through an experimental validation in which a cyborg swarm was successfully navigated through an unknown sandy field with obstacles and hills.","This research contributes to the domain of swarm robotics and showcases the potential of integrating biological organisms with robotics and control theory to create more intelligent autonomous systems with real-world applications."],"url":"http://arxiv.org/abs/2403.17392v2","category":"cs.RO"}
{"created":"2024-03-26 05:12:11","title":"A framework to identify supercritical and subcritical Turing bifurcations: Case study of a system sustaining cubic and quadratic autocatalysis","abstract":"In this work, we focus on an autocatalytic reaction-diffusion model and carry out multiple scale weakly nonlinear analysis. A cubic and a quadratic autocatalytic reaction system is analysed. We develop a framework to identify the critical surfaces in parameter space across which the nature of the Turing bifurcation changes from supercritical to subcritical. These are verified by direct numerical simulations of the system. Using weakly nonlinear analysis, we derive equations up to the fifth order that governs the amplitude of the spatial patterns. The limit point of the bifurcating solution is captured accurately by extending the analysis to the fifth order for the case of subcritical bifurcation. The numerical solutions are in good agreement with the predictions of the weakly nonlinear analysis for supercritical bifurcations. We show that when multiple steady states arise Turing patterns can coexist with another spatially uniform steady states. Furthermore, we show that our framework can be extended to get different patterns like squares and hexagons in a two-dimensional domain. We show that the shape of Turing patterns is influenced by the domain size. This shows that the geometry can influence the kind of patterns formed in natural systems. This study will aid the experimentalist identify operating conditions where Turing patterns can be obtained.","sentences":["In this work, we focus on an autocatalytic reaction-diffusion model and carry out multiple scale weakly nonlinear analysis.","A cubic and a quadratic autocatalytic reaction system is analysed.","We develop a framework to identify the critical surfaces in parameter space across which the nature of the Turing bifurcation changes from supercritical to subcritical.","These are verified by direct numerical simulations of the system.","Using weakly nonlinear analysis, we derive equations up to the fifth order that governs the amplitude of the spatial patterns.","The limit point of the bifurcating solution is captured accurately by extending the analysis to the fifth order for the case of subcritical bifurcation.","The numerical solutions are in good agreement with the predictions of the weakly nonlinear analysis for supercritical bifurcations.","We show that when multiple steady states arise Turing patterns can coexist with another spatially uniform steady states.","Furthermore, we show that our framework can be extended to get different patterns like squares and hexagons in a two-dimensional domain.","We show that the shape of Turing patterns is influenced by the domain size.","This shows that the geometry can influence the kind of patterns formed in natural systems.","This study will aid the experimentalist identify operating conditions where Turing patterns can be obtained."],"url":"http://arxiv.org/abs/2403.17386v1","category":"nlin.AO"}
{"created":"2024-03-26 04:09:08","title":"CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning","abstract":"Unsupervised Domain Adaptation (UDA) aims to adapt models from labeled source domains to unlabeled target domains. When adapting to adverse scenes, existing UDA methods fail to perform well due to the lack of instructions, leading their models to overlook discrepancies within all adverse scenes. To tackle this, we propose CoDA which instructs models to distinguish, focus, and learn from these discrepancies at scene and image levels. Specifically, CoDA consists of a Chain-of-Domain (CoD) strategy and a Severity-Aware Visual Prompt Tuning (SAVPT) mechanism. CoD focuses on scene-level instructions to divide all adverse scenes into easy and hard scenes, guiding models to adapt from source to easy domains with easy scene images, and then to hard domains with hard scene images, thereby laying a solid foundation for whole adaptations. Building upon this foundation, we employ SAVPT to dive into more detailed image-level instructions to boost performance. SAVPT features a novel metric Severity that divides all adverse scene images into low-severity and high-severity images. Then Severity directs visual prompts and adapters, instructing models to concentrate on unified severity features instead of scene-specific features, without adding complexity to the model architecture. CoDA achieves SOTA performances on widely-used benchmarks under all adverse scenes. Notably, CoDA outperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and Foggy Zurich benchmarks, respectively. Our code is available at https://github.com/Cuzyoung/CoDA","sentences":["Unsupervised Domain Adaptation (UDA) aims to adapt models from labeled source domains to unlabeled target domains.","When adapting to adverse scenes, existing UDA methods fail to perform well due to the lack of instructions, leading their models to overlook discrepancies within all adverse scenes.","To tackle this, we propose CoDA which instructs models to distinguish, focus, and learn from these discrepancies at scene and image levels.","Specifically, CoDA consists of a Chain-of-Domain (CoD) strategy and a Severity-Aware Visual Prompt Tuning (SAVPT) mechanism.","CoD focuses on scene-level instructions to divide all adverse scenes into easy and hard scenes, guiding models to adapt from source to easy domains with easy scene images, and then to hard domains with hard scene images, thereby laying a solid foundation for whole adaptations.","Building upon this foundation, we employ SAVPT to dive into more detailed image-level instructions to boost performance.","SAVPT features a novel metric Severity that divides all adverse scene images into low-severity and high-severity images.","Then Severity directs visual prompts and adapters, instructing models to concentrate on unified severity features instead of scene-specific features, without adding complexity to the model architecture.","CoDA achieves SOTA performances on widely-used benchmarks under all adverse scenes.","Notably, CoDA outperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and Foggy Zurich benchmarks, respectively.","Our code is available at https://github.com/Cuzyoung/CoDA"],"url":"http://arxiv.org/abs/2403.17369v1","category":"cs.CV"}
{"created":"2024-03-26 03:51:01","title":"Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models","abstract":"We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.","sentences":["We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA).","Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information.","Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions.","Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources.","We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers.","Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods."],"url":"http://arxiv.org/abs/2403.17359v1","category":"cs.CL"}
{"created":"2024-03-26 03:31:24","title":"On the Heating of the Slow Solar-Wind by Imbalanced Alfv\u00e9n-Wave Turbulence from 0.06 au to 1 au: Parker Solar Probe and Solar Orbiter observations","abstract":"In this work we analyze plasma and magnetic field data provided by the Parker Solar Probe (\\emph{PSP}) and Solar Orbiter (\\emph{SO}) missions to investigate the radial evolution of the heating of Alfv\\'enic slow wind (ASW) by imbalanced Alfv\\'en-Wave (AW) turbulent fluctuations from 0.06 au to 1 au. in our analysis we focus on slow solar-wind intervals with highly imbalanced and incompressible turbulence (i.e., magnetic compressibility $C_B=\\delta B/B\\leq 0.25$, plasma compressibility $C_n=\\delta n/n\\leq 0.25$ and normalized cross-helicity $\\sigma_c\\geq 0.65$). First, we estimate the AW turbulent dissipation rate from the wave energy equation and find that the radial profile trend is similar to the proton heating rate. Second, we find that the scaling of the empirical AW turbulent dissipation rate $Q_W$ obtained from the wave energy equation matches the scaling from the phenomenological AW turbulent dissipation rate $Q_{\\rm CH09}$ (with $Q_{\\rm CH09}\\simeq 1.55 Q_W$) derived by~\\cite{chandran09} based on the model of reflection-driven turbulence. Our results suggest that, as in the fast solar wind, AW turbulence plays a major role in the ion heating that occurs in incompressible slow-wind streams.","sentences":["In this work we analyze plasma and magnetic field data provided by the Parker Solar Probe (\\emph{PSP}) and Solar Orbiter (\\emph{SO}) missions to investigate the radial evolution of the heating of Alfv\\'enic slow wind (ASW) by imbalanced Alfv\\'en-Wave (AW) turbulent fluctuations from 0.06 au to 1 au.","in our analysis we focus on slow solar-wind intervals with highly imbalanced and incompressible turbulence (i.e., magnetic compressibility $C_B=\\delta B/B\\leq 0.25$, plasma compressibility $C_n=\\delta n/n\\leq 0.25$ and normalized cross-helicity $\\sigma_c\\geq 0.65$).","First, we estimate the AW turbulent dissipation rate from the wave energy equation and find that the radial profile trend is similar to the proton heating rate.","Second, we find that the scaling of the empirical AW turbulent dissipation rate $Q_W$ obtained from the wave energy equation matches the scaling from the phenomenological AW turbulent dissipation rate $Q_{\\rm CH09}$ (with $Q_{\\rm CH09}\\simeq 1.55 Q_W$) derived by~\\cite{chandran09} based on the model of reflection-driven turbulence.","Our results suggest that, as in the fast solar wind, AW turbulence plays a major role in the ion heating that occurs in incompressible slow-wind streams."],"url":"http://arxiv.org/abs/2403.17352v1","category":"physics.space-ph"}
{"created":"2024-03-26 03:26:40","title":"On three dimensional flows of viscoelastic fluids of Giesekus type","abstract":"Viscoelastic rate-type fluids are popular models of choice in many applications involving flows of fluid-like materials with complex micro-structure. A well-developed mathematical theory for the most of these classical fluid models is however missing. The main purpose of this study is to provide a complete proof of long-time and large-data existence of weak solutions to unsteady internal three-dimensional flows of Giesekus fluids subject to a no-slip boundary condition. As a new auxiliary tool, we provide the identification of certain biting limits in the parabolic setting, presented here within the framework of evolutionary Stokes problems. We also generalize the long-time and large-data existence result to higher dimensions, to viscoelastic models with multiple relaxation mechanisms and to viscoelastic models with different type of dissipation.","sentences":["Viscoelastic rate-type fluids are popular models of choice in many applications involving flows of fluid-like materials with complex micro-structure.","A well-developed mathematical theory for the most of these classical fluid models is however missing.","The main purpose of this study is to provide a complete proof of long-time and large-data existence of weak solutions to unsteady internal three-dimensional flows of Giesekus fluids subject to a no-slip boundary condition.","As a new auxiliary tool, we provide the identification of certain biting limits in the parabolic setting, presented here within the framework of evolutionary Stokes problems.","We also generalize the long-time and large-data existence result to higher dimensions, to viscoelastic models with multiple relaxation mechanisms and to viscoelastic models with different type of dissipation."],"url":"http://arxiv.org/abs/2403.17348v1","category":"math.AP"}
{"created":"2024-03-26 03:08:00","title":"How many bits does your quantum estimation return?","abstract":"We give two upper bounds to the mutual information in arbitrary quantum estimation strategies. The first is based on some simple Fourier properties of the estimation apparatus. The second is derived using the first but, interestingly, depends only on the Fisher information of the parameter, so it is valid even beyond quantum estimation. We illustrate the usefulness of these bounds by characterizing the quantum phase estimation algorithm in the presence of noise. In addition, for the noiseless case, we extend the analysis beyond applying the bound and we discuss the optimal entangled and adaptive strategies, clarifying inaccuracies appearing on this topic in the literature.","sentences":["We give two upper bounds to the mutual information in arbitrary quantum estimation strategies.","The first is based on some simple Fourier properties of the estimation apparatus.","The second is derived using the first but, interestingly, depends only on the Fisher information of the parameter, so it is valid even beyond quantum estimation.","We illustrate the usefulness of these bounds by characterizing the quantum phase estimation algorithm in the presence of noise.","In addition, for the noiseless case, we extend the analysis beyond applying the bound and we discuss the optimal entangled and adaptive strategies, clarifying inaccuracies appearing on this topic in the literature."],"url":"http://arxiv.org/abs/2403.17345v1","category":"quant-ph"}
{"created":"2024-03-26 02:49:08","title":"Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems","abstract":"Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced. Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. %as well as infeasibility. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In particular, we parameterize our controller and use bilevel optimization, where RL is used to learn the optimal parameters while MPC computes the optimal control input. We validate our method by applying it to the challenging automated merging control problem for Connected and Automated Vehicles (CAVs) at conflicting roadways. Results demonstrate improved performance and a significant reduction in the number of infeasible cases compared to traditional heuristic approaches used for tuning CBF-based controllers, showcasing the effectiveness of the proposed method.","sentences":["Optimal control methods provide solutions to safety-critical problems but easily become intractable.","Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss.","This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced.","Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness.","%as well as infeasibility.","To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF).","In particular, we parameterize our controller and use bilevel optimization, where RL is used to learn the optimal parameters while MPC computes the optimal control input.","We validate our method by applying it to the challenging automated merging control problem for Connected and Automated Vehicles (CAVs) at conflicting roadways.","Results demonstrate improved performance and a significant reduction in the number of infeasible cases compared to traditional heuristic approaches used for tuning CBF-based controllers, showcasing the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.17338v1","category":"eess.SY"}
{"created":"2024-03-26 02:24:32","title":"Deep Support Vectors","abstract":"While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.","sentences":["While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored.","This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models.","We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning.","Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models.","Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM.","The code will be available."],"url":"http://arxiv.org/abs/2403.17329v1","category":"cs.LG"}
{"created":"2024-03-26 02:08:49","title":"On the Impact of Random Node Sampling on Adaptive Diffusion Networks","abstract":"In this paper, we analyze the effects of random sampling on adaptive diffusion networks. These networks consist in a collection of nodes that can measure and process data, and that can communicate with each other to pursue a common goal of estimating an unknown system. In particular, we consider in our theoretical analysis the diffusion least-mean-squares algorithm in a scenario in which the nodes are randomly sampled. Hence, each node may or may not adapt its local estimate at a certain iteration. Our model shows that, if the nodes cooperate, a reduction in the sampling probability leads to a slight decrease in the steady-state Network Mean-Square Deviation (NMSD), assuming that the environment is stationary and that all other parameters of the algorithm are kept fixed. Furthermore, under certain circumstances, this can also ensure the stability of the algorithm in situations in which it would otherwise be unstable. Although counter-intuitive, our findings are backed by simulation results, which match the theoretical curves well.","sentences":["In this paper, we analyze the effects of random sampling on adaptive diffusion networks.","These networks consist in a collection of nodes that can measure and process data, and that can communicate with each other to pursue a common goal of estimating an unknown system.","In particular, we consider in our theoretical analysis the diffusion least-mean-squares algorithm in a scenario in which the nodes are randomly sampled.","Hence, each node may or may not adapt its local estimate at a certain iteration.","Our model shows that, if the nodes cooperate, a reduction in the sampling probability leads to a slight decrease in the steady-state Network Mean-Square Deviation (NMSD), assuming that the environment is stationary and that all other parameters of the algorithm are kept fixed.","Furthermore, under certain circumstances, this can also ensure the stability of the algorithm in situations in which it would otherwise be unstable.","Although counter-intuitive, our findings are backed by simulation results, which match the theoretical curves well."],"url":"http://arxiv.org/abs/2403.17323v1","category":"eess.SP"}
{"created":"2024-03-26 00:36:03","title":"Linear Numerical Schemes for a $\\textbf{Q}$-Tensor System for Nematic Liquid Crystals","abstract":"In this work, we present three linear numerical schemes to model nematic liquid crystals using the Landau-de Gennes $\\textbf{Q}$-tensor theory. The first scheme is based on using a truncation procedure of the energy, which allows for an unconditionally energy stable first order accurate decoupled scheme. The second scheme uses a modified second order accurate optimal dissipation algorithm, which gives a second order accurate coupled scheme. Finally, the third scheme uses a new idea to decouple the unknowns from the second scheme which allows us to obtain accurate dynamics while improving computational efficiency. We present several numerical experiments to offer a comparative study of the accuracy, efficiency and the ability of the numerical schemes to represent realistic dynamics.","sentences":["In this work, we present three linear numerical schemes to model nematic liquid crystals using the Landau-de Gennes $\\textbf{Q}$-tensor theory.","The first scheme is based on using a truncation procedure of the energy, which allows for an unconditionally energy stable first order accurate decoupled scheme.","The second scheme uses a modified second order accurate optimal dissipation algorithm, which gives a second order accurate coupled scheme.","Finally, the third scheme uses a new idea to decouple the unknowns from the second scheme which allows us to obtain accurate dynamics while improving computational efficiency.","We present several numerical experiments to offer a comparative study of the accuracy, efficiency and the ability of the numerical schemes to represent realistic dynamics."],"url":"http://arxiv.org/abs/2403.17289v1","category":"math.NA"}
{"created":"2024-03-25 23:19:19","title":"Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning","abstract":"This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers. By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation. To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture. Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks. Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated. To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn. The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameters. The key factors of two learning strategies and corresponding effects were explored in context-aware and context-unaware scenarios, enabling us to identify the scenarios where the methods demonstrate optimal performance, derive conclusive insights, and contribute to a broader range of learning-based engineering applications.","sentences":["This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers.","By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation.","To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture.","Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks.","Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated.","To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn.","The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameters.","The key factors of two learning strategies and corresponding effects were explored in context-aware and context-unaware scenarios, enabling us to identify the scenarios where the methods demonstrate optimal performance, derive conclusive insights, and contribute to a broader range of learning-based engineering applications."],"url":"http://arxiv.org/abs/2403.17266v1","category":"cs.RO"}
{"created":"2024-03-25 23:04:09","title":"Latency-Aware Generative Semantic Communications with Pre-Trained Diffusion Models","abstract":"Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this paper, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.","sentences":["Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process.","This enables semantic communications at extremely low data rates in future wireless networks.","In this paper, we develop a latency-aware semantic communications framework with pre-trained generative models.","The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent.","For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel.","Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints.","At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics.","Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications."],"url":"http://arxiv.org/abs/2403.17256v1","category":"cs.IT"}
{"created":"2024-03-25 22:49:56","title":"DASA: Delay-Adaptive Multi-Agent Stochastic Approximation","abstract":"We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \\texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \\texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\\tmix$ and on the average delay $\\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA applications, including multi-agent and distributed temporal difference (TD) learning, Q-learning and stochastic optimization with correlated data.","sentences":["We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server.","We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays.","To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation.","We provide a finite-time analysis of \\texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains.","Significantly advancing existing results, \\texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\\tmix$ and on the average delay $\\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling.","Our work is relevant for various SA applications, including multi-agent and distributed temporal difference (TD) learning, Q-learning and stochastic optimization with correlated data."],"url":"http://arxiv.org/abs/2403.17247v1","category":"cs.AI"}
{"created":"2024-03-25 22:44:28","title":"Review Ecosystems to access Educational XR Experiences: a Scoping Review","abstract":"Educators, developers, and other stakeholders face challenges when creating, adapting, and utilizing virtual and augmented reality (XR) experiences for teaching curriculum topics. User created reviews of these applications provide important information about their relevance and effectiveness in supporting achievement of educational outcomes. To make these reviews accessible, relevant, and useful, they must be readily available and presented in a format that supports decision-making by educators. This paper identifies best practices for developing a new review ecosystem by analyzing existing approaches to providing reviews of interactive experiences. It focuses on the form and format of these reviews, as well as the mechanisms for sharing information about experiences and identifying which ones are most effective. The paper also examines the incentives that drive review creation and maintenance, ensuring that new experiences receive attention from reviewers and that relevant information is updated when necessary. The strategies and opportunities for developing an educational XR (eduXR) review ecosystem include methods for measuring properties such as quality metrics, engaging a broad range of stakeholders in the review process, and structuring the system as a closed loop managed by feedback and incentive structures to ensure stability and productivity. Computing educators are well-positioned to lead the development of these review ecosystems, which can relate XR experiences to the potential opportunities for teaching and learning that they offer.","sentences":["Educators, developers, and other stakeholders face challenges when creating, adapting, and utilizing virtual and augmented reality (XR) experiences for teaching curriculum topics.","User created reviews of these applications provide important information about their relevance and effectiveness in supporting achievement of educational outcomes.","To make these reviews accessible, relevant, and useful, they must be readily available and presented in a format that supports decision-making by educators.","This paper identifies best practices for developing a new review ecosystem by analyzing existing approaches to providing reviews of interactive experiences.","It focuses on the form and format of these reviews, as well as the mechanisms for sharing information about experiences and identifying which ones are most effective.","The paper also examines the incentives that drive review creation and maintenance, ensuring that new experiences receive attention from reviewers and that relevant information is updated when necessary.","The strategies and opportunities for developing an educational XR (eduXR) review ecosystem include methods for measuring properties such as quality metrics, engaging a broad range of stakeholders in the review process, and structuring the system as a closed loop managed by feedback and incentive structures to ensure stability and productivity.","Computing educators are well-positioned to lead the development of these review ecosystems, which can relate XR experiences to the potential opportunities for teaching and learning that they offer."],"url":"http://arxiv.org/abs/2403.17243v1","category":"cs.CY"}
{"created":"2024-03-25 22:25:03","title":"A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme with A Mobile-Robot System Study","abstract":"This paper develops an adaptive state tracking control scheme for discrete-time systems, using the least-squares algorithm, as the new solution to the long-standing discrete-time adaptive state tracking control problem to which the Lyapunov method (well-developed for the continuous-time adaptive state tracking problem) is not applicable. The new adaptive state tracking scheme is based on a recently-developed new discrete-time error model which has been used for gradient algorithm based state tracking control schemes, and uses the least-squares algorithm for parameter adaptation. The new least-squares algorithm is derived to minimize an accumulative estimation error, to ensure certain optimality for parameter estimation. The system stability and output tracking properties are studied. Technical results are presented in terms of plant-model matching, error model, adaptive law, optimality formulation, and stability and tracking analysis. The developed adaptive control scheme is applied to a discrete-time multiple mobile robot system to meet an adaptive state tracking objective. In addition, a collision avoidance mechanism is proposed to prevent collisions in the whole tracking process. Simulation results are presented, which verify the desired system state tracking properties under the developed least-squares algorithm based adaptive control scheme.","sentences":["This paper develops an adaptive state tracking control scheme for discrete-time systems, using the least-squares algorithm, as the new solution to the long-standing discrete-time adaptive state tracking control problem to which the Lyapunov method (well-developed for the continuous-time adaptive state tracking problem) is not applicable.","The new adaptive state tracking scheme is based on a recently-developed new discrete-time error model which has been used for gradient algorithm based state tracking control schemes, and uses the least-squares algorithm for parameter adaptation.","The new least-squares algorithm is derived to minimize an accumulative estimation error, to ensure certain optimality for parameter estimation.","The system stability and output tracking properties are studied.","Technical results are presented in terms of plant-model matching, error model, adaptive law, optimality formulation, and stability and tracking analysis.","The developed adaptive control scheme is applied to a discrete-time multiple mobile robot system to meet an adaptive state tracking objective.","In addition, a collision avoidance mechanism is proposed to prevent collisions in the whole tracking process.","Simulation results are presented, which verify the desired system state tracking properties under the developed least-squares algorithm based adaptive control scheme."],"url":"http://arxiv.org/abs/2403.17235v1","category":"eess.SY"}
{"created":"2024-03-25 21:08:26","title":"Strategies to Improve Real-World Applicability of Laparoscopic Anatomy Segmentation Models","abstract":"Accurate identification and localization of anatomical structures of varying size and appearance in laparoscopic imaging are necessary to leverage the potential of computer vision techniques for surgical decision support. Segmentation performance of such models is traditionally reported using metrics of overlap such as IoU. However, imbalanced and unrealistic representation of classes in the training data and suboptimal selection of reported metrics have the potential to skew nominal segmentation performance and thereby ultimately limit clinical translation. In this work, we systematically analyze the impact of class characteristics (i.e., organ size differences), training and test data composition (i.e., representation of positive and negative examples), and modeling parameters (i.e., foreground-to-background class weight) on eight segmentation metrics: accuracy, precision, recall, IoU, F1 score, specificity, Hausdorff Distance, and Average Symmetric Surface Distance. Based on our findings, we propose two simple yet effective strategies to improve real-world applicability of image segmentation models in laparoscopic surgical data: (1) inclusion of negative examples in the training process and (2) adaptation of foreground-background weights in segmentation models to maximize model performance with respect to specific metrics of interest, depending on the clinical use case.","sentences":["Accurate identification and localization of anatomical structures of varying size and appearance in laparoscopic imaging are necessary to leverage the potential of computer vision techniques for surgical decision support.","Segmentation performance of such models is traditionally reported using metrics of overlap such as IoU. However, imbalanced and unrealistic representation of classes in the training data and suboptimal selection of reported metrics have the potential to skew nominal segmentation performance and thereby ultimately limit clinical translation.","In this work, we systematically analyze the impact of class characteristics (i.e., organ size differences), training and test data composition (i.e., representation of positive and negative examples), and modeling parameters (i.e., foreground-to-background class weight) on eight segmentation metrics: accuracy, precision, recall, IoU, F1 score, specificity, Hausdorff Distance, and Average Symmetric Surface Distance.","Based on our findings, we propose two simple yet effective strategies to improve real-world applicability of image segmentation models in laparoscopic surgical data: (1) inclusion of negative examples in the training process and (2) adaptation of foreground-background weights in segmentation models to maximize model performance with respect to specific metrics of interest, depending on the clinical use case."],"url":"http://arxiv.org/abs/2403.17192v1","category":"cs.CV"}
{"created":"2024-03-25 20:44:01","title":"Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study","abstract":"Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage. Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors. Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets. With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers. The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered. In this study, we selected four types of deep models that were recently proposed and evaluated their performance for stroke segmentation: a pure Transformer-based architecture (DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention mechanisms in their design, an advanced hybrid model that incorporates CNNs with Transformers (FCT), and the well-known self-adaptive nnUNet framework with its configuration based on given data. We examined their performance on two publicly available datasets, and found that the nnUNet achieved the best results with the simplest design among all. Revealing the robustness issue of Transformers to such variabilities serves as a potential reason for their weaker performance. Furthermore, nnUNet's success underscores the significant impact of preprocessing and postprocessing techniques in enhancing segmentation results, surpassing the focus solely on architectural designs","sentences":["Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage.","Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors.","Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets.","With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers.","The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered.","In this study, we selected four types of deep models that were recently proposed and evaluated their performance for stroke segmentation: a pure Transformer-based architecture (DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention mechanisms in their design, an advanced hybrid model that incorporates CNNs with Transformers (FCT), and the well-known self-adaptive nnUNet framework with its configuration based on given data.","We examined their performance on two publicly available datasets, and found that the nnUNet achieved the best results with the simplest design among all.","Revealing the robustness issue of Transformers to such variabilities serves as a potential reason for their weaker performance.","Furthermore, nnUNet's success underscores the significant impact of preprocessing and postprocessing techniques in enhancing segmentation results, surpassing the focus solely on architectural designs"],"url":"http://arxiv.org/abs/2403.17177v1","category":"eess.IV"}
{"created":"2024-03-25 19:50:07","title":"Hearing the shape of an arena with spectral swarm robotics","abstract":"Swarm robotics promises adaptability to unknown situations and robustness against failures. However, it still struggles with global tasks that require understanding the broader context in which the robots operate, such as identifying the shape of the arena in which the robots are embedded. Biological swarms, such as shoals of fish, flocks of birds, and colonies of insects, routinely solve global geometrical problems through the diffusion of local cues. This paradigm can be explicitly described by mathematical models that could be directly computed and exploited by a robotic swarm. Diffusion over a domain is mathematically encapsulated by the Laplacian, a linear operator that measures the local curvature of a function. Crucially the geometry of a domain can generally be reconstructed from the eigenspectrum of its Laplacian. Here we introduce spectral swarm robotics where robots diffuse information to their neighbors to emulate the Laplacian operator - enabling them to \"hear\" the spectrum of their arena. We reveal a universal scaling that links the optimal number of robots (a global parameter) with their optimal radius of interaction (a local parameter). We validate experimentally spectral swarm robotics under challenging conditions with the one-shot classification of arena shapes using a sparse swarm of Kilobots. Spectral methods can assist with challenging tasks where robots need to build an emergent consensus on their environment, such as adaptation to unknown terrains, division of labor, or quorum sensing. Spectral methods may extend beyond robotics to analyze and coordinate swarms of agents of various natures, such as traffic or crowds, and to better understand the long-range dynamics of natural systems emerging from short-range interactions.","sentences":["Swarm robotics promises adaptability to unknown situations and robustness against failures.","However, it still struggles with global tasks that require understanding the broader context in which the robots operate, such as identifying the shape of the arena in which the robots are embedded.","Biological swarms, such as shoals of fish, flocks of birds, and colonies of insects, routinely solve global geometrical problems through the diffusion of local cues.","This paradigm can be explicitly described by mathematical models that could be directly computed and exploited by a robotic swarm.","Diffusion over a domain is mathematically encapsulated by the Laplacian, a linear operator that measures the local curvature of a function.","Crucially the geometry of a domain can generally be reconstructed from the eigenspectrum of its Laplacian.","Here we introduce spectral swarm robotics where robots diffuse information to their neighbors to emulate the Laplacian operator - enabling them to \"hear\" the spectrum of their arena.","We reveal a universal scaling that links the optimal number of robots (a global parameter) with their optimal radius of interaction (a local parameter).","We validate experimentally spectral swarm robotics under challenging conditions with the one-shot classification of arena shapes using a sparse swarm of Kilobots.","Spectral methods can assist with challenging tasks where robots need to build an emergent consensus on their environment, such as adaptation to unknown terrains, division of labor, or quorum sensing.","Spectral methods may extend beyond robotics to analyze and coordinate swarms of agents of various natures, such as traffic or crowds, and to better understand the long-range dynamics of natural systems emerging from short-range interactions."],"url":"http://arxiv.org/abs/2403.17147v1","category":"cs.RO"}
{"created":"2024-03-25 19:40:26","title":"Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language","abstract":"Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as well. Furthermore, we experiment with multilingual and cross-lingual experiments that could benefit many low-resource languages.","sentences":["Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects.","There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships.","However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English.","This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German.","Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset.","We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision.","We train several state-of-the-art machine learning models on the automatically created dataset and release them as well.","Furthermore, we experiment with multilingual and cross-lingual experiments that could benefit many low-resource languages."],"url":"http://arxiv.org/abs/2403.17143v2","category":"cs.CL"}
{"created":"2024-03-25 19:39:17","title":"Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control","abstract":"Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o reinforcement learning. A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained. While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees. Thus, the approximation properties required for control with neural networks are assumed, rather than proved. In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It suffices to generate the weights uniformly over a sphere and the biases uniformly over an interval. We show how the result can be used to get approximations of required accuracy in a model reference adaptive control application.","sentences":["Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o reinforcement learning.","A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained.","While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees.","Thus, the approximation properties required for control with neural networks are assumed, rather than proved.","In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons.","It suffices to generate the weights uniformly over a sphere and the biases uniformly over an interval.","We show how the result can be used to get approximations of required accuracy in a model reference adaptive control application."],"url":"http://arxiv.org/abs/2403.17142v1","category":"math.OC"}
{"created":"2024-03-25 19:23:44","title":"An Equilibrium Analysis of the Arad-Rubinstein Game","abstract":"Colonel Blotto games with discrete strategy spaces effectively illustrate the intricate nature of multidimensional strategic reasoning. This paper studies the equilibrium set of such games where, in line with prior experimental work, the tie-breaking rule is allowed to be flexible. We begin by pointing out that equilibrium constructions known from the literature extend to our class of games. However, we also note that irrespective of the tie-breaking rule, the equilibrium set is excessively large. Specifically, any pure strategy that allocates at most twice the fair share to each battlefield is used with positive probability in some equilibrium. Furthermore, refinements based on the elimination of weakly dominated strategies prove ineffective. To derive specific predictions amid this multiplicity, we compute strategies resulting from long-run adaptive learning.","sentences":["Colonel Blotto games with discrete strategy spaces effectively illustrate the intricate nature of multidimensional strategic reasoning.","This paper studies the equilibrium set of such games where, in line with prior experimental work, the tie-breaking rule is allowed to be flexible.","We begin by pointing out that equilibrium constructions known from the literature extend to our class of games.","However, we also note that irrespective of the tie-breaking rule, the equilibrium set is excessively large.","Specifically, any pure strategy that allocates at most twice the fair share to each battlefield is used with positive probability in some equilibrium.","Furthermore, refinements based on the elimination of weakly dominated strategies prove ineffective.","To derive specific predictions amid this multiplicity, we compute strategies resulting from long-run adaptive learning."],"url":"http://arxiv.org/abs/2403.17139v2","category":"cs.GT"}
{"created":"2024-03-25 19:18:25","title":"Adaptive Step Duration for Precise Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds","abstract":"This paper introduces a novel multi-step preview foot placement planning algorithm designed to enhance the robustness of bipedal robotic walking across challenging terrains with restricted footholds. Traditional one-step preview planning struggles to maintain stability when stepping areas are severely limited, such as with random stepping stones. In this work, we developed a discrete-time Model Predictive Control (MPC) based on the step-to-step discrete evolution of the Divergent Component of Motion (DCM) of bipedal locomotion. This approach adaptively changes the step duration for optimal foot placement under constraints, thereby ensuring the robot's operational viability over multiple future steps and significantly improving its ability to navigate through environments with tight constraints on possible footholds. The effectiveness of this planning algorithm is demonstrated through simulations that include a variety of complex stepping-stone configurations and external perturbations. These tests underscore the algorithm's improved performance for navigating foothold-restricted environments, even with the presence of external disturbances.","sentences":["This paper introduces a novel multi-step preview foot placement planning algorithm designed to enhance the robustness of bipedal robotic walking across challenging terrains with restricted footholds.","Traditional one-step preview planning struggles to maintain stability when stepping areas are severely limited, such as with random stepping stones.","In this work, we developed a discrete-time Model Predictive Control (MPC) based on the step-to-step discrete evolution of the Divergent Component of Motion (DCM) of bipedal locomotion.","This approach adaptively changes the step duration for optimal foot placement under constraints, thereby ensuring the robot's operational viability over multiple future steps and significantly improving its ability to navigate through environments with tight constraints on possible footholds.","The effectiveness of this planning algorithm is demonstrated through simulations that include a variety of complex stepping-stone configurations and external perturbations.","These tests underscore the algorithm's improved performance for navigating foothold-restricted environments, even with the presence of external disturbances."],"url":"http://arxiv.org/abs/2403.17136v1","category":"cs.RO"}
{"created":"2024-03-25 19:07:32","title":"The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition","abstract":"In-context Learning (ICL) has emerged as a powerful paradigm for performing natural language tasks with Large Language Models (LLM) without updating the models' parameters, in contrast to the traditional gradient-based finetuning. The promise of ICL is that the LLM can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost. The ability of LLMs to perform tasks in this few-shot manner relies on their background knowledge of the task (or task priors). However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition, where the mapping from text to emotions can differ widely due to variability in human annotations. In this work, we design experiments and propose measurements to explicitly quantify the consistency of proxies of LLM priors and their pull on the posteriors. We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that the larger the model, the stronger these effects become. Our results suggest that caution is needed when using ICL with larger LLMs for affect-centered tasks outside their pre-training domain and when interpreting ICL results.","sentences":["In-context Learning (ICL) has emerged as a powerful paradigm for performing natural language tasks with Large Language Models (LLM) without updating the models' parameters, in contrast to the traditional gradient-based finetuning.","The promise of ICL is that the LLM can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost.","The ability of LLMs to perform tasks in this few-shot manner relies on their background knowledge of the task (or task priors).","However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors.","This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition, where the mapping from text to emotions can differ widely due to variability in human annotations.","In this work, we design experiments and propose measurements to explicitly quantify the consistency of proxies of LLM priors and their pull on the posteriors.","We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions.","We also find that the larger the model, the stronger these effects become.","Our results suggest that caution is needed when using ICL with larger LLMs for affect-centered tasks outside their pre-training domain and when interpreting ICL results."],"url":"http://arxiv.org/abs/2403.17125v1","category":"cs.CL"}
{"created":"2024-03-25 18:00:42","title":"Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions","abstract":"In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts. We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model. Project page: https://compvis.github.io/attribute-control. Code is available at https://github.com/CompVis/attribute-control.","sentences":["In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images.","However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person'').","Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously.","We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models.","Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts.","We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model.","Project page: https://compvis.github.io/attribute-control.","Code is available at https://github.com/CompVis/attribute-control."],"url":"http://arxiv.org/abs/2403.17064v1","category":"cs.CV"}
{"created":"2024-03-25 17:59:41","title":"Invertible Diffusion Models for Compressed Sensing","abstract":"While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference.","sentences":["While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment.","Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS.","To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method.","IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning.","To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks.","As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory.","In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction.","Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR.","Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference."],"url":"http://arxiv.org/abs/2403.17006v1","category":"cs.CV"}
{"created":"2024-03-25 17:59:26","title":"Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution","abstract":"Diffusion models are just at a tipping point for image super-resolution task. Nevertheless, it is not trivial to capitalize on diffusion models for video super-resolution which necessitates not only the preservation of visual appearance from low-resolution to high-resolution videos, but also the temporal consistency across video frames. In this paper, we propose a novel approach, pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video super-resolution. SATeCo pivots on learning spatial-temporal guidance from low-resolution videos to calibrate both latent-space high-resolution video denoising and pixel-space video reconstruction. Technically, SATeCo freezes all the parameters of the pre-trained UNet and VAE, and only optimizes two deliberately-designed spatial feature adaptation (SFA) and temporal feature alignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame features via adaptively estimating affine parameters for each pixel, guaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA delves into feature interaction within a 3D local window (tubelet) through self-attention, and executes cross-attention between tubelet and its low-resolution counterpart to guide temporal feature alignment. Extensive experiments conducted on the REDS4 and Vid4 datasets demonstrate the effectiveness of our approach.","sentences":["Diffusion models are just at a tipping point for image super-resolution task.","Nevertheless, it is not trivial to capitalize on diffusion models for video super-resolution which necessitates not only the preservation of visual appearance from low-resolution to high-resolution videos, but also the temporal consistency across video frames.","In this paper, we propose a novel approach, pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video super-resolution.","SATeCo pivots on learning spatial-temporal guidance from low-resolution videos to calibrate both latent-space high-resolution video denoising and pixel-space video reconstruction.","Technically, SATeCo freezes all the parameters of the pre-trained UNet and VAE, and only optimizes two deliberately-designed spatial feature adaptation (SFA) and temporal feature alignment (TFA) modules, in the decoder of UNet and VAE.","SFA modulates frame features via adaptively estimating affine parameters for each pixel, guaranteeing pixel-wise guidance for high-resolution frame synthesis.","TFA delves into feature interaction within a 3D local window (tubelet) through self-attention, and executes cross-attention between tubelet and its low-resolution counterpart to guide temporal feature alignment.","Extensive experiments conducted on the REDS4 and Vid4 datasets demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.17000v1","category":"cs.CV"}
{"created":"2024-03-25 17:54:21","title":"Complex-Valued Signal Recovery using the Bayesian LASSO","abstract":"Recovering complex-valued image recovery from noisy indirect data is important in applications such as ultrasound imaging and synthetic aperture radar. While there are many effective algorithms to recover point estimates of the magnitude, fewer are designed to recover the phase. Quantifying uncertainty in the estimate can also provide valuable information for real-time decision making. This investigation therefore proposes a new Bayesian inference method that recovers point estimates while also quantifying the uncertainty for complex-valued signals or images given noisy and indirect observation data. Our method is motivated by the Bayesian LASSO approach for real-valued sparse signals, and here we demonstrate that the Bayesian LASSO can be effectively adapted to recover complex-valued images whose magnitude is sparse in some (e.g.~the gradient) domain. Numerical examples demonstrate our algorithm's robustness to noise as well as its computational efficiency.","sentences":["Recovering complex-valued image recovery from noisy indirect data is important in applications such as ultrasound imaging and synthetic aperture radar.","While there are many effective algorithms to recover point estimates of the magnitude, fewer are designed to recover the phase.","Quantifying uncertainty in the estimate can also provide valuable information for real-time decision making.","This investigation therefore proposes a new Bayesian inference method that recovers point estimates while also quantifying the uncertainty for complex-valued signals or images given noisy and indirect observation data.","Our method is motivated by the Bayesian LASSO approach for real-valued sparse signals, and here we demonstrate that the Bayesian LASSO can be effectively adapted to recover complex-valued images whose magnitude is sparse in some (e.g.~the gradient) domain.","Numerical examples demonstrate our algorithm's robustness to noise as well as its computational efficiency."],"url":"http://arxiv.org/abs/2403.16992v1","category":"math.NA"}
{"created":"2024-03-25 17:48:06","title":"Dynamic Relative Representations for Goal-Oriented Semantic Communications","abstract":"In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating mismatches among devices, while optimizing energy consumption, delay, and effectiveness.","sentences":["In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions.","However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding.","In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data.","This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment.","We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications.","Numerical results demonstrate our methodology's effectiveness in mitigating mismatches among devices, while optimizing energy consumption, delay, and effectiveness."],"url":"http://arxiv.org/abs/2403.16986v1","category":"cs.NI"}
{"created":"2024-03-25 17:46:51","title":"Towards Low-Latency and Energy-Efficient Hybrid P2P-CDN Live Video Streaming","abstract":"Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an increasingly popular approach in both live and video-on-demand (VoD) applications. However, designing a scalable and adaptable framework that reduces servers energy consumption and supports low latency and high quality services, particularly for live video streaming scenarios, is still challenging for Over-The-Top (OTT) service providers. To address such challenges, this paper introduces a new hybrid P2P-CDN framework that leverages new networking and computing paradigms, i.e., Network Function Virtualization (NFV) and edge computing for live video streaming. The proposed framework introduces a multi-layer architecture and a tree of possible actions therein (an action tree), taking into account all available resources from peers, edge, and CDN servers to efficiently distribute video fetching and transcoding tasks across a hybrid P2P-CDN network, consequently enhancing the users latency and video quality. We also discuss our testbed designed to validate the framework and compare it with baseline methods. The experimental results indicate that the proposed framework improves user Quality of Experience (QoE), reduces client serving latency, and improves edge server energy consumption compared to baseline approaches.","sentences":["Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an increasingly popular approach in both live and video-on-demand (VoD) applications.","However, designing a scalable and adaptable framework that reduces servers energy consumption and supports low latency and high quality services, particularly for live video streaming scenarios, is still challenging for Over-The-Top (OTT) service providers.","To address such challenges, this paper introduces a new hybrid P2P-CDN framework that leverages new networking and computing paradigms, i.e., Network Function Virtualization (NFV) and edge computing for live video streaming.","The proposed framework introduces a multi-layer architecture and a tree of possible actions therein (an action tree), taking into account all available resources from peers, edge, and CDN servers to efficiently distribute video fetching and transcoding tasks across a hybrid P2P-CDN network, consequently enhancing the users latency and video quality.","We also discuss our testbed designed to validate the framework and compare it with baseline methods.","The experimental results indicate that the proposed framework improves user Quality of Experience (QoE), reduces client serving latency, and improves edge server energy consumption compared to baseline approaches."],"url":"http://arxiv.org/abs/2403.16985v1","category":"cs.MM"}
{"created":"2024-03-25 17:43:55","title":"Robust Filter Design for Graph Signals","abstract":"Our goal in this paper is the robust design of filters acting on signals observed over graphs subject to small perturbations of their edges. The focus is on developing a method to identify spectral and polynomial graph filters that can adapt to the perturbations in the underlying graph structure while ensuring the filters adhere to the desired spectral mask. To address this, we propose a novel approach that leverages approximate closed-form expressions for the perturbed eigendecomposition of the Laplacian matrix associated with the nominal topology. Furthermore, when dealing with noisy input signals for graph filters, we propose a strategy for designing FIR filters that jointly minimize the approximation error with respect to the ideal filter and the estimation error of the output, ensuring robustness against both graph perturbations and noise. Numerical results validate the effectiveness of our proposed strategies, highlighting their capability to efficiently manage perturbations and noise.","sentences":["Our goal in this paper is the robust design of filters acting on signals observed over graphs subject to small perturbations of their edges.","The focus is on developing a method to identify spectral and polynomial graph filters that can adapt to the perturbations in the underlying graph structure while ensuring the filters adhere to the desired spectral mask.","To address this, we propose a novel approach that leverages approximate closed-form expressions for the perturbed eigendecomposition of the Laplacian matrix associated with the nominal topology.","Furthermore, when dealing with noisy input signals for graph filters, we propose a strategy for designing FIR filters that jointly minimize the approximation error with respect to the ideal filter and the estimation error of the output, ensuring robustness against both graph perturbations and noise.","Numerical results validate the effectiveness of our proposed strategies, highlighting their capability to efficiently manage perturbations and noise."],"url":"http://arxiv.org/abs/2403.16983v1","category":"cs.DM"}
{"created":"2024-03-25 17:12:43","title":"Network-Assisted Delivery of Adaptive Video Streaming Services through CDN, SDN, and MEC","abstract":"Multimedia applications, mainly video streaming services, are currently the dominant source of network load worldwide. In recent Video-on-Demand (VoD) and live video streaming services, traditional streaming delivery techniques have been replaced by adaptive solutions based on the HTTP protocol. Current trends toward high-resolution (e.g., 8K) and/or low-latency VoD and live video streaming pose new challenges to end-to-end (E2E) bandwidth demand and have stringent delay requirements. To do this, video providers typically rely on Content Delivery Networks (CDNs) to ensure that they provide scalable video streaming services. To support future streaming scenarios involving millions of users, it is necessary to increase the CDNs' efficiency. It is widely agreed that these requirements may be satisfied by adopting emerging networking techniques to present Network-Assisted Video Streaming (NAVS) methods. Motivated by this, this thesis goes one step beyond traditional pure client-based HAS algorithms by incorporating (an) in-network component(s) with a broader view of the network to present completely transparent NAVS solutions for HAS clients.","sentences":["Multimedia applications, mainly video streaming services, are currently the dominant source of network load worldwide.","In recent Video-on-Demand (VoD) and live video streaming services, traditional streaming delivery techniques have been replaced by adaptive solutions based on the HTTP protocol.","Current trends toward high-resolution (e.g., 8K) and/or low-latency VoD and live video streaming pose new challenges to end-to-end (E2E) bandwidth demand and have stringent delay requirements.","To do this, video providers typically rely on Content Delivery Networks (CDNs) to ensure that they provide scalable video streaming services.","To support future streaming scenarios involving millions of users, it is necessary to increase the CDNs' efficiency.","It is widely agreed that these requirements may be satisfied by adopting emerging networking techniques to present Network-Assisted Video Streaming (NAVS) methods.","Motivated by this, this thesis goes one step beyond traditional pure client-based HAS algorithms by incorporating (an) in-network component(s) with a broader view of the network to present completely transparent NAVS solutions for HAS clients."],"url":"http://arxiv.org/abs/2403.16951v1","category":"cs.MM"}
{"created":"2024-03-25 17:10:39","title":"Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling","abstract":"Reinforcement Learning (RL)-based recommender systems have demonstrated promising performance in meeting user expectations by learning to make accurate next-item recommendations from historical user-item interactions. However, existing offline RL-based sequential recommendation methods face the challenge of obtaining effective user feedback from the environment. Effectively modeling the user state and shaping an appropriate reward for recommendation remains a challenge. In this paper, we leverage language understanding capabilities and adapt large language models (LLMs) as an environment (LE) to enhance RL-based recommenders. The LE is learned from a subset of user-item interaction data, thus reducing the need for large training data, and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately capture nuanced user preferences on actions. Moreover, the LE allows to generate positive actions that augment the limited offline training data. We propose a LE Augmentation (LEA) method to further improve recommendation performance by optimising jointly the supervised component and the RL policy, using the augmented actions and historical user signals. We use LEA, the state and reward models in conjunction with state-of-the-art RL recommenders and report experimental results on two publicly available datasets.","sentences":["Reinforcement Learning (RL)-based recommender systems have demonstrated promising performance in meeting user expectations by learning to make accurate next-item recommendations from historical user-item interactions.","However, existing offline RL-based sequential recommendation methods face the challenge of obtaining effective user feedback from the environment.","Effectively modeling the user state and shaping an appropriate reward for recommendation remains a challenge.","In this paper, we leverage language understanding capabilities and adapt large language models (LLMs) as an environment (LE) to enhance RL-based recommenders.","The LE is learned from a subset of user-item interaction data, thus reducing the need for large training data, and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately capture nuanced user preferences on actions.","Moreover, the LE allows to generate positive actions that augment the limited offline training data.","We propose a LE Augmentation (LEA) method to further improve recommendation performance by optimising jointly the supervised component and the RL policy, using the augmented actions and historical user signals.","We use LEA, the state and reward models in conjunction with state-of-the-art RL recommenders and report experimental results on two publicly available datasets."],"url":"http://arxiv.org/abs/2403.16948v1","category":"cs.IR"}
{"created":"2024-03-25 16:57:02","title":"Backpropagation through space, time, and the brain","abstract":"Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of BPTT in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, performing an effective spatiotemporal convolution. For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint states necessary for useful parameter updates.","sentences":["Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task.","However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality.","How such networks can perform efficient credit assignment, remains, to a large extent, an open question.","In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT).","However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints.","We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons.","We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent.","The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of BPTT in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity.","In particular, GLE exploits the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation.","For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, performing an effective spatiotemporal convolution.","For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint states necessary for useful parameter updates."],"url":"http://arxiv.org/abs/2403.16933v1","category":"q-bio.NC"}
{"created":"2024-03-25 16:47:36","title":"Stochastic Active Discretizations for Accelerating Temporal Uncertainty Management of Gas Pipeline Loads","abstract":"We propose a predictor-corrector adaptive method for the simulation of hyperbolic partial differential equations (PDEs) on networks under general uncertainty in parameters, initial conditions, or boundary conditions. The approach is based on the stochastic finite volume (SFV) framework that circumvents sampling schemes or simulation ensembles while also preserving fundamental properties, in particular hyperbolicity of the resulting systems and conservation of the discrete solutions. The initial boundary value problem (IBVP) on a set of network-connected one-dimensional domains that represent a pipeline is represented using active discretization of the physical and stochastic spaces, and we evaluate the propagation of uncertainty through network nodes by solving a junction Riemann problem. The adaptivity of our method in refining discretization based on error metrics enables computationally tractable evaluation of intertemporal uncertainty in order to support decisions about timing and quantity of pipeline operations to maximize delivery under transient and uncertain conditions. We illustrate our computational method using simulations for a representative network.","sentences":["We propose a predictor-corrector adaptive method for the simulation of hyperbolic partial differential equations (PDEs) on networks under general uncertainty in parameters, initial conditions, or boundary conditions.","The approach is based on the stochastic finite volume (SFV) framework that circumvents sampling schemes or simulation ensembles while also preserving fundamental properties, in particular hyperbolicity of the resulting systems and conservation of the discrete solutions.","The initial boundary value problem (IBVP) on a set of network-connected one-dimensional domains that represent a pipeline is represented using active discretization of the physical and stochastic spaces, and we evaluate the propagation of uncertainty through network nodes by solving a junction Riemann problem.","The adaptivity of our method in refining discretization based on error metrics enables computationally tractable evaluation of intertemporal uncertainty in order to support decisions about timing and quantity of pipeline operations to maximize delivery under transient and uncertain conditions.","We illustrate our computational method using simulations for a representative network."],"url":"http://arxiv.org/abs/2403.16929v1","category":"math.NA"}
{"created":"2024-03-25 16:31:56","title":"Solving the unique continuation problem for Schr\u00f6dinger equations with low regularity solutions using a stabilized finite element method","abstract":"In this paper, we consider the unique continuation problem for the Schr\\\"odinger equations. We prove a H\\\"older type conditional stability estimate and build up a parameterized stabilized finite element scheme adaptive to the \\textit{a priori} knowledge of the solution, achieving error estimates in interior domains with convergence up to continuous stability. The approximability of the scheme to solutions with only $H^1$-regularity is studied and the convergence rate for solutions with regularity higher than $H^1$ is also shown. Comparisons in terms of different parameterization for different regularities will be illustrated with respect to the convergence and condition numbers of the linear systems. Finally, numerical experiments will be given to illustrate the theory.","sentences":["In this paper, we consider the unique continuation problem for the Schr\\\"odinger equations.","We prove a H\\\"older type conditional stability estimate and build up a parameterized stabilized finite element scheme adaptive to the \\textit{a priori} knowledge of the solution, achieving error estimates in interior domains with convergence up to continuous stability.","The approximability of the scheme to solutions with only $H^1$-regularity is studied and the convergence rate for solutions with regularity higher than $H^1$ is also shown.","Comparisons in terms of different parameterization for different regularities will be illustrated with respect to the convergence and condition numbers of the linear systems.","Finally, numerical experiments will be given to illustrate the theory."],"url":"http://arxiv.org/abs/2403.16914v1","category":"math.NA"}
{"created":"2024-03-25 16:31:55","title":"New Intent Discovery with Attracting and Dispersing Prototype","abstract":"New Intent Discovery (NID) aims to recognize known and infer new intent categories with the help of limited labeled and large-scale unlabeled data. The task is addressed as a feature-clustering problem and recent studies augment instance representation. However, existing methods fail to capture cluster-friendly representations, since they show less capability to effectively control and coordinate within-cluster and between-cluster distances. Tailored to the NID problem, we propose a Robust and Adaptive Prototypical learning (RAP) framework for globally distinct decision boundaries for both known and new intent categories. Specifically, a robust prototypical attracting learning (RPAL) method is designed to compel instances to gravitate toward their corresponding prototype, achieving greater within-cluster compactness. To attain larger between-cluster separation, another adaptive prototypical dispersing learning (APDL) method is devised to maximize the between-cluster distance from the prototype-to-prototype perspective. Experimental results evaluated on three challenging benchmarks (CLINC, BANKING, and StackOverflow) of our method with better cluster-friendly representation demonstrate that RAP brings in substantial improvements over the current state-of-the-art methods (even large language model) by a large margin (average +5.5% improvement).","sentences":["New Intent Discovery (NID) aims to recognize known and infer new intent categories with the help of limited labeled and large-scale unlabeled data.","The task is addressed as a feature-clustering problem and recent studies augment instance representation.","However, existing methods fail to capture cluster-friendly representations, since they show less capability to effectively control and coordinate within-cluster and between-cluster distances.","Tailored to the NID problem, we propose a Robust and Adaptive Prototypical learning (RAP) framework for globally distinct decision boundaries for both known and new intent categories.","Specifically, a robust prototypical attracting learning (RPAL) method is designed to compel instances to gravitate toward their corresponding prototype, achieving greater within-cluster compactness.","To attain larger between-cluster separation, another adaptive prototypical dispersing learning (APDL) method is devised to maximize the between-cluster distance from the prototype-to-prototype perspective.","Experimental results evaluated on three challenging benchmarks (CLINC, BANKING, and StackOverflow) of our method with better cluster-friendly representation demonstrate that RAP brings in substantial improvements over the current state-of-the-art methods (even large language model) by a large margin (average +5.5% improvement)."],"url":"http://arxiv.org/abs/2403.16913v1","category":"cs.CL"}
{"created":"2024-03-26 17:58:07","title":"Towards Explaining Hypercomplex Neural Networks","abstract":"Hypercomplex neural networks are gaining increasing interest in the deep learning community. The attention directed towards hypercomplex models originates from several aspects, spanning from purely theoretical and mathematical characteristics to the practical advantage of lightweight models over conventional networks, and their unique properties to capture both global and local relations. In particular, a branch of these architectures, parameterized hypercomplex neural networks (PHNNs), has also gained popularity due to their versatility across a multitude of application domains. Nonetheless, only few attempts have been made to explain or interpret their intricacies. In this paper, we propose inherently interpretable PHNNs and quaternion-like networks, thus without the need for any post-hoc method. To achieve this, we define a type of cosine-similarity transform within the parameterized hypercomplex domain. This PHB-cos transform induces weight alignment with relevant input features and allows to reduce the model into a single linear transform, rendering it directly interpretable. In this work, we start to draw insights into how this unique branch of neural models operates. We observe that hypercomplex networks exhibit a tendency to concentrate on the shape around the main object of interest, in addition to the shape of the object itself. We provide a thorough analysis, studying single neurons of different layers and comparing them against how real-valued networks learn. The code of the paper is available at https://github.com/ispamm/HxAI.","sentences":["Hypercomplex neural networks are gaining increasing interest in the deep learning community.","The attention directed towards hypercomplex models originates from several aspects, spanning from purely theoretical and mathematical characteristics to the practical advantage of lightweight models over conventional networks, and their unique properties to capture both global and local relations.","In particular, a branch of these architectures, parameterized hypercomplex neural networks (PHNNs), has also gained popularity due to their versatility across a multitude of application domains.","Nonetheless, only few attempts have been made to explain or interpret their intricacies.","In this paper, we propose inherently interpretable PHNNs and quaternion-like networks, thus without the need for any post-hoc method.","To achieve this, we define a type of cosine-similarity transform within the parameterized hypercomplex domain.","This PHB-cos transform induces weight alignment with relevant input features and allows to reduce the model into a single linear transform, rendering it directly interpretable.","In this work, we start to draw insights into how this unique branch of neural models operates.","We observe that hypercomplex networks exhibit a tendency to concentrate on the shape around the main object of interest, in addition to the shape of the object itself.","We provide a thorough analysis, studying single neurons of different layers and comparing them against how real-valued networks learn.","The code of the paper is available at https://github.com/ispamm/HxAI."],"url":"http://arxiv.org/abs/2403.17929v1","category":"cs.CV"}
{"created":"2024-03-26 17:55:11","title":"TC4D: Trajectory-Conditioned Text-to-4D Generation","abstract":"Recent techniques for text-to-4D generation synthesize dynamic 3D scenes using supervision from pre-trained text-to-video models. However, existing representations for motion, such as deformation models or time-dependent neural representations, are limited in the amount of motion they can generate-they cannot synthesize motion extending far beyond the bounding box used for volume rendering. The lack of a more flexible motion model contributes to the gap in realism between 4D generation methods and recent, near-photorealistic video generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D generation, which factors motion into global and local components. We represent the global motion of a scene's bounding box using rigid transformation along a trajectory parameterized by a spline. We learn local deformations that conform to the global trajectory using supervision from a text-to-video model. Our approach enables the synthesis of scenes animated along arbitrary trajectories, compositional scene generation, and significant improvements to the realism and amount of generated motion, which we evaluate qualitatively and through a user study. Video results can be viewed on our website: https://sherwinbahmani.github.io/tc4d.","sentences":["Recent techniques for text-to-4D generation synthesize dynamic 3D scenes using supervision from pre-trained text-to-video models.","However, existing representations for motion, such as deformation models or time-dependent neural representations, are limited in the amount of motion they can generate-they cannot synthesize motion extending far beyond the bounding box used for volume rendering.","The lack of a more flexible motion model contributes to the gap in realism between 4D generation methods and recent, near-photorealistic video generation models.","Here, we propose TC4D: trajectory-conditioned text-to-4D generation, which factors motion into global and local components.","We represent the global motion of a scene's bounding box using rigid transformation along a trajectory parameterized by a spline.","We learn local deformations that conform to the global trajectory using supervision from a text-to-video model.","Our approach enables the synthesis of scenes animated along arbitrary trajectories, compositional scene generation, and significant improvements to the realism and amount of generated motion, which we evaluate qualitatively and through a user study.","Video results can be viewed on our website: https://sherwinbahmani.github.io/tc4d."],"url":"http://arxiv.org/abs/2403.17920v1","category":"cs.CV"}
{"created":"2024-03-26 17:45:30","title":"WKB asymptotics of Stokes matrices, spectral curves and rhombus inequalities","abstract":"We consider an $n\\times n$ system of ODEs on $\\mathbb{P}^1$ with a simple pole $A$ at $z=0$ and a double pole $u={\\rm diag}(u_1, \\dots, u_n)$ at $z=\\infty$. This is the simplest situation in which the monodromy data of the system are described by upper and lower triangular Stokes matrices $S_\\pm$, and we impose reality conditions which imply $S_-=S_+^\\dagger$. We study leading WKB exponents of Stokes matrices in parametrizations given by generalized minors and by spectral coordinates, and we show that for $u$ on the caterpillar line (which corresponds to the limit $(u_{j+1}-u_j)/(u_j - u_{j-1}) \\to \\infty$ for $j=2, \\cdots, n-1$), the real parts of these exponents are given by periods of certain cycles on the degenerate spectral curve $\\Gamma(u_{\\rm cat}(t), A)$.   These cycles admit unique deformations for $u$ near the caterpillar line. Using the spectral network theory, we give for $n=2$, and $n=3$ exact WKB predictions for asymptotics of generalized minors in terms of periods of these cycles. Boalch's theorem from Poisson geometry implies that real parts of leading WKB exponents satisfy the rhombus (or interlacing) inequalities. We show that these inequalities are in correspondence with finite webs of the canonical foliation on the root curve $\\Gamma^r(u, A)$, and that they follow from the positivity of the corresponding periods. We conjecture that a similar mechanism applies for $n>3$.   We also outline the relation of the spectral coordinates with the cluster structures considered by Goncharov-Shen, and with ${\\mathcal N}=2$ supersymmetric quantum field theories in dimension four associated with some simple quivers.","sentences":["We consider an $n\\times n$ system of ODEs on $\\mathbb{P}^1$ with a simple pole $A$ at $z=0$ and a double pole $u={\\rm diag}(u_1, \\dots, u_n)$ at $z=\\infty$. This is the simplest situation in which the monodromy data of the system are described by upper and lower triangular Stokes matrices $S_\\pm$, and we impose reality conditions which imply $S_-=S_+^\\dagger$. We study leading WKB exponents of Stokes matrices in parametrizations given by generalized minors and by spectral coordinates, and we show that for $u$ on the caterpillar line (which corresponds to the limit $(u_{j+1}-u_j)/(u_j - u_{j-1}) \\to \\infty$ for $j=2, \\cdots, n-1$), the real parts of these exponents are given by periods of certain cycles on the degenerate spectral curve $\\Gamma(u_{\\rm cat}(t), A)$.   These cycles admit unique deformations for $u$ near the caterpillar line.","Using the spectral network theory, we give for $n=2$, and $n=3$ exact WKB predictions for asymptotics of generalized minors in terms of periods of these cycles.","Boalch's theorem from Poisson geometry implies that real parts of leading WKB exponents satisfy the rhombus (or interlacing) inequalities.","We show that these inequalities are in correspondence with finite webs of the canonical foliation on the root curve $\\Gamma^r(u, A)$, and that they follow from the positivity of the corresponding periods.","We conjecture that a similar mechanism applies for $n>3$.   We also outline the relation of the spectral coordinates with the cluster structures considered by Goncharov-Shen, and with ${\\mathcal N}=2$ supersymmetric quantum field theories in dimension four associated with some simple quivers."],"url":"http://arxiv.org/abs/2403.17906v1","category":"math-ph"}
{"created":"2024-03-26 17:45:06","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","abstract":"We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)\" approach recently introduced in astronomical imaging. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation, considering radial k-space sampling acquisition sequences. Our preliminary results suggest that R2D2 achieves: (i) suboptimal performance compared to its unrolled incarnation R2D2-Net, which is however non-scalable due to the necessary embedding of NUFFT-based data-consistency layers; (ii) superior reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based approximation for data consistency; (iii) superior reconstruction quality to PnP, while only requiring few iterations.","sentences":["We propose a new approach for non-Cartesian magnetic resonance image reconstruction.","While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale.","Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability.","To address this scalability challenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)\" approach recently introduced in astronomical imaging.","R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs.","The method can be interpreted as a learned version of the Matching Pursuit algorithm.","We demonstrate R2D2 in simulation, considering radial k-space sampling acquisition sequences.","Our preliminary results suggest that R2D2 achieves: (i) suboptimal performance compared to its unrolled incarnation R2D2-Net, which is however non-scalable due to the necessary embedding of NUFFT-based data-consistency layers; (ii) superior reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based approximation for data consistency; (iii) superior reconstruction quality to PnP, while only requiring few iterations."],"url":"http://arxiv.org/abs/2403.17905v2","category":"eess.IV"}
{"created":"2024-03-26 17:41:48","title":"A severe local flood and social events show a similar impact on human mobility","abstract":"While a social event, such as a concert or a food festival, is a common experience to people, a natural disaster is experienced by a fewer individuals. The ordinary and common ground experience of social events could be therefore used to better understand the complex impacts of uncommon, but devastating natural events on society, such as floods. Based on this idea, we present a comparison - in terms of human mobility -, between an extreme local flood that occurred in 2017 in Switzerland, and social events which took place in the same region, in the weeks before and after the inundation. Using mobile phone location data, we show that the severe local flood and social events have a similar impact on human mobility, both at the national scale and at a local scale. At the national level, we found a small difference between the distributions of visitors and their travelled distances among the several weeks in which the events took place. At the local level, instead, we detected the anomalies (in time series) in the number of people travelling each road and railway, and we found that the distributions of anomalies, and of their clusters, are comparable between the flood and the social events. Hence, our findings suggest that the knowledge on ubiquitous social events can be employed to characterise the impacts of rare natural disasters on human mobility. The proposed methods at the local level can thus be used to analyse the disturbances in complex spatial networks and, in general, as complementary approaches for the analyses of complex systems.","sentences":["While a social event, such as a concert or a food festival, is a common experience to people, a natural disaster is experienced by a fewer individuals.","The ordinary and common ground experience of social events could be therefore used to better understand the complex impacts of uncommon, but devastating natural events on society, such as floods.","Based on this idea, we present a comparison - in terms of human mobility -, between an extreme local flood that occurred in 2017 in Switzerland, and social events which took place in the same region, in the weeks before and after the inundation.","Using mobile phone location data, we show that the severe local flood and social events have a similar impact on human mobility, both at the national scale and at a local scale.","At the national level, we found a small difference between the distributions of visitors and their travelled distances among the several weeks in which the events took place.","At the local level, instead, we detected the anomalies (in time series) in the number of people travelling each road and railway, and we found that the distributions of anomalies, and of their clusters, are comparable between the flood and the social events.","Hence, our findings suggest that the knowledge on ubiquitous social events can be employed to characterise the impacts of rare natural disasters on human mobility.","The proposed methods at the local level can thus be used to analyse the disturbances in complex spatial networks and, in general, as complementary approaches for the analyses of complex systems."],"url":"http://arxiv.org/abs/2403.17899v1","category":"physics.soc-ph"}
{"created":"2024-03-26 17:33:54","title":"Global asymptotics for $\u03b2$-Krawtchouk corners processes via multi-level loop equations","abstract":"We introduce a two-parameter family of probability distributions, indexed by $\\beta/2 = \\theta > 0$ and $K \\in \\mathbb{Z}_{\\geq 0}$, that are called $\\beta$-Krawtchouk corners processes. These measures are related to Jack symmetric functions, and can be thought of as integrable discretizations of $\\beta$-corners processes from random matrix theory, or alternatively as non-determinantal measures on lozenge tilings of infinite domains. We show that as $K$ tends to infinity the height function of these models concentrates around an explicit limit shape, and prove that its fluctuations are asymptotically described by a pull-back of the Gaussian free field, which agrees with the one for Wigner matrices. The main tools we use to establish our results are certain multi-level loop equations introduced in our earlier work arXiv:2108.07710.","sentences":["We introduce a two-parameter family of probability distributions, indexed by $\\beta/2 = \\theta > 0$ and $K \\in \\mathbb{Z}_{\\geq 0}$, that are called $\\beta$-Krawtchouk corners processes.","These measures are related to Jack symmetric functions, and can be thought of as integrable discretizations of $\\beta$-corners processes from random matrix theory, or alternatively as non-determinantal measures on lozenge tilings of infinite domains.","We show that as $K$ tends to infinity the height function of these models concentrates around an explicit limit shape, and prove that its fluctuations are asymptotically described by a pull-back of the Gaussian free field, which agrees with the one for Wigner matrices.","The main tools we use to establish our results are certain multi-level loop equations introduced in our earlier work arXiv:2108.07710."],"url":"http://arxiv.org/abs/2403.17895v1","category":"math.PR"}
{"created":"2024-03-26 17:22:27","title":"Searching for large dark matter clumps using the Galileo Satnav clock variations","abstract":"This study presents bounds on transient variations of fundamental constants, with typical timescales ranging from minutes to months, using clocks in space. The underlying phenomenology describing such transient variations relies on models for Dark Matter (DM) which suggest possible encounters of macroscopic compact objects with the Earth, due to the motion of the solar system in the galactic halo. If such compact objects possess an effective feeble interaction with the ordinary matter beyond the gravitational one, it may result in effective transient variations of fundamental constants. Such variations leave signatures on clocks onboard GNSS satellites. In this paper, we introduce a phenomenological study dedicated to the search for such DM transient objects using the network of passive hydrogen masers (H-Masers) onboard Galileo satellites. We first model the signature of transient variations of fundamental constants as a frequency modulation in the difference between two satellite clocks, considering the satellite trajectories relative to the transient event. Then, we present first results based on a fast analysis method, the maximum reach analysis. The main result is a significant extension of the discovery range for DM transients, with a sensitivity never achieved before. We investigate indeed the range of transient sizes from $10^5$ to $10^9$ kilometres, which, apart from indirect and model-dependent non-transient effects, has never been explored previously.","sentences":["This study presents bounds on transient variations of fundamental constants, with typical timescales ranging from minutes to months, using clocks in space.","The underlying phenomenology describing such transient variations relies on models for Dark Matter (DM) which suggest possible encounters of macroscopic compact objects with the Earth, due to the motion of the solar system in the galactic halo.","If such compact objects possess an effective feeble interaction with the ordinary matter beyond the gravitational one, it may result in effective transient variations of fundamental constants.","Such variations leave signatures on clocks onboard GNSS satellites.","In this paper, we introduce a phenomenological study dedicated to the search for such DM transient objects using the network of passive hydrogen masers (H-Masers) onboard Galileo satellites.","We first model the signature of transient variations of fundamental constants as a frequency modulation in the difference between two satellite clocks, considering the satellite trajectories relative to the transient event.","Then, we present first results based on a fast analysis method, the maximum reach analysis.","The main result is a significant extension of the discovery range for DM transients, with a sensitivity never achieved before.","We investigate indeed the range of transient sizes from $10^5$ to $10^9$ kilometres, which, apart from indirect and model-dependent non-transient effects, has never been explored previously."],"url":"http://arxiv.org/abs/2403.17890v1","category":"astro-ph.CO"}
{"created":"2024-03-26 17:21:24","title":"2D Gaussian Splatting for Geometrically Accurate Radiance Fields","abstract":"3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering. Our code will be made publicly available.","sentences":["3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking.","However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians.","We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images.","Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks.","Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically.","To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization.","Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions.","We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering.","Our code will be made publicly available."],"url":"http://arxiv.org/abs/2403.17888v1","category":"cs.CV"}
{"created":"2024-03-26 17:11:51","title":"Low-Latency Neural Stereo Streaming","abstract":"The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.","sentences":["The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime.","While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance.","This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming.","Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding.","Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs."],"url":"http://arxiv.org/abs/2403.17879v1","category":"cs.CV"}
{"created":"2024-03-26 17:03:17","title":"Scaling Mixed-Integer Programming for Certification of Neural Network Controllers Using Bounds Tightening","abstract":"Neural networks offer a computationally efficient approximation of model predictive control, but they lack guarantees on the resulting controlled system's properties. Formal certification of neural networks is crucial for ensuring safety, particularly in safety-critical domains such as autonomous vehicles. One approach to formally certify properties of neural networks is to solve a mixed-integer program based on the network. This approach suffers from scalability issues due to the complexity of solving the resulting mixed-integer programs. Nevertheless, these issues can be (partially) mitigated via bound-tightening techniques prior to forming the mixed-integer program, which results in tighter formulations and faster optimisation. This paper presents bound-tightening techniques in the context of neural network explicit control policies. Bound tightening is particularly important when considering problems spanning multiple time steps of a controlled system, as the bounds must be propagated through the problem depth. Several strategies for bound tightening are evaluated in terms of both computational complexity and tightness of the bounds.","sentences":["Neural networks offer a computationally efficient approximation of model predictive control, but they lack guarantees on the resulting controlled system's properties.","Formal certification of neural networks is crucial for ensuring safety, particularly in safety-critical domains such as autonomous vehicles.","One approach to formally certify properties of neural networks is to solve a mixed-integer program based on the network.","This approach suffers from scalability issues due to the complexity of solving the resulting mixed-integer programs.","Nevertheless, these issues can be (partially) mitigated via bound-tightening techniques prior to forming the mixed-integer program, which results in tighter formulations and faster optimisation.","This paper presents bound-tightening techniques in the context of neural network explicit control policies.","Bound tightening is particularly important when considering problems spanning multiple time steps of a controlled system, as the bounds must be propagated through the problem depth.","Several strategies for bound tightening are evaluated in terms of both computational complexity and tightness of the bounds."],"url":"http://arxiv.org/abs/2403.17874v1","category":"math.OC"}
{"created":"2024-03-26 16:45:58","title":"Ill-posedness of the hydrostatic Euler-Boussinesq equations and failure of hydrostatic limit","abstract":"We investigate the hydrostatic approximation for inviscid stratified fluids, described by the two-dimensional Euler-Boussinesq equations in a periodic channel. Through a perturbative analysis of the hydrostatic homogeneous setting, we exhibit a stratified steady state violating the Miles-Howard criterion and generating a growing mode, both for the linearized hydrostatic and non-hydrostatic equations. By leveraging long-wave nonlinear instability for the original Euler-Boussinesq system, we demonstrate the breakdown of the hydrostatic limit around such unstable profiles. Finally, we establish the generic nonlinear ill-posedness of the limiting hydrostatic system in Sobolev spaces.","sentences":["We investigate the hydrostatic approximation for inviscid stratified fluids, described by the two-dimensional Euler-Boussinesq equations in a periodic channel.","Through a perturbative analysis of the hydrostatic homogeneous setting, we exhibit a stratified steady state violating the Miles-Howard criterion and generating a growing mode, both for the linearized hydrostatic and non-hydrostatic equations.","By leveraging long-wave nonlinear instability for the original Euler-Boussinesq system, we demonstrate the breakdown of the hydrostatic limit around such unstable profiles.","Finally, we establish the generic nonlinear ill-posedness of the limiting hydrostatic system in Sobolev spaces."],"url":"http://arxiv.org/abs/2403.17857v1","category":"math.AP"}
{"created":"2024-03-26 16:42:30","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic","abstract":"Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.","sentences":["Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog.","It is a critical component for modern dialog system design and discourse analysis.","Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain.","This work explores a neural-symbolic approach as a potential solution to these problems.","We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model.","We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance.","Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines."],"url":"http://arxiv.org/abs/2403.17853v1","category":"cs.CL"}
{"created":"2024-03-26 16:14:15","title":"The memory of Rayleigh-Taylor turbulence","abstract":"In this work, we consider the problem of inferring the initial conditions of a Rayleigh-Taylor mixing zone by measuring the 0D turbulent quantities at an unspecified time. To this aim, we have generated a comprehensive dataset through direct numerical simulations (DNS), focusing on miscible fluids with slight density contrasts. The initial interface deformations in these simulations are characterized by an annular spectrum which is parametrized by four non dimensional numbers. %In order to study the sensitivity of 0D turbulent quantities to the initial interface perturbation distributions, we build a surrogate model for the simulations using a physics-informed neural network (PINN). This allows us to compute the Sobol indices for the turbulent quantities, disentangling the effects of the initial parameters on the growth of the mixing layer. Within a Bayesian framework, we use a Markov chain Monte-Carlo method to determine the posterior distributions of initial conditions given various state variables. %This sheds light on the inertial or diffusive trajectories along with how the initial conditions are progressively forgotten during transition to turbulence. Moreover, it identifies which turbulent quantities are better predictors for the dynamics of Rayleigh-Taylor mixing zones by more effectively retaining the memory of the flow. By inferring the initial conditions and forward propagating its maximum a posteriori (MAP) estimate, we propose a strategy to model the Rayleigh-Taylor transition to turbulence.","sentences":["In this work, we consider the problem of inferring the initial conditions of a Rayleigh-Taylor mixing zone by measuring the 0D turbulent quantities at an unspecified time.","To this aim, we have generated a comprehensive dataset through direct numerical simulations (DNS), focusing on miscible fluids with slight density contrasts.","The initial interface deformations in these simulations are characterized by an annular spectrum which is parametrized by four non dimensional numbers.","%In order to study the sensitivity of 0D turbulent quantities to the initial interface perturbation distributions, we build a surrogate model for the simulations using a physics-informed neural network (PINN).","This allows us to compute the Sobol indices for the turbulent quantities, disentangling the effects of the initial parameters on the growth of the mixing layer.","Within a Bayesian framework, we use a Markov chain Monte-Carlo method to determine the posterior distributions of initial conditions given various state variables.","%This sheds light on the inertial or diffusive trajectories along with how the initial conditions are progressively forgotten during transition to turbulence.","Moreover, it identifies which turbulent quantities are better predictors for the dynamics of Rayleigh-Taylor mixing zones by more effectively retaining the memory of the flow.","By inferring the initial conditions and forward propagating its maximum a posteriori (MAP) estimate, we propose a strategy to model the Rayleigh-Taylor transition to turbulence."],"url":"http://arxiv.org/abs/2403.17832v1","category":"physics.flu-dyn"}
{"created":"2024-03-26 16:00:31","title":"DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing","abstract":"3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in https://github.com/maturk/dn-splatter.","sentences":["3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times.","However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization.","We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application.","Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry.","We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes.","Our code will be released in https://github.com/maturk/dn-splatter."],"url":"http://arxiv.org/abs/2403.17822v1","category":"cs.CV"}
{"created":"2024-03-26 15:57:16","title":"Multiple solutions for quasilinear elliptic problems with concave and convex nonlinearities","abstract":"We prove the existence of multiple signed bounded solutions for a quasilinear elliptic equation with concave and convex nonlinearities. For this, we use a variational approach in an intersection Banach space indroduced by Candela and Palmieri, and a truncation technique given by Garcia Azorero and Peral.","sentences":["We prove the existence of multiple signed bounded solutions for a quasilinear elliptic equation with concave and convex nonlinearities.","For this, we use a variational approach in an intersection Banach space indroduced by Candela and Palmieri, and a truncation technique given by Garcia Azorero and Peral."],"url":"http://arxiv.org/abs/2403.17821v1","category":"math.AP"}
{"created":"2024-03-26 15:40:44","title":"Stabilization for degenerate equations with drift and small singular term","abstract":"We consider a degenerate/singular wave equation in one dimension, with drift and in presence of a leading operator which is not in divergence form. We impose a homogeneous Dirichlet boundary condition where the degeneracy occurs and a boundary damping at the other endpoint. We provide some conditions for the uniform exponential decay of solutions for the associated Cauchy problem.","sentences":["We consider a degenerate/singular wave equation in one dimension, with drift and in presence of a leading operator which is not in divergence form.","We impose a homogeneous Dirichlet boundary condition where the degeneracy occurs and a boundary damping at the other endpoint.","We provide some conditions for the uniform exponential decay of solutions for the associated Cauchy problem."],"url":"http://arxiv.org/abs/2403.17802v1","category":"math.AP"}
{"created":"2024-03-26 15:27:24","title":"Neural Exponential Stabilization of Control-affine Nonlinear Systems","abstract":"This paper proposes a novel learning-based approach for achieving exponential stabilization of nonlinear control-affine systems. We leverage the Control Contraction Metrics (CCMs) framework to co-synthesize Neural Contraction Metrics (NCMs) and Neural Network (NN) controllers. First, we transform the infinite-dimensional semi-definite program (SDP) for CCM computation into a tractable inequality feasibility problem using element-wise bounds of matrix-valued functions. The terms in the inequality can be efficiently computed by our novel algorithms. Second, we propose a free parametrization of NCMs guaranteeing positive definiteness and the satisfaction of a partial differential equation, regardless of trainable parameters. Third, this parametrization and the inequality condition enable the design of contractivity-enforcing regularizers, which can be incorporated while designing the NN controller for exponential stabilization of the underlying nonlinear systems. Furthermore, when the training loss goes to zero, we provide formal guarantees on verification of the NCM and the exponentional stabilization under the NN controller. Finally, we validate our method through benchmark experiments on set-point stabilization and increasing the region of attraction of a locally pre-stabilized closed-loop system.","sentences":["This paper proposes a novel learning-based approach for achieving exponential stabilization of nonlinear control-affine systems.","We leverage the Control Contraction Metrics (CCMs) framework to co-synthesize Neural Contraction Metrics (NCMs) and Neural Network (NN) controllers.","First, we transform the infinite-dimensional semi-definite program (SDP) for CCM computation into a tractable inequality feasibility problem using element-wise bounds of matrix-valued functions.","The terms in the inequality can be efficiently computed by our novel algorithms.","Second, we propose a free parametrization of NCMs guaranteeing positive definiteness and the satisfaction of a partial differential equation, regardless of trainable parameters.","Third, this parametrization and the inequality condition enable the design of contractivity-enforcing regularizers, which can be incorporated while designing the NN controller for exponential stabilization of the underlying nonlinear systems.","Furthermore, when the training loss goes to zero, we provide formal guarantees on verification of the NCM and the exponentional stabilization under the NN controller.","Finally, we validate our method through benchmark experiments on set-point stabilization and increasing the region of attraction of a locally pre-stabilized closed-loop system."],"url":"http://arxiv.org/abs/2403.17793v1","category":"eess.SY"}
{"created":"2024-03-26 15:21:18","title":"A PAC-Bayesian Framework for Optimal Control with Stability Guarantees","abstract":"Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost function that averages out the random uncertainties affecting the dynamics of nonlinear systems. For tractability reasons, this problem is typically addressed by minimizing an empirical cost, which represents the average cost across a finite dataset of sampled disturbances. However, this approach raises the challenge of quantifying the control performance against out-of-sample uncertainties. Particularly, in scenarios where the training dataset is small, SNOC policies are prone to overfitting, resulting in significant discrepancies between the empirical cost and the true cost, i.e., the average SNOC cost incurred during control deployment. Therefore, establishing generalization bounds on the true cost is crucial for ensuring reliability in real-world applications. In this paper, we introduce a novel approach that leverages PAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on these bounds, we propose a new method for designing optimal controllers, offering a principled way to incorporate prior knowledge into the synthesis process, which aids in improving the control policy and mitigating overfitting. Furthermore, by leveraging recent parametrizations of stabilizing controllers for nonlinear systems, our framework inherently ensures closed-loop stability. The effectiveness of our proposed method in incorporating prior knowledge and combating overfitting is shown by designing neural network controllers for tasks in cooperative robotics.","sentences":["Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost function that averages out the random uncertainties affecting the dynamics of nonlinear systems.","For tractability reasons, this problem is typically addressed by minimizing an empirical cost, which represents the average cost across a finite dataset of sampled disturbances.","However, this approach raises the challenge of quantifying the control performance against out-of-sample uncertainties.","Particularly, in scenarios where the training dataset is small, SNOC policies are prone to overfitting, resulting in significant discrepancies between the empirical cost and the true cost, i.e., the average SNOC cost incurred during control deployment.","Therefore, establishing generalization bounds on the true cost is crucial for ensuring reliability in real-world applications.","In this paper, we introduce a novel approach that leverages PAC-Bayes theory to provide rigorous generalization bounds for SNOC.","Based on these bounds, we propose a new method for designing optimal controllers, offering a principled way to incorporate prior knowledge into the synthesis process, which aids in improving the control policy and mitigating overfitting.","Furthermore, by leveraging recent parametrizations of stabilizing controllers for nonlinear systems, our framework inherently ensures closed-loop stability.","The effectiveness of our proposed method in incorporating prior knowledge and combating overfitting is shown by designing neural network controllers for tasks in cooperative robotics."],"url":"http://arxiv.org/abs/2403.17790v1","category":"eess.SY"}
{"created":"2024-03-26 15:14:25","title":"Facet formation in slow three-dimensional fracture","abstract":"Cracks develop various surface patterns as they propagate in three-dimensional (3D) materials. Facet formation in nominally tensile (mode-I) fracture emerge in the slow, non-inertial regime and oftentimes takes the form of surface steps. We show that the same phase-field framework that recently shed basic light on dynamic (inertial) tensile fracture in 3D, also gives rise to crack surface steps. Step formation is shown to be an intrinsically nonlinear phenomenon that involves two essential physical ingredients: finite-strength quenched disorder and a small, mesoscopic anti-plane shear (mode-III) loading component (on top of the dominant tensile, mode-I loading component). We quantify the interplay between disorder (both its strength and spatial correlation length) and mesoscopic mode I+III mixity in controlling step formation. Finally, we show that surface steps grow out of the small-scale, background surface roughness and are composed of two overlapping crack segments connected by a bridging crack, in agreement with experiments.","sentences":["Cracks develop various surface patterns as they propagate in three-dimensional (3D) materials.","Facet formation in nominally tensile (mode-I) fracture emerge in the slow, non-inertial regime and oftentimes takes the form of surface steps.","We show that the same phase-field framework that recently shed basic light on dynamic (inertial) tensile fracture in 3D, also gives rise to crack surface steps.","Step formation is shown to be an intrinsically nonlinear phenomenon that involves two essential physical ingredients: finite-strength quenched disorder and a small, mesoscopic anti-plane shear (mode-III) loading component (on top of the dominant tensile, mode-I loading component).","We quantify the interplay between disorder (both its strength and spatial correlation length) and mesoscopic mode I+III mixity in controlling step formation.","Finally, we show that surface steps grow out of the small-scale, background surface roughness and are composed of two overlapping crack segments connected by a bridging crack, in agreement with experiments."],"url":"http://arxiv.org/abs/2403.17781v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 15:07:58","title":"Secure Aggregation is Not Private Against Membership Inference Attacks","abstract":"Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offers weak privacy against membership inference attacks even in a single training round. Indeed, it is difficult to hide a local update by adding other independent local updates when the updates are of high dimension. Our findings underscore the imperative for additional privacy-enhancing mechanisms, such as noise injection, in federated learning.","sentences":["Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates.","Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified.","In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update.","We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg.","By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg.","Our numerical results unveil that, contrary to prevailing claims, SecAgg offers weak privacy against membership inference attacks even in a single training round.","Indeed, it is difficult to hide a local update by adding other independent local updates when the updates are of high dimension.","Our findings underscore the imperative for additional privacy-enhancing mechanisms, such as noise injection, in federated learning."],"url":"http://arxiv.org/abs/2403.17775v1","category":"cs.LG"}
{"created":"2024-03-26 15:00:20","title":"Coupled Nonlinear Schr\u00f6dinger (CNLS) Equations for two interacting electrostatic wavepackets in a non-Maxwellian fluid plasma model","abstract":"The nonlinear dynamics of two co-propagating electrostatic wavepackets, characterized by different wavenumbers and amplitudes, in a 1D non-magnetized plasma fluid model is considered, from first principles. The original plasma model, consisting of \\kappa-distributed electrons evolving against a cold ion background, is reduced, by means of a multiple-scale perturbation method to a pair of asymmetric coupled nonlinear Schr\\\"odinger (CNLS) equations for the dynamics of the wavepacket envelopes.   Exact analytical expressions are derived for the dispersion, self-modulation, and cross-modulation coefficients involved in the CNLS equations, as functions of the wavenumbers and the spectral index \\kappa characterizing the electron profile. An analytical investigation of the modulational instability (MI) properties of this pair of wavepackets reveals that MI occurs in most parts of the parameter space.   The instability windows and the corresponding growth rate are calculated in a number of case studies. Two-wave interaction favors MI by extending its range of occurrence and by enhancing its growth rate. Growth rate patterns obtained for different \\kappa suggest that deviation from Maxwellian equilibrium, for low \\kappa values, leads to enhanced MI of the interacting wave pair.   To the best of our knowledge, the dynamics of two co-propagating wavepackets in a plasma described by a fluid model with \\kappa-distributed electrons is investigated thoroughly with respect to their MI properties as a function of \\kappa for the first time, in the framework of an asymmetric CNLS system. Although we have focused on electrostatic wavepacket propagation in non-Maxwellian plasma, the results are generic and may be used as basis to model energy localization in nonlinear optics, in hydrodynamics or in dispersive media with Kerr-type nonlinearities where MI is relevant.","sentences":["The nonlinear dynamics of two co-propagating electrostatic wavepackets, characterized by different wavenumbers and amplitudes, in a 1D non-magnetized plasma fluid model is considered, from first principles.","The original plasma model, consisting of \\kappa-distributed electrons evolving against a cold ion background, is reduced, by means of a multiple-scale perturbation method to a pair of asymmetric coupled nonlinear Schr\\\"odinger (CNLS) equations for the dynamics of the wavepacket envelopes.   ","Exact analytical expressions are derived for the dispersion, self-modulation, and cross-modulation coefficients involved in the CNLS equations, as functions of the wavenumbers and the spectral index \\kappa characterizing the electron profile.","An analytical investigation of the modulational instability (MI) properties of this pair of wavepackets reveals that MI occurs in most parts of the parameter space.   ","The instability windows and the corresponding growth rate are calculated in a number of case studies.","Two-wave interaction favors MI by extending its range of occurrence and by enhancing its growth rate.","Growth rate patterns obtained for different \\kappa suggest that deviation from Maxwellian equilibrium, for low \\kappa values, leads to enhanced MI of the interacting wave pair.   ","To the best of our knowledge, the dynamics of two co-propagating wavepackets in a plasma described by a fluid model with \\kappa-distributed electrons is investigated thoroughly with respect to their MI properties as a function of \\kappa for the first time, in the framework of an asymmetric CNLS system.","Although we have focused on electrostatic wavepacket propagation in non-Maxwellian plasma, the results are generic and may be used as basis to model energy localization in nonlinear optics, in hydrodynamics or in dispersive media with Kerr-type nonlinearities where MI is relevant."],"url":"http://arxiv.org/abs/2403.17772v1","category":"physics.plasm-ph"}
{"created":"2024-03-26 14:53:24","title":"MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash Representations","abstract":"We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing multiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM effectively tracks camera positions and incrementally builds a scalable multi-map representation for both small and large indoor environments. It dynamically allocates sub-maps for newly observed local regions, enabling constraint-free mapping without prior scene information. Unlike traditional grid-based methods, we use three orthogonal axis-aligned planes for hash-encoding scene properties, significantly reducing hash collisions and the number of trainable parameters. This hybrid approach not only speeds up convergence but also enhances the fidelity of surface reconstruction. Furthermore, our optimization strategy concurrently optimizes all sub-maps intersecting with the current camera frustum, ensuring global consistency. Extensive testing on both real-world and synthetic datasets has shown that MUTE-SLAM delivers state-of-the-art surface reconstruction quality and competitive tracking performance across diverse indoor settings. The code will be made public upon acceptance of the paper.","sentences":["We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing multiple tri-plane hash-encodings for efficient scene representation.","MUTE-SLAM effectively tracks camera positions and incrementally builds a scalable multi-map representation for both small and large indoor environments.","It dynamically allocates sub-maps for newly observed local regions, enabling constraint-free mapping without prior scene information.","Unlike traditional grid-based methods, we use three orthogonal axis-aligned planes for hash-encoding scene properties, significantly reducing hash collisions and the number of trainable parameters.","This hybrid approach not only speeds up convergence but also enhances the fidelity of surface reconstruction.","Furthermore, our optimization strategy concurrently optimizes all sub-maps intersecting with the current camera frustum, ensuring global consistency.","Extensive testing on both real-world and synthetic datasets has shown that MUTE-SLAM delivers state-of-the-art surface reconstruction quality and competitive tracking performance across diverse indoor settings.","The code will be made public upon acceptance of the paper."],"url":"http://arxiv.org/abs/2403.17765v1","category":"cs.CV"}
{"created":"2024-03-26 17:59:58","title":"Efficient Video Object Segmentation via Modulated Cross-Attention Memory","abstract":"Recently, transformer-based approaches have shown promising results for semi-supervised video object segmentation. However, these approaches typically struggle on long videos due to increased GPU memory demands, as they frequently expand the memory bank every few frames. We propose a transformer-based approach, named MAVOS, that introduces an optimized and dynamic long-term modulated cross-attention (MCA) memory to model temporal smoothness without requiring frequent memory expansion. The proposed MCA effectively encodes both local and global features at various levels of granularity while efficiently maintaining consistent speed regardless of the video length. Extensive experiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017, demonstrate the effectiveness of our proposed contributions leading to real-time inference and markedly reduced memory demands without any degradation in segmentation accuracy on long videos. Compared to the best existing transformer-based approach, our MAVOS increases the speed by 7.6x, while significantly reducing the GPU memory by 87% with comparable segmentation performance on short and long video datasets. Notably on the LVOS dataset, our MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second (FPS) on a single V100 GPU. Our code and models will be publicly available at: https://github.com/Amshaker/MAVOS.","sentences":["Recently, transformer-based approaches have shown promising results for semi-supervised video object segmentation.","However, these approaches typically struggle on long videos due to increased GPU memory demands, as they frequently expand the memory bank every few frames.","We propose a transformer-based approach, named MAVOS, that introduces an optimized and dynamic long-term modulated cross-attention (MCA) memory to model temporal smoothness without requiring frequent memory expansion.","The proposed MCA effectively encodes both local and global features at various levels of granularity while efficiently maintaining consistent speed regardless of the video length.","Extensive experiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017, demonstrate the effectiveness of our proposed contributions leading to real-time inference and markedly reduced memory demands without any degradation in segmentation accuracy on long videos.","Compared to the best existing transformer-based approach, our MAVOS increases the speed by 7.6x, while significantly reducing the GPU memory by 87% with comparable segmentation performance on short and long video datasets.","Notably on the LVOS dataset, our MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second (FPS) on a single V100 GPU.","Our code and models will be publicly available at: https://github.com/Amshaker/MAVOS."],"url":"http://arxiv.org/abs/2403.17937v1","category":"cs.CV"}
{"created":"2024-03-26 17:58:22","title":"Track Everything Everywhere Fast and Robustly","abstract":"We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video. The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications. OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence. To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions. While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision foundation models. Our system utilizes monocular depth estimation to represent scene geometry and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process. Our experiments demonstrate a substantial improvement in training speed (more than \\textbf{10 times} faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion.","sentences":["We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video.","The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications.","OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence.","To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions.","While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision foundation models.","Our system utilizes monocular depth estimation to represent scene geometry and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process.","Our experiments demonstrate a substantial improvement in training speed (more than \\textbf{10 times} faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion."],"url":"http://arxiv.org/abs/2403.17931v1","category":"cs.CV"}
{"created":"2024-03-26 17:56:33","title":"Optimizing Vaccine Site Locations While Considering Travel Inconvenience and Public Health Outcomes","abstract":"During the COVID-19 pandemic, there were over three million infections in Los Angeles County (LAC). To facilitate distribution when vaccines first became available, LAC set up six mega-sites for dispensing a large number of vaccines to the public. To understand if another choice of mega-site location would have improved accessibility and health outcomes, and to provide insight into future vaccine allocation problems, we propose a multi-objective mixed integer linear programming model that balances travel convenience, infection reduction, and equitable distribution. We provide a tractable objective formulation that effectively proxies real-world public health goals of reducing infections while considering travel inconvenience and equitable distribution of resources. Compared with the solution empirically used in LAC in 2020, we recommend more dispersed mega-site locations that result in a 28% reduction in travel inconvenience and avert an additional 1,000 infections.","sentences":["During the COVID-19 pandemic, there were over three million infections in Los Angeles County (LAC).","To facilitate distribution when vaccines first became available, LAC set up six mega-sites for dispensing a large number of vaccines to the public.","To understand if another choice of mega-site location would have improved accessibility and health outcomes, and to provide insight into future vaccine allocation problems, we propose a multi-objective mixed integer linear programming model that balances travel convenience, infection reduction, and equitable distribution.","We provide a tractable objective formulation that effectively proxies real-world public health goals of reducing infections while considering travel inconvenience and equitable distribution of resources.","Compared with the solution empirically used in LAC in 2020, we recommend more dispersed mega-site locations that result in a 28% reduction in travel inconvenience and avert an additional 1,000 infections."],"url":"http://arxiv.org/abs/2403.17923v1","category":"math.OC"}
{"created":"2024-03-26 17:43:04","title":"On the Dynamics of Point-Vortices with Positive Intensities collapsing with the boundary","abstract":"In this paper we study the point-vortex dynamics with positive intensities. We show that in the half-plane and in a disk, collapses of point-vortices with the boundary in finite time are impossible, hence the solution of the dynamics is global in time. We also give some necessary conditions for the existence of collapses with the boundary in general smooth bounded domains, in particular that the trajectory of at least one point-vortex has no limit. Some minor results are obtained with signed intensities.","sentences":["In this paper we study the point-vortex dynamics with positive intensities.","We show that in the half-plane and in a disk, collapses of point-vortices with the boundary in finite time are impossible, hence the solution of the dynamics is global in time.","We also give some necessary conditions for the existence of collapses with the boundary in general smooth bounded domains, in particular that the trajectory of at least one point-vortex has no limit.","Some minor results are obtained with signed intensities."],"url":"http://arxiv.org/abs/2403.17900v1","category":"math.AP"}
{"created":"2024-03-26 17:16:04","title":"Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data","abstract":"Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications. To advance the development of machine learning algorithms in this domain, our study introduces the \\textit{Sen2Fire} dataset--a challenging satellite remote sensing dataset tailored for wildfire detection. This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches. Each patch has a size of 512$\\times$512 pixels with 13 bands. Given the distinctive sensitivities of various wavebands to wildfire responses, our research focuses on optimizing wildfire detection by evaluating different wavebands and employing a combination of spectral indices, such as normalized burn ratio (NBR) and normalized difference vegetation index (NDVI). The results suggest that, in contrast to using all bands for wildfire detection, selecting specific band combinations yields superior performance. Additionally, our study underscores the positive impact of integrating Sentinel-5 aerosol data for wildfire detection. The code and dataset are available online (https://zenodo.org/records/10881058).","sentences":["Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications.","To advance the development of machine learning algorithms in this domain, our study introduces the \\textit{Sen2Fire} dataset--a challenging satellite remote sensing dataset tailored for wildfire detection.","This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches.","Each patch has a size of 512$\\times$512 pixels with 13 bands.","Given the distinctive sensitivities of various wavebands to wildfire responses, our research focuses on optimizing wildfire detection by evaluating different wavebands and employing a combination of spectral indices, such as normalized burn ratio (NBR) and normalized difference vegetation index (NDVI).","The results suggest that, in contrast to using all bands for wildfire detection, selecting specific band combinations yields superior performance.","Additionally, our study underscores the positive impact of integrating Sentinel-5 aerosol data for wildfire detection.","The code and dataset are available online (https://zenodo.org/records/10881058)."],"url":"http://arxiv.org/abs/2403.17884v1","category":"cs.CV"}
{"created":"2024-03-26 16:57:55","title":"Boosting Diffusion Models with Moving Average Sampling in Frequency Domain","abstract":"Diffusion models have recently brought a powerful revolution in image generation. Despite showing impressive generative capabilities, most of these models rely on the current sample to denoise the next one, possibly resulting in denoising instability. In this paper, we reinterpret the iterative denoising process as model optimization and leverage a moving average mechanism to ensemble all the prior samples. Instead of simply applying moving average to the denoised samples at different timesteps, we first map the denoised samples to data space and then perform moving average to avoid distribution shift across timesteps. In view that diffusion models evolve the recovery from low-frequency components to high-frequency details, we further decompose the samples into different frequency components and execute moving average separately on each component. We name the complete approach \"Moving Average Sampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into mainstream pre-trained diffusion models and sampling schedules. Extensive experiments on both unconditional and conditional diffusion models demonstrate that our MASF leads to superior performances compared to the baselines, with almost negligible additional complexity cost.","sentences":["Diffusion models have recently brought a powerful revolution in image generation.","Despite showing impressive generative capabilities, most of these models rely on the current sample to denoise the next one, possibly resulting in denoising instability.","In this paper, we reinterpret the iterative denoising process as model optimization and leverage a moving average mechanism to ensemble all the prior samples.","Instead of simply applying moving average to the denoised samples at different timesteps, we first map the denoised samples to data space and then perform moving average to avoid distribution shift across timesteps.","In view that diffusion models evolve the recovery from low-frequency components to high-frequency details, we further decompose the samples into different frequency components and execute moving average separately on each component.","We name the complete approach \"Moving Average Sampling in Frequency domain (MASF)\".","MASF could be seamlessly integrated into mainstream pre-trained diffusion models and sampling schedules.","Extensive experiments on both unconditional and conditional diffusion models demonstrate that our MASF leads to superior performances compared to the baselines, with almost negligible additional complexity cost."],"url":"http://arxiv.org/abs/2403.17870v1","category":"cs.CV"}
{"created":"2024-03-26 16:46:05","title":"Parallelizable Parametric Nonlinear System Identification via tuning of a Moving Horizon State Estimator","abstract":"This paper introduces a novel optimization-based approach for parametric nonlinear system identification. Building upon the prediction error method framework, traditionally used for linear system identification, we extend its capabilities to nonlinear systems. The predictions are computed using a moving horizon state estimator with a constant arrival cost. Eventually, both the system parameters and the arrival cost are estimated by minimizing the sum of the squared prediction errors. Since the predictions are induced by the state estimator, the method can be viewed as the tuning of a state estimator, based on its predictive capacities. The present extension of the prediction error method not only enhances performance for nonlinear systems but also enables learning from multiple trajectories with unknown initial states, broadening its applicability in practical scenarios. Additionally, the novel formulation leaves room for the design of efficient and parallelizable optimization algorithms, since each output prediction only depends on a fixed window of past actions and measurements. In the special case of linear time-invariant systems, we show an important property of the proposed method which suggests asymptotic consistency under reasonable assumptions. Numerical examples illustrate the effectiveness and practicality of the approach, and one of the examples also highlights the necessity for the arrival cost.","sentences":["This paper introduces a novel optimization-based approach for parametric nonlinear system identification.","Building upon the prediction error method framework, traditionally used for linear system identification, we extend its capabilities to nonlinear systems.","The predictions are computed using a moving horizon state estimator with a constant arrival cost.","Eventually, both the system parameters and the arrival cost are estimated by minimizing the sum of the squared prediction errors.","Since the predictions are induced by the state estimator, the method can be viewed as the tuning of a state estimator, based on its predictive capacities.","The present extension of the prediction error method not only enhances performance for nonlinear systems but also enables learning from multiple trajectories with unknown initial states, broadening its applicability in practical scenarios.","Additionally, the novel formulation leaves room for the design of efficient and parallelizable optimization algorithms, since each output prediction only depends on a fixed window of past actions and measurements.","In the special case of linear time-invariant systems, we show an important property of the proposed method which suggests asymptotic consistency under reasonable assumptions.","Numerical examples illustrate the effectiveness and practicality of the approach, and one of the examples also highlights the necessity for the arrival cost."],"url":"http://arxiv.org/abs/2403.17858v1","category":"math.OC"}
{"created":"2024-03-26 16:38:12","title":"Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial Vehicles","abstract":"Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple agents from respective start to goal locations such that no paths conflict. We address the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles which are subject to location-dependent noise restrictions. We solve this problem by searching a constraint tree for which the subproblem at each node is a set of shortest path problems subject to the noise and fuel constraints and conflict zone avoidance. A labeling algorithm is presented to solve this subproblem, including the conflict zones which are treated as dynamic obstacles. We present the experimental results of the algorithms for various graph sizes and number of agents.","sentences":["Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple agents from respective start to goal locations such that no paths conflict.","We address the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles which are subject to location-dependent noise restrictions.","We solve this problem by searching a constraint tree for which the subproblem at each node is a set of shortest path problems subject to the noise and fuel constraints and conflict zone avoidance.","A labeling algorithm is presented to solve this subproblem, including the conflict zones which are treated as dynamic obstacles.","We present the experimental results of the algorithms for various graph sizes and number of agents."],"url":"http://arxiv.org/abs/2403.17849v1","category":"math.OC"}
{"created":"2024-03-26 16:33:12","title":"Mechanistic Design and Scaling of Hybrid Architectures","abstract":"The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.","sentences":["The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation.","We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws.","Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives.","We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters.","Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks.","The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes.","Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology."],"url":"http://arxiv.org/abs/2403.17844v1","category":"cs.LG"}
{"created":"2024-03-26 16:22:06","title":"An MBE-CASSCF Approach for the Accurate Treatment of Large Active Spaces","abstract":"We present a novel implementation of the complete active space self-consistent field (CASSCF) method that makes use of the many-body expanded full configuration interaction (MBE-FCI) method to incrementally approximate electronic structures within large active spaces. On the basis of a hybrid first-order algorithm employing both Super-CI and quasi-Newton strategies for the optimization of molecular orbitals, we demonstrate both computational efficacy and high accuracy of the resulting MBE-CASSCF method. We assess the performance of our implementation on a set of established numerical tests before applying MBE-CASSCF in the investigation of the triplet-quintet spin gap of an iron(II) tetraphenylporphyrin model system with active spaces as large as 50 electrons in 50 orbitals.","sentences":["We present a novel implementation of the complete active space self-consistent field (CASSCF) method that makes use of the many-body expanded full configuration interaction (MBE-FCI) method to incrementally approximate electronic structures within large active spaces.","On the basis of a hybrid first-order algorithm employing both Super-CI and quasi-Newton strategies for the optimization of molecular orbitals, we demonstrate both computational efficacy and high accuracy of the resulting MBE-CASSCF method.","We assess the performance of our implementation on a set of established numerical tests before applying MBE-CASSCF in the investigation of the triplet-quintet spin gap of an iron(II) tetraphenylporphyrin model system with active spaces as large as 50 electrons in 50 orbitals."],"url":"http://arxiv.org/abs/2403.17836v1","category":"physics.chem-ph"}
{"created":"2024-03-26 15:42:01","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","abstract":"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.","sentences":["Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images.","Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly.","Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency.","In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models.","Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score.","Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data.","Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs."],"url":"http://arxiv.org/abs/2403.17804v1","category":"cs.CV"}
{"created":"2024-03-26 15:31:37","title":"A new double-pass type of the optical spring","abstract":"In detuned optical cavities, the radiation pressure force acting on the mirrors depends on their displacements. This is equivalent to the rigidity (the optical spring), inserted between the mirrors. This effect can be used for optimization of the mechanical susceptibility of probe mirrors in high-precision force sensors. However, in some cases, the use of detuned cavities or even just any high-finesse cavities could be problematic due to technological constraints.   We consider a new type of the optical spring that does not require the cavity (but can use a resonance tuned one to increase the optomechanical coupling). Instead, it uses the double interaction of the probing light with the mechanical object. We propose two possible implementation of this concept, suitable, respectively, for the atomic spin ensembles and for the laser gravitational-wave detectors.","sentences":["In detuned optical cavities, the radiation pressure force acting on the mirrors depends on their displacements.","This is equivalent to the rigidity (the optical spring), inserted between the mirrors.","This effect can be used for optimization of the mechanical susceptibility of probe mirrors in high-precision force sensors.","However, in some cases, the use of detuned cavities or even just any high-finesse cavities could be problematic due to technological constraints.   ","We consider a new type of the optical spring that does not require the cavity (but can use a resonance tuned one to increase the optomechanical coupling).","Instead, it uses the double interaction of the probing light with the mechanical object.","We propose two possible implementation of this concept, suitable, respectively, for the atomic spin ensembles and for the laser gravitational-wave detectors."],"url":"http://arxiv.org/abs/2403.17795v1","category":"quant-ph"}
{"created":"2024-03-26 15:27:42","title":"Fermihedral: On the Optimal Compilation for Fermion-to-Qubit Encoding","abstract":"This paper introduces Fermihedral, a compiler framework focusing on discovering the optimal Fermion-to-qubit encoding for targeted Fermionic Hamiltonians. Fermion-to-qubit encoding is a crucial step in harnessing quantum computing for efficient simulation of Fermionic quantum systems. Utilizing Pauli algebra, Fermihedral redefines complex constraints and objectives of Fermion-to-qubit encoding into a Boolean Satisfiability problem which can then be solved with high-performance solvers. To accommodate larger-scale scenarios, this paper proposed two new strategies that yield approximate optimal solutions mitigating the overhead from the exponentially large number of clauses. Evaluation across diverse Fermionic systems highlights the superiority of Fermihedral, showcasing substantial reductions in implementation costs, gate counts, and circuit depth in the compiled circuits. Real-system experiments on IonQ's device affirm its effectiveness, notably enhancing simulation accuracy.","sentences":["This paper introduces Fermihedral, a compiler framework focusing on discovering the optimal Fermion-to-qubit encoding for targeted Fermionic Hamiltonians.","Fermion-to-qubit encoding is a crucial step in harnessing quantum computing for efficient simulation of Fermionic quantum systems.","Utilizing Pauli algebra, Fermihedral redefines complex constraints and objectives of Fermion-to-qubit encoding into a Boolean Satisfiability problem which can then be solved with high-performance solvers.","To accommodate larger-scale scenarios, this paper proposed two new strategies that yield approximate optimal solutions mitigating the overhead from the exponentially large number of clauses.","Evaluation across diverse Fermionic systems highlights the superiority of Fermihedral, showcasing substantial reductions in implementation costs, gate counts, and circuit depth in the compiled circuits.","Real-system experiments on IonQ's device affirm its effectiveness, notably enhancing simulation accuracy."],"url":"http://arxiv.org/abs/2403.17794v2","category":"quant-ph"}
{"created":"2024-03-26 15:21:12","title":"Molecular groundstate determination via short pulses on superconducting qubits","abstract":"Quantum computing is currently hindered by hardware noise. We present a freestyle superconducting pulse optimization method, incorporating two-qubit channels, which enhances flexibility, execution speed, and noise resilience. A minimal 0.22 ns pulse is shown to determine the H2 groundstate to within chemical accuracy upon real-hardware, approaching the quantum speed limit. Similarly, a pulse significantly shorter than circuit-based counterparts is found for the LiH molecule, attaining state-of-the-art accuracy. The method is general and can potentially accelerate performance across various quantum computing components and hardware.","sentences":["Quantum computing is currently hindered by hardware noise.","We present a freestyle superconducting pulse optimization method, incorporating two-qubit channels, which enhances flexibility, execution speed, and noise resilience.","A minimal 0.22 ns pulse is shown to determine the H2 groundstate to within chemical accuracy upon real-hardware, approaching the quantum speed limit.","Similarly, a pulse significantly shorter than circuit-based counterparts is found for the LiH molecule, attaining state-of-the-art accuracy.","The method is general and can potentially accelerate performance across various quantum computing components and hardware."],"url":"http://arxiv.org/abs/2403.17789v1","category":"quant-ph"}
{"created":"2024-03-26 15:20:56","title":"System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners","abstract":"The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications. This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation. The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud. Moreover, we present a novel factor graph-based pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration. The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization. Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration. Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan.","sentences":["The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications.","This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation.","The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud.","Moreover, we present a novel factor graph-based pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration.","The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization.","Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration.","Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan."],"url":"http://arxiv.org/abs/2403.17788v1","category":"cs.RO"}
{"created":"2024-03-26 15:18:59","title":"Query Refinement for Diverse Top-$k$ Selection","abstract":"Database queries are often used to select and rank items as decision support for many applications. As automated decision-making tools become more prevalent, there is a growing recognition of the need to diversify their outcomes. In this paper, we define and study the problem of modifying the selection conditions of an ORDER BY query so that the result of the modified query closely fits some user-defined notion of diversity while simultaneously maintaining the intent of the original query. We show the hardness of this problem and propose a Mixed Integer Linear Programming (MILP) based solution. We further present optimizations designed to enhance the scalability and applicability of the solution in real-life scenarios. We investigate the performance characteristics of our algorithm and show its efficiency and the usefulness of our optimizations.","sentences":["Database queries are often used to select and rank items as decision support for many applications.","As automated decision-making tools become more prevalent, there is a growing recognition of the need to diversify their outcomes.","In this paper, we define and study the problem of modifying the selection conditions of an ORDER BY query so that the result of the modified query closely fits some user-defined notion of diversity while simultaneously maintaining the intent of the original query.","We show the hardness of this problem and propose a Mixed Integer Linear Programming (MILP) based solution.","We further present optimizations designed to enhance the scalability and applicability of the solution in real-life scenarios.","We investigate the performance characteristics of our algorithm and show its efficiency and the usefulness of our optimizations."],"url":"http://arxiv.org/abs/2403.17786v2","category":"cs.DB"}
{"created":"2024-03-26 14:54:02","title":"Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph","abstract":"We study the computational limits of the following general hypothesis testing problem. Let H=H_n be an \\emph{arbitrary} undirected graph on n vertices. We study the detection task between a ``null'' Erd\\H{o}s-R\\'{e}nyi random graph G(n,p) and a ``planted'' random graph which is the union of G(n,p) together with a random copy of H=H_n. Our notion of planted model is a generalization of a plethora of recently studied models initiated with the study of the planted clique model (Jerrum 1992), which corresponds to the special case where H is a k-clique and p=1/2.   Over the last decade, several papers have studied the power of low-degree polynomials for limited choices of H's in the above task. In this work, we adopt a unifying perspective and characterize the power of \\emph{constant degree} polynomials for the detection task, when \\emph{H=H_n is any arbitrary graph} and for \\emph{any p=\\Omega(1).} Perhaps surprisingly, we prove that the optimal constant degree polynomial is always given by simply \\emph{counting stars} in the input random graph. As a direct corollary, we conclude that the class of constant-degree polynomials is only able to ``sense'' the degree distribution of the planted graph H, and no other graph theoretic property of it.","sentences":["We study the computational limits of the following general hypothesis testing problem.","Let H=H_n be an \\emph{arbitrary} undirected graph on n vertices.","We study the detection task between a ``null'' Erd\\H{o}s-R\\'{e}nyi random graph G(n,p) and a ``planted'' random graph which is the union of G(n,p) together with a random copy of H=H_n.","Our notion of planted model is a generalization of a plethora of recently studied models initiated with the study of the planted clique model (Jerrum 1992), which corresponds to the special case where H is a k-clique and p=1/2.   ","Over the last decade, several papers have studied the power of low-degree polynomials for limited choices of H's in the above task.","In this work, we adopt a unifying perspective and characterize the power of \\emph{constant degree} polynomials for the detection task, when \\emph{H=H_n is any arbitrary graph} and for \\emph{any p=\\Omega(1).}","Perhaps surprisingly, we prove that the optimal constant degree polynomial is always given by simply \\emph{counting stars} in the input random graph.","As a direct corollary, we conclude that the class of constant-degree polynomials is only able to ``sense'' the degree distribution of the planted graph H, and no other graph theoretic property of it."],"url":"http://arxiv.org/abs/2403.17766v1","category":"math.ST"}
{"created":"2024-03-26 14:19:22","title":"On Structural Non-commutativity in Affine Feedback of SISO Nonlinear Systems","abstract":"The affine feedback connection of SISO nonlinear systems modeled by Chen--Fliess series is shown to be a group action on the plant which is isomorphic to the semi-direct product of shuffle and additive group of non-commutative formal power series. The additive and multiplicative feedback loops in an affine feedback connection are thus proven to be structurally non-commutative. A flip in the order of these loops results in a net additive feedback loop.","sentences":["The affine feedback connection of SISO nonlinear systems modeled by Chen--Fliess series is shown to be a group action on the plant which is isomorphic to the semi-direct product of shuffle and additive group of non-commutative formal power series.","The additive and multiplicative feedback loops in an affine feedback connection are thus proven to be structurally non-commutative.","A flip in the order of these loops results in a net additive feedback loop."],"url":"http://arxiv.org/abs/2403.17730v1","category":"math.OC"}
{"created":"2024-03-26 14:11:19","title":"Regularity for nonlocal equations with local Neumann boundary conditions","abstract":"In this article we establish fine results on the boundary behavior of solutions to nonlocal equations in $C^{k,\\gamma}$ domains which satisfy local Neumann conditions on the boundary. Such solutions typically blow up at the boundary like $v \\asymp d^{s-1}$ and are sometimes called large solutions. In this setup we prove optimal regularity results for the quotients $v/d^{s-1}$, depending on the regularity of the domain and on the data of the problem. The results of this article will be important in a forthcoming work on nonlocal free boundary problems.","sentences":["In this article we establish fine results on the boundary behavior of solutions to nonlocal equations in $C^{k,\\gamma}$ domains which satisfy local Neumann conditions on the boundary.","Such solutions typically blow up at the boundary like $v \\asymp d^{s-1}$ and are sometimes called large solutions.","In this setup we prove optimal regularity results for the quotients $v/d^{s-1}$, depending on the regularity of the domain and on the data of the problem.","The results of this article will be important in a forthcoming work on nonlocal free boundary problems."],"url":"http://arxiv.org/abs/2403.17723v1","category":"math.AP"}
{"created":"2024-03-26 13:58:21","title":"Using quantum computers in control: interval matrix properties","abstract":"Quantum computing provides a powerful framework for tackling computational problems that are classically intractable. The goal of this paper is to explore the use of quantum computers for solving relevant problems in systems and control theory. In the recent literature, different quantum algorithms have been developed to tackle binary optimization, which plays an important role in various control-theoretic problems. As a prototypical example, we consider the verification of interval matrix properties such as non-singularity and stability on a quantum computer. We present a quantum algorithm solving these problems and we study its performance in simulation. Our results demonstrate that quantum computers provide a promising tool for control whose applicability to further computationally complex problems remains to be explored.","sentences":["Quantum computing provides a powerful framework for tackling computational problems that are classically intractable.","The goal of this paper is to explore the use of quantum computers for solving relevant problems in systems and control theory.","In the recent literature, different quantum algorithms have been developed to tackle binary optimization, which plays an important role in various control-theoretic problems.","As a prototypical example, we consider the verification of interval matrix properties such as non-singularity and stability on a quantum computer.","We present a quantum algorithm solving these problems and we study its performance in simulation.","Our results demonstrate that quantum computers provide a promising tool for control whose applicability to further computationally complex problems remains to be explored."],"url":"http://arxiv.org/abs/2403.17711v1","category":"eess.SY"}
