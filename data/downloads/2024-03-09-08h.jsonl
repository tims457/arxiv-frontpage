{"created":"2024-03-06 18:56:36","title":"Bridging Language and Items for Retrieval and Recommendation","abstract":"This paper introduces BLaIR, a series of pretrained sentence embedding models specialized for recommendation scenarios. BLaIR is trained to learn correlations between item metadata and potential natural language context, which is useful for retrieving and recommending items. To pretrain BLaIR, we collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews and 48 million items from 33 categories, significantly expanding beyond the scope of previous versions. We evaluate the generalization ability of BLaIR across multiple domains and tasks, including a new task named complex product search, referring to retrieving relevant items given long, complex natural language contexts. Leveraging large language models like ChatGPT, we correspondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical results on the new task, as well as conventional retrieval and recommendation tasks, demonstrate that BLaIR exhibit strong text and item representation capacity. Our datasets, code, and checkpoints are available at: https://github.com/hyp1231/AmazonReviews2023.","sentences":["This paper introduces BLaIR, a series of pretrained sentence embedding models specialized for recommendation scenarios.","BLaIR is trained to learn correlations between item metadata and potential natural language context, which is useful for retrieving and recommending items.","To pretrain BLaIR, we collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews and 48 million items from 33 categories, significantly expanding beyond the scope of previous versions.","We evaluate the generalization ability of BLaIR across multiple domains and tasks, including a new task named complex product search, referring to retrieving relevant items given long, complex natural language contexts.","Leveraging large language models like ChatGPT, we correspondingly construct a semi-synthetic evaluation set, Amazon-C4.","Empirical results on the new task, as well as conventional retrieval and recommendation tasks, demonstrate that BLaIR exhibit strong text and item representation capacity.","Our datasets, code, and checkpoints are available at: https://github.com/hyp1231/AmazonReviews2023."],"url":"http://arxiv.org/abs/2403.03952v1","category":"cs.IR"}
{"created":"2024-03-06 18:56:33","title":"Investigating the Collective Nature of Cavity Modified Chemical Kinetics under Vibrational Strong Coupling","abstract":"In this paper we develop quantum dynamical methods capable of treating the dynamics of chemically reacting systems in an optical cavity in the vibrationally strong-coupling (VSC) limit at finite temperatures and in the presence of a dissipative solvent in both the few and many molecule limits. In the context of two simple models we demonstrate how reactivity in the {\\em collective} VSC regime does not exhibit altered rate behavior in equilibrium, but may exhibit resonant cavity modification of reactivity when the system is explicitly out of equilibrium. Our results suggest experimental protocols that may be used to modify reactivity in the collective regime and point to features not included in the models studied which demand further scrutiny.","sentences":["In this paper we develop quantum dynamical methods capable of treating the dynamics of chemically reacting systems in an optical cavity in the vibrationally strong-coupling (VSC) limit at finite temperatures and in the presence of a dissipative solvent in both the few and many molecule limits.","In the context of two simple models we demonstrate how reactivity in the {\\em collective} VSC regime does not exhibit altered rate behavior in equilibrium, but may exhibit resonant cavity modification of reactivity when the system is explicitly out of equilibrium.","Our results suggest experimental protocols that may be used to modify reactivity in the collective regime and point to features not included in the models studied which demand further scrutiny."],"url":"http://arxiv.org/abs/2403.03951v1","category":"quant-ph"}
{"created":"2024-03-06 18:55:47","title":"Stop Regressing: Training Value Functions via Classification for Scalable Deep RL","abstract":"Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.","sentences":["Value functions are a central component of deep reinforcement learning (RL).","These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values.","However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging.","This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks.","Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions.","We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains.","These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains.","Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity.","Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost."],"url":"http://arxiv.org/abs/2403.03950v1","category":"cs.LG"}
{"created":"2024-03-06 18:55:36","title":"Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation","abstract":"Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in \"digital twin\" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel \"inverse distillation\" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection. Project website and videos at https://real-to-sim-to-real.github.io/RialTo/","sentences":["Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors.","Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection.","To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in \"digital twin\" simulation environments constructed on the fly from small amounts of real-world data.","To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments.","We also introduce a novel \"inverse distillation\" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required.","We evaluate RialTo","across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks.","RialTo increases (over 67%) in policy robustness without requiring extensive human data collection.","Project website and videos at https://real-to-sim-to-real.github.io/RialTo/"],"url":"http://arxiv.org/abs/2403.03949v1","category":"cs.RO"}
{"created":"2024-03-06 18:43:14","title":"Demographic Dynamics and Artificial Intelligence: Challenges and Opportunities in Europe and Africa for 2050","abstract":"This paper explores the complex relationship between demographics and artificial intelligence (AI) advances in Europe and Africa, projecting into the year 2050. The advancement of AI technologies has occurred at diverse rates, with Africa lagging behind Europe. Moreover, the imminent economic consequences of demographic shifts require a more careful examination of immigration patterns, with Africa emerging as a viable labor pool for European countries. However, within these dynamics, questions are raised about the differences in AI proficiency between African immigrants and Europeans by 2050. This paper examines demographic trends and AI developments to unravel insights into the multifaceted challenges and opportunities that lie ahead in the realms of technology, the economy, and society as we look ahead to 2050.","sentences":["This paper explores the complex relationship between demographics and artificial intelligence (AI) advances in Europe and Africa, projecting into the year 2050.","The advancement of AI technologies has occurred at diverse rates, with Africa lagging behind Europe.","Moreover, the imminent economic consequences of demographic shifts require a more careful examination of immigration patterns, with Africa emerging as a viable labor pool for European countries.","However, within these dynamics, questions are raised about the differences in AI proficiency between African immigrants and Europeans by 2050.","This paper examines demographic trends and AI developments to unravel insights into the multifaceted challenges and opportunities that lie ahead in the realms of technology, the economy, and society as we look ahead to 2050."],"url":"http://arxiv.org/abs/2403.03935v1","category":"cs.CY"}
{"created":"2024-03-06 18:39:41","title":"Extreme Precipitation Nowcasting using Transformer-based Generative Models","abstract":"This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \\url{https://github.com/Cmeo97/NowcastingGPT}.","sentences":["This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization.","Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy.","We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events.","We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events.","The code is available at \\url{https://github.com/Cmeo97/NowcastingGPT}."],"url":"http://arxiv.org/abs/2403.03929v1","category":"cs.LG"}
{"created":"2024-03-06 18:37:11","title":"Permutation Symmetry Restoration in Disordered Materials","abstract":"A disordered solid, such as an athermal jammed packing of soft spheres exists in a rugged potential-energy landscape in which there are a myriad of stable configurations that defy easy enumeration and characterization. Nevertheless, in three-dimensional monodisperse particle packings, we find an astonishing regularity in the distribution of basin volumes, $V_{Nd}$, as measured by their frequency of occurrence in a random sampling algorithm. Ordering the basins according to their size, from the largest at $n=1$, to the smallest, we find approximately that $V_{Nd} \\propto n^{-1}$. This statistical regularity persists up to the largest systems for which we can collect sufficient data. In monodisperse packings there is \"permutation symmetry\" since identical particles can always be interchanged without affecting the system or its properties. Introducing any polydispersity breaks this symmetry and leads to a proliferation of distinct configurations. We present an algorithm that partially restores permutation symmetry to such polydisperse packings.","sentences":["A disordered solid, such as an athermal jammed packing of soft spheres exists in a rugged potential-energy landscape in which there are a myriad of stable configurations that defy easy enumeration and characterization.","Nevertheless, in three-dimensional monodisperse particle packings, we find an astonishing regularity in the distribution of basin volumes, $V_{Nd}$, as measured by their frequency of occurrence in a random sampling algorithm.","Ordering the basins according to their size, from the largest at $n=1$, to the smallest, we find approximately that $V_{Nd} \\propto n^{-1}$.","This statistical regularity persists up to the largest systems for which we can collect sufficient data.","In monodisperse packings there is \"permutation symmetry\" since identical particles can always be interchanged without affecting the system or its properties.","Introducing any polydispersity breaks this symmetry and leads to a proliferation of distinct configurations.","We present an algorithm that partially restores permutation symmetry to such polydisperse packings."],"url":"http://arxiv.org/abs/2403.03926v1","category":"cond-mat.soft"}
{"created":"2024-03-06 18:37:06","title":"Consciousness qua Mortal Computation","abstract":"Computational functionalism posits that consciousness is a computation. Here we show, perhaps surprisingly, that it cannot be a Turing computation. Rather, computational functionalism implies that consciousness is a novel type of computation that has recently been proposed by Geoffrey Hinton, called mortal computation.","sentences":["Computational functionalism posits that consciousness is a computation.","Here we show, perhaps surprisingly, that it cannot be a Turing computation.","Rather, computational functionalism implies that consciousness is a novel type of computation that has recently been proposed by Geoffrey Hinton, called mortal computation."],"url":"http://arxiv.org/abs/2403.03925v1","category":"q-bio.NC"}
{"created":"2024-03-06 18:29:18","title":"Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts","abstract":"This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts. We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement. Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development. We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructional dynamics. This paper emphasizes the importance of aligning AI/ML technologies with pedagogical goals to realize their full potential in educational settings, advocating for a balanced approach that considers ethical considerations, data quality, and the integration of human expertise.","sentences":["This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts.","We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement.","Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development.","We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructional dynamics.","This paper emphasizes the importance of aligning AI/ML technologies with pedagogical goals to realize their full potential in educational settings, advocating for a balanced approach that considers ethical considerations, data quality, and the integration of human expertise."],"url":"http://arxiv.org/abs/2403.03920v1","category":"cs.AI"}
{"created":"2024-03-06 18:14:22","title":"A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets","abstract":"Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set. Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them.","sentences":["Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP.","Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages.","In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run.","We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures.","In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features.","Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set.","Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD).","In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them."],"url":"http://arxiv.org/abs/2403.03909v1","category":"cs.CL"}
{"created":"2024-03-06 17:54:50","title":"DART: Implicit Doppler Tomography for Radar Novel View Synthesis","abstract":"Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images.","sentences":["Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking.","However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function.","Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images.","We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization.","In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images."],"url":"http://arxiv.org/abs/2403.03896v1","category":"cs.CV"}
{"created":"2024-03-06 17:52:08","title":"IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators","abstract":"Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations. Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.","sentences":["Code understanding and generation have fast become some of the most popular applications of language models (LMs).","Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts.","In particular, most mainstream Code-LMs have been pre-trained on source code files alone.","In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   ","To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations.","Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages.","Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following."],"url":"http://arxiv.org/abs/2403.03894v1","category":"cs.AI"}
{"created":"2024-03-06 17:51:43","title":"From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models","abstract":"To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field. Code and data are available at https://github.com/for-ai/goodtriever.","sentences":["To date, toxicity mitigation in language models has almost entirely been focused on single-language settings.","As language models embrace multilingual capabilities, it's crucial our safety measures keep pace.","Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages.","In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques.","We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios.","This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation.","We also explore how model size and data quantity affect the success of these mitigation efforts.","Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages.","Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field.","Code and data are available at https://github.com/for-ai/goodtriever."],"url":"http://arxiv.org/abs/2403.03893v1","category":"cs.CL"}
{"created":"2024-03-06 17:50:26","title":"Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation","abstract":"This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics. Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world.","sentences":["This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation.","HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories.","The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions.","To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser).","Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics.","Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world."],"url":"http://arxiv.org/abs/2403.03890v1","category":"cs.RO"}
{"created":"2024-03-06 17:41:41","title":"Latent Dataset Distillation with Diffusion Models","abstract":"The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges. LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images. By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.","sentences":["The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets.","However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model.","In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged.","One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets.","However, the final accuracy is lower if the employed model architecture differs from the model used during distillation.","Another challenge is the generation of high-resolution images, e.g., 128x128 and higher.","In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges.","LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images.","By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy.","We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256).","As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p.","for 1 and 10 images per class, respectively."],"url":"http://arxiv.org/abs/2403.03881v1","category":"cs.CV"}
{"created":"2024-03-06 17:38:33","title":"Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model","abstract":"Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups. The primary tool for diagnosis is cystoscopy, which heavily relies on doctors' expertise and interpretation. Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections. To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines CNNs with a lightweight positional-encoding-free transformer and dual attention gates that fuse self and spatial attention for feature enhancement. The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference. Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystoscopic imaging as despite its small size it rivals large models in performance.","sentences":["Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups.","The primary tool for diagnosis is cystoscopy, which heavily relies on doctors' expertise and interpretation.","Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections.","To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines CNNs with a lightweight positional-encoding-free transformer and dual attention gates that fuse self and spatial attention for feature enhancement.","The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference.","Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystoscopic imaging as despite its small size it rivals large models in performance."],"url":"http://arxiv.org/abs/2403.03879v1","category":"cs.CV"}
{"created":"2024-03-06 17:37:43","title":"On the stack of 0-dimensional coherent sheaves: structural aspects","abstract":"Let $X$ be a quasiprojective scheme. In this expository note we collect a series of useful structural results on the stack $\\mathscr{C}oh^n(X)$ parametrising $0$-dimensional coherent sheaves of length $n$ over $X$. For instance, we discuss its functoriality (in particular its behaviour along \\'etale maps), the support morphism to $\\mathrm{Sym}^n(X)$, and its relationship with the Quot scheme of points $\\mathrm{Quot}_X(\\mathcal E,n)$ for fixed $\\mathcal E\\in \\mathrm{Coh}(X)$.","sentences":["Let $X$ be a quasiprojective scheme.","In this expository note we collect a series of useful structural results on the stack $\\mathscr{C}oh^n(X)$ parametrising $0$-dimensional coherent sheaves of length $n$ over $X$. For instance, we discuss its functoriality (in particular its behaviour along \\'etale maps), the support morphism to $\\mathrm{Sym}^n(X)$, and its relationship with the Quot scheme of points $\\mathrm{Quot}_X(\\mathcal E,n)$ for fixed $\\mathcal E\\in \\mathrm{Coh}(X)$."],"url":"http://arxiv.org/abs/2403.03878v1","category":"math.AG"}
{"created":"2024-03-06 17:35:27","title":"Impoverished Language Technology: The Lack of (Social) Class in NLP","abstract":"Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies.","sentences":["Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception.","Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology.","While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent.","We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status.","However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics.","Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies."],"url":"http://arxiv.org/abs/2403.03874v1","category":"cs.CL"}
{"created":"2024-03-06 17:16:44","title":"KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions","abstract":"Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs' instruction-following capabilities for knowledge intensive writing tasks.","sentences":["Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents.","In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer.","To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain.","Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer.","We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs.","Each turn includes a user instruction, a model response, and a human evaluation of the model response.","Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits.","Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement.","Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs' instruction-following capabilities for knowledge intensive writing tasks."],"url":"http://arxiv.org/abs/2403.03866v1","category":"cs.CL"}
{"created":"2024-03-06 17:15:04","title":"Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning","abstract":"This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles. The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems.","sentences":["This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering.","We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning.","We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills.","The dataset is generated automatically from code authored by humans.","All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations.","It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size.","Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks.","We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles.","The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems."],"url":"http://arxiv.org/abs/2403.03864v1","category":"cs.CV"}
{"created":"2024-03-06 17:13:24","title":"X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification","abstract":"In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some of them might appear thousands of times, while others might only appear sporadically or not at all. For practical deployment, it is crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits. Here, X can span from 0 to positive infinity. The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios. To solve X-shot, we propose BinBin (Binary INference Based on INstruction following) that leverages the Indirect Supervision from a large collection of NLP tasks via instruction following, bolstered by Weak Supervision provided by large language models. BinBin surpasses previous state-of-the-art techniques on three benchmark datasets across multiple domains. To our knowledge, this is the first work addressing X-shot learning, where X remains variable.","sentences":["In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention.","Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios.","Yet, in real-world settings, label occurrences vary greatly.","Some of them might appear thousands of times, while others might only appear sporadically or not at all.","For practical deployment, it is crucial that a system can adapt to any label occurrence.","We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits.","Here, X can span from 0 to positive infinity.","The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios.","To solve X-shot, we propose BinBin (Binary INference Based on INstruction following) that leverages the Indirect Supervision from a large collection of NLP tasks via instruction following, bolstered by Weak Supervision provided by large language models.","BinBin surpasses previous state-of-the-art techniques on three benchmark datasets across multiple domains.","To our knowledge, this is the first work addressing X-shot learning, where X remains variable."],"url":"http://arxiv.org/abs/2403.03863v1","category":"cs.CL"}
{"created":"2024-03-06 17:02:39","title":"Accelerating Convergence of Score-Based Diffusion Models, Provably","abstract":"Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory accommodates $\\ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution.","sentences":["Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase.","Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited.","In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers.","Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\\sqrt{T})$ for the DDPM sampler.","The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2.","Our theory accommodates $\\ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution."],"url":"http://arxiv.org/abs/2403.03852v1","category":"cs.LG"}
{"created":"2024-03-06 16:49:33","title":"MedMamba: Vision Mamba for Medical Image Classification","abstract":"Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct extensive experiments using three publicly available medical datasets with different imaging techniques (i.e., Kvasir (endoscopic images), FETAL_PLANES_DB (ultrasound images) and Covid19-Pneumonia-Normal Chest X-Ray (X-ray images)) and two private datasets built by ourselves. Experimental results show that the proposed MedMamba performs well in detecting lesions in various medical images. To the best of our knowledge, this is the first Vision Mamba tailored for medical image classification. The purpose of this work is to establish a new baseline for medical image classification tasks and provide valuable insights for the future development of more efficient and effective SSM-based artificial intelligence algorithms and application systems in the medical. Source code has been available at https://github.com/YubiaoYue/MedMamba.","sentences":["Medical image classification is a very fundamental and crucial task in the field of computer vision.","These years, CNN-based and Transformer-based models are widely used in classifying various medical images.","Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity.","Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity.","Inspired by this, we propose Vision Mamba for medical image classification (MedMamba).","More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency.","To demonstrate the potential of MedMamba, we conduct extensive experiments using three publicly available medical datasets with different imaging techniques (i.e., Kvasir (endoscopic images), FETAL_PLANES_DB (ultrasound images) and Covid19-Pneumonia-Normal Chest X-Ray (X-ray images)) and two private datasets built by ourselves.","Experimental results show that the proposed MedMamba performs well in detecting lesions in various medical images.","To the best of our knowledge, this is the first Vision Mamba tailored for medical image classification.","The purpose of this work is to establish a new baseline for medical image classification tasks and provide valuable insights for the future development of more efficient and effective SSM-based artificial intelligence algorithms and application systems in the medical.","Source code has been available at https://github.com/YubiaoYue/MedMamba."],"url":"http://arxiv.org/abs/2403.03849v1","category":"eess.IV"}
{"created":"2024-03-06 16:36:11","title":"Political polarisation in turbulent times: Tracking polarisation trends and partisan news link sharing on Finnish Twitter, 2015-2023","abstract":"The study analyses polarisation on Finnish social media with data from the platform X, which was known as Twitter during the time of data collection (during the Sipil\\\"a and Marin governments, 2015-2023). The users were clustered into three different ideological groups - the Conservative Right, the Moderate Right, and the Liberal Left - based on their retweeting of tweets referring to the different political parties in Finland. Trends in polarisation of several topics encompassing the most recent political crises - immigration, climate change, COVID-19, and security policy - between these ideological groups is analysed using network methods. To what extent the polarisation of each topic aligns with the polarisation of the other topics is also studied. In addition, the sharing of news links is examined in relation to the ideological groups of the users as well as to the sentiment and the virality of the tweets in which news links are shared.","sentences":["The study analyses polarisation on Finnish social media with data from the platform X, which was known as Twitter during the time of data collection (during the Sipil\\\"a and Marin governments, 2015-2023).","The users were clustered into three different ideological groups - the Conservative Right, the Moderate Right, and the Liberal Left - based on their retweeting of tweets referring to the different political parties in Finland.","Trends in polarisation of several topics encompassing the most recent political crises - immigration, climate change, COVID-19, and security policy - between these ideological groups is analysed using network methods.","To what extent the polarisation of each topic aligns with the polarisation of the other topics is also studied.","In addition, the sharing of news links is examined in relation to the ideological groups of the users as well as to the sentiment and the virality of the tweets in which news links are shared."],"url":"http://arxiv.org/abs/2403.03842v1","category":"cs.SI"}
{"created":"2024-03-06 16:26:40","title":"Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning","abstract":"Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.","sentences":["Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure.","Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects.","However, a broader evaluation of Cobweb as a model of human categorization remains lacking.","The current study addresses this gap.","It establishes Cobweb's alignment with classical human category learning effects.","It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model.","These findings set the stage for future research on Cobweb as a comprehensive model of human category learning."],"url":"http://arxiv.org/abs/2403.03835v1","category":"cs.LG"}
{"created":"2024-03-06 16:22:49","title":"Your device may know you better than you know yourself -- continuous authentication on novel dataset using machine learning","abstract":"This research aims to further understanding in the field of continuous authentication using behavioral biometrics. We are contributing a novel dataset that encompasses the gesture data of 15 users playing Minecraft with a Samsung Tablet, each for a duration of 15 minutes. Utilizing this dataset, we employed machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest Neighbors (KNN), and Support Vector Classifier (SVC), to determine the authenticity of specific user actions. Our most robust model was SVC, which achieved an average accuracy of approximately 90%, demonstrating that touch dynamics can effectively distinguish users. However, further studies are needed to make it viable option for authentication systems","sentences":["This research aims to further understanding in the field of continuous authentication using behavioral biometrics.","We are contributing a novel dataset that encompasses the gesture data of 15 users playing Minecraft with a Samsung Tablet, each for a duration of 15 minutes.","Utilizing this dataset, we employed machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest Neighbors (KNN), and Support Vector Classifier (SVC), to determine the authenticity of specific user actions.","Our most robust model was SVC, which achieved an average accuracy of approximately 90%, demonstrating that touch dynamics can effectively distinguish users.","However, further studies are needed to make it viable option for authentication systems"],"url":"http://arxiv.org/abs/2403.03832v1","category":"cs.AI"}
{"created":"2024-03-06 16:18:02","title":"From Clicks to Security: Investigating Continuous Authentication via Mouse Dynamics","abstract":"In the realm of computer security, the importance of efficient and reliable user authentication methods has become increasingly critical. This paper examines the potential of mouse movement dynamics as a consistent metric for continuous authentication. By analyzing user mouse movement patterns in two contrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate the distinctive behavioral patterns inherent in high-intensity and low-intensity UI interactions. The study extends beyond conventional methodologies by employing a range of machine learning models. These models are carefully selected to assess their effectiveness in capturing and interpreting the subtleties of user behavior as reflected in their mouse movements. This multifaceted approach allows for a more nuanced and comprehensive understanding of user interaction patterns. Our findings reveal that mouse movement dynamics can serve as a reliable indicator for continuous user authentication. The diverse machine learning models employed in this study demonstrate competent performance in user verification, marking an improvement over previous methods used in this field. This research contributes to the ongoing efforts to enhance computer security and highlights the potential of leveraging user behavior, specifically mouse dynamics, in developing robust authentication systems.","sentences":["In the realm of computer security, the importance of efficient and reliable user authentication methods has become increasingly critical.","This paper examines the potential of mouse movement dynamics as a consistent metric for continuous authentication.","By analyzing user mouse movement patterns in two contrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate the distinctive behavioral patterns inherent in high-intensity and low-intensity UI interactions.","The study extends beyond conventional methodologies by employing a range of machine learning models.","These models are carefully selected to assess their effectiveness in capturing and interpreting the subtleties of user behavior as reflected in their mouse movements.","This multifaceted approach allows for a more nuanced and comprehensive understanding of user interaction patterns.","Our findings reveal that mouse movement dynamics can serve as a reliable indicator for continuous user authentication.","The diverse machine learning models employed in this study demonstrate competent performance in user verification, marking an improvement over previous methods used in this field.","This research contributes to the ongoing efforts to enhance computer security and highlights the potential of leveraging user behavior, specifically mouse dynamics, in developing robust authentication systems."],"url":"http://arxiv.org/abs/2403.03828v1","category":"cs.AI"}
{"created":"2024-03-06 16:15:13","title":"Temporal Enhanced Floating Car Observers","abstract":"Floating Car Observers (FCOs) are an innovative method to collect traffic data by deploying sensor-equipped vehicles to detect and locate other vehicles. We demonstrate that even a small penetration rate of FCOs can identify a significant amount of vehicles at a given intersection. This is achieved through the emulation of detection within a microscopic traffic simulation. Additionally, leveraging data from previous moments can enhance the detection of vehicles in the current frame. Our findings indicate that, with a 20-second observation window, it is possible to recover up to 20\\% of vehicles that are not visible by FCOs in the current timestep. To exploit this, we developed a data-driven strategy, utilizing sequences of Bird's Eye View (BEV) representations of detected vehicles and deep learning models. This approach aims to bring currently undetected vehicles into view in the present moment, enhancing the currently detected vehicles. Results of different spatiotemporal architectures show that up to 41\\% of the vehicles can be recovered into the current timestep at their current position. This enhancement enriches the information initially available by the FCO, allowing an improved estimation of traffic states and metrics (e.g. density and queue length) for improved implementation of traffic management strategies.","sentences":["Floating Car Observers (FCOs) are an innovative method to collect traffic data by deploying sensor-equipped vehicles to detect and locate other vehicles.","We demonstrate that even a small penetration rate of FCOs can identify a significant amount of vehicles at a given intersection.","This is achieved through the emulation of detection within a microscopic traffic simulation.","Additionally, leveraging data from previous moments can enhance the detection of vehicles in the current frame.","Our findings indicate that, with a 20-second observation window, it is possible to recover up to 20\\% of vehicles that are not visible by FCOs in the current timestep.","To exploit this, we developed a data-driven strategy, utilizing sequences of Bird's Eye View (BEV) representations of detected vehicles and deep learning models.","This approach aims to bring currently undetected vehicles into view in the present moment, enhancing the currently detected vehicles.","Results of different spatiotemporal architectures show that up to 41\\% of the vehicles can be recovered into the current timestep at their current position.","This enhancement enriches the information initially available by the FCO, allowing an improved estimation of traffic states and metrics (e.g. density and queue length) for improved implementation of traffic management strategies."],"url":"http://arxiv.org/abs/2403.03825v1","category":"cs.CV"}
{"created":"2024-03-06 16:06:08","title":"Does Documentation Matter? An Empirical Study of Practitioners' Perspective on Open-Source Software Adoption","abstract":"In recent years, open-source software (OSS) has become increasingly prevalent in developing software products. While OSS documentation is the primary source of information provided by the developers' community about a product, its role in the industry's adoption process has yet to be examined. We conducted semi-structured interviews and an online survey to provide insight into this area. Based on interviews and survey insights, we developed a topic model to collect relevant information from OSS documentation automatically. Additionally, according to our survey responses regarding challenges associated with OSS documentation, we propose a novel information augmentation approach, DocMentor, by combining OSS documentation corpus TF-IDF scores and ChatGPT. Through explaining technical terms and providing examples and references, our approach enhances the documentation context and improves practitioners' understanding. Our tool's effectiveness is assessed by surveying practitioners.","sentences":["In recent years, open-source software (OSS) has become increasingly prevalent in developing software products.","While OSS documentation is the primary source of information provided by the developers' community about a product, its role in the industry's adoption process has yet to be examined.","We conducted semi-structured interviews and an online survey to provide insight into this area.","Based on interviews and survey insights, we developed a topic model to collect relevant information from OSS documentation automatically.","Additionally, according to our survey responses regarding challenges associated with OSS documentation, we propose a novel information augmentation approach, DocMentor, by combining OSS documentation corpus TF-IDF scores and ChatGPT.","Through explaining technical terms and providing examples and references, our approach enhances the documentation context and improves practitioners' understanding.","Our tool's effectiveness is assessed by surveying practitioners."],"url":"http://arxiv.org/abs/2403.03819v1","category":"cs.SE"}
{"created":"2024-03-06 16:01:44","title":"Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ","abstract":"Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages beyond their intended use. Most models are more accurate when they respond faithfully. However, differences across models are large, and there is a long tail of languages where models are neither accurate nor faithful. We explore differences in tokenization as a potential explanation for our findings, identifying possible correlations that warrant further investigation.","sentences":["Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers.","However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen).","Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages.","Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use.","For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages.","With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy.","All LLMs we test respond faithfully and/or accurately for at least some languages beyond their intended use.","Most models are more accurate when they respond faithfully.","However, differences across models are large, and there is a long tail of languages where models are neither accurate nor faithful.","We explore differences in tokenization as a potential explanation for our findings, identifying possible correlations that warrant further investigation."],"url":"http://arxiv.org/abs/2403.03814v1","category":"cs.CL"}
{"created":"2024-03-06 16:00:50","title":"ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing","abstract":"Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics. With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions. However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about. Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge. We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting techniques. Furthermore, acknowledging that the business prefers pricing used cars based on the number of days the vehicle was listed for sale, we show how ProbSAINT can be used as a dynamic forecasting model for predicting price probabilities for different expected offer duration. Our experiments further indicate that ProbSAINT is especially accurate on instances where it is highly certain. This proves the applicability of its probabilistic predictions in real-world scenarios where trustworthiness is crucial.","sentences":["Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics.","With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions.","However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about.","Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge.","We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting techniques.","Furthermore, acknowledging that the business prefers pricing used cars based on the number of days the vehicle was listed for sale, we show how ProbSAINT can be used as a dynamic forecasting model for predicting price probabilities for different expected offer duration.","Our experiments further indicate that ProbSAINT is especially accurate on instances where it is highly certain.","This proves the applicability of its probabilistic predictions in real-world scenarios where trustworthiness is crucial."],"url":"http://arxiv.org/abs/2403.03812v1","category":"cs.LG"}
{"created":"2024-03-06 15:59:39","title":"Confidence-Aware Decision-Making and Control for Tool Selection","abstract":"Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence. To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task. The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces). Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable. Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems.","sentences":["Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive.","While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability.","This reflective monitoring can enhance their embodied decision power, robustness and safety.","Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions.","We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action).","This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence.","To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task.","The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces).","Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable.","Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems."],"url":"http://arxiv.org/abs/2403.03808v1","category":"cs.RO"}
{"created":"2024-03-06 15:37:22","title":"KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs","abstract":"Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings.","sentences":["Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes.","Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data.","To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE.","Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships.","KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs.","Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE).","The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings."],"url":"http://arxiv.org/abs/2403.03791v1","category":"cs.LG"}
{"created":"2024-03-06 15:23:26","title":"Neural Architecture Search using Particle Swarm and Ant Colony Optimization","abstract":"Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are used as the SI algorithms. Furthermore, models developed through such metaheuristics may be combined using stacking ensembles. In the context of this paper, we focus on training and optimizing CNNs using the Swarm Intelligence (SI) components of OpenNAS. Two major types of SI algorithms, namely PSO and ACO, are compared to see which is more effective in generating higher model accuracies. It is shown, with our experimental design, that the PSO algorithm performs better than ACO. The performance improvement of PSO is most notable with a more complex dataset. As a baseline, the performance of fine-tuned pre-trained models is also evaluated.","sentences":["Neural network models have a number of hyperparameters that must be chosen along with their architecture.","This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters.","In most cases, default hyperparameters and architectures are used.","Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures.","A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures.","A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research.","OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach.","Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are used as the SI algorithms.","Furthermore, models developed through such metaheuristics may be combined using stacking ensembles.","In the context of this paper, we focus on training and optimizing CNNs using the Swarm Intelligence (SI) components of OpenNAS.","Two major types of SI algorithms, namely PSO and ACO, are compared to see which is more effective in generating higher model accuracies.","It is shown, with our experimental design, that the PSO algorithm performs better than ACO.","The performance improvement of PSO is most notable with a more complex dataset.","As a baseline, the performance of fine-tuned pre-trained models is also evaluated."],"url":"http://arxiv.org/abs/2403.03781v1","category":"cs.NE"}
{"created":"2024-03-06 15:15:42","title":"ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport","abstract":"We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our method, called Expectile-Regularised Neural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime).","sentences":["We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials.","The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction.","We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials.","Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning.","We formally justify the efficiency of our method, called Expectile-Regularised Neural Optimal Transport (ENOT).","ENOT outperforms previous state-of-the-art approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime)."],"url":"http://arxiv.org/abs/2403.03777v1","category":"cs.LG"}
{"created":"2024-03-06 15:08:21","title":"Photonic-electronic spiking neuron with multi-modal and multi-wavelength excitatory and inhibitory operation for high-speed neuromorphic sensing and computing","abstract":"We report a multi-modal spiking neuron that allows optical and electronic input and control, and wavelength-multiplexing operation, for use in novel high-speed neuromorphic sensing and computing functionalities. The photonic-electronic neuron is built with a micro-scale, nanostructure resonant tunnelling diode (RTD) with photodetection (PD) capability. Leveraging the advantageous intrinsic properties of this RTD-PD system, namely highly nonlinear characteristics, photo-sensitivity, light-induced I-V curve shift, and the ability to deliver excitable responses under electrical and optical inputs, we successfully achieve flexible neuromorphic spike activation and inhibition regimes through photonic-electrical control. We also demonstrate the ability of this RTD-PD spiking sensing-processing neuron to operate under the simultaneous arrival of multiple wavelength-multiplexed optical signals, due to its large photodetection spectral window (covering the 1310 and 1550 nm telecom wavelength bands). Our results highlight the potential of RTD photonic-electronic neurons to reproduce multiple key excitatory and inhibitory spiking regimes, at high speed (ns-rate spiking responses, with faster sub-ns regimes theoretically predicted) and low energy (requiring only ~10 mV and ~150 microW, electrical and optical input amplitudes, respectively), similar in nature to those commonly found in the biological neurons of the visual system and the brain. This work offers a highly promising approach for the realisation of high-speed, energy-efficient photonic-electronic spiking neurons and spiking neural networks, enabling multi-modal and multi-wavelength operation for sensing and information processing tasks. This work therefore paves the way for innovative high-speed, photonic-electronic, and spike-based neuromorphic sensing and computing systems and artificial intelligence hardware.","sentences":["We report a multi-modal spiking neuron that allows optical and electronic input and control, and wavelength-multiplexing operation, for use in novel high-speed neuromorphic sensing and computing functionalities.","The photonic-electronic neuron is built with a micro-scale, nanostructure resonant tunnelling diode (RTD) with photodetection (PD) capability.","Leveraging the advantageous intrinsic properties of this RTD-PD system, namely highly nonlinear characteristics, photo-sensitivity, light-induced I-V curve shift, and the ability to deliver excitable responses under electrical and optical inputs, we successfully achieve flexible neuromorphic spike activation and inhibition regimes through photonic-electrical control.","We also demonstrate the ability of this RTD-PD spiking sensing-processing neuron to operate under the simultaneous arrival of multiple wavelength-multiplexed optical signals, due to its large photodetection spectral window (covering the 1310 and 1550 nm telecom wavelength bands).","Our results highlight the potential of RTD photonic-electronic neurons to reproduce multiple key excitatory and inhibitory spiking regimes, at high speed (ns-rate spiking responses, with faster sub-ns regimes theoretically predicted) and low energy (requiring only ~10 mV and ~150 microW, electrical and optical input amplitudes, respectively), similar in nature to those commonly found in the biological neurons of the visual system and the brain.","This work offers a highly promising approach for the realisation of high-speed, energy-efficient photonic-electronic spiking neurons and spiking neural networks, enabling multi-modal and multi-wavelength operation for sensing and information processing tasks.","This work therefore paves the way for innovative high-speed, photonic-electronic, and spike-based neuromorphic sensing and computing systems and artificial intelligence hardware."],"url":"http://arxiv.org/abs/2403.03775v1","category":"physics.optics"}
{"created":"2024-03-06 15:03:09","title":"DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models","abstract":"The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater effectiveness than a comparator set of two approved drug in 5/8 colorectal cancer (CRC) organoids. This highlights DeepCRE's ability to identify a collection of drug candidates with superior therapeutic effects, underscoring its potential to revolutionize the field of therapeutic development.","sentences":["The field of pharmaceutical development and therapeutic application both face substantial challenges.","Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails.","One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development.","Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis.","Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development.","DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\\% in patient-level CRE, and a 5-fold increase in indication-level CRE.","Furthermore, DeepCRE has identified six drug candidates that show significantly greater effectiveness than a comparator set of two approved drug in 5/8 colorectal cancer (CRC) organoids.","This highlights DeepCRE's ability to identify a collection of drug candidates with superior therapeutic effects, underscoring its potential to revolutionize the field of therapeutic development."],"url":"http://arxiv.org/abs/2403.03768v1","category":"cs.AI"}
{"created":"2024-03-06 15:03:04","title":"Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks","abstract":"The critical micelle concentration (CMC) of surfactant molecules is an essential property for surfactant applications in industry. Recently, classical QSPR and Graph Neural Networks (GNNs), a deep learning technique, have been successfully applied to predict the CMC of surfactants at room temperature. However, these models have not yet considered the temperature dependency of the CMC, which is highly relevant for practical applications. We herein develop a GNN model for temperature-dependent CMC prediction of surfactants. We collect about 1400 data points from public sources for all surfactant classes, i.e., ionic, nonionic, and zwitterionic, at multiple temperatures. We test the predictive quality of the model for following scenarios: i) when CMC data for surfactants are present in the training of the model in at least one different temperature, and ii) CMC data for surfactants are not present in the training, i.e., generalizing to unseen surfactants. In both test scenarios, our model exhibits a high predictive performance of R$^2 \\geq $ 0.94 on test data. We also find that the model performance varies by surfactant class. Finally, we evaluate the model for sugar-based surfactants with complex molecular structures, as these represent a more sustainable alternative to synthetic surfactants and are therefore of great interest for future applications in the personal and home care industries.","sentences":["The critical micelle concentration (CMC) of surfactant molecules is an essential property for surfactant applications in industry.","Recently, classical QSPR and Graph Neural Networks (GNNs), a deep learning technique, have been successfully applied to predict the CMC of surfactants at room temperature.","However, these models have not yet considered the temperature dependency of the CMC, which is highly relevant for practical applications.","We herein develop a GNN model for temperature-dependent CMC prediction of surfactants.","We collect about 1400 data points from public sources for all surfactant classes, i.e., ionic, nonionic, and zwitterionic, at multiple temperatures.","We test the predictive quality of the model for following scenarios: i) when CMC data for surfactants are present in the training of the model in at least one different temperature, and ii) CMC data for surfactants are not present in the training, i.e., generalizing to unseen surfactants.","In both test scenarios, our model exhibits a high predictive performance of R$^2 \\geq $ 0.94 on test data.","We also find that the model performance varies by surfactant class.","Finally, we evaluate the model for sugar-based surfactants with complex molecular structures, as these represent a more sustainable alternative to synthetic surfactants and are therefore of great interest for future applications in the personal and home care industries."],"url":"http://arxiv.org/abs/2403.03767v1","category":"physics.chem-ph"}
{"created":"2024-03-06 14:37:30","title":"German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset","abstract":"The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and release the absinth dataset to foster further research on hallucination detection in German.","sentences":["The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks.","Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document.","Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries.","However, these works primarily focus on English and recent multilingual approaches lack German data.","This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings.","We open-source and release the absinth dataset to foster further research on hallucination detection in German."],"url":"http://arxiv.org/abs/2403.03750v1","category":"cs.CL"}
{"created":"2024-03-06 14:34:07","title":"Towards Safe and Aligned Large Language Models for Medicine","abstract":"The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale approaches used by the machine learning community to develop safe and aligned LLMs. We hope that this work casts light on the safety and alignment of medical LLMs and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of LLMs in medicine.","sentences":["The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks.","While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights.","To this end, we carry out the first safety evaluation for medical LLMs.","Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale approaches used by the machine learning community to develop safe and aligned LLMs.","We hope that this work casts light on the safety and alignment of medical LLMs and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of LLMs in medicine."],"url":"http://arxiv.org/abs/2403.03744v1","category":"cs.AI"}
{"created":"2024-03-06 14:30:09","title":"SUPClust: Active Learning at the Boundaries","abstract":"Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.","sentences":["Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire.","In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes.","By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions.","We demonstrate experimentally that labeling these points leads to strong model performance.","This improvement is observed even in scenarios characterized by strong class imbalance."],"url":"http://arxiv.org/abs/2403.03741v1","category":"cs.LG"}
{"created":"2024-03-06 14:28:49","title":"A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network","abstract":"Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art. Ablation studies have verified the efficacy of the quantized RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU. The proposed add&bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture.","sentences":["Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden.","However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations.","A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture.","The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations.","The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2.","Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art.","Ablation studies have verified the efficacy of the quantized RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU.","The proposed add&bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture."],"url":"http://arxiv.org/abs/2403.03739v1","category":"cs.LG"}
{"created":"2024-03-06 14:19:11","title":"Learning 3D object-centric representation through prediction","abstract":"As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.","sentences":["As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning.","While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking.","Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion.","The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes.","This results in object representations being learned as an essential byproduct of learning to predict."],"url":"http://arxiv.org/abs/2403.03730v1","category":"cs.CV"}
{"created":"2024-03-06 14:18:24","title":"Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training","abstract":"This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.","sentences":["This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models.","We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels.","By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies.","Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes."],"url":"http://arxiv.org/abs/2403.03728v1","category":"cs.LG"}
{"created":"2024-03-06 14:15:20","title":"Diffusion on language model embeddings for protein sequence generation","abstract":"Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space. This work advances the field of protein design and sets the stage for conditional models by providing a robust framework for scalable and high-quality protein sequence generation.","sentences":["Protein design requires a deep understanding of the inherent complexities of the protein universe.","While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued.","Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences.","DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance.","We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities.","Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space.","This work advances the field of protein design and sets the stage for conditional models by providing a robust framework for scalable and high-quality protein sequence generation."],"url":"http://arxiv.org/abs/2403.03726v1","category":"cs.LG"}
{"created":"2024-03-06 13:59:53","title":"Intent-aware Recommendation via Disentangled Graph Contrastive Learning","abstract":"Graph neural network (GNN) based recommender systems have become one of the mainstream trends due to the powerful learning ability from user behavior data. Understanding the user intents from behavior data is the key to recommender systems, which poses two basic requirements for GNN-based recommender systems. One is how to learn complex and diverse intents especially when the user behavior is usually inadequate in reality. The other is different behaviors have different intent distributions, so how to establish their relations for a more explainable recommender system. In this paper, we present the Intent-aware Recommendation via Disentangled Graph Contrastive Learning (IDCL), which simultaneously learns interpretable intents and behavior distributions over those intents. Specifically, we first model the user behavior data as a user-item-concept graph, and design a GNN based behavior disentangling module to learn the different intents. Then we propose the intent-wise contrastive learning to enhance the intent disentangling and meanwhile infer the behavior distributions. Finally, the coding rate reduction regularization is introduced to make the behaviors of different intents orthogonal. Extensive experiments demonstrate the effectiveness of IDCL in terms of substantial improvement and the interpretability.","sentences":["Graph neural network (GNN) based recommender systems have become one of the mainstream trends due to the powerful learning ability from user behavior data.","Understanding the user intents from behavior data is the key to recommender systems, which poses two basic requirements for GNN-based recommender systems.","One is how to learn complex and diverse intents especially when the user behavior is usually inadequate in reality.","The other is different behaviors have different intent distributions, so how to establish their relations for a more explainable recommender system.","In this paper, we present the Intent-aware Recommendation via Disentangled Graph Contrastive Learning (IDCL), which simultaneously learns interpretable intents and behavior distributions over those intents.","Specifically, we first model the user behavior data as a user-item-concept graph, and design a GNN based behavior disentangling module to learn the different intents.","Then we propose the intent-wise contrastive learning to enhance the intent disentangling and meanwhile infer the behavior distributions.","Finally, the coding rate reduction regularization is introduced to make the behaviors of different intents orthogonal.","Extensive experiments demonstrate the effectiveness of IDCL in terms of substantial improvement and the interpretability."],"url":"http://arxiv.org/abs/2403.03714v1","category":"cs.IR"}
{"created":"2024-03-06 13:49:15","title":"Amplitude analysis of the $\u039b_b^0\\to pK^-\u03b3$ decay","abstract":"The resonant structure of the radiative decay $\\Lambda_b^0\\to pK^-\\gamma$ in the region of proton-kaon invariant-mass up to 2.5 GeV$/c^2$ is studied using proton-proton collision data recorded at centre-of-mass energies of 7, 8, and 13 TeV collected with the LHCb detector, corresponding to a total integrated luminosity of 9 fb$^{-1}$. Results are given in terms of fit and interference fractions between the different components contributing to this final state. Only $\\Lambda$ resonances decaying to $pK^-$ are found to be relevant, where the largest contributions stem from the $\\Lambda(1520)$, $\\Lambda(1600)$, $\\Lambda(1800)$, and $\\Lambda(1890)$ states.","sentences":["The resonant structure of the radiative decay $\\Lambda_b^0\\to pK^-\\gamma$ in the region of proton-kaon invariant-mass up to 2.5 GeV$/c^2$ is studied using proton-proton collision data recorded at centre-of-mass energies of 7, 8, and 13 TeV collected with the LHCb detector, corresponding to a total integrated luminosity of 9 fb$^{-1}$. Results are given in terms of fit and interference fractions between the different components contributing to this final state.","Only $\\Lambda$ resonances decaying to $pK^-$ are found to be relevant, where the largest contributions stem from the $\\Lambda(1520)$, $\\Lambda(1600)$, $\\Lambda(1800)$, and $\\Lambda(1890)$ states."],"url":"http://arxiv.org/abs/2403.03710v1","category":"hep-ex"}
{"created":"2024-03-06 13:29:00","title":"Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies","abstract":"Neural networks have become a cornerstone of machine learning. As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment. In this survey we answer three research questions: \"What types of model parallelism exist?\", \"What are the challenges of model parallelism?\", and \"What is a modern use-case of model parallelism?\" We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions. The dimensions along which neural networks can be parallelised are intra-operator and inter-operator. We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph. We answer the last question by collecting and listing how parallelism is applied in modern multi-billion parameter transformer networks, to the extend that this is possible with the limited information shared about these networks.","sentences":["Neural networks have become a cornerstone of machine learning.","As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment.","In this survey we answer three research questions: \"What types of model parallelism exist?\", \"What are the challenges of model parallelism?","\", and \"What is a modern use-case of model parallelism?\"","We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions.","The dimensions along which neural networks can be parallelised are intra-operator and inter-operator.","We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph.","We answer the last question by collecting and listing how parallelism is applied in modern multi-billion parameter transformer networks, to the extend that this is possible with the limited information shared about these networks."],"url":"http://arxiv.org/abs/2403.03699v1","category":"cs.DC"}
{"created":"2024-03-06 13:27:34","title":"Towards Controllable Time Series Generation","abstract":"Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications. Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets. This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions. To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.   In this paper, we propose \\textbf{C}ontrollable \\textbf{T}ime \\textbf{S}eries (\\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key feature of \\textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interplay between latent features and external conditions. Moreover, we develop a comprehensive evaluation scheme for CTSG. Extensive experiments across three real-world time series datasets showcase \\textsf{CTS}'s exceptional capabilities in generating high-quality, controllable outputs. This underscores its adeptness in seamlessly integrating latent features with external conditions. Extending \\textsf{CTS} to the image domain highlights its remarkable potential for explainability and further reinforces its versatility across different modalities.","sentences":["Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications.","Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets.","This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions.","To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.   ","In this paper, we propose \\textbf{C}ontrollable \\textbf{T}ime \\textbf{S}eries (\\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG.","A key feature of \\textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interplay between latent features and external conditions.","Moreover, we develop a comprehensive evaluation scheme for CTSG.","Extensive experiments across three real-world time series datasets showcase \\textsf{CTS}'s exceptional capabilities in generating high-quality, controllable outputs.","This underscores its adeptness in seamlessly integrating latent features with external conditions.","Extending \\textsf{CTS} to the image domain highlights its remarkable potential for explainability and further reinforces its versatility across different modalities."],"url":"http://arxiv.org/abs/2403.03698v1","category":"cs.LG"}
{"created":"2024-03-06 13:17:41","title":"MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition","abstract":"In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature. To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer. This integration facilitates a more nuanced extraction of both local and global features from molecular images. MolNexTR can predict atoms and bonds simultaneously and understand their layout rules. It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures. We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing module to get the final SMILES output. These modules synergistically enhance the model's robustness against the diverse styles of molecular imagery found in real literature. In our test sets, MolNexTR has demonstrated superior performance, achieving an accuracy rate of 81-97%, marking a significant advancement in the domain of molecular structure recognition. Scientific contribution: MolNexTR is a novel image-to-graph model that incorporates a unique dual-stream encoder to extract complex molecular image features, and combines chemical rules to predict atoms and bonds while understanding atom and bond layout rules. In addition, it employs a series of novel augmentation algorithms to significantly enhance the robustness and performance of the model.","sentences":["In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature.","To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer.","This integration facilitates a more nuanced extraction of both local and global features from molecular images.","MolNexTR can predict atoms and bonds simultaneously and understand their layout rules.","It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures.","We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing module to get the final SMILES output.","These modules synergistically enhance the model's robustness against the diverse styles of molecular imagery found in real literature.","In our test sets, MolNexTR has demonstrated superior performance, achieving an accuracy rate of 81-97%, marking a significant advancement in the domain of molecular structure recognition.","Scientific contribution: MolNexTR is a novel image-to-graph model that incorporates a unique dual-stream encoder to extract complex molecular image features, and combines chemical rules to predict atoms and bonds while understanding atom and bond layout rules.","In addition, it employs a series of novel augmentation algorithms to significantly enhance the robustness and performance of the model."],"url":"http://arxiv.org/abs/2403.03691v1","category":"cs.CV"}
{"created":"2024-03-07 16:57:24","title":"Designing variational ansatz for quantum-enabled simulation of non-unitary dynamical evolution -- an excursion into Dicke supperradiance","abstract":"Adaptive Variational Quantum Dynamics (AVQD) algorithms offer a promising approach to providing quantum-enabled solutions for systems treated within the purview of open quantum dynamical evolution. In this study, we employ the unrestricted vectorization variant of AVQD to simulate and benchmark various non-unitarily evolving systems. We exemplify how construction of an expressible ansatz unitary and the associated operator pool can be implemented to analyze examples such as the Fenna Matthews Olson complex (FMO) and even the permutational invariant Dicke model of quantum optics. We furthermore show an efficient decomposition scheme for the ansatz used, which can extend its applications to a wide range of other open quantum system scenarios in near future. In all cases the results obtained are in excellent agreement with exact numerical computations which bolsters the effectiveness of this technique. Our successful demonstrations pave the way for utilizing this adaptive variational technique to study complex systems in chemistry and physics, like light harvesting devices, thermal, and opto mechanical switches, to name a few.","sentences":["Adaptive Variational Quantum Dynamics (AVQD) algorithms offer a promising approach to providing quantum-enabled solutions for systems treated within the purview of open quantum dynamical evolution.","In this study, we employ the unrestricted vectorization variant of AVQD to simulate and benchmark various non-unitarily evolving systems.","We exemplify how construction of an expressible ansatz unitary and the associated operator pool can be implemented to analyze examples such as the Fenna Matthews Olson complex (FMO) and even the permutational invariant Dicke model of quantum optics.","We furthermore show an efficient decomposition scheme for the ansatz used, which can extend its applications to a wide range of other open quantum system scenarios in near future.","In all cases the results obtained are in excellent agreement with exact numerical computations which bolsters the effectiveness of this technique.","Our successful demonstrations pave the way for utilizing this adaptive variational technique to study complex systems in chemistry and physics, like light harvesting devices, thermal, and opto mechanical switches, to name a few."],"url":"http://arxiv.org/abs/2403.04653v1","category":"quant-ph"}
{"created":"2024-03-07 18:14:02","title":"QRtree -- Decision Tree dialect specification of QRscript","abstract":"This specification document specifies the syntax and semantics of QRtree, which is a specific dialect of QRscript particularly suited to represent decision trees without chance nodes. The term dialect identifies one of the possible sub-languages that can be encoded inside of an eQR code via QRscript. This specification will describe an intermediate representation of QRtree, made through a language derived by the three-address code. It will then define the transformation rules from the intermediate representation to a binary code. The latter is a binary representation called eQRtreebytecode. These rules can also be applied inversely to transform the eQRtreeBytecode into the intermediate representation. This specification document will pay particular attention to the creation of a compact eQRtreebytecode, as the maximum number of bits that can be stored in a QR code is, at the time of writing, equal to 2953 bytes (in the case of QR code version 40 with a \"low\" error correction level).","sentences":["This specification document specifies the syntax and semantics of QRtree, which is a specific dialect of QRscript particularly suited to represent decision trees without chance nodes.","The term dialect identifies one of the possible sub-languages that can be encoded inside of an eQR code via QRscript.","This specification will describe an intermediate representation of QRtree, made through a language derived by the three-address code.","It will then define the transformation rules from the intermediate representation to a binary code.","The latter is a binary representation called eQRtreebytecode.","These rules can also be applied inversely to transform the eQRtreeBytecode into the intermediate representation.","This specification document will pay particular attention to the creation of a compact eQRtreebytecode, as the maximum number of bits that can be stored in a QR code is, at the time of writing, equal to 2953 bytes (in the case of QR code version 40 with a \"low\" error correction level)."],"url":"http://arxiv.org/abs/2403.04716v1","category":"cs.NI"}
{"created":"2024-03-06 18:59:00","title":"Understanding Stabilizer Codes Under Local Decoherence Through a General Statistical Mechanics Mapping","abstract":"We consider the problem of a generic stabilizer Hamiltonian under local, incoherent Pauli errors. Using two different approaches -- (i) Haah's polynomial formalism arXiv:1204.1063 and (ii) the homological perspective on CSS codes -- we construct a mapping from the $n$th moment of the decohered ground state density matrix to a classical statistical mechanics model. We demonstrate that various measures of information capacity -- (i) quantum relative entropy, (ii) coherent information, and (iii) entanglement negativity -- map to thermodynamic quantities in the statistical mechanics model and can be used to characterize the decoding phase transition. As examples, we analyze the 3D toric code and X-cube model, deriving bounds on their optimal decoding thresholds and gaining insight into their information properties under decoherence. Additionally, we demonstrate that the SM mapping acts an an \"ungauging\" map; the classical models that describe a given code under decoherence also can be gauged to obtain the same code. Finally, we comment on correlated errors and non-CSS stabilizer codes.","sentences":["We consider the problem of a generic stabilizer Hamiltonian under local, incoherent Pauli errors.","Using two different approaches -- (i) Haah's polynomial formalism arXiv:1204.1063 and (ii) the homological perspective on CSS codes -- we construct a mapping from the $n$th moment of the decohered ground state density matrix to a classical statistical mechanics model.","We demonstrate that various measures of information capacity -- (i) quantum relative entropy, (ii) coherent information, and (iii) entanglement negativity -- map to thermodynamic quantities in the statistical mechanics model and can be used to characterize the decoding phase transition.","As examples, we analyze the 3D toric code and X-cube model, deriving bounds on their optimal decoding thresholds and gaining insight into their information properties under decoherence.","Additionally, we demonstrate that the SM mapping acts an an \"ungauging\" map; the classical models that describe a given code under decoherence also can be gauged to obtain the same code.","Finally, we comment on correlated errors and non-CSS stabilizer codes."],"url":"http://arxiv.org/abs/2403.03955v1","category":"quant-ph"}
{"created":"2024-03-06 18:52:48","title":"Pressure-enhanced $f$-electron orbital weighting in UTe2 mapped by quantum interferometry","abstract":"The phase landscape of UTe$_2$ features a remarkable diversity of superconducting phases under applied pressure and magnetic field. Recent quantum oscillation studies at ambient pressure have revealed the quasi-2D Fermi surface of this material. However, the pressure-dependence of the Fermi surface remains an open question. Here we track the evolution of the UTe$_2$ Fermi surface as a function of pressure up to 19.5 kbar by measuring quantum interference oscillations. We find that in sufficient magnetic field to suppress both superconductivity at low pressures and incommensurate antiferromagnetism at higher pressures, the quasi-2D Fermi surface found at ambient pressure smoothly connects to that at 19.5 kbar, with no signs of a reconstruction over this pressure interval. The warping of the cylindrical Fermi sheets continuously increases with pressure, which is consistent with increased $f$-orbital contribution at the Fermi level, up to and beyond the critical pressure at which superconductivity is truncated. These findings highlight the value of high pressure quantum interference measurements as a new probe of the electronic structure in heavy fermion materials.","sentences":["The phase landscape of UTe$_2$ features a remarkable diversity of superconducting phases under applied pressure and magnetic field.","Recent quantum oscillation studies at ambient pressure have revealed the quasi-2D Fermi surface of this material.","However, the pressure-dependence of the Fermi surface remains an open question.","Here we track the evolution of the UTe$_2$ Fermi surface as a function of pressure up to 19.5 kbar by measuring quantum interference oscillations.","We find that in sufficient magnetic field to suppress both superconductivity at low pressures and incommensurate antiferromagnetism at higher pressures, the quasi-2D Fermi surface found at ambient pressure smoothly connects to that at 19.5 kbar, with no signs of a reconstruction over this pressure interval.","The warping of the cylindrical Fermi sheets continuously increases with pressure, which is consistent with increased $f$-orbital contribution at the Fermi level, up to and beyond the critical pressure at which superconductivity is truncated.","These findings highlight the value of high pressure quantum interference measurements as a new probe of the electronic structure in heavy fermion materials."],"url":"http://arxiv.org/abs/2403.03946v1","category":"cond-mat.supr-con"}
{"created":"2024-03-06 18:52:39","title":"SPEAR:Exact Gradient Inversion of Batches in Federated Learning","abstract":"Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \\emph{the first algorithm reconstructing whole batches with $b >1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers batches of $b \\lesssim 25$ elements exactly while being tractable for large network widths and depths.","sentences":["Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data.","Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients.","Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction.","In this work, we propose \\emph{the first algorithm reconstructing whole batches with $b >1$ exactly}.","This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm.","Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable.","We provide an efficient GPU implementation for fully connected networks and show that it recovers batches of $b \\lesssim 25$ elements exactly while being tractable for large network widths and depths."],"url":"http://arxiv.org/abs/2403.03945v1","category":"cs.LG"}
{"created":"2024-03-06 18:51:34","title":"MR.RGM: An R Package for Fitting Bayesian Multivariate Bidirectional Mendelian Randomization Networks","abstract":"Motivation: Mendelian randomization (MR) infers causal relationships between exposures and outcomes using genetic variants as instrumental variables. Typically, MR considers only a pair of exposure and outcome at a time, limiting its capability of capturing the entire causal network. We overcome this limitation by developing 'MR.RGM' (Mendelian randomization via reciprocal graphical model), a fast R-package that implements the Bayesian reciprocal graphical model and enables practitioners to construct holistic causal networks with possibly cyclic/reciprocal causation and proper uncertainty quantifications, offering a comprehensive understanding of complex biological systems and their interconnections. Results: We developed 'MR.RGM', an open-source R package that applies bidirectional MR using a network-based strategy, enabling the exploration of causal relationships among multiple variables in complex biological systems. 'MR.RGM' holds the promise of unveiling intricate interactions and advancing our understanding of genetic networks, disease risks, and phenotypic complexities.","sentences":["Motivation: Mendelian randomization (MR) infers causal relationships between exposures and outcomes using genetic variants as instrumental variables.","Typically, MR considers only a pair of exposure and outcome at a time, limiting its capability of capturing the entire causal network.","We overcome this limitation by developing 'MR.RGM' (Mendelian randomization via reciprocal graphical model), a fast R-package that implements the Bayesian reciprocal graphical model and enables practitioners to construct holistic causal networks with possibly cyclic/reciprocal causation and proper uncertainty quantifications, offering a comprehensive understanding of complex biological systems and their interconnections.","Results:","We developed 'MR.RGM', an open-source R package that applies bidirectional MR using a network-based strategy, enabling the exploration of causal relationships among multiple variables in complex biological systems.","'MR.RGM' holds the promise of unveiling intricate interactions and advancing our understanding of genetic networks, disease risks, and phenotypic complexities."],"url":"http://arxiv.org/abs/2403.03944v1","category":"stat.AP"}
{"created":"2024-03-06 18:51:19","title":"Separate and Detailed Treatment of Absolute Signal and Noise Enables NMR Under Adverse Circumstances","abstract":"When deploying a spectrometer in an adverse environment, such as during a typical ODNP experiment or experiments that require low-volume low-field measurements, a clear and modern protocol for characterizing and quantifying the absolute signal and noise levels proves essential. This paper provides such a protocol. It also highlights the clarity and insight that comes from (1) discussing NMR signal intensities in (conserved) units of square root instantaneous power that are derived from a theory and notation developed initially for ESR spectroscopy; as well as (2) characterizing the spectral distribution of the noise.   Crucially, the strategy introduced here applies not only to ODNP measurements, but to all low-field NMR. Low-field NMR offers immense flexibility: it enables integration with other instrumentation and deploys in practical applications not accessible to higher-field instrumentation. More generally, the protocol introduced here should apply to a wide range of instruments, and should prove especially useful in cases subject to design constraints that requires integration with multiple other modules that are not dedicated to NMR but that control other forms of spectroscopy or other crucial aspects of the measurement. However, in the specific case of ODNP, this protocol demonstrates that the absolute signal and noise level can be estimated from the clarified theory presented here, and uses that theory to identify the inefficient distribution of fields in the hairpin loop probe as the main remaining bottleneck for the improvement of low-field low-volume ODNP SNR.","sentences":["When deploying a spectrometer in an adverse environment, such as during a typical ODNP experiment or experiments that require low-volume low-field measurements, a clear and modern protocol for characterizing and quantifying the absolute signal and noise levels proves essential.","This paper provides such a protocol.","It also highlights the clarity and insight that comes from (1) discussing NMR signal intensities in (conserved) units of square root instantaneous power that are derived from a theory and notation developed initially for ESR spectroscopy; as well as (2) characterizing the spectral distribution of the noise.   ","Crucially, the strategy introduced here applies not only to ODNP measurements, but to all low-field NMR.","Low-field NMR offers immense flexibility: it enables integration with other instrumentation and deploys in practical applications not accessible to higher-field instrumentation.","More generally, the protocol introduced here should apply to a wide range of instruments, and should prove especially useful in cases subject to design constraints that requires integration with multiple other modules that are not dedicated to NMR but that control other forms of spectroscopy or other crucial aspects of the measurement.","However, in the specific case of ODNP, this protocol demonstrates that the absolute signal and noise level can be estimated from the clarified theory presented here, and uses that theory to identify the inefficient distribution of fields in the hairpin loop probe as the main remaining bottleneck for the improvement of low-field low-volume ODNP SNR."],"url":"http://arxiv.org/abs/2403.03943v1","category":"physics.chem-ph"}
{"created":"2024-03-06 17:58:53","title":"Electrical Load Forecasting Model Using Hybrid LSTM Neural Networks with Online Correction","abstract":"Accurate electrical load forecasting is of great importance for the efficient operation and control of modern power systems. In this work, a hybrid long short-term memory (LSTM)-based model with online correction is developed for day-ahead electrical load forecasting. Firstly, four types of features are extracted from the original electrical load dataset, including the historical time series, time index features, historical statistical features, and similarity features. Then, a hybrid LSTM-based electrical load forecasting model is designed, where an LSTM neural network block and a fully-connected neural network block are integrated that can model both temporal features (historical time series) and non-temporal features (the rest features). A gradient regularization-based offline training algorithm and an output layer parameter fine-tuning-based online model correction method are developed to enhance the model's capabilities to defend against disturbance and adapt to the latest load data distribution, thus improving the forecasting accuracy. At last, extensive experiments are carried out to validate the effectiveness of the proposed electrical load forecasting strategy with superior accuracy compared with commonly used forecasting models.","sentences":["Accurate electrical load forecasting is of great importance for the efficient operation and control of modern power systems.","In this work, a hybrid long short-term memory (LSTM)-based model with online correction is developed for day-ahead electrical load forecasting.","Firstly, four types of features are extracted from the original electrical load dataset, including the historical time series, time index features, historical statistical features, and similarity features.","Then, a hybrid LSTM-based electrical load forecasting model is designed, where an LSTM neural network block and a fully-connected neural network block are integrated that can model both temporal features (historical time series) and non-temporal features (the rest features).","A gradient regularization-based offline training algorithm and an output layer parameter fine-tuning-based online model correction method are developed to enhance the model's capabilities to defend against disturbance and adapt to the latest load data distribution, thus improving the forecasting accuracy.","At last, extensive experiments are carried out to validate the effectiveness of the proposed electrical load forecasting strategy with superior accuracy compared with commonly used forecasting models."],"url":"http://arxiv.org/abs/2403.03898v1","category":"eess.SY"}
{"created":"2024-03-06 17:06:07","title":"ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain Adaptive Semantic Segmentation","abstract":"We consider unsupervised domain adaptation (UDA) for semantic segmentation in which the model is trained on a labeled source dataset and adapted to an unlabeled target dataset. Unfortunately, current self-training methods are susceptible to misclassified pseudo-labels resulting from erroneous predictions. Since certain classes are typically associated with less reliable predictions in UDA, reducing the impact of such pseudo-labels without skewing the training towards some classes is notoriously difficult. To this end, we propose an extensive cut-and-paste strategy (ECAP) to leverage reliable pseudo-labels through data augmentation. Specifically, ECAP maintains a memory bank of pseudo-labeled target samples throughout training and cut-and-pastes the most confident ones onto the current training batch. We implement ECAP on top of the recent method MIC and boost its performance on two synthetic-to-real domain adaptation benchmarks. Notably, MIC+ECAP reaches an unprecedented performance of 69.1 mIoU on the Synthia->Cityscapes benchmark. Our code is available at https://github.com/ErikBrorsson/ECAP.","sentences":["We consider unsupervised domain adaptation (UDA) for semantic segmentation in which the model is trained on a labeled source dataset and adapted to an unlabeled target dataset.","Unfortunately, current self-training methods are susceptible to misclassified pseudo-labels resulting from erroneous predictions.","Since certain classes are typically associated with less reliable predictions in UDA, reducing the impact of such pseudo-labels without skewing the training towards some classes is notoriously difficult.","To this end, we propose an extensive cut-and-paste strategy (ECAP) to leverage reliable pseudo-labels through data augmentation.","Specifically, ECAP maintains a memory bank of pseudo-labeled target samples throughout training and cut-and-pastes the most confident ones onto the current training batch.","We implement ECAP on top of the recent method MIC and boost its performance on two synthetic-to-real domain adaptation benchmarks.","Notably, MIC+ECAP reaches an unprecedented performance of 69.1 mIoU on the Synthia->Cityscapes benchmark.","Our code is available at https://github.com/ErikBrorsson/ECAP."],"url":"http://arxiv.org/abs/2403.03854v1","category":"cs.CV"}
{"created":"2024-03-06 16:57:36","title":"Numerical study of a viscous breaking water wave and the limit of vanishing viscosity","abstract":"We introduce a numerical strategy to study the evolution of 2D water waves in the presence of a plunging jet. The free-surface Navier-Stokes solution is obtained with a finite but small viscosity. We observe the formation of a surface boundary layer where the vorticity is localised. We highlight convergence to the inviscid solution. The effects of dissipation on the development of a singularity at the tip of the wave is also investigated by characterising the vorticity boundary layer appearing near the interface.","sentences":["We introduce a numerical strategy to study the evolution of 2D water waves in the presence of a plunging jet.","The free-surface Navier-Stokes solution is obtained with a finite but small viscosity.","We observe the formation of a surface boundary layer where the vorticity is localised.","We highlight convergence to the inviscid solution.","The effects of dissipation on the development of a singularity at the tip of the wave is also investigated by characterising the vorticity boundary layer appearing near the interface."],"url":"http://arxiv.org/abs/2403.03851v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 16:28:31","title":"An Adaptive Multivariate Functional EWMA Control Chart","abstract":"In many modern industrial scenarios, the measurements of the quality characteristics of interest are often required to be represented as functional data or profiles. This motivates the growing interest in extending traditional univariate statistical process monitoring (SPM) schemes to the functional data setting. This article proposes a new SPM scheme, which is referred to as adaptive multivariate functional EWMA (AMFEWMA), to extend the well-known exponentially weighted moving average (EWMA) control chart from the univariate scalar to the multivariate functional setting. The favorable performance of the AMFEWMA control chart over existing methods is assessed via an extensive Monte Carlo simulation. Its practical applicability is demonstrated through a case study in the monitoring of the quality of a resistance spot welding process in the automotive industry through the online observations of dynamic resistance curves, which are associated with multiple spot welds on the same car body and recognized as the full technological signature of the process.","sentences":["In many modern industrial scenarios, the measurements of the quality characteristics of interest are often required to be represented as functional data or profiles.","This motivates the growing interest in extending traditional univariate statistical process monitoring (SPM) schemes to the functional data setting.","This article proposes a new SPM scheme, which is referred to as adaptive multivariate functional EWMA (AMFEWMA), to extend the well-known exponentially weighted moving average (EWMA) control chart from the univariate scalar to the multivariate functional setting.","The favorable performance of the AMFEWMA control chart over existing methods is assessed via an extensive Monte Carlo simulation.","Its practical applicability is demonstrated through a case study in the monitoring of the quality of a resistance spot welding process in the automotive industry through the online observations of dynamic resistance curves, which are associated with multiple spot welds on the same car body and recognized as the full technological signature of the process."],"url":"http://arxiv.org/abs/2403.03837v1","category":"stat.ME"}
{"created":"2024-03-06 16:08:51","title":"HoLens: A Visual Analytics Design for Higher-order Movement Modeling and Visualization","abstract":"Higher-order patterns reveal sequential multistep state transitions, which are usually superior to origin-destination analysis, which depicts only first-order geospatial movement patterns. Conventional methods for higher-order movement modeling first construct a directed acyclic graph (DAG) of movements, then extract higher-order patterns from the DAG. However, DAG-based methods heavily rely on the identification of movement keypoints that are challenging for sparse movements and fail to consider the temporal variants that are critical for movements in urban environments. To overcome the limitations, we propose HoLens, a novel approach for modeling and visualizing higher-order movement patterns in the context of an urban environment. HoLens mainly makes twofold contributions: first, we design an auto-adaptive movement aggregation algorithm that self-organizes movements hierarchically by considering spatial proximity, contextual information, and temporal variability; second, we develop an interactive visual analytics interface consisting of well-established visualization techniques, including the H-Flow for visualizing the higher-order patterns on the map and the higher-order state sequence chart for representing the higher-order state transitions. Two real-world case studies manifest that the method can adaptively aggregate the data and exhibit the process of how to explore the higher-order patterns by HoLens. We also demonstrate our approach's feasibility, usability, and effectiveness through an expert interview with three domain experts.","sentences":["Higher-order patterns reveal sequential multistep state transitions, which are usually superior to origin-destination analysis, which depicts only first-order geospatial movement patterns.","Conventional methods for higher-order movement modeling first construct a directed acyclic graph (DAG) of movements, then extract higher-order patterns from the DAG.","However, DAG-based methods heavily rely on the identification of movement keypoints that are challenging for sparse movements and fail to consider the temporal variants that are critical for movements in urban environments.","To overcome the limitations, we propose HoLens, a novel approach for modeling and visualizing higher-order movement patterns in the context of an urban environment.","HoLens mainly makes twofold contributions: first, we design an auto-adaptive movement aggregation algorithm that self-organizes movements hierarchically by considering spatial proximity, contextual information, and temporal variability; second, we develop an interactive visual analytics interface consisting of well-established visualization techniques, including the H-Flow for visualizing the higher-order patterns on the map and the higher-order state sequence chart for representing the higher-order state transitions.","Two real-world case studies manifest that the method can adaptively aggregate the data and exhibit the process of how to explore the higher-order patterns by HoLens.","We also demonstrate our approach's feasibility, usability, and effectiveness through an expert interview with three domain experts."],"url":"http://arxiv.org/abs/2403.03822v1","category":"cs.HC"}
{"created":"2024-03-06 15:35:53","title":"Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery","abstract":"Ship detection needs to identify ship locations from remote sensing (RS) scenes. However, due to different imaging payloads, various appearances of ships, and complicated background interference from the bird's eye view, it is difficult to set up a unified paradigm for achieving multi-source ship detection. Therefore, in this article, considering that the large language models (LLMs) emerge the powerful generalization ability, a novel unified visual-language model called Popeye is proposed for multi-source ship detection from RS imagery. First, to bridge the interpretation gap between multi-source images for ship detection, a novel image-instruction-answer way is designed to integrate the various ship detection ways (e.g., horizontal bounding box (HBB), oriented bounding box (OBB)) into a unified labeling paradigm. Then, in view of this, a cross-modal image interpretation method is developed for the proposed Popeye to enhance interactive comprehension ability between visual and language content, which can be easily migrated into any multi-source ship detection task. Subsequently, owing to objective domain differences, a knowledge adaption mechanism is designed to adapt the pre-trained visual-language knowledge from the nature scene into the RS domain for multi-source ship detection. In addition, the segment anything model (SAM) is also seamlessly integrated into the proposed Popeye to achieve pixel-level ship segmentation without additional training costs. Finally, extensive experiments are conducted on the newly constructed instruction dataset named MMShip, and the results indicate that the proposed Popeye outperforms current specialist, open-vocabulary, and other visual-language models for zero-shot multi-source ship detection.","sentences":["Ship detection needs to identify ship locations from remote sensing (RS) scenes.","However, due to different imaging payloads, various appearances of ships, and complicated background interference from the bird's eye view, it is difficult to set up a unified paradigm for achieving multi-source ship detection.","Therefore, in this article, considering that the large language models (LLMs) emerge the powerful generalization ability, a novel unified visual-language model called Popeye is proposed for multi-source ship detection from RS imagery.","First, to bridge the interpretation gap between multi-source images for ship detection, a novel image-instruction-answer way is designed to integrate the various ship detection ways (e.g., horizontal bounding box (HBB), oriented bounding box (OBB)) into a unified labeling paradigm.","Then, in view of this, a cross-modal image interpretation method is developed for the proposed Popeye to enhance interactive comprehension ability between visual and language content, which can be easily migrated into any multi-source ship detection task.","Subsequently, owing to objective domain differences, a knowledge adaption mechanism is designed to adapt the pre-trained visual-language knowledge from the nature scene into the RS domain for multi-source ship detection.","In addition, the segment anything model (SAM) is also seamlessly integrated into the proposed Popeye to achieve pixel-level ship segmentation without additional training costs.","Finally, extensive experiments are conducted on the newly constructed instruction dataset named MMShip, and the results indicate that the proposed Popeye outperforms current specialist, open-vocabulary, and other visual-language models for zero-shot multi-source ship detection."],"url":"http://arxiv.org/abs/2403.03790v1","category":"cs.CV"}
{"created":"2024-03-06 15:26:26","title":"Noise-induced oscillations for the mean-field Dissipative Contact Process","abstract":"We study a dissipative version of the contact process, with mean-field interaction, which admits a simple epidemiological interpretation. The propagation of chaos and the corresponding normal fluctuations reveal that the noise present in the finite-size system induces oscillations with a nearly deterministic period and a randomly varying amplitude. This is reminiscent of the emergence of pandemic waves in real epidemics.","sentences":["We study a dissipative version of the contact process, with mean-field interaction, which admits a simple epidemiological interpretation.","The propagation of chaos and the corresponding normal fluctuations reveal that the noise present in the finite-size system induces oscillations with a nearly deterministic period and a randomly varying amplitude.","This is reminiscent of the emergence of pandemic waves in real epidemics."],"url":"http://arxiv.org/abs/2403.03783v1","category":"math.PR"}
{"created":"2024-03-06 15:16:31","title":"Ancestor regression in structural vector autoregressive models","abstract":"We present a new method for causal discovery in linear structural vector autoregressive models. We adapt an idea designed for independent observations to the case of time series while retaining its favorable properties, i.e., explicit error control for false causal discovery, at least asymptotically. We apply our method to several real-world bivariate time series datasets and discuss its findings which mostly agree with common understanding. The arrow of time in a model can be interpreted as background knowledge on possible causal mechanisms. Hence, our ideas could be extended to incorporating different background knowledge, even for independent observations.","sentences":["We present a new method for causal discovery in linear structural vector autoregressive models.","We adapt an idea designed for independent observations to the case of time series while retaining its favorable properties, i.e., explicit error control for false causal discovery, at least asymptotically.","We apply our method to several real-world bivariate time series datasets and discuss its findings which mostly agree with common understanding.","The arrow of time in a model can be interpreted as background knowledge on possible causal mechanisms.","Hence, our ideas could be extended to incorporating different background knowledge, even for independent observations."],"url":"http://arxiv.org/abs/2403.03778v1","category":"stat.ME"}
{"created":"2024-03-06 14:28:53","title":"Self-supervised Photographic Image Layout Representation Learning","abstract":"In the domain of image layout representation learning, the critical process of translating image layouts into succinct vector forms is increasingly significant across diverse applications, such as image retrieval, manipulation, and generation. Most approaches in this area heavily rely on costly labeled datasets and notably lack in adapting their modeling and learning methods to the specific nuances of photographic image layouts. This shortfall makes the learning process for photographic image layouts suboptimal. In our research, we directly address these challenges. We innovate by defining basic layout primitives that encapsulate various levels of layout information and by mapping these, along with their interconnections, onto a heterogeneous graph structure. This graph is meticulously engineered to capture the intricate layout information within the pixel domain explicitly. Advancing further, we introduce novel pretext tasks coupled with customized loss functions, strategically designed for effective self-supervised learning of these layout graphs. Building on this foundation, we develop an autoencoder-based network architecture skilled in compressing these heterogeneous layout graphs into precise, dimensionally-reduced layout representations. Additionally, we introduce the LODB dataset, which features a broader range of layout categories and richer semantics, serving as a comprehensive benchmark for evaluating the effectiveness of layout representation learning methods. Our extensive experimentation on this dataset demonstrates the superior performance of our approach in the realm of photographic image layout representation learning.","sentences":["In the domain of image layout representation learning, the critical process of translating image layouts into succinct vector forms is increasingly significant across diverse applications, such as image retrieval, manipulation, and generation.","Most approaches in this area heavily rely on costly labeled datasets and notably lack in adapting their modeling and learning methods to the specific nuances of photographic image layouts.","This shortfall makes the learning process for photographic image layouts suboptimal.","In our research, we directly address these challenges.","We innovate by defining basic layout primitives that encapsulate various levels of layout information and by mapping these, along with their interconnections, onto a heterogeneous graph structure.","This graph is meticulously engineered to capture the intricate layout information within the pixel domain explicitly.","Advancing further, we introduce novel pretext tasks coupled with customized loss functions, strategically designed for effective self-supervised learning of these layout graphs.","Building on this foundation, we develop an autoencoder-based network architecture skilled in compressing these heterogeneous layout graphs into precise, dimensionally-reduced layout representations.","Additionally, we introduce the LODB dataset, which features a broader range of layout categories and richer semantics, serving as a comprehensive benchmark for evaluating the effectiveness of layout representation learning methods.","Our extensive experimentation on this dataset demonstrates the superior performance of our approach in the realm of photographic image layout representation learning."],"url":"http://arxiv.org/abs/2403.03740v1","category":"cs.CV"}
{"created":"2024-03-06 14:12:38","title":"CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection","abstract":"Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results, but they often do not generalize well to target domains outside the source (or training) data distribution. To reduce such domain gaps and thus to make 3DOD models more generalizable, we introduce a novel unsupervised domain adaptation (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the domain gap in the cross-modal Bird's Eye View (BEV) representations. Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate domain-invariant features, which disrupt the discrimination of whether a feature instance comes from a source or an unseen target domain. Overall, our CMDA framework guides the 3DOD model to generate highly informative and domain-adaptive features for novel data distributions. In our extensive experiments with large-scale benchmarks, such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance.","sentences":["Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results, but they often do not generalize well to target domains outside the source (or training) data distribution.","To reduce such domain gaps and thus to make 3DOD models more generalizable, we introduce a novel unsupervised domain adaptation (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the domain gap in the cross-modal Bird's Eye View (BEV) representations.","Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate domain-invariant features, which disrupt the discrimination of whether a feature instance comes from a source or an unseen target domain.","Overall, our CMDA framework guides the 3DOD model to generate highly informative and domain-adaptive features for novel data distributions.","In our extensive experiments with large-scale benchmarks, such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.03721v2","category":"cs.CV"}
{"created":"2024-03-06 14:12:18","title":"Criminal organizations exhibit hysteresis, resilience, and robustness by balancing security and efficiency","abstract":"The interplay between criminal organizations and law enforcement disruption strategies is crucial in criminology. Criminal enterprises, like legitimate businesses, balance visibility and security to thrive. This study uses evolutionary game theory to analyze criminal networks' dynamics, resilience to interventions, and responses to external conditions. We find strong hysteresis effects, challenging traditional deterrence-focused strategies. Optimal thresholds for organization formation or dissolution are defined by these effects. Stricter punishment doesn't always deter organized crime linearly. Network structure, particularly link density and skill assortativity, significantly influences organization formation and stability. These insights advocate for adaptive policy-making and strategic law enforcement to effectively disrupt criminal networks.","sentences":["The interplay between criminal organizations and law enforcement disruption strategies is crucial in criminology.","Criminal enterprises, like legitimate businesses, balance visibility and security to thrive.","This study uses evolutionary game theory to analyze criminal networks' dynamics, resilience to interventions, and responses to external conditions.","We find strong hysteresis effects, challenging traditional deterrence-focused strategies.","Optimal thresholds for organization formation or dissolution are defined by these effects.","Stricter punishment doesn't always deter organized crime linearly.","Network structure, particularly link density and skill assortativity, significantly influences organization formation and stability.","These insights advocate for adaptive policy-making and strategic law enforcement to effectively disrupt criminal networks."],"url":"http://arxiv.org/abs/2403.03720v1","category":"physics.soc-ph"}
{"created":"2024-03-06 14:11:45","title":"Multimodal Transformer for Comics Text-Cloze","abstract":"This work explores a closure task in comics, a medium where visual and textual elements are intricately intertwined. Specifically, Text-cloze refers to the task of selecting the correct text to use in a comic panel, given its neighboring panels. Traditional methods based on recurrent neural networks have struggled with this task due to limited OCR accuracy and inherent model limitations. We introduce a novel Multimodal Large Language Model (Multimodal-LLM) architecture, specifically designed for Text-cloze, achieving a 10% improvement over existing state-of-the-art models in both its easy and hard variants. Central to our approach is a Domain-Adapted ResNet-50 based visual encoder, fine-tuned to the comics domain in a self-supervised manner using SimCLR. This encoder delivers comparable results to more complex models with just one-fifth of the parameters. Additionally, we release new OCR annotations for this dataset, enhancing model input quality and resulting in another 1% improvement. Finally, we extend the task to a generative format, establishing new baselines and expanding the research possibilities in the field of comics analysis.","sentences":["This work explores a closure task in comics, a medium where visual and textual elements are intricately intertwined.","Specifically, Text-cloze refers to the task of selecting the correct text to use in a comic panel, given its neighboring panels.","Traditional methods based on recurrent neural networks have struggled with this task due to limited OCR accuracy and inherent model limitations.","We introduce a novel Multimodal Large Language Model (Multimodal-LLM) architecture, specifically designed for Text-cloze, achieving a 10% improvement over existing state-of-the-art models in both its easy and hard variants.","Central to our approach is a Domain-Adapted ResNet-50 based visual encoder, fine-tuned to the comics domain in a self-supervised manner using SimCLR.","This encoder delivers comparable results to more complex models with just one-fifth of the parameters.","Additionally, we release new OCR annotations for this dataset, enhancing model input quality and resulting in another 1% improvement.","Finally, we extend the task to a generative format, establishing new baselines and expanding the research possibilities in the field of comics analysis."],"url":"http://arxiv.org/abs/2403.03719v1","category":"cs.CV"}
{"created":"2024-03-06 13:43:36","title":"Multi-Grained Cross-modal Alignment for Learning Open-vocabulary Semantic Segmentation from Text Supervision","abstract":"Recently, learning open-vocabulary semantic segmentation from text supervision has achieved promising downstream performance. Nevertheless, current approaches encounter an alignment granularity gap owing to the absence of dense annotations, wherein they learn coarse image/region-text alignment during training yet perform group/pixel-level predictions at inference. Such discrepancy leads to suboptimal learning efficiency and inferior zero-shot segmentation results. In this paper, we introduce a Multi-Grained Cross-modal Alignment (MGCA) framework, which explicitly learns pixel-level alignment along with object- and region-level alignment to bridge the granularity gap without any dense annotations. Specifically, MGCA ingeniously constructs pseudo multi-granular semantic correspondences upon image-text pairs and collaborates with hard sampling strategies to facilitate fine-grained cross-modal contrastive learning. Further, we point out the defects of existing group and pixel prediction units in downstream segmentation and develop an adaptive semantic unit which effectively mitigates their dilemmas including under- and over-segmentation. Training solely on CC3M, our method achieves significant advancements over state-of-the-art methods, demonstrating its effectiveness and efficiency.","sentences":["Recently, learning open-vocabulary semantic segmentation from text supervision has achieved promising downstream performance.","Nevertheless, current approaches encounter an alignment granularity gap owing to the absence of dense annotations, wherein they learn coarse image/region-text alignment during training yet perform group/pixel-level predictions at inference.","Such discrepancy leads to suboptimal learning efficiency and inferior zero-shot segmentation results.","In this paper, we introduce a Multi-Grained Cross-modal Alignment (MGCA) framework, which explicitly learns pixel-level alignment along with object- and region-level alignment to bridge the granularity gap without any dense annotations.","Specifically, MGCA ingeniously constructs pseudo multi-granular semantic correspondences upon image-text pairs and collaborates with hard sampling strategies to facilitate fine-grained cross-modal contrastive learning.","Further, we point out the defects of existing group and pixel prediction units in downstream segmentation and develop an adaptive semantic unit which effectively mitigates their dilemmas including under- and over-segmentation.","Training solely on CC3M, our method achieves significant advancements over state-of-the-art methods, demonstrating its effectiveness and efficiency."],"url":"http://arxiv.org/abs/2403.03707v1","category":"cs.CV"}
{"created":"2024-03-06 13:39:18","title":"Causal Prototype-inspired Contrast Adaptation for Unsupervised Domain Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery","abstract":"Semantic segmentation of high-resolution remote sensing imagery (HRSI) suffers from the domain shift, resulting in poor performance of the model in another unseen domain. Unsupervised domain adaptive (UDA) semantic segmentation aims to adapt the semantic segmentation model trained on the labeled source domain to an unlabeled target domain. However, the existing UDA semantic segmentation models tend to align pixels or features based on statistical information related to labels in source and target domain data, and make predictions accordingly, which leads to uncertainty and fragility of prediction results. In this paper, we propose a causal prototype-inspired contrast adaptation (CPCA) method to explore the invariant causal mechanisms between different HRSIs domains and their semantic labels. It firstly disentangles causal features and bias features from the source and target domain images through a causal feature disentanglement module. Then, a causal prototypical contrast module is used to learn domain invariant causal features. To further de-correlate causal and bias features, a causal intervention module is introduced to intervene on the bias features to generate counterfactual unbiased samples. By forcing the causal features to meet the principles of separability, invariance and intervention, CPCA can simulate the causal factors of source and target domains, and make decisions on the target domain based on the causal features, which can observe improved generalization ability. Extensive experiments under three cross-domain tasks indicate that CPCA is remarkably superior to the state-of-the-art methods.","sentences":["Semantic segmentation of high-resolution remote sensing imagery (HRSI) suffers from the domain shift, resulting in poor performance of the model in another unseen domain.","Unsupervised domain adaptive (UDA) semantic segmentation aims to adapt the semantic segmentation model trained on the labeled source domain to an unlabeled target domain.","However, the existing UDA semantic segmentation models tend to align pixels or features based on statistical information related to labels in source and target domain data, and make predictions accordingly, which leads to uncertainty and fragility of prediction results.","In this paper, we propose a causal prototype-inspired contrast adaptation (CPCA) method to explore the invariant causal mechanisms between different HRSIs domains and their semantic labels.","It firstly disentangles causal features and bias features from the source and target domain images through a causal feature disentanglement module.","Then, a causal prototypical contrast module is used to learn domain invariant causal features.","To further de-correlate causal and bias features, a causal intervention module is introduced to intervene on the bias features to generate counterfactual unbiased samples.","By forcing the causal features to meet the principles of separability, invariance and intervention, CPCA can simulate the causal factors of source and target domains, and make decisions on the target domain based on the causal features, which can observe improved generalization ability.","Extensive experiments under three cross-domain tasks indicate that CPCA is remarkably superior to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.03704v1","category":"cs.CV"}
{"created":"2024-03-06 12:57:48","title":"Simplified PCNet with Robustness","abstract":"Graph Neural Networks (GNNs) have garnered significant attention for their success in learning the representation of homophilic or heterophilic graphs. However, they cannot generalize well to real-world graphs with different levels of homophily. In response, the Possion-Charlier Network (PCNet) \\cite{li2024pc}, the previous work, allows graph representation to be learned from heterophily to homophily. Although PCNet alleviates the heterophily issue, there remain some challenges in further improving the efficacy and efficiency. In this paper, we simplify PCNet and enhance its robustness. We first extend the filter order to continuous values and reduce its parameters. Two variants with adaptive neighborhood sizes are implemented. Theoretical analysis shows our model's robustness to graph structure perturbations or adversarial attacks. We validate our approach through semi-supervised learning tasks on various datasets representing both homophilic and heterophilic graphs.","sentences":["Graph Neural Networks (GNNs) have garnered significant attention for their success in learning the representation of homophilic or heterophilic graphs.","However, they cannot generalize well to real-world graphs with different levels of homophily.","In response, the Possion-Charlier Network (PCNet) \\cite{li2024pc}, the previous work, allows graph representation to be learned from heterophily to homophily.","Although PCNet alleviates the heterophily issue, there remain some challenges in further improving the efficacy and efficiency.","In this paper, we simplify PCNet and enhance its robustness.","We first extend the filter order to continuous values and reduce its parameters.","Two variants with adaptive neighborhood sizes are implemented.","Theoretical analysis shows our model's robustness to graph structure perturbations or adversarial attacks.","We validate our approach through semi-supervised learning tasks on various datasets representing both homophilic and heterophilic graphs."],"url":"http://arxiv.org/abs/2403.03676v1","category":"cs.LG"}
{"created":"2024-03-06 12:53:27","title":"Revisiting phonon thermal transport in two-dimensional gallium nitride: higher-order phonon-phonon and phonon-electron scattering","abstract":"Two-dimensional gallium nitride (2D-GaN) has great potential in power electronics and optoelectronics. Heat dissipation is a critical issue for these applications of 2D-GaN. Previous studies showed that higher-order phonon-phonon scattering has extremely strong effects on the lattice thermal conductivity of 2D-GaN, which exhibits noticeable discrepancies with lattice thermal conductivity calculated from molecular dynamics. In this work, it is found that the fourth-order interatomic force constants (4th-IFCs) of 2D-GaN are quite sensitive to atomic displacement in the finite different method. The effects of the four-phonon scattering can be severely overestimated with non-convergent 4th-IFCs. The lattice thermal conductivity from three-phonon scattering is reduced by 65.6% due to four-phonon scattering. The reflection symmetry allows significantly more four-phonon processes than three-phonon processes. It was previously thought the electron-phonon interactions have significant effects on the lattice thermal conductivity of two-dimensional materials. However, the effects of phonon-electron interactions on the lattice thermal conductivity of both n-type and p-type 2D-GaN at high charge carrier concentrations can be neglected due to the few phonon-electron scattering channels and the relatively strong four-phonon scattering.","sentences":["Two-dimensional gallium nitride (2D-GaN) has great potential in power electronics and optoelectronics.","Heat dissipation is a critical issue for these applications of 2D-GaN. Previous studies showed that higher-order phonon-phonon scattering has extremely strong effects on the lattice thermal conductivity of 2D-GaN, which exhibits noticeable discrepancies with lattice thermal conductivity calculated from molecular dynamics.","In this work, it is found that the fourth-order interatomic force constants (4th-IFCs) of 2D-GaN are quite sensitive to atomic displacement in the finite different method.","The effects of the four-phonon scattering can be severely overestimated with non-convergent 4th-IFCs.","The lattice thermal conductivity from three-phonon scattering is reduced by 65.6% due to four-phonon scattering.","The reflection symmetry allows significantly more four-phonon processes than three-phonon processes.","It was previously thought the electron-phonon interactions have significant effects on the lattice thermal conductivity of two-dimensional materials.","However, the effects of phonon-electron interactions on the lattice thermal conductivity of both n-type and p-type 2D-GaN at high charge carrier concentrations can be neglected due to the few phonon-electron scattering channels and the relatively strong four-phonon scattering."],"url":"http://arxiv.org/abs/2403.03673v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-06 12:47:14","title":"CDC: A Simple Framework for Complex Data Clustering","abstract":"In today's data-driven digital era, the amount as well as complexity, such as multi-view, non-Euclidean, and multi-relational, of the collected data are growing exponentially or even faster. Clustering, which unsupervisely extracts valid knowledge from data, is extremely useful in practice. However, existing methods are independently developed to handle one particular challenge at the expense of the others. In this work, we propose a simple but effective framework for complex data clustering (CDC) that can efficiently process different types of data with linear complexity. We first utilize graph filtering to fuse geometry structure and attribute information. We then reduce the complexity with high-quality anchors that are adaptively learned via a novel similarity-preserving regularizer. We illustrate the cluster-ability of our proposed method theoretically and experimentally. In particular, we deploy CDC to graph data of size 111M.","sentences":["In today's data-driven digital era, the amount as well as complexity, such as multi-view, non-Euclidean, and multi-relational, of the collected data are growing exponentially or even faster.","Clustering, which unsupervisely extracts valid knowledge from data, is extremely useful in practice.","However, existing methods are independently developed to handle one particular challenge at the expense of the others.","In this work, we propose a simple but effective framework for complex data clustering (CDC) that can efficiently process different types of data with linear complexity.","We first utilize graph filtering to fuse geometry structure and attribute information.","We then reduce the complexity with high-quality anchors that are adaptively learned via a novel similarity-preserving regularizer.","We illustrate the cluster-ability of our proposed method theoretically and experimentally.","In particular, we deploy CDC to graph data of size 111M."],"url":"http://arxiv.org/abs/2403.03670v1","category":"cs.LG"}
{"created":"2024-03-06 12:31:02","title":"Harnessing Meta-Learning for Improving Full-Frame Video Stabilization","abstract":"Video stabilization is a longstanding computer vision problem, particularly pixel-level synthesis solutions for video stabilization which synthesize full frames add to the complexity of this task. These techniques aim to stabilize videos by synthesizing full frames while enhancing the stability of the considered video. This intensifies the complexity of the task due to the distinct mix of unique motion profiles and visual content present in each video sequence, making robust generalization with fixed parameters difficult. In our study, we introduce a novel approach to enhance the performance of pixel-level synthesis solutions for video stabilization by adapting these models to individual input video sequences. The proposed adaptation exploits low-level visual cues accessible during test-time to improve both the stability and quality of resulting videos. We highlight the efficacy of our methodology of \"test-time adaptation\" through simple fine-tuning of one of these models, followed by significant stability gain via the integration of meta-learning techniques. Notably, significant improvement is achieved with only a single adaptation step. The versatility of the proposed algorithm is demonstrated by consistently improving the performance of various pixel-level synthesis models for video stabilization in real-world scenarios.","sentences":["Video stabilization is a longstanding computer vision problem, particularly pixel-level synthesis solutions for video stabilization which synthesize full frames add to the complexity of this task.","These techniques aim to stabilize videos by synthesizing full frames while enhancing the stability of the considered video.","This intensifies the complexity of the task due to the distinct mix of unique motion profiles and visual content present in each video sequence, making robust generalization with fixed parameters difficult.","In our study, we introduce a novel approach to enhance the performance of pixel-level synthesis solutions for video stabilization by adapting these models to individual input video sequences.","The proposed adaptation exploits low-level visual cues accessible during test-time to improve both the stability and quality of resulting videos.","We highlight the efficacy of our methodology of \"test-time adaptation\" through simple fine-tuning of one of these models, followed by significant stability gain via the integration of meta-learning techniques.","Notably, significant improvement is achieved with only a single adaptation step.","The versatility of the proposed algorithm is demonstrated by consistently improving the performance of various pixel-level synthesis models for video stabilization in real-world scenarios."],"url":"http://arxiv.org/abs/2403.03662v1","category":"cs.CV"}
{"created":"2024-03-06 12:29:13","title":"Robust Graph Structure Learning under Heterophily","abstract":"Graph is a fundamental mathematical structure in characterizing relations between different objects and has been widely used on various learning tasks. Most methods implicitly assume a given graph to be accurate and complete. However, real data is inevitably noisy and sparse, which will lead to inferior results. Despite the remarkable success of recent graph representation learning methods, they inherently presume that the graph is homophilic, and largely overlook heterophily, where most connected nodes are from different classes. In this regard, we propose a novel robust graph structure learning method to achieve a high-quality graph from heterophilic data for downstream tasks. We first apply a high-pass filter to make each node more distinctive from its neighbors by encoding structure information into the node features. Then, we learn a robust graph with an adaptive norm characterizing different levels of noise. Afterwards, we propose a novel regularizer to further refine the graph structure. Clustering and semi-supervised classification experiments on heterophilic graphs verify the effectiveness of our method.","sentences":["Graph is a fundamental mathematical structure in characterizing relations between different objects and has been widely used on various learning tasks.","Most methods implicitly assume a given graph to be accurate and complete.","However, real data is inevitably noisy and sparse, which will lead to inferior results.","Despite the remarkable success of recent graph representation learning methods, they inherently presume that the graph is homophilic, and largely overlook heterophily, where most connected nodes are from different classes.","In this regard, we propose a novel robust graph structure learning method to achieve a high-quality graph from heterophilic data for downstream tasks.","We first apply a high-pass filter to make each node more distinctive from its neighbors by encoding structure information into the node features.","Then, we learn a robust graph with an adaptive norm characterizing different levels of noise.","Afterwards, we propose a novel regularizer to further refine the graph structure.","Clustering and semi-supervised classification experiments on heterophilic graphs verify the effectiveness of our method."],"url":"http://arxiv.org/abs/2403.03659v1","category":"cs.LG"}
{"created":"2024-03-06 12:27:16","title":"A practical and efficient approach for Bayesian reservoir inversion: Insights from the Alvheim field data","abstract":"Stochastic reservoir characterization, a critical aspect of subsurface exploration for oil and gas reservoirs, relies on stochastic methods to model and understand subsurface properties using seismic data. This paper addresses the computational challenges associated with Bayesian reservoir inversion methods, focusing on two key obstacles: the demanding forward model and the high dimensionality of Gaussian random fields. Leveraging the generalized Bayesian approach, we replace the intricate forward function with a computationally efficient multivariate adaptive regression splines method, resulting in a 34 acceleration in computational efficiency. For handling high-dimensional Gaussian random fields, we employ a fast Fourier transform (FFT) technique. Additionally, we explore the preconditioned Crank-Nicolson method for sampling, providing a more efficient exploration of high-dimensional parameter spaces. The practicality and efficacy of our approach are tested extensively in simulations and its validity is demonstrated in application to the Alvheim field data.","sentences":["Stochastic reservoir characterization, a critical aspect of subsurface exploration for oil and gas reservoirs, relies on stochastic methods to model and understand subsurface properties using seismic data.","This paper addresses the computational challenges associated with Bayesian reservoir inversion methods, focusing on two key obstacles: the demanding forward model and the high dimensionality of Gaussian random fields.","Leveraging the generalized Bayesian approach, we replace the intricate forward function with a computationally efficient multivariate adaptive regression splines method, resulting in a 34 acceleration in computational efficiency.","For handling high-dimensional Gaussian random fields, we employ a fast Fourier transform (FFT) technique.","Additionally, we explore the preconditioned Crank-Nicolson method for sampling, providing a more efficient exploration of high-dimensional parameter spaces.","The practicality and efficacy of our approach are tested extensively in simulations and its validity is demonstrated in application to the Alvheim field data."],"url":"http://arxiv.org/abs/2403.03656v1","category":"stat.AP"}
{"created":"2024-03-06 12:22:56","title":"3D Printed Waveguide for Augmented Reality","abstract":"Mass production of augmented reality (AR) waveguides has been challenging due to the intricate nature of the fabrication technique and the high precision required for its optical characteristics. In this paper, we have presented a novel and low-cost approach for fabricating geometric optical waveguides designed for AR applications utilizing 3D printing techniques. To strike a balance between optical performance and fabrication feasibility, we have optimized the conventional geometric waveguide design to facilitate easier fabrication. It is worth noting that our proposed method does not require molding, dicing, and post-surface polishing after printing. A prototype based on this method has been successfully fabricated, showing the immersion between the virtual image and the real-world scene. The proposed method has great potential for adaptation to mass production in various AR applications.","sentences":["Mass production of augmented reality (AR) waveguides has been challenging due to the intricate nature of the fabrication technique and the high precision required for its optical characteristics.","In this paper, we have presented a novel and low-cost approach for fabricating geometric optical waveguides designed for AR applications utilizing 3D printing techniques.","To strike a balance between optical performance and fabrication feasibility, we have optimized the conventional geometric waveguide design to facilitate easier fabrication.","It is worth noting that our proposed method does not require molding, dicing, and post-surface polishing after printing.","A prototype based on this method has been successfully fabricated, showing the immersion between the virtual image and the real-world scene.","The proposed method has great potential for adaptation to mass production in various AR applications."],"url":"http://arxiv.org/abs/2403.03652v1","category":"physics.optics"}
{"created":"2024-03-06 11:56:30","title":"Online Photon Guiding with 3D Gaussians for Caustics Rendering","abstract":"In production rendering systems, caustics are typically rendered via photon mapping and gathering, a process often hindered by insufficient photon density. In this paper, we propose a novel photon guiding method to improve the photon density and overall quality for caustic rendering. The key insight of our approach is the application of a global 3D Gaussian mixture model, used in conjunction with an adaptive light sampler. This combination effectively guides photon emission in expansive 3D scenes with multiple light sources. By employing a global 3D Gaussian mixture, our method precisely models the distribution of the points of interest. To sample emission directions from the distribution at any observation point, we introduce a novel directional transform of the 3D Gaussian, which ensures accurate photon emission guiding. Furthermore, our method integrates a global light cluster tree, which models the contribution distribution of light sources to the image, facilitating effective light source selection. We conduct experiments demonstrating that our approach robustly outperforms existing photon guiding techniques across a variety of scenarios, significantly advancing the quality of caustic rendering.","sentences":["In production rendering systems, caustics are typically rendered via photon mapping and gathering, a process often hindered by insufficient photon density.","In this paper, we propose a novel photon guiding method to improve the photon density and overall quality for caustic rendering.","The key insight of our approach is the application of a global 3D Gaussian mixture model, used in conjunction with an adaptive light sampler.","This combination effectively guides photon emission in expansive 3D scenes with multiple light sources.","By employing a global 3D Gaussian mixture, our method precisely models the distribution of the points of interest.","To sample emission directions from the distribution at any observation point, we introduce a novel directional transform of the 3D Gaussian, which ensures accurate photon emission guiding.","Furthermore, our method integrates a global light cluster tree, which models the contribution distribution of light sources to the image, facilitating effective light source selection.","We conduct experiments demonstrating that our approach robustly outperforms existing photon guiding techniques across a variety of scenarios, significantly advancing the quality of caustic rendering."],"url":"http://arxiv.org/abs/2403.03641v1","category":"cs.GR"}
{"created":"2024-03-06 10:24:44","title":"Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices","abstract":"This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs). We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment. After assigning a treatment, the experimenter observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using gathered samples. The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than optimizing only the propensity score. Based on this idea, in each round of our experiment, the experimenter optimizes the covariate density and propensity score based on past observations. To design an adaptive experiment, we first derive the efficient covariate density and propensity score that minimizes the semiparametric efficiency bound, a lower bound for the asymptotic variance given a fixed covariate density and a fixed propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound.","sentences":["This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs).","We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment.","After assigning a treatment, the experimenter observes the corresponding outcome immediately.","At the end of the experiment, the experimenter estimates an ATE using gathered samples.","The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance.","Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability).","As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than optimizing only the propensity score.","Based on this idea, in each round of our experiment, the experimenter optimizes the covariate density and propensity score based on past observations.","To design an adaptive experiment, we first derive the efficient covariate density and propensity score that minimizes the semiparametric efficiency bound, a lower bound for the asymptotic variance given a fixed covariate density and a fixed propensity score.","Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment.","Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound."],"url":"http://arxiv.org/abs/2403.03589v1","category":"stat.ME"}
{"created":"2024-03-06 10:00:10","title":"Speed limits to the growth of Krylov complexity in open quantum systems","abstract":"Recently, the propagation of information through quantum many-body systems, developed to study quantum chaos, have found many application from black holes to disordered spin systems. Among other quantitative tools, Krylov complexity has been explored as a diagnostic tool for information scrambling in quantum many-body systems. We introduce a universal limit to the growth of the Krylov complexity in dissipative open quantum systems by utilizing the uncertainty relation for non-hermitian operators. We also present the analytical results of Krylov complexity for characteristic behavior of Lanczos coefficients in dissipative systems. The validity of these results are demonstrated by explicit study of transverse-field Ising model under dissipative effects.","sentences":["Recently, the propagation of information through quantum many-body systems, developed to study quantum chaos, have found many application from black holes to disordered spin systems.","Among other quantitative tools, Krylov complexity has been explored as a diagnostic tool for information scrambling in quantum many-body systems.","We introduce a universal limit to the growth of the Krylov complexity in dissipative open quantum systems by utilizing the uncertainty relation for non-hermitian operators.","We also present the analytical results of Krylov complexity for characteristic behavior of Lanczos coefficients in dissipative systems.","The validity of these results are demonstrated by explicit study of transverse-field Ising model under dissipative effects."],"url":"http://arxiv.org/abs/2403.03584v1","category":"quant-ph"}
{"created":"2024-03-06 08:37:35","title":"Contraction rates and projection subspace estimation with Gaussian process priors in high dimension","abstract":"This work explores the dimension reduction problem for Bayesian nonparametric regression and density estimation. More precisely, we are interested in estimating a functional parameter $f$ over the unit ball in $\\mathbb{R}^d$, which depends only on a $d_0$-dimensional subspace of $\\mathbb{R}^d$, with $d_0 < d$.It is well-known that rescaled Gaussian process priors over the function space achieve smoothness adaptation and posterior contraction with near minimax-optimal rates. Moreover, hierarchical extensions of this approach, equipped with subspace projection, can also adapt to the intrinsic dimension $d_0$ (\\cite{Tokdar2011DimensionAdapt}).When the ambient dimension $d$ does not vary with $n$, the minimax rate remains of the order $n^{-\\beta/(2\\beta +d_0)}$.%When $d$ does not vary with $n$, the order of the minimax rate remains the same regardless of the ambient dimension $d$. However, this is up to multiplicative constants that can become prohibitively large when $d$ grows. The dependences between the contraction rate and the ambient dimension have not been fully explored yet and this work provides a first insight: we let the dimension $d$ grow with $n$ and, by combining the arguments of \\cite{Tokdar2011DimensionAdapt} and \\cite{Jiang2021VariableSelection}, we derive a growth rate for $d$ that still leads to posterior consistency with minimax rate.The optimality of this growth rate is then discussed.Additionally, we provide a set of assumptions under which consistent estimation of $f$ leads to a correct estimation of the subspace projection, assuming that $d_0$ is known.","sentences":["This work explores the dimension reduction problem for Bayesian nonparametric regression and density estimation.","More precisely, we are interested in estimating a functional parameter $f$ over the unit ball in $\\mathbb{R}^d$, which depends only on a $d_0$-dimensional subspace of $\\mathbb{R}^d$, with $d_0 <","d$.It is well-known that rescaled Gaussian process priors over the function space achieve smoothness adaptation and posterior contraction with near minimax-optimal rates.","Moreover, hierarchical extensions of this approach, equipped with subspace projection, can also adapt to the intrinsic dimension $d_0$ (\\cite{Tokdar2011DimensionAdapt}).When the ambient dimension $d$ does not vary with $n$, the minimax rate remains of the order $n^{-\\beta/(2\\beta","+d_0)}$.%When $d$ does not vary with $n$, the order of the minimax rate remains the same regardless of the ambient dimension $d$.","However, this is up to multiplicative constants that can become prohibitively large when $d$ grows.","The dependences between the contraction rate and the ambient dimension have not been fully explored yet and this work provides a first insight: we let the dimension $d$ grow with $n$ and, by combining the arguments of \\cite{Tokdar2011DimensionAdapt} and \\cite{Jiang2021VariableSelection}, we derive a growth rate for $d$ that still leads to posterior consistency with minimax rate.","The optimality of this growth rate is then discussed.","Additionally, we provide a set of assumptions under which consistent estimation of $f$ leads to a correct estimation of the subspace projection, assuming that $d_0$ is known."],"url":"http://arxiv.org/abs/2403.03540v1","category":"math.ST"}
{"created":"2024-03-06 08:35:29","title":"Gadolinium dose reduction for brain MRI using conditional deep learning","abstract":"Recently, deep learning (DL)-based methods have been proposed for the computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate adverse side effects while preserving diagnostic value. Currently, the two main challenges for these approaches are the accurate prediction of contrast enhancement and the synthesis of realistic images. In this work, we address both challenges by utilizing the contrast signal encoded in the subtraction images of pre-contrast and post-contrast image pairs. To avoid the synthesis of any noise or artifacts and solely focus on contrast signal extraction and enhancement from low-dose subtraction images, we train our DL model using noise-free standard-dose subtraction images as targets. As a result, our model predicts the contrast enhancement signal only; thereby enabling synthesization of images beyond the standard dose. Furthermore, we adapt the embedding idea of recent diffusion-based models to condition our model on physical parameters affecting the contrast enhancement behavior. We demonstrate the effectiveness of our approach on synthetic and real datasets using various scanners, field strengths, and contrast agents.","sentences":["Recently, deep learning (DL)-based methods have been proposed for the computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate adverse side effects while preserving diagnostic value.","Currently, the two main challenges for these approaches are the accurate prediction of contrast enhancement and the synthesis of realistic images.","In this work, we address both challenges by utilizing the contrast signal encoded in the subtraction images of pre-contrast and post-contrast image pairs.","To avoid the synthesis of any noise or artifacts and solely focus on contrast signal extraction and enhancement from low-dose subtraction images, we train our DL model using noise-free standard-dose subtraction images as targets.","As a result, our model predicts the contrast enhancement signal only; thereby enabling synthesization of images beyond the standard dose.","Furthermore, we adapt the embedding idea of recent diffusion-based models to condition our model on physical parameters affecting the contrast enhancement behavior.","We demonstrate the effectiveness of our approach on synthetic and real datasets using various scanners, field strengths, and contrast agents."],"url":"http://arxiv.org/abs/2403.03539v1","category":"eess.IV"}
{"created":"2024-03-06 08:29:45","title":"Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications","abstract":"Few-shot learning (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \\emph{related} training tasks. In this paper, we try to understand FSL by delving into two key questions: (1) How to quantify the relationship between \\emph{training} and \\emph{novel} tasks? (2) How does the relationship affect the \\emph{adaptation difficulty} on novel tasks for different models? To answer the two questions, we introduce Task Attribute Distance (TAD) built upon attributes as a metric to quantify the task relatedness. Unlike many existing metrics, TAD is model-agnostic, making it applicable to different FSL models. Then, we utilize TAD metric to establish a theoretical connection between task relatedness and task adaptation difficulty. By deriving the generalization error bound on a novel task, we discover how TAD measures the adaptation difficulty on novel tasks for FSL models. To validate our TAD metric and theoretical findings, we conduct experiments on three benchmarks. Our experimental results confirm that TAD metric effectively quantifies the task relatedness and reflects the adaptation difficulty on novel tasks for various FSL methods, even if some of them do not learn attributes explicitly or human-annotated attributes are not available. Finally, we present two applications of the proposed TAD metric: data augmentation and test-time intervention, which further verify its effectiveness and general applicability. The source code is available at https://github.com/hu-my/TaskAttributeDistance.","sentences":["Few-shot learning (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \\emph{related} training tasks.","In this paper, we try to understand FSL by delving into two key questions: (1) How to quantify the relationship between \\emph{training} and \\emph{novel} tasks?","(2) How does the relationship affect the \\emph{adaptation difficulty} on novel tasks for different models?","To answer the two questions, we introduce Task Attribute Distance (TAD) built upon attributes as a metric to quantify the task relatedness.","Unlike many existing metrics, TAD is model-agnostic, making it applicable to different FSL models.","Then, we utilize TAD metric to establish a theoretical connection between task relatedness and task adaptation difficulty.","By deriving the generalization error bound on a novel task, we discover how TAD measures the adaptation difficulty on novel tasks for FSL models.","To validate our TAD metric and theoretical findings, we conduct experiments on three benchmarks.","Our experimental results confirm that TAD metric effectively quantifies the task relatedness and reflects the adaptation difficulty on novel tasks for various FSL methods, even if some of them do not learn attributes explicitly or human-annotated attributes are not available.","Finally, we present two applications of the proposed TAD metric: data augmentation and test-time intervention, which further verify its effectiveness and general applicability.","The source code is available at https://github.com/hu-my/TaskAttributeDistance."],"url":"http://arxiv.org/abs/2403.03535v1","category":"cs.CV"}
{"created":"2024-03-07 16:03:12","title":"$B$-meson production at forward rapidities in $pp$ collisions at the LHC: Estimating the intrinsic bottom contribution","abstract":"The production of $B$ mesons at forward rapidities is strongly sensitive to the behavior of the gluon and bottom distribution functions for small and large values of the Bjorken-$x$ variable. In this exploratory study, we estimate the cross-section for the $B^{\\pm}$ meson production in the kinematic range probed by the LHCb detector and that will be analyzed by the future Forward Physics Facility (FPF) considering the hybrid formalism, the solution of the running coupling Balitsky-Kovchegov equation and distinct description of the bottom distribution function. We assume an ansatz for the intrinsic bottom component in the proton wave function, and estimate its impact on the transverse momentum, rapidity and Feynman-$x$ distributions. Our results indicate that the presence of an intrinsic bottom strongly modifies the magnitude of the cross-section at ultra-forward rapidities ($y \\ge 6$), which implies an enhancement of the $B^{\\pm}$ production at the FPF. Possible implications on the prompt neutrino flux at ultra-high energies are also briefly discussed.","sentences":["The production of $B$ mesons at forward rapidities is strongly sensitive to the behavior of the gluon and bottom distribution functions for small and large values of the Bjorken-$x$ variable.","In this exploratory study, we estimate the cross-section for the $B^{\\pm}$ meson production in the kinematic range probed by the LHCb detector and that will be analyzed by the future Forward Physics Facility (FPF) considering the hybrid formalism, the solution of the running coupling Balitsky-Kovchegov equation and distinct description of the bottom distribution function.","We assume an ansatz for the intrinsic bottom component in the proton wave function, and estimate its impact on the transverse momentum, rapidity and Feynman-$x$ distributions.","Our results indicate that the presence of an intrinsic bottom strongly modifies the magnitude of the cross-section at ultra-forward rapidities ($y \\ge 6$), which implies an enhancement of the $B^{\\pm}$ production at the FPF.","Possible implications on the prompt neutrino flux at ultra-high energies are also briefly discussed."],"url":"http://arxiv.org/abs/2403.04619v1","category":"hep-ph"}
{"created":"2024-03-06 18:59:51","title":"Decoupling the electronic gap from the spin Chern number in disordered higher-order topological insulators","abstract":"In two-dimensional topological insulators, a disorder induced topological phase transition is typically identified with an Anderson localization transition at the Fermi energy. However, in higher-order, spin-resolved topological insulators it is the spectral gap of the spin-spectrum, in addition to the bulk mobility gap, which protects the non-trivial topology of the ground state. In this work, we show that these two gaps, the bulk electronic and spin gap, evolve distinctly upon introduction of disorder. This decoupling leads to a unique situation in which an Anderson localization transition occurs below the Fermi energy at the topological transition. Furthermore, in the clean limit the bulk-boundary correspondence of such higher-order insulators is dictated by crystalline protected topology, coexisting with the spin-resolved topology. By removing the crystalline symmetry, disorder allows for isolated study of the bulk-boundary correspondence of spin-resolved topology for which we demonstrate the absence of protected edge and corner modes in the Hamiltonian and yet the edge modes in the eigenstates of the projected spin operator survive. Our work shows that a non-zero spin-Chern number, in the absence of a non-trivial $\\mathbb{Z}_{2}$ index, does not dictate the existence of protected edge modes, resolving a fundamental question posed in 2009.","sentences":["In two-dimensional topological insulators, a disorder induced topological phase transition is typically identified with an Anderson localization transition at the Fermi energy.","However, in higher-order, spin-resolved topological insulators it is the spectral gap of the spin-spectrum, in addition to the bulk mobility gap, which protects the non-trivial topology of the ground state.","In this work, we show that these two gaps, the bulk electronic and spin gap, evolve distinctly upon introduction of disorder.","This decoupling leads to a unique situation in which an Anderson localization transition occurs below the Fermi energy at the topological transition.","Furthermore, in the clean limit the bulk-boundary correspondence of such higher-order insulators is dictated by crystalline protected topology, coexisting with the spin-resolved topology.","By removing the crystalline symmetry, disorder allows for isolated study of the bulk-boundary correspondence of spin-resolved topology for which we demonstrate the absence of protected edge and corner modes in the Hamiltonian and yet the edge modes in the eigenstates of the projected spin operator survive.","Our work shows that a non-zero spin-Chern number, in the absence of a non-trivial $\\mathbb{Z}_{2}$ index, does not dictate the existence of protected edge modes, resolving a fundamental question posed in 2009."],"url":"http://arxiv.org/abs/2403.03957v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-06 18:49:45","title":"Scalar curvature deformations with non-compact boundaries","abstract":"We develop a general deformation principle for families of Riemannian metrics on smooth manifolds with possibly non-compact boundary, preserving lower scalar curvature bounds. The principle is used in order to strengthen boundary conditions, from mean convex to totally geodesic or doubling. The deformation principle preserves further geometric properties such as completeness and a given quasi-isometry type. As an application, we prove non-existence results for Riemannian metrics with (uniformly) positive scalar curvature and mean convex boundary.","sentences":["We develop a general deformation principle for families of Riemannian metrics on smooth manifolds with possibly non-compact boundary, preserving lower scalar curvature bounds.","The principle is used in order to strengthen boundary conditions, from mean convex to totally geodesic or doubling.","The deformation principle preserves further geometric properties such as completeness and a given quasi-isometry type.","As an application, we prove non-existence results for Riemannian metrics with (uniformly) positive scalar curvature and mean convex boundary."],"url":"http://arxiv.org/abs/2403.03941v1","category":"math.DG"}
{"created":"2024-03-06 18:45:20","title":"Non-resonant conditions for the Klein-Gordon equation on the circle","abstract":"We consider the infinite dimensional vector of frequencies $\\omega(m)=( \\sqrt{j^2+m})_{j\\in \\mathbb{Z}}$, $m\\in [1,2]$ arising form a linear Klein-Gordon equation on the one dimensional torus and prove that there exists a positive measure set of masses $m'$s for which $\\omega(m)$ satisfies a diophantine condition similar to the one introduced by Bourgain in (JFA, 2005), in the context of Schr\\\"odinger equation with convolution potential. The main difficulties we have to deal with are the asymptotically linear nature of the (infinitely many) $\\omega_{j}'$s and the degeneracy coming from having only one parameter at disposal for their modulation. As an application we provide estimates on the inverse of the adjoint action of the associated quadratic Hamiltonian on homogenenous polynomials of any degree in Gevrey category.","sentences":["We consider the infinite dimensional vector of frequencies $\\omega(m)=( \\sqrt{j^2+m})_{j\\in \\mathbb{Z}}$, $m\\in [1,2]$ arising form a linear Klein-Gordon equation on the one dimensional torus and prove that there exists a positive measure set of masses $m'$s for which $\\omega(m)$ satisfies a diophantine condition similar to the one introduced by Bourgain in (JFA, 2005), in the context of Schr\\\"odinger equation with convolution potential.","The main difficulties we have to deal with are the asymptotically linear nature of the (infinitely many) $\\omega_{j}'$s and the degeneracy coming from having only one parameter at disposal for their modulation.","As an application we provide estimates on the inverse of the adjoint action of the associated quadratic Hamiltonian on homogenenous polynomials of any degree in Gevrey category."],"url":"http://arxiv.org/abs/2403.03936v1","category":"math.AP"}
{"created":"2024-03-06 18:41:01","title":"Physical viability of traversable Finslerian wormholes with traceless fluid under conformal symmetry","abstract":"The current study explores the novel potential of traversable wormhole solutions within the framework of Finsler geometry, incorporating conformal symmetry alongside traceless fluid dynamics. Using the Conformal Killing vector approach, we have discussed the wormholes based on traceless fluid within the intriguing framework of Finsler geometry. The field equations and the associated conformal factor are obtained specifically under the condition of conformal motion in Finsler geometry. Furthermore, we have successfully derived and examined the shape function, considering a range of values for the Finslerian parameter $\\lambda$. Our investigation extends to fundamental physical characteristics such as proper radial distance, active mass function, and total gravitational energy, aiming to understand their influence on the traversability of the wormhole. The observation of energy condition violations provides evidence for the exotic matter's presence near the throat, reinforcing the assertion of the Finslerian wormhole's traversability.","sentences":["The current study explores the novel potential of traversable wormhole solutions within the framework of Finsler geometry, incorporating conformal symmetry alongside traceless fluid dynamics.","Using the Conformal Killing vector approach, we have discussed the wormholes based on traceless fluid within the intriguing framework of Finsler geometry.","The field equations and the associated conformal factor are obtained specifically under the condition of conformal motion in Finsler geometry.","Furthermore, we have successfully derived and examined the shape function, considering a range of values for the Finslerian parameter $\\lambda$.","Our investigation extends to fundamental physical characteristics such as proper radial distance, active mass function, and total gravitational energy, aiming to understand their influence on the traversability of the wormhole.","The observation of energy condition violations provides evidence for the exotic matter's presence near the throat, reinforcing the assertion of the Finslerian wormhole's traversability."],"url":"http://arxiv.org/abs/2403.03931v1","category":"gr-qc"}
{"created":"2024-03-06 18:34:24","title":"Relaxation of maximally entangled quantum states of two nonequivalent nuclear spins in a liquid","abstract":"We investigate both experimentally and theoretically the relaxation of pseudo-pure maximally entangled states (Bell states) of two nuclear spins 1H-13C belonging to a molecule in a liquid. The Bell states are obtained by a method based on a detuned Hartmann-Hahn cross-polarization condition. Their entangled character is verified by quantum-state tomography. Our relaxation measurements reveal different relaxation rates for different Bell states. We interpret this difference as originating from cross-correlations between different relaxation mechanisms, thereby demonstrating that the measurements of the differential relaxation of Bell states are potentially useful for advanced NMR characterization of liquids.","sentences":["We investigate both experimentally and theoretically the relaxation of pseudo-pure maximally entangled states (Bell states) of two nuclear spins 1H-13C belonging to a molecule in a liquid.","The Bell states are obtained by a method based on a detuned Hartmann-Hahn cross-polarization condition.","Their entangled character is verified by quantum-state tomography.","Our relaxation measurements reveal different relaxation rates for different Bell states.","We interpret this difference as originating from cross-correlations between different relaxation mechanisms, thereby demonstrating that the measurements of the differential relaxation of Bell states are potentially useful for advanced NMR characterization of liquids."],"url":"http://arxiv.org/abs/2403.03924v1","category":"quant-ph"}
{"created":"2024-03-06 18:33:51","title":"Did Translation Models Get More Robust Without Anyone Even Noticing?","abstract":"Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting issues. In this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation. Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data. This is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness. Next, we show that similar trends hold for social media translation experiments -- LLMs are more robust to social media text. We include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise. Altogether, we show that robustness to many types of noise has increased.","sentences":["Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting issues.","In this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation.","Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data.","This is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness.","Next, we show that similar trends hold for social media translation experiments -- LLMs are more robust to social media text.","We include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise.","Altogether, we show that robustness to many types of noise has increased."],"url":"http://arxiv.org/abs/2403.03923v1","category":"cs.CL"}
{"created":"2024-03-06 18:27:18","title":"Risk-Sensitive Mean Field Games with Common Noise: A Theoretical Study with Applications to Interbank Markets","abstract":"In this paper, we address linear-quadratic-Gaussian (LQG) risk-sensitive mean field games (MFGs) with common noise. In this framework agents are exposed to a common noise and aim to minimize an exponential cost functional that reflects their risk sensitivity. We leverage the convex analysis method to derive the optimal strategies of agents in the limit as the number of agents goes to infinity. These strategies yield a Nash equilibrium for the limiting model. The model is then applied to interbank markets, focusing on optimizing lending and borrowing activities to assess systemic and individual bank risks when reserves drop below a critical threshold. We employ Fokker-Planck equations and the first hitting time method to formulate the overall probability of a bank or market default. We observe that the risk-averse behavior of agents reduces the probability of individual defaults and systemic risk, enhancing the resilience of the financial system. Adopting a similar approach based on stochastic Fokker-Planck equations, we further expand our analysis to investigate the conditional probabilities of individual default under specific trajectories of the common market shock.","sentences":["In this paper, we address linear-quadratic-Gaussian (LQG) risk-sensitive mean field games (MFGs) with common noise.","In this framework agents are exposed to a common noise and aim to minimize an exponential cost functional that reflects their risk sensitivity.","We leverage the convex analysis method to derive the optimal strategies of agents in the limit as the number of agents goes to infinity.","These strategies yield a Nash equilibrium for the limiting model.","The model is then applied to interbank markets, focusing on optimizing lending and borrowing activities to assess systemic and individual bank risks when reserves drop below a critical threshold.","We employ Fokker-Planck equations and the first hitting time method to formulate the overall probability of a bank or market default.","We observe that the risk-averse behavior of agents reduces the probability of individual defaults and systemic risk, enhancing the resilience of the financial system.","Adopting a similar approach based on stochastic Fokker-Planck equations, we further expand our analysis to investigate the conditional probabilities of individual default under specific trajectories of the common market shock."],"url":"http://arxiv.org/abs/2403.03915v1","category":"math.OC"}
{"created":"2024-03-06 17:49:13","title":"Non-reflective traveling waves in finite thin beams: A parametric study","abstract":"In the present study, the authors introduce a geometrically improved model inspired by the mammalian basilar membrane's properties and special vibratory behavior while conducting a parametric investigation. The goal of this model is to mimic the broadband non-reflective traveling wave response to excitation frequencies, as observed in the basilar membrane. Simple structural elements, such as beams, springs, and dashpots are utilized for this purpose. Therefore, the beam's equation of motion is developed using Hamilton's principle, and the Galerkin method is implemented as the discretization scheme. Then, the model is verified by comparing its outcome to results presented in the literature. Finally, a parametric study is conducted to reveal the effect of different parameters, i.e., the absorber's location and coefficients' values, excitation frequency, geometric tapering, and material grading on the non-reflective traveling wave response of the beam. The results reveal that traveling waves and their quality are strongly dependent on these parameters. In addition, this study suggests that adopting a single spring-damper system within the span of the beam as the wave reflection absorber may not address the entire bandwidth.","sentences":["In the present study, the authors introduce a geometrically improved model inspired by the mammalian basilar membrane's properties and special vibratory behavior while conducting a parametric investigation.","The goal of this model is to mimic the broadband non-reflective traveling wave response to excitation frequencies, as observed in the basilar membrane.","Simple structural elements, such as beams, springs, and dashpots are utilized for this purpose.","Therefore, the beam's equation of motion is developed using Hamilton's principle, and the Galerkin method is implemented as the discretization scheme.","Then, the model is verified by comparing its outcome to results presented in the literature.","Finally, a parametric study is conducted to reveal the effect of different parameters, i.e., the absorber's location and coefficients' values, excitation frequency, geometric tapering, and material grading on the non-reflective traveling wave response of the beam.","The results reveal that traveling waves and their quality are strongly dependent on these parameters.","In addition, this study suggests that adopting a single spring-damper system within the span of the beam as the wave reflection absorber may not address the entire bandwidth."],"url":"http://arxiv.org/abs/2403.03889v1","category":"math.DS"}
{"created":"2024-03-06 17:43:05","title":"Towards a Schauder theory for fractional viscous Hamilton--Jacobi equations","abstract":"We survey some results on Lipschitz and Schauder regularity estimates for viscous Hamilton--Jacobi equations with subcritical L\\'evy diffusions. The Schauder estimates, along with existence of smooth solutions, are obtained with the help of a Duhamel formula and $L^1$ bounds on the spatial derivatives of the heat kernel. Our results cover very general nonlocal and mixed local-nonlocal diffusions, including strongly anisotropic, nonsymmetric, mixed order, and spectrally one-sided models.","sentences":["We survey some results on Lipschitz and Schauder regularity estimates for viscous Hamilton--Jacobi equations with subcritical L\\'evy diffusions.","The Schauder estimates, along with existence of smooth solutions, are obtained with the help of a Duhamel formula and $L^1$ bounds on the spatial derivatives of the heat kernel.","Our results cover very general nonlocal and mixed local-nonlocal diffusions, including strongly anisotropic, nonsymmetric, mixed order, and spectrally one-sided models."],"url":"http://arxiv.org/abs/2403.03884v1","category":"math.AP"}
{"created":"2024-03-06 17:40:26","title":"Graph neural network outputs are almost surely asymptotically constant","abstract":"Graph neural networks (GNNs) are the predominant architectures for a variety of learning tasks on graphs. We present a new angle on the expressive power of GNNs by studying how the predictions of a GNN probabilistic classifier evolve as we apply it on larger graphs drawn from some random graph model. We show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly. This convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers. Our results apply to a broad class of random graph models, including the (sparse) Erd\\H{o}s-R\\'enyi model and the stochastic block model. We empirically validate these findings, observing that the convergence phenomenon already manifests itself on graphs of relatively modest size.","sentences":["Graph neural networks (GNNs) are the predominant architectures for a variety of learning tasks on graphs.","We present a new angle on the expressive power of GNNs by studying how the predictions of a GNN probabilistic classifier evolve as we apply it on larger graphs drawn from some random graph model.","We show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly.","This convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers.","Our results apply to a broad class of random graph models, including the (sparse) Erd\\H{o}s-R\\'enyi model and the stochastic block model.","We empirically validate these findings, observing that the convergence phenomenon already manifests itself on graphs of relatively modest size."],"url":"http://arxiv.org/abs/2403.03880v1","category":"cs.LG"}
{"created":"2024-03-06 17:29:34","title":"The algebra $\\mathcal{D}(W)$ via strong Darboux transformations","abstract":"The Matrix Bochner Problem aims to classify weight matrices $W$ such that its algebra $\\mathcal D(W)$, of all differential operators that have a sequence of these matrix orthogonal polynomials as eigenfunctions, contains a second-order differential operator. In [5] it is proven that, under certain assumptions, the solutions to the Matrix Bochner Problem can be obtained through a noncommutative bispectral Darboux transformation of some classical scalar weights.   The main aim of this paper is to introduce the concept of strong Darboux transformation among weight matrices and explore the relationship between the algebras $\\mathcal{D}(W)$ and $\\mathcal{D}(\\widetilde{W})$ when $\\widetilde{W}$ is a strong Darboux transformation of $W$. Starting from a direct sum of classical scalar weights $\\widetilde W$, and leveraging our complete knowledge of the algebra of $\\mathcal D(\\widetilde W)$, we can easily determine the algebra $\\mathcal D(W)$ of a weight $W$ that is a strong Darboux transformation of $\\widetilde W$.","sentences":["The Matrix Bochner Problem aims to classify weight matrices $W$ such that its algebra $\\mathcal D(W)$, of all differential operators that have a sequence of these matrix orthogonal polynomials as eigenfunctions, contains a second-order differential operator.","In [5] it is proven that, under certain assumptions, the solutions to the Matrix Bochner Problem can be obtained through a noncommutative bispectral Darboux transformation of some classical scalar weights.   ","The main aim of this paper is to introduce the concept of strong Darboux transformation among weight matrices and explore the relationship between the algebras $\\mathcal{D}(W)$ and $\\mathcal{D}(\\widetilde{W})$ when $\\widetilde{W}$ is a strong Darboux transformation of $W$. Starting from a direct sum of classical scalar weights $\\widetilde W$, and leveraging our complete knowledge of the algebra of $\\mathcal D(\\widetilde W)$, we can easily determine the algebra $\\mathcal D(W)$ of a weight $W$ that is a strong Darboux transformation of $\\widetilde W$."],"url":"http://arxiv.org/abs/2403.03873v1","category":"math.CA"}
{"created":"2024-03-06 17:23:28","title":"Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data","abstract":"Vertical Federated Learning (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data. In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients. Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features. Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments. We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL. By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision. With these properties, DVFL is fault tolerant and secure. We implement DVFL to train split neural networks and show that model performance is comparable to VFL on a variety of classification datasets.","sentences":["Vertical Federated Learning (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data.","In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients.","Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features.","Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments.","We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL.","By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision.","With these properties, DVFL is fault tolerant and secure.","We implement DVFL to train split neural networks and show that model performance is comparable to VFL on a variety of classification datasets."],"url":"http://arxiv.org/abs/2403.03871v1","category":"cs.LG"}
{"created":"2024-03-06 17:10:15","title":"ProxNF: Neural Field Proximal Training for High-Resolution 4D Dynamic Image Reconstruction","abstract":"Accurate spatiotemporal image reconstruction methods are needed for a wide range of biomedical research areas but face challenges due to data incompleteness and computational burden. Data incompleteness arises from the undersampling often required to increase frame rates and reduce acquisition times, while computational burden emerges due to the memory footprint of high-resolution images with three spatial dimensions and extended time horizons. Neural fields, an emerging class of neural networks that act as continuous representations of spatiotemporal objects, have previously been introduced to solve these dynamic imaging problems by reframing image reconstruction to a problem of estimating network parameters. Neural fields can address the twin challenges of data incompleteness and computational burden by exploiting underlying redundancies in these spatiotemporal objects. This work proposes ProxNF, a novel neural field training approach for spatiotemporal image reconstruction leveraging proximal splitting methods to separate computations involving the imaging operator from updates of the network parameter. Specifically, ProxNF evaluates the (subsampled) gradient of the data-fidelity term in the image domain and uses a fully supervised learning approach to update the neural field parameters. By reducing the memory footprint and the computational cost of evaluating the imaging operator, the proposed ProxNF approach allows for reconstructing large, high-resolution spatiotemporal images. This method is demonstrated in two numerical studies involving virtual dynamic contrast-enhanced photoacoustic computed tomography imaging of an anatomically realistic dynamic numerical mouse phantom and a two-compartment model of tumor perfusion.","sentences":["Accurate spatiotemporal image reconstruction methods are needed for a wide range of biomedical research areas but face challenges due to data incompleteness and computational burden.","Data incompleteness arises from the undersampling often required to increase frame rates and reduce acquisition times, while computational burden emerges due to the memory footprint of high-resolution images with three spatial dimensions and extended time horizons.","Neural fields, an emerging class of neural networks that act as continuous representations of spatiotemporal objects, have previously been introduced to solve these dynamic imaging problems by reframing image reconstruction to a problem of estimating network parameters.","Neural fields can address the twin challenges of data incompleteness and computational burden by exploiting underlying redundancies in these spatiotemporal objects.","This work proposes ProxNF, a novel neural field training approach for spatiotemporal image reconstruction leveraging proximal splitting methods to separate computations involving the imaging operator from updates of the network parameter.","Specifically, ProxNF evaluates the (subsampled) gradient of the data-fidelity term in the image domain and uses a fully supervised learning approach to update the neural field parameters.","By reducing the memory footprint and the computational cost of evaluating the imaging operator, the proposed ProxNF approach allows for reconstructing large, high-resolution spatiotemporal images.","This method is demonstrated in two numerical studies involving virtual dynamic contrast-enhanced photoacoustic computed tomography imaging of an anatomically realistic dynamic numerical mouse phantom and a two-compartment model of tumor perfusion."],"url":"http://arxiv.org/abs/2403.03860v1","category":"eess.IV"}
{"created":"2024-03-06 17:06:11","title":"Public-data Assisted Private Stochastic Optimization: Power and Limitations","abstract":"We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\\epsilon,\\delta)$-PA-DP has excess risk $\\tilde{\\Omega}\\big(\\min\\big\\{\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} \\big\\} \\big)$, where $d$ is the dimension, ${n_{\\text{pub}}}$ is the number of public samples, ${n_{\\text{priv}}}$ is the number of private samples, and $n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \\textit{unlabeled} public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\\tilde{O}({n_{\\text{priv}}}\\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\\tilde{O}\\big(\\frac{1}{\\sqrt{{n_{\\text{priv}}}}} + \\frac{1}{\\sqrt{{n_{\\text{priv}}}\\epsilon}}\\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite fat-shattering dimension with applications to neural networks and non-Euclidean geometries.","sentences":["We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms.","Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data.","For complete/labeled public data, we show that any $(\\epsilon,\\delta)$-PA-DP has excess risk $\\tilde{\\Omega}\\big(\\min\\big\\{\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} \\big\\} \\big)$, where $d$ is the dimension, ${n_{\\text{pub}}}$ is the number of public samples, ${n_{\\text{priv}}}$ is the number of private samples, and $n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form.","Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal.","We also study PA-DP supervised learning with \\textit{unlabeled} public samples.","In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning.","For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\\tilde{O}({n_{\\text{priv}}}\\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\\tilde{O}\\big(\\frac{1}{\\sqrt{{n_{\\text{priv}}}}} + \\frac{1}{\\sqrt{{n_{\\text{priv}}}\\epsilon}}\\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate.","Finally, we provide extensions of this result to general hypothesis classes with finite fat-shattering dimension with applications to neural networks and non-Euclidean geometries."],"url":"http://arxiv.org/abs/2403.03856v1","category":"cs.LG"}
{"created":"2024-03-06 16:41:26","title":"Electromagnetic inverse wave scattering in anisotropic media via reduced order modeling","abstract":"The inverse wave scattering problem seeks to estimate a heterogeneous, inaccessible medium, modeled by unknown variable coefficients in wave equations, from transient recordings of waves generated by probing signals. It is a widely studied inverse problem with important applications, that is typically formulated as a nonlinear least squares data fit optimization. For typical measurement setups and band-limited probing signals, the least squares objective function has spurious local minima far and near the true solution, so Newton-type optimization methods fail. We introduce a different approach, for electromagnetic inverse wave scattering in lossless, anisotropic media. Our reduced order model (ROM) is an algebraic, discrete time dynamical system derived from Maxwell's equations with four important properties: (1) It is data driven, without knowledge of the medium. (2) The data to ROM mapping is nonlinear and yet the ROM can be obtained in a non-iterative fashion. (3) It has a special algebraic structure that captures the causal Wave propagation. (4) The ROM interpolates the data on a uniform time grid. We show how to obtain from the ROM an estimate of the wave field at inaccessible points inside the unknown medium. The use of this wave is twofold: First, it defines a computationally inexpensive imaging function designed to estimate the support of reflective structures in the medium, modeled by jump discontinuities of the matrix valued dielectric permittivity. Second, it gives an objective function for quantitative estimation of the dielectric permittivity, that has better behavior than the least squares data fitting objective function. The methodology introduced in this paper applies to Maxwell's equations in three dimensions. To avoid high computational costs, we limit the study to a cylindrical domain filled with an orthotropic medium, so the problem becomes two dimensional.","sentences":["The inverse wave scattering problem seeks to estimate a heterogeneous, inaccessible medium, modeled by unknown variable coefficients in wave equations, from transient recordings of waves generated by probing signals.","It is a widely studied inverse problem with important applications, that is typically formulated as a nonlinear least squares data fit optimization.","For typical measurement setups and band-limited probing signals, the least squares objective function has spurious local minima far and near the true solution, so Newton-type optimization methods fail.","We introduce a different approach, for electromagnetic inverse wave scattering in lossless, anisotropic media.","Our reduced order model (ROM) is an algebraic, discrete time dynamical system derived from Maxwell's equations with four important properties: (1) It is data driven, without knowledge of the medium.","(2) The data to ROM mapping is nonlinear and yet the ROM can be obtained in a non-iterative fashion.","(3) It has a special algebraic structure that captures the causal Wave propagation.","(4) The ROM interpolates the data on a uniform time grid.","We show how to obtain from the ROM an estimate of the wave field at inaccessible points inside the unknown medium.","The use of this wave is twofold:","First, it defines a computationally inexpensive imaging function designed to estimate the support of reflective structures in the medium, modeled by jump discontinuities of the matrix valued dielectric permittivity.","Second, it gives an objective function for quantitative estimation of the dielectric permittivity, that has better behavior than the least squares data fitting objective function.","The methodology introduced in this paper applies to Maxwell's equations in three dimensions.","To avoid high computational costs, we limit the study to a cylindrical domain filled with an orthotropic medium, so the problem becomes two dimensional."],"url":"http://arxiv.org/abs/2403.03844v1","category":"math.NA"}
{"created":"2024-03-06 16:33:37","title":"Caloric functions and boundary regularity for the fractional Laplacian in Lipschitz open sets","abstract":"We give Martin representation of nonnegative functions caloric with respect to the fractional Laplacian in Lipschitz open sets. The caloric functions are defined in terms of the mean value property for the space-time isotropic $\\alpha$-stable L\\'evy process. To derive the representation, we first establish the existence of the parabolic Martin kernel. This involves proving new boundary regularity results for both the fractional heat equation and the fractional Poisson equation with Dirichlet exterior conditions. Specifically, we demonstrate that the ratio of the solution and the Green function is H\\\"older continuous up to the boundary.","sentences":["We give Martin representation of nonnegative functions caloric with respect to the fractional Laplacian in Lipschitz open sets.","The caloric functions are defined in terms of the mean value property for the space-time isotropic $\\alpha$-stable L\\'evy process.","To derive the representation, we first establish the existence of the parabolic Martin kernel.","This involves proving new boundary regularity results for both the fractional heat equation and the fractional Poisson equation with Dirichlet exterior conditions.","Specifically, we demonstrate that the ratio of the solution and the Green function is H\\\"older continuous up to the boundary."],"url":"http://arxiv.org/abs/2403.03840v1","category":"math.AP"}
{"created":"2024-03-06 16:23:27","title":"Second order Sobolev regularity for normalized parabolic $p(x)$-Laplace equations via the algebraic structure","abstract":"Denote by $\\Delta$ the Laplacian and by $\\Delta_\\infty$ the $\\infty$-Laplacian. A fundamental inequality is proved for the algebraic structure of $\\Delta v\\Delta_\\infty v$: for every $v\\in C^{\\infty}$, $$\\bigg| |D^2vDv|^2-\\Delta v\\Delta_\\infty v-\\frac{1}{2}[|D^2v|^2-(\\Delta v)^2]|Dv|^2\\bigg| \\le\\frac{n-2}{2}[|D^2v|^2|Dv|^2-|D^2vDv|^2]$$ Based on this, we prove the result: When $n\\ge2$ and $p(x)\\in(1,2)\\cup(2,3+\\frac{2}{n-2})$, the viscosity solutions to parabolic normalized $p(x)$-Laplace equation have the $W^{2,2}_{loc}$-regularity in the spatial variable and the $W^{1,2}_{loc}$-regularity in the time variable.","sentences":["Denote by $\\Delta$ the Laplacian and by $\\Delta_\\infty$ the $\\infty$-Laplacian.","A fundamental inequality is proved for the algebraic structure of $\\Delta v\\Delta_\\infty v$: for every $v\\in C^{\\infty}$, $$\\bigg| |D^2vDv|^2-\\Delta v\\Delta_\\infty v-\\frac{1}{2}[|D^2v|^2-(\\Delta v)^2]|Dv|^2\\bigg| \\le\\frac{n-2}{2}[|D^2v|^2|Dv|^2-|D^2vDv|^2]$$ Based on this, we prove the result: When $n\\ge2$ and $p(x)\\in(1,2)\\cup(2,3+\\frac{2}{n-2})$, the viscosity solutions to parabolic normalized $p(x)$-Laplace equation have the $W^{2,2}_{loc}$-regularity in the spatial variable and the $W^{1,2}_{loc}$-regularity in the time variable."],"url":"http://arxiv.org/abs/2403.03834v1","category":"math.AP"}
{"created":"2024-03-06 16:17:34","title":"Linear and nonlinear system identification under $\\ell_1$- and group-Lasso regularization via L-BFGS-B","abstract":"In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks. We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python implementation of the proposed identification method is available in the package \\texttt{jax-sysid}, available at \\url{https://github.com/bemporad/jax-sysid}.","sentences":["In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm.","For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view.","The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks.","We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022).","A Python implementation of the proposed identification method is available in the package \\texttt{jax-sysid}, available at \\url{https://github.com/bemporad/jax-sysid}."],"url":"http://arxiv.org/abs/2403.03827v1","category":"eess.SY"}
{"created":"2024-03-06 16:06:40","title":"Identifying Black Holes Through Space Telescopes and Deep Learning","abstract":"The EHT has captured a series of images of black holes, demonstrating the possibility of detecting them through a radio interferometer. These images could provide valuable information about the gravitational environment near the event horizon. However, accurate detection and parameter estimation for candidate black holes are necessary. This paper explores the potential for identifying black holes in the ultraviolet band using space telescopes. Firstly, a data pipeline is established for generating simulated observations. Next, we present an ensemble neural network model for detecting and estimating the parameters of black holes with different angular sizes. The model can achieve a mean average precision value [0.5] of 0.9176 when reaching the imaging FWHM ($\\theta_c$) of the telescope and maintains its detection ability until $0.33\\theta_c$. These results indicate that our methodology enables super-resolution recognition. The model accurately estimates the size of the accretion disk, the inclination angle, the positional angle, and the temperature. This provides a specific scheme for automatically detecting captured images using neural networks. Moreover, the validity of the model is assessed by analyzing the shadow of M87*, which indicates that the model can discriminate the black hole from background noise and other celestial objects with a confidence level of 0.639. Our work demonstrates the feasibility of detecting black holes in the UV band and provides a new method for the accurate and real-time detection of candidate black holes and further parameter estimation.","sentences":["The EHT has captured a series of images of black holes, demonstrating the possibility of detecting them through a radio interferometer.","These images could provide valuable information about the gravitational environment near the event horizon.","However, accurate detection and parameter estimation for candidate black holes are necessary.","This paper explores the potential for identifying black holes in the ultraviolet band using space telescopes.","Firstly, a data pipeline is established for generating simulated observations.","Next, we present an ensemble neural network model for detecting and estimating the parameters of black holes with different angular sizes.","The model can achieve a mean average precision value [0.5] of 0.9176 when reaching the imaging FWHM ($\\theta_c$) of the telescope and maintains its detection ability until $0.33\\theta_c$. These results indicate that our methodology enables super-resolution recognition.","The model accurately estimates the size of the accretion disk, the inclination angle, the positional angle, and the temperature.","This provides a specific scheme for automatically detecting captured images using neural networks.","Moreover, the validity of the model is assessed by analyzing the shadow of M87*, which indicates that the model can discriminate the black hole from background noise and other celestial objects with a confidence level of 0.639.","Our work demonstrates the feasibility of detecting black holes in the UV band and provides a new method for the accurate and real-time detection of candidate black holes and further parameter estimation."],"url":"http://arxiv.org/abs/2403.03821v2","category":"astro-ph.IM"}
{"created":"2024-03-06 15:57:56","title":"A Precision Drone Landing System using Visual and IR Fiducial Markers and a Multi-Payload Camera","abstract":"We propose a method for autonomous precision drone landing with fiducial markers and a gimbal-mounted, multi-payload camera with wide-angle, zoom, and IR sensors. The method has minimal data requirements; it depends primarily on the direction from the drone to the landing pad, enabling it to switch dynamically between the camera's different sensors and zoom factors, and minimizing auxiliary sensor requirements. It eliminates the need for data such as altitude above ground level, straight-line distance to the landing pad, fiducial marker size, and 6 DoF marker pose (of which the orientation is problematic). We leverage the zoom and wide-angle cameras, as well as visual April Tag fiducial markers to conduct successful precision landings from much longer distances than in previous work (168m horizontal distance, 102m altitude). We use two types of April Tags in the IR spectrum - active and passive - for precision landing both at daytime and nighttime, instead of simple IR beacons used in most previous work. The active IR landing pad is heated; the novel, passive one is unpowered, at ambient temperature, and depends on its high reflectivity and an IR differential between the ground and the sky. Finally, we propose a high-level control policy to manage initial search for the landing pad and subsequent searches if it is lost - not addressed in previous work. The method demonstrates successful landings with the landing skids at least touching the landing pad, achieving an average error of 0.19m. It also demonstrates successful recovery and landing when the landing pad is temporarily obscured.","sentences":["We propose a method for autonomous precision drone landing with fiducial markers and a gimbal-mounted, multi-payload camera with wide-angle, zoom, and IR sensors.","The method has minimal data requirements; it depends primarily on the direction from the drone to the landing pad, enabling it to switch dynamically between the camera's different sensors and zoom factors, and minimizing auxiliary sensor requirements.","It eliminates the need for data such as altitude above ground level, straight-line distance to the landing pad, fiducial marker size, and 6 DoF marker pose (of which the orientation is problematic).","We leverage the zoom and wide-angle cameras, as well as visual April Tag fiducial markers to conduct successful precision landings from much longer distances than in previous work (168m horizontal distance, 102m altitude).","We use two types of April Tags in the IR spectrum - active and passive - for precision landing both at daytime and nighttime, instead of simple IR beacons used in most previous work.","The active IR landing pad is heated; the novel, passive one is unpowered, at ambient temperature, and depends on its high reflectivity and an IR differential between the ground and the sky.","Finally, we propose a high-level control policy to manage initial search for the landing pad and subsequent searches if it is lost - not addressed in previous work.","The method demonstrates successful landings with the landing skids at least touching the landing pad, achieving an average error of 0.19m. It also demonstrates successful recovery and landing when the landing pad is temporarily obscured."],"url":"http://arxiv.org/abs/2403.03806v1","category":"cs.RO"}
{"created":"2024-03-06 15:55:48","title":"Dynamic Scaling of Two-Dimensional Polar Flocks","abstract":"We propose a hydrodynamic description of the homogeneous ordered phase of polar flocks. Starting from symmetry principles, we construct the appropriate equation for the dynamics of the Goldstone mode associated with the broken rotational symmetry. We then focus on the two-dimensional case considering both \"Malthusian flocks\" for which the density field is a fast variable that does not enter the hydrodynamic description and \"Vicsek flocks\" for which it does. In both cases, we argue in favor of scaling relations that allow to compute exactly the scaling exponents, which are found in excellent agreement with previous simulations of the Vicsek model and with the numerical integration of our hydrodynamic equations.","sentences":["We propose a hydrodynamic description of the homogeneous ordered phase of polar flocks.","Starting from symmetry principles, we construct the appropriate equation for the dynamics of the Goldstone mode associated with the broken rotational symmetry.","We then focus on the two-dimensional case considering both \"Malthusian flocks\" for which the density field is a fast variable that does not enter the hydrodynamic description and \"Vicsek flocks\" for which it does.","In both cases, we argue in favor of scaling relations that allow to compute exactly the scaling exponents, which are found in excellent agreement with previous simulations of the Vicsek model and with the numerical integration of our hydrodynamic equations."],"url":"http://arxiv.org/abs/2403.03804v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-06 15:50:35","title":"Strengthening nuclear symmetry energy constraints using multiple resonant shattering flares of neutron stars with realistic mass uncertainties","abstract":"With current and planned gravitational-wave (GW) observing runs, coincident multimessenger timing of Resonant Shattering Flares (RSFs) and GWs may soon allow for neutron star (NS) asteroseismology to be used to constrain the nuclear symmetry energy, an important property of fundamental nuclear physics that influences the composition and equation of state of NSs. In this work we examine the effects of combining multiple RSF detections on these symmetry energy constraints, and consider how realistic uncertainties in the masses of the progenitor NSs may weaken them. We show that the detection of subsequent multimessenger events has the potential to substantially improve constraints beyond those obtained from the first, and that this improvement is insensitive to the mass of the NSs which produce the RSFs and its uncertainty. This sets these asteroseismic constraints apart from bulk NS properties such as radius, for which the NS mass is highly important, meaning that any multimessenger RSF and GW events can equally improve our knowledge of fundamental physics.","sentences":["With current and planned gravitational-wave (GW) observing runs, coincident multimessenger timing of Resonant Shattering Flares (RSFs) and GWs may soon allow for neutron star (NS) asteroseismology to be used to constrain the nuclear symmetry energy, an important property of fundamental nuclear physics that influences the composition and equation of state of NSs.","In this work we examine the effects of combining multiple RSF detections on these symmetry energy constraints, and consider how realistic uncertainties in the masses of the progenitor NSs may weaken them.","We show that the detection of subsequent multimessenger events has the potential to substantially improve constraints beyond those obtained from the first, and that this improvement is insensitive to the mass of the NSs which produce the RSFs and its uncertainty.","This sets these asteroseismic constraints apart from bulk NS properties such as radius, for which the NS mass is highly important, meaning that any multimessenger RSF and GW events can equally improve our knowledge of fundamental physics."],"url":"http://arxiv.org/abs/2403.03798v1","category":"astro-ph.HE"}
{"created":"2024-03-06 15:50:29","title":"Modeling thermocapillary microgear rotation and transfer to translational particle propulsion","abstract":"In this study, we investigate the thermocapillary rotation of microgears at fluid interfaces and extend the concept of geometric asymmetry to the translational propulsion of micron-sized particles. We introduce a transient numerical model that couples the Navier-Stokes equations with heat transfer, displaying particle motion through a moving mesh interface. The model incorporates absorbed light illumination as a heat source and predicts both rotational and translational speeds of particles. Our simulations explore the influence of microgear design geometry and determine the scale at which thermocapillary Marangoni motion could serve as a viable propulsion method. A clear correlation between Reynolds number and propulsion efficiency can be recognized. To transfer the asymmetry-based propulsion principle from rotational to directed translational motion, various particle geometries are considered. The exploration of breaking geometric symmetry for translational propulsion is mostly ignored in the existing literature, thus warranting further discussion. Therefore, we analyse expected translational speeds in comparison to corresponding microgears to provide insights into this promising propulsion method.","sentences":["In this study, we investigate the thermocapillary rotation of microgears at fluid interfaces and extend the concept of geometric asymmetry to the translational propulsion of micron-sized particles.","We introduce a transient numerical model that couples the Navier-Stokes equations with heat transfer, displaying particle motion through a moving mesh interface.","The model incorporates absorbed light illumination as a heat source and predicts both rotational and translational speeds of particles.","Our simulations explore the influence of microgear design geometry and determine the scale at which thermocapillary Marangoni motion could serve as a viable propulsion method.","A clear correlation between Reynolds number and propulsion efficiency can be recognized.","To transfer the asymmetry-based propulsion principle from rotational to directed translational motion, various particle geometries are considered.","The exploration of breaking geometric symmetry for translational propulsion is mostly ignored in the existing literature, thus warranting further discussion.","Therefore, we analyse expected translational speeds in comparison to corresponding microgears to provide insights into this promising propulsion method."],"url":"http://arxiv.org/abs/2403.03797v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 15:47:22","title":"Complete NLO corrections to top-quark pair production with isolated photons","abstract":"We compute for the first time the so-called complete NLO corrections to top-quark pair production with one and two isolated photons in the di-lepton top-quark decay channel. The Narrow Width Approximation is used for the modeling of unstable top quarks and $W$ bosons. Higher-order QCD and EW effects as well as photon bremsstrahlung are consistently included at all stages: in production and top-quark decays. We present results at the integrated and differential fiducial cross-section level for both processes for the LHC Run II center-of-mass energy of $\\sqrt{s}=13$ TeV. In addition, we investigate the scale choice in photonic observables. Finally, the individual size of each subleading contribution is discussed in detail and the origin of the main subleading corrections is scrutinised. For the latter case, alternative calculations are performed in which the subleading NLO corrections are included only in the production of $t\\bar{t}\\gamma$ and $t\\bar{t}\\gamma\\gamma$.","sentences":["We compute for the first time the so-called complete NLO corrections to top-quark pair production with one and two isolated photons in the di-lepton top-quark decay channel.","The Narrow Width Approximation is used for the modeling of unstable top quarks and $W$ bosons.","Higher-order QCD and EW effects as well as photon bremsstrahlung are consistently included at all stages: in production and top-quark decays.","We present results at the integrated and differential fiducial cross-section level for both processes for the LHC Run II center-of-mass energy of $\\sqrt{s}=13$ TeV. In addition, we investigate the scale choice in photonic observables.","Finally, the individual size of each subleading contribution is discussed in detail and the origin of the main subleading corrections is scrutinised.","For the latter case, alternative calculations are performed in which the subleading NLO corrections are included only in the production of $t\\bar{t}\\gamma$ and $t\\bar{t}\\gamma\\gamma$."],"url":"http://arxiv.org/abs/2403.03796v1","category":"hep-ph"}
{"created":"2024-03-06 15:42:37","title":"Convergence rate for a regularized scalar conservation law","abstract":"This work revisits a recent finding by the first author concerning the local convergence of a regularized scalar conservation law. We significantly improve the original statement by establishing a global convergence result within the Lebesgue spaces $L^\\infty_{\\mathrm{loc}}(\\mathbb{R}^+;L^p(\\mathbb{R}))$, for any $p \\in [1,\\infty)$, as the regularization parameter $\\ell$ approaches zero. Notably, we demonstrate that this stability result is accompanied by a quantifiable rate of convergence. A key insight in our proof lies in the observation that the fluctuations of the solutions remain under control in low regularity spaces, allowing for a potential quantification of their behavior in the limit as $\\ell\\to 0$. This is achieved through a careful asymptotic analysis of the perturbative terms in the regularized equation, which, in our view, constitutes a pivotal contribution to the core findings of this paper.","sentences":["This work revisits a recent finding by the first author concerning the local convergence of a regularized scalar conservation law.","We significantly improve the original statement by establishing a global convergence result within the Lebesgue spaces $L^\\infty_{\\mathrm{loc}}(\\mathbb{R}^+;L^p(\\mathbb{R}))$, for any $p \\in [1,\\infty)$, as the regularization parameter $\\ell$ approaches zero.","Notably, we demonstrate that this stability result is accompanied by a quantifiable rate of convergence.","A key insight in our proof lies in the observation that the fluctuations of the solutions remain under control in low regularity spaces, allowing for a potential quantification of their behavior in the limit as $\\ell\\to 0$.","This is achieved through a careful asymptotic analysis of the perturbative terms in the regularized equation, which, in our view, constitutes a pivotal contribution to the core findings of this paper."],"url":"http://arxiv.org/abs/2403.03794v1","category":"math.AP"}
{"created":"2024-03-06 15:40:30","title":"Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks","abstract":"We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., \"Ignore previous instructions and...\"), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.   Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches.","sentences":["We introduce a new family of prompt injection attacks, termed Neural Exec.","Unlike known attacks that rely on handcrafted strings (e.g., \"Ignore previous instructions and...\"), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.   ","Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality.","In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications.","More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches."],"url":"http://arxiv.org/abs/2403.03792v1","category":"cs.CR"}
{"created":"2024-03-06 15:34:23","title":"Recovering orthogonality from quasi-nature of Spectral transformations","abstract":"In this contribution, quasi-orthogonality of polynomials generated by Geronimus and Uvarov transformations is analyzed. An attempt is made to discuss the recovery of the source orthogonal polynomial from the quasi-Geronimus and quasi-Uvarov polynomials of order one. Moreover, the discussion on the difference equation satisfied by quasi-Geronimus and quasi-Uvarov polynomials is presented. Furthermore, the orthogonality of quasi-Geronimus and quasi-Uvarov polynomials is achieved through the reduction of the degree of coefficients in the difference equation. During this procedure, alternative representations of the parameters responsible for achieving orthogonality are derived. One of these representations involves the Stieltjes transform of the measure. Finally, the recurrence coefficients ensuring the existence of a measure that makes the quasi-Geronimus Laguerre polynomial of order one an orthogonal polynomial are calculated.","sentences":["In this contribution, quasi-orthogonality of polynomials generated by Geronimus and Uvarov transformations is analyzed.","An attempt is made to discuss the recovery of the source orthogonal polynomial from the quasi-Geronimus and quasi-Uvarov polynomials of order one.","Moreover, the discussion on the difference equation satisfied by quasi-Geronimus and quasi-Uvarov polynomials is presented.","Furthermore, the orthogonality of quasi-Geronimus and quasi-Uvarov polynomials is achieved through the reduction of the degree of coefficients in the difference equation.","During this procedure, alternative representations of the parameters responsible for achieving orthogonality are derived.","One of these representations involves the Stieltjes transform of the measure.","Finally, the recurrence coefficients ensuring the existence of a measure that makes the quasi-Geronimus Laguerre polynomial of order one an orthogonal polynomial are calculated."],"url":"http://arxiv.org/abs/2403.03789v1","category":"math.CA"}
{"created":"2024-03-06 15:30:00","title":"A quantitative second order Sobolev regularity for (inhmogeneous) normalized $p(\\cdot)$-Laplace equations","abstract":"Let $\\Omega$ be a domain of $\\mathbb R^n$ with $n\\ge 2$ and $p(\\cdot)$ be a local Lipschitz funcion in $\\Omega$ with $1<p(x)<\\infty$ in $\\Omega$. We build up an interior quantitative second order Sobolev regularity for the normalized $p(\\cdot)$-Laplace equation $-\\Delta^N_{p(\\cdot)}u=0$ in $\\Omega$ as well as the corresponding inhomogeneous equation $-\\Delta^N_{p(\\cdot)}u=f$ in $\\Omega$ with $f\\in C^0(\\Omega)$.   In particular, given any viscosity solution $u$ to $-\\Delta^N_{p(\\cdot)}u=0$ in $\\Omega$, we prove the following:   (i) in dimension $n=2$, for any subdomain $U\\Subset\\Omega$ and any $\\beta\\ge 0$, one has $|Du|^\\beta Du\\in L^{2+\\delta}(U)$ locally with a quantitative upper bound, and moreover, the map $(x_1,x_2)\\to |Du|^\\beta(u_{x_1},-u_{x_2})$ is quasiregular in $U$ in the sense that   $$|D[|Du|^\\beta Du]|^2\\leq -C\\det D[|Du|^\\beta Du] \\quad \\mbox{a.e. in $U$}.$$   (ii) in dimension $n\\geq3$, for any subdomain $U\\Subset\\Omega$ with   $ \\inf_U p(x)>1$ and $\\sup_Up(x)<3+\\frac2{n-2}$, one has $D^2u\\in L^{2+\\delta}(U)$ locally with a quantitative upper bound, and also with a pointwise upper bound $$|D^2u|^2\\le -C\\sum_{1\\leq i<j\\le n}[u_{x_ix_j}u_{x_jx_i}-u_{x_ix_i}u_{x_jx_j}] \\quad \\mbox{a.e. in $U$}.$$   Here constants $\\delta>0$ and $C\\geq 1$ are independent of $u$. These extend the related results obtaind by Adamowicz-H\\\"ast\\\"o \\cite{AH2010} when $n=2$ and $\\beta=0$.","sentences":["Let $\\Omega$ be a domain of $\\mathbb R^n$ with $n\\ge 2$ and $p(\\cdot)$ be a local Lipschitz funcion in $\\Omega$ with $1<p(x)<\\infty$ in $\\Omega$. We build up an interior quantitative second order Sobolev regularity for the normalized $p(\\cdot)$-Laplace equation $-\\Delta^N_{p(\\cdot)}u=0$ in $\\Omega$ as well as the corresponding inhomogeneous equation $-\\Delta^N_{p(\\cdot)}u=f$ in $\\Omega$ with $f\\in C^0(\\Omega)$.   In particular, given any viscosity solution $u$ to $-\\Delta^N_{p(\\cdot)}u=0$ in $\\Omega$, we prove the following:   (i) in dimension $n=2$, for any subdomain $U\\Subset\\Omega$ and any $\\beta\\ge 0$, one has $|Du|^\\beta Du\\in L^{2+\\delta}(U)$ locally with a quantitative upper bound, and moreover, the map $(x_1,x_2)\\to |Du|^\\beta(u_{x_1},-u_{x_2})$ is quasiregular in $U$ in the sense that   $$|D[|Du|^\\beta Du]|^2\\leq -C\\det D[|Du|^\\beta","Du] \\quad \\mbox{a.e.","in $U$}.$$   (ii) in dimension $n\\geq3$, for any subdomain $U\\Subset\\Omega$ with   $ \\inf_U p(x)>1$ and $\\sup_Up(x)<3+\\frac2{n-2}$, one has $D^2u\\in L^{2+\\delta}(U)$ locally with a quantitative upper bound, and also with a pointwise upper bound $$|D^2u|^2\\le","-C\\sum_{1\\leq","i<j\\le n}[u_{x_ix_j}u_{x_jx_i}-u_{x_ix_i}u_{x_jx_j}]","\\quad \\mbox{a.e.","in $U$}.$$   Here constants $\\delta>0$ and $C\\geq 1$ are independent of $u$. These extend the related results obtaind by Adamowicz-H\\\"ast\\\"o \\cite{AH2010} when $n=2$ and $\\beta=0$."],"url":"http://arxiv.org/abs/2403.03784v1","category":"math.AP"}
{"created":"2024-03-06 15:26:05","title":"On the Injectivity Radius of the Stiefel Manifold: Numerical investigations and an explicit construction of a cut point at short distance","abstract":"Arguably, geodesics are the most important geometric objects on a differentiable manifold. They describe candidates for shortest paths and are guaranteed to be unique shortest paths when the starting velocity stays within the so-called injectivity radius of the manifold. In this work, we investigate the injectivity radius of the Stiefel manifold under the canonical metric. The Stiefel manifold $St(n,p)$ is the set of rectangular matrices of dimension $n$-by-$p$ with orthogonal columns, sometimes also called the space of orthogonal $p$-frames in $\\mathbb{R}^n$. Using a standard curvature argument, Rentmeesters has shown in 2013 that the injectivity radius of the Stiefel manifold is bounded by $\\sqrt{\\frac{4}{5}}\\pi$. It is an open question, whether this bound is sharp. With the definition of the injectivity radius via cut points of geodesics, we gain access to the information of the injectivity radius by investigating geodesics. More precisely, we consider the behavior of special variations of geodesics, called Jacobi fields. By doing so, we are able to present an explicit example of a cut point. In addition, since the theoretical analysis of geodesics for cut points and especially conjugate points as a type of cut points is difficult, we investigate the question of the sharpness of the bound by means of numerical experiments.","sentences":["Arguably, geodesics are the most important geometric objects on a differentiable manifold.","They describe candidates for shortest paths and are guaranteed to be unique shortest paths when the starting velocity stays within the so-called injectivity radius of the manifold.","In this work, we investigate the injectivity radius of the Stiefel manifold under the canonical metric.","The Stiefel manifold $St(n,p)$ is the set of rectangular matrices of dimension $n$-by-$p$ with orthogonal columns, sometimes also called the space of orthogonal $p$-frames in $\\mathbb{R}^n$. Using a standard curvature argument, Rentmeesters has shown in 2013 that the injectivity radius of the Stiefel manifold is bounded by $\\sqrt{\\frac{4}{5}}\\pi$. It is an open question, whether this bound is sharp.","With the definition of the injectivity radius via cut points of geodesics, we gain access to the information of the injectivity radius by investigating geodesics.","More precisely, we consider the behavior of special variations of geodesics, called Jacobi fields.","By doing so, we are able to present an explicit example of a cut point.","In addition, since the theoretical analysis of geodesics for cut points and especially conjugate points as a type of cut points is difficult, we investigate the question of the sharpness of the bound by means of numerical experiments."],"url":"http://arxiv.org/abs/2403.03782v1","category":"math.NA"}
{"created":"2024-03-06 15:00:00","title":"Nonlinear Landau fan diagram and aperiodic magnetic oscillations in three-dimensional systems","abstract":"Quantum oscillations offer a powerful probe for the geometry and topology of the Fermi surface in metals. Onsager's semiclassical quantization relation governs these periodic oscillations in 1/B, leading to a linear Landau fan diagram. However, higher-order magnetic susceptibility-induced corrections give rise to a generalized Onsager's relation, manifesting in experiments as a nonlinear Landau fan diagram and aperiodic quantum oscillations. Here, we explore the generalized Onsager's relation to three-dimensional (3D) systems to capture the B-induced corrections in the free energy and the Fermi surface. We unravel the manifestation of these corrections in the nonlinear Landau fan diagrams and aperiodic quantum oscillations by deriving the B-dependent oscillation frequency and the generalized Lifshitz-Kosevich equation, respectively. Our theory explains the necessary conditions to observe these fascinating effects and predicts the magnetic field dependence of the cyclotron mass. As a concrete example, we elucidate these effects in a 3D spin-orbit coupled system and extract zero-field magnetic response functions from analytically obtained Landau levels. Our comprehensive study deepens and advances our understanding of aperiodic quantum oscillations.","sentences":["Quantum oscillations offer a powerful probe for the geometry and topology of the Fermi surface in metals.","Onsager's semiclassical quantization relation governs these periodic oscillations in 1/B, leading to a linear Landau fan diagram.","However, higher-order magnetic susceptibility-induced corrections give rise to a generalized Onsager's relation, manifesting in experiments as a nonlinear Landau fan diagram and aperiodic quantum oscillations.","Here, we explore the generalized Onsager's relation to three-dimensional (3D) systems to capture the B-induced corrections in the free energy and the Fermi surface.","We unravel the manifestation of these corrections in the nonlinear Landau fan diagrams and aperiodic quantum oscillations by deriving the B-dependent oscillation frequency and the generalized Lifshitz-Kosevich equation, respectively.","Our theory explains the necessary conditions to observe these fascinating effects and predicts the magnetic field dependence of the cyclotron mass.","As a concrete example, we elucidate these effects in a 3D spin-orbit coupled system and extract zero-field magnetic response functions from analytically obtained Landau levels.","Our comprehensive study deepens and advances our understanding of aperiodic quantum oscillations."],"url":"http://arxiv.org/abs/2403.03765v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 14:52:43","title":"A new pairwise boost quantum number from celestial states","abstract":"Infrared effects in the scattering of particles in gravity and electrodynamics entail an exchange of relativistic angular momentum between pairs of particles and the gauge field. Due to this exchange particles can carry an asymptotically non-vanishing \"pairwise\" boost-like angular momentum proportional to the product of their couplings to the field. At the quantum level this asymptotic angular momentum suggests the existence of a new quantum number carried by multi-particle states. We argue that such quantum number is related to a modification of the action of the generators of Lorentz transformations on multi-particle states. We derive such a modification using a group-theoretic argument based on the little group of the conformal primary basis for asymptotic states. The corresponding representation is an extension of the ordinary multi-particle Fock representation of the Poincar\\'e group. The new multi-particle states belonging to such representation no longer factorize into tensor products of one-particle states. Viewed from a gravitational point of view, our results provide evidence for a universal breakdown of the description of multi-particle sates in terms of Fock space due to infrared back-reaction.","sentences":["Infrared effects in the scattering of particles in gravity and electrodynamics entail an exchange of relativistic angular momentum between pairs of particles and the gauge field.","Due to this exchange particles can carry an asymptotically non-vanishing \"pairwise\" boost-like angular momentum proportional to the product of their couplings to the field.","At the quantum level this asymptotic angular momentum suggests the existence of a new quantum number carried by multi-particle states.","We argue that such quantum number is related to a modification of the action of the generators of Lorentz transformations on multi-particle states.","We derive such a modification using a group-theoretic argument based on the little group of the conformal primary basis for asymptotic states.","The corresponding representation is an extension of the ordinary multi-particle Fock representation of the Poincar\\'e group.","The new multi-particle states belonging to such representation no longer factorize into tensor products of one-particle states.","Viewed from a gravitational point of view, our results provide evidence for a universal breakdown of the description of multi-particle sates in terms of Fock space due to infrared back-reaction."],"url":"http://arxiv.org/abs/2403.03760v1","category":"hep-th"}
{"created":"2024-03-06 14:52:27","title":"Homoclinic classes of geodesic flows on rank 1 manifolds","abstract":"We prove that the homoclinic class of every hyperbolic periodic orbit of a geodesic flow over a $C^\\infty$ closed rank 1 Riemannian manifold equals the unit tangent bundle. As an application, we give a proof using symbolic dynamics of the theorem of Knieper on the uniqueness of the measure of maximal entropy and theorems of Burns et al on the uniqueness of equilibrium states.","sentences":["We prove that the homoclinic class of every hyperbolic periodic orbit of a geodesic flow over a $C^\\infty$ closed rank 1 Riemannian manifold equals the unit tangent bundle.","As an application, we give a proof using symbolic dynamics of the theorem of Knieper on the uniqueness of the measure of maximal entropy and theorems of Burns et al on the uniqueness of equilibrium states."],"url":"http://arxiv.org/abs/2403.03759v1","category":"math.DS"}
{"created":"2024-03-06 14:27:29","title":"Probabilistic Topic Modelling with Transformer Representations","abstract":"Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity. The corresponding source code is available at https://github.com/ArikReuter/TNTM.","sentences":["Topic modelling was mostly dominated by Bayesian graphical models during the last decade.","With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors.","We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling.","Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA).","We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility.","Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity.","The corresponding source code is available at https://github.com/ArikReuter/TNTM."],"url":"http://arxiv.org/abs/2403.03737v1","category":"cs.LG"}
{"created":"2024-03-06 14:20:39","title":"Detection prospects of Very and Ultra High-Energy gamma rays from extended sources with ASTRI, CTA, and LHAASO","abstract":"Context. The recent discovery of several ultra high-energy gamma-ray emitters in our Galaxy constitutes a significant advancement towards unveiling its most powerful accelerators and their properties. Nonetheless, in order to unambiguously locate the regions where the highest energy particles are produced and understand the responsible physical mechanisms, detailed spectral and morphological studies are required, especially given that most of the observed sources were found to be significantly extended. Aims. In these regards, pointing observations with the next-generation Imaging Atmospheric Cherenkov Telescopes, like the Cherenkov Telescope Array (CTA) Observatory and the ASTRI Mini-Array (ASTRI), are expected to provide significant improvements. Here we aim at identifying the most promising sources to target in future observations. Methods. To this purpose, we performed a comparative analysis of the expected performance of ASTRI and CTA, computing their differential sensitivities towards extended sources, and further explored their capabilities with respect to specific case studies, including follow-ups of existing gamma-ray source catalogs. Results. We find that almost all of the sources so far detected by LHAASO-WCDA and HGPS will be in the reach of ASTRI and CTA with 300 and 50 hours of exposure, respectively. For the highest energy emitters detected by LHAASO-KM2A, in turn, we provide the list of the most promising objects to be investigated. We further examined specific classes of sources in order to identify potentially detectable gamma-ray emitters, such as passive molecular clouds (i.e. illuminated by the cosmic-ray sea) and pulsars surrounded by a halo of runaway particles.","sentences":["Context.","The recent discovery of several ultra high-energy gamma-ray emitters in our Galaxy constitutes a significant advancement towards unveiling its most powerful accelerators and their properties.","Nonetheless, in order to unambiguously locate the regions where the highest energy particles are produced and understand the responsible physical mechanisms, detailed spectral and morphological studies are required, especially given that most of the observed sources were found to be significantly extended.","Aims.","In these regards, pointing observations with the next-generation Imaging Atmospheric Cherenkov Telescopes, like the Cherenkov Telescope Array (CTA) Observatory and the ASTRI Mini-Array (ASTRI), are expected to provide significant improvements.","Here we aim at identifying the most promising sources to target in future observations.","Methods.","To this purpose, we performed a comparative analysis of the expected performance of ASTRI and CTA, computing their differential sensitivities towards extended sources, and further explored their capabilities with respect to specific case studies, including follow-ups of existing gamma-ray source catalogs.","Results.","We find that almost all of the sources so far detected by LHAASO-WCDA and HGPS will be in the reach of ASTRI and CTA with 300 and 50 hours of exposure, respectively.","For the highest energy emitters detected by LHAASO-KM2A, in turn, we provide the list of the most promising objects to be investigated.","We further examined specific classes of sources in order to identify potentially detectable gamma-ray emitters, such as passive molecular clouds (i.e. illuminated by the cosmic-ray sea) and pulsars surrounded by a halo of runaway particles."],"url":"http://arxiv.org/abs/2403.03731v1","category":"astro-ph.HE"}
{"created":"2024-03-06 14:18:52","title":"Case studies on time-dependent Ginzburg-Landau simulations for superconducting applications","abstract":"The macroscopic electromagnetic properties of type II superconductors are primarily influenced by the behavior of microscopic superconducting flux quantum units. Time-dependent Ginzburg-Landau (TDGL) equations provide an elegant and powerful tool for describing and examining both the statics and dynamics of these superconducting entities. They have been instrumental in replicating and elucidating numerous experimental results over the past decades.This paper provides a comprehensive overview of the progress in TDGL simulations, focusing on three key aspects of superconductor applications. The initial section delves into vortex rectification in superconductors described within the TDGL framework. We specifically highlight the superconducting diode effect achieved through asymmetric pinning landscapes and the reversible manipulation of vortex ratchets with dynamic pinning landscapes. The subsequent section reviews the achievements of TDGL simulations concerning the critical current density of superconductors, emphasizing the optimization of pinning sites, particularly vortex pinning and dynamics in polycrystalline Nb$_3$Sn with grain boundaries. The third part concentrates on numerical modeling of vortex penetration and dynamics in superconducting radio frequency (SRF) cavities, including a discussion of superconductor insulator superconductor multilayer structures. In the last section, we present key findings, insights, and perspectives derived from the discussed simulations.","sentences":["The macroscopic electromagnetic properties of type II superconductors are primarily influenced by the behavior of microscopic superconducting flux quantum units.","Time-dependent Ginzburg-Landau (TDGL) equations provide an elegant and powerful tool for describing and examining both the statics and dynamics of these superconducting entities.","They have been instrumental in replicating and elucidating numerous experimental results over the past decades.","This paper provides a comprehensive overview of the progress in TDGL simulations, focusing on three key aspects of superconductor applications.","The initial section delves into vortex rectification in superconductors described within the TDGL framework.","We specifically highlight the superconducting diode effect achieved through asymmetric pinning landscapes and the reversible manipulation of vortex ratchets with dynamic pinning landscapes.","The subsequent section reviews the achievements of TDGL simulations concerning the critical current density of superconductors, emphasizing the optimization of pinning sites, particularly vortex pinning and dynamics in polycrystalline Nb$_3$Sn with grain boundaries.","The third part concentrates on numerical modeling of vortex penetration and dynamics in superconducting radio frequency (SRF) cavities, including a discussion of superconductor insulator superconductor multilayer structures.","In the last section, we present key findings, insights, and perspectives derived from the discussed simulations."],"url":"http://arxiv.org/abs/2403.03729v1","category":"cond-mat.supr-con"}
{"created":"2024-03-06 13:36:31","title":"Online model error correction with neural networks: application to the Integrated Forecasting System","abstract":"In recent years, there has been significant progress in the development of fully data-driven global numerical weather prediction models. These machine learning weather prediction models have their strength, notably accuracy and low computational requirements, but also their weakness: they struggle to represent fundamental dynamical balances, and they are far from being suitable for data assimilation experiments. Hybrid modelling emerges as a promising approach to address these limitations. Hybrid models integrate a physics-based core component with a statistical component, typically a neural network, to enhance prediction capabilities. In this article, we propose to develop a model error correction for the operational Integrated Forecasting System (IFS) of the European Centre for Medium-Range Weather Forecasts using a neural network. The neural network is initially pre-trained offline using a large dataset of operational analyses and analysis increments. Subsequently, the trained network is integrated into the IFS within the Object-Oriented Prediction System (OOPS) so as to be used in data assimilation and forecast experiments. It is then further trained online using a recently developed variant of weak-constraint 4D-Var. The results show that the pre-trained neural network already provides a reliable model error correction, which translates into reduced forecast errors in many conditions and that the online training further improves the accuracy of the hybrid model in many conditions.","sentences":["In recent years, there has been significant progress in the development of fully data-driven global numerical weather prediction models.","These machine learning weather prediction models have their strength, notably accuracy and low computational requirements, but also their weakness: they struggle to represent fundamental dynamical balances, and they are far from being suitable for data assimilation experiments.","Hybrid modelling emerges as a promising approach to address these limitations.","Hybrid models integrate a physics-based core component with a statistical component, typically a neural network, to enhance prediction capabilities.","In this article, we propose to develop a model error correction for the operational Integrated Forecasting System (IFS) of the European Centre for Medium-Range Weather Forecasts using a neural network.","The neural network is initially pre-trained offline using a large dataset of operational analyses and analysis increments.","Subsequently, the trained network is integrated into the IFS within the Object-Oriented Prediction System (OOPS) so as to be used in data assimilation and forecast experiments.","It is then further trained online using a recently developed variant of weak-constraint 4D-Var.","The results show that the pre-trained neural network already provides a reliable model error correction, which translates into reduced forecast errors in many conditions and that the online training further improves the accuracy of the hybrid model in many conditions."],"url":"http://arxiv.org/abs/2403.03702v1","category":"stat.ML"}
{"created":"2024-03-06 13:23:55","title":"Spectral Phase Transition and Optimal PCA in Block-Structured Spiked models","abstract":"We discuss the inhomogeneous spiked Wigner model, a theoretical framework recently introduced to study structured noise in various learning scenarios, through the prism of random matrix theory, with a specific focus on its spectral properties. Our primary objective is to find an optimal spectral method and to extend the celebrated \\cite{BBP} (BBP) phase transition criterion -- well-known in the homogeneous case -- to our inhomogeneous, block-structured, Wigner model. We provide a thorough rigorous analysis of a transformed matrix and show that the transition for the appearance of 1) an outlier outside the bulk of the limiting spectral distribution and 2) a positive overlap between the associated eigenvector and the signal, occurs precisely at the optimal threshold, making the proposed spectral method optimal within the class of iterative methods for the inhomogeneous Wigner problem.","sentences":["We discuss the inhomogeneous spiked Wigner model, a theoretical framework recently introduced to study structured noise in various learning scenarios, through the prism of random matrix theory, with a specific focus on its spectral properties.","Our primary objective is to find an optimal spectral method and to extend the celebrated \\cite{BBP} (BBP) phase transition criterion -- well-known in the homogeneous case -- to our inhomogeneous, block-structured, Wigner model.","We provide a thorough rigorous analysis of a transformed matrix and show that the transition for the appearance of 1) an outlier outside the bulk of the limiting spectral distribution and 2) a positive overlap between the associated eigenvector and the signal, occurs precisely at the optimal threshold, making the proposed spectral method optimal within the class of iterative methods for the inhomogeneous Wigner problem."],"url":"http://arxiv.org/abs/2403.03695v1","category":"stat.ML"}
{"created":"2024-03-06 13:15:21","title":"General2Specialized LLMs Translation for E-commerce","abstract":"Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.","sentences":["Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents.","Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods.","To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain.","Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce.","The paradigm can be used for the NMT models based on Large language models (LLMs).","Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4."],"url":"http://arxiv.org/abs/2403.03689v1","category":"cs.CL"}
{"created":"2024-03-06 13:07:42","title":"3D Object Visibility Prediction in Autonomous Driving","abstract":"With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth. The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control. At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation. In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility. By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model's effectiveness and efficiency. Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios.","sentences":["With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth.","The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control.","At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation.","In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility.","By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model's effectiveness and efficiency.","Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios."],"url":"http://arxiv.org/abs/2403.03681v1","category":"cs.RO"}
{"created":"2024-03-06 13:01:55","title":"Application of Deep Learning Reduced-Order Modeling for Single-Phase Flow in Faulted Porous Media","abstract":"We apply reduced-order modeling (ROM) techniques to single-phase flow in faulted porous media, accounting for changing rock properties and fault geometry variations using a radial basis function mesh deformation method. This approach benefits from a mixed-dimensional framework that effectively manages the resulting non-conforming mesh. To streamline complex and repetitive calculations such as sensitivity analysis and solution of inverse problems, we utilize the Deep Learning Reduced Order Model (DL-ROM). This non-intrusive neural network-based technique is evaluated against the traditional Proper Orthogonal Decomposition (POD) method across various scenarios, demonstrating DL-ROM's capacity to expedite complex analyses with promising accuracy and efficiency.","sentences":["We apply reduced-order modeling (ROM) techniques to single-phase flow in faulted porous media, accounting for changing rock properties and fault geometry variations using a radial basis function mesh deformation method.","This approach benefits from a mixed-dimensional framework that effectively manages the resulting non-conforming mesh.","To streamline complex and repetitive calculations such as sensitivity analysis and solution of inverse problems, we utilize the Deep Learning Reduced Order Model (DL-ROM).","This non-intrusive neural network-based technique is evaluated against the traditional Proper Orthogonal Decomposition (POD) method across various scenarios, demonstrating DL-ROM's capacity to expedite complex analyses with promising accuracy and efficiency."],"url":"http://arxiv.org/abs/2403.03678v1","category":"math.NA"}
{"created":"2024-03-06 12:37:49","title":"Provable Filter for Real-world Graph Clustering","abstract":"Graph clustering, an important unsupervised problem, has been shown to be more resistant to advances in Graph Neural Networks (GNNs). In addition, almost all clustering methods focus on homophilic graphs and ignore heterophily. This significantly limits their applicability in practice, since real-world graphs exhibit a structural disparity and cannot simply be classified as homophily and heterophily. Thus, a principled way to handle practical graphs is urgently needed. To fill this gap, we provide a novel solution with theoretical support. Interestingly, we find that most homophilic and heterophilic edges can be correctly identified on the basis of neighbor information. Motivated by this finding, we construct two graphs that are highly homophilic and heterophilic, respectively. They are used to build low-pass and high-pass filters to capture holistic information. Important features are further enhanced by the squeeze-and-excitation block. We validate our approach through extensive experiments on both homophilic and heterophilic graphs. Empirical results demonstrate the superiority of our method compared to state-of-the-art clustering methods.","sentences":["Graph clustering, an important unsupervised problem, has been shown to be more resistant to advances in Graph Neural Networks (GNNs).","In addition, almost all clustering methods focus on homophilic graphs and ignore heterophily.","This significantly limits their applicability in practice, since real-world graphs exhibit a structural disparity and cannot simply be classified as homophily and heterophily.","Thus, a principled way to handle practical graphs is urgently needed.","To fill this gap, we provide a novel solution with theoretical support.","Interestingly, we find that most homophilic and heterophilic edges can be correctly identified on the basis of neighbor information.","Motivated by this finding, we construct two graphs that are highly homophilic and heterophilic, respectively.","They are used to build low-pass and high-pass filters to capture holistic information.","Important features are further enhanced by the squeeze-and-excitation block.","We validate our approach through extensive experiments on both homophilic and heterophilic graphs.","Empirical results demonstrate the superiority of our method compared to state-of-the-art clustering methods."],"url":"http://arxiv.org/abs/2403.03666v1","category":"cs.LG"}
{"created":"2024-03-06 12:08:14","title":"K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data","abstract":"Sourced from various sensors and organized chronologically, Multivariate Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g., correlations among sensors. To capture these dependencies, Graph Neural Networks (GNNs) have emerged as powerful tools, yet their effectiveness is restricted by the quality of graph construction from MTS data. Typically, existing approaches construct graphs solely from MTS signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies. To address this challenge, we propose a novel framework named K-Link, leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias. Leveraging the knowledge embedded in LLMs, such as physical principles, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-level knowledge. To harness the potential of the knowledge-link graph in enhancing the graph derived from MTS data, we propose a graph alignment module, facilitating the transfer of semantic knowledge within the knowledge-link graph into the MTS-derived graph. By doing so, we can improve the graph quality, ensuring effective representation learning with GNNs for MTS data. Extensive experiments demonstrate the efficacy of our approach for superior performance across various MTS-related downstream tasks.","sentences":["Sourced from various sensors and organized chronologically, Multivariate Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g., correlations among sensors.","To capture these dependencies, Graph Neural Networks (GNNs) have emerged as powerful tools, yet their effectiveness is restricted by the quality of graph construction from MTS data.","Typically, existing approaches construct graphs solely from MTS signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies.","To address this challenge, we propose a novel framework named K-Link, leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias.","Leveraging the knowledge embedded in LLMs, such as physical principles, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-level knowledge.","To harness the potential of the knowledge-link graph in enhancing the graph derived from MTS data, we propose a graph alignment module, facilitating the transfer of semantic knowledge within the knowledge-link graph into the MTS-derived graph.","By doing so, we can improve the graph quality, ensuring effective representation learning with GNNs for MTS data.","Extensive experiments demonstrate the efficacy of our approach for superior performance across various MTS-related downstream tasks."],"url":"http://arxiv.org/abs/2403.03645v1","category":"cs.AI"}
{"created":"2024-03-06 11:51:57","title":"Stochastic partial differential equations for superprocesses in random environments","abstract":"Let $X=(X_t, t\\geq 0)$ be a superprocess in a random environment described by a Gaussian noise $W^g=\\{W^g(t,x), t\\geq 0, x\\in \\mathbb{R}^d\\}$ white in time and colored in space with correlation kernel $g(x,y)$. We show that when $d=1$, $X_t$ admits a jointly continuous density function $X_t(x)$ that is a unique in law solution to a stochastic partial differential equation   \\begin{align*} \\frac{\\partial }{\\partial t}X_t(x)=\\frac{\\Delta}{2} X_t(x)+\\sqrt{X_t(x)} \\dot{V}(t,x)+X_t(x)\\dot{W}^g(t, x) , \\quad X_t(x)\\geq 0, \\end{align*} where $V=\\{V(t,x), t\\geq 0, x\\in \\mathbb{R}\\}$ is a space-time white noise and is orthogonal with $W^g$. When $d\\geq 2$, we prove that $X_t$ is singular and hence density does not exist.","sentences":["Let $X=(X_t, t\\geq 0)$ be a superprocess in a random environment described by a Gaussian noise $W^g=\\{W^g(t,x), t\\geq 0, x\\in \\mathbb{R}^d\\}$ white in time and colored in space with correlation kernel $g(x,y)$. We show that when $d=1$, $X_t$ admits a jointly continuous density function $X_t(x)$ that is a unique in law solution to a stochastic partial differential equation   \\begin{align*} \\frac{\\partial }{\\partial t}X_t(x)=\\frac{\\Delta}{2} X_t(x)+\\sqrt{X_t(x)} \\dot{V}(t,x)+X_t(x)\\dot{W}^g(t, x) , \\quad X_t(x)\\geq 0, \\end{align*} where $V=\\{V(t,x), t\\geq 0, x\\in \\mathbb{R}\\}$ is a space-time white noise and is orthogonal with $W^g$.","When $d\\geq 2$, we prove that $X_t$ is singular and hence density does not exist."],"url":"http://arxiv.org/abs/2403.03638v1","category":"math.PR"}
{"created":"2024-03-06 11:39:20","title":"A reduced-order modeling of pattern formations","abstract":"Chemical and biochemical reactions can exhibit surprisingly different behaviours from multiple steady-state solutions to oscillatory solutions and chaotic behaviours. Such behaviour has been of great interest to researchers for many decades. The Briggs-Rauscher, Belousov-Zhabotinskii and Bray-Liebhafsky reactions, for which periodic variations in concentrations can be visualized by changes in colour, are experimental examples of oscillating behaviour in chemical systems. These type of systems are modelled by a system of partial differential equations coupled by a nonlinearity.   However, analysing the pattern, one may suspect that the dynamic is only generated by a finite number of spatial Fourier modes. In fluid dynamics, it is shown that for large times, the solution is determined by a finite number of spatial Fourier modes, called determining modes. In the article, we first introduce the concept of determining modes and show that, indeed, it is sufficient to characterise the dynamic by only a finite number of spatial Fourier modes.   In particular, we analyse the exact number of the determining modes of $u$ and $v$, where the couple $(u,v)$ solves the following stochastic system   \\begin{equation*}   \\partial_t{u}(t) = r_1\\Delta u(t) -\\alpha_1u(t)- \\gamma_1u(t)v^2(t) + f(1 - u(t)) + g(t),\\quad   \\partial_t{v}(t) = r_2\\Delta v(t) -\\alpha_2v(t) + \\gamma_2 u(t)v^2(t) + h(t),\\quad   u(0) = u_0,\\;v(0) = v_0,   \\end{equation*}   where $r_1,r_2,\\gamma_1,\\gamma_2>0$, $\\alpha_1,\\alpha_2 \\ge 0$ and $g,h$ are time depending mappings specified later.","sentences":["Chemical and biochemical reactions can exhibit surprisingly different behaviours from multiple steady-state solutions to oscillatory solutions and chaotic behaviours.","Such behaviour has been of great interest to researchers for many decades.","The Briggs-Rauscher, Belousov-Zhabotinskii and Bray-Liebhafsky reactions, for which periodic variations in concentrations can be visualized by changes in colour, are experimental examples of oscillating behaviour in chemical systems.","These type of systems are modelled by a system of partial differential equations coupled by a nonlinearity.   ","However, analysing the pattern, one may suspect that the dynamic is only generated by a finite number of spatial Fourier modes.","In fluid dynamics, it is shown that for large times, the solution is determined by a finite number of spatial Fourier modes, called determining modes.","In the article, we first introduce the concept of determining modes and show that, indeed, it is sufficient to characterise the dynamic by only a finite number of spatial Fourier modes.   ","In particular, we analyse the exact number of the determining modes of $u$ and $v$, where the couple $(u,v)$ solves the following stochastic system   \\begin{equation*}   \\partial_t{u}(t) = r_1\\Delta u(t) -\\alpha_1u(t)- \\gamma_1u(t)v^2(t) + f(1 - u(t))","+ g(t),\\quad   \\partial_t{v}(t) = r_2\\Delta v(t) -\\alpha_2v(t)","+ \\gamma_2 u(t)v^2(t)","+ h(t),\\quad   u(0) = u_0,\\;v(0) = v_0,   \\end{equation*}   where $r_1,r_2,\\gamma_1,\\gamma_2>0$, $\\alpha_1,\\alpha_2 \\ge 0$ and $g,h$ are time depending mappings specified later."],"url":"http://arxiv.org/abs/2403.03632v1","category":"math.AP"}
{"created":"2024-03-06 11:37:46","title":"The chiral critical locus and topological structures","abstract":"We study a differential graded VOA associated to the derived critical locus of a function $f$ on a smooth oriented $D$-dimensional variety $(X,\\mathbf{vol})$. Informally, this VOA, $\\mathbf{crit}^{ch}_{f}$, is just the algebra of chiral differential operators on the derived critical locus $\\mathbf{crit}_{f}$. We prove, using a generalization of a physical construction of Witten, the $\\mathbf{crit}^{ch}_{f}$ admits a \\emph{topological structure} if $f$ is homogeneous for a $\\mathbf{G}_{m}$ action on $(X,\\mathbf{vol})$. If $\\mathbf{vol}$ has weight $b$ and $f$ has weight $a$, we compute the rank of the topological structure in terms of the discrete invariants of the theory to be $$d=\\Big(D-\\frac{2b}{a}\\Big).$$ We conclude with some remarks about BV quantization and a simple computation of characters.","sentences":["We study a differential graded VOA associated to the derived critical locus of a function $f$ on a smooth oriented $D$-dimensional variety $(X,\\mathbf{vol})$. Informally, this VOA, $\\mathbf{crit}^{ch}_{f}$, is just the algebra of chiral differential operators on the derived critical locus $\\mathbf{crit}_{f}$. We prove, using a generalization of a physical construction of Witten, the $\\mathbf{crit}^{ch}_{f}$ admits a \\emph{topological structure} if $f$ is homogeneous for a $\\mathbf{G}_{m}$ action on $(X,\\mathbf{vol})$. If $\\mathbf{vol}$ has weight $b$ and $f$ has weight $a$, we compute the rank of the topological structure in terms of the discrete invariants of the theory to be $$d=\\Big(D-\\frac{2b}{a}\\Big).$$ We conclude with some remarks about BV quantization and a simple computation of characters."],"url":"http://arxiv.org/abs/2403.03630v1","category":"math.AG"}
{"created":"2024-03-06 11:02:07","title":"Comparison Performance of Spectrogram and Scalogram as Input of Acoustic Recognition Task","abstract":"Acoustic recognition is a common task for deep learning in recent researches, with the employment of spectral feature extraction such as Short-time Fourier transform and Wavelet transform. However, not many researches have found that discuss the advantages and drawbacks, as well as performance comparison amongst spectral feature extractors. In this consideration, this paper aims to comparing the attributes of these two transform types, called spectrogram and scalogram. A Convolutional Neural Networks for acoustic faults recognition is implemented, then the performance of these two types of spectral extractor is recorded for comparison. A latest research on the same audio database is considered for benchmarking to see how good the designed spectrogram and scalogram is. The advantages and limitations of them are also analyzed. By doing so, the results of this paper provide indications for application scenarios of spectrogram and scalogram, as well as potential further research directions in acoustic recognition.","sentences":["Acoustic recognition is a common task for deep learning in recent researches, with the employment of spectral feature extraction such as Short-time Fourier transform and Wavelet transform.","However, not many researches have found that discuss the advantages and drawbacks, as well as performance comparison amongst spectral feature extractors.","In this consideration, this paper aims to comparing the attributes of these two transform types, called spectrogram and scalogram.","A Convolutional Neural Networks for acoustic faults recognition is implemented, then the performance of these two types of spectral extractor is recorded for comparison.","A latest research on the same audio database is considered for benchmarking to see how good the designed spectrogram and scalogram is.","The advantages and limitations of them are also analyzed.","By doing so, the results of this paper provide indications for application scenarios of spectrogram and scalogram, as well as potential further research directions in acoustic recognition."],"url":"http://arxiv.org/abs/2403.03611v1","category":"eess.AS"}
{"created":"2024-03-06 10:55:50","title":"GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding","abstract":"Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.","sentences":["Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision.","In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes.","Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering.","The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene.","Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances.","Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified."],"url":"http://arxiv.org/abs/2403.03608v1","category":"cs.CV"}
{"created":"2024-03-07 14:43:24","title":"An Introduction to T-Systems -- with a special Emphasis on Sparse Moment Problems, Sparse Positivstellens\u00e4tze, and Sparse Nichtnegativstellens\u00e4tze","abstract":"These are the lecture notes based on [dD23] for the (upcoming) lecture \"T-systems with a special emphasis on sparse moment problems and sparse Positivstellens\\\"atze\" in the summer semester 2024 at the University of Konstanz. The main purpose of this lecture is to prove the sparse Positiv- and Nichtnegativstellens\\\"atze of Samuel Karlin (1963) and to apply them to the algebraic setting. That means given finitely many monomials, e.g. $1, x^2, x^3, x^6, x^7, x^9,$ how do all linear combinations of these look like which are strictly positive or non-negative on some interval $[a,b]$ or $[0,\\infty)$, e.g. describe and even write down all $f(x) = a_0 + a_1 x^2 + a_2 x^3 + a_3 x^6 + a_4 x^7 + a_5 x^9$ with $f(x)>0$ or $f(x)\\geq 0$ on $[a,b]$ or $[0,\\infty)$, respectively. To do this we introduce the theoretical framework in which this question can be answered: T-systems. We study these T-systems to arrive at Karlin's Positiv- and Nichtnegativstellensatz but we also do not hide the limitations of the T-systems approach. The main limitation is the Curtis$-$Mairhuber$-$Sieklucki Theorem which essentially states that every T-system is only one-dimensional and hence we can only apply these results to the univariate polynomial case. This can also be understood as a lesson or even a warning that this approach has been investigated and found to fail, i.e., learning about these results and limitations shall save students and researchers from following old footpaths which lead to a dead end. We took great care finding the correct historical references where the results appeared first but are perfectly aware that like people before we not always succeed.","sentences":["These are the lecture notes based on [dD23] for the (upcoming) lecture \"T-systems with a special emphasis on sparse moment problems and sparse Positivstellens\\\"atze\" in the summer semester 2024 at the University of Konstanz.","The main purpose of this lecture is to prove the sparse Positiv- and Nichtnegativstellens\\\"atze of Samuel Karlin (1963) and to apply them to the algebraic setting.","That means given finitely many monomials, e.g. $1, x^2, x^3, x^6, x^7, x^9,$ how do all linear combinations of these look like which are strictly positive or non-negative on some interval $","[a,b]$ or $[0,\\infty)$, e.g. describe and even write down all $f(x)","= a_0 + a_1 x^2 + a_2 x^3 + a_3 x^6 + a_4 x^7 +","a_5 x^9$ with $f(x)>0$ or $f(x)\\geq 0$ on $[a,b]$ or $[0,\\infty)$, respectively.","To do this we introduce the theoretical framework in which this question can be answered: T-systems.","We study these T-systems to arrive at Karlin's Positiv- and Nichtnegativstellensatz but we also do not hide the limitations of the T-systems approach.","The main limitation is the Curtis$-$Mairhuber$-$Sieklucki Theorem which essentially states that every T-system is only one-dimensional and hence we can only apply these results to the univariate polynomial case.","This can also be understood as a lesson or even a warning that this approach has been investigated and found to fail, i.e., learning about these results and limitations shall save students and researchers from following old footpaths which lead to a dead end.","We took great care finding the correct historical references where the results appeared first but are perfectly aware that like people before we not always succeed."],"url":"http://arxiv.org/abs/2403.04548v1","category":"math.CA"}
{"created":"2024-03-06 18:47:11","title":"Settling the Competition Complexity of Additive Buyers over Independent Items","abstract":"The competition complexity of an auction setting is the number of additional bidders needed such that the simple mechanism of selling items separately (with additional bidders) achieves greater revenue than the optimal but complex (randomized, prior-dependent, Bayesian-truthful) optimal mechanism without the additional bidders. Our main result settles the competition complexity of $n$ bidders with additive values over $m < n$ independent items at $\\Theta(\\sqrt{nm})$. The $O(\\sqrt{nm})$ upper bound is due to [BW19], and our main result improves the prior lower bound of $\\Omega(\\ln n)$ to $\\Omega(\\sqrt{nm})$.   Our main result follows from an explicit construction of a Bayesian IC auction for $n$ bidders with additive values over $m<n$ independent items drawn from the Equal Revenue curve truncated at $\\sqrt{nm}$ ($\\mathcal{ER}_{\\le \\sqrt{nm}}$), which achieves revenue that exceeds $\\text{SRev}_{n+\\sqrt{nm}}(\\mathcal{ER}_{\\le \\sqrt{nm}}^m)$.   Along the way, we show that the competition complexity of $n$ bidders with additive values over $m$ independent items is exactly equal to the minimum $c$ such that $\\text{SRev}_{n+c}(\\mathcal{ER}_{\\le p}^m) \\geq \\text{Rev}_n(\\mathcal{ER}_{\\le p}^m)$ for all $p$ (that is, some truncated Equal Revenue witnesses the worst-case competition complexity). Interestingly, we also show that the untruncated Equal Revenue curve does not witness the worst-case competition complexity when $n > m$: $\\text{SRev}_n(\\mathcal{ER}^m) = nm+O_m(\\ln (n)) \\leq \\text{SRev}_{n+O_m(\\ln (n))}(\\mathcal{ER}^m)$, and therefore our result can only follow by considering all possible truncations.","sentences":["The competition complexity of an auction setting is the number of additional bidders needed such that the simple mechanism of selling items separately (with additional bidders) achieves greater revenue than the optimal but complex (randomized, prior-dependent, Bayesian-truthful) optimal mechanism without the additional bidders.","Our main result settles the competition complexity of $n$ bidders with additive values over $m < n$ independent items at $\\Theta(\\sqrt{nm})$. The $O(\\sqrt{nm})$ upper bound is due to [BW19], and our main result improves the prior lower bound of $\\Omega(\\ln n)$ to $\\Omega(\\sqrt{nm})$.   Our main result follows from an explicit construction of a Bayesian IC auction for $n$ bidders with additive values over $m<n$ independent items drawn from the Equal Revenue curve truncated at $\\sqrt{nm}$ ($\\mathcal{ER}_{\\le \\sqrt{nm}}$), which achieves revenue that exceeds $\\text{SRev}_{n+\\sqrt{nm}}(\\mathcal{ER}_{\\le \\sqrt{nm}}^m)$.   Along the way, we show that the competition complexity of $n$ bidders with additive values over $m$ independent items is exactly equal to the minimum $c$ such that $\\text{SRev}_{n+c}(\\mathcal{ER}_{\\le p}^m) \\geq","\\text{Rev}_n(\\mathcal{ER}_{\\le p}^m)$ for all $p$ (that is, some truncated Equal Revenue witnesses the worst-case competition complexity).","Interestingly, we also show that the untruncated Equal Revenue curve does not witness the worst-case competition complexity when $n > m$: $\\text{SRev}_n(\\mathcal{ER}^m) = nm+O_m(\\ln (n))","\\leq \\text{SRev}_{n+O_m(\\ln (n))}(\\mathcal{ER}^m)$, and therefore our result can only follow by considering all possible truncations."],"url":"http://arxiv.org/abs/2403.03937v1","category":"cs.GT"}
{"created":"2024-03-06 18:29:17","title":"Multi-parameter quantum estimation of single- and two-mode pure Gaussian states","abstract":"We discuss the ultimate precision bounds on the multiparameter estimation of single- and two-mode pure Gaussian states. By leveraging on previous approaches that focused on the estimation of a complex displacement only, we derive the Holevo Cram\\'er-Rao bound (HCRB) for both displacement and squeezing parameter characterizing single and two-mode squeezed states. In the single-mode scenario, we obtain an analytical bound and find that it degrades monotonically as the squeezing increases. Furthermore, we prove that heterodyne detection is nearly optimal in the large squeezing limit, but in general the optimal measurement must include non-Gaussian resources. On the other hand, in the two-mode setting, the HCRB improves as the squeezing parameter grows and we show that it can be attained using double-homodyne detection.","sentences":["We discuss the ultimate precision bounds on the multiparameter estimation of single- and two-mode pure Gaussian states.","By leveraging on previous approaches that focused on the estimation of a complex displacement only, we derive the Holevo Cram\\'er-Rao bound (HCRB) for both displacement and squeezing parameter characterizing single and two-mode squeezed states.","In the single-mode scenario, we obtain an analytical bound and find that it degrades monotonically as the squeezing increases.","Furthermore, we prove that heterodyne detection is nearly optimal in the large squeezing limit, but in general the optimal measurement must include non-Gaussian resources.","On the other hand, in the two-mode setting, the HCRB improves as the squeezing parameter grows and we show that it can be attained using double-homodyne detection."],"url":"http://arxiv.org/abs/2403.03919v1","category":"quant-ph"}
{"created":"2024-03-06 18:14:36","title":"A Unified Model for Active Battery Equalization Systems","abstract":"Lithium-ion battery packs demand effective active equalization systems to enhance their usable capacity and lifetime. Despite numerous topologies and control schemes proposed in the literature, conducting quantitative analyses, comprehensive comparisons, and systematic optimization of their performance remains challenging due to the absence of a unified mathematical model at the pack level. To address this gap, we introduce a novel, hypergraph-based approach to establish the first unified model for various active battery equalization systems. This model reveals the intrinsic relationship between battery cells and equalizers by representing them as the vertices and hyperedges of hypergraphs, respectively. With the developed model, we identify the necessary condition for all equalization systems to achieve balance through controllability analysis, offering valuable insights for selecting the number of equalizers. Moreover, we prove that the battery equalization time is inversely correlated with the second smallest eigenvalue of the hypergraph's Laplacian matrix of each equalization system. This significantly simplifies the selection and optimized design of equalization systems, obviating the need for extensive experiments or simulations to derive the equalization time. Illustrative results demonstrate the efficiency of the proposed model and validate our findings.","sentences":["Lithium-ion battery packs demand effective active equalization systems to enhance their usable capacity and lifetime.","Despite numerous topologies and control schemes proposed in the literature, conducting quantitative analyses, comprehensive comparisons, and systematic optimization of their performance remains challenging due to the absence of a unified mathematical model at the pack level.","To address this gap, we introduce a novel, hypergraph-based approach to establish the first unified model for various active battery equalization systems.","This model reveals the intrinsic relationship between battery cells and equalizers by representing them as the vertices and hyperedges of hypergraphs, respectively.","With the developed model, we identify the necessary condition for all equalization systems to achieve balance through controllability analysis, offering valuable insights for selecting the number of equalizers.","Moreover, we prove that the battery equalization time is inversely correlated with the second smallest eigenvalue of the hypergraph's Laplacian matrix of each equalization system.","This significantly simplifies the selection and optimized design of equalization systems, obviating the need for extensive experiments or simulations to derive the equalization time.","Illustrative results demonstrate the efficiency of the proposed model and validate our findings."],"url":"http://arxiv.org/abs/2403.03910v1","category":"eess.SY"}
{"created":"2024-03-06 17:23:28","title":"Learning to Decode Collaboratively with Multiple Language Models","abstract":"We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned latent decisions, we show models trained with our method exhibit several interesting collaboration patterns, e.g., template-filling. Our code is available at https://github.com/clinicalml/co-llm.","sentences":["We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level.","We model the decision of which LLM generates the next token as a latent variable.","By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision.","Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand.","Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models.","On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models.","Through qualitative analysis of the learned latent decisions, we show models trained with our method exhibit several interesting collaboration patterns, e.g., template-filling.","Our code is available at https://github.com/clinicalml/co-llm."],"url":"http://arxiv.org/abs/2403.03870v1","category":"cs.CL"}
{"created":"2024-03-06 17:18:24","title":"Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage","abstract":"Conformal prediction builds marginally valid prediction intervals which cover the unknown outcome of a randomly drawn new test point with a prescribed probability. In practice, a common scenario is that, after seeing the test unit(s), practitioners decide which test unit(s) to focus on in a data-driven manner, and wish to quantify the uncertainty for the focal unit(s). In such cases, marginally valid prediction intervals for these focal units can be misleading due to selection bias. This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected. Its general form works for arbitrary selection rules, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We then work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p-values, and selection based on properties of preliminary conformal prediction sets. The performance of our methods is demonstrated via applications in drug discovery and health risk prediction.","sentences":["Conformal prediction builds marginally valid prediction intervals which cover the unknown outcome of a randomly drawn new test point with a prescribed probability.","In practice, a common scenario is that, after seeing the test unit(s), practitioners decide which test unit(s) to focus on in a data-driven manner, and wish to quantify the uncertainty for the focal unit(s).","In such cases, marginally valid prediction intervals for these focal units can be misleading due to selection bias.","This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected.","Its general form works for arbitrary selection rules, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers.","We then work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p-values, and selection based on properties of preliminary conformal prediction sets.","The performance of our methods is demonstrated via applications in drug discovery and health risk prediction."],"url":"http://arxiv.org/abs/2403.03868v1","category":"stat.ME"}
{"created":"2024-03-06 16:48:33","title":"Flexible Optimization for Cyber-Physical and Human Systems","abstract":"Can we allow humans to pick among different, yet reasonably similar, decisions? Are we able to construct optimization problems whose outcome are sets of feasible, close-to-optimal decisions for human users to pick from, instead of a single, hardly explainable, do-as-I-say ``optimal'' directive?   In this paper, we explore two complementary ways to render optimization problems stemming from cyber-physical applications flexible. In doing so, the optimization outcome is a trade off between engineering best and flexibility for the users to decide to do something slightly different. The first method is based on robust optimization and convex reformulations. The second method is stochastic and inspired from stochastic optimization with decision-dependent distributions.","sentences":["Can we allow humans to pick among different, yet reasonably similar, decisions?","Are we able to construct optimization problems whose outcome are sets of feasible, close-to-optimal decisions for human users to pick from, instead of a single, hardly explainable, do-as-I-say ``optimal'' directive?   ","In this paper, we explore two complementary ways to render optimization problems stemming from cyber-physical applications flexible.","In doing so, the optimization outcome is a trade off between engineering best and flexibility for the users to decide to do something slightly different.","The first method is based on robust optimization and convex reformulations.","The second method is stochastic and inspired from stochastic optimization with decision-dependent distributions."],"url":"http://arxiv.org/abs/2403.03847v1","category":"math.OC"}
{"created":"2024-03-06 16:31:56","title":"Feature Selection as Deep Sequential Generative Learning","abstract":"Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores. (2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding;(3) We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop. Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters.","sentences":["Feature selection aims to identify the most pattern-discriminative feature subset.","In prior literature, filter (e.g., backward elimination) and embedded (e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly.","To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences.","Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses.","Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores.","(2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding;(3)","We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop.","Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters."],"url":"http://arxiv.org/abs/2403.03838v1","category":"cs.LG"}
{"created":"2024-03-06 16:03:37","title":"Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters","abstract":"The optimization of a black-box simulator over control parameters $\\mathbf{x}$ arises in a myriad of scientific applications. In such applications, the simulator often takes the form $f(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta}$ are parameters that are uncertain in practice. Robust optimization aims to optimize the objective $\\mathbb{E}[f(\\mathbf{x},\\boldsymbol{\\Theta})]$, where $\\boldsymbol{\\Theta} \\sim \\mathcal{P}$ is a random variable that models uncertainty on $\\boldsymbol{\\theta}$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point $(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\mathbf{x}$ and $\\boldsymbol{\\theta}$ are optimized separately via different acquisition functions. As such, these approaches do not employ a joint acquisition over $(\\mathbf{x},\\boldsymbol{\\theta})$, and thus may fail to fully exploit control-to-noise interactions for effective robust optimization. To address this, we propose a new Bayesian optimization method called Targeted Variance Reduction (TVR). The TVR leverages a novel joint acquisition function over $(\\mathbf{x},\\boldsymbol{\\theta})$, which targets variance reduction on the objective within the desired region of improvement. Under a Gaussian process surrogate on $f$, the TVR acquisition can be evaluated in closed form, and reveals an insightful exploration-exploitation-precision trade-off for robust black-box optimization. The TVR can further accommodate a broad class of non-Gaussian distributions on $\\mathcal{P}$ via a careful integration of normalizing flows. We demonstrate the improved performance of TVR over the state-of-the-art in a suite of numerical experiments and an application to the robust design of automobile brake discs under operational uncertainty.","sentences":["The optimization of a black-box simulator over control parameters $\\mathbf{x}$ arises in a myriad of scientific applications.","In such applications, the simulator often takes the form $f(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta}$ are parameters that are uncertain in practice.","Robust optimization aims to optimize the objective $\\mathbb{E}[f(\\mathbf{x},\\boldsymbol{\\Theta})]$, where $\\boldsymbol{\\Theta} \\sim \\mathcal{P}$ is a random variable that models uncertainty on $\\boldsymbol{\\theta}$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point $(\\mathbf{x},\\boldsymbol{\\theta})$, where $\\mathbf{x}$ and $\\boldsymbol{\\theta}$ are optimized separately via different acquisition functions.","As such, these approaches do not employ a joint acquisition over $(\\mathbf{x},\\boldsymbol{\\theta})$, and thus may fail to fully exploit control-to-noise interactions for effective robust optimization.","To address this, we propose a new Bayesian optimization method called Targeted Variance Reduction (TVR).","The TVR leverages a novel joint acquisition function over $(\\mathbf{x},\\boldsymbol{\\theta})$, which targets variance reduction on the objective within the desired region of improvement.","Under a Gaussian process surrogate on $f$, the TVR acquisition can be evaluated in closed form, and reveals an insightful exploration-exploitation-precision trade-off for robust black-box optimization.","The TVR can further accommodate a broad class of non-Gaussian distributions on $\\mathcal{P}$ via a careful integration of normalizing flows.","We demonstrate the improved performance of TVR over the state-of-the-art in a suite of numerical experiments and an application to the robust design of automobile brake discs under operational uncertainty."],"url":"http://arxiv.org/abs/2403.03816v1","category":"stat.ML"}
{"created":"2024-03-06 16:01:43","title":"Synthesis and Structural Analysis of Multilayered Graphene via Microwave Atmospheric Pressure Plasma","abstract":"This study reports the successful synthesis of multilayered graphene sheets via microwave atmospheric pressure plasma. This innovative approach streamlines and expedites graphene production and other carbon nanostructures, eliminating the need for catalysts, solvents, or complex processing conditions. Ethanol is directly injected into a microwave-generated argon plasma plume, leading to the formation of graphene. Raman spectroscopy revealed characteristic peaks (2D, G, and D bands) confirming graphene composition, with defects indicated by the D band. X-ray diffraction analysis supported these findings, indicating a broad peak at 25 degree corresponding to the (002) plane, affirming a multi-layered graphene structure. Scanning electron microscopy exhibited crumpled, randomly oriented graphene sheets, albeit with uneven structures suggesting impurity incorporation. The presence of defects was quantified through the intensity ratio of the D to G band (ID/IG) in Raman spectroscopy, revealing a value of 0.80, signifying the presence of defects in the synthesized graphene. The 2D to G band intensity ratio (I2D/IG) suggested the existence of 7-10 graphene layers, highlighting the need for further optimization for enhanced graphene quality and purity.","sentences":["This study reports the successful synthesis of multilayered graphene sheets via microwave atmospheric pressure plasma.","This innovative approach streamlines and expedites graphene production and other carbon nanostructures, eliminating the need for catalysts, solvents, or complex processing conditions.","Ethanol is directly injected into a microwave-generated argon plasma plume, leading to the formation of graphene.","Raman spectroscopy revealed characteristic peaks (2D, G, and D bands) confirming graphene composition, with defects indicated by the D band.","X-ray diffraction analysis supported these findings, indicating a broad peak at 25 degree corresponding to the (002) plane, affirming a multi-layered graphene structure.","Scanning electron microscopy exhibited crumpled, randomly oriented graphene sheets, albeit with uneven structures suggesting impurity incorporation.","The presence of defects was quantified through the intensity ratio of the D to G band (ID/IG) in Raman spectroscopy, revealing a value of 0.80, signifying the presence of defects in the synthesized graphene.","The 2D to G band intensity ratio (I2D/IG) suggested the existence of 7-10 graphene layers, highlighting the need for further optimization for enhanced graphene quality and purity."],"url":"http://arxiv.org/abs/2403.03813v1","category":"physics.plasm-ph"}
{"created":"2024-03-06 16:00:46","title":"Incentivized Learning in Principal-Agent Bandit Games","abstract":"This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent. The principal and the agent have misaligned objectives and the choice of action is only left to the agent. However, the principal can influence the agent's decisions by offering incentives which add up to his rewards. The principal aims to iteratively learn an incentive policy to maximize her own total utility. This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem. We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings. Finally, we support our theoretical guarantees through numerical experiments.","sentences":["This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent.","The principal and the agent have misaligned objectives and the choice of action is only left to the agent.","However, the principal can influence the agent's decisions by offering incentives which add up to his rewards.","The principal aims to iteratively learn an incentive policy to maximize her own total utility.","This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem.","We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings.","Finally, we support our theoretical guarantees through numerical experiments."],"url":"http://arxiv.org/abs/2403.03811v1","category":"stat.ML"}
{"created":"2024-03-06 15:54:54","title":"Resonant switching current detector based on underdamped Josephson junctions","abstract":"Current-biased Josephson junctions can act as detectors of electromagnetic radiation. At optimal conditions, their sensitivity is limited by fluctuations causing stochastic switching from the superconducting to the resistive state. This work provides a quantitative description of a stochastic switching current detector, based on an underdamped Josephson junction. It is shown that activation of a Josephson plasma resonance can greatly enhance the detector responsivity in proportion to the quality factor of the junction. The ways of tuning the detector for achieving optimal operation are discussed. For realistic parameters of Nb/AlOx/Nb tunnel junctions, the sensitivity and noise-equivalent power can reach values of $S\\simeq 2\\times10^{13}$ (V/W) and $NEP\\simeq 5\\times10^{-24}$ (WHz$^{-1/2}$), respectively. These outstanding characteristics facilitate both bolometric and single-photon detection in microwave and terahertz ranges.","sentences":["Current-biased Josephson junctions can act as detectors of electromagnetic radiation.","At optimal conditions, their sensitivity is limited by fluctuations causing stochastic switching from the superconducting to the resistive state.","This work provides a quantitative description of a stochastic switching current detector, based on an underdamped Josephson junction.","It is shown that activation of a Josephson plasma resonance can greatly enhance the detector responsivity in proportion to the quality factor of the junction.","The ways of tuning the detector for achieving optimal operation are discussed.","For realistic parameters of Nb/AlOx/Nb tunnel junctions, the sensitivity and noise-equivalent power can reach values of $S\\simeq 2\\times10^{13}$ (V/W) and $NEP\\simeq 5\\times10^{-24}$ (WHz$^{-1/2}$), respectively.","These outstanding characteristics facilitate both bolometric and single-photon detection in microwave and terahertz ranges."],"url":"http://arxiv.org/abs/2403.03803v1","category":"cond-mat.supr-con"}
{"created":"2024-03-06 15:30:41","title":"A machine learning workflow to address credit default prediction","abstract":"Due to the recent increase in interest in Financial Technology (FinTech), applications like credit default prediction (CDP) are gaining significant industrial and academic attention. In this regard, CDP plays a crucial role in assessing the creditworthiness of individuals and businesses, enabling lenders to make informed decisions regarding loan approvals and risk management. In this paper, we propose a workflow-based approach to improve CDP, which refers to the task of assessing the probability that a borrower will default on his or her credit obligations. The workflow consists of multiple steps, each designed to leverage the strengths of different techniques featured in machine learning pipelines and, thus best solve the CDP task. We employ a comprehensive and systematic approach starting with data preprocessing using Weight of Evidence encoding, a technique that ensures in a single-shot data scaling by removing outliers, handling missing values, and making data uniform for models working with different data types. Next, we train several families of learning models, introducing ensemble techniques to build more robust models and hyperparameter optimization via multi-objective genetic algorithms to consider both predictive accuracy and financial aspects. Our research aims at contributing to the FinTech industry in providing a tool to move toward more accurate and reliable credit risk assessment, benefiting both lenders and borrowers.","sentences":["Due to the recent increase in interest in Financial Technology (FinTech), applications like credit default prediction (CDP) are gaining significant industrial and academic attention.","In this regard, CDP plays a crucial role in assessing the creditworthiness of individuals and businesses, enabling lenders to make informed decisions regarding loan approvals and risk management.","In this paper, we propose a workflow-based approach to improve CDP, which refers to the task of assessing the probability that a borrower will default on his or her credit obligations.","The workflow consists of multiple steps, each designed to leverage the strengths of different techniques featured in machine learning pipelines and, thus best solve the CDP task.","We employ a comprehensive and systematic approach starting with data preprocessing using Weight of Evidence encoding, a technique that ensures in a single-shot data scaling by removing outliers, handling missing values, and making data uniform for models working with different data types.","Next, we train several families of learning models, introducing ensemble techniques to build more robust models and hyperparameter optimization via multi-objective genetic algorithms to consider both predictive accuracy and financial aspects.","Our research aims at contributing to the FinTech industry in providing a tool to move toward more accurate and reliable credit risk assessment, benefiting both lenders and borrowers."],"url":"http://arxiv.org/abs/2403.03785v1","category":"cs.CE"}
{"created":"2024-03-06 15:06:16","title":"Verified Training for Counterfactual Explanation Robustness under Data Shift","abstract":"Counterfactual explanations (CEs) enhance the interpretability of machine learning models by describing what changes to an input are necessary to change its prediction to a desired class. These explanations are commonly used to guide users' actions, e.g., by describing how a user whose loan application was denied can be approved for a loan in the future. Existing approaches generate CEs by focusing on a single, fixed model, and do not provide any formal guarantees on the CEs' future validity. When models are updated periodically to account for data shift, if the generated CEs are not robust to the shifts, users' actions may no longer have the desired impacts on their predictions. This paper introduces VeriTraCER, an approach that jointly trains a classifier and an explainer to explicitly consider the robustness of the generated CEs to small model shifts. VeriTraCER optimizes over a carefully designed loss function that ensures the verifiable robustness of CEs to local model updates, thus providing deterministic guarantees to CE validity. Our empirical evaluation demonstrates that VeriTraCER generates CEs that (1) are verifiably robust to small model updates and (2) display competitive robustness to state-of-the-art approaches in handling empirical model updates including random initialization, leave-one-out, and distribution shifts.","sentences":["Counterfactual explanations (CEs) enhance the interpretability of machine learning models by describing what changes to an input are necessary to change its prediction to a desired class.","These explanations are commonly used to guide users' actions, e.g., by describing how a user whose loan application was denied can be approved for a loan in the future.","Existing approaches generate CEs by focusing on a single, fixed model, and do not provide any formal guarantees on the CEs' future validity.","When models are updated periodically to account for data shift, if the generated CEs are not robust to the shifts, users' actions may no longer have the desired impacts on their predictions.","This paper introduces VeriTraCER, an approach that jointly trains a classifier and an explainer to explicitly consider the robustness of the generated CEs to small model shifts.","VeriTraCER optimizes over a carefully designed loss function that ensures the verifiable robustness of CEs to local model updates, thus providing deterministic guarantees to CE validity.","Our empirical evaluation demonstrates that VeriTraCER generates CEs that (1) are verifiably robust to small model updates and (2) display competitive robustness to state-of-the-art approaches in handling empirical model updates including random initialization, leave-one-out, and distribution shifts."],"url":"http://arxiv.org/abs/2403.03773v1","category":"cs.LG"}
{"created":"2024-03-06 15:06:11","title":"AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs","abstract":"Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets. In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees. In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems. In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on benchmark datasets when compared with existing sequential implementations. Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it. This allows us to apply DirectLiNGAM to causal inference on large-scale gene expression data with genetic interventions yielding competitive results compared with specialized continuous optimization methods, and Var-LiNGAM for causal discovery on U.S. stock data.","sentences":["Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets.","In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees.","In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems.","In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on benchmark datasets when compared with existing sequential implementations.","Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it.","This allows us to apply DirectLiNGAM to causal inference on large-scale gene expression data with genetic interventions yielding competitive results compared with specialized continuous optimization methods, and Var-LiNGAM for causal discovery on U.S. stock data."],"url":"http://arxiv.org/abs/2403.03772v1","category":"cs.LG"}
{"created":"2024-03-06 15:01:29","title":"How to find optimal quantum states for optical micromanipulation and metrology in complex scattering problems: tutorial","abstract":"The interaction of quantum light with matter is of great importance to a wide range of scientific disciplines, ranging from optomechanics to high precision measurements. A central issue we discuss here, is how to make optimal use of both the spatial and the quantum degrees of freedom of light for characterizing and manipulating arbitrary observable parameters in a linear scattering system into which suitably engineered light fields are injected. Here, we discuss a comprehensive framework based on a quantum operator that can be assembled solely from the scattering matrix of a system and its dependence on the corresponding local parameter, making this operator experimentally measurable from the far-field using only classical light. From this, the effect of quantum light in the near-field, i.e., in the vicinity of the target object, can be inferred. Based on this framework, it is straightforward to formulate optimal protocols on how to jointly design both the spatial shape and the quantum characteristics of light for micromanipulation as well as for parameter estimation in arbitrarily complex media. Also the forces of the quantum vacuum naturally emerge from this formalism. The aim of our tutorial is to bring different perspectives into alignment and thereby build a bridge between the different communities of wave control, quantum optics, micromanipulation, quantum metrology and vacuum physics.","sentences":["The interaction of quantum light with matter is of great importance to a wide range of scientific disciplines, ranging from optomechanics to high precision measurements.","A central issue we discuss here, is how to make optimal use of both the spatial and the quantum degrees of freedom of light for characterizing and manipulating arbitrary observable parameters in a linear scattering system into which suitably engineered light fields are injected.","Here, we discuss a comprehensive framework based on a quantum operator that can be assembled solely from the scattering matrix of a system and its dependence on the corresponding local parameter, making this operator experimentally measurable from the far-field using only classical light.","From this, the effect of quantum light in the near-field, i.e., in the vicinity of the target object, can be inferred.","Based on this framework, it is straightforward to formulate optimal protocols on how to jointly design both the spatial shape and the quantum characteristics of light for micromanipulation as well as for parameter estimation in arbitrarily complex media.","Also the forces of the quantum vacuum naturally emerge from this formalism.","The aim of our tutorial is to bring different perspectives into alignment and thereby build a bridge between the different communities of wave control, quantum optics, micromanipulation, quantum metrology and vacuum physics."],"url":"http://arxiv.org/abs/2403.03766v1","category":"quant-ph"}
{"created":"2024-03-06 14:57:09","title":"Room Impulse Response Estimation using Optimal Transport: Simulation-Informed Inference","abstract":"The ability to accurately estimate room impulse responses (RIRs) is integral to many applications of spatial audio processing. Regrettably, estimating the RIR using ambient signals, such as speech or music, remains a challenging problem due to, e.g., low signal-to-noise ratios, finite sample lengths, and poor spectral excitation. Commonly, in order to improve the conditioning of the estimation problem, priors are placed on the amplitudes of the RIR. Although serving as a regularizer, this type of prior is generally not useful when only approximate knowledge of the delay structure is available, which, for example, is the case when the prior is a simulated RIR from an approximation of the room geometry. In this work, we target the delay structure itself, constructing a prior based on the concept of optimal transport. As illustrated using both simulated and measured data, the resulting method is able to beneficially incorporate information even from simple simulation models, displaying considerable robustness to perturbations in the assumed room dimensions and its temperature.","sentences":["The ability to accurately estimate room impulse responses (RIRs) is integral to many applications of spatial audio processing.","Regrettably, estimating the RIR using ambient signals, such as speech or music, remains a challenging problem due to, e.g., low signal-to-noise ratios, finite sample lengths, and poor spectral excitation.","Commonly, in order to improve the conditioning of the estimation problem, priors are placed on the amplitudes of the RIR.","Although serving as a regularizer, this type of prior is generally not useful when only approximate knowledge of the delay structure is available, which, for example, is the case when the prior is a simulated RIR from an approximation of the room geometry.","In this work, we target the delay structure itself, constructing a prior based on the concept of optimal transport.","As illustrated using both simulated and measured data, the resulting method is able to beneficially incorporate information even from simple simulation models, displaying considerable robustness to perturbations in the assumed room dimensions and its temperature."],"url":"http://arxiv.org/abs/2403.03762v1","category":"eess.SP"}
{"created":"2024-03-06 14:53:24","title":"Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations","abstract":"Quantum comb is an essential tool for characterizing complex quantum protocols in quantum information processing. In this work, we introduce PQComb, a framework leveraging parameterized quantum circuits to explore the capabilities of quantum combs for general quantum process transformation tasks and beyond. By optimizing PQComb for time-reversal simulations of unknown unitary evolutions, we develop a simpler protocol for unknown qubit unitary inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This demonstrates the utility of quantum comb structures and showcases PQComb's potential for solving complex quantum tasks. Our results pave the way for broader PQComb applications in quantum computing and quantum information, emphasizing its versatility for tackling diverse problems in quantum machine learning.","sentences":["Quantum comb is an essential tool for characterizing complex quantum protocols in quantum information processing.","In this work, we introduce PQComb, a framework leveraging parameterized quantum circuits to explore the capabilities of quantum combs for general quantum process transformation tasks and beyond.","By optimizing PQComb for time-reversal simulations of unknown unitary evolutions, we develop a simpler protocol for unknown qubit unitary inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023].","This demonstrates the utility of quantum comb structures and showcases PQComb's potential for solving complex quantum tasks.","Our results pave the way for broader PQComb applications in quantum computing and quantum information, emphasizing its versatility for tackling diverse problems in quantum machine learning."],"url":"http://arxiv.org/abs/2403.03761v1","category":"quant-ph"}
{"created":"2024-03-06 14:42:40","title":"Maximizing Energy Charging for UAV-assisted MEC Systems with SWIPT","abstract":"A Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) scheme with simultaneous wireless information and power transfer (SWIPT) is proposed in this paper. Unlike existing MEC-WPT schemes that disregard the downlink period for returning computing results to the ground equipment (GEs), our proposed scheme actively considers and capitalizes on this period. By leveraging the SWIPT technique, the UAV can simultaneously transmit energy and the computing results during the downlink period. In this scheme, our objective is to maximize the remaining energy among all GEs by jointly optimizing computing task scheduling, UAV transmit and receive beamforming, BS receive beamforming, GEs' transmit power and power splitting ratio for information decoding, time scheduling, and UAV trajectory. We propose an alternating optimization algorithm that utilizes the semidefinite relaxation (SDR), singular value decomposition (SVD), and fractional programming (FP) methods to effectively solve the nonconvex problem. Numerous experiments validate the effectiveness of the proposed scheme.","sentences":["A Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) scheme with simultaneous wireless information and power transfer (SWIPT) is proposed in this paper.","Unlike existing MEC-WPT schemes that disregard the downlink period for returning computing results to the ground equipment (GEs), our proposed scheme actively considers and capitalizes on this period.","By leveraging the SWIPT technique, the UAV can simultaneously transmit energy and the computing results during the downlink period.","In this scheme, our objective is to maximize the remaining energy among all GEs by jointly optimizing computing task scheduling, UAV transmit and receive beamforming, BS receive beamforming, GEs' transmit power and power splitting ratio for information decoding, time scheduling, and UAV trajectory.","We propose an alternating optimization algorithm that utilizes the semidefinite relaxation (SDR), singular value decomposition (SVD), and fractional programming (FP) methods to effectively solve the nonconvex problem.","Numerous experiments validate the effectiveness of the proposed scheme."],"url":"http://arxiv.org/abs/2403.03756v1","category":"eess.SP"}
{"created":"2024-03-06 14:16:46","title":"Robust MITL planning under uncertain navigation times","abstract":"In environments like offices, the duration of a robot's navigation between two locations may vary over time. For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) - a rich robot task specification language that allows us to capture explicit time requirements. Our objective is to find a strategy that maximizes the temporal robustness of the robot's MITL task. As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day. Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness. We show the scalability of our planning algorithms in simulations of robotic tasks.","sentences":["In environments like offices, the duration of a robot's navigation between two locations may vary over time.","For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way.","In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) - a rich robot task specification language that allows us to capture explicit time requirements.","Our objective is to find a strategy that maximizes the temporal robustness of the robot's MITL task.","As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day.","Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known.","Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness.","We show the scalability of our planning algorithms in simulations of robotic tasks."],"url":"http://arxiv.org/abs/2403.03727v1","category":"cs.RO"}
{"created":"2024-03-06 14:15:18","title":"To Trust or Not to Trust: Assignment Mechanisms with Predictions in the Private Graph Model","abstract":"The realm of algorithms with predictions has led to the development of several new algorithms that leverage (potentially erroneous) predictions to enhance their performance guarantees. The challenge is to devise algorithms that achieve optimal approximation guarantees as the prediction quality varies from perfect (consistency) to imperfect (robustness). This framework is particularly appealing in mechanism design contexts, where predictions might convey private information about the agents. In this paper, we design strategyproof mechanisms that leverage predictions to achieve improved approximation guarantees for several variants of the Generalized Assignment Problem (GAP) in the private graph model. In this model, first introduced by Dughmi & Ghosh (2010), the set of resources that an agent is compatible with is private information. For the Bipartite Matching Problem (BMP), we give a deterministic group-strategyproof (GSP) mechanism that is $(1 +1/\\gamma)$-consistent and $(1 + \\gamma)$-robust, where $\\gamma \\ge 1$ is some confidence parameter. We also prove that this is best possible. Remarkably, our mechanism draws inspiration from the renowned Gale-Shapley algorithm, incorporating predictions as a crucial element. Additionally, we give a randomized mechanism that is universally GSP and improves on the guarantees in expectation. The other GAP variants that we consider all make use of a unified greedy mechanism that adds edges to the assignment according to a specific order. Our universally GSP mechanism randomizes over the greedy mechanism, our mechanism for BMP and the predicted assignment, leading to $(1+3/\\gamma)$-consistency and $(3+\\gamma)$-robustness in expectation. All our mechanisms also provide more fine-grained approximation guarantees that interpolate between the consistency and the robustness, depending on some natural error measure of the prediction.","sentences":["The realm of algorithms with predictions has led to the development of several new algorithms that leverage (potentially erroneous) predictions to enhance their performance guarantees.","The challenge is to devise algorithms that achieve optimal approximation guarantees as the prediction quality varies from perfect (consistency) to imperfect (robustness).","This framework is particularly appealing in mechanism design contexts, where predictions might convey private information about the agents.","In this paper, we design strategyproof mechanisms that leverage predictions to achieve improved approximation guarantees for several variants of the Generalized Assignment Problem (GAP) in the private graph model.","In this model, first introduced by Dughmi & Ghosh (2010), the set of resources that an agent is compatible with is private information.","For the Bipartite Matching Problem (BMP), we give a deterministic group-strategyproof (GSP) mechanism that is $(1 +1/\\gamma)$-consistent and $(1 + \\gamma)$-robust, where $\\gamma \\ge 1$ is some confidence parameter.","We also prove that this is best possible.","Remarkably, our mechanism draws inspiration from the renowned Gale-Shapley algorithm, incorporating predictions as a crucial element.","Additionally, we give a randomized mechanism that is universally GSP and improves on the guarantees in expectation.","The other GAP variants that we consider all make use of a unified greedy mechanism that adds edges to the assignment according to a specific order.","Our universally GSP mechanism randomizes over the greedy mechanism, our mechanism for BMP and the predicted assignment, leading to $(1+3/\\gamma)$-consistency and $(3+\\gamma)$-robustness in expectation.","All our mechanisms also provide more fine-grained approximation guarantees that interpolate between the consistency and the robustness, depending on some natural error measure of the prediction."],"url":"http://arxiv.org/abs/2403.03725v1","category":"cs.GT"}
{"created":"2024-03-06 14:15:01","title":"In the Search of Optimal Tree Networks: Hardness and Heuristics","abstract":"Demand-aware communication networks are networks whose topology is optimized toward the traffic they need to serve. These networks have recently been enabled by novel optical communication technologies and are investigated intensively in the context of datacenters. In this work, we consider networks with one of the most common topologies~ -- a binary tree.   We show that finding an optimal demand-aware binary tree network is NP-hard. Then, we propose optimization algorithms that generate efficient binary tree networks on real-life and synthetic workloads.","sentences":["Demand-aware communication networks are networks whose topology is optimized toward the traffic they need to serve.","These networks have recently been enabled by novel optical communication technologies and are investigated intensively in the context of datacenters.","In this work, we consider networks with one of the most common topologies~ -- a binary tree.   ","We show that finding an optimal demand-aware binary tree network is NP-hard.","Then, we propose optimization algorithms that generate efficient binary tree networks on real-life and synthetic workloads."],"url":"http://arxiv.org/abs/2403.03724v1","category":"cs.NI"}
{"created":"2024-03-06 13:12:21","title":"A methodology for the cross-dock door platforms design under uncertainty","abstract":"The cross dock door design problem consists of deciding on the number and capacity of inbound and outbound doors for receiving product pallets from origin nodes and exiting them to destination nodes. The uncertainty, realized in scenarios, lies in the occurrence of these nodes, the number and cost of the pallets, and the disruption of the capacity of the doors. It is represented using a stochastic two stage binary quadratic model. The first stage decisions are related to the cross dock infrastructure design, and the second stage decisions are related to the node to assignments of the doors. This is the first time, as far as we know, that a stochastic two stage binary quadratic model has been presented for minimizing the construction cost of the infrastructure and its exploitation expected cost in the scenarios. Given the difficulty of solving this combinatorial problem, a mathematically equivalent mixed integer linear formulation is introduced. However, searching an optimal solution is still impractical for commercial solvers. Thus, a scenario cluster decomposition based matheuristic algorithm is introduced to obtain feasible solutions with small optimality gap and reasonable computational effort. A broad study to validate the proposal gives solutions with a much smaller gap than the ones provided by a state of the art general solver. In fact, the proposal provides solutions with a 1 to 5% optimality gap, while the solver does it with up to a 12% gap, if any, and requires a wall time two orders of magnitude higher.","sentences":["The cross dock door design problem consists of deciding on the number and capacity of inbound and outbound doors for receiving product pallets from origin nodes and exiting them to destination nodes.","The uncertainty, realized in scenarios, lies in the occurrence of these nodes, the number and cost of the pallets, and the disruption of the capacity of the doors.","It is represented using a stochastic two stage binary quadratic model.","The first stage decisions are related to the cross dock infrastructure design, and the second stage decisions are related to the node to assignments of the doors.","This is the first time, as far as we know, that a stochastic two stage binary quadratic model has been presented for minimizing the construction cost of the infrastructure and its exploitation expected cost in the scenarios.","Given the difficulty of solving this combinatorial problem, a mathematically equivalent mixed integer linear formulation is introduced.","However, searching an optimal solution is still impractical for commercial solvers.","Thus, a scenario cluster decomposition based matheuristic algorithm is introduced to obtain feasible solutions with small optimality gap and reasonable computational effort.","A broad study to validate the proposal gives solutions with a much smaller gap than the ones provided by a state of the art general solver.","In fact, the proposal provides solutions with a 1 to 5% optimality gap, while the solver does it with up to a 12% gap, if any, and requires a wall time two orders of magnitude higher."],"url":"http://arxiv.org/abs/2403.03686v1","category":"math.OC"}
{"created":"2024-03-06 12:58:25","title":"Automatic Bi-modal Question Title Generation for Stack Overflow with Prompt Learning","abstract":"When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.","sentences":["When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help.","Therefore, improving the quality of question titles has attracted the wide attention of researchers.","An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body.","However, this study ignored the helpful information in their corresponding problem descriptions.","Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body.","Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks.","Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles.","Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model.","To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts).","To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow.","Our corpus includes 179,119 high-quality question posts for six popular programming languages.","Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation.","Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction."],"url":"http://arxiv.org/abs/2403.03677v1","category":"cs.SE"}
{"created":"2024-03-06 12:55:21","title":"Adversarial Infrared Geometry: Using Geometry to Perform Adversarial Attack against Infrared Pedestrian Detectors","abstract":"Currently, infrared imaging technology enjoys widespread usage, with infrared object detection technology experiencing a surge in prominence. While previous studies have delved into physical attacks on infrared object detectors, the implementation of these techniques remains complex. For instance, some approaches entail the use of bulb boards or infrared QR suits as perturbations to execute attacks, which entail costly optimization and cumbersome deployment processes. Other methodologies involve the utilization of irregular aerogel as physical perturbations for infrared attacks, albeit at the expense of optimization expenses and perceptibility issues. In this study, we propose a novel infrared physical attack termed Adversarial Infrared Geometry (\\textbf{AdvIG}), which facilitates efficient black-box query attacks by modeling diverse geometric shapes (lines, triangles, ellipses) and optimizing their physical parameters using Particle Swarm Optimization (PSO). Extensive experiments are conducted to evaluate the effectiveness, stealthiness, and robustness of AdvIG. In digital attack experiments, line, triangle, and ellipse patterns achieve attack success rates of 93.1\\%, 86.8\\%, and 100.0\\%, respectively, with average query times of 71.7, 113.1, and 2.57, respectively, thereby confirming the efficiency of AdvIG. Physical attack experiments are conducted to assess the attack success rate of AdvIG at different distances. On average, the line, triangle, and ellipse achieve attack success rates of 61.1\\%, 61.2\\%, and 96.2\\%, respectively. Further experiments are conducted to comprehensively analyze AdvIG, including ablation experiments, transfer attack experiments, and adversarial defense mechanisms. Given the superior performance of our method as a simple and efficient black-box adversarial attack in both digital and physical environments, we advocate for widespread attention to AdvIG.","sentences":["Currently, infrared imaging technology enjoys widespread usage, with infrared object detection technology experiencing a surge in prominence.","While previous studies have delved into physical attacks on infrared object detectors, the implementation of these techniques remains complex.","For instance, some approaches entail the use of bulb boards or infrared QR suits as perturbations to execute attacks, which entail costly optimization and cumbersome deployment processes.","Other methodologies involve the utilization of irregular aerogel as physical perturbations for infrared attacks, albeit at the expense of optimization expenses and perceptibility issues.","In this study, we propose a novel infrared physical attack termed Adversarial Infrared Geometry (\\textbf{AdvIG}), which facilitates efficient black-box query attacks by modeling diverse geometric shapes (lines, triangles, ellipses) and optimizing their physical parameters using Particle Swarm Optimization (PSO).","Extensive experiments are conducted to evaluate the effectiveness, stealthiness, and robustness of AdvIG.","In digital attack experiments, line, triangle, and ellipse patterns achieve attack success rates of 93.1\\%, 86.8\\%, and 100.0\\%, respectively, with average query times of 71.7, 113.1, and 2.57, respectively, thereby confirming the efficiency of AdvIG.","Physical attack experiments are conducted to assess the attack success rate of AdvIG at different distances.","On average, the line, triangle, and ellipse achieve attack success rates of 61.1\\%, 61.2\\%, and 96.2\\%, respectively.","Further experiments are conducted to comprehensively analyze AdvIG, including ablation experiments, transfer attack experiments, and adversarial defense mechanisms.","Given the superior performance of our method as a simple and efficient black-box adversarial attack in both digital and physical environments, we advocate for widespread attention to AdvIG."],"url":"http://arxiv.org/abs/2403.03674v1","category":"cs.CV"}
{"created":"2024-03-06 12:43:53","title":"Spectral Algorithms on Manifolds through Diffusion","abstract":"The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space. Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space. We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces. Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives. These bounds offer two significant advantages: firstly, they are exclusively contingent on the intrinsic dimension of the input manifolds, thereby providing a more focused analysis. Secondly, they enable the efficient derivation of convergence rates for derivatives of any k-th order, all of which can be accomplished within the ambit of the same spectral algorithms. Furthermore, we establish minimax lower bounds to demonstrate the asymptotic optimality of these conclusions in specific contexts. Our study confirms that the spectral algorithms are practically significant in the broader context of high-dimensional approximation.","sentences":["The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space.","Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space.","We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces.","Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives.","These bounds offer two significant advantages: firstly, they are exclusively contingent on the intrinsic dimension of the input manifolds, thereby providing a more focused analysis.","Secondly, they enable the efficient derivation of convergence rates for derivatives of any k-th order, all of which can be accomplished within the ambit of the same spectral algorithms.","Furthermore, we establish minimax lower bounds to demonstrate the asymptotic optimality of these conclusions in specific contexts.","Our study confirms that the spectral algorithms are practically significant in the broader context of high-dimensional approximation."],"url":"http://arxiv.org/abs/2403.03669v2","category":"stat.ML"}
{"created":"2024-03-06 12:33:15","title":"Robust Safety-Critical Control for Systems with Sporadic Measurements and Dwell Time Constraints","abstract":"This paper presents extensions of control barrier function (CBF) theory to systems with disturbances wherein a controller only receives measurements infrequently and operates open-loop between measurements, while still satisfying state constraints. The paper considers both impulsive and continuous actuators, and models the actuators, measurements, disturbances, and timing constraints as a hybrid dynamical system. We then design an open-loop observer that bounds the worst-case uncertainty between measurements. We develop definitions of CBFs for both actuation cases, and corresponding conditions on the control input to guarantee satisfaction of the state constraints. We apply these conditions to simulations of a satellite rendezvous in an elliptical orbit and autonomous orbit stationkeeping.","sentences":["This paper presents extensions of control barrier function (CBF) theory to systems with disturbances wherein a controller only receives measurements infrequently and operates open-loop between measurements, while still satisfying state constraints.","The paper considers both impulsive and continuous actuators, and models the actuators, measurements, disturbances, and timing constraints as a hybrid dynamical system.","We then design an open-loop observer that bounds the worst-case uncertainty between measurements.","We develop definitions of CBFs for both actuation cases, and corresponding conditions on the control input to guarantee satisfaction of the state constraints.","We apply these conditions to simulations of a satellite rendezvous in an elliptical orbit and autonomous orbit stationkeeping."],"url":"http://arxiv.org/abs/2403.03663v1","category":"eess.SY"}
{"created":"2024-03-06 12:28:14","title":"Finite elements for Mat\u00e9rn-type random fields: Uncertainty in computational mechanics and design optimization","abstract":"This work highlights an approach for incorporating realistic uncertainties into scientific computing workflows based on finite elements, focusing on applications in computational mechanics and design optimization. We leverage Mat\\'ern-type Gaussian random fields (GRFs) generated using the SPDE method to model aleatoric uncertainties, including environmental influences, variating material properties, and geometric ambiguities. Our focus lies on delivering practical GRF realizations that accurately capture imperfections and variations and understanding how they impact the predictions of computational models and the topology of optimized designs. We describe a numerical algorithm based on solving a generalized SPDE to sample GRFs on arbitrary meshed domains. The algorithm leverages established techniques and integrates seamlessly with the open-source finite element library MFEM and associated scientific computing workflows, like those found in industrial and national laboratory settings. Our solver scales efficiently for large-scale problems and supports various domain types, including surfaces and embedded manifolds. We showcase its versatility through biomechanics and topology optimization applications. The flexibility and efficiency of SPDE-based GRF generation empower us to run large-scale optimization problems on 2D and 3D domains, including finding optimized designs on embedded surfaces, and to generate topologies beyond the reach of conventional techniques. Moreover, these capabilities allow us to model geometric uncertainties of reconstructed submanifolds, such as the surfaces of cerebral aneurysms. In addition to offering benefits in these specific domains, the proposed techniques transcend specific applications and generalize to arbitrary forward and backward problems in uncertainty quantification involving finite elements.","sentences":["This work highlights an approach for incorporating realistic uncertainties into scientific computing workflows based on finite elements, focusing on applications in computational mechanics and design optimization.","We leverage Mat\\'ern-type Gaussian random fields (GRFs) generated using the SPDE method to model aleatoric uncertainties, including environmental influences, variating material properties, and geometric ambiguities.","Our focus lies on delivering practical GRF realizations that accurately capture imperfections and variations and understanding how they impact the predictions of computational models and the topology of optimized designs.","We describe a numerical algorithm based on solving a generalized SPDE to sample GRFs on arbitrary meshed domains.","The algorithm leverages established techniques and integrates seamlessly with the open-source finite element library MFEM and associated scientific computing workflows, like those found in industrial and national laboratory settings.","Our solver scales efficiently for large-scale problems and supports various domain types, including surfaces and embedded manifolds.","We showcase its versatility through biomechanics and topology optimization applications.","The flexibility and efficiency of SPDE-based GRF generation empower us to run large-scale optimization problems on 2D and 3D domains, including finding optimized designs on embedded surfaces, and to generate topologies beyond the reach of conventional techniques.","Moreover, these capabilities allow us to model geometric uncertainties of reconstructed submanifolds, such as the surfaces of cerebral aneurysms.","In addition to offering benefits in these specific domains, the proposed techniques transcend specific applications and generalize to arbitrary forward and backward problems in uncertainty quantification involving finite elements."],"url":"http://arxiv.org/abs/2403.03658v1","category":"cs.CE"}
{"created":"2024-03-06 12:26:04","title":"Kronos: A Robust Sharding Blockchain Consensus with Optimal Communication Overhead","abstract":"Sharding enhances blockchain scalability by dividing the network into shards, each managing specific unspent transaction outputs or accounts. Cross-shard transactions pose a critical challenge to the security and efficiency of sharding blockchains. Current solutions, however, either prioritize security with assumptions and substantial investments, or focus on reducing overhead and overlooking security considerations.   In this paper, we present Kronos, a generic and efficient sharding blockchain consensus ensuring robust security. We introduce a buffer mechanism for atomic cross-shard transaction processing. Shard members collectively maintain a buffer to manage cross-shard inputs, ensuring that a transaction is committed only if all inputs are available, and no fund is transferred for invalid requests. While ensuring security, Kronos processes transactions with optimal intra-shard communication overhead. Additionally, we propose a reduction for transaction invalidity proof generation to simple and fast multicasting, leading to atomic rejection without executing full-fledged Byzantine fault tolerance protocol in optimistic scenarios. Moreover, Kronos adopts a newly designed batch mechanism, reducing inter-shard message complexity to $O((m$log$m/b)\\lambda)$.   Kronos operates without dependence on any time or client honesty assumption, serving as a plug-in sharding blockchain consensus supporting applications in diverse network environments including asynchronous ones. We implement Kronos using two prominent BFT protocols: Speeding Dumbo and HotStuff. Extensive experiments demonstrate Kronos achieving a substantial throughput of 68.6ktx/sec with 1.7sec latency. Compared with state-of-the-art solutions, Kronos outperforms in all cases, achieving up to a 42x improvement in throughput and a 50% reduction in latency when cross-shard transactions dominate the workload.","sentences":["Sharding enhances blockchain scalability by dividing the network into shards, each managing specific unspent transaction outputs or accounts.","Cross-shard transactions pose a critical challenge to the security and efficiency of sharding blockchains.","Current solutions, however, either prioritize security with assumptions and substantial investments, or focus on reducing overhead and overlooking security considerations.   ","In this paper, we present Kronos, a generic and efficient sharding blockchain consensus ensuring robust security.","We introduce a buffer mechanism for atomic cross-shard transaction processing.","Shard members collectively maintain a buffer to manage cross-shard inputs, ensuring that a transaction is committed only if all inputs are available, and no fund is transferred for invalid requests.","While ensuring security, Kronos processes transactions with optimal intra-shard communication overhead.","Additionally, we propose a reduction for transaction invalidity proof generation to simple and fast multicasting, leading to atomic rejection without executing full-fledged Byzantine fault tolerance protocol in optimistic scenarios.","Moreover, Kronos adopts a newly designed batch mechanism, reducing inter-shard message complexity to $O((m$log$m/b)\\lambda)$.   Kronos operates without dependence on any time or client honesty assumption, serving as a plug-in sharding blockchain consensus supporting applications in diverse network environments including asynchronous ones.","We implement Kronos using two prominent BFT protocols: Speeding Dumbo and HotStuff.","Extensive experiments demonstrate Kronos achieving a substantial throughput of 68.6ktx/sec with 1.7sec latency.","Compared with state-of-the-art solutions, Kronos outperforms in all cases, achieving up to a 42x improvement in throughput and a 50% reduction in latency when cross-shard transactions dominate the workload."],"url":"http://arxiv.org/abs/2403.03655v1","category":"cs.CR"}
{"created":"2024-03-06 12:24:05","title":"Actuation manifold from snapshot data","abstract":"We propose a data-driven methodology to learn a low-dimensional actuation manifold of controlled flows. The starting point is resolving snapshot flow data for a representative ensemble of actuations. Key enablers for the actuation manifold are isometric mapping as encoder and k-nearest neighbour regression as a decoder. This methodology is tested for the fluidic pinball, a cluster of three parallel cylinders perpendicular to the oncoming uniform flow. The centers of these cylinders are the vertices of an equilateral triangle pointing upstream. The flow is manipulated by constant rotation of the cylinders, i.e. described by three actuation parameters. The Reynolds number based on a cylinder diameter is chosen to be 30. The unforced flow yields statistically symmetric unforced periodic shedding represented by a one-dimensional limit cycle. The proposed methodology yields a five-dimensional manifold describing a wide range of dynamics with small representation error. Interestingly, the manifold coordinates automatically unveil physically meaningful parameters. Two of them describe the downstream periodic vortex shedding. The other three ones describe the near-field actuation, i.e. the strength of boat-tailing, the Magnus effect and forward stagnation point. The manifold is shown to be a key enabler for control-oriented flow estimation.","sentences":["We propose a data-driven methodology to learn a low-dimensional actuation manifold of controlled flows.","The starting point is resolving snapshot flow data for a representative ensemble of actuations.","Key enablers for the actuation manifold are isometric mapping as encoder and k-nearest neighbour regression as a decoder.","This methodology is tested for the fluidic pinball, a cluster of three parallel cylinders perpendicular to the oncoming uniform flow.","The centers of these cylinders are the vertices of an equilateral triangle pointing upstream.","The flow is manipulated by constant rotation of the cylinders, i.e. described by three actuation parameters.","The Reynolds number based on a cylinder diameter is chosen to be 30.","The unforced flow yields statistically symmetric unforced periodic shedding represented by a one-dimensional limit cycle.","The proposed methodology yields a five-dimensional manifold describing a wide range of dynamics with small representation error.","Interestingly, the manifold coordinates automatically unveil physically meaningful parameters.","Two of them describe the downstream periodic vortex shedding.","The other three ones describe the near-field actuation, i.e. the strength of boat-tailing, the Magnus effect and forward stagnation point.","The manifold is shown to be a key enabler for control-oriented flow estimation."],"url":"http://arxiv.org/abs/2403.03653v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 12:05:56","title":"A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation","abstract":"The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefore, this paper aims to summarize and review recent theoretical methods and applied research utilizing reinforcement learning to address spatial resource allocation problems. It provides a summary and comprehensive overview of its fundamental principles, related methodologies, and applied research. Additionally, it highlights several unresolved issues that urgently require attention in this direction for the future.","sentences":["The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life.","As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities.","In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities.","Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems.","These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems.","Therefore, this paper aims to summarize and review recent theoretical methods and applied research utilizing reinforcement learning to address spatial resource allocation problems.","It provides a summary and comprehensive overview of its fundamental principles, related methodologies, and applied research.","Additionally, it highlights several unresolved issues that urgently require attention in this direction for the future."],"url":"http://arxiv.org/abs/2403.03643v2","category":"cs.LG"}
{"created":"2024-03-06 11:52:33","title":"Efficient Search and Learning for Agile Locomotion on Stepping Stones","abstract":"Legged robots have become capable of performing highly dynamic maneuvers in the past few years. However, agile locomotion in highly constrained environments such as stepping stones is still a challenge. In this paper, we propose a combination of model-based control, search, and learning to design efficient control policies for agile locomotion on stepping stones. In our framework, we use nonlinear model predictive control (NMPC) to generate whole-body motions for a given contact plan. To efficiently search for an optimal contact plan, we propose to use Monte Carlo tree search (MCTS). While the combination of MCTS and NMPC can quickly find a feasible plan for a given environment (a few seconds), it is not yet suitable to be used as a reactive policy. Hence, we generate a dataset for optimal goal-conditioned policy for a given scene and learn it through supervised learning. In particular, we leverage the power of diffusion models in handling multi-modality in the dataset. We test our proposed framework on a scenario where our quadruped robot Solo12 successfully jumps to different goals in a highly constrained environment.","sentences":["Legged robots have become capable of performing highly dynamic maneuvers in the past few years.","However, agile locomotion in highly constrained environments such as stepping stones is still a challenge.","In this paper, we propose a combination of model-based control, search, and learning to design efficient control policies for agile locomotion on stepping stones.","In our framework, we use nonlinear model predictive control (NMPC) to generate whole-body motions for a given contact plan.","To efficiently search for an optimal contact plan, we propose to use Monte Carlo tree search (MCTS).","While the combination of MCTS and NMPC can quickly find a feasible plan for a given environment (a few seconds), it is not yet suitable to be used as a reactive policy.","Hence, we generate a dataset for optimal goal-conditioned policy for a given scene and learn it through supervised learning.","In particular, we leverage the power of diffusion models in handling multi-modality in the dataset.","We test our proposed framework on a scenario where our quadruped robot Solo12 successfully jumps to different goals in a highly constrained environment."],"url":"http://arxiv.org/abs/2403.03639v1","category":"cs.RO"}
{"created":"2024-03-06 11:51:52","title":"Exact objectives of random linear programs and mean widths of random polyhedrons","abstract":"We consider \\emph{random linear programs} (rlps) as a subclass of \\emph{random optimization problems} (rops) and study their typical behavior. Our particular focus is on appropriate linear objectives which connect the rlps to the mean widths of random polyhedrons/polytopes. Utilizing the powerful machinery of \\emph{random duality theory} (RDT) \\cite{StojnicRegRndDlt10}, we obtain, in a large dimensional context, the exact characterizations of the program's objectives. In particular, for any $\\alpha=\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\in(0,\\infty)$, any unit vector $\\mathbf{c}\\in{\\mathbb R}^n$, any fixed $\\mathbf{a}\\in{\\mathbb R}^n$, and $A\\in {\\mathbb R}^{m\\times n}$ with iid standard normal entries, we have   \\begin{eqnarray*}   \\lim_{n\\rightarrow\\infty}{\\mathbb P}_{A} \\left ( (1-\\epsilon) \\xi_{opt}(\\alpha;\\mathbf{a})   \\leq \\min_{A\\mathbf{x}\\leq \\mathbf{a}}\\mathbf{c}^T\\mathbf{x} \\leq (1+\\epsilon) \\xi_{opt}(\\alpha;\\mathbf{a}) \\right ) \\longrightarrow 1, \\end{eqnarray*}   where   \\begin{equation*} \\xi_{opt}(\\alpha;\\mathbf{a}) \\triangleq \\min_{x>0} \\sqrt{x^2- x^2 \\lim_{n\\rightarrow\\infty} \\frac{\\sum_{i=1}^{m} \\left ( \\frac{1}{2} \\left (\\left ( \\frac{\\mathbf{a}_i}{x}\\right )^2 + 1\\right ) \\mbox{erfc}\\left( \\frac{\\mathbf{a}_i}{x\\sqrt{2}}\\right ) - \\frac{\\mathbf{a}_i}{x} \\frac{e^{-\\frac{\\mathbf{a}_i^2}{2x^2}}}{\\sqrt{2\\pi}} \\right )   }{n} }. \\end{equation*}   For example, for $\\mathbf{a}=\\mathbf{1}$, one uncovers   \\begin{equation*}   \\xi_{opt}(\\alpha)   =   \\min_{x>0} \\sqrt{x^2- x^2 \\alpha \\left ( \\frac{1}{2} \\left ( \\frac{1}{x^2} + 1\\right ) \\mbox{erfc} \\left ( \\frac{1}{x\\sqrt{2}}\\right ) - \\frac{1}{x} \\frac{e^{-\\frac{1}{2x^2}}}{\\sqrt{2\\pi}} \\right ) }. \\end{equation*}   Moreover, $2 \\xi_{opt}(\\alpha)$ is precisely the concentrating point of the mean width of the polyhedron $\\{\\mathbf{x}|A\\mathbf{x} \\leq \\mathbf{1}\\}$.","sentences":["We consider \\emph{random linear programs} (rlps) as a subclass of \\emph{random optimization problems} (rops) and study their typical behavior.","Our particular focus is on appropriate linear objectives which connect the rlps to the mean widths of random polyhedrons/polytopes.","Utilizing the powerful machinery of \\emph{random duality theory} (RDT) \\cite{StojnicRegRndDlt10}, we obtain, in a large dimensional context, the exact characterizations of the program's objectives.","In particular, for any $\\alpha=\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\in(0,\\infty)$, any unit vector $\\mathbf{c}\\in{\\mathbb R}^n$, any fixed $\\mathbf{a}\\in{\\mathbb R}^n$, and $A\\in {\\mathbb R}^{m\\times n}$ with iid standard normal entries, we have   \\begin{eqnarray*}   \\lim_{n\\rightarrow\\infty}{\\mathbb P}_{A} \\left ( (1-\\epsilon) \\xi_{opt}(\\alpha;\\mathbf{a})   ","\\leq \\min_{A\\mathbf{x}\\leq \\mathbf{a}}\\mathbf{c}^T\\mathbf{x} \\leq (1+\\epsilon) \\xi_{opt}(\\alpha;\\mathbf{a}) \\right )","\\longrightarrow 1, \\end{eqnarray*}   where   \\begin{equation*} \\xi_{opt}(\\alpha;\\mathbf{a})","\\triangleq \\min_{x>0} \\sqrt{x^2- x^2 \\lim_{n\\rightarrow\\infty} \\frac{\\sum_{i=1}^{m} \\left ( \\frac{1}{2} \\left (\\left ( \\frac{\\mathbf{a}_i}{x}\\right )^2","+ 1\\right ) \\mbox{erfc}\\left( \\frac{\\mathbf{a}_i}{x\\sqrt{2}}\\right ) - \\frac{\\mathbf{a}_i}{x} \\frac{e^{-\\frac{\\mathbf{a}_i^2}{2x^2}}}{\\sqrt{2\\pi}} \\right )   }{n} }.","\\end{equation*}   For example, for $\\mathbf{a}=\\mathbf{1}$, one uncovers   \\begin{equation*}   \\xi_{opt}(\\alpha)   =   \\min_{x>0} \\sqrt{x^2- x^2 \\alpha \\left ( \\frac{1}{2} \\left ( \\frac{1}{x^2} + 1\\right ) \\mbox{erfc} \\left ( \\frac{1}{x\\sqrt{2}}\\right ) - \\frac{1}{x} \\frac{e^{-\\frac{1}{2x^2}}}{\\sqrt{2\\pi}} \\right ) }.","\\end{equation*}   Moreover, $2 \\xi_{opt}(\\alpha)$ is precisely the concentrating point of the mean width of the polyhedron $\\{\\mathbf{x}|A\\mathbf{x} \\leq \\mathbf{1}\\}$."],"url":"http://arxiv.org/abs/2403.03637v1","category":"math.OC"}
{"created":"2024-03-06 11:42:35","title":"A hybrid dynamical system approach to the impulsive control of spacecraft rendezvous (extended version)","abstract":"This paper introduces a hybrid dynamical system methodology for managing impulsive control in spacecraft rendezvous and proximity operations under the Hill-Clohessy-Wiltshire model. We address the control design problem by isolating the out-of-plane from the in-plane dynamics and present a feedback control law for each of them. This law is based on a Lyapunov function tailored to each of the dynamics, capable of addressing thruster saturation and also a minimum impulse bit. These Lyapunov functions were found by reformulating the system's dynamics into coordinates that more intuitively represent their physical behavior. The effectiveness of our control laws is then shown through numerical simulation. This is an extended version of an ECC24 article of the same name, which includes the proofs omitted for lack of space.","sentences":["This paper introduces a hybrid dynamical system methodology for managing impulsive control in spacecraft rendezvous and proximity operations under the Hill-Clohessy-Wiltshire model.","We address the control design problem by isolating the out-of-plane from the in-plane dynamics and present a feedback control law for each of them.","This law is based on a Lyapunov function tailored to each of the dynamics, capable of addressing thruster saturation and also a minimum impulse bit.","These Lyapunov functions were found by reformulating the system's dynamics into coordinates that more intuitively represent their physical behavior.","The effectiveness of our control laws is then shown through numerical simulation.","This is an extended version of an ECC24 article of the same name, which includes the proofs omitted for lack of space."],"url":"http://arxiv.org/abs/2403.03633v1","category":"eess.SY"}
{"created":"2024-03-06 11:31:29","title":"Some direct and inverse problems for the Restricted Signed sumset in set of integers","abstract":"Given a positive integer $h$ and a nonempty finite set of integers $A=\\{a_{1},a_{2},\\ldots,a_{k}\\}$, the restricted $h$-fold signed sumset of $A$, denoted by $h^{\\wedge}_{\\pm}A$, is defined as $$h^{\\wedge}_{\\pm}A=\\left\\lbrace \\sum_{i=1}^{k} \\lambda_{i} a_{i}: \\lambda_{i} \\in \\left\\lbrace -1, 0, 1\\right\\rbrace \\ \\text{for} \\ i= 1, 2, \\ldots, k \\ \\text{and} \\ \\sum_{i=1}^{k} \\left| \\lambda_{i} \\right| =h\\right\\rbrace.$$ The direct problem associated with this sumset is to find the optimal lower bound of $|h^{\\wedge}_{\\pm}A|$, and the inverse problem associated with this sumset is to determine the structure of the underlying set $A$, when $|h^{\\wedge}_{\\pm}A|$ attains the optimal lower bound. Bhanja, Komatsu and Pandey studied the direct and inverse problem for the restricted $h$-fold signed sumset for $h=2, 3$, and $k$ and conjectured some direct and inverse results for $h \\geq 4$. In this paper, we prove these conjectures for $h=4$. We also prove the direct and inverse theorems for arbitrary $h$ under certain restrictions on the set $A$ which are particular cases of the conjectures. Moreover, we prove these conjectures for arithmetic progressions.","sentences":["Given a positive integer $h$ and a nonempty finite set of integers $A=\\{a_{1},a_{2},\\ldots,a_{k}\\}$, the restricted $h$-fold signed sumset of $A$, denoted by $h^{\\wedge}_{\\pm}A$, is defined as $$h^{\\wedge}_{\\pm}A=\\left\\lbrace \\sum_{i=1}^{k} \\lambda_{i} a_{i}: \\lambda_{i} \\in \\left\\lbrace -1, 0, 1\\right\\rbrace \\ \\text{for} \\ i= 1, 2, \\ldots, k \\ \\text{and} \\ \\sum_{i=1}^{k} \\left| \\lambda_{i} \\right| =h\\right\\rbrace.$$ The direct problem associated with this sumset is to find the optimal lower bound of $|h^{\\wedge}_{\\pm}A|$, and the inverse problem associated with this sumset is to determine the structure of the underlying set $A$, when $|h^{\\wedge}_{\\pm}A|$ attains the optimal lower bound.","Bhanja, Komatsu and Pandey studied the direct and inverse problem for the restricted $h$-fold signed sumset for $h=2, 3$, and $k$ and conjectured some direct and inverse results for $h \\geq","4$.","In this paper, we prove these conjectures for $h=4$. We also prove the direct and inverse theorems for arbitrary $h$ under certain restrictions on the set $A$ which are particular cases of the conjectures.","Moreover, we prove these conjectures for arithmetic progressions."],"url":"http://arxiv.org/abs/2403.03625v1","category":"math.NT"}
{"created":"2024-03-06 11:31:08","title":"Data-Driven Superstabilizing Control under Quadratically-Bounded Errors-in-Variables Noise","abstract":"The Error-in-Variables model of system identification/control involves nontrivial input and measurement corruption of observed data, resulting in generically nonconvex optimization problems. This paper performs full-state-feedback stabilizing control of all discrete-time linear systems that are consistent with observed data for which the input and measurement noise obey quadratic bounds. Instances of such quadratic bounds include elementwise norm bounds (at each time sample), energy bounds (across the entire signal), and chance constraints arising from (sub)gaussian noise. Superstabilizing controllers are generated through the solution of a sum-of-squares hierarchy of semidefinite programs. A theorem of alternatives is employed to eliminate the input and measurement noise process, thus improving tractability. Effectiveness of the scheme is generated on an example system in the chance-constrained set-membership setting where the input and state-measurement noise are i.i.d. normally distributed.","sentences":["The Error-in-Variables model of system identification/control involves nontrivial input and measurement corruption of observed data, resulting in generically nonconvex optimization problems.","This paper performs full-state-feedback stabilizing control of all discrete-time linear systems that are consistent with observed data for which the input and measurement noise obey quadratic bounds.","Instances of such quadratic bounds include elementwise norm bounds (at each time sample), energy bounds (across the entire signal), and chance constraints arising from (sub)gaussian noise.","Superstabilizing controllers are generated through the solution of a sum-of-squares hierarchy of semidefinite programs.","A theorem of alternatives is employed to eliminate the input and measurement noise process, thus improving tractability.","Effectiveness of the scheme is generated on an example system in the chance-constrained set-membership setting where the input and state-measurement noise are i.i.d. normally distributed."],"url":"http://arxiv.org/abs/2403.03624v1","category":"math.OC"}
{"created":"2024-03-06 10:44:55","title":"Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application","abstract":"Cylinder pressure-based control is a key enabler for advanced pre-mixed combustion concepts. Besides guaranteeing robust and safe operation, it allows for cylinder pressure and heat release shaping. This requires fast control-oriented combustion models. Over the years, mean-value models have been proposed that can predict combustion measures (e.g., Gross Indicated Mean Effective Pressure, or the crank angle where 50% of the total heat is released) or models that predict the full in-cylinder pressure. However, these models are not able to capture cyclic variations. This is important in the control design for combustion concepts, like Reactivity Controlled Compression Ignition, that can suffer from large cyclic variations. In this study, the in-cylinder pressure and cyclic variation are modelled using a data-based approach. The model combines Principle Component Decomposition and Gaussian Process Regression. A detailed study is performed on the effects of the different hyperparameters and kernel choices. The approach is applicable to any combustion concept, but most valuable for advance combustion concepts with large cyclic variation. The potential of the proposed approach is demonstrated for an Reactivity Controlled Compression Ignition engine running on Diesel and E85. The prediction quality of the evaluated combustion measures has an overall accuracy of 13.5% and 65.5% in mean behaviour and standard deviation, respectively. The peak-pressure rise-rate is traditionally hard to predict, in the proposed model it has an accuracy of 22.7% and 96.4% in mean behaviour and standard deviation, respectively. This Principle Component Decomposition-based approach is an important step towards in-cylinder pressure shaping. The use of Gaussian Process Regression provides important information on cyclic variation and provides next-cycle controls information on safety and performance criteria.","sentences":["Cylinder pressure-based control is a key enabler for advanced pre-mixed combustion concepts.","Besides guaranteeing robust and safe operation, it allows for cylinder pressure and heat release shaping.","This requires fast control-oriented combustion models.","Over the years, mean-value models have been proposed that can predict combustion measures (e.g., Gross Indicated Mean Effective Pressure, or the crank angle where 50% of the total heat is released) or models that predict the full in-cylinder pressure.","However, these models are not able to capture cyclic variations.","This is important in the control design for combustion concepts, like Reactivity Controlled Compression Ignition, that can suffer from large cyclic variations.","In this study, the in-cylinder pressure and cyclic variation are modelled using a data-based approach.","The model combines Principle Component Decomposition and Gaussian Process Regression.","A detailed study is performed on the effects of the different hyperparameters and kernel choices.","The approach is applicable to any combustion concept, but most valuable for advance combustion concepts with large cyclic variation.","The potential of the proposed approach is demonstrated for an Reactivity Controlled Compression Ignition engine running on Diesel and E85.","The prediction quality of the evaluated combustion measures has an overall accuracy of 13.5% and 65.5% in mean behaviour and standard deviation, respectively.","The peak-pressure rise-rate is traditionally hard to predict, in the proposed model it has an accuracy of 22.7% and 96.4% in mean behaviour and standard deviation, respectively.","This Principle Component Decomposition-based approach is an important step towards in-cylinder pressure shaping.","The use of Gaussian Process Regression provides important information on cyclic variation and provides next-cycle controls information on safety and performance criteria."],"url":"http://arxiv.org/abs/2403.03602v2","category":"eess.SY"}
{"created":"2024-03-06 10:01:35","title":"RouteExplainer: An Explanation Framework for Vehicle Routing Problem","abstract":"The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs). We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining reasonable accuracy, thereby highlighting its potential for deployment in practical applications. Moreover, on the subject of a tourist route, we qualitatively evaluate explanations generated by our framework. This evaluation not only validates our framework but also shows the synergy between explanation frameworks and LLMs. See https://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo.","sentences":["The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems.","While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored.","In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route.","Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP.","To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs).","We quantitatively evaluate our edge classifier on four different VRPs.","The results demonstrate its rapid computation while maintaining reasonable accuracy, thereby highlighting its potential for deployment in practical applications.","Moreover, on the subject of a tourist route, we qualitatively evaluate explanations generated by our framework.","This evaluation not only validates our framework but also shows the synergy between explanation frameworks and LLMs.","See https://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo."],"url":"http://arxiv.org/abs/2403.03585v1","category":"cs.LG"}
{"created":"2024-03-06 09:58:57","title":"Interactive Bayesian Generative Models for Abnormality Detection in Vehicular Networks","abstract":"The following paper proposes a novel Vehicle-to-Everything (V2X) network abnormality detection scheme based on Bayesian generative models for enhanced network self-awareness functionality at the Base station (BS). In the learning phase, multi-modal data signals contrived by the vehicles' integrated and sensing module are imbued into data-driven Generalized Dynamic Bayesian network (GDBN) models. Following that, during the testing phase, an Interactive Modified Markov Jump Particle filter (IM-MJPF) is utilized to forecast forthcoming network states and vehicle trajectories by leveraging the assimilated semantics embedded in the coupled multi-GDBNs. This approach involves learning statistically correlated association between evolving trajectories and network communication links. Security and surveillance of Internet of Vehicles (IOVs) links are performed online with high detection probabilities by matching predicted with observed network connectivity maps (graphs).","sentences":["The following paper proposes a novel Vehicle-to-Everything (V2X) network abnormality detection scheme based on Bayesian generative models for enhanced network self-awareness functionality at the Base station (BS).","In the learning phase, multi-modal data signals contrived by the vehicles' integrated and sensing module are imbued into data-driven Generalized Dynamic Bayesian network (GDBN) models.","Following that, during the testing phase, an Interactive Modified Markov Jump Particle filter (IM-MJPF) is utilized to forecast forthcoming network states and vehicle trajectories by leveraging the assimilated semantics embedded in the coupled multi-GDBNs.","This approach involves learning statistically correlated association between evolving trajectories and network communication links.","Security and surveillance of Internet of Vehicles (IOVs) links are performed online with high detection probabilities by matching predicted with observed network connectivity maps (graphs)."],"url":"http://arxiv.org/abs/2403.03583v1","category":"eess.SP"}
{"created":"2024-03-06 09:56:44","title":"Stabilization via localized controls in nonlocal models of crowd dynamics","abstract":"We consider a control system driven by a nonlocal continuity equation. Admissible controls are Lipschitz vector fields acting inside a fixed open set. We demonstrate that small perturbations of the initial measure, traced along Wasserstein geodesics, may be neutralized by admissible controls. More specifically, initial perturbations of order $\\varepsilon$ can be reduced to order $\\varepsilon^{1+\\kappa}$, where $\\kappa$ is a positive constant.","sentences":["We consider a control system driven by a nonlocal continuity equation.","Admissible controls are Lipschitz vector fields acting inside a fixed open set.","We demonstrate that small perturbations of the initial measure, traced along Wasserstein geodesics, may be neutralized by admissible controls.","More specifically, initial perturbations of order $\\varepsilon$ can be reduced to order $\\varepsilon^{1+\\kappa}$, where $\\kappa$ is a positive constant."],"url":"http://arxiv.org/abs/2403.03580v1","category":"math.OC"}
{"created":"2024-03-06 09:34:27","title":"Time-optimal Point-to-point Motion Planning: A Two-stage Approach","abstract":"This paper proposes a two-stage approach to formulate the time-optimal point-to-point motion planning problem, involving a first stage with a fixed time grid and a second stage with a variable time grid. The proposed approach brings benefits through its straightforward optimal control problem formulation with a fixed and low number of control steps for manageable computational complexity and the avoidance of interpolation errors associated with time scaling, especially when aiming to reach a distant goal. Additionally, an asynchronous nonlinear model predictive control (NMPC) update scheme is integrated with this two-stage approach to address delayed and fluctuating computation times, facilitating online replanning. The effectiveness of the proposed two-stage approach and NMPC implementation is demonstrated through numerical examples centered on autonomous navigation with collision avoidance.","sentences":["This paper proposes a two-stage approach to formulate the time-optimal point-to-point motion planning problem, involving a first stage with a fixed time grid and a second stage with a variable time grid.","The proposed approach brings benefits through its straightforward optimal control problem formulation with a fixed and low number of control steps for manageable computational complexity and the avoidance of interpolation errors associated with time scaling, especially when aiming to reach a distant goal.","Additionally, an asynchronous nonlinear model predictive control (NMPC) update scheme is integrated with this two-stage approach to address delayed and fluctuating computation times, facilitating online replanning.","The effectiveness of the proposed two-stage approach and NMPC implementation is demonstrated through numerical examples centered on autonomous navigation with collision avoidance."],"url":"http://arxiv.org/abs/2403.03573v1","category":"cs.RO"}
{"created":"2024-03-06 09:30:31","title":"Anisotropic power diagrams for polycrystal modelling: efficient generation of curved grains via optimal transport","abstract":"The microstructure of metals and foams can be effectively modelled with anisotropic power diagrams (APDs), which provide control over the shape of individual grains. One major obstacle to the wider adoption of APDs is the computational cost that is associated with their generation. We propose a novel approach to generate APDs with prescribed statistical properties, including fine control over the size of individual grains. To this end, we rely on fast optimal transport algorithms that stream well on Graphics Processing Units (GPU) and handle non-uniform, anisotropic distance functions. This allows us to find APDs that best fit experimental data in (tens of) seconds, which unlocks their use for computational homogenisation. This is especially relevant to machine learning methods that require the generation of large collections of representative microstructures as training data. The paper is accompanied by a Python library, PyAPD, which is freely available at: www.github.com/mbuze/PyAPD.","sentences":["The microstructure of metals and foams can be effectively modelled with anisotropic power diagrams (APDs), which provide control over the shape of individual grains.","One major obstacle to the wider adoption of APDs is the computational cost that is associated with their generation.","We propose a novel approach to generate APDs with prescribed statistical properties, including fine control over the size of individual grains.","To this end, we rely on fast optimal transport algorithms that stream well on Graphics Processing Units (GPU) and handle non-uniform, anisotropic distance functions.","This allows us to find APDs that best fit experimental data in (tens of) seconds, which unlocks their use for computational homogenisation.","This is especially relevant to machine learning methods that require the generation of large collections of representative microstructures as training data.","The paper is accompanied by a Python library, PyAPD, which is freely available at: www.github.com/mbuze/PyAPD."],"url":"http://arxiv.org/abs/2403.03571v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 15:23:44","title":"A.C.I.D -- An Improved LSD Technique for Accurate Line Profile Retrieval","abstract":"Stellar activity and planetary effects induce radial velocity (RV) offsets and cause temporal distortions in the shape of the stellar line profile. Hence, accurately probing the stellar line profile offers a wealth of information on both the star itself and any orbiting planets. Typically, Cross-Correlation Functions (CCFs) are used as a proxy for the stellar line profile. The shape of CCFs, however, can be distorted by line blending and aliasing limiting the stellar and planetary physics that can be probed from them. Least-squares deconvolution (LSD) offers an alternative that directly fits the mean line profile of the spectrum to produce a high-precision profile. In this paper, we introduce our novel method ACID (Accurate Continuum fItting and Deconvolution) that builds on LSD techniques by simultaneously fitting the spectral continuum and line profile as well as performing LSD in effective optical depth. Tests on model data revealed ACID can accurately identify and correct the spectral continuum to retrieve an injected line profile. ACID was also applied to archival HARPS data obtained during the transit of HD189733b. The application of the Reloaded Rossiter-McLaughlin technique to both ACID profiles and HARPS CCFs shows ACID residual profiles improved the out-of-line RMS by over 5% compared to CCFs. Furthermore, ACID profiles are shown to exhibit a Voigt profile shape that better describes the expected profile shape of the stellar line profile. This improved representation shows that ACID better preserves the stellar and planetary physics encoded in the stellar line profile shape for slow rotating stars.","sentences":["Stellar activity and planetary effects induce radial velocity (RV) offsets and cause temporal distortions in the shape of the stellar line profile.","Hence, accurately probing the stellar line profile offers a wealth of information on both the star itself and any orbiting planets.","Typically, Cross-Correlation Functions (CCFs) are used as a proxy for the stellar line profile.","The shape of CCFs, however, can be distorted by line blending and aliasing limiting the stellar and planetary physics that can be probed from them.","Least-squares deconvolution (LSD) offers an alternative that directly fits the mean line profile of the spectrum to produce a high-precision profile.","In this paper, we introduce our novel method ACID (Accurate Continuum fItting and Deconvolution) that builds on LSD techniques by simultaneously fitting the spectral continuum and line profile as well as performing LSD in effective optical depth.","Tests on model data revealed ACID can accurately identify and correct the spectral continuum to retrieve an injected line profile.","ACID was also applied to archival HARPS data obtained during the transit of HD189733b.","The application of the Reloaded Rossiter-McLaughlin technique to both ACID profiles and HARPS CCFs shows ACID residual profiles improved the out-of-line RMS by over 5% compared to CCFs.","Furthermore, ACID profiles are shown to exhibit a Voigt profile shape that better describes the expected profile shape of the stellar line profile.","This improved representation shows that ACID better preserves the stellar and planetary physics encoded in the stellar line profile shape for slow rotating stars."],"url":"http://arxiv.org/abs/2403.04579v1","category":"astro-ph.EP"}
