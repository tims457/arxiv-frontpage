{"created":"2024-02-05 09:18:49","title":"SynthVision -- Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data","abstract":"Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision model performance in medical image recognition. The study findings revealed significant insights into the performance of the vision model trained on synthetic images generated through diffusion models. The vision model showed exceptional performance in accurately identifying cases of genital warts. It achieved an accuracy rate of 96% underscoring its effectiveness in medical image classification. For HPV cases the model demonstrated a high precision of 99% and a recall of 94%. In normal cases the precision was 95% with an impressive recall of 99%. These metrics indicate the model capability to correctly identify true positive cases and minimize false positives. The model achieved an F1 Score of 96% for HPV cases and 97% for normal cases. The high F1 Score across both categories highlights the balanced nature of the model precision and recall ensuring reliability and robustness in its predictions.","sentences":["Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism.","However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data.","We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data.","In our study, we employed a two phase experimental design using diffusion models.","In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts.","The second phase involved the training and testing vision model using this synthetic dataset.","This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision model performance in medical image recognition.","The study findings revealed significant insights into the performance of the vision model trained on synthetic images generated through diffusion models.","The vision model showed exceptional performance in accurately identifying cases of genital warts.","It achieved an accuracy rate of 96% underscoring its effectiveness in medical image classification.","For HPV cases the model demonstrated a high precision of 99% and a recall of 94%.","In normal cases the precision was 95% with an impressive recall of 99%.","These metrics indicate the model capability to correctly identify true positive cases and minimize false positives.","The model achieved an F1 Score of 96% for HPV cases and 97% for normal cases.","The high F1 Score across both categories highlights the balanced nature of the model precision and recall ensuring reliability and robustness in its predictions."],"url":"http://arxiv.org/abs/2402.02826v1","category":"cs.CV"}
{"created":"2024-02-05 11:55:30","title":"Domain Adaptation of Multilingual Semantic Search -- Literature Review","abstract":"This literature review gives an overview of current approaches to perform domain adaptation in a low-resource and approaches to perform multilingual semantic search in a low-resource setting. We developed a new typology to cluster domain adaptation approaches based on the part of dense textual information retrieval systems, which they adapt, focusing on how to combine them efficiently. We also explore the possibilities of combining multilingual semantic search with domain adaptation approaches for dense retrievers in a low-resource setting.","sentences":["This literature review gives an overview of current approaches to perform domain adaptation in a low-resource and approaches to perform multilingual semantic search in a low-resource setting.","We developed a new typology to cluster domain adaptation approaches based on the part of dense textual information retrieval systems, which they adapt, focusing on how to combine them efficiently.","We also explore the possibilities of combining multilingual semantic search with domain adaptation approaches for dense retrievers in a low-resource setting."],"url":"http://arxiv.org/abs/2402.02932v1","category":"cs.IR"}
{"created":"2024-02-05 00:45:52","title":"Geodetic Research on Deception Island and its Environment (South Shetland Islands, Bransfield Sea and Antarctic Peninsula) During Spanish Antarctic Campaigns (1987-2007)","abstract":"Since 1987, Spain has been continuously developing several scientific projects, mainly based on Earth Sciences, in Geodesy, Geochemistry, Geology or Volcanology. The need of a geodetic reference frame when doing hydrographic and topographic mapping meant the organization of the earlier campaigns with the main goals of updating the existing cartography and of making new maps of the area. During this period of time, new techniques arose in Space Geodesy improving the classical methodology and making possible its applications to other different fields such as tectonic or volcanism. Spanish Antarctic Geodetic activities from the 1987/1988 to 2006/2007 campaigns are described as well as a geodetic and a levelling network are presented. The first network, RGAE, was designed and established to define a reference frame in the region formed by the South Shetlands Islands, the Bransfield Sea and the Antarctic Peninsula whereas the second one, REGID, was planned to control the volcanic activity in Deception Island. Finally, the horizontal and vertical deformation models are described too, as well as the strategy which has been followed when computing an experimental geoid.","sentences":["Since 1987, Spain has been continuously developing several scientific projects, mainly based on Earth Sciences, in Geodesy, Geochemistry, Geology or Volcanology.","The need of a geodetic reference frame when doing hydrographic and topographic mapping meant the organization of the earlier campaigns with the main goals of updating the existing cartography and of making new maps of the area.","During this period of time, new techniques arose in Space Geodesy improving the classical methodology and making possible its applications to other different fields such as tectonic or volcanism.","Spanish Antarctic Geodetic activities from the 1987/1988 to 2006/2007 campaigns are described as well as a geodetic and a levelling network are presented.","The first network, RGAE, was designed and established to define a reference frame in the region formed by the South Shetlands Islands, the Bransfield Sea and the Antarctic Peninsula whereas the second one, REGID, was planned to control the volcanic activity in Deception Island.","Finally, the horizontal and vertical deformation models are described too, as well as the strategy which has been followed when computing an experimental geoid."],"url":"http://arxiv.org/abs/2402.02650v1","category":"physics.geo-ph"}
{"created":"2024-02-04 15:17:09","title":"CompeteSMoE -- Effective Training of Sparse Mixture of Experts via Competition","abstract":"Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, effective training of SMoE has proven to be challenging due to the representation collapse issue, which causes parameter redundancy and limited representation potentials. In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse. By routing inputs only to experts with the highest neural response, we show that, under mild assumptions, competition enjoys the same convergence rate as the optimal estimator. We further propose CompeteSMoE, an effective and efficient algorithm to train large language models by deploying a simple router that predicts the competition outcomes. Consequently, CompeteSMoE enjoys strong performance gains from the competition routing policy while having low computation overheads. Our extensive empirical evaluations on two transformer architectures and a wide range of tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies.","sentences":["Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width.","However, effective training of SMoE has proven to be challenging due to the representation collapse issue, which causes parameter redundancy and limited representation potentials.","In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse.","By routing inputs only to experts with the highest neural response, we show that, under mild assumptions, competition enjoys the same convergence rate as the optimal estimator.","We further propose CompeteSMoE, an effective and efficient algorithm to train large language models by deploying a simple router that predicts the competition outcomes.","Consequently, CompeteSMoE enjoys strong performance gains from the competition routing policy while having low computation overheads.","Our extensive empirical evaluations on two transformer architectures and a wide range of tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies."],"url":"http://arxiv.org/abs/2402.02526v1","category":"cs.LG"}
{"created":"2024-02-05 17:52:58","title":"ActiveAnno3D -- An Active Learning Framework for Multi-Modal 3D Object Detection","abstract":"The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the training data and 75.0 mAP when using the complete nuScenes dataset. We integrate our active learning framework into the proAnno labeling tool to enable AI-assisted data selection and labeling and minimize the labeling costs. Finally, we provide code, weights, and visualization results on our website: https://active3d-framework.github.io/active3d-framework.","sentences":["The curation of large-scale datasets is still costly and requires much time and resources.","Data is often manually labeled, and the challenge of creating high-quality datasets remains.","In this work, we fill the research gap using active learning for multi-modal 3D object detection.","We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training.","We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance.","Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset.","We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset.","BEVFusion achieved an mAP of 64.31 when using half of the training data and 75.0 mAP when using the complete nuScenes dataset.","We integrate our active learning framework into the proAnno labeling tool to enable AI-assisted data selection and labeling and minimize the labeling costs.","Finally, we provide code, weights, and visualization results on our website: https://active3d-framework.github.io/active3d-framework."],"url":"http://arxiv.org/abs/2402.03235v1","category":"cs.CV"}
{"created":"2024-02-04 12:15:56","title":"On Minimum Trace Factor Analysis -- An Old Song Sung to a New Tune","abstract":"Dimensionality reduction methods, such as principal component analysis (PCA) and factor analysis, are central to many problems in data science. There are, however, serious and well-understood challenges to finding robust low dimensional approximations for data with significant heteroskedastic noise. This paper introduces a relaxed version of Minimum Trace Factor Analysis (MTFA), a convex optimization method with roots dating back to the work of Ledermann in 1940. This relaxation is particularly effective at not overfitting to heteroskedastic perturbations and addresses the commonly cited Heywood cases in factor analysis and the recently identified \"curse of ill-conditioning\" for existing spectral methods. We provide theoretical guarantees on the accuracy of the resulting low rank subspace and the convergence rate of the proposed algorithm to compute that matrix. We develop a number of interesting connections to existing methods, including HeteroPCA, Lasso, and Soft-Impute, to fill an important gap in the already large literature on low rank matrix estimation. Numerical experiments benchmark our results against several recent proposals for dealing with heteroskedastic noise.","sentences":["Dimensionality reduction methods, such as principal component analysis (PCA) and factor analysis, are central to many problems in data science.","There are, however, serious and well-understood challenges to finding robust low dimensional approximations for data with significant heteroskedastic noise.","This paper introduces a relaxed version of Minimum Trace Factor Analysis (MTFA), a convex optimization method with roots dating back to the work of Ledermann in 1940.","This relaxation is particularly effective at not overfitting to heteroskedastic perturbations and addresses the commonly cited Heywood cases in factor analysis and the recently identified \"curse of ill-conditioning\" for existing spectral methods.","We provide theoretical guarantees on the accuracy of the resulting low rank subspace and the convergence rate of the proposed algorithm to compute that matrix.","We develop a number of interesting connections to existing methods, including HeteroPCA, Lasso, and Soft-Impute, to fill an important gap in the already large literature on low rank matrix estimation.","Numerical experiments benchmark our results against several recent proposals for dealing with heteroskedastic noise."],"url":"http://arxiv.org/abs/2402.02459v1","category":"stat.ML"}
