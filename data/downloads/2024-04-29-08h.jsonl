{"created":"2024-04-26 17:59:14","title":"A tale of two localizations: coexistence of flat bands and Anderson localization in a photonics-inspired amorphous system","abstract":"Emerging experimental platforms use amorphousness, a constrained form of disorder, to tailor meta-material properties. We study localization under this type of disorder in a class of $2D$ models generalizing recent experiments on photonic systems. We explore two kinds of localization that emerge in these models: Anderson localization by disorder, and the existence of compact, macroscopically degenerate localized states as in many crystalline flat bands. We find localization properties to depend on the symmetry class within a family of amorphized kagom\\'{e} tight-binding models, set by a tunable synthetic magnetic field. The flat-band-like degeneracy innate to kagom\\'{e} lattices survives under amorphousness without on-site disorder. This phenomenon arises from the cooperation between the structure of the compact localized states and the geometry of the amorphous graph. For particular values of the field, such states emerge in the amorphous system that were not present on the kagom\\'{e} lattice in the same field. For generic states, the standard paradigm of Anderson localization is found to apply as expected for systems with particle-hole symmetry (class D), while a similar interpretation does not extend to our results in the general unitary case (class A). The structure of amorphous graphs, which arise in current photonics experiments, allows exact statements about flat-band-like states, including such states that only exist in amorphous systems, and demonstrates how the qualitative behavior of a disordered system can be tuned at fixed graph topology.","sentences":["Emerging experimental platforms use amorphousness, a constrained form of disorder, to tailor meta-material properties.","We study localization under this type of disorder in a class of $2D$ models generalizing recent experiments on photonic systems.","We explore two kinds of localization that emerge in these models: Anderson localization by disorder, and the existence of compact, macroscopically degenerate localized states as in many crystalline flat bands.","We find localization properties to depend on the symmetry class within a family of amorphized kagom\\'{e} tight-binding models, set by a tunable synthetic magnetic field.","The flat-band-like degeneracy innate to kagom\\'{e} lattices survives under amorphousness without on-site disorder.","This phenomenon arises from the cooperation between the structure of the compact localized states and the geometry of the amorphous graph.","For particular values of the field, such states emerge in the amorphous system that were not present on the kagom\\'{e} lattice in the same field.","For generic states, the standard paradigm of Anderson localization is found to apply as expected for systems with particle-hole symmetry (class D), while a similar interpretation does not extend to our results in the general unitary case (class A).","The structure of amorphous graphs, which arise in current photonics experiments, allows exact statements about flat-band-like states, including such states that only exist in amorphous systems, and demonstrates how the qualitative behavior of a disordered system can be tuned at fixed graph topology."],"url":"http://arxiv.org/abs/2404.17578v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-26 17:58:30","title":"On Quasi-Locality and Decay of Correlations for Long-Range Models of Open Quantum Spin Systems","abstract":"We consider models of open quantum spin systems with irreversible dynamics and show that general quasi-locality results for long-range models, e.g. as proven for the Heisenberg dynamics associated to quantum systems in [27], naturally extend to this setting. Given these bounds, we provide two applications. First, we use these results to obtain estimates on a strictly local approximation of these finite-volume, irreversible dynamics. Next, we show how these bounds can be used to estimate correlation decay in various states.","sentences":["We consider models of open quantum spin systems with irreversible dynamics and show that general quasi-locality results for long-range models, e.g. as proven for the Heisenberg dynamics associated to quantum systems in [27], naturally extend to this setting.","Given these bounds, we provide two applications.","First, we use these results to obtain estimates on a strictly local approximation of these finite-volume, irreversible dynamics.","Next, we show how these bounds can be used to estimate correlation decay in various states."],"url":"http://arxiv.org/abs/2404.17577v1","category":"math-ph"}
{"created":"2024-04-26 17:58:09","title":"Enhancing Longitudinal Clinical Trial Efficiency with Digital Twins and Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM)","abstract":"Clinical trials are critical in advancing medical treatments but often suffer from immense time and financial burden. Advances in statistical methodologies and artificial intelligence (AI) present opportunities to address these inefficiencies. Here we introduce Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM) as an advantageous combination of prognostic covariate adjustment (PROCOVA) and Mixed Models for Repeated Measures (MMRM). PROCOVA-MMRM utilizes time-matched prognostic scores generated from AI models to enhance the precision of treatment effect estimators for longitudinal continuous outcomes, enabling reductions in sample size and enrollment times. We first provide a description of the background and implementation of PROCOVA-MMRM, followed by two case study reanalyses where we compare the performance of PROCOVA-MMRM versus the unadjusted MMRM. These reanalyses demonstrate significant improvements in statistical power and precision in clinical indications with unmet medical need, specifically Alzheimer's Disease (AD) and Amyotrophic Lateral Sclerosis (ALS). We also explore the potential for sample size reduction with the prospective implementation of PROCOVA-MMRM, finding that the same or better results could have been achieved with fewer participants in these historical trials if the enhanced precision provided by PROCOVA-MMRM had been prospectively leveraged. We also confirm the robustness of the statistical properties of PROCOVA-MMRM in a variety of realistic simulation scenarios. Altogether, PROCOVA-MMRM represents a rigorous method of incorporating advances in the prediction of time-matched prognostic scores generated by AI into longitudinal analysis, potentially reducing both the cost and time required to bring new treatments to patients while adhering to regulatory standards.","sentences":["Clinical trials are critical in advancing medical treatments but often suffer from immense time and financial burden.","Advances in statistical methodologies and artificial intelligence (AI) present opportunities to address these inefficiencies.","Here we introduce Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM) as an advantageous combination of prognostic covariate adjustment (PROCOVA) and Mixed Models for Repeated Measures (MMRM).","PROCOVA-MMRM utilizes time-matched prognostic scores generated from AI models to enhance the precision of treatment effect estimators for longitudinal continuous outcomes, enabling reductions in sample size and enrollment times.","We first provide a description of the background and implementation of PROCOVA-MMRM, followed by two case study reanalyses where we compare the performance of PROCOVA-MMRM versus the unadjusted MMRM.","These reanalyses demonstrate significant improvements in statistical power and precision in clinical indications with unmet medical need, specifically Alzheimer's Disease (AD) and Amyotrophic Lateral Sclerosis (ALS).","We also explore the potential for sample size reduction with the prospective implementation of PROCOVA-MMRM, finding that the same or better results could have been achieved with fewer participants in these historical trials if the enhanced precision provided by PROCOVA-MMRM had been prospectively leveraged.","We also confirm the robustness of the statistical properties of PROCOVA-MMRM in a variety of realistic simulation scenarios.","Altogether, PROCOVA-MMRM represents a rigorous method of incorporating advances in the prediction of time-matched prognostic scores generated by AI into longitudinal analysis, potentially reducing both the cost and time required to bring new treatments to patients while adhering to regulatory standards."],"url":"http://arxiv.org/abs/2404.17576v1","category":"stat.AP"}
{"created":"2024-04-26 17:58:07","title":"Low-energy nine-layer rhombohedral stacking of transition metal dichalcogenides","abstract":"Transition-metal dichalcogenides (TMDs) show unique physical, optical, and electronic properties. The known phases of TMDs are 2H and 3R in bulk form, 1T and associated reconstructions, and 1H in monolayer form. This paper reports a hypothetical phase, 9R, that may exist in TMDs (Mo, W)(S, Se, Te)$_2$, meeting both dynamical stability and elastic stability criteria. 9R phase has the same space group as 3R, $i.e.$ rhombohedral $R3m$ without inversion symmetry, and has 9 layers in a conventional unit cell. We find that 9R has an energy within 1 meV per formula unit of 3R and can be energetically favored by a particular strain condition. We further calculate the electronic, elastic, piezoelectric, Raman, and second-harmonic generation signatures of 9R TMDs and compare them with the corresponding 2H and 3R phases. 9R has similar properties to 3R but shows distinctive Raman peaks in the low-frequency regime, improved piezoelectric properties, and unique band splitting arising from layer coupling at the conduction band minimum. These distinct properties make 9R an attractive candidate for applications in piezotronics and valleytronics.","sentences":["Transition-metal dichalcogenides (TMDs) show unique physical, optical, and electronic properties.","The known phases of TMDs are 2H and 3R in bulk form, 1T and associated reconstructions, and 1H in monolayer form.","This paper reports a hypothetical phase, 9R, that may exist in TMDs (Mo, W)(S, Se, Te)$_2$, meeting both dynamical stability and elastic stability criteria.","9R phase has the same space group as 3R, $i.e.$ rhombohedral $R3m$ without inversion symmetry, and has 9 layers in a conventional unit cell.","We find that 9R has an energy within 1 meV per formula unit of 3R and can be energetically favored by a particular strain condition.","We further calculate the electronic, elastic, piezoelectric, Raman, and second-harmonic generation signatures of 9R TMDs and compare them with the corresponding 2H and 3R phases.","9R has similar properties to 3R but shows distinctive Raman peaks in the low-frequency regime, improved piezoelectric properties, and unique band splitting arising from layer coupling at the conduction band minimum.","These distinct properties make 9R an attractive candidate for applications in piezotronics and valleytronics."],"url":"http://arxiv.org/abs/2404.17575v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-26 17:55:26","title":"Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual Try-on in Videos","abstract":"Video try-on is a challenging task and has not been well tackled in previous works. The main obstacle lies in preserving the details of the clothing and modeling the coherent motions simultaneously. Faced with those difficulties, we address video try-on by proposing a diffusion-based framework named \"Tunnel Try-on.\" The core idea is excavating a \"focus tunnel\" in the input video that gives close-up shots around the clothing regions. We zoom in on the region in the tunnel to better preserve the fine details of the clothing. To generate coherent motions, we first leverage the Kalman filter to construct smooth crops in the focus tunnel and inject the position embedding of the tunnel into attention layers to improve the continuity of the generated videos. In addition, we develop an environment encoder to extract the context information outside the tunnels as supplementary cues. Equipped with these techniques, Tunnel Try-on keeps the fine details of the clothing and synthesizes stable and smooth videos. Demonstrating significant advancements, Tunnel Try-on could be regarded as the first attempt toward the commercial-level application of virtual try-on in videos.","sentences":["Video try-on is a challenging task and has not been well tackled in previous works.","The main obstacle lies in preserving the details of the clothing and modeling the coherent motions simultaneously.","Faced with those difficulties, we address video try-on by proposing a diffusion-based framework named \"Tunnel Try-on.\"","The core idea is excavating a \"focus tunnel\" in the input video that gives close-up shots around the clothing regions.","We zoom in on the region in the tunnel to better preserve the fine details of the clothing.","To generate coherent motions, we first leverage the Kalman filter to construct smooth crops in the focus tunnel and inject the position embedding of the tunnel into attention layers to improve the continuity of the generated videos.","In addition, we develop an environment encoder to extract the context information outside the tunnels as supplementary cues.","Equipped with these techniques, Tunnel Try-on keeps the fine details of the clothing and synthesizes stable and smooth videos.","Demonstrating significant advancements, Tunnel Try-on could be regarded as the first attempt toward the commercial-level application of virtual try-on in videos."],"url":"http://arxiv.org/abs/2404.17571v1","category":"cs.CV"}
{"created":"2024-04-26 17:54:45","title":"A manufacturable platform for photonic quantum computing","abstract":"Whilst holding great promise for low noise, ease of operation and networking, useful photonic quantum computing has been precluded by the need for beyond-state-of-the-art components, manufactured by the millions. Here we introduce a manufacturable platform for quantum computing with photons. We benchmark a set of monolithically-integrated silicon photonics-based modules to generate, manipulate, network, and detect photonic qubits, demonstrating dual-rail photonic qubits with $99.98\\% \\pm 0.01\\%$ state preparation and measurement fidelity, Hong-Ou-Mandel quantum interference between independent photon sources with $99.50\\%\\pm0.25\\%$ visibility, two-qubit fusion with $99.22\\%\\pm0.12\\%$ fidelity, and a chip-to-chip qubit interconnect with $99.72\\%\\pm0.04\\%$ fidelity, not accounting for loss. In addition, we preview a selection of next generation technologies, demonstrating low-loss silicon nitride waveguides and components, fabrication-tolerant photon sources, high-efficiency photon-number-resolving detectors, low-loss chip-to-fiber coupling, and barium titanate electro-optic phase shifters.","sentences":["Whilst holding great promise for low noise, ease of operation and networking, useful photonic quantum computing has been precluded by the need for beyond-state-of-the-art components, manufactured by the millions.","Here we introduce a manufacturable platform for quantum computing with photons.","We benchmark a set of monolithically-integrated silicon photonics-based modules to generate, manipulate, network, and detect photonic qubits, demonstrating dual-rail photonic qubits with $99.98\\% \\pm 0.01\\%$ state preparation and measurement fidelity, Hong-Ou-Mandel quantum interference between independent photon sources with $99.50\\%\\pm0.25\\%$ visibility, two-qubit fusion with $99.22\\%\\pm0.12\\%$ fidelity, and a chip-to-chip qubit interconnect with $99.72\\%\\pm0.04\\%$ fidelity, not accounting for loss.","In addition, we preview a selection of next generation technologies, demonstrating low-loss silicon nitride waveguides and components, fabrication-tolerant photon sources, high-efficiency photon-number-resolving detectors, low-loss chip-to-fiber coupling, and barium titanate electro-optic phase shifters."],"url":"http://arxiv.org/abs/2404.17570v1","category":"quant-ph"}
{"created":"2024-04-26 17:54:38","title":"MaPa: Text-driven Photorealistic Material Painting for 3D Shapes","abstract":"This paper aims to generate materials for 3D meshes from text descriptions. Unlike existing methods that synthesize texture maps, we propose to generate segment-wise procedural material graphs as the appearance representation, which supports high-quality rendering and provides substantial flexibility in editing. Instead of relying on extensive paired data, i.e., 3D meshes with material graphs and corresponding text descriptions, to train a material graph generative model, we propose to leverage the pre-trained 2D diffusion model as a bridge to connect the text and material graphs. Specifically, our approach decomposes a shape into a set of segments and designs a segment-controlled diffusion model to synthesize 2D images that are aligned with mesh parts. Based on generated images, we initialize parameters of material graphs and fine-tune them through the differentiable rendering module to produce materials in accordance with the textual description. Extensive experiments demonstrate the superior performance of our framework in photorealism, resolution, and editability over existing methods. Project page: https://zhanghe3z.github.io/MaPa/","sentences":["This paper aims to generate materials for 3D meshes from text descriptions.","Unlike existing methods that synthesize texture maps, we propose to generate segment-wise procedural material graphs as the appearance representation, which supports high-quality rendering and provides substantial flexibility in editing.","Instead of relying on extensive paired data, i.e., 3D meshes with material graphs and corresponding text descriptions, to train a material graph generative model, we propose to leverage the pre-trained 2D diffusion model as a bridge to connect the text and material graphs.","Specifically, our approach decomposes a shape into a set of segments and designs a segment-controlled diffusion model to synthesize 2D images that are aligned with mesh parts.","Based on generated images, we initialize parameters of material graphs and fine-tune them through the differentiable rendering module to produce materials in accordance with the textual description.","Extensive experiments demonstrate the superior performance of our framework in photorealism, resolution, and editability over existing methods.","Project page: https://zhanghe3z.github.io/MaPa/"],"url":"http://arxiv.org/abs/2404.17569v1","category":"cs.CV"}
{"created":"2024-04-26 17:51:05","title":"Extended genus fields of abelian extensions of rational function fields","abstract":"In this paper we obtain the extended genus field of a finite abelian extension of a global rational function field. We first study the case of a cyclic extension of prime power degree. Next, we use that the extended genus fields of a composite of two cyclotomic extensions of a global rational function field is equal to the composite of their respective extended genus fields, to obtain our main result. This result is that the extended genus field of a general finite abelian extension of a global rational function field, is given explicitly in terms of the field and of the extended genus field of its \"cyclotomic projection\".","sentences":["In this paper we obtain the extended genus field of a finite abelian extension of a global rational function field.","We first study the case of a cyclic extension of prime power degree.","Next, we use that the extended genus fields of a composite of two cyclotomic extensions of a global rational function field is equal to the composite of their respective extended genus fields, to obtain our main result.","This result is that the extended genus field of a general finite abelian extension of a global rational function field, is given explicitly in terms of the field and of the extended genus field of its \"cyclotomic projection\"."],"url":"http://arxiv.org/abs/2404.17566v1","category":"math.NT"}
{"created":"2024-04-26 17:44:15","title":"Boosting e-BH via conditional calibration","abstract":"The e-BH procedure is an e-value-based multiple testing procedure that provably controls the false discovery rate (FDR) under any dependence structure between the e-values. Despite this appealing theoretical FDR control guarantee, the e-BH procedure often suffers from low power in practice. In this paper, we propose a general framework that boosts the power of e-BH without sacrificing its FDR control under arbitrary dependence. This is achieved by the technique of conditional calibration, where we take as input the e-values and calibrate them to be a set of \"boosted e-values\" that are guaranteed to be no less -- and are often more -- powerful than the original ones. Our general framework is explicitly instantiated in three classes of multiple testing problems: (1) testing under parametric models, (2) conditional independence testing under the model-X setting, and (3) model-free conformalized selection. Extensive numerical experiments show that our proposed method significantly improves the power of e-BH while continuing to control the FDR. We also demonstrate the effectiveness of our method through an application to an observational study dataset for identifying individuals whose counterfactuals satisfy certain properties.","sentences":["The e-BH procedure is an e-value-based multiple testing procedure that provably controls the false discovery rate (FDR) under any dependence structure between the e-values.","Despite this appealing theoretical FDR control guarantee, the e-BH procedure often suffers from low power in practice.","In this paper, we propose a general framework that boosts the power of e-BH without sacrificing its FDR control under arbitrary dependence.","This is achieved by the technique of conditional calibration, where we take as input the e-values and calibrate them to be a set of \"boosted e-values\" that are guaranteed to be no less -- and are often more -- powerful than the original ones.","Our general framework is explicitly instantiated in three classes of multiple testing problems: (1) testing under parametric models, (2) conditional independence testing under the model-X setting, and (3) model-free conformalized selection.","Extensive numerical experiments show that our proposed method significantly improves the power of e-BH while continuing to control the FDR.","We also demonstrate the effectiveness of our method through an application to an observational study dataset for identifying individuals whose counterfactuals satisfy certain properties."],"url":"http://arxiv.org/abs/2404.17562v1","category":"stat.ME"}
{"created":"2024-04-26 17:42:29","title":"Structured Conformal Inference for Matrix Completion with Applications to Group Recommender Systems","abstract":"We develop a conformal inference method to construct joint confidence regions for structured groups of missing entries within a sparsely observed matrix. This method is useful to provide reliable uncertainty estimation for group-level collaborative filtering; for example, it can be applied to help suggest a movie for a group of friends to watch together. Unlike standard conformal techniques, which make inferences for one individual at a time, our method achieves stronger group-level guarantees by carefully assembling a structured calibration data set mimicking the patterns expected among the test group of interest. We propose a generalized weighted conformalization framework to deal with the lack of exchangeability arising from such structured calibration, and in this process we introduce several innovations to overcome computational challenges. The practicality and effectiveness of our method are demonstrated through extensive numerical experiments and an analysis of the MovieLens 100K data set.","sentences":["We develop a conformal inference method to construct joint confidence regions for structured groups of missing entries within a sparsely observed matrix.","This method is useful to provide reliable uncertainty estimation for group-level collaborative filtering; for example, it can be applied to help suggest a movie for a group of friends to watch together.","Unlike standard conformal techniques, which make inferences for one individual at a time, our method achieves stronger group-level guarantees by carefully assembling a structured calibration data set mimicking the patterns expected among the test group of interest.","We propose a generalized weighted conformalization framework to deal with the lack of exchangeability arising from such structured calibration, and in this process we introduce several innovations to overcome computational challenges.","The practicality and effectiveness of our method are demonstrated through extensive numerical experiments and an analysis of the MovieLens 100K data set."],"url":"http://arxiv.org/abs/2404.17561v1","category":"stat.ME"}
{"created":"2024-04-26 17:36:27","title":"Dynamics of spin helices in the diluted one-dimensional $XX$ model","abstract":"Motivated by discrepancies between recent cold atom experiments and the associated theory, we explore the effect of immobile holes on the quantum dynamics of $x$-$z$ spin helices in the one-dimensional $XX$ model. We calculate the exact spin dynamics by mapping onto a system of non-interacting fermions, averaging over the distribution of holes. At small hole densities we find that the helical spin pattern decays exponentially, with a pitch dependence that agrees with the experiments. At large hole densities we instead find persistent oscillations. While our analytic approach does not generalize to the $XXZ$ model with arbitrary anisotropies, we validate a matrix product state technique which might be used to model the experiments in those settings.","sentences":["Motivated by discrepancies between recent cold atom experiments and the associated theory, we explore the effect of immobile holes on the quantum dynamics of $x$-$z$ spin helices in the one-dimensional $XX$ model.","We calculate the exact spin dynamics by mapping onto a system of non-interacting fermions, averaging over the distribution of holes.","At small hole densities we find that the helical spin pattern decays exponentially, with a pitch dependence that agrees with the experiments.","At large hole densities we instead find persistent oscillations.","While our analytic approach does not generalize to the $XXZ$ model with arbitrary anisotropies, we validate a matrix product state technique which might be used to model the experiments in those settings."],"url":"http://arxiv.org/abs/2404.17558v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-26 17:34:40","title":"Curvature Correlators in Nonperturbative 2D Lorentzian Quantum Gravity","abstract":"Correlation functions are ubiquitous tools in quantum field theory from both a fundamental and a practical point of view. However, up to now their use in theories of quantum gravity beyond perturbative and asymptotically flat regimes has been limited, due to difficulties associated with diffeomorphism invariance and the dynamical nature of geometry. We present an analysis of a manifestly diffeomorphism-invariant, nonperturbative two-point curvature correlator in two-dimensional Lorentzian quantum gravity. It is based on the recently introduced quantum Ricci curvature and uses a lattice regularization of the full path integral in terms of causal dynamical triangulations. We discuss some of the subtleties and ambiguities in defining connected correlators in theories of dynamical geometry, and provide strong evidence from Monte Carlo simulations that the connected two-point curvature correlator in 2D Lorentzian quantum gravity vanishes. This work paves the way for an analogous investigation in higher dimensions.","sentences":["Correlation functions are ubiquitous tools in quantum field theory from both a fundamental and a practical point of view.","However, up to now their use in theories of quantum gravity beyond perturbative and asymptotically flat regimes has been limited, due to difficulties associated with diffeomorphism invariance and the dynamical nature of geometry.","We present an analysis of a manifestly diffeomorphism-invariant, nonperturbative two-point curvature correlator in two-dimensional Lorentzian quantum gravity.","It is based on the recently introduced quantum Ricci curvature and uses a lattice regularization of the full path integral in terms of causal dynamical triangulations.","We discuss some of the subtleties and ambiguities in defining connected correlators in theories of dynamical geometry, and provide strong evidence from Monte Carlo simulations that the connected two-point curvature correlator in 2D Lorentzian quantum gravity vanishes.","This work paves the way for an analogous investigation in higher dimensions."],"url":"http://arxiv.org/abs/2404.17556v1","category":"hep-th"}
{"created":"2024-04-26 17:31:41","title":"Federated Transfer Component Analysis Towards Effective VNF Profiling","abstract":"The increasing concerns of knowledge transfer and data privacy challenge the traditional gather-and-analyse paradigm in networks. Specifically, the intelligent orchestration of Virtual Network Functions (VNFs) requires understanding and profiling the resource consumption. However, profiling all kinds of VNFs is time-consuming. It is important to consider transferring the well-profiled VNF knowledge to other lack-profiled VNF types while keeping data private. To this end, this paper proposes a Federated Transfer Component Analysis (FTCA) method between the source and target VNFs. FTCA first trains Generative Adversarial Networks (GANs) based on the source VNF profiling data, and the trained GANs model is sent to the target VNF domain. Then, FTCA realizes federated domain adaptation by using the generated source VNF data and less target VNF profiling data, while keeping the raw data locally. Experiments show that the proposed FTCA can effectively predict the required resources for the target VNF. Specifically, the RMSE index of the regression model decreases by 38.5% and the R-squared metric advances up to 68.6%.","sentences":["The increasing concerns of knowledge transfer and data privacy challenge the traditional gather-and-analyse paradigm in networks.","Specifically, the intelligent orchestration of Virtual Network Functions (VNFs) requires understanding and profiling the resource consumption.","However, profiling all kinds of VNFs is time-consuming.","It is important to consider transferring the well-profiled VNF knowledge to other lack-profiled VNF types while keeping data private.","To this end, this paper proposes a Federated Transfer Component Analysis (FTCA) method between the source and target VNFs.","FTCA first trains Generative Adversarial Networks (GANs) based on the source VNF profiling data, and the trained GANs model is sent to the target VNF domain.","Then, FTCA realizes federated domain adaptation by using the generated source VNF data and less target VNF profiling data, while keeping the raw data locally.","Experiments show that the proposed FTCA can effectively predict the required resources for the target VNF.","Specifically, the RMSE index of the regression model decreases by 38.5% and the R-squared metric advances up to 68.6%."],"url":"http://arxiv.org/abs/2404.17553v1","category":"cs.DC"}
{"created":"2024-04-26 17:27:39","title":"The Role of Marketing in Public Policy Decision Making: The Case of Fuel Subsidy Removal in Nigeria","abstract":"Public policy decision making has become more complex and complicated in recent times. Some authors have attributed this to the fact that public policy decision makers now have more variables to consider in every decision more than ever before. Others have argued that the rate of civilization, globalization and information technology has made the public to be more enlightened and abreast with the activities of government and so can oppose government decisions if they are unfavourable. This tends to increase government need for more and better information in order to satisfy the public. Consequently, this paper examined the issue of fuel subsidy removal in Nigeria, the impact of the policy on the public as well as the country and the role marketing principles would have played if the Nigerian government had taken some time to investigate what should be done, how it should be done and when it should be done. It also proposed a roadmap for future policies that have direct implications for the general public.","sentences":["Public policy decision making has become more complex and complicated in recent times.","Some authors have attributed this to the fact that public policy decision makers now have more variables to consider in every decision more than ever before.","Others have argued that the rate of civilization, globalization and information technology has made the public to be more enlightened and abreast with the activities of government and so can oppose government decisions if they are unfavourable.","This tends to increase government need for more and better information in order to satisfy the public.","Consequently, this paper examined the issue of fuel subsidy removal in Nigeria, the impact of the policy on the public as well as the country and the role marketing principles would have played if the Nigerian government had taken some time to investigate what should be done, how it should be done and when it should be done.","It also proposed a roadmap for future policies that have direct implications for the general public."],"url":"http://arxiv.org/abs/2404.17551v1","category":"econ.GN"}
{"created":"2024-04-26 17:27:32","title":"CoCar NextGen: a Multi-Purpose Platform for Connected Autonomous Driving Research","abstract":"Real world testing is of vital importance to the success of automated driving. While many players in the business design purpose build testing vehicles, we designed and build a modular platform that offers high flexibility for any kind of scenario. CoCar NextGen is equipped with next generation hardware that addresses all future use cases. Its extensive, redundant sensor setup allows to develop cross-domain data driven approaches that manage the transfer to other sensor setups. Together with the possibility of being deployed on public roads, this creates a unique research platform that supports the road to automated driving on SAE Level 5.","sentences":["Real world testing is of vital importance to the success of automated driving.","While many players in the business design purpose build testing vehicles, we designed and build a modular platform that offers high flexibility for any kind of scenario.","CoCar NextGen is equipped with next generation hardware that addresses all future use cases.","Its extensive, redundant sensor setup allows to develop cross-domain data driven approaches that manage the transfer to other sensor setups.","Together with the possibility of being deployed on public roads, this creates a unique research platform that supports the road to automated driving on SAE Level 5."],"url":"http://arxiv.org/abs/2404.17550v1","category":"cs.RO"}
{"created":"2024-04-26 17:18:32","title":"Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo","abstract":"Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.","sentences":["Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence.","In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems.","In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences.","We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning.","As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function.","These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions.","We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks."],"url":"http://arxiv.org/abs/2404.17546v1","category":"cs.LG"}
{"created":"2024-04-26 17:13:02","title":"Applications of Lifted Nonlinear Cuts to Convex Relaxations of the AC Power Flow Equations","abstract":"We demonstrate that valid inequalities, or lifted nonlinear cuts (LNC), can be projected to tighten the Second Order Cone (SOC), Convex DistFlow (CDF), and Network Flow (NF) relaxations of the AC Optimal Power Flow (AC-OPF) problem. We conduct experiments on 36 cases from the PGLib-OPF library for two objective functions, (1) power generation maximization and (2) generation cost minimization. Significant optimality gap improvements are shown for the maximization problem, where the LNC strengthen the SOC and CDF relaxations in 100% of the test cases, with average and maximum differences in the optimality gaps of 23.1% and 93.5% respectively. The NF relaxation is strengthened in 79.2% of test cases, with average and maximum differences in the optimality gaps of 3.45% and 21.2% respectively. We also study the trade-off between relaxation quality and solve time, demonstrating that the strengthened CDF relaxation outperforms the strengthened SOC formulation in terms of runtime and number of iterations needed, while the strengthened NF formulation is the most scalable with the lowest relaxation quality provided by these LNC.","sentences":["We demonstrate that valid inequalities, or lifted nonlinear cuts (LNC), can be projected to tighten the Second Order Cone (SOC), Convex DistFlow (CDF), and Network Flow (NF) relaxations of the AC Optimal Power Flow (AC-OPF) problem.","We conduct experiments on 36 cases from the PGLib-OPF library for two objective functions, (1) power generation maximization and (2) generation cost minimization.","Significant optimality gap improvements are shown for the maximization problem, where the LNC strengthen the SOC and CDF relaxations in 100% of the test cases, with average and maximum differences in the optimality gaps of 23.1% and 93.5% respectively.","The NF relaxation is strengthened in 79.2% of test cases, with average and maximum differences in the optimality gaps of 3.45% and 21.2% respectively.","We also study the trade-off between relaxation quality and solve time, demonstrating that the strengthened CDF relaxation outperforms the strengthened SOC formulation in terms of runtime and number of iterations needed, while the strengthened NF formulation is the most scalable with the lowest relaxation quality provided by these LNC."],"url":"http://arxiv.org/abs/2404.17541v1","category":"math.OC"}
{"created":"2024-04-26 17:09:59","title":"Schwarz Modular Operads Revisited: $\\mathcal{SM}=\\mathcal{S}\\circ\\mathcal{M}$","abstract":"We prove that the Feynman category encoding Schwarz's variant of modular operads is Koszul. Our proof uses a generalization of the theory of distributive laws to the groupoid colored setting.","sentences":["We prove that the Feynman category encoding Schwarz's variant of modular operads is Koszul.","Our proof uses a generalization of the theory of distributive laws to the groupoid colored setting."],"url":"http://arxiv.org/abs/2404.17540v1","category":"math.AT"}
{"created":"2024-04-26 17:03:14","title":"Artinian rings which are not generalized Rickart","abstract":"In this note, we show that there exist non-unital right artinian rings which are not generalized Rickart. In particular, we provide examples to show that, [16, Corollary 2.31] is not true for non-unital artinian rings.","sentences":["In this note, we show that there exist non-unital right artinian rings which are not generalized Rickart.","In particular, we provide examples to show that, [16, Corollary 2.31] is not true for non-unital artinian rings."],"url":"http://arxiv.org/abs/2404.17537v1","category":"math.RA"}
{"created":"2024-04-26 17:02:43","title":"Besicovitch's 1/2 problem and linear programming","abstract":"We consider the following classical conjecture of Besicovitch: a $1$-dimensional Borel set in the plane with finite Hausdorff $1$-dimensional measure $\\mathcal{H}^1$ which has lower density strictly larger than $\\frac{1}{2}$ almost everywhere must be countably rectifiable. We improve the best known bound, due to Preiss and Ti\\v{s}er, showing that the statement is indeed true if $\\frac{1}{2}$ is replaced by $\\frac{7}{10}$ (in fact we improve the Preiss-Ti\\v{s}er bound even for the corresponding statement in general metric spaces). More importantly, we propose a family of variational problems to produce the latter and many other similar bounds and we study several properties of them, paving the way for further improvements.","sentences":["We consider the following classical conjecture of Besicovitch: a $1$-dimensional Borel set in the plane with finite Hausdorff $1$-dimensional measure $\\mathcal{H}^1$ which has lower density strictly larger than $\\frac{1}{2}$ almost everywhere must be countably rectifiable.","We improve the best known bound, due to Preiss and Ti\\v{s}er, showing that the statement is indeed true if $\\frac{1}{2}$ is replaced by $\\frac{7}{10}$ (in fact we improve the Preiss-Ti\\v{s}er bound even for the corresponding statement in general metric spaces).","More importantly, we propose a family of variational problems to produce the latter and many other similar bounds and we study several properties of them, paving the way for further improvements."],"url":"http://arxiv.org/abs/2404.17536v1","category":"math.CA"}
{"created":"2024-04-26 17:01:38","title":"Using Neural Implicit Flow To Represent Latent Dynamics Of Canonical Systems","abstract":"The recently introduced class of architectures known as Neural Operators has emerged as highly versatile tools applicable to a wide range of tasks in the field of Scientific Machine Learning (SciML), including data representation and forecasting. In this study, we investigate the capabilities of Neural Implicit Flow (NIF), a recently developed mesh-agnostic neural operator, for representing the latent dynamics of canonical systems such as the Kuramoto-Sivashinsky (KS), forced Korteweg-de Vries (fKdV), and Sine-Gordon (SG) equations, as well as for extracting dynamically relevant information from them. Finally we assess the applicability of NIF as a dimensionality reduction algorithm and conduct a comparative analysis with another widely recognized family of neural operators, known as Deep Operator Networks (DeepONets).","sentences":["The recently introduced class of architectures known as Neural Operators has emerged as highly versatile tools applicable to a wide range of tasks in the field of Scientific Machine Learning (SciML), including data representation and forecasting.","In this study, we investigate the capabilities of Neural Implicit Flow (NIF), a recently developed mesh-agnostic neural operator, for representing the latent dynamics of canonical systems such as the Kuramoto-Sivashinsky (KS), forced Korteweg-de Vries (fKdV), and Sine-Gordon (SG) equations, as well as for extracting dynamically relevant information from them.","Finally we assess the applicability of NIF as a dimensionality reduction algorithm and conduct a comparative analysis with another widely recognized family of neural operators, known as Deep Operator Networks (DeepONets)."],"url":"http://arxiv.org/abs/2404.17535v1","category":"cs.LG"}
{"created":"2024-04-26 16:59:26","title":"Exploring the Distinctiveness and Fidelity of the Descriptions Generated by Large Vision-Language Models","abstract":"Large Vision-Language Models (LVLMs) are gaining traction for their remarkable ability to process and integrate visual and textual data. Despite their popularity, the capacity of LVLMs to generate precise, fine-grained textual descriptions has not been fully explored. This study addresses this gap by focusing on \\textit{distinctiveness} and \\textit{fidelity}, assessing how models like Open-Flamingo, IDEFICS, and MiniGPT-4 can distinguish between similar objects and accurately describe visual features. We proposed the Textual Retrieval-Augmented Classification (TRAC) framework, which, by leveraging its generative capabilities, allows us to delve deeper into analyzing fine-grained visual description generation. This research provides valuable insights into the generation quality of LVLMs, enhancing the understanding of multimodal language models. Notably, MiniGPT-4 stands out for its better ability to generate fine-grained descriptions, outperforming the other two models in this aspect. The code is provided at \\url{https://anonymous.4open.science/r/Explore_FGVDs-E277}.","sentences":["Large Vision-Language Models (LVLMs) are gaining traction for their remarkable ability to process and integrate visual and textual data.","Despite their popularity, the capacity of LVLMs to generate precise, fine-grained textual descriptions has not been fully explored.","This study addresses this gap by focusing on \\textit{distinctiveness} and \\textit{fidelity}, assessing how models like Open-Flamingo, IDEFICS, and MiniGPT-4 can distinguish between similar objects and accurately describe visual features.","We proposed the Textual Retrieval-Augmented Classification (TRAC) framework, which, by leveraging its generative capabilities, allows us to delve deeper into analyzing fine-grained visual description generation.","This research provides valuable insights into the generation quality of LVLMs, enhancing the understanding of multimodal language models.","Notably, MiniGPT-4 stands out for its better ability to generate fine-grained descriptions, outperforming the other two models in this aspect.","The code is provided at \\url{https://anonymous.4open.science/r/Explore_FGVDs-E277}."],"url":"http://arxiv.org/abs/2404.17534v1","category":"cs.CV"}
{"created":"2024-04-26 16:57:52","title":"Rigidity of spin fill-ins with non-negative scalar curvature","abstract":"We establish new mean curvature rigidity theorems of spin fill-ins with non-negative scalar curvature using two different spinorial techniques. Our results address two questions by Miao and Gromov, respectively. The first technique is based on extending boundary spinors satisfying a generalized eigenvalue equation via the Fredholm alternative for an APS boundary value problem, while the second is a comparison result in the spirit of Llarull and Lott using index theory. We also show that the latter implies a new Witten-type integral inequality for the mass of an asymptotically Schwarzschild manifold which holds even when the scalar curvature is not assumed to be non-negative.","sentences":["We establish new mean curvature rigidity theorems of spin fill-ins with non-negative scalar curvature using two different spinorial techniques.","Our results address two questions by Miao and Gromov, respectively.","The first technique is based on extending boundary spinors satisfying a generalized eigenvalue equation via the Fredholm alternative for an APS boundary value problem, while the second is a comparison result in the spirit of Llarull and Lott using index theory.","We also show that the latter implies a new Witten-type integral inequality for the mass of an asymptotically Schwarzschild manifold which holds even when the scalar curvature is not assumed to be non-negative."],"url":"http://arxiv.org/abs/2404.17533v1","category":"math.DG"}
{"created":"2024-04-26 16:47:42","title":"Kaledin classes and formality criteria","abstract":"We develop a general obstruction theory to the formality of algebraic structures over any commutative ground ring. It relies on the construction of Kaledin obstruction classes that faithfully detect the formality of differential graded algebras over operads or properads, possibly colored in groupoids. The present treatment generalizes the previous obstruction classes in two directions: outside characteristic zero and including a wider range of algebraic structures. This enables us to establish novel formality criteria, including formality descent with torsion coefficients, formality in families, intrinsic formality, and criteria in terms of chain-level lifts of homology automorphism.","sentences":["We develop a general obstruction theory to the formality of algebraic structures over any commutative ground ring.","It relies on the construction of Kaledin obstruction classes that faithfully detect the formality of differential graded algebras over operads or properads, possibly colored in groupoids.","The present treatment generalizes the previous obstruction classes in two directions: outside characteristic zero and including a wider range of algebraic structures.","This enables us to establish novel formality criteria, including formality descent with torsion coefficients, formality in families, intrinsic formality, and criteria in terms of chain-level lifts of homology automorphism."],"url":"http://arxiv.org/abs/2404.17529v1","category":"math.AT"}
{"created":"2024-04-26 16:46:28","title":"Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields","abstract":"Generalizable NeRF aims to synthesize novel views for unseen scenes. Common practices involve constructing variance-based cost volumes for geometry reconstruction and encoding 3D descriptors for decoding novel views. However, existing methods show limited generalization ability in challenging conditions due to inaccurate geometry, sub-optimal descriptors, and decoding strategies. We address these issues point by point. First, we find the variance-based cost volume exhibits failure patterns as the features of pixels corresponding to the same point can be inconsistent across different views due to occlusions or reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to amplify the contribution of consistent pixel pairs and suppress inconsistent ones. Unlike previous methods that solely fuse 2D features into descriptors, our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D context into descriptors through spatial and inter-view interaction. When decoding the descriptors, we observe the two existing decoding strategies excel in different areas, which are complementary. A Consistency-Aware Fusion (CAF) strategy is proposed to leverage the advantages of both. We incorporate the above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains state-of-the-art performance across multiple datasets. Code is available at https://github.com/TQTQliu/GeFu .","sentences":["Generalizable NeRF aims to synthesize novel views for unseen scenes.","Common practices involve constructing variance-based cost volumes for geometry reconstruction and encoding 3D descriptors for decoding novel views.","However, existing methods show limited generalization ability in challenging conditions due to inaccurate geometry, sub-optimal descriptors, and decoding strategies.","We address these issues point by point.","First, we find the variance-based cost volume exhibits failure patterns as the features of pixels corresponding to the same point can be inconsistent across different views due to occlusions or reflections.","We introduce an Adaptive Cost Aggregation (ACA) approach to amplify the contribution of consistent pixel pairs and suppress inconsistent ones.","Unlike previous methods that solely fuse 2D features into descriptors, our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D context into descriptors through spatial and inter-view interaction.","When decoding the descriptors, we observe the two existing decoding strategies excel in different areas, which are complementary.","A Consistency-Aware Fusion (CAF) strategy is proposed to leverage the advantages of both.","We incorporate the above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu).","GeFu attains state-of-the-art performance across multiple datasets.","Code is available at https://github.com/TQTQliu/GeFu ."],"url":"http://arxiv.org/abs/2404.17528v1","category":"cs.CV"}
{"created":"2024-04-26 16:41:24","title":"Large Language Model Agent as a Mechanical Designer","abstract":"Conventional mechanical design paradigms rely on experts systematically refining concepts through experience-guided modification and FEA to meet specific requirements. However, this approach can be time-consuming and heavily dependent on prior knowledge and experience. While numerous machine learning models have been developed to streamline this intensive and expert-driven iterative process, these methods typically demand extensive training data and considerable computational resources. Furthermore, methods based on deep learning are usually restricted to the specific domains and tasks for which they were trained, limiting their applicability across different tasks. This creates a trade-off between the efficiency of automation and the demand for resources. In this study, we present a novel approach that integrates pre-trained LLMs with a FEM module. The FEM module evaluates each design and provides essential feedback, guiding the LLMs to continuously learn, plan, generate, and optimize designs without the need for domain-specific training. We demonstrate the effectiveness of our proposed framework in managing the iterative optimization of truss structures, showcasing its capability to reason about and refine designs according to structured feedback and criteria. Our results reveal that these LLM-based agents can successfully generate truss designs that comply with natural language specifications with a success rate of up to 90%, which varies according to the applied constraints. By employing prompt-based optimization techniques we show that LLM based agents exhibit optimization behavior when provided with solution-score pairs to iteratively refine designs to meet specifications. This ability of LLM agents to produce viable designs and optimize them based on their inherent reasoning capabilities highlights their potential to develop and implement effective design strategies autonomously.","sentences":["Conventional mechanical design paradigms rely on experts systematically refining concepts through experience-guided modification and FEA to meet specific requirements.","However, this approach can be time-consuming and heavily dependent on prior knowledge and experience.","While numerous machine learning models have been developed to streamline this intensive and expert-driven iterative process, these methods typically demand extensive training data and considerable computational resources.","Furthermore, methods based on deep learning are usually restricted to the specific domains and tasks for which they were trained, limiting their applicability across different tasks.","This creates a trade-off between the efficiency of automation and the demand for resources.","In this study, we present a novel approach that integrates pre-trained LLMs with a FEM module.","The FEM module evaluates each design and provides essential feedback, guiding the LLMs to continuously learn, plan, generate, and optimize designs without the need for domain-specific training.","We demonstrate the effectiveness of our proposed framework in managing the iterative optimization of truss structures, showcasing its capability to reason about and refine designs according to structured feedback and criteria.","Our results reveal that these LLM-based agents can successfully generate truss designs that comply with natural language specifications with a success rate of up to 90%, which varies according to the applied constraints.","By employing prompt-based optimization techniques we show that LLM based agents exhibit optimization behavior when provided with solution-score pairs to iteratively refine designs to meet specifications.","This ability of LLM agents to produce viable designs and optimize them based on their inherent reasoning capabilities highlights their potential to develop and implement effective design strategies autonomously."],"url":"http://arxiv.org/abs/2404.17525v1","category":"cs.LG"}
{"created":"2024-04-26 16:41:00","title":"On the Use of Large Language Models to Generate Capability Ontologies","abstract":"Capability ontologies are increasingly used to model functionalities of systems or machines. The creation of such ontological models with all properties and constraints of capabilities is very complex and can only be done by ontology experts. However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts. Therefore, this paper investigates how LLMs can be used to create capability ontologies. We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs. Errors in the generated ontologies are recorded and compared. To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used. The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors.","sentences":["Capability ontologies are increasingly used to model functionalities of systems or machines.","The creation of such ontological models with all properties and constraints of capabilities is very complex and can only be done by ontology experts.","However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts.","Therefore, this paper investigates how LLMs can be used to create capability ontologies.","We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs.","Errors in the generated ontologies are recorded and compared.","To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used.","The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors."],"url":"http://arxiv.org/abs/2404.17524v1","category":"cs.AI"}
{"created":"2024-04-26 16:40:49","title":"Enhancing Legal Compliance and Regulation Analysis with Large Language Models","abstract":"This research explores the application of Large Language Models (LLMs) for automating the extraction of requirement-related legal content in the food safety domain and checking legal compliance of regulatory artifacts. With Industry 4.0 revolutionizing the food industry and with the General Data Protection Regulation (GDPR) reshaping privacy policies and data processing agreements, there is a growing gap between regulatory analysis and recent technological advancements. This study aims to bridge this gap by leveraging LLMs, namely BERT and GPT models, to accurately classify legal provisions and automate compliance checks. Our findings demonstrate promising results, indicating LLMs' significant potential to enhance legal compliance and regulatory analysis efficiency, notably by reducing manual workload and improving accuracy within reasonable time and financial constraints.","sentences":["This research explores the application of Large Language Models (LLMs) for automating the extraction of requirement-related legal content in the food safety domain and checking legal compliance of regulatory artifacts.","With Industry 4.0 revolutionizing the food industry and with the General Data Protection Regulation (GDPR) reshaping privacy policies and data processing agreements, there is a growing gap between regulatory analysis and recent technological advancements.","This study aims to bridge this gap by leveraging LLMs, namely BERT and GPT models, to accurately classify legal provisions and automate compliance checks.","Our findings demonstrate promising results, indicating LLMs' significant potential to enhance legal compliance and regulatory analysis efficiency, notably by reducing manual workload and improving accuracy within reasonable time and financial constraints."],"url":"http://arxiv.org/abs/2404.17522v1","category":"cs.SE"}
{"created":"2024-04-26 16:40:01","title":"A Cognitive-Driven Trajectory Prediction Model for Autonomous Driving in Mixed Autonomy Environment","abstract":"As autonomous driving technology progresses, the need for precise trajectory prediction models becomes paramount. This paper introduces an innovative model that infuses cognitive insights into trajectory prediction, focusing on perceived safety and dynamic decision-making. Distinct from traditional approaches, our model excels in analyzing interactions and behavior patterns in mixed autonomy traffic scenarios. It represents a significant leap forward, achieving marked performance improvements on several key datasets. Specifically, it surpasses existing benchmarks with gains of 16.2% on the Next Generation Simulation (NGSIM), 27.4% on the Highway Drone (HighD), and 19.8% on the Macao Connected Autonomous Driving (MoCAD) dataset. Our proposed model shows exceptional proficiency in handling corner cases, essential for real-world applications. Moreover, its robustness is evident in scenarios with missing or limited data, outperforming most of the state-of-the-art baselines. This adaptability and resilience position our model as a viable tool for real-world autonomous driving systems, heralding a new standard in vehicle trajectory prediction for enhanced safety and efficiency.","sentences":["As autonomous driving technology progresses, the need for precise trajectory prediction models becomes paramount.","This paper introduces an innovative model that infuses cognitive insights into trajectory prediction, focusing on perceived safety and dynamic decision-making.","Distinct from traditional approaches, our model excels in analyzing interactions and behavior patterns in mixed autonomy traffic scenarios.","It represents a significant leap forward, achieving marked performance improvements on several key datasets.","Specifically, it surpasses existing benchmarks with gains of 16.2% on the Next Generation Simulation (NGSIM), 27.4% on the Highway Drone (HighD), and 19.8% on the Macao Connected Autonomous Driving (MoCAD) dataset.","Our proposed model shows exceptional proficiency in handling corner cases, essential for real-world applications.","Moreover, its robustness is evident in scenarios with missing or limited data, outperforming most of the state-of-the-art baselines.","This adaptability and resilience position our model as a viable tool for real-world autonomous driving systems, heralding a new standard in vehicle trajectory prediction for enhanced safety and efficiency."],"url":"http://arxiv.org/abs/2404.17520v1","category":"cs.RO"}
{"created":"2024-04-26 16:32:40","title":"On the impossibility of certain $({n^2+n+k}_{n+1})$ configurations","abstract":"This paper investigates the impossibility of certain $({n^2+n+k}_{n+1})$ configurations. Firstly, for $k=2$, the result of \\cite{gropp1992non} that $\\frac{n^2+n}{2}$ is even and $n+1$ is a perfect square or $\\frac{n^2+n}{2}$ is odd and $n-1$ is a perfect square is reproved using the incidence matrix $N$ and analysing the form of $N^TN$. Then, for all $k$, configurations where paralellism is a transitive property are considered. It is then analogously established that if $n\\equiv0$ or $n\\equiv k-1$ mod $k$ for $k$ even, then $\\frac{n^2+n}{k}$ is even and $n+1$ is a perfect square or $\\frac{n^2+n}{k}$ is odd and $n-(k-1)$ is a perfect square. Finally, the case $k=3$ is investigated in full generality.","sentences":["This paper investigates the impossibility of certain $({n^2+n+k}_{n+1})$ configurations.","Firstly, for $k=2$, the result of \\cite{gropp1992non} that $\\frac{n^2+n}{2}$ is even and $n+1$ is a perfect square or $\\frac{n^2+n}{2}$ is odd and $n-1$ is a perfect square is reproved using the incidence matrix $N$ and analysing the form of $N^TN$. Then, for all $k$, configurations where paralellism is a transitive property are considered.","It is then analogously established that if $n\\equiv0$ or $n\\equiv k-1$ mod $k$ for $k$ even, then $\\frac{n^2+n}{k}$ is even and $n+1$ is a perfect square or $\\frac{n^2+n}{k}$ is odd and $n-(k-1)$ is a perfect square.","Finally, the case $k=3$ is investigated in full generality."],"url":"http://arxiv.org/abs/2404.17514v1","category":"math.CO"}
{"created":"2024-04-26 16:28:34","title":"A Comprehensive Evaluation on Event Reasoning of Large Language Models","abstract":"Event reasoning is a fundamental ability that underlies many applications. It requires event schema knowledge to perform global reasoning and needs to deal with the diversity of the inter-event relations and the reasoning paradigms. How well LLMs accomplish event reasoning on various relations and reasoning paradigms remains unknown. To mitigate this disparity, we comprehensively evaluate the abilities of event reasoning of LLMs. We introduce a novel benchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of evaluation of schema and instance and is comprehensive in relations and reasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs have abilities to accomplish event reasoning but their performances are far from satisfactory. We also notice the imbalance of event reasoning abilities in LLMs. Besides, LLMs have event schema knowledge, however, they're not aligned with humans on how to utilize the knowledge. Based on these findings, we introduce two methods to guide the LLMs to utilize the event schema knowledge. Both methods achieve improvements.","sentences":["Event reasoning is a fundamental ability that underlies many applications.","It requires event schema knowledge to perform global reasoning and needs to deal with the diversity of the inter-event relations and the reasoning paradigms.","How well LLMs accomplish event reasoning on various relations and reasoning paradigms remains unknown.","To mitigate this disparity, we comprehensively evaluate the abilities of event reasoning of LLMs.","We introduce a novel benchmark EV2 for EValuation of EVent reasoning.","EV2 consists of two levels of evaluation of schema and instance and is comprehensive in relations and reasoning paradigms.","We conduct extensive experiments on EV2.","We find that LLMs have abilities to accomplish event reasoning but their performances are far from satisfactory.","We also notice the imbalance of event reasoning abilities in LLMs.","Besides, LLMs have event schema knowledge, however, they're not aligned with humans on how to utilize the knowledge.","Based on these findings, we introduce two methods to guide the LLMs to utilize the event schema knowledge.","Both methods achieve improvements."],"url":"http://arxiv.org/abs/2404.17513v1","category":"cs.CL"}
{"created":"2024-04-26 16:27:24","title":"On the spectral edge of non-Hermitian random matrices","abstract":"For general non-Hermitian random matrices $X$ and deterministic deformation matrices $A$, we prove that the local eigenvalue statistics of $A+X$ close to the typical edge points of its spectrum are universal. Furthermore, we show that under natural assumptions on $A$ the spectrum of $A+X$ does not have outliers at a distance larger than the natural fluctuation scale of the eigenvalues. As a consequence, the number of eigenvalues in each component of $\\mathrm{Spec}(A+X)$ is deterministic.","sentences":["For general non-Hermitian random matrices $X$ and deterministic deformation matrices $A$, we prove that the local eigenvalue statistics of $A+X$ close to the typical edge points of its spectrum are universal.","Furthermore, we show that under natural assumptions on $A$ the spectrum of $A+X$ does not have outliers at a distance larger than the natural fluctuation scale of the eigenvalues.","As a consequence, the number of eigenvalues in each component of $\\mathrm{Spec}(A+X)$ is deterministic."],"url":"http://arxiv.org/abs/2404.17512v1","category":"math.PR"}
{"created":"2024-04-26 16:12:53","title":"Phase diagram of generalized XY model using tensor renormalization group","abstract":"We use the higher-order tensor renormalization group method to study the two-dimensional generalized XY model that admits integer and half-integer vortices. This model is the deformation of the classical XY model and has a rich phase structure consisting of nematic, ferromagnetic, and disordered phases and three transition lines belonging to the Berezinskii-Kosterlitz-Thouless and Ising class. We explore the model for a wide range of temperatures, $T$, and the deformation parameter, $\\Delta$, and compute specific heat along with integer and half-integer magnetic susceptibility, finding both BKT-like and Ising-like transitions and the region where they meet.","sentences":["We use the higher-order tensor renormalization group method to study the two-dimensional generalized XY model that admits integer and half-integer vortices.","This model is the deformation of the classical XY model and has a rich phase structure consisting of nematic, ferromagnetic, and disordered phases and three transition lines belonging to the Berezinskii-Kosterlitz-Thouless and Ising class.","We explore the model for a wide range of temperatures, $T$, and the deformation parameter, $\\Delta$, and compute specific heat along with integer and half-integer magnetic susceptibility, finding both BKT-like and Ising-like transitions and the region where they meet."],"url":"http://arxiv.org/abs/2404.17504v1","category":"hep-lat"}
{"created":"2024-04-26 15:57:37","title":"On the integrability of generalized $N$-center problems","abstract":"In this paper, we study the rational integrability of the $N$-center problem with rational weak and moderate forces. We show that the problem is not rationally integrable for all but a finite number of values $\\alpha\\in]0,2[$, where $\\alpha$ is the order of the singularities. We identify the remaining cases and give the necessary conditions for integrability.","sentences":["In this paper, we study the rational integrability of the $N$-center problem with rational weak and moderate forces.","We show that the problem is not rationally integrable for all but a finite number of values $\\alpha\\in]0,2[$, where $\\alpha$ is the order of the singularities.","We identify the remaining cases and give the necessary conditions for integrability."],"url":"http://arxiv.org/abs/2404.17500v1","category":"math.DS"}
{"created":"2024-04-26 15:51:59","title":"Merchants of Vulnerabilities: How Bug Bounty Programs Benefit Software Vendors","abstract":"Software vulnerabilities enable exploitation by malicious hackers, compromising systems and data security. This paper examines bug bounty programs (BBPs) that incentivize ethical hackers to discover and responsibly disclose vulnerabilities to software vendors. Using game-theoretic models, we capture the strategic interactions between software vendors, ethical hackers, and malicious hackers. First, our analysis shows that software vendors can increase expected profits by participating in BBPs, explaining their growing adoption and the success of BBP platforms. Second, we find that vendors with BBPs will release software earlier, albeit with more potential vulnerabilities, as BBPs enable coordinated vulnerability disclosure and mitigation. Third, the optimal number of ethical hackers to invite to a BBP depends solely on the expected number of malicious hackers seeking exploitation. This optimal number of ethical hackers is lower than but increases with the expected malicious hacker count. Finally, higher bounties incentivize ethical hackers to exert more effort, thereby increasing the probability that they will discover severe vulnerabilities first while reducing the success probability of malicious hackers. These findings highlight BBPs' potential benefits for vendors beyond profitability. Earlier software releases are enabled by managing risks through coordinated disclosure. As cybersecurity threats evolve, BBP adoption will likely gain momentum, providing vendors with a valuable tool for enhancing security posture and stakeholder trust. Moreover, BBPs envelop vulnerability identification and disclosure into new market relationships and transactions, impacting software vendors' incentives regarding product security choices like release timing.","sentences":["Software vulnerabilities enable exploitation by malicious hackers, compromising systems and data security.","This paper examines bug bounty programs (BBPs) that incentivize ethical hackers to discover and responsibly disclose vulnerabilities to software vendors.","Using game-theoretic models, we capture the strategic interactions between software vendors, ethical hackers, and malicious hackers.","First, our analysis shows that software vendors can increase expected profits by participating in BBPs, explaining their growing adoption and the success of BBP platforms.","Second, we find that vendors with BBPs will release software earlier, albeit with more potential vulnerabilities, as BBPs enable coordinated vulnerability disclosure and mitigation.","Third, the optimal number of ethical hackers to invite to a BBP depends solely on the expected number of malicious hackers seeking exploitation.","This optimal number of ethical hackers is lower than but increases with the expected malicious hacker count.","Finally, higher bounties incentivize ethical hackers to exert more effort, thereby increasing the probability that they will discover severe vulnerabilities first while reducing the success probability of malicious hackers.","These findings highlight BBPs' potential benefits for vendors beyond profitability.","Earlier software releases are enabled by managing risks through coordinated disclosure.","As cybersecurity threats evolve, BBP adoption will likely gain momentum, providing vendors with a valuable tool for enhancing security posture and stakeholder trust.","Moreover, BBPs envelop vulnerability identification and disclosure into new market relationships and transactions, impacting software vendors' incentives regarding product security choices like release timing."],"url":"http://arxiv.org/abs/2404.17497v1","category":"cs.CR"}
{"created":"2024-04-26 15:51:33","title":"From STEM-EDXS data to phase separation and quantification using physics-guided NMF","abstract":"We present the development of a new algorithm which combines state-of-the-art energy-dispersive X-ray (EDX) spectroscopy theory and a suitable machine learning formulation for the hyperspectral unmixing of scanning transmission electron microscope EDX spectrum images. The algorithm is based on non-negative matrix factorization (NMF) incorporating a physics-guided factorization model. It optimizes a Poisson likelihood, under additional simplex constraint together with user-chosen sparsity-inducing and smoothing regularizations, and is based on iterative multiplicative updates. The fluorescence of X-rays is fully modeled thanks to state-of-the-art theoretical work. It is shown that the output of the algorithm can be used for a direct chemical quantification. With this approach, it is straightforward to include a priori knowledge on the specimen such as the presence or absence of certain chemical elements in some of its phases. This work is implemented within two open-source Python packages, espm and emtables, which are used here for data simulation, data analysis and quantification. Using simulated data, we demonstrate that incorporating physical modeling in the decomposition helps retrieve meaningful components from spatially and spectrally mixed phases, even when the data are very noisy. For synthetic data with a higher signal, the regularizations yield a tenfold increase in the quality of the reconstructed abundance maps compared to standard NMF. Our approach is further validated on experimental data with a known ground truth, where state-of-the art results are achieved by using prior knowledge about the sample. Our model can be generalized to any other scanning spectroscopy techniques where underlying physical modeling can be linearized.","sentences":["We present the development of a new algorithm which combines state-of-the-art energy-dispersive X-ray (EDX) spectroscopy theory and a suitable machine learning formulation for the hyperspectral unmixing of scanning transmission electron microscope EDX spectrum images.","The algorithm is based on non-negative matrix factorization (NMF) incorporating a physics-guided factorization model.","It optimizes a Poisson likelihood, under additional simplex constraint together with user-chosen sparsity-inducing and smoothing regularizations, and is based on iterative multiplicative updates.","The fluorescence of X-rays is fully modeled thanks to state-of-the-art theoretical work.","It is shown that the output of the algorithm can be used for a direct chemical quantification.","With this approach, it is straightforward to include a priori knowledge on the specimen such as the presence or absence of certain chemical elements in some of its phases.","This work is implemented within two open-source Python packages, espm and emtables, which are used here for data simulation, data analysis and quantification.","Using simulated data, we demonstrate that incorporating physical modeling in the decomposition helps retrieve meaningful components from spatially and spectrally mixed phases, even when the data are very noisy.","For synthetic data with a higher signal, the regularizations yield a tenfold increase in the quality of the reconstructed abundance maps compared to standard NMF.","Our approach is further validated on experimental data with a known ground truth, where state-of-the art results are achieved by using prior knowledge about the sample.","Our model can be generalized to any other scanning spectroscopy techniques where underlying physical modeling can be linearized."],"url":"http://arxiv.org/abs/2404.17496v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-26 15:48:09","title":"Causally Abstracted Multi-armed Bandits","abstract":"Multi-armed bandits (MAB) and causal MABs (CMAB) are established frameworks for decision-making problems. The majority of prior work typically studies and solves individual MAB and CMAB in isolation for a given problem and associated data. However, decision-makers are often faced with multiple related problems and multi-scale observations where joint formulations are needed in order to efficiently exploit the problem structures and data dependencies. Transfer learning for CMABs addresses the situation where models are defined on identical variables, although causal connections may differ. In this work, we extend transfer learning to setups involving CMABs defined on potentially different variables, with varying degrees of granularity, and related via an abstraction map. Formally, we introduce the problem of causally abstracted MABs (CAMABs) by relying on the theory of causal abstraction in order to express a rigorous abstraction map. We propose algorithms to learn in a CAMAB, and study their regret. We illustrate the limitations and the strengths of our algorithms on a real-world scenario related to online advertising.","sentences":["Multi-armed bandits (MAB) and causal MABs (CMAB) are established frameworks for decision-making problems.","The majority of prior work typically studies and solves individual MAB and CMAB in isolation for a given problem and associated data.","However, decision-makers are often faced with multiple related problems and multi-scale observations where joint formulations are needed in order to efficiently exploit the problem structures and data dependencies.","Transfer learning for CMABs addresses the situation where models are defined on identical variables, although causal connections may differ.","In this work, we extend transfer learning to setups involving CMABs defined on potentially different variables, with varying degrees of granularity, and related via an abstraction map.","Formally, we introduce the problem of causally abstracted MABs (CAMABs) by relying on the theory of causal abstraction in order to express a rigorous abstraction map.","We propose algorithms to learn in a CAMAB, and study their regret.","We illustrate the limitations and the strengths of our algorithms on a real-world scenario related to online advertising."],"url":"http://arxiv.org/abs/2404.17493v1","category":"cs.LG"}
{"created":"2024-04-26 15:46:22","title":"Computationally Efficient Algorithms for Simulating Isotropic Gaussian Random Fields on Graphs with Euclidean Edges","abstract":"This work addresses the problem of simulating Gaussian random fields that are continuously indexed over a class of metric graphs, termed graphs with Euclidean edges, being more general and flexible than linear networks. We introduce three general algorithms that allow to reconstruct a wide spectrum of random fields having a covariance function that depends on a specific metric, called resistance metric, and proposed in recent literature. The algorithms are applied to a synthetic case study consisting of a street network. They prove to be fast and accurate in that they reproduce the target covariance function and provide random fields whose finite-dimensional distributions are approximately Gaussian.","sentences":["This work addresses the problem of simulating Gaussian random fields that are continuously indexed over a class of metric graphs, termed graphs with Euclidean edges, being more general and flexible than linear networks.","We introduce three general algorithms that allow to reconstruct a wide spectrum of random fields having a covariance function that depends on a specific metric, called resistance metric, and proposed in recent literature.","The algorithms are applied to a synthetic case study consisting of a street network.","They prove to be fast and accurate in that they reproduce the target covariance function and provide random fields whose finite-dimensional distributions are approximately Gaussian."],"url":"http://arxiv.org/abs/2404.17491v1","category":"math.ST"}
{"created":"2024-04-26 15:43:49","title":"Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation","abstract":"Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image and natural language data, thanks to the domain-specific augmentation techniques that are both intuitive and effective. Nonetheless, in tabular domain, the predominant augmentation technique for creating views is through corrupting tabular entries via swapping values, which is not as sound or effective. We propose a simple yet powerful improvement to this augmentation technique: corrupting tabular data conditioned on class identity. Specifically, when corrupting a specific tabular entry from an anchor row, instead of randomly sampling a value in the same feature column from the entire table uniformly, we only sample from rows that are identified to be within the same class as the anchor row. We assume the semi-supervised learning setting, and adopt the pseudo labeling technique for obtaining class identities over all table rows. We also explore the novel idea of selecting features to be corrupted based on feature correlation structures. Extensive experiments show that the proposed approach consistently outperforms the conventional corruption method for tabular data classification tasks. Our code is available at https://github.com/willtop/Tabular-Class-Conditioned-SSL.","sentences":["Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space.","Contrastive learning has witnessed success in image and natural language data, thanks to the domain-specific augmentation techniques that are both intuitive and effective.","Nonetheless, in tabular domain, the predominant augmentation technique for creating views is through corrupting tabular entries via swapping values, which is not as sound or effective.","We propose a simple yet powerful improvement to this augmentation technique: corrupting tabular data conditioned on class identity.","Specifically, when corrupting a specific tabular entry from an anchor row, instead of randomly sampling a value in the same feature column from the entire table uniformly, we only sample from rows that are identified to be within the same class as the anchor row.","We assume the semi-supervised learning setting, and adopt the pseudo labeling technique for obtaining class identities over all table rows.","We also explore the novel idea of selecting features to be corrupted based on feature correlation structures.","Extensive experiments show that the proposed approach consistently outperforms the conventional corruption method for tabular data classification tasks.","Our code is available at https://github.com/willtop/Tabular-Class-Conditioned-SSL."],"url":"http://arxiv.org/abs/2404.17489v1","category":"cs.LG"}
{"created":"2024-04-26 15:43:24","title":"Low Cost Machine Vision for Insect Classification","abstract":"Preserving the number and diversity of insects is one of our society's most important goals in the area of environmental sustainability. A prerequisite for this is a systematic and up-scaled monitoring in order to detect correlations and identify countermeasures. Therefore, automatized monitoring using live traps is important, but so far there is no system that provides image data of sufficient detailed information for entomological classification.   In this work, we present an imaging method as part of a multisensor system developed as a low-cost, scalable, open-source system that is adaptable to classical trap types. The image quality meets the requirements needed for classification in the taxonomic tree. Therefore, illumination and resolution have been optimized and motion artefacts have been suppressed. The system is evaluated exemplarily on a dataset consisting of 16 insect species of the same as well as different genus, family and order. We demonstrate that standard CNN-architectures like ResNet50 (pretrained on iNaturalist data) or MobileNet perform very well for the prediction task after re-training. Smaller custom made CNNs also lead to promising results. Classification accuracy of $>96\\%$ has been achieved. Moreover, it was proved that image cropping of insects is necessary for classification of species with high inter-class similarity.","sentences":["Preserving the number and diversity of insects is one of our society's most important goals in the area of environmental sustainability.","A prerequisite for this is a systematic and up-scaled monitoring in order to detect correlations and identify countermeasures.","Therefore, automatized monitoring using live traps is important, but so far there is no system that provides image data of sufficient detailed information for entomological classification.   ","In this work, we present an imaging method as part of a multisensor system developed as a low-cost, scalable, open-source system that is adaptable to classical trap types.","The image quality meets the requirements needed for classification in the taxonomic tree.","Therefore, illumination and resolution have been optimized and motion artefacts have been suppressed.","The system is evaluated exemplarily on a dataset consisting of 16 insect species of the same as well as different genus, family and order.","We demonstrate that standard CNN-architectures like ResNet50 (pretrained on iNaturalist data) or MobileNet perform very well for the prediction task after re-training.","Smaller custom made CNNs also lead to promising results.","Classification accuracy of $>96\\%$ has been achieved.","Moreover, it was proved that image cropping of insects is necessary for classification of species with high inter-class similarity."],"url":"http://arxiv.org/abs/2404.17488v1","category":"cs.CV"}
{"created":"2024-04-26 15:43:06","title":"Conformal Prediction with Learned Features","abstract":"In this paper, we focus on the problem of conformal prediction with conditional guarantees. Prior work has shown that it is impossible to construct nontrivial prediction sets with full conditional coverage guarantees. A wealth of research has considered relaxations of full conditional guarantees, relying on some predefined uncertainty structures. Departing from this line of thinking, we propose Partition Learning Conformal Prediction (PLCP), a framework to improve conditional validity of prediction sets through learning uncertainty-guided features from the calibration data. We implement PLCP efficiently with alternating gradient descent, utilizing off-the-shelf machine learning models. We further analyze PLCP theoretically and provide conditional guarantees for infinite and finite sample sizes. Finally, our experimental results over four real-world and synthetic datasets show the superior performance of PLCP compared to state-of-the-art methods in terms of coverage and length in both classification and regression scenarios.","sentences":["In this paper, we focus on the problem of conformal prediction with conditional guarantees.","Prior work has shown that it is impossible to construct nontrivial prediction sets with full conditional coverage guarantees.","A wealth of research has considered relaxations of full conditional guarantees, relying on some predefined uncertainty structures.","Departing from this line of thinking, we propose Partition Learning Conformal Prediction (PLCP), a framework to improve conditional validity of prediction sets through learning uncertainty-guided features from the calibration data.","We implement PLCP efficiently with alternating gradient descent, utilizing off-the-shelf machine learning models.","We further analyze PLCP theoretically and provide conditional guarantees for infinite and finite sample sizes.","Finally, our experimental results over four real-world and synthetic datasets show the superior performance of PLCP compared to state-of-the-art methods in terms of coverage and length in both classification and regression scenarios."],"url":"http://arxiv.org/abs/2404.17487v1","category":"cs.LG"}
{"created":"2024-04-26 15:42:24","title":"TextGaze: Gaze-Controllable Face Generation with Natural Language","abstract":"Generating face image with specific gaze information has attracted considerable attention. Existing approaches typically input gaze values directly for face generation, which is unnatural and requires annotated gaze datasets for training, thereby limiting its application. In this paper, we present a novel gaze-controllable face generation task. Our approach inputs textual descriptions that describe human gaze and head behavior and generates corresponding face images. Our work first introduces a text-of-gaze dataset containing over 90k text descriptions spanning a dense distribution of gaze and head poses. We further propose a gaze-controllable text-to-face method. Our method contains a sketch-conditioned face diffusion module and a model-based sketch diffusion module. We define a face sketch based on facial landmarks and eye segmentation map. The face diffusion module generates face images from the face sketch, and the sketch diffusion module employs a 3D face model to generate face sketch from text description. Experiments on the FFHQ dataset show the effectiveness of our method. We will release our dataset and code for future research.","sentences":["Generating face image with specific gaze information has attracted considerable attention.","Existing approaches typically input gaze values directly for face generation, which is unnatural and requires annotated gaze datasets for training, thereby limiting its application.","In this paper, we present a novel gaze-controllable face generation task.","Our approach inputs textual descriptions that describe human gaze and head behavior and generates corresponding face images.","Our work first introduces a text-of-gaze dataset containing over 90k text descriptions spanning a dense distribution of gaze and head poses.","We further propose a gaze-controllable text-to-face method.","Our method contains a sketch-conditioned face diffusion module and a model-based sketch diffusion module.","We define a face sketch based on facial landmarks and eye segmentation map.","The face diffusion module generates face images from the face sketch, and the sketch diffusion module employs a 3D face model to generate face sketch from text description.","Experiments on the FFHQ dataset show the effectiveness of our method.","We will release our dataset and code for future research."],"url":"http://arxiv.org/abs/2404.17486v1","category":"cs.CV"}
{"created":"2024-04-26 15:37:50","title":"Sparse Reconstruction of Optical Doppler Tomography Based on State Space Model","abstract":"Optical Doppler Tomography (ODT) is a blood flow imaging technique popularly used in bioengineering applications. The fundamental unit of ODT is the 1D frequency response along the A-line (depth), named raw A-scan. A 2D ODT image (B-scan) is obtained by first sensing raw A-scans along the B-line (width), and then constructing the B-scan from these raw A-scans via magnitude-phase analysis and post-processing. To obtain a high-resolution B-scan with a precise flow map, densely sampled A-scans are required in current methods, causing both computational and storage burdens. To address this issue, in this paper we propose a novel sparse reconstruction framework with four main sequential steps: 1) early magnitude-phase fusion that encourages rich interaction of the complementary information in magnitude and phase, 2) State Space Model (SSM)-based representation learning, inspired by recent successes in Mamba and VMamba, to naturally capture both the intra-A-scan sequential information and between-A-scan interactions, 3) an Inception-based Feedforward Network module (IncFFN) to further boost the SSM-module, and 4) a B-line Pixel Shuffle (BPS) layer to effectively reconstruct the final results. In the experiments on real-world animal data, our method shows clear effectiveness in reconstruction accuracy. As the first application of SSM for image reconstruction tasks, we expect our work to inspire related explorations in not only efficient ODT imaging techniques but also generic image enhancement.","sentences":["Optical Doppler Tomography (ODT) is a blood flow imaging technique popularly used in bioengineering applications.","The fundamental unit of ODT is the 1D frequency response along the A-line (depth), named raw A-scan.","A 2D ODT image (B-scan) is obtained by first sensing raw A-scans along the B-line (width), and then constructing the B-scan from these raw A-scans via magnitude-phase analysis and post-processing.","To obtain a high-resolution B-scan with a precise flow map, densely sampled A-scans are required in current methods, causing both computational and storage burdens.","To address this issue, in this paper we propose a novel sparse reconstruction framework with four main sequential steps: 1) early magnitude-phase fusion that encourages rich interaction of the complementary information in magnitude and phase, 2) State Space Model (SSM)-based representation learning, inspired by recent successes in Mamba and VMamba, to naturally capture both the intra-A-scan sequential information and between-A-scan interactions, 3) an Inception-based Feedforward Network module (IncFFN) to further boost the SSM-module, and 4) a B-line Pixel Shuffle (BPS) layer to effectively reconstruct the final results.","In the experiments on real-world animal data, our method shows clear effectiveness in reconstruction accuracy.","As the first application of SSM for image reconstruction tasks, we expect our work to inspire related explorations in not only efficient ODT imaging techniques but also generic image enhancement."],"url":"http://arxiv.org/abs/2404.17484v1","category":"cs.CV"}
{"created":"2024-04-26 15:31:25","title":"ReproHum #0087-01: Human Evaluation Reproduction Report for Generating Fact Checking Explanations","abstract":"This paper presents a partial reproduction of Generating Fact Checking Explanations by Anatanasova et al (2020) as part of the ReproHum element of the ReproNLP shared task to reproduce the findings of NLP research regarding human evaluation. This shared task aims to investigate the extent to which NLP as a field is becoming more or less reproducible over time. Following the instructions provided by the task organisers and the original authors, we collect relative rankings of 3 fact-checking explanations (comprising a gold standard and the outputs of 2 models) for 40 inputs on the criteria of Coverage. The results of our reproduction and reanalysis of the original work's raw results lend support to the original findings, with similar patterns seen between the original work and our reproduction. Whilst we observe slight variation from the original results, our findings support the main conclusions drawn by the original authors pertaining to the efficacy of their proposed models.","sentences":["This paper presents a partial reproduction of Generating Fact Checking Explanations by Anatanasova et al (2020) as part of the ReproHum element of the ReproNLP shared task to reproduce the findings of NLP research regarding human evaluation.","This shared task aims to investigate the extent to which NLP as a field is becoming more or less reproducible over time.","Following the instructions provided by the task organisers and the original authors, we collect relative rankings of 3 fact-checking explanations (comprising a gold standard and the outputs of 2 models) for 40 inputs on the criteria of Coverage.","The results of our reproduction and reanalysis of the original work's raw results lend support to the original findings, with similar patterns seen between the original work and our reproduction.","Whilst we observe slight variation from the original results, our findings support the main conclusions drawn by the original authors pertaining to the efficacy of their proposed models."],"url":"http://arxiv.org/abs/2404.17481v1","category":"cs.CL"}
{"created":"2024-04-26 15:31:23","title":"A Very-High-Energy Gamma-Ray View of the Transient Sky","abstract":"The development of the latest generation of Imaging Atmospheric Cherenkov Telescopes (IACTs) over recent decades has led to the discovery of new extreme astrophysical phenomena in the very-high-energy (VHE, E > 100 GeV) gamma-ray regime. Time-domain and multi-messenger astronomy are inevitably connected to the physics of transient VHE emitters, which show unexpected (and mostly unpredictable) flaring or exploding episodes at different timescales. These transients often share the physical processes responsible for the production of the gamma-ray emission, through cosmic-ray acceleration, magnetic reconnection, jet production and/or outflows, and shocks interactions. In this review, we present an up-to-date overview of the VHE transients field, spanning from novae to supernovae, neutrino counterparts or fast radio bursts, among others, and we outline the expectations for future facilities.","sentences":["The development of the latest generation of Imaging Atmospheric Cherenkov Telescopes (IACTs) over recent decades has led to the discovery of new extreme astrophysical phenomena in the very-high-energy (VHE, E > 100 GeV) gamma-ray regime.","Time-domain and multi-messenger astronomy are inevitably connected to the physics of transient VHE emitters, which show unexpected (and mostly unpredictable) flaring or exploding episodes at different timescales.","These transients often share the physical processes responsible for the production of the gamma-ray emission, through cosmic-ray acceleration, magnetic reconnection, jet production and/or outflows, and shocks interactions.","In this review, we present an up-to-date overview of the VHE transients field, spanning from novae to supernovae, neutrino counterparts or fast radio bursts, among others, and we outline the expectations for future facilities."],"url":"http://arxiv.org/abs/2404.17480v1","category":"astro-ph.HE"}
{"created":"2024-04-26 15:29:12","title":"Scalable Adaptive Traffic Light Control Over a Traffic Network Including Turns, Transit Delays, and Blocking","abstract":"We develop adaptive data-driven traffic light controllers for a grid-like traffic network considering straight, left-turn, and right-turn traffic flows. The analysis incorporates transit delays and blocking effects on vehicle movements between neighboring intersections. Using a stochastic hybrid system model with parametric traffic light controllers, we use Infinitesimal Perturbation Analysis (IPA) to derive a data-driven cost gradient estimator with respect to controllable parameters. We then iteratively adjust them through an online gradient-based algorithm to improve performance metrics. By integrating a flexible modeling framework to represent diverse intersection and traffic network configurations with event-driven IPA-based adaptive controllers, we develop a general scalable, adaptive framework for real-time traffic light control in multi-intersection traffic networks.","sentences":["We develop adaptive data-driven traffic light controllers for a grid-like traffic network considering straight, left-turn, and right-turn traffic flows.","The analysis incorporates transit delays and blocking effects on vehicle movements between neighboring intersections.","Using a stochastic hybrid system model with parametric traffic light controllers, we use Infinitesimal Perturbation Analysis (IPA) to derive a data-driven cost gradient estimator with respect to controllable parameters.","We then iteratively adjust them through an online gradient-based algorithm to improve performance metrics.","By integrating a flexible modeling framework to represent diverse intersection and traffic network configurations with event-driven IPA-based adaptive controllers, we develop a general scalable, adaptive framework for real-time traffic light control in multi-intersection traffic networks."],"url":"http://arxiv.org/abs/2404.17479v1","category":"eess.SY"}
{"created":"2024-04-26 15:23:47","title":"CEval: A Benchmark for Evaluating Counterfactual Text Generation","abstract":"Counterfactual text generation aims to minimally change a text, such that it is classified differently. Judging advancements in method development for counterfactual text generation is hindered by a non-uniform usage of data sets and metrics in related work. We propose CEval, a benchmark for comparing counterfactual text generation methods. CEval unifies counterfactual and text quality metrics, includes common counterfactual datasets with human annotations, standard baselines (MICE, GDBA, CREST) and the open-source language model LLAMA-2. Our experiments found no perfect method for generating counterfactual text. Methods that excel at counterfactual metrics often produce lower-quality text while LLMs with simple prompts generate high-quality text but struggle with counterfactual criteria. By making CEval available as an open-source Python library, we encourage the community to contribute more methods and maintain consistent evaluation in future work.","sentences":["Counterfactual text generation aims to minimally change a text, such that it is classified differently.","Judging advancements in method development for counterfactual text generation is hindered by a non-uniform usage of data sets and metrics in related work.","We propose CEval, a benchmark for comparing counterfactual text generation methods.","CEval unifies counterfactual and text quality metrics, includes common counterfactual datasets with human annotations, standard baselines (MICE, GDBA, CREST) and the open-source language model LLAMA-2.","Our experiments found no perfect method for generating counterfactual text.","Methods that excel at counterfactual metrics often produce lower-quality text while LLMs with simple prompts generate high-quality text but struggle with counterfactual criteria.","By making CEval available as an open-source Python library, we encourage the community to contribute more methods and maintain consistent evaluation in future work."],"url":"http://arxiv.org/abs/2404.17475v1","category":"cs.CL"}
{"created":"2024-04-26 15:11:27","title":"Multicontinuum homogenization in perforated domains","abstract":"In this paper, we develop a general framework for multicontinuum homogenization in perforated domains. The simulations of problems in perforated domains are expensive and, in many applications, coarse-grid macroscopic models are developed. Many previous approaches include homogenization, multiscale finite element methods, and so on. In our paper, we design multicontinuum homogenization based on our recently proposed framework. In this setting, we distinguish different spatial regions in perforations based on their sizes. For example, very thin perforations are considered as one continua, while larger perforations are considered as another continua. By differentiating perforations in this way, we are able to predict flows in each of them more accurately. We present a framework by formulating cell problems for each continuum using appropriate constraints for the solution averages and their gradients. These cell problem solutions are used in a multiscale expansion and in deriving novel macroscopic systems for multicontinuum homogenization. Our proposed approaches are designed for problems without scale separation. We present numerical results for two continuum problems and demonstrate the accuracy of the proposed methods.","sentences":["In this paper, we develop a general framework for multicontinuum homogenization in perforated domains.","The simulations of problems in perforated domains are expensive and, in many applications, coarse-grid macroscopic models are developed.","Many previous approaches include homogenization, multiscale finite element methods, and so on.","In our paper, we design multicontinuum homogenization based on our recently proposed framework.","In this setting, we distinguish different spatial regions in perforations based on their sizes.","For example, very thin perforations are considered as one continua, while larger perforations are considered as another continua.","By differentiating perforations in this way, we are able to predict flows in each of them more accurately.","We present a framework by formulating cell problems for each continuum using appropriate constraints for the solution averages and their gradients.","These cell problem solutions are used in a multiscale expansion and in deriving novel macroscopic systems for multicontinuum homogenization.","Our proposed approaches are designed for problems without scale separation.","We present numerical results for two continuum problems and demonstrate the accuracy of the proposed methods."],"url":"http://arxiv.org/abs/2404.17471v1","category":"math.NA"}
{"created":"2024-04-26 15:09:44","title":"On Elliptical and Inverse Elliptical Wishart distributions: Review, new results, and applications","abstract":"This paper deals with matrix-variate distributions, from Wishart to Inverse Elliptical Wishart distributions over the set of symmetric definite positive matrices. Similar to the multivariate scenario, (Inverse) Elliptical Wishart distributions form a vast and general family of distributions, encompassing, for instance, Wishart or $t$-Wishart ones. The first objective of this study is to present a unified overview of Wishart, Inverse Wishart, Elliptical Wishart, and Inverse Elliptical Wishart distributions through their fundamental properties. This involves leveraging the stochastic representation of these distributions to establish key statistical properties of the Normalized Wishart distribution. Subsequently, this enables the computation of expectations, variances, and Kronecker moments for Elliptical Wishart and Inverse Elliptical Wishart distributions. As an illustrative application, the practical utility of these generalized Elliptical Wishart distributions is demonstrated using a real electroencephalographic dataset. This showcases their effectiveness in accurately modeling heterogeneous data.","sentences":["This paper deals with matrix-variate distributions, from Wishart to Inverse Elliptical Wishart distributions over the set of symmetric definite positive matrices.","Similar to the multivariate scenario, (Inverse) Elliptical Wishart distributions form a vast and general family of distributions, encompassing, for instance, Wishart or $t$-Wishart ones.","The first objective of this study is to present a unified overview of Wishart, Inverse Wishart, Elliptical Wishart, and Inverse Elliptical Wishart distributions through their fundamental properties.","This involves leveraging the stochastic representation of these distributions to establish key statistical properties of the Normalized Wishart distribution.","Subsequently, this enables the computation of expectations, variances, and Kronecker moments for Elliptical Wishart and Inverse Elliptical Wishart distributions.","As an illustrative application, the practical utility of these generalized Elliptical Wishart distributions is demonstrated using a real electroencephalographic dataset.","This showcases their effectiveness in accurately modeling heterogeneous data."],"url":"http://arxiv.org/abs/2404.17468v1","category":"math.ST"}
{"created":"2024-04-26 15:09:44","title":"Holographic $\\frac{1}{2}$-BPS surface defects in ABJM","abstract":"We study the class of $\\text{AdS}_3\\times \\mathbb{CP}^3$ solutions to massive Type IIA supergravity with $\\mathfrak{osp}(6|2)$ superconformal algebra recently constructed in \\cite{Macpherson:2023cbl}. These solutions are foliations over an interval preserving $\\mathcal{N}=(0,6)$ supersymmetry in two dimensions, that in the massless limit can be mapped to the $\\text{AdS}_4\\times \\mathbb{CP}^3$ solution of ABJM/ABJ. We show that in the massive case extra NS5-D8 branes, that we interpret as $\\frac{1}{2}$-BPS surface defect branes within the ABJ theory, backreact in the geometry and turn one of the 3d field theory directions onto an energy scale, generating a flow towards a 2d CFT. We construct explicit quiver field theories that we propose flow in the IR to the $(0,6)$ SCFTs dual to the solutions. Finally, we show that the $\\text{AdS}_3$ solutions realise geometrically, in terms of large gauge transformations, an extension to the massive case of Seiberg duality in ABJ theories proposed in the literature.","sentences":["We study the class of $\\text{AdS}_3\\times \\mathbb{CP}^3$ solutions to massive Type IIA supergravity with $\\mathfrak{osp}(6|2)$ superconformal algebra recently constructed in \\cite{Macpherson:2023cbl}.","These solutions are foliations over an interval preserving $\\mathcal{N}=(0,6)$ supersymmetry in two dimensions, that in the massless limit can be mapped to the $\\text{AdS}_4\\times \\mathbb{CP}^3$ solution of ABJM/ABJ.","We show that in the massive case extra NS5-D8 branes, that we interpret as $\\frac{1}{2}$-BPS surface defect branes within the ABJ theory, backreact in the geometry and turn one of the 3d field theory directions onto an energy scale, generating a flow towards a 2d CFT.","We construct explicit quiver field theories that we propose flow in the IR to the $(0,6)$ SCFTs dual to the solutions.","Finally, we show that the $\\text{AdS}_3$ solutions realise geometrically, in terms of large gauge transformations, an extension to the massive case of Seiberg duality in ABJ theories proposed in the literature."],"url":"http://arxiv.org/abs/2404.17469v1","category":"hep-th"}
{"created":"2024-04-26 15:05:26","title":"Bayesian Federated Inference for Survival Models","abstract":"In cancer research, overall survival and progression free survival are often analyzed with the Cox model. To estimate accurately the parameters in the model, sufficient data and, more importantly, sufficient events need to be observed. In practice, this is often a problem. Merging data sets from different medical centers may help, but this is not always possible due to strict privacy legislation and logistic difficulties. Recently, the Bayesian Federated Inference (BFI) strategy for generalized linear models was proposed. With this strategy the statistical analyses are performed in the local centers where the data were collected (or stored) and only the inference results are combined to a single estimated model; merging data is not necessary. The BFI methodology aims to compute from the separate inference results in the local centers what would have been obtained if the analysis had been based on the merged data sets. In this paper we generalize the BFI methodology as initially developed for generalized linear models to survival models. Simulation studies and real data analyses show excellent performance; i.e., the results obtained with the BFI methodology are very similar to the results obtained by analyzing the merged data. An R package for doing the analyses is available.","sentences":["In cancer research, overall survival and progression free survival are often analyzed with the Cox model.","To estimate accurately the parameters in the model, sufficient data and, more importantly, sufficient events need to be observed.","In practice, this is often a problem.","Merging data sets from different medical centers may help, but this is not always possible due to strict privacy legislation and logistic difficulties.","Recently, the Bayesian Federated Inference (BFI) strategy for generalized linear models was proposed.","With this strategy the statistical analyses are performed in the local centers where the data were collected (or stored) and only the inference results are combined to a single estimated model; merging data is not necessary.","The BFI methodology aims to compute from the separate inference results in the local centers what would have been obtained if the analysis had been based on the merged data sets.","In this paper we generalize the BFI methodology as initially developed for generalized linear models to survival models.","Simulation studies and real data analyses show excellent performance; i.e., the results obtained with the BFI methodology are very similar to the results obtained by analyzing the merged data.","An R package for doing the analyses is available."],"url":"http://arxiv.org/abs/2404.17464v1","category":"stat.ME"}
{"created":"2024-04-26 15:02:21","title":"Superresolution imaging of two incoherent optical sources with unequal brightnesses","abstract":"Resolving the separation between two incoherent optical sources with high precision is of great significance for fluorescence imaging and astronomical observations. In this paper, we focus on a more general scenario where two sources have unequal brightnesses. We give the ultimate precision limit with respect to separation by using the quantum Fisher information. Through the calculation of the classical Fisher information, we analyze and compare several specific measurement schemes including direct measurement, Gaussian mode measurement and zero-photon measurement. The results indicate that Gaussian mode measurement is the nearly optimal for a small separation. Our work provides a positive complement to the aspect of superresolution imaging of incoherent sources.","sentences":["Resolving the separation between two incoherent optical sources with high precision is of great significance for fluorescence imaging and astronomical observations.","In this paper, we focus on a more general scenario where two sources have unequal brightnesses.","We give the ultimate precision limit with respect to separation by using the quantum Fisher information.","Through the calculation of the classical Fisher information, we analyze and compare several specific measurement schemes including direct measurement, Gaussian mode measurement and zero-photon measurement.","The results indicate that Gaussian mode measurement is the nearly optimal for a small separation.","Our work provides a positive complement to the aspect of superresolution imaging of incoherent sources."],"url":"http://arxiv.org/abs/2404.17463v1","category":"quant-ph"}
{"created":"2024-04-26 14:58:23","title":"Integrated Sensing and Communication Channel Modeling: A Survey","abstract":"Integrated sensing and communication (ISAC) is expected to play a crucial role in the sixth-generation (6G) mobile communication systems, offering potential applications in the scenarios of intelligent transportation, smart factories, etc. The performance of radar sensing in ISAC systems is closely related to the characteristics of radar sensing and communication channels. Therefore, ISAC channel modeling serves as a fundamental cornerstone for evaluating and optimizing ISAC systems. This article provides a comprehensive survey on the ISAC channel modeling methods. Furthermore, the methods of target radar cross section (RCS) modeling and clutter RCS modeling are summarized. Finally, we discuss the future research trends related to ISAC channel modeling in various scenarios.","sentences":["Integrated sensing and communication (ISAC) is expected to play a crucial role in the sixth-generation (6G) mobile communication systems, offering potential applications in the scenarios of intelligent transportation, smart factories, etc.","The performance of radar sensing in ISAC systems is closely related to the characteristics of radar sensing and communication channels.","Therefore, ISAC channel modeling serves as a fundamental cornerstone for evaluating and optimizing ISAC systems.","This article provides a comprehensive survey on the ISAC channel modeling methods.","Furthermore, the methods of target radar cross section (RCS) modeling and clutter RCS modeling are summarized.","Finally, we discuss the future research trends related to ISAC channel modeling in various scenarios."],"url":"http://arxiv.org/abs/2404.17462v1","category":"cs.NI"}
{"created":"2024-04-26 14:57:56","title":"Multi-layer random features and the approximation power of neural networks","abstract":"A neural architecture with randomly initialized weights, in the infinite width limit, is equivalent to a Gaussian Random Field whose covariance function is the so-called Neural Network Gaussian Process kernel (NNGP). We prove that a reproducing kernel Hilbert space (RKHS) defined by the NNGP contains only functions that can be approximated by the architecture. To achieve a certain approximation error the required number of neurons in each layer is defined by the RKHS norm of the target function. Moreover, the approximation can be constructed from a supervised dataset by a random multi-layer representation of an input vector, together with training of the last layer's weights.   For a 2-layer NN and a domain equal to an $n-1$-dimensional sphere in ${\\mathbb R}^n$, we compare the number of neurons required by Barron's theorem and by the multi-layer features construction. We show that if eigenvalues of the integral operator of the NNGP decay slower than $k^{-n-\\frac{2}{3}}$ where $k$ is an order of an eigenvalue, then our theorem guarantees a more succinct neural network approximation than Barron's theorem. We also make some computational experiments to verify our theoretical findings. Our experiments show that realistic neural networks easily learn target functions even when both theorems do not give any guarantees.","sentences":["A neural architecture with randomly initialized weights, in the infinite width limit, is equivalent to a Gaussian Random Field whose covariance function is the so-called Neural Network Gaussian Process kernel (NNGP).","We prove that a reproducing kernel Hilbert space (RKHS) defined by the NNGP contains only functions that can be approximated by the architecture.","To achieve a certain approximation error the required number of neurons in each layer is defined by the RKHS norm of the target function.","Moreover, the approximation can be constructed from a supervised dataset by a random multi-layer representation of an input vector, together with training of the last layer's weights.   ","For a 2-layer NN and a domain equal to an $n-1$-dimensional sphere in ${\\mathbb R}^n$, we compare the number of neurons required by Barron's theorem and by the multi-layer features construction.","We show that if eigenvalues of the integral operator of the NNGP decay slower than $k^{-n-\\frac{2}{3}}$ where $k$ is an order of an eigenvalue, then our theorem guarantees a more succinct neural network approximation than Barron's theorem.","We also make some computational experiments to verify our theoretical findings.","Our experiments show that realistic neural networks easily learn target functions even when both theorems do not give any guarantees."],"url":"http://arxiv.org/abs/2404.17461v1","category":"cs.LG"}
{"created":"2024-04-26 14:50:46","title":"Converting High-Performance and Low-Latency SNNs through Explicit Modelling of Residual Error in ANNs","abstract":"Spiking neural networks (SNNs) have garnered interest due to their energy efficiency and superior effectiveness on neuromorphic chips compared with traditional artificial neural networks (ANNs). One of the mainstream approaches to implementing deep SNNs is the ANN-SNN conversion, which integrates the efficient training strategy of ANNs with the energy-saving potential and fast inference capability of SNNs. However, under extreme low-latency conditions, the existing conversion theory suggests that the problem of misrepresentation of residual membrane potentials in SNNs, i.e., the inability of IF neurons with a reset-by-subtraction mechanism to respond to residual membrane potentials beyond the range from resting potential to threshold, leads to a performance gap in the converted SNNs compared to the original ANNs. This severely limits the possibility of practical application of SNNs on delay-sensitive edge devices. Existing conversion methods addressing this problem usually involve modifying the state of the conversion spiking neurons. However, these methods do not consider their adaptability and compatibility with neuromorphic chips. We propose a new approach based on explicit modeling of residual errors as additive noise. The noise is incorporated into the activation function of the source ANN, which effectively reduces the residual error. Our experiments on the CIFAR10/100 dataset verify that our approach exceeds the prevailing ANN-SNN conversion methods and directly trained SNNs concerning accuracy and the required time steps. Overall, our method provides new ideas for improving SNN performance under ultra-low-latency conditions and is expected to promote practical neuromorphic hardware applications for further development.","sentences":["Spiking neural networks (SNNs) have garnered interest due to their energy efficiency and superior effectiveness on neuromorphic chips compared with traditional artificial neural networks (ANNs).","One of the mainstream approaches to implementing deep SNNs is the ANN-SNN conversion, which integrates the efficient training strategy of ANNs with the energy-saving potential and fast inference capability of SNNs.","However, under extreme low-latency conditions, the existing conversion theory suggests that the problem of misrepresentation of residual membrane potentials in SNNs, i.e., the inability of IF neurons with a reset-by-subtraction mechanism to respond to residual membrane potentials beyond the range from resting potential to threshold, leads to a performance gap in the converted SNNs compared to the original ANNs.","This severely limits the possibility of practical application of SNNs on delay-sensitive edge devices.","Existing conversion methods addressing this problem usually involve modifying the state of the conversion spiking neurons.","However, these methods do not consider their adaptability and compatibility with neuromorphic chips.","We propose a new approach based on explicit modeling of residual errors as additive noise.","The noise is incorporated into the activation function of the source ANN, which effectively reduces the residual error.","Our experiments on the CIFAR10/100 dataset verify that our approach exceeds the prevailing ANN-SNN conversion methods and directly trained SNNs concerning accuracy and the required time steps.","Overall, our method provides new ideas for improving SNN performance under ultra-low-latency conditions and is expected to promote practical neuromorphic hardware applications for further development."],"url":"http://arxiv.org/abs/2404.17456v1","category":"cs.NE"}
{"created":"2024-04-26 14:48:24","title":"Domain Adaptive and Fine-grained Anomaly Detection for Single-cell Sequencing Data and Beyond","abstract":"Fined-grained anomalous cell detection from affected tissues is critical for clinical diagnosis and pathological research. Single-cell sequencing data provide unprecedented opportunities for this task. However, current anomaly detection methods struggle to handle domain shifts prevalent in multi-sample and multi-domain single-cell sequencing data, leading to suboptimal performance. Moreover, these methods fall short of distinguishing anomalous cells into pathologically distinct subtypes. In response, we propose ACSleuth, a novel, reconstruction deviation-guided generative framework that integrates the detection, domain adaptation, and fine-grained annotating of anomalous cells into a methodologically cohesive workflow. Notably, we present the first theoretical analysis of using reconstruction deviations output by generative models for anomaly detection in lieu of domain shifts. This analysis informs us to develop a novel and superior maximum mean discrepancy-based anomaly scorer in ACSleuth. Extensive benchmarks over various single-cell data and other types of tabular data demonstrate ACSleuth's superiority over the state-of-the-art methods in identifying and subtyping anomalies in multi-sample and multi-domain contexts. Our code is available at https://github.com/Catchxu/ACsleuth.","sentences":["Fined-grained anomalous cell detection from affected tissues is critical for clinical diagnosis and pathological research.","Single-cell sequencing data provide unprecedented opportunities for this task.","However, current anomaly detection methods struggle to handle domain shifts prevalent in multi-sample and multi-domain single-cell sequencing data, leading to suboptimal performance.","Moreover, these methods fall short of distinguishing anomalous cells into pathologically distinct subtypes.","In response, we propose ACSleuth, a novel, reconstruction deviation-guided generative framework that integrates the detection, domain adaptation, and fine-grained annotating of anomalous cells into a methodologically cohesive workflow.","Notably, we present the first theoretical analysis of using reconstruction deviations output by generative models for anomaly detection in lieu of domain shifts.","This analysis informs us to develop a novel and superior maximum mean discrepancy-based anomaly scorer in ACSleuth.","Extensive benchmarks over various single-cell data and other types of tabular data demonstrate ACSleuth's superiority over the state-of-the-art methods in identifying and subtyping anomalies in multi-sample and multi-domain contexts.","Our code is available at https://github.com/Catchxu/ACsleuth."],"url":"http://arxiv.org/abs/2404.17454v1","category":"cs.LG"}
{"created":"2024-04-26 14:43:19","title":"Any-Quantile Probabilistic Forecasting of Short-Term Electricity Demand","abstract":"Power systems operate under uncertainty originating from multiple factors that are impossible to account for deterministically. Distributional forecasting is used to control and mitigate risks associated with this uncertainty. Recent progress in deep learning has helped to significantly improve the accuracy of point forecasts, while accurate distributional forecasting still presents a significant challenge. In this paper, we propose a novel general approach for distributional forecasting capable of predicting arbitrary quantiles. We show that our general approach can be seamlessly applied to two distinct neural architectures leading to the state-of-the-art distributional forecasting results in the context of short-term electricity demand forecasting task. We empirically validate our method on 35 hourly electricity demand time-series for European countries. Our code is available here: https://github.com/boreshkinai/any-quantile.","sentences":["Power systems operate under uncertainty originating from multiple factors that are impossible to account for deterministically.","Distributional forecasting is used to control and mitigate risks associated with this uncertainty.","Recent progress in deep learning has helped to significantly improve the accuracy of point forecasts, while accurate distributional forecasting still presents a significant challenge.","In this paper, we propose a novel general approach for distributional forecasting capable of predicting arbitrary quantiles.","We show that our general approach can be seamlessly applied to two distinct neural architectures leading to the state-of-the-art distributional forecasting results in the context of short-term electricity demand forecasting task.","We empirically validate our method on 35 hourly electricity demand time-series for European countries.","Our code is available here: https://github.com/boreshkinai/any-quantile."],"url":"http://arxiv.org/abs/2404.17451v1","category":"cs.LG"}
{"created":"2024-04-26 14:38:51","title":"On the Meaning of Local Symmetries: Epistemic-Ontological Dialectic","abstract":"We propose our account of the meaning of local symmetries. We argue that the general covariance principle and gauge principle both are principles of democratic epistemic access to the law of physics, leading to ontological insights about the objective nature of spacetime. We further argue that relationality is a core notion of general-relativistic gauge field theory, tacitly encoded by its (active) local symmetries.","sentences":["We propose our account of the meaning of local symmetries.","We argue that the general covariance principle and gauge principle both are principles of democratic epistemic access to the law of physics, leading to ontological insights about the objective nature of spacetime.","We further argue that relationality is a core notion of general-relativistic gauge field theory, tacitly encoded by its (active) local symmetries."],"url":"http://arxiv.org/abs/2404.17449v1","category":"physics.hist-ph"}
{"created":"2024-04-26 14:34:20","title":"Skeletal Kinetics Reduction for Astrophysical Reaction Networks","abstract":"A novel methodology is developed to extract accurate skeletal reaction models for nuclear combustion. Local sensitivities of isotope mass fractions with respect to reaction rates are modeled based on the forced optimally time-dependent (f-OTD) scheme. These sensitivities are then analyzed temporally to generate skeletal models. The methodology is demonstrated by conducting skeletal reduction of constant density and temperature burning of carbon and oxygen relevant to SNe Ia. The 495-isotopes Torch model is chosen as the detailed reaction network. A map of maximum production of $^{56}\\text{Ni}$ in SNe Ia is produced for different temperatures, densities, and proton to neutron ratios. The f-OTD simulations and the sensitivity analyses are then performed with initial conditions from this map. A series of skeletal models are derived and their performances are assessed by comparison against currently existing skeletal models. Previous models have been constructed intuitively by assuming the dominance of $\\alpha$-chain reactions. The comparison of the newly generated skeletal models against previous models is based on the predicted energy release and $^{44}\\text{Ti}$ and $^{56}\\text{Ni}$ abundances by each model. The consequences of $\\mathtt{y}_e \\neq 0.5$ in the initial composition are also explored where $\\mathtt{y}_e$ is the electron fraction. The simulated results show that $^{56}\\text{Ni}$ production decreases by decreasing $\\mathtt{y}_e$ as expected, and that the $^{43}\\text{Sc}$ is a key isotope in proton and neutron channels toward $^{56}\\text{Ni}$ production. It is shown that an f-OTD skeletal model with 150 isotopes can accurately predict the $^{56}\\text{Ni}$ abundance in SNe Ia for $\\mathtt{y}_e \\lesssim 0.5$ initial conditions.","sentences":["A novel methodology is developed to extract accurate skeletal reaction models for nuclear combustion.","Local sensitivities of isotope mass fractions with respect to reaction rates are modeled based on the forced optimally time-dependent (f-OTD) scheme.","These sensitivities are then analyzed temporally to generate skeletal models.","The methodology is demonstrated by conducting skeletal reduction of constant density and temperature burning of carbon and oxygen relevant to SNe Ia.","The 495-isotopes Torch model is chosen as the detailed reaction network.","A map of maximum production of $^{56}\\text{Ni}$ in SNe Ia is produced for different temperatures, densities, and proton to neutron ratios.","The f-OTD simulations and the sensitivity analyses are then performed with initial conditions from this map.","A series of skeletal models are derived and their performances are assessed by comparison against currently existing skeletal models.","Previous models have been constructed intuitively by assuming the dominance of $\\alpha$-chain reactions.","The comparison of the newly generated skeletal models against previous models is based on the predicted energy release and $^{44}\\text{Ti}$ and $^{56}\\text{Ni}$ abundances by each model.","The consequences of $\\mathtt{y}_e \\neq 0.5$ in the initial composition are also explored where $\\mathtt{y}_e$ is the electron fraction.","The simulated results show that $^{56}\\text{Ni}$ production decreases by decreasing $\\mathtt{y}_e$ as expected, and that the $^{43}\\text{Sc}$ is a key isotope in proton and neutron channels toward $^{56}\\text{Ni}$ production.","It is shown that an f-OTD skeletal model with 150 isotopes can accurately predict the $^{56}\\text{Ni}$ abundance in SNe Ia for $\\mathtt{y}_e \\lesssim 0.5$ initial conditions."],"url":"http://arxiv.org/abs/2404.17447v1","category":"astro-ph.SR"}
{"created":"2024-04-26 14:29:16","title":"\"ChatGPT Is Here to Help, Not to Replace Anybody\" -- An Evaluation of Students' Opinions On Integrating ChatGPT In CS Courses","abstract":"Large Language Models (LLMs) like GPT and Bard are capable of producing code based on textual descriptions, with remarkable efficacy. Such technology will have profound implications for computing education, raising concerns about cheating, excessive dependence, and a decline in computational thinking skills, among others. There has been extensive research on how teachers should handle this challenge but it is also important to understand how students feel about this paradigm shift. In this research, 52 first-year CS students were surveyed in order to assess their views on technologies with code-generation capabilities, both from academic and professional perspectives. Our findings indicate that while students generally favor the academic use of GPT, they don't over rely on it, only mildly asking for its help. Although most students benefit from GPT, some struggle to use it effectively, urging the need for specific GPT training. Opinions on GPT's impact on their professional lives vary, but there is a consensus on its importance in academic practice.","sentences":["Large Language Models (LLMs) like GPT and Bard are capable of producing code based on textual descriptions, with remarkable efficacy.","Such technology will have profound implications for computing education, raising concerns about cheating, excessive dependence, and a decline in computational thinking skills, among others.","There has been extensive research on how teachers should handle this challenge but it is also important to understand how students feel about this paradigm shift.","In this research, 52 first-year CS students were surveyed in order to assess their views on technologies with code-generation capabilities, both from academic and professional perspectives.","Our findings indicate that while students generally favor the academic use of GPT, they don't over rely on it, only mildly asking for its help.","Although most students benefit from GPT, some struggle to use it effectively, urging the need for specific GPT training.","Opinions on GPT's impact on their professional lives vary, but there is a consensus on its importance in academic practice."],"url":"http://arxiv.org/abs/2404.17443v1","category":"cs.ET"}
{"created":"2024-04-26 14:28:18","title":"Uniform Generalization Bounds on Data-Dependent Hypothesis Sets via PAC-Bayesian Theory on Random Sets","abstract":"We propose data-dependent uniform generalization bounds by approaching the problem from a PAC-Bayesian perspective. We first apply the PAC-Bayesian framework on `random sets' in a rigorous way, where the training algorithm is assumed to output a data-dependent hypothesis set after observing the training data. This approach allows us to prove data-dependent bounds, which can be applicable in numerous contexts. To highlight the power of our approach, we consider two main applications. First, we propose a PAC-Bayesian formulation of the recently developed fractal-dimension-based generalization bounds. The derived results are shown to be tighter and they unify the existing results around one simple proof technique. Second, we prove uniform bounds over the trajectories of continuous Langevin dynamics and stochastic gradient Langevin dynamics. These results provide novel information about the generalization properties of noisy algorithms.","sentences":["We propose data-dependent uniform generalization bounds by approaching the problem from a PAC-Bayesian perspective.","We first apply the PAC-Bayesian framework on `random sets' in a rigorous way, where the training algorithm is assumed to output a data-dependent hypothesis set after observing the training data.","This approach allows us to prove data-dependent bounds, which can be applicable in numerous contexts.","To highlight the power of our approach, we consider two main applications.","First, we propose a PAC-Bayesian formulation of the recently developed fractal-dimension-based generalization bounds.","The derived results are shown to be tighter and they unify the existing results around one simple proof technique.","Second, we prove uniform bounds over the trajectories of continuous Langevin dynamics and stochastic gradient Langevin dynamics.","These results provide novel information about the generalization properties of noisy algorithms."],"url":"http://arxiv.org/abs/2404.17442v1","category":"stat.ML"}
{"created":"2024-04-26 14:27:05","title":"Comparison results for Markov tree distributions","abstract":"We develop comparison results for Markov tree distributions extending ordering results from the literature on discrete time Markov processes and recently studied ordering results for conditionally independent factor models to tree structures. Based on fairly natural positive dependence conditions, our main contribution is a comparison result with respect to the supermodular order. Since this order is a pure dependence order, it has many applications in optimal transport, finance, and insurance. As an illustrative example, we consider hidden Markov models and study distributional robustness for functionals of the random walk under model uncertainty. Further, we show that, surprisingly, more general comparison results via the recently established rearrangement-based Schur order for conditional distributions, which implies an ordering of Chatterjee's rank correlation, do not carry over from star structures to trees. Several examples and a detailed discussion of the assumptions demonstrate the generality of our results and provide further insights into the behavior of multidimensional distributions.","sentences":["We develop comparison results for Markov tree distributions extending ordering results from the literature on discrete time Markov processes and recently studied ordering results for conditionally independent factor models to tree structures.","Based on fairly natural positive dependence conditions, our main contribution is a comparison result with respect to the supermodular order.","Since this order is a pure dependence order, it has many applications in optimal transport, finance, and insurance.","As an illustrative example, we consider hidden Markov models and study distributional robustness for functionals of the random walk under model uncertainty.","Further, we show that, surprisingly, more general comparison results via the recently established rearrangement-based Schur order for conditional distributions, which implies an ordering of Chatterjee's rank correlation, do not carry over from star structures to trees.","Several examples and a detailed discussion of the assumptions demonstrate the generality of our results and provide further insights into the behavior of multidimensional distributions."],"url":"http://arxiv.org/abs/2404.17441v1","category":"math.ST"}
{"created":"2024-04-26 14:22:50","title":"Real-World Deployment of a Hierarchical Uncertainty-Aware Collaborative Multiagent Planning System","abstract":"We would like to enable a collaborative multiagent team to navigate at long length scales and under uncertainty in real-world environments. In practice, planning complexity scales with the number of agents in the team, with the length scale of the environment, and with environmental uncertainty. Enabling tractable planning requires developing abstract models that can represent complex, high-quality plans. However, such models often abstract away information needed to generate directly-executable plans for real-world agents in real-world environments, as planning in such detail, especially in the presence of real-world uncertainty, would be computationally intractable. In this paper, we describe the deployment of a planning system that used a hierarchy of planners to execute collaborative multiagent navigation tasks in real-world, unknown environments. By developing a planning system that was robust to failures at every level of the planning hierarchy, we enabled the team to complete collaborative navigation tasks, even in the presence of imperfect planning abstractions and real-world uncertainty. We deployed our approach on a Clearpath Husky-Jackal team navigating in a structured outdoor environment, and demonstrated that the system enabled the agents to successfully execute collaborative plans.","sentences":["We would like to enable a collaborative multiagent team to navigate at long length scales and under uncertainty in real-world environments.","In practice, planning complexity scales with the number of agents in the team, with the length scale of the environment, and with environmental uncertainty.","Enabling tractable planning requires developing abstract models that can represent complex, high-quality plans.","However, such models often abstract away information needed to generate directly-executable plans for real-world agents in real-world environments, as planning in such detail, especially in the presence of real-world uncertainty, would be computationally intractable.","In this paper, we describe the deployment of a planning system that used a hierarchy of planners to execute collaborative multiagent navigation tasks in real-world, unknown environments.","By developing a planning system that was robust to failures at every level of the planning hierarchy, we enabled the team to complete collaborative navigation tasks, even in the presence of imperfect planning abstractions and real-world uncertainty.","We deployed our approach on a Clearpath Husky-Jackal team navigating in a structured outdoor environment, and demonstrated that the system enabled the agents to successfully execute collaborative plans."],"url":"http://arxiv.org/abs/2404.17438v1","category":"cs.RO"}
{"created":"2024-04-26 14:22:02","title":"Transformer For Low-frequency Extrapolating of Seismic Data","abstract":"Full waveform inversion (FWI) is used to reconstruct the physical properties of subsurface media which plays an important role in seismic exploration. However, the precision of FWI is seriously affected by the absence or inaccuracy of low-frequency information. Therefore, reconstructing the low-frequency signals accurately is highly significant in seismic data processing. Low-frequency extrapolation of seismic records can be approached as a deep learning regression problem. Thus, to obtain low-frequency information from band-limited seismic records, a novel network structure called low-frequency extrapolation transformer (LFET) is proposed to construct the nonlinear mapping relationship between the data missing low-frequency and low-frequency data in a supervised learning approach, which is inspired by the transformer model widely used in natural language processing (NLP). We apply multi-head self-attention (MSA) modules to model the remote dependencies of seismic data. Based on this, we introduce a shifted window partitioning approach to reduce the calculating amount. Due to the field data are not suitable for supervised learning, we generate synthetic seismic records using submodels selected from the benchmark Marmousi model as training data whose characteristics are similar to that of the field data. A single trace of synthetic band-limited seismic data in the time domain is used as the input data, and the parameters of LFET are updated based on the errors between the predicted trace and the corresponding label. The experimental results on the data generated by different models, different wavelets, and different kinds of field marine data demonstrate the feasibility and generalization of the proposed method. Furthermore, the proposed method achieves higher accuracy with lower computational expense than the traditional CNN method.","sentences":["Full waveform inversion (FWI) is used to reconstruct the physical properties of subsurface media which plays an important role in seismic exploration.","However, the precision of FWI is seriously affected by the absence or inaccuracy of low-frequency information.","Therefore, reconstructing the low-frequency signals accurately is highly significant in seismic data processing.","Low-frequency extrapolation of seismic records can be approached as a deep learning regression problem.","Thus, to obtain low-frequency information from band-limited seismic records, a novel network structure called low-frequency extrapolation transformer (LFET) is proposed to construct the nonlinear mapping relationship between the data missing low-frequency and low-frequency data in a supervised learning approach, which is inspired by the transformer model widely used in natural language processing (NLP).","We apply multi-head self-attention (MSA) modules to model the remote dependencies of seismic data.","Based on this, we introduce a shifted window partitioning approach to reduce the calculating amount.","Due to the field data are not suitable for supervised learning, we generate synthetic seismic records using submodels selected from the benchmark Marmousi model as training data whose characteristics are similar to that of the field data.","A single trace of synthetic band-limited seismic data in the time domain is used as the input data, and the parameters of LFET are updated based on the errors between the predicted trace and the corresponding label.","The experimental results on the data generated by different models, different wavelets, and different kinds of field marine data demonstrate the feasibility and generalization of the proposed method.","Furthermore, the proposed method achieves higher accuracy with lower computational expense than the traditional CNN method."],"url":"http://arxiv.org/abs/2404.17437v1","category":"physics.geo-ph"}
{"created":"2024-04-26 14:21:28","title":"Bayesian synthesis of astrometric wobble and total light curves in close binary supermassive black holes","abstract":"We test the potential of Bayesian synthesis of upcoming multi-instrument data to extract orbital parameters and individual light curves of close binary supermassive black holes (CB-SMBH) with subparsec separations. Next generation (ng) interferometers, will make possible the observation of astrometric wobbles in CB-SMBH. Combining them with periodic variable time-domain data from surveys like the Vera C. Rubin Legacy Survey of Space and Time (LSST), allows for a more information on CB-SMBH candidates compared to standalone observational methods. Our method reliably determines binary parameters and component fluxes from binary total flux across long-term, intermediate and short-term binary dynamics and observational configurations, assuming ten annual observations, even in short period \"q-accrete\" objects. Expected CB-SMBH astrometric wobbles constructed from binary dynamical parameters, might serve in refining observational strategies for CB-SMBH. Combination of inferred mass ratio, light curves of binary components, and observed photocenter wobbles can be a proxy for the activity states of CB-SMBH components.","sentences":["We test the potential of Bayesian synthesis of upcoming multi-instrument data to extract orbital parameters and individual light curves of close binary supermassive black holes (CB-SMBH) with subparsec separations.","Next generation (ng) interferometers, will make possible the observation of astrometric wobbles in CB-SMBH.","Combining them with periodic variable time-domain data from surveys like the Vera C. Rubin Legacy Survey of Space and Time (LSST), allows for a more information on CB-SMBH candidates compared to standalone observational methods.","Our method reliably determines binary parameters and component fluxes from binary total flux across long-term, intermediate and short-term binary dynamics and observational configurations, assuming ten annual observations, even in short period \"q-accrete\" objects.","Expected CB-SMBH astrometric wobbles constructed from binary dynamical parameters, might serve in refining observational strategies for CB-SMBH.","Combination of inferred mass ratio, light curves of binary components, and observed photocenter wobbles can be a proxy for the activity states of CB-SMBH components."],"url":"http://arxiv.org/abs/2404.17435v1","category":"astro-ph.IM"}
{"created":"2024-04-26 14:20:31","title":"PromptCIR: Blind Compressed Image Restoration with Prompt Learning","abstract":"Blind Compressed Image Restoration (CIR) has garnered significant attention due to its practical applications. It aims to mitigate compression artifacts caused by unknown quality factors, particularly with JPEG codecs. Existing works on blind CIR often seek assistance from a quality factor prediction network to facilitate their network to restore compressed images. However, the predicted numerical quality factor lacks spatial information, preventing network adaptability toward image contents. Recent studies in prompt-learning-based image restoration have showcased the potential of prompts to generalize across varied degradation types and degrees. This motivated us to design a prompt-learning-based compressed image restoration network, dubbed PromptCIR, which can effectively restore images from various compress levels. Specifically, PromptCIR exploits prompts to encode compression information implicitly, where prompts directly interact with soft weights generated from image features, thus providing dynamic content-aware and distortion-aware guidance for the restoration process. The light-weight prompts enable our method to adapt to different compression levels, while introducing minimal parameter overhead. Overall, PromptCIR leverages the powerful transformer-based backbone with the dynamic prompt module to proficiently handle blind CIR tasks, winning first place in the NTIRE 2024 challenge of blind compressed image enhancement track. Extensive experiments have validated the effectiveness of our proposed PromptCIR. The code is available at https://github.com/lbc12345/PromptCIR-NTIRE24.","sentences":["Blind Compressed Image Restoration (CIR) has garnered significant attention due to its practical applications.","It aims to mitigate compression artifacts caused by unknown quality factors, particularly with JPEG codecs.","Existing works on blind CIR often seek assistance from a quality factor prediction network to facilitate their network to restore compressed images.","However, the predicted numerical quality factor lacks spatial information, preventing network adaptability toward image contents.","Recent studies in prompt-learning-based image restoration have showcased the potential of prompts to generalize across varied degradation types and degrees.","This motivated us to design a prompt-learning-based compressed image restoration network, dubbed PromptCIR, which can effectively restore images from various compress levels.","Specifically, PromptCIR exploits prompts to encode compression information implicitly, where prompts directly interact with soft weights generated from image features, thus providing dynamic content-aware and distortion-aware guidance for the restoration process.","The light-weight prompts enable our method to adapt to different compression levels, while introducing minimal parameter overhead.","Overall, PromptCIR leverages the powerful transformer-based backbone with the dynamic prompt module to proficiently handle blind CIR tasks, winning first place in the NTIRE 2024 challenge of blind compressed image enhancement track.","Extensive experiments have validated the effectiveness of our proposed PromptCIR.","The code is available at https://github.com/lbc12345/PromptCIR-NTIRE24."],"url":"http://arxiv.org/abs/2404.17433v1","category":"cs.CV"}
{"created":"2024-04-26 14:08:36","title":"Lower Bounds for the Minimum Spanning Tree Cycle Intersection Problem","abstract":"Minimum spanning trees are important tools in the analysis and design of networks. Many practical applications require their computation, ranging from biology and linguistics to economy and telecommunications. The set of cycles of a network has a vector space structure. Given a spanning tree, the set of non-tree edges defines cycles that determine a basis. The intersection of two such cycles is the number of edges they have in common and the intersection number -- denoted $\\cap(G)$ -- is the number of non-empty pairwise intersections of the cycles of the basis. The Minimum Spanning Tree Cycle Intersection problem consists in finding a spanning tree such that the intersection number is minimum. This problem is relevant in order to integrate discrete differential forms. In this paper, we present two lower bounds of the intersection number of an arbitrary connected graph $G=(V,E)$. In the first part, we prove the following statement: $$\\frac{1}{2}\\left(\\frac{\\nu^2}{n-1} - \\nu\\right) \\leq \\cap(G),$$ where $n = |V|$ and $\\nu$ is the \\emph{cyclomatic number} of $G$. In the second part, based on some experimental results and a new observation, we conjecture the following improved tight lower bound: $$(n-1) \\binom{q}{2} + q \\ r\\leq \\cap(G),$$ where $2 \\nu = q (n-1) + r$ is the integer division of $2 \\nu$ and $n-1$. This is the first result in a general context, that is for an arbitrary connected graph.","sentences":["Minimum spanning trees are important tools in the analysis and design of networks.","Many practical applications require their computation, ranging from biology and linguistics to economy and telecommunications.","The set of cycles of a network has a vector space structure.","Given a spanning tree, the set of non-tree edges defines cycles that determine a basis.","The intersection of two such cycles is the number of edges they have in common and the intersection number -- denoted $\\cap(G)$ -- is the number of non-empty pairwise intersections of the cycles of the basis.","The Minimum Spanning Tree Cycle Intersection problem consists in finding a spanning tree such that the intersection number is minimum.","This problem is relevant in order to integrate discrete differential forms.","In this paper, we present two lower bounds of the intersection number of an arbitrary connected graph $G=(V,E)$. In the first part, we prove the following statement: $$\\frac{1}{2}\\left(\\frac{\\nu^2}{n-1} - \\nu\\right) \\leq \\cap(G),$$ where $n = |V|$ and $\\nu$ is the \\emph{cyclomatic number} of $G$. In the second part, based on some experimental results and a new observation, we conjecture the following improved tight lower bound: $$(n-1) \\binom{q}{2} + q \\ r\\leq \\cap(G),$$ where $2 \\nu = q (n-1)","+ r$ is the integer division of $2 \\nu$ and $n-1$. This is the first result in a general context, that is for an arbitrary connected graph."],"url":"http://arxiv.org/abs/2404.17428v1","category":"cs.DM"}
{"created":"2024-04-26 14:03:23","title":"One-Shot Image Restoration","abstract":"Image restoration, or inverse problems in image processing, has long been an extensively studied topic. In recent years supervised learning approaches have become a popular strategy attempting to tackle this task. Unfortunately, most supervised learning-based methods are highly demanding in terms of computational resources and training data (sample complexity). In addition, trained models are sensitive to domain changes, such as varying acquisition systems, signal sampling rates, resolution and contrast. In this work, we try to answer a fundamental question: Can supervised learning models generalize well solely by learning from one image or even part of an image? If so, then what is the minimal amount of patches required to achieve acceptable generalization? To this end, we focus on an efficient patch-based learning framework that requires a single image input-output pair for training. Experimental results demonstrate the applicability, robustness and computational efficiency of the proposed approach for supervised image deblurring and super-resolution. Our results showcase significant improvement of learning models' sample efficiency, generalization and time complexity, that can hopefully be leveraged for future real-time applications, and applied to other signals and modalities.","sentences":["Image restoration, or inverse problems in image processing, has long been an extensively studied topic.","In recent years supervised learning approaches have become a popular strategy attempting to tackle this task.","Unfortunately, most supervised learning-based methods are highly demanding in terms of computational resources and training data (sample complexity).","In addition, trained models are sensitive to domain changes, such as varying acquisition systems, signal sampling rates, resolution and contrast.","In this work, we try to answer a fundamental question: Can supervised learning models generalize well solely by learning from one image or even part of an image?","If so, then what is the minimal amount of patches required to achieve acceptable generalization?","To this end, we focus on an efficient patch-based learning framework that requires a single image input-output pair for training.","Experimental results demonstrate the applicability, robustness and computational efficiency of the proposed approach for supervised image deblurring and super-resolution.","Our results showcase significant improvement of learning models' sample efficiency, generalization and time complexity, that can hopefully be leveraged for future real-time applications, and applied to other signals and modalities."],"url":"http://arxiv.org/abs/2404.17426v1","category":"eess.IV"}
{"created":"2024-04-26 13:58:50","title":"\u00c9quations en diviseurs","abstract":"Let $d(n) \\subset \\mathbb{N}$ be the set of the $\\tau(n)$ divisors of $n$. We generalize a method of Tenenbaum and de la Bret\\`eche for the study of the set $d(n)$. Among other things, we establish that $$ |\\{(d_1,d_2,d_3) \\in d(n)^3 : d_1+d_2=d_3\\}| \\le \\tau(n)^{2-\\delta} $$ with $\\delta=0.045072$.","sentences":["Let $d(n)","\\subset \\mathbb{N}$ be the set of the $\\tau(n)$ divisors of $n$. We generalize a method of Tenenbaum and de la Bret\\`eche for the study of the set $d(n)$. Among other things, we establish that $$ |\\{(d_1,d_2,d_3)","\\in d(n)^3 :","d_1+d_2=d_3\\}| \\le \\tau(n)^{2-\\delta} $$ with $\\delta=0.045072$."],"url":"http://arxiv.org/abs/2404.17424v1","category":"math.NT"}
{"created":"2024-04-26 13:58:46","title":"Forming off-center massive black hole binaries in dwarf galaxies through Jacobi capture","abstract":"It is well established that black holes reside in the central regions of virtually all types of known galaxies. Recent observational and numerical studies however challenge this picture, suggesting that intermediate-mass black holes in dwarf galaxies may be found on orbits far from the center. In particular, constant-density cores minimize orbital energy losses due to dynamical friction, and allow black holes to settle on stable off-center orbits. Using controlled simulations, we study the dynamics of off-center black holes in dwarf galaxies with such cores. We propose a new scenario to describe off-center mergers of massive black holes, starting with a Jacobi capture. We focus on initially circular and co-planar black hole orbits and explore a large parameter space of black hole masses and orbital parameters. We find that Jacobi captures are a complex and chaotic phenomenon that occurs in about 13% of cases in this simplified setup, and we quantify how the likelihood of capture depends on the simulation parameters. We note that this percentage is likely an upper limit of the general case. Nevertheless, we show that Jacobi captures in cored dwarf galaxies can facilitate the formation of off-center black hole binaries, and that this process is sufficiently common to have a substantial effect on black hole growth. While our setup only allows for temporary captures, we expect dissipative forces from baryons and post-Newtonian corrections to maintain the captures over time and to lead to the formation of stable binary systems. This motivates future studies of the effectiveness of such dissipative forces, within stripped nuclei or globular clusters, in forming stable bound systems.","sentences":["It is well established that black holes reside in the central regions of virtually all types of known galaxies.","Recent observational and numerical studies however challenge this picture, suggesting that intermediate-mass black holes in dwarf galaxies may be found on orbits far from the center.","In particular, constant-density cores minimize orbital energy losses due to dynamical friction, and allow black holes to settle on stable off-center orbits.","Using controlled simulations, we study the dynamics of off-center black holes in dwarf galaxies with such cores.","We propose a new scenario to describe off-center mergers of massive black holes, starting with a Jacobi capture.","We focus on initially circular and co-planar black hole orbits and explore a large parameter space of black hole masses and orbital parameters.","We find that Jacobi captures are a complex and chaotic phenomenon that occurs in about 13% of cases in this simplified setup, and we quantify how the likelihood of capture depends on the simulation parameters.","We note that this percentage is likely an upper limit of the general case.","Nevertheless, we show that Jacobi captures in cored dwarf galaxies can facilitate the formation of off-center black hole binaries, and that this process is sufficiently common to have a substantial effect on black hole growth.","While our setup only allows for temporary captures, we expect dissipative forces from baryons and post-Newtonian corrections to maintain the captures over time and to lead to the formation of stable binary systems.","This motivates future studies of the effectiveness of such dissipative forces, within stripped nuclei or globular clusters, in forming stable bound systems."],"url":"http://arxiv.org/abs/2404.17423v1","category":"astro-ph.GA"}
{"created":"2024-04-26 13:58:40","title":"Sibson's formula for higher order Voronoi diagrams","abstract":"Let $S$ be a set of $n$ points in general position in $\\mathbb{R}^d$. The order-$k$ Voronoi diagram of $S$, $V_k(S)$, is a subdivision of $\\mathbb{R}^d$ into cells whose points have the same $k$ nearest points of $S$.   Sibson, in his seminal paper from 1980 (A vector identity for the Dirichlet tessellation), gives a formula to express a point $Q$ of $S$ as a convex combination of other points of $S$ by using ratios of volumes of the intersection of cells of $V_2(S)$ and the cell of $Q$ in $V_1(S)$. The natural neighbour interpolation method is based on Sibson's formula. We generalize his result to express $Q$ as a convex combination of other points of $S$ by using ratios of volumes from Voronoi diagrams of any given order.","sentences":["Let $S$ be a set of $n$ points in general position in $\\mathbb{R}^d$. The order-$k$ Voronoi diagram of $S$, $V_k(S)$, is a subdivision of $\\mathbb{R}^d$ into cells whose points have the same $k$ nearest points of $S$.   Sibson, in his seminal paper from 1980 (A vector identity for the Dirichlet tessellation), gives a formula to express a point $Q$ of $S$ as a convex combination of other points of $S$ by using ratios of volumes of the intersection of cells of $V_2(S)$ and the cell of $Q$ in $V_1(S)$. The natural neighbour interpolation method is based on Sibson's formula.","We generalize his result to express $Q$ as a convex combination of other points of $S$ by using ratios of volumes from Voronoi diagrams of any given order."],"url":"http://arxiv.org/abs/2404.17422v1","category":"cs.CG"}
{"created":"2024-04-26 13:57:42","title":"Finite Key Security of Simplified Trusted Node Networks","abstract":"Simplified trusted nodes (STNs) are a form of trusted node for quantum key distribution (QKD) networks which do not require running a full QKD stack every instance (i.e., they do not need to run error correction and privacy amplification each session). Such systems hold the advantage that they may be implemented with weaker computational abilities, than regular TNs, while still keeping up with key generation rate demands. The downside is that noise tolerance is lower. However, to get a better understanding of their suitability in various scenarios, one requires practical, finite-key security bounds for STN networks. So far, only theoretical asymptotic bounds are known. In this work we derive a new proof of security for STN chains in the finite key setting. We also derive a novel cost function allowing us to evaluate when STNs would be beneficial from a computational cost perspective, compared with regular TN networks.","sentences":["Simplified trusted nodes (STNs) are a form of trusted node for quantum key distribution (QKD) networks which do not require running a full QKD stack every instance (i.e., they do not need to run error correction and privacy amplification each session).","Such systems hold the advantage that they may be implemented with weaker computational abilities, than regular TNs, while still keeping up with key generation rate demands.","The downside is that noise tolerance is lower.","However, to get a better understanding of their suitability in various scenarios, one requires practical, finite-key security bounds for STN networks.","So far, only theoretical asymptotic bounds are known.","In this work we derive a new proof of security for STN chains in the finite key setting.","We also derive a novel cost function allowing us to evaluate when STNs would be beneficial from a computational cost perspective, compared with regular TN networks."],"url":"http://arxiv.org/abs/2404.17420v1","category":"quant-ph"}
{"created":"2024-04-26 13:55:39","title":"Multi-view Image Prompted Multi-view Diffusion for Improved 3D Generation","abstract":"Using image as prompts for 3D generation demonstrate particularly strong performances compared to using text prompts alone, for images provide a more intuitive guidance for the 3D generation process. In this work, we delve into the potential of using multiple image prompts, instead of a single image prompt, for 3D generation. Specifically, we build on ImageDream, a novel image-prompt multi-view diffusion model, to support multi-view images as the input prompt. Our method, dubbed MultiImageDream, reveals that transitioning from a single-image prompt to multiple-image prompts enhances the performance of multi-view and 3D object generation according to various quantitative evaluation metrics and qualitative assessments. This advancement is achieved without the necessity of fine-tuning the pre-trained ImageDream multi-view diffusion model.","sentences":["Using image as prompts for 3D generation demonstrate particularly strong performances compared to using text prompts alone, for images provide a more intuitive guidance for the 3D generation process.","In this work, we delve into the potential of using multiple image prompts, instead of a single image prompt, for 3D generation.","Specifically, we build on ImageDream, a novel image-prompt multi-view diffusion model, to support multi-view images as the input prompt.","Our method, dubbed MultiImageDream, reveals that transitioning from a single-image prompt to multiple-image prompts enhances the performance of multi-view and 3D object generation according to various quantitative evaluation metrics and qualitative assessments.","This advancement is achieved without the necessity of fine-tuning the pre-trained ImageDream multi-view diffusion model."],"url":"http://arxiv.org/abs/2404.17419v1","category":"cs.CV"}
{"created":"2024-04-26 13:39:54","title":"Characterizing Public Debt Cycles: Don't Ignore the Impact of Financial Cycles","abstract":"Based on the quarterly data from 26 advanced economies (AEs) and 18 emerging market economies (EMs) over the past two decades, this paper estimates the short- and medium-term impacts of financial cycles on the duration and amplitude of public debt cycles. The results indicate that public debt expansions are larger than their contractions in duration and amplitude, aligning with the \"deficit bias hypothesis\" and being more pronounced in EMs than in AEs. The impacts of various financial cycles are different. Specifically, credit cycles in EMs significantly impact the duration and amplitude of public debt cycles. Notably, short- and medium-term credit booms in EMs shorten the duration of public debt contractions and reduce the amplitude. Fast credit growth in AEs prolongs the duration of public debt expansions and increases the amplitude. However, credit cycles in AEs show no significant impact. For house price cycles, the overall impact is stronger in EMs than in AEs, differing between short- and medium-term cycles. Finally, the impact of equity price cycles is significant in the short term, but not in the medium term. Equity price busts are more likely to prolong the expansion of public debt in EMs while increasing the amplitude of public debt contractions in AEs. Uncovering the impacts of multiple financial cycles on public debt cycles provides implications for better debt policies under different financial conditions.","sentences":["Based on the quarterly data from 26 advanced economies (AEs) and 18 emerging market economies (EMs) over the past two decades, this paper estimates the short- and medium-term impacts of financial cycles on the duration and amplitude of public debt cycles.","The results indicate that public debt expansions are larger than their contractions in duration and amplitude, aligning with the \"deficit bias hypothesis\" and being more pronounced in EMs than in AEs.","The impacts of various financial cycles are different.","Specifically, credit cycles in EMs significantly impact the duration and amplitude of public debt cycles.","Notably, short- and medium-term credit booms in EMs shorten the duration of public debt contractions and reduce the amplitude.","Fast credit growth in AEs prolongs the duration of public debt expansions and increases the amplitude.","However, credit cycles in AEs show no significant impact.","For house price cycles, the overall impact is stronger in EMs than in AEs, differing between short- and medium-term cycles.","Finally, the impact of equity price cycles is significant in the short term, but not in the medium term.","Equity price busts are more likely to prolong the expansion of public debt in EMs while increasing the amplitude of public debt contractions in AEs.","Uncovering the impacts of multiple financial cycles on public debt cycles provides implications for better debt policies under different financial conditions."],"url":"http://arxiv.org/abs/2404.17412v1","category":"econ.GN"}
{"created":"2024-04-26 13:31:13","title":"Lens Stochastic Diffraction: A Signature of Compact Structures in Gravitational-Wave Data","abstract":"Every signal propagating through the universe is diffracted by the gravitational fields of intervening objects, aka gravitational lenses. Diffraction is most efficient when caused by compact lenses, which invariably produce additional images of a source. The signals associated with additional images are generically faint, but their collective effect may be detectable with coherent sources, such as gravitational waves (GWs), where both amplitude and phase are measured. Here, I describe lens stochastic diffraction (LSD): Poisson-distributed fluctuations after GW events caused by compact lenses. The amplitude and temporal distribution of these signals encode crucial information about the mass and abundance of compact lenses. Through the collective stochastic signal, LSD offers an order-of-magnitude improvement over single lens analysis for objects with mass $\\gtrsim 10^3 M_\\odot$. This gain can improve limits on compact dark-matter halos and allows next-generation instruments to detect supermassive black holes, given the abundance inferred from quasar luminosity studies.","sentences":["Every signal propagating through the universe is diffracted by the gravitational fields of intervening objects, aka gravitational lenses.","Diffraction is most efficient when caused by compact lenses, which invariably produce additional images of a source.","The signals associated with additional images are generically faint, but their collective effect may be detectable with coherent sources, such as gravitational waves (GWs), where both amplitude and phase are measured.","Here, I describe lens stochastic diffraction (LSD): Poisson-distributed fluctuations after GW events caused by compact lenses.","The amplitude and temporal distribution of these signals encode crucial information about the mass and abundance of compact lenses.","Through the collective stochastic signal, LSD offers an order-of-magnitude improvement over single lens analysis for objects with mass $\\gtrsim 10^3 M_\\odot$. This gain can improve limits on compact dark-matter halos and allows next-generation instruments to detect supermassive black holes, given the abundance inferred from quasar luminosity studies."],"url":"http://arxiv.org/abs/2404.17405v1","category":"gr-qc"}
{"created":"2024-04-26 13:21:31","title":"Spatial-frequency Dual-Domain Feature Fusion Network for Low-Light Remote Sensing Image Enhancement","abstract":"Low-light remote sensing images generally feature high resolution and high spatial complexity, with continuously distributed surface features in space. This continuity in scenes leads to extensive long-range correlations in spatial domains within remote sensing images. Convolutional Neural Networks, which rely on local correlations for long-distance modeling, struggle to establish long-range correlations in such images. On the other hand, transformer-based methods that focus on global information face high computational complexities when processing high-resolution remote sensing images. From another perspective, Fourier transform can compute global information without introducing a large number of parameters, enabling the network to more efficiently capture the overall image structure and establish long-range correlations. Therefore, we propose a Dual-Domain Feature Fusion Network (DFFN) for low-light remote sensing image enhancement. Specifically, this challenging task of low-light enhancement is divided into two more manageable sub-tasks: the first phase learns amplitude information to restore image brightness, and the second phase learns phase information to refine details. To facilitate information exchange between the two phases, we designed an information fusion affine block that combines data from different phases and scales. Additionally, we have constructed two dark light remote sensing datasets to address the current lack of datasets in dark light remote sensing image enhancement. Extensive evaluations show that our method outperforms existing state-of-the-art methods. The code is available at https://github.com/iijjlk/DFFN.","sentences":["Low-light remote sensing images generally feature high resolution and high spatial complexity, with continuously distributed surface features in space.","This continuity in scenes leads to extensive long-range correlations in spatial domains within remote sensing images.","Convolutional Neural Networks, which rely on local correlations for long-distance modeling, struggle to establish long-range correlations in such images.","On the other hand, transformer-based methods that focus on global information face high computational complexities when processing high-resolution remote sensing images.","From another perspective, Fourier transform can compute global information without introducing a large number of parameters, enabling the network to more efficiently capture the overall image structure and establish long-range correlations.","Therefore, we propose a Dual-Domain Feature Fusion Network (DFFN) for low-light remote sensing image enhancement.","Specifically, this challenging task of low-light enhancement is divided into two more manageable sub-tasks: the first phase learns amplitude information to restore image brightness, and the second phase learns phase information to refine details.","To facilitate information exchange between the two phases, we designed an information fusion affine block that combines data from different phases and scales.","Additionally, we have constructed two dark light remote sensing datasets to address the current lack of datasets in dark light remote sensing image enhancement.","Extensive evaluations show that our method outperforms existing state-of-the-art methods.","The code is available at https://github.com/iijjlk/DFFN."],"url":"http://arxiv.org/abs/2404.17400v1","category":"cs.CV"}
{"created":"2024-04-26 13:19:27","title":"Online Policy Learning and Inference by Matrix Completion","abstract":"Making online decisions can be challenging when features are sparse and orthogonal to historical ones, especially when the optimal policy is learned through collaborative filtering. We formulate the problem as a matrix completion bandit (MCB), where the expected reward under each arm is characterized by an unknown low-rank matrix. The $\\epsilon$-greedy bandit and the online gradient descent algorithm are explored. Policy learning and regret performance are studied under a specific schedule for exploration probabilities and step sizes. A faster decaying exploration probability yields smaller regret but learns the optimal policy less accurately. We investigate an online debiasing method based on inverse propensity weighting (IPW) and a general framework for online policy inference. The IPW-based estimators are asymptotically normal under mild arm-optimality conditions. Numerical simulations corroborate our theoretical findings. Our methods are applied to the San Francisco parking pricing project data, revealing intriguing discoveries and outperforming the benchmark policy.","sentences":["Making online decisions can be challenging when features are sparse and orthogonal to historical ones, especially when the optimal policy is learned through collaborative filtering.","We formulate the problem as a matrix completion bandit (MCB), where the expected reward under each arm is characterized by an unknown low-rank matrix.","The $\\epsilon$-greedy bandit and the online gradient descent algorithm are explored.","Policy learning and regret performance are studied under a specific schedule for exploration probabilities and step sizes.","A faster decaying exploration probability yields smaller regret but learns the optimal policy less accurately.","We investigate an online debiasing method based on inverse propensity weighting (IPW) and a general framework for online policy inference.","The IPW-based estimators are asymptotically normal under mild arm-optimality conditions.","Numerical simulations corroborate our theoretical findings.","Our methods are applied to the San Francisco parking pricing project data, revealing intriguing discoveries and outperforming the benchmark policy."],"url":"http://arxiv.org/abs/2404.17398v1","category":"stat.ML"}
{"created":"2024-04-26 13:18:09","title":"Individual particle approach to the diffusive shock acceleration. Effect of the non-uniform flow velocity downstream of the shock","abstract":"The momentum distribution of particles accelerated at strong non-relativistic shocks may be influenced by the spatial distribution of the flow speed around the shock. This phenomenon becomes evident in the cosmic-ray modified shock, where the particle spectrum itself determines the flow velocity profile upstream. However, what if the flow speed is not uniform downstream as well? Hydrodynamics indicates that its spatial variation over the length scales involved in the acceleration of particles in supernova remnants (SNRs) could be noticeable.} {In the present paper, we address this issue, initially following Bell's approach to particle acceleration and then by solving the kinetic equation. We obtained an analytical solution for the momentum distribution of particles accelerated at the cosmic-ray modified shock with spatially variable flow speed downstream.} {We parameterized the downstream speed profile to illustrate its effect on two model cases, the test particle and non-linear acceleration at the shock.The resulting particle spectrum is generally softer in Sedov SNRs because the flow speed distribution reduces the overall shock compression accessible to particles with higher momenta. On the other hand, the flow structure in young SNRs could lead to harder spectra. The diffusive properties of particles play a crucial role as they determine the distance from the shock, and, as a consequence, the flow speed that particles encounter downstream. We discuss the effect of the plasma velocity gradient to be (partially) responsible for the evolution of the radio index and for the high-energy break visible in gamma rays from some SNRs. We expect that the effect from the gradient of the flow velocity downstream could be prominent in regions of SNRs with higher diffusion coefficient and lower magnetic field, i.e. where acceleration of particles is not very efficient.","sentences":["The momentum distribution of particles accelerated at strong non-relativistic shocks may be influenced by the spatial distribution of the flow speed around the shock.","This phenomenon becomes evident in the cosmic-ray modified shock, where the particle spectrum itself determines the flow velocity profile upstream.","However, what if the flow speed is not uniform downstream as well?","Hydrodynamics indicates that its spatial variation over the length scales involved in the acceleration of particles in supernova remnants (SNRs) could be noticeable.}","{In the present paper, we address this issue, initially following Bell's approach to particle acceleration and then by solving the kinetic equation.","We obtained an analytical solution for the momentum distribution of particles accelerated at the cosmic-ray modified shock with spatially variable flow speed downstream.}","{We parameterized the downstream speed profile to illustrate its effect on two model cases, the test particle and non-linear acceleration at the shock.","The resulting particle spectrum is generally softer in Sedov SNRs because the flow speed distribution reduces the overall shock compression accessible to particles with higher momenta.","On the other hand, the flow structure in young SNRs could lead to harder spectra.","The diffusive properties of particles play a crucial role as they determine the distance from the shock, and, as a consequence, the flow speed that particles encounter downstream.","We discuss the effect of the plasma velocity gradient to be (partially) responsible for the evolution of the radio index and for the high-energy break visible in gamma rays from some SNRs.","We expect that the effect from the gradient of the flow velocity downstream could be prominent in regions of SNRs with higher diffusion coefficient and lower magnetic field, i.e. where acceleration of particles is not very efficient."],"url":"http://arxiv.org/abs/2404.17397v1","category":"astro-ph.HE"}
{"created":"2024-04-26 13:09:35","title":"M3BAT: Unsupervised Domain Adaptation for Multimodal Mobile Sensing with Multi-Branch Adversarial Training","abstract":"Over the years, multimodal mobile sensing has been used extensively for inferences regarding health and well being, behavior, and context. However, a significant challenge hindering the widespread deployment of such models in real world scenarios is the issue of distribution shift. This is the phenomenon where the distribution of data in the training set differs from the distribution of data in the real world, the deployment environment. While extensively explored in computer vision and natural language processing, and while prior research in mobile sensing briefly addresses this concern, current work primarily focuses on models dealing with a single modality of data, such as audio or accelerometer readings, and consequently, there is little research on unsupervised domain adaptation when dealing with multimodal sensor data. To address this gap, we did extensive experiments with domain adversarial neural networks (DANN) showing that they can effectively handle distribution shifts in multimodal sensor data. Moreover, we proposed a novel improvement over DANN, called M3BAT, unsupervised domain adaptation for multimodal mobile sensing with multi-branch adversarial training, to account for the multimodality of sensor data during domain adaptation with multiple branches. Through extensive experiments conducted on two multimodal mobile sensing datasets, three inference tasks, and 14 source-target domain pairs, including both regression and classification, we demonstrate that our approach performs effectively on unseen domains. Compared to directly deploying a model trained in the source domain to the target domain, the model shows performance increases up to 12% AUC (area under the receiver operating characteristics curves) on classification tasks, and up to 0.13 MAE (mean absolute error) on regression tasks.","sentences":["Over the years, multimodal mobile sensing has been used extensively for inferences regarding health and well being, behavior, and context.","However, a significant challenge hindering the widespread deployment of such models in real world scenarios is the issue of distribution shift.","This is the phenomenon where the distribution of data in the training set differs from the distribution of data in the real world, the deployment environment.","While extensively explored in computer vision and natural language processing, and while prior research in mobile sensing briefly addresses this concern, current work primarily focuses on models dealing with a single modality of data, such as audio or accelerometer readings, and consequently, there is little research on unsupervised domain adaptation when dealing with multimodal sensor data.","To address this gap, we did extensive experiments with domain adversarial neural networks (DANN) showing that they can effectively handle distribution shifts in multimodal sensor data.","Moreover, we proposed a novel improvement over DANN, called M3BAT, unsupervised domain adaptation for multimodal mobile sensing with multi-branch adversarial training, to account for the multimodality of sensor data during domain adaptation with multiple branches.","Through extensive experiments conducted on two multimodal mobile sensing datasets, three inference tasks, and 14 source-target domain pairs, including both regression and classification, we demonstrate that our approach performs effectively on unseen domains.","Compared to directly deploying a model trained in the source domain to the target domain, the model shows performance increases up to 12% AUC (area under the receiver operating characteristics curves) on classification tasks, and up to 0.13 MAE (mean absolute error) on regression tasks."],"url":"http://arxiv.org/abs/2404.17391v1","category":"cs.LG"}
{"created":"2024-04-26 13:06:52","title":"How Could AI Support Design Education? A Study Across Fields Fuels Situating Analytics","abstract":"We use the process and findings from a case study of design educators' practices of assessment and feedback to fuel theorizing about how to make AI useful in service of human experience. We build on Suchman's theory of situated actions. We perform a qualitative study of 11 educators in 5 fields, who teach design processes situated in project-based learning contexts. Through qualitative data gathering and analysis, we derive codes: design process; assessment and feedback challenges; and computational support.   We twice invoke creative cognition's family resemblance principle. First, to explain how design instructors already use assessment rubrics and second, to explain the analogous role for design creativity analytics: no particular trait is necessary or sufficient; each only tends to indicate good design work. Human teachers remain essential. We develop a set of situated design creativity analytics--Fluency, Flexibility, Visual Consistency, Multiscale Organization, and Legible Contrast--to support instructors' efforts, by providing on-demand, learning objectives-based assessment and feedback to students.   We theorize a methodology, which we call situating analytics, firstly because making AI support living human activity depends on aligning what analytics measure with situated practices. Further, we realize that analytics can become most significant to users by situating them through interfaces that integrate them into the material contexts of their use. Here, this means situating design creativity analytics into actual design environments. Through the case study, we identify situating analytics as a methodology for explaining analytics to users, because the iterative process of alignment with practice has the potential to enable data scientists to derive analytics that make sense as part of and support situated human experiences.","sentences":["We use the process and findings from a case study of design educators' practices of assessment and feedback to fuel theorizing about how to make AI useful in service of human experience.","We build on Suchman's theory of situated actions.","We perform a qualitative study of 11 educators in 5 fields, who teach design processes situated in project-based learning contexts.","Through qualitative data gathering and analysis, we derive codes: design process; assessment and feedback challenges; and computational support.   ","We twice invoke creative cognition's family resemblance principle.","First, to explain how design instructors already use assessment rubrics and second, to explain the analogous role for design creativity analytics: no particular trait is necessary or sufficient; each only tends to indicate good design work.","Human teachers remain essential.","We develop a set of situated design creativity analytics--Fluency, Flexibility, Visual Consistency, Multiscale Organization, and Legible Contrast--to support instructors' efforts, by providing on-demand, learning objectives-based assessment and feedback to students.   ","We theorize a methodology, which we call situating analytics, firstly because making AI support living human activity depends on aligning what analytics measure with situated practices.","Further, we realize that analytics can become most significant to users by situating them through interfaces that integrate them into the material contexts of their use.","Here, this means situating design creativity analytics into actual design environments.","Through the case study, we identify situating analytics as a methodology for explaining analytics to users, because the iterative process of alignment with practice has the potential to enable data scientists to derive analytics that make sense as part of and support situated human experiences."],"url":"http://arxiv.org/abs/2404.17390v1","category":"cs.HC"}
{"created":"2024-04-26 13:05:35","title":"Magnetic domains in ultrathin, bulk-like and proximity-coupled Europium Oxide","abstract":"The control of electron spins in materials that are simultaneously ferromagnetic and insulating opens up a wealth of quantum phenomena in spin-based electronics. Thin films of europium oxide (EuO) are ideal for the generation and manipulation of spin-polarized states, but so far there are no experimental literature reports on the magnetic domain patterns for EuO. However, at these microscopic length scales, magnetic relaxation between the remanent and demagnetized states takes place in any spintronic device. This relaxation process involves displacements of magnetic domain walls and can therefore be strongly influenced by the film structure and thickness. Here we present an investigation of the temperature-dependent behavior of magnetic domains and hysteresis in bulk-like (25 nm) and ultrathin (3 nm) EuO films. Magneto-optical Kerr microscopy is used, a technique that is a valuable tool to explore microscopic features such as spin dynamics and magnetic domain walls. Significant Kerr rotation in EuO led to high-contrast magnetic domain images in thick films, facilitating observation of domain dynamics. The critical temperature (TC) and coercivity shows strong thickness-dependent variations. The analysis and comparison of hysteresis loops and domain imaging in EuO and EuO/Co reveal proximity effect-induced antiferromagnetic coupling of both layers. To elucidate the magnetization reversal dynamics in EuO, micromagnetic simulations using MuMax3 were performed below and above TC. This comprehensive approach aims to comprehend the impact of magnetism and magnetic proximity effect in EuO on the micromagnetic scale, potentially extending its magnetic ordering beyond TC.","sentences":["The control of electron spins in materials that are simultaneously ferromagnetic and insulating opens up a wealth of quantum phenomena in spin-based electronics.","Thin films of europium oxide (EuO) are ideal for the generation and manipulation of spin-polarized states, but so far there are no experimental literature reports on the magnetic domain patterns for EuO.","However, at these microscopic length scales, magnetic relaxation between the remanent and demagnetized states takes place in any spintronic device.","This relaxation process involves displacements of magnetic domain walls and can therefore be strongly influenced by the film structure and thickness.","Here we present an investigation of the temperature-dependent behavior of magnetic domains and hysteresis in bulk-like (25 nm) and ultrathin (3 nm) EuO films.","Magneto-optical Kerr microscopy is used, a technique that is a valuable tool to explore microscopic features such as spin dynamics and magnetic domain walls.","Significant Kerr rotation in EuO led to high-contrast magnetic domain images in thick films, facilitating observation of domain dynamics.","The critical temperature (TC) and coercivity shows strong thickness-dependent variations.","The analysis and comparison of hysteresis loops and domain imaging in EuO and EuO/Co reveal proximity effect-induced antiferromagnetic coupling of both layers.","To elucidate the magnetization reversal dynamics in EuO, micromagnetic simulations using MuMax3 were performed below and above TC.","This comprehensive approach aims to comprehend the impact of magnetism and magnetic proximity effect in EuO on the micromagnetic scale, potentially extending its magnetic ordering beyond TC."],"url":"http://arxiv.org/abs/2404.17388v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-26 13:02:20","title":"Stochastic Bregman Subgradient Methods for Nonsmooth Nonconvex Optimization Problems","abstract":"This paper focuses on the problem of minimizing a locally Lipschitz continuous function. Motivated by the effectiveness of Bregman gradient methods in training nonsmooth deep neural networks and the recent progress in stochastic subgradient methods for nonsmooth nonconvex optimization problems \\cite{bolte2021conservative,bolte2022subgradient,xiao2023adam}, we investigate the long-term behavior of stochastic Bregman subgradient methods in such context, especially when the objective function lacks Clarke regularity. We begin by exploring a general framework for Bregman-type methods, establishing their convergence by a differential inclusion approach. For practical applications, we develop a stochastic Bregman subgradient method that allows the subproblems to be solved inexactly. Furthermore, we demonstrate how a single timescale momentum can be integrated into the Bregman subgradient method with slight modifications to the momentum update. Additionally, we introduce a Bregman proximal subgradient method for solving composite optimization problems possibly with constraints, whose convergence can be guaranteed based on the general framework. Numerical experiments on training nonsmooth neural networks are conducted to validate the effectiveness of our proposed methods.","sentences":["This paper focuses on the problem of minimizing a locally Lipschitz continuous function.","Motivated by the effectiveness of Bregman gradient methods in training nonsmooth deep neural networks and the recent progress in stochastic subgradient methods for nonsmooth nonconvex optimization problems \\cite{bolte2021conservative,bolte2022subgradient,xiao2023adam}, we investigate the long-term behavior of stochastic Bregman subgradient methods in such context, especially when the objective function lacks Clarke regularity.","We begin by exploring a general framework for Bregman-type methods, establishing their convergence by a differential inclusion approach.","For practical applications, we develop a stochastic Bregman subgradient method that allows the subproblems to be solved inexactly.","Furthermore, we demonstrate how a single timescale momentum can be integrated into the Bregman subgradient method with slight modifications to the momentum update.","Additionally, we introduce a Bregman proximal subgradient method for solving composite optimization problems possibly with constraints, whose convergence can be guaranteed based on the general framework.","Numerical experiments on training nonsmooth neural networks are conducted to validate the effectiveness of our proposed methods."],"url":"http://arxiv.org/abs/2404.17386v1","category":"math.OC"}
{"created":"2024-04-26 12:57:53","title":"Counterexamples to generalizations of the Erd\u0151s $B+B+t$ problem","abstract":"Following their resolution of the Erd\\H{o}s $B+B+t$ problem, Kra Moreira, Richter, and Robertson posed a number of questions and conjectures related to infinite configurations in positive density subsets of the integers and other amenable groups. We give a negative answer to several of these questions and conjectures by producing families of counterexamples based on a construction of Ernst Straus.   Included among our counterexamples, we exhibit, for any $\\varepsilon > 0$, a set $A \\subseteq \\mathbb{N}$ with multiplicative upper Banach density at least $1 - \\varepsilon$ such that $A$ does not contain any dilated product set $\\{b_1b_2t : b_1, b_2 \\in B, b_1 \\ne b_2\\}$ for an infinite set $B \\subseteq \\mathbb{N}$ and $t \\in \\mathbb{Q}_{>0}$. We also prove the existence of a set $A \\subseteq \\mathbb{N}$ with additive upper Banach density at least $1 - \\varepsilon$ such that $A$ does not contain any polynomial configuration $\\{b_1^2 + b_2 + t : b_1, b_2 \\in B, b_1 < b_2\\}$ for an infinite set $B \\subseteq \\mathbb{N}$ and $t \\in \\mathbb{Z}$. Counterexamples to some closely related problems are also discussed.","sentences":["Following their resolution of the Erd\\H{o}s $B+B+t$ problem, Kra Moreira, Richter, and Robertson posed a number of questions and conjectures related to infinite configurations in positive density subsets of the integers and other amenable groups.","We give a negative answer to several of these questions and conjectures by producing families of counterexamples based on a construction of Ernst Straus.   ","Included among our counterexamples, we exhibit, for any $\\varepsilon > 0$, a set $A \\subseteq \\mathbb{N}$ with multiplicative upper Banach density at least $1 - \\varepsilon$ such that $A$ does not contain any dilated product set $\\{b_1b_2t : b_1, b_2 \\in B, b_1 \\ne b_2\\}$ for an infinite set $B \\subseteq \\mathbb{N}$ and $t \\in \\mathbb{Q}_{>0}$. We also prove the existence of a set $A \\subseteq \\mathbb{N}$ with additive upper Banach density at least $1 - \\varepsilon$ such that $A$ does not contain any polynomial configuration $\\{b_1^2 + b_2 + t : b_1, b_2 \\in B, b_1 < b_2\\}$ for an infinite set $B \\subseteq \\mathbb{N}$ and $t \\in \\mathbb{Z}$. Counterexamples to some closely related problems are also discussed."],"url":"http://arxiv.org/abs/2404.17383v1","category":"math.CO"}
{"created":"2024-04-26 12:57:27","title":"Perturbations in dense matter relativistic stars induced by internal sources","abstract":"Einstein's field equations with a background source term that induces perturbations and the applications of this new formalism to a compact dense matter relativistic star are presented. We introduce a new response kernel in the field equations between the metric and fluid perturbations. A source term which drives or induces the sub-hydro mesoscopic scales perturbations in the astrophysical system, is of importance here. Deterministic as well as stochastic perturbations for the radial case are worked out as solutions of field equations. We also touch upon polar perturbations that are deterministic with oscillatory parts. The stochastic perturbation are of significance in terms of two point or point separated correlations which form the building blocks for studying equilibrium and non-equilibrium statistical mechanics for the system. Our main aim is to build a theory for intermediate scale physics, for dense exotic matter and investigate structure of the compact astrophysical objects. Specifically, turbulence which connects various scales in superfluid matter in the dense stars is an area gaining importance. The work presented here is the starting point for new theoretical frame work that touches upon various yet unexplored scales where mechanical and dynamical effects interior to the matter of the star are of significance. Thus there is scope for extending studies in asteroseismology to mesoscopic effects at the new intermediate scales in the cold dense matter fluid . This is expected to enable us to probe astrophysical features at refined scales through theoretical formulations as well as for observational consequences.","sentences":["Einstein's field equations with a background source term that induces perturbations and the applications of this new formalism to a compact dense matter relativistic star are presented.","We introduce a new response kernel in the field equations between the metric and fluid perturbations.","A source term which drives or induces the sub-hydro mesoscopic scales perturbations in the astrophysical system, is of importance here.","Deterministic as well as stochastic perturbations for the radial case are worked out as solutions of field equations.","We also touch upon polar perturbations that are deterministic with oscillatory parts.","The stochastic perturbation are of significance in terms of two point or point separated correlations which form the building blocks for studying equilibrium and non-equilibrium statistical mechanics for the system.","Our main aim is to build a theory for intermediate scale physics, for dense exotic matter and investigate structure of the compact astrophysical objects.","Specifically, turbulence which connects various scales in superfluid matter in the dense stars is an area gaining importance.","The work presented here is the starting point for new theoretical frame work that touches upon various yet unexplored scales where mechanical and dynamical effects interior to the matter of the star are of significance.","Thus there is scope for extending studies in asteroseismology to mesoscopic effects at the new intermediate scales in the cold dense matter fluid .","This is expected to enable us to probe astrophysical features at refined scales through theoretical formulations as well as for observational consequences."],"url":"http://arxiv.org/abs/2404.17382v1","category":"astro-ph.HE"}
{"created":"2024-04-26 12:52:45","title":"Quantum Adjoint Convolutional Layers for Effective Data Representation","abstract":"Quantum Convolutional Layer (QCL) is considered as one of the core of Quantum Convolutional Neural Networks (QCNNs) due to its efficient data feature extraction capability. However, the current principle of QCL is not as mathematically understandable as Classical Convolutional Layer (CCL) due to its black-box structure. Moreover, classical data mapping in many QCLs is inefficient. To this end, firstly, the Quantum Adjoint Convolution Operation (QACO) consisting of a quantum amplitude encoding and its inverse is theoretically shown to be equivalent to the quantum normalization of the convolution operation based on the Frobenius inner product while achieving an efficient characterization of the data. Subsequently, QACO is extended into a Quantum Adjoint Convolutional Layer (QACL) by Quantum Phase Estimation (QPE) to compute all Frobenius inner products in parallel. At last, comparative simulation experiments are carried out on PennyLane and TensorFlow platforms, mainly for the two cases of kernel fixed and unfixed in QACL. The results demonstrate that QACL with the insight of special quantum properties for the same images, provides higher training accuracy in MNIST and Fashion MNIST classification experiments, but sacrifices the learning performance to some extent. Predictably, our research lays the foundation for the development of efficient and interpretable quantum convolutional networks and also advances the field of quantum machine vision.","sentences":["Quantum Convolutional Layer (QCL) is considered as one of the core of Quantum Convolutional Neural Networks (QCNNs) due to its efficient data feature extraction capability.","However, the current principle of QCL is not as mathematically understandable as Classical Convolutional Layer (CCL) due to its black-box structure.","Moreover, classical data mapping in many QCLs is inefficient.","To this end, firstly, the Quantum Adjoint Convolution Operation (QACO) consisting of a quantum amplitude encoding and its inverse is theoretically shown to be equivalent to the quantum normalization of the convolution operation based on the Frobenius inner product while achieving an efficient characterization of the data.","Subsequently, QACO is extended into a Quantum Adjoint Convolutional Layer (QACL) by Quantum Phase Estimation (QPE) to compute all Frobenius inner products in parallel.","At last, comparative simulation experiments are carried out on PennyLane and TensorFlow platforms, mainly for the two cases of kernel fixed and unfixed in QACL.","The results demonstrate that QACL with the insight of special quantum properties for the same images, provides higher training accuracy in MNIST and Fashion MNIST classification experiments, but sacrifices the learning performance to some extent.","Predictably, our research lays the foundation for the development of efficient and interpretable quantum convolutional networks and also advances the field of quantum machine vision."],"url":"http://arxiv.org/abs/2404.17378v1","category":"quant-ph"}
{"created":"2024-04-26 12:43:24","title":"CEM-GMsFEM for Poisson equations in heterogeneous perforated domains","abstract":"In this paper, we propose a novel multiscale model reduction strategy tailored to address the Poisson equation within heterogeneous perforated domains. The numerical simulation of this intricate problem is impeded by its multiscale characteristics, necessitating an exceptionally fine mesh to adequately capture all relevant details. To overcome the challenges inherent in the multiscale nature of the perforations, we introduce a coarse space constructed using the Constraint Energy Minimizing Generalized Multiscale Finite Element Method (CEM-GMsFEM). This involves constructing basis functions through a sequence of local energy minimization problems over eigenspaces containing localized information pertaining to the heterogeneities. Through our analysis, we demonstrate that the oversampling layers depend on the local eigenvalues, thereby implicating the local geometry as well. Additionally, we provide numerical examples to illustrate the efficacy of the proposed scheme.","sentences":["In this paper, we propose a novel multiscale model reduction strategy tailored to address the Poisson equation within heterogeneous perforated domains.","The numerical simulation of this intricate problem is impeded by its multiscale characteristics, necessitating an exceptionally fine mesh to adequately capture all relevant details.","To overcome the challenges inherent in the multiscale nature of the perforations, we introduce a coarse space constructed using the Constraint Energy Minimizing Generalized Multiscale Finite Element Method (CEM-GMsFEM).","This involves constructing basis functions through a sequence of local energy minimization problems over eigenspaces containing localized information pertaining to the heterogeneities.","Through our analysis, we demonstrate that the oversampling layers depend on the local eigenvalues, thereby implicating the local geometry as well.","Additionally, we provide numerical examples to illustrate the efficacy of the proposed scheme."],"url":"http://arxiv.org/abs/2404.17372v1","category":"math.NA"}
{"created":"2024-04-26 12:43:16","title":"Off-axis holographic imaging with undetected light","abstract":"Quantum imaging with undetected light (QIUL) can retrieve amplitude and phase information of an object by exploiting the quantum correlations of photon-pairs generated through spontaneous parametric down conversion (SPDC), where the illumination and detection can be carried at very distinct wavelength ranges. This fact allows to benefit from a mature detection technology in the visible spectral range, while probing the object at a more exotic wavelength. Here we experimentally implement a QIUL approach with Fourier off-axis holography in a hybrid-type induced-coherence non-linear interferometer. Our approach reconstructs the amplitude and phase information of an object with a single shot in a wide-field configuration, being an alternative in front of techniques that require multiple acquisition frames, such as phase-shifting holography.","sentences":["Quantum imaging with undetected light (QIUL) can retrieve amplitude and phase information of an object by exploiting the quantum correlations of photon-pairs generated through spontaneous parametric down conversion (SPDC), where the illumination and detection can be carried at very distinct wavelength ranges.","This fact allows to benefit from a mature detection technology in the visible spectral range, while probing the object at a more exotic wavelength.","Here we experimentally implement a QIUL approach with Fourier off-axis holography in a hybrid-type induced-coherence non-linear interferometer.","Our approach reconstructs the amplitude and phase information of an object with a single shot in a wide-field configuration, being an alternative in front of techniques that require multiple acquisition frames, such as phase-shifting holography."],"url":"http://arxiv.org/abs/2404.17370v1","category":"physics.optics"}
{"created":"2024-04-26 12:42:39","title":"Assessing the Potential of AI for Spatially Sensitive Nature-Related Financial Risks","abstract":"There is growing recognition among financial institutions, financial regulators and policy makers of the importance of addressing nature-related risks and opportunities. Evaluating and assessing nature-related risks for financial institutions is challenging due to the large volume of heterogeneous data available on nature and the complexity of investment value chains and the various components' relationship to nature. The dual problem of scaling data analytics and analysing complex systems can be addressed using Artificial Intelligence (AI). We address issues such as plugging existing data gaps with discovered data, data estimation under uncertainty, time series analysis and (near) real-time updates. This report presents potential AI solutions for models of two distinct use cases, the Brazil Beef Supply Use Case and the Water Utility Use Case. Our two use cases cover a broad perspective within sustainable finance. The Brazilian cattle farming use case is an example of greening finance - integrating nature-related considerations into mainstream financial decision-making to transition investments away from sectors with poor historical track records and unsustainable operations. The deployment of nature-based solutions in the UK water utility use case is an example of financing green - driving investment to nature-positive outcomes. The two use cases also cover different sectors, geographies, financial assets and AI modelling techniques, providing an overview on how AI could be applied to different challenges relating to nature's integration into finance. This report is primarily aimed at financial institutions but is also of interest to ESG data providers, TNFD, systems modellers, and, of course, AI practitioners.","sentences":["There is growing recognition among financial institutions, financial regulators and policy makers of the importance of addressing nature-related risks and opportunities.","Evaluating and assessing nature-related risks for financial institutions is challenging due to the large volume of heterogeneous data available on nature and the complexity of investment value chains and the various components' relationship to nature.","The dual problem of scaling data analytics and analysing complex systems can be addressed using Artificial Intelligence (AI).","We address issues such as plugging existing data gaps with discovered data, data estimation under uncertainty, time series analysis and (near) real-time updates.","This report presents potential AI solutions for models of two distinct use cases, the Brazil Beef Supply Use Case and the Water Utility Use Case.","Our two use cases cover a broad perspective within sustainable finance.","The Brazilian cattle farming use case is an example of greening finance - integrating nature-related considerations into mainstream financial decision-making to transition investments away from sectors with poor historical track records and unsustainable operations.","The deployment of nature-based solutions in the UK water utility use case is an example of financing green - driving investment to nature-positive outcomes.","The two use cases also cover different sectors, geographies, financial assets and AI modelling techniques, providing an overview on how AI could be applied to different challenges relating to nature's integration into finance.","This report is primarily aimed at financial institutions but is also of interest to ESG data providers, TNFD, systems modellers, and, of course, AI practitioners."],"url":"http://arxiv.org/abs/2404.17369v1","category":"q-fin.CP"}
{"created":"2024-04-26 12:41:42","title":"Inefficient star formation in high Mach number environments I. The turbulent support analytical model","abstract":"The star formation rate (SFR), the number of stars formed per unit of time, is a fundamental quantity in the evolution of the Universe. While turbulence is believed to play a crucial role in setting the SFR, the exact mechanism remains unclear. Turbulence promotes star formation by compressing the gas, but also slows it down by stabilizing the gas against gravity. Most widely-used analytical models rely on questionable assumptions, including: $i)$ integrating over the density PDF, a one-point statistical description that ignores spatial correlation, $ii)$ selecting self-gravitating gas based on a density threshold that often ignores turbulent dispersion, $iii)$ assuming the freefall time as the timescale for estimating SFR without considering the need to rejuvenate the density PDF, $iv)$ assuming the density PDF to be lognormal. Improving upon the only existing model that incorporates the spatial correlation of the density field, we present a new analytical model. We calculate the time needed to rejuvenate density fluctuations of a given density and spatial scale, revealing that it is generally much longer than the freefall time, rendering the latter inappropriate for use. We make specific predictions regarding the role of the Mach number, $ M $, and the driving scale of turbulence divided by the mean Jeans length. At low to moderate Mach numbers, turbulence does not reduce and may even slightly promote star formation by broadening the PDF. However, at higher Mach numbers, most density fluctuations are stabilized by turbulent dispersion, leading to a steep drop in the SFR as the Mach number increases.","sentences":["The star formation rate (SFR), the number of stars formed per unit of time, is a fundamental quantity in the evolution of the Universe.","While turbulence is believed to play a crucial role in setting the SFR, the exact mechanism remains unclear.","Turbulence promotes star formation by compressing the gas, but also slows it down by stabilizing the gas against gravity.","Most widely-used analytical models rely on questionable assumptions, including: $i)$ integrating over the density PDF, a one-point statistical description that ignores spatial correlation, $ii)$ selecting self-gravitating gas based on a density threshold that often ignores turbulent dispersion, $iii)$ assuming the freefall time as the timescale for estimating SFR without considering the need to rejuvenate the density PDF, $iv)$ assuming the density PDF to be lognormal.","Improving upon the only existing model that incorporates the spatial correlation of the density field, we present a new analytical model.","We calculate the time needed to rejuvenate density fluctuations of a given density and spatial scale, revealing that it is generally much longer than the freefall time, rendering the latter inappropriate for use.","We make specific predictions regarding the role of the Mach number, $ M $, and the driving scale of turbulence divided by the mean Jeans length.","At low to moderate Mach numbers, turbulence does not reduce and may even slightly promote star formation by broadening the PDF.","However, at higher Mach numbers, most density fluctuations are stabilized by turbulent dispersion, leading to a steep drop in the SFR as the Mach number increases."],"url":"http://arxiv.org/abs/2404.17368v1","category":"astro-ph.GA"}
{"created":"2024-04-26 12:30:32","title":"Similarity Equivariant Graph Neural Networks for Homogenization of Metamaterials","abstract":"Soft, porous mechanical metamaterials exhibit pattern transformations that may have important applications in soft robotics, sound reduction and biomedicine. To design these innovative materials, it is important to be able to simulate them accurately and quickly, in order to tune their mechanical properties. Since conventional simulations using the finite element method entail a high computational cost, in this article we aim to develop a machine learning-based approach that scales favorably to serve as a surrogate model. To ensure that the model is also able to handle various microstructures, including those not encountered during training, we include the microstructure as part of the network input. Therefore, we introduce a graph neural network that predicts global quantities (energy, stress stiffness) as well as the pattern transformations that occur (the kinematics). To make our model as accurate and data-efficient as possible, various symmetries are incorporated into the model. The starting point is an E(n)-equivariant graph neural network (which respects translation, rotation and reflection) that has periodic boundary conditions (i.e., it is in-/equivariant with respect to the choice of RVE), is scale in-/equivariant, can simulate large deformations, and can predict scalars, vectors as well as second and fourth order tensors (specifically energy, stress and stiffness). The incorporation of scale equivariance makes the model equivariant with respect to the similarities group, of which the Euclidean group E(n) is a subgroup. We show that this network is more accurate and data-efficient than graph neural networks with fewer symmetries. To create an efficient graph representation of the finite element discretization, we use only the internal geometrical hole boundaries from the finite element mesh to achieve a better speed-up and scaling with the mesh size.","sentences":["Soft, porous mechanical metamaterials exhibit pattern transformations that may have important applications in soft robotics, sound reduction and biomedicine.","To design these innovative materials, it is important to be able to simulate them accurately and quickly, in order to tune their mechanical properties.","Since conventional simulations using the finite element method entail a high computational cost, in this article we aim to develop a machine learning-based approach that scales favorably to serve as a surrogate model.","To ensure that the model is also able to handle various microstructures, including those not encountered during training, we include the microstructure as part of the network input.","Therefore, we introduce a graph neural network that predicts global quantities (energy, stress stiffness) as well as the pattern transformations that occur (the kinematics).","To make our model as accurate and data-efficient as possible, various symmetries are incorporated into the model.","The starting point is an E(n)-equivariant graph neural network (which respects translation, rotation and reflection) that has periodic boundary conditions (i.e., it is in-/equivariant with respect to the choice of RVE), is scale in-/equivariant, can simulate large deformations, and can predict scalars, vectors as well as second and fourth order tensors (specifically energy, stress and stiffness).","The incorporation of scale equivariance makes the model equivariant with respect to the similarities group, of which the Euclidean group E(n) is a subgroup.","We show that this network is more accurate and data-efficient than graph neural networks with fewer symmetries.","To create an efficient graph representation of the finite element discretization, we use only the internal geometrical hole boundaries from the finite element mesh to achieve a better speed-up and scaling with the mesh size."],"url":"http://arxiv.org/abs/2404.17365v1","category":"cond-mat.soft"}
{"created":"2024-04-26 12:27:57","title":"MV-VTON: Multi-View Virtual Try-On with Diffusion Models","abstract":"The goal of image-based virtual try-on is to generate an image of the target person naturally wearing the given clothing. However, most existing methods solely focus on the frontal try-on using the frontal clothing. When the views of the clothing and person are significantly inconsistent, particularly when the person's view is non-frontal, the results are unsatisfactory. To address this challenge, we introduce Multi-View Virtual Try-ON (MV-VTON), which aims to reconstruct the dressing results of a person from multiple views using the given clothes. On the one hand, given that single-view clothes provide insufficient information for MV-VTON, we instead employ two images, i.e., the frontal and back views of the clothing, to encompass the complete view as much as possible. On the other hand, the diffusion models that have demonstrated superior abilities are adopted to perform our MV-VTON. In particular, we propose a view-adaptive selection method where hard-selection and soft-selection are applied to the global and local clothing feature extraction, respectively. This ensures that the clothing features are roughly fit to the person's view. Subsequently, we suggest a joint attention block to align and fuse clothing features with person features. Additionally, we collect a MV-VTON dataset, i.e., Multi-View Garment (MVG), in which each person has multiple photos with diverse views and poses. Experiments show that the proposed method not only achieves state-of-the-art results on MV-VTON task using our MVG dataset, but also has superiority on frontal-view virtual try-on task using VITON-HD and DressCode datasets. Codes and datasets will be publicly released at https://github.com/hywang2002/MV-VTON .","sentences":["The goal of image-based virtual try-on is to generate an image of the target person naturally wearing the given clothing.","However, most existing methods solely focus on the frontal try-on using the frontal clothing.","When the views of the clothing and person are significantly inconsistent, particularly when the person's view is non-frontal, the results are unsatisfactory.","To address this challenge, we introduce Multi-View Virtual Try-ON (MV-VTON), which aims to reconstruct the dressing results of a person from multiple views using the given clothes.","On the one hand, given that single-view clothes provide insufficient information for MV-VTON, we instead employ two images, i.e., the frontal and back views of the clothing, to encompass the complete view as much as possible.","On the other hand, the diffusion models that have demonstrated superior abilities are adopted to perform our MV-VTON.","In particular, we propose a view-adaptive selection method where hard-selection and soft-selection are applied to the global and local clothing feature extraction, respectively.","This ensures that the clothing features are roughly fit to the person's view.","Subsequently, we suggest a joint attention block to align and fuse clothing features with person features.","Additionally, we collect a MV-VTON dataset, i.e., Multi-View Garment (MVG), in which each person has multiple photos with diverse views and poses.","Experiments show that the proposed method not only achieves state-of-the-art results on MV-VTON task using our MVG dataset, but also has superiority on frontal-view virtual try-on task using VITON-HD and DressCode datasets.","Codes and datasets will be publicly released at https://github.com/hywang2002/MV-VTON ."],"url":"http://arxiv.org/abs/2404.17364v1","category":"cs.CV"}
{"created":"2024-04-26 12:22:43","title":"The Kirkwood-Bethe hypothesis for bubble dynamics, cavitation and underwater explosions","abstract":"Pressure-driven bubble dynamics is a major topic of current research in fluid dynamics, driven by innovative medical therapies, sonochemistry, material treatments, and geophysical exploration. First proposed in 1942, the Kirkwood-Bethe hypothesis provides a simple means to close the equations that govern pressure-driven bubble dynamics as well as the resulting flow field and acoustic emissions in spherical symmetry. The models derived from the Kirkwood-Bethe hypothesis can be solved using standard numerical integration methods at a fraction of the computational cost required for fully resolved simulations. Here, the theoretical foundation of the Kirkwood-Bethe hypothesis and contemporary models derived from it are gathered and reviewed, as well as generalized to account for spherically symmetric, cylindrically symmetric, and planar one-dimensional domains. In addition, the underpinning assumptions are clarified and new results that scrutinize the predictive capabilities of the Kirkwood-Bethe hypothesis with respect to the complex acoustic impedance experienced by curved acoustic waves and the formation of shock waves are presented. Although the Kirkwood-Bethe hypothesis is built upon simplifying assumptions and lacks some basic acoustic properties, models derived from it are able to provide accurate predictions under the specific conditions associated with pressure-driven bubble dynamics, cavitation and underwater explosions.","sentences":["Pressure-driven bubble dynamics is a major topic of current research in fluid dynamics, driven by innovative medical therapies, sonochemistry, material treatments, and geophysical exploration.","First proposed in 1942, the Kirkwood-Bethe hypothesis provides a simple means to close the equations that govern pressure-driven bubble dynamics as well as the resulting flow field and acoustic emissions in spherical symmetry.","The models derived from the Kirkwood-Bethe hypothesis can be solved using standard numerical integration methods at a fraction of the computational cost required for fully resolved simulations.","Here, the theoretical foundation of the Kirkwood-Bethe hypothesis and contemporary models derived from it are gathered and reviewed, as well as generalized to account for spherically symmetric, cylindrically symmetric, and planar one-dimensional domains.","In addition, the underpinning assumptions are clarified and new results that scrutinize the predictive capabilities of the Kirkwood-Bethe hypothesis with respect to the complex acoustic impedance experienced by curved acoustic waves and the formation of shock waves are presented.","Although the Kirkwood-Bethe hypothesis is built upon simplifying assumptions and lacks some basic acoustic properties, models derived from it are able to provide accurate predictions under the specific conditions associated with pressure-driven bubble dynamics, cavitation and underwater explosions."],"url":"http://arxiv.org/abs/2404.17361v1","category":"physics.flu-dyn"}
{"created":"2024-04-26 12:21:57","title":"UniRGB-IR: A Unified Framework for Visible-Infrared Downstream Tasks via Adapter Tuning","abstract":"Semantic analysis on visible (RGB) and infrared (IR) images has gained attention for its ability to be more accurate and robust under low-illumination and complex weather conditions. Due to the lack of pre-trained foundation models on the large-scale infrared image datasets, existing methods prefer to design task-specific frameworks and directly fine-tune them with pre-trained foundation models on their RGB-IR semantic relevance datasets, which results in poor scalability and limited generalization. In this work, we propose a scalable and efficient framework called UniRGB-IR to unify RGB-IR downstream tasks, in which a novel adapter is developed to efficiently introduce richer RGB-IR features into the pre-trained RGB-based foundation model. Specifically, our framework consists of a vision transformer (ViT) foundation model, a Multi-modal Feature Pool (MFP) module and a Supplementary Feature Injector (SFI) module. The MFP and SFI modules cooperate with each other as an adpater to effectively complement the ViT features with the contextual multi-scale features. During training process, we freeze the entire foundation model to inherit prior knowledge and only optimize the MFP and SFI modules. Furthermore, to verify the effectiveness of our framework, we utilize the ViT-Base as the pre-trained foundation model to perform extensive experiments. Experimental results on various RGB-IR downstream tasks demonstrate that our method can achieve state-of-the-art performance. The source code and results are available at https://github.com/PoTsui99/UniRGB-IR.git.","sentences":["Semantic analysis on visible (RGB) and infrared (IR) images has gained attention for its ability to be more accurate and robust under low-illumination and complex weather conditions.","Due to the lack of pre-trained foundation models on the large-scale infrared image datasets, existing methods prefer to design task-specific frameworks and directly fine-tune them with pre-trained foundation models on their RGB-IR semantic relevance datasets, which results in poor scalability and limited generalization.","In this work, we propose a scalable and efficient framework called UniRGB-IR to unify RGB-IR downstream tasks, in which a novel adapter is developed to efficiently introduce richer RGB-IR features into the pre-trained RGB-based foundation model.","Specifically, our framework consists of a vision transformer (ViT) foundation model, a Multi-modal Feature Pool (MFP) module and a Supplementary Feature Injector (SFI) module.","The MFP and SFI modules cooperate with each other as an adpater to effectively complement the ViT features with the contextual multi-scale features.","During training process, we freeze the entire foundation model to inherit prior knowledge and only optimize the MFP and SFI modules.","Furthermore, to verify the effectiveness of our framework, we utilize the ViT-Base as the pre-trained foundation model to perform extensive experiments.","Experimental results on various RGB-IR downstream tasks demonstrate that our method can achieve state-of-the-art performance.","The source code and results are available at https://github.com/PoTsui99/UniRGB-IR.git."],"url":"http://arxiv.org/abs/2404.17360v1","category":"cs.CV"}
{"created":"2024-04-26 12:13:41","title":"Simultaneous Tri-Modal Medical Image Fusion and Super-Resolution using Conditional Diffusion Model","abstract":"In clinical practice, tri-modal medical image fusion, compared to the existing dual-modal technique, can provide a more comprehensive view of the lesions, aiding physicians in evaluating the disease's shape, location, and biological activity. However, due to the limitations of imaging equipment and considerations for patient safety, the quality of medical images is usually limited, leading to sub-optimal fusion performance, and affecting the depth of image analysis by the physician. Thus, there is an urgent need for a technology that can both enhance image resolution and integrate multi-modal information. Although current image processing methods can effectively address image fusion and super-resolution individually, solving both problems synchronously remains extremely challenging. In this paper, we propose TFS-Diff, a simultaneously realize tri-modal medical image fusion and super-resolution model. Specially, TFS-Diff is based on the diffusion model generation of a random iterative denoising process. We also develop a simple objective function and the proposed fusion super-resolution loss, effectively evaluates the uncertainty in the fusion and ensures the stability of the optimization process. And the channel attention module is proposed to effectively integrate key information from different modalities for clinical diagnosis, avoiding information loss caused by multiple image processing. Extensive experiments on public Harvard datasets show that TFS-Diff significantly surpass the existing state-of-the-art methods in both quantitative and visual evaluations. The source code will be available at GitHub.","sentences":["In clinical practice, tri-modal medical image fusion, compared to the existing dual-modal technique, can provide a more comprehensive view of the lesions, aiding physicians in evaluating the disease's shape, location, and biological activity.","However, due to the limitations of imaging equipment and considerations for patient safety, the quality of medical images is usually limited, leading to sub-optimal fusion performance, and affecting the depth of image analysis by the physician.","Thus, there is an urgent need for a technology that can both enhance image resolution and integrate multi-modal information.","Although current image processing methods can effectively address image fusion and super-resolution individually, solving both problems synchronously remains extremely challenging.","In this paper, we propose TFS-Diff, a simultaneously realize tri-modal medical image fusion and super-resolution model.","Specially, TFS-Diff is based on the diffusion model generation of a random iterative denoising process.","We also develop a simple objective function and the proposed fusion super-resolution loss, effectively evaluates the uncertainty in the fusion and ensures the stability of the optimization process.","And the channel attention module is proposed to effectively integrate key information from different modalities for clinical diagnosis, avoiding information loss caused by multiple image processing.","Extensive experiments on public Harvard datasets show that TFS-Diff significantly surpass the existing state-of-the-art methods in both quantitative and visual evaluations.","The source code will be available at GitHub."],"url":"http://arxiv.org/abs/2404.17357v1","category":"eess.IV"}
{"created":"2024-04-26 12:06:39","title":"Testing the Origins of Neutrino Mass with Supernova Neutrino Time Delay","abstract":"The origin of neutrino masses remains unknown. Both the vacuum mass and the dark mass generated by the neutrino interactions with DM particles or fields can fit the current oscillation data. The dark mass squared is proportional to the DM number density and therefore varies on the galactic scale with much larger values around the Galactic Center. This affects the group velocity and the arrival time delay of core-collapse supernovae neutrinos. This time delay, especially for the $\\nu_e$ neutronization peak with a sharp time structure, can be used to distinguish the vacuum and dark neutrino masses. For illustration, we explore the potential of DUNE which is sensitive to $\\nu_e$. Our simulations show that DUNE can distinguish the two neutrino mass origins at more than $5\\sigma\\,$C.L., depending on the observed local value of neutrino mass, the neutrino mass ordering, the DM density profile, and the SN location.","sentences":["The origin of neutrino masses remains unknown.","Both the vacuum mass and the dark mass generated by the neutrino interactions with DM particles or fields can fit the current oscillation data.","The dark mass squared is proportional to the DM number density and therefore varies on the galactic scale with much larger values around the Galactic Center.","This affects the group velocity and the arrival time delay of core-collapse supernovae neutrinos.","This time delay, especially for the $\\nu_e$ neutronization peak with a sharp time structure, can be used to distinguish the vacuum and dark neutrino masses.","For illustration, we explore the potential of DUNE which is sensitive to $\\nu_e$. Our simulations show that DUNE can distinguish the two neutrino mass origins at more than $5\\sigma\\,$C.L., depending on the observed local value of neutrino mass, the neutrino mass ordering, the DM density profile, and the SN location."],"url":"http://arxiv.org/abs/2404.17352v1","category":"hep-ph"}
{"created":"2024-04-26 11:57:17","title":"On the Road to Clarity: Exploring Explainable AI for World Models in a Driver Assistance System","abstract":"In Autonomous Driving (AD) transparency and safety are paramount, as mistakes are costly. However, neural networks used in AD systems are generally considered black boxes. As a countermeasure, we have methods of explainable AI (XAI), such as feature relevance estimation and dimensionality reduction. Coarse graining techniques can also help reduce dimensionality and find interpretable global patterns. A specific coarse graining method is Renormalization Groups from statistical physics. It has previously been applied to Restricted Boltzmann Machines (RBMs) to interpret unsupervised learning. We refine this technique by building a transparent backbone model for convolutional variational autoencoders (VAE) that allows mapping latent values to input features and has performance comparable to trained black box VAEs. Moreover, we propose a custom feature map visualization technique to analyze the internal convolutional layers in the VAE to explain internal causes of poor reconstruction that may lead to dangerous traffic scenarios in AD applications. In a second key contribution, we propose explanation and evaluation techniques for the internal dynamics and feature relevance of prediction networks. We test a long short-term memory (LSTM) network in the computer vision domain to evaluate the predictability and in future applications potentially safety of prediction models. We showcase our methods by analyzing a VAE-LSTM world model that predicts pedestrian perception in an urban traffic situation.","sentences":["In Autonomous Driving (AD) transparency and safety are paramount, as mistakes are costly.","However, neural networks used in AD systems are generally considered black boxes.","As a countermeasure, we have methods of explainable AI (XAI), such as feature relevance estimation and dimensionality reduction.","Coarse graining techniques can also help reduce dimensionality and find interpretable global patterns.","A specific coarse graining method is Renormalization Groups from statistical physics.","It has previously been applied to Restricted Boltzmann Machines (RBMs) to interpret unsupervised learning.","We refine this technique by building a transparent backbone model for convolutional variational autoencoders (VAE) that allows mapping latent values to input features and has performance comparable to trained black box VAEs.","Moreover, we propose a custom feature map visualization technique to analyze the internal convolutional layers in the VAE to explain internal causes of poor reconstruction that may lead to dangerous traffic scenarios in AD applications.","In a second key contribution, we propose explanation and evaluation techniques for the internal dynamics and feature relevance of prediction networks.","We test a long short-term memory (LSTM) network in the computer vision domain to evaluate the predictability and in future applications potentially safety of prediction models.","We showcase our methods by analyzing a VAE-LSTM world model that predicts pedestrian perception in an urban traffic situation."],"url":"http://arxiv.org/abs/2404.17350v1","category":"cs.LG"}
{"created":"2024-04-26 11:51:53","title":"InspectorRAGet: An Introspection Platform for RAG Evaluation","abstract":"Large Language Models (LLM) have become a popular approach for implementing Retrieval Augmented Generation (RAG) systems, and a significant amount of effort has been spent on building good models and metrics. In spite of increased recognition of the need for rigorous evaluation of RAG systems, few tools exist that go beyond the creation of model output and automatic calculation. We present InspectorRAGet, an introspection platform for RAG evaluation. InspectorRAGet allows the user to analyze aggregate and instance-level performance of RAG systems, using both human and algorithmic metrics as well as annotator quality. InspectorRAGet is suitable for multiple use cases and is available publicly to the community. The demo video is available at https://youtu.be/MJhe8QIXcEc","sentences":["Large Language Models (LLM) have become a popular approach for implementing Retrieval Augmented Generation (RAG) systems, and a significant amount of effort has been spent on building good models and metrics.","In spite of increased recognition of the need for rigorous evaluation of RAG systems, few tools exist that go beyond the creation of model output and automatic calculation.","We present InspectorRAGet, an introspection platform for RAG evaluation.","InspectorRAGet allows the user to analyze aggregate and instance-level performance of RAG systems, using both human and algorithmic metrics as well as annotator quality.","InspectorRAGet is suitable for multiple use cases and is available publicly to the community.","The demo video is available at https://youtu.be/MJhe8QIXcEc"],"url":"http://arxiv.org/abs/2404.17347v1","category":"cs.SE"}
{"created":"2024-04-26 11:46:05","title":"Can a Multichoice Dataset be Repurposed for Extractive Question Answering?","abstract":"The rapid evolution of Natural Language Processing (NLP) has favored major languages such as English, leaving a significant gap for many others due to limited resources. This is especially evident in the context of data annotation, a task whose importance cannot be underestimated, but which is time-consuming and costly. Thus, any dataset for resource-poor languages is precious, in particular when it is task-specific. Here, we explore the feasibility of repurposing existing datasets for a new NLP task: we repurposed the Belebele dataset (Bandarkar et al., 2023), which was designed for multiple-choice question answering (MCQA), to enable extractive QA (EQA) in the style of machine reading comprehension. We present annotation guidelines and a parallel EQA dataset for English and Modern Standard Arabic (MSA). We also present QA evaluation results for several monolingual and cross-lingual QA pairs including English, MSA, and five Arabic dialects. Our aim is to enable others to adapt our approach for the 120+ other language variants in Belebele, many of which are deemed under-resourced. We also conduct a thorough analysis and share our insights from the process, which we hope will contribute to a deeper understanding of the challenges and the opportunities associated with task reformulation in NLP research.","sentences":["The rapid evolution of Natural Language Processing (NLP) has favored major languages such as English, leaving a significant gap for many others due to limited resources.","This is especially evident in the context of data annotation, a task whose importance cannot be underestimated, but which is time-consuming and costly.","Thus, any dataset for resource-poor languages is precious, in particular when it is task-specific.","Here, we explore the feasibility of repurposing existing datasets for a new NLP task: we repurposed the Belebele dataset (Bandarkar et al., 2023), which was designed for multiple-choice question answering (MCQA), to enable extractive QA (EQA) in the style of machine reading comprehension.","We present annotation guidelines and a parallel EQA dataset for English and Modern Standard Arabic (MSA).","We also present QA evaluation results for several monolingual and cross-lingual QA pairs including English, MSA, and five Arabic dialects.","Our aim is to enable others to adapt our approach for the 120+ other language variants in Belebele, many of which are deemed under-resourced.","We also conduct a thorough analysis and share our insights from the process, which we hope will contribute to a deeper understanding of the challenges and the opportunities associated with task reformulation in NLP research."],"url":"http://arxiv.org/abs/2404.17342v1","category":"cs.CL"}
{"created":"2024-04-26 11:37:45","title":"Metronome: tracing variation in poetic meters via local sequence alignment","abstract":"All poetic forms come from somewhere. Prosodic templates can be copied for generations, altered by individuals, imported from foreign traditions, or fundamentally changed under the pressures of language evolution. Yet these relationships are notoriously difficult to trace across languages and times. This paper introduces an unsupervised method for detecting structural similarities in poems using local sequence alignment. The method relies on encoding poetic texts as strings of prosodic features using a four-letter alphabet; these sequences are then aligned to derive a distance measure based on weighted symbol (mis)matches. Local alignment allows poems to be clustered according to emergent properties of their underlying prosodic patterns. We evaluate method performance on a meter recognition tasks against strong baselines and show its potential for cross-lingual and historical research using three short case studies: 1) mutations in quantitative meter in classical Latin, 2) European diffusion of the Renaissance hendecasyllable, and 3) comparative alignment of modern meters in 18--19th century Czech, German and Russian. We release an implementation of the algorithm as a Python package with an open license.","sentences":["All poetic forms come from somewhere.","Prosodic templates can be copied for generations, altered by individuals, imported from foreign traditions, or fundamentally changed under the pressures of language evolution.","Yet these relationships are notoriously difficult to trace across languages and times.","This paper introduces an unsupervised method for detecting structural similarities in poems using local sequence alignment.","The method relies on encoding poetic texts as strings of prosodic features using a four-letter alphabet; these sequences are then aligned to derive a distance measure based on weighted symbol (mis)matches.","Local alignment allows poems to be clustered according to emergent properties of their underlying prosodic patterns.","We evaluate method performance on a meter recognition tasks against strong baselines and show its potential for cross-lingual and historical research using three short case studies: 1) mutations in quantitative meter in classical Latin, 2) European diffusion of the Renaissance hendecasyllable, and 3) comparative alignment of modern meters in 18--19th century Czech, German and Russian.","We release an implementation of the algorithm as a Python package with an open license."],"url":"http://arxiv.org/abs/2404.17337v1","category":"cs.CL"}
{"created":"2024-04-26 11:34:11","title":"Introducing cosmosGPT: Monolingual Training for Turkish Language Models","abstract":"The number of open source language models that can produce Turkish is increasing day by day, as in other languages. In order to create the basic versions of such models, the training of multilingual models is usually continued with Turkish corpora. The alternative is to train the model with only Turkish corpora. In this study, we first introduce the cosmosGPT models that we created with this alternative method. Then, we introduce new finetune datasets for basic language models to fulfill user requests and new evaluation datasets for measuring the capabilities of Turkish language models. Finally, a comprehensive comparison of the adapted Turkish language models on different capabilities is presented. The results show that the language models we built with the monolingual corpus have promising performance despite being about 10 times smaller than the others.","sentences":["The number of open source language models that can produce Turkish is increasing day by day, as in other languages.","In order to create the basic versions of such models, the training of multilingual models is usually continued with Turkish corpora.","The alternative is to train the model with only Turkish corpora.","In this study, we first introduce the cosmosGPT models that we created with this alternative method.","Then, we introduce new finetune datasets for basic language models to fulfill user requests and new evaluation datasets for measuring the capabilities of Turkish language models.","Finally, a comprehensive comparison of the adapted Turkish language models on different capabilities is presented.","The results show that the language models we built with the monolingual corpus have promising performance despite being about 10 times smaller than the others."],"url":"http://arxiv.org/abs/2404.17336v1","category":"cs.CL"}
{"created":"2024-04-26 11:32:53","title":"A Novel Spike Transformer Network for Depth Estimation from Event Cameras via Cross-modality Knowledge Distillation","abstract":"Depth estimation is crucial for interpreting complex environments, especially in areas such as autonomous vehicle navigation and robotics. Nonetheless, obtaining accurate depth readings from event camera data remains a formidable challenge. Event cameras operate differently from traditional digital cameras, continuously capturing data and generating asynchronous binary spikes that encode time, location, and light intensity. Yet, the unique sampling mechanisms of event cameras render standard image based algorithms inadequate for processing spike data. This necessitates the development of innovative, spike-aware algorithms tailored for event cameras, a task compounded by the irregularity, continuity, noise, and spatial and temporal characteristics inherent in spiking data.Harnessing the strong generalization capabilities of transformer neural networks for spatiotemporal data, we propose a purely spike-driven spike transformer network for depth estimation from spiking camera data. To address performance limitations with Spiking Neural Networks (SNN), we introduce a novel single-stage cross-modality knowledge transfer framework leveraging knowledge from a large vision foundational model of artificial neural networks (ANN) (DINOv2) to enhance the performance of SNNs with limited data. Our experimental results on both synthetic and real datasets show substantial improvements over existing models, with notable gains in Absolute Relative and Square Relative errors (49% and 39.77% improvements over the benchmark model Spike-T, respectively). Besides accuracy, the proposed model also demonstrates reduced power consumptions, a critical factor for practical applications.","sentences":["Depth estimation is crucial for interpreting complex environments, especially in areas such as autonomous vehicle navigation and robotics.","Nonetheless, obtaining accurate depth readings from event camera data remains a formidable challenge.","Event cameras operate differently from traditional digital cameras, continuously capturing data and generating asynchronous binary spikes that encode time, location, and light intensity.","Yet, the unique sampling mechanisms of event cameras render standard image based algorithms inadequate for processing spike data.","This necessitates the development of innovative, spike-aware algorithms tailored for event cameras, a task compounded by the irregularity, continuity, noise, and spatial and temporal characteristics inherent in spiking data.","Harnessing the strong generalization capabilities of transformer neural networks for spatiotemporal data, we propose a purely spike-driven spike transformer network for depth estimation from spiking camera data.","To address performance limitations with Spiking Neural Networks (SNN), we introduce a novel single-stage cross-modality knowledge transfer framework leveraging knowledge from a large vision foundational model of artificial neural networks (ANN) (DINOv2) to enhance the performance of SNNs with limited data.","Our experimental results on both synthetic and real datasets show substantial improvements over existing models, with notable gains in Absolute Relative and Square Relative errors (49% and 39.77% improvements over the benchmark model Spike-T, respectively).","Besides accuracy, the proposed model also demonstrates reduced power consumptions, a critical factor for practical applications."],"url":"http://arxiv.org/abs/2404.17335v1","category":"cs.CV"}
{"created":"2024-04-26 11:31:37","title":"New bounds on heavy axions with an X-ray free electron laser","abstract":"We present new exclusion bounds obtained at the European X-ray Free Electron Laser facility (EuXFEL) on axion-like particles (ALPs) in the mass range $10^{-3}\\;\\rm{eV} \\lesssim m_a \\lesssim 10^{4}\\;\\rm{eV}$, which is relatively unconstrained by laboratory searches. Our experiment exploits the Primakoff effect via which photons can, in the presence of a strong external electric field, decay into axions, which then convert back into photons after passing through an opaque wall. While similar searches have been performed previously at a 3$^{\\rm rd}$ generation synchrotron \\cite{Yamaji:2018ufo} our work demonstrates improved sensitivity, exploiting the higher brightness of X-rays at EuXFEL.","sentences":["We present new exclusion bounds obtained at the European X-ray Free Electron Laser facility (EuXFEL) on axion-like particles (ALPs) in the mass range $10^{-3}\\;\\rm{eV} \\lesssim m_a \\lesssim 10^{4}\\;\\rm{eV}$, which is relatively unconstrained by laboratory searches.","Our experiment exploits the Primakoff effect via which photons can, in the presence of a strong external electric field, decay into axions, which then convert back into photons after passing through an opaque wall.","While similar searches have been performed previously at a 3$^{\\rm rd}$ generation synchrotron \\cite{Yamaji:2018ufo} our work demonstrates improved sensitivity, exploiting the higher brightness of X-rays at EuXFEL."],"url":"http://arxiv.org/abs/2404.17333v1","category":"hep-ph"}
{"created":"2024-04-26 11:27:49","title":"An Extendable Cloud-Native Alloy Property Explorer","abstract":"The ability to rapidly evaluate materials properties through atomistic simulation approaches is the foundation of many new artificial intelligence-based approaches to materials identification and design. This depends on the availability of accurate descriptions of atomic bonding through various forms of interatomic potentials. We present an efficient, robust platform for calculating materials properties, i.e., APEX, the Alloy Property Explorer. APEX enables the rapid evolution of interatomic potential development and optimization, which is of particular importance in fine-tuning new classes of general AI-based foundation models to forms that are readily applicable to impacting materials development. APEX is an open-source, extendable, and cloud-native platform for material property calculations using a range of atomistic simulation methodologies that effectively manages diverse computational resources and is built upon user-friendly features including automatic results visualization, web-based platforms and NoSQL database client. It is designed for expert and non-specialist users, lowers the barrier to entry for interdisciplinary research within the \"AI for Materials\" framework. We describe the foundation and use of APEX, as well as provide an example of its application to properties of titanium for a wide-range of bonding descriptions.","sentences":["The ability to rapidly evaluate materials properties through atomistic simulation approaches is the foundation of many new artificial intelligence-based approaches to materials identification and design.","This depends on the availability of accurate descriptions of atomic bonding through various forms of interatomic potentials.","We present an efficient, robust platform for calculating materials properties, i.e., APEX, the Alloy Property Explorer.","APEX enables the rapid evolution of interatomic potential development and optimization, which is of particular importance in fine-tuning new classes of general AI-based foundation models to forms that are readily applicable to impacting materials development.","APEX is an open-source, extendable, and cloud-native platform for material property calculations using a range of atomistic simulation methodologies that effectively manages diverse computational resources and is built upon user-friendly features including automatic results visualization, web-based platforms and NoSQL database client.","It is designed for expert and non-specialist users, lowers the barrier to entry for interdisciplinary research within the \"AI for Materials\" framework.","We describe the foundation and use of APEX, as well as provide an example of its application to properties of titanium for a wide-range of bonding descriptions."],"url":"http://arxiv.org/abs/2404.17330v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-26 11:20:48","title":"Inflation via Moduli Potentials in a Nested Warped Geometry","abstract":"We analyze the effective four-dimensional dynamics of the extra-dimensional moduli fields in curved braneworlds having nested warping, with particular emphasis on the doubly warped model which is interesting in the light of current collider constraints on the mass of the Kaluza-Klein graviton. The presence of a non-zero brane cosmological constant ($\\Omega$) naturally induces an effective moduli potential in the four-dimensional action, which shows distinct features in dS ($\\Omega>0$) and AdS ($\\Omega<0$) branches. For the observationally interesting case of dS 4-branes, a metastable minimum in the potential arises along the first modulus, with no minima along the higher moduli. The underlying nested geometry also leads to interesting separable forms of the non-canonical kinetic terms in the Einstein frame, where the brane curvature directly impacts the kinetic properties of only the first modulus. We subsequently explore the ability of curved multiply warped geometries to drive inflation with an in-built exit mechanism, by assuming predominant slow roll along each modular direction on a case-by-case basis. We find slow roll on top of the metastable plateau along the first modular direction to be the most viable scenario, with the higher-dimensional moduli parametrically tuning the height of the potential without significant impact on the inflationary observables. On the other hand, while slow roll along the higher moduli can successfully inflate the background and eventually lead to an exit, consistency with observations seemingly requires unphysical hierarchies among the extra-dimensional radii, thus disfavouring such scenarios.","sentences":["We analyze the effective four-dimensional dynamics of the extra-dimensional moduli fields in curved braneworlds having nested warping, with particular emphasis on the doubly warped model which is interesting in the light of current collider constraints on the mass of the Kaluza-Klein graviton.","The presence of a non-zero brane cosmological constant ($\\Omega$) naturally induces an effective moduli potential in the four-dimensional action, which shows distinct features in dS ($\\Omega>0$) and AdS ($\\Omega<0$) branches.","For the observationally interesting case of dS 4-branes, a metastable minimum in the potential arises along the first modulus, with no minima along the higher moduli.","The underlying nested geometry also leads to interesting separable forms of the non-canonical kinetic terms in the Einstein frame, where the brane curvature directly impacts the kinetic properties of only the first modulus.","We subsequently explore the ability of curved multiply warped geometries to drive inflation with an in-built exit mechanism, by assuming predominant slow roll along each modular direction on a case-by-case basis.","We find slow roll on top of the metastable plateau along the first modular direction to be the most viable scenario, with the higher-dimensional moduli parametrically tuning the height of the potential without significant impact on the inflationary observables.","On the other hand, while slow roll along the higher moduli can successfully inflate the background and eventually lead to an exit, consistency with observations seemingly requires unphysical hierarchies among the extra-dimensional radii, thus disfavouring such scenarios."],"url":"http://arxiv.org/abs/2404.17326v1","category":"hep-ph"}
{"created":"2024-04-26 11:07:09","title":"A Deep Dive into Effects of Structural Bias on CMA-ES Performance along Affine Trajectories","abstract":"To guide the design of better iterative optimisation heuristics, it is imperative to understand how inherent structural biases within algorithm components affect the performance on a wide variety of search landscapes. This study explores the impact of structural bias in the modular Covariance Matrix Adaptation Evolution Strategy (modCMA), focusing on the roles of various modulars within the algorithm. Through an extensive investigation involving 435,456 configurations of modCMA, we identified key modules that significantly influence structural bias of various classes. Our analysis utilized the Deep-BIAS toolbox for structural bias detection and classification, complemented by SHAP analysis for quantifying module contributions. The performance of these configurations was tested on a sequence of affine-recombined functions, maintaining fixed optimum locations while gradually varying the landscape features. Our results demonstrate an interplay between module-induced structural bias and algorithm performance across different landscape characteristics.","sentences":["To guide the design of better iterative optimisation heuristics, it is imperative to understand how inherent structural biases within algorithm components affect the performance on a wide variety of search landscapes.","This study explores the impact of structural bias in the modular Covariance Matrix Adaptation Evolution Strategy (modCMA), focusing on the roles of various modulars within the algorithm.","Through an extensive investigation involving 435,456 configurations of modCMA, we identified key modules that significantly influence structural bias of various classes.","Our analysis utilized the Deep-BIAS toolbox for structural bias detection and classification, complemented by SHAP analysis for quantifying module contributions.","The performance of these configurations was tested on a sequence of affine-recombined functions, maintaining fixed optimum locations while gradually varying the landscape features.","Our results demonstrate an interplay between module-induced structural bias and algorithm performance across different landscape characteristics."],"url":"http://arxiv.org/abs/2404.17323v1","category":"cs.NE"}
{"created":"2024-04-26 11:03:59","title":"Filtered Boolean powers of finite simple non-abelian Mal'cev algebras","abstract":"Let $\\mathbf{A}$ be a finite simple non-abelian Mal'cev algebra (e.g. a group, loop, ring). We investigate the Boolean power $\\mathbf{D}$ of $\\mathbf{A}$ by the countable atomless Boolean algebra $\\mathbf{B}$ filtered at some idempotents $e_1,\\dots,e_n$ of $\\mathbf{A}$. When $e_1,\\dots,e_n$ are all idempotents of $\\mathbf{A}$ we establish two concrete representations of $\\mathbf{D}$: as the Fra\\\"iss\\'e limit of the class of finite direct powers of $\\mathbf{A}$, and as congruence classes of the countable free algebra in the variety generated by $\\mathbf{A}$. Further, for arbitrary $e_1,\\dots,e_n$, we show that $\\mathbf{D}$ is $\\omega$-categorical and that its automorphism group has the small index property, strong uncountable cofinality and the Bergman property. As necessary background we establish some general properties of congruences and automorphisms of filtered Boolean powers of $\\mathbf{A}$ by any Boolean algebra $\\mathbf{B}$, including a semidirect decomposition for their automorphism groups.","sentences":["Let $\\mathbf{A}$ be a finite simple non-abelian Mal'cev algebra (e.g. a group, loop, ring).","We investigate the Boolean power $\\mathbf{D}$ of $\\mathbf{A}$ by the countable atomless Boolean algebra $\\mathbf{B}$ filtered at some idempotents $e_1,\\dots,e_n$ of $\\mathbf{A}$. When $e_1,\\dots,e_n$ are all idempotents of $\\mathbf{A}$ we establish two concrete representations of $\\mathbf{D}$: as the Fra\\\"iss\\'e limit of the class of finite direct powers of $\\mathbf{A}$, and as congruence classes of the countable free algebra in the variety generated by $\\mathbf{A}$. Further, for arbitrary $e_1,\\dots,e_n$, we show that $\\mathbf{D}$ is $\\omega$-categorical and that its automorphism group has the small index property, strong uncountable cofinality and the Bergman property.","As necessary background we establish some general properties of congruences and automorphisms of filtered Boolean powers of $\\mathbf{A}$ by any Boolean algebra $\\mathbf{B}$, including a semidirect decomposition for their automorphism groups."],"url":"http://arxiv.org/abs/2404.17322v1","category":"math.LO"}
{"created":"2024-04-26 11:02:04","title":"Fractional Order Sunflower Equation: Stability, Bifurcation and Chaos","abstract":"The sunflower equation describes the motion of the tip of a plant due to the auxin transportation under the influence of gravity. This work proposes the fractional-order generalization to this delay differential equation. The equation contains two fractional orders and infinitely many equilibrium points. The coefficients in the linearized equation near the equilibrium points are delay-dependent. We provide a detailed stability analysis of each equilibrium point. We observed the following bifurcation phenomena: stable for all the delay values, a single stable region in the delayed interval, and a stability switch. We also observed a multi-scroll chaotic attractor for some values of the parameters.","sentences":["The sunflower equation describes the motion of the tip of a plant due to the auxin transportation under the influence of gravity.","This work proposes the fractional-order generalization to this delay differential equation.","The equation contains two fractional orders and infinitely many equilibrium points.","The coefficients in the linearized equation near the equilibrium points are delay-dependent.","We provide a detailed stability analysis of each equilibrium point.","We observed the following bifurcation phenomena: stable for all the delay values, a single stable region in the delayed interval, and a stability switch.","We also observed a multi-scroll chaotic attractor for some values of the parameters."],"url":"http://arxiv.org/abs/2404.17321v1","category":"math.DS"}
{"created":"2024-04-26 10:58:35","title":"Cattaneo--type subdiffusion equation","abstract":"The ordinary subdiffusion equation, with fractional time derivatives of at most first order, describes a process in which the propagation velocity of diffusing molecules is unlimited. To avoid this non-physical property the Cattaneo diffusion equation has been proposed. Compared to the ordinary subdiffusion equation, the Cattaneo equation contains an additional time derivative of order greater than one and less than or equal to two. The fractional order of the additional derivative depends on the subdiffusion exponent. We study a Cattaneo-type subdiffusion equation (CTSE) that differs from the ordinary subdiffusion equation by an additional integro--differential operator (AO) which may be independent of subdiffusion parameters. The AO describes processes affecting ordinary subdiffusion. The equation is derived combining the modified diffusive flux equation with the continuity equation. It can also be obtained within the continuous time random walk model with the waiting time distribution for the molecule to jump controlled by the kernel of AO. As examples, the CTSEs with the additional Caputo fractional time derivative of the order independent of the subdiffusion exponent and with the AO generated by a slowly varying function are considered. We discuss whether the ordinary subdiffusion equation and CTSE provide qualitative differences in the description of subdiffusion.","sentences":["The ordinary subdiffusion equation, with fractional time derivatives of at most first order, describes a process in which the propagation velocity of diffusing molecules is unlimited.","To avoid this non-physical property the Cattaneo diffusion equation has been proposed.","Compared to the ordinary subdiffusion equation, the Cattaneo equation contains an additional time derivative of order greater than one and less than or equal to two.","The fractional order of the additional derivative depends on the subdiffusion exponent.","We study a Cattaneo-type subdiffusion equation (CTSE) that differs from the ordinary subdiffusion equation by an additional integro--differential operator (AO) which may be independent of subdiffusion parameters.","The AO describes processes affecting ordinary subdiffusion.","The equation is derived combining the modified diffusive flux equation with the continuity equation.","It can also be obtained within the continuous time random walk model with the waiting time distribution for the molecule to jump controlled by the kernel of AO.","As examples, the CTSEs with the additional Caputo fractional time derivative of the order independent of the subdiffusion exponent and with the AO generated by a slowly varying function are considered.","We discuss whether the ordinary subdiffusion equation and CTSE provide qualitative differences in the description of subdiffusion."],"url":"http://arxiv.org/abs/2404.17319v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-26 10:55:37","title":"Colosseum: The Open RAN Digital Twin","abstract":"Recent years have witnessed the Open Radio Access Network (RAN) paradigm transforming the fundamental ways cellular systems are deployed, managed, and optimized. This shift is led by concepts such as openness, softwarization, programmability, interoperability, and intelligence of the network, all of which had never been applied to the cellular ecosystem before. The realization of the Open RAN vision into practical architectures, intelligent data-driven control loops, and efficient software implementations, however, is a multifaceted challenge, which requires (i) datasets to train Artificial Intelligence (AI) and Machine Learning (ML) models; (ii) facilities to test models without disrupting production networks; (iii) continuous and automated validation of the RAN software; and (iv) significant testing and integration efforts. This paper poses itself as a tutorial on how Colosseum - the world's largest wireless network emulator with hardware in the loop - can provide the research infrastructure and tools to fill the gap between the Open RAN vision, and the deployment and commercialization of open and programmable networks. We describe how Colosseum implements an Open RAN digital twin through a high-fidelity Radio Frequency (RF) channel emulator and end-to-end softwarized O-RAN and 5G-compliant protocol stacks, thus allowing users to reproduce and experiment upon topologies representative of real-world cellular deployments. Then, we detail the twinning infrastructure of Colosseum, as well as the automation pipelines for RF and protocol stack twinning. Finally, we showcase a broad range of Open RAN use cases implemented on Colosseum, including the real-time connection between the digital twin and real-world networks, and the development, prototyping, and testing of AI/ML solutions for Open RAN.","sentences":["Recent years have witnessed the Open Radio Access Network (RAN) paradigm transforming the fundamental ways cellular systems are deployed, managed, and optimized.","This shift is led by concepts such as openness, softwarization, programmability, interoperability, and intelligence of the network, all of which had never been applied to the cellular ecosystem before.","The realization of the Open RAN vision into practical architectures, intelligent data-driven control loops, and efficient software implementations, however, is a multifaceted challenge, which requires (i) datasets to train Artificial Intelligence (AI) and Machine Learning (ML) models; (ii) facilities to test models without disrupting production networks; (iii) continuous and automated validation of the RAN software; and (iv) significant testing and integration efforts.","This paper poses itself as a tutorial on how Colosseum - the world's largest wireless network emulator with hardware in the loop - can provide the research infrastructure and tools to fill the gap between the Open RAN vision, and the deployment and commercialization of open and programmable networks.","We describe how Colosseum implements an Open RAN digital twin through a high-fidelity Radio Frequency (RF) channel emulator and end-to-end softwarized O-RAN and 5G-compliant protocol stacks, thus allowing users to reproduce and experiment upon topologies representative of real-world cellular deployments.","Then, we detail the twinning infrastructure of Colosseum, as well as the automation pipelines for RF and protocol stack twinning.","Finally, we showcase a broad range of Open RAN use cases implemented on Colosseum, including the real-time connection between the digital twin and real-world networks, and the development, prototyping, and testing of AI/ML solutions for Open RAN."],"url":"http://arxiv.org/abs/2404.17317v1","category":"cs.NI"}
{"created":"2024-04-26 10:55:06","title":"Certified MaxSAT Preprocessing","abstract":"Building on the progress in Boolean satisfiability (SAT) solving over the last decades, maximum satisfiability (MaxSAT) has become a viable approach for solving NP-hard optimization problems, but ensuring correctness of MaxSAT solvers has remained an important concern. For SAT, this is largely a solved problem thanks to the use of proof logging, meaning that solvers emit machine-verifiable proofs of (un)satisfiability to certify correctness. However, for MaxSAT, proof logging solvers have started being developed only very recently. Moreover, these nascent efforts have only targeted the core solving process, ignoring the preprocessing phase where input problem instances can be substantially reformulated before being passed on to the solver proper. In this work, we demonstrate how pseudo-Boolean proof logging can be used to certify the correctness of a wide range of modern MaxSAT preprocessing techniques. By combining and extending the VeriPB and CakePB tools, we provide formally verified, end-to-end proof checking that the input and preprocessed output MaxSAT problem instances have the same optimal value. An extensive evaluation on applied MaxSAT benchmarks shows that our approach is feasible in practice.","sentences":["Building on the progress in Boolean satisfiability (SAT) solving over the last decades, maximum satisfiability (MaxSAT) has become a viable approach for solving NP-hard optimization problems, but ensuring correctness of MaxSAT solvers has remained an important concern.","For SAT, this is largely a solved problem thanks to the use of proof logging, meaning that solvers emit machine-verifiable proofs of (un)satisfiability to certify correctness.","However, for MaxSAT, proof logging solvers have started being developed only very recently.","Moreover, these nascent efforts have only targeted the core solving process, ignoring the preprocessing phase where input problem instances can be substantially reformulated before being passed on to the solver proper.","In this work, we demonstrate how pseudo-Boolean proof logging can be used to certify the correctness of a wide range of modern MaxSAT preprocessing techniques.","By combining and extending the VeriPB and CakePB tools, we provide formally verified, end-to-end proof checking that the input and preprocessed output MaxSAT problem instances have the same optimal value.","An extensive evaluation on applied MaxSAT benchmarks shows that our approach is feasible in practice."],"url":"http://arxiv.org/abs/2404.17316v1","category":"cs.AI"}
{"created":"2024-04-26 10:41:20","title":"Conjugacy geodesics and growth in dihedral Artin groups","abstract":"In this paper we describe conjugacy geodesic representatives in any dihedral Artin group $G(m)$, $m\\geq 3$, which we then use to calculate asymptotics for the conjugacy growth of $G(m)$, and show that the conjugacy growth series of $G(m)$ with respect to the `free product' generating set $\\{x, y\\}$ is transcendental. This, together with recent results on Artin groups and contracting elements, implies that all Artin groups of XXL-type have transcendental conjugacy growth series for some generating set.   We prove two additional properties of $G(m)$ that connect to conjugacy, namely that the permutation conjugator length function is constant, and that the falsification by fellow traveler property (FFTP) holds with respect to $\\{x, y\\}$. These imply that the language of all conjugacy geodesics in $G(m)$ with respect to $\\{x, y\\}$ is regular.","sentences":["In this paper we describe conjugacy geodesic representatives in any dihedral Artin group $G(m)$, $m\\geq 3$, which we then use to calculate asymptotics for the conjugacy growth of $G(m)$, and show that the conjugacy growth series of $G(m)$ with respect to the `free product' generating set $\\{x, y\\}$ is transcendental.","This, together with recent results on Artin groups and contracting elements, implies that all Artin groups of XXL-type have transcendental conjugacy growth series for some generating set.   ","We prove two additional properties of $G(m)$ that connect to conjugacy, namely that the permutation conjugator length function is constant, and that the falsification by fellow traveler property (FFTP) holds with respect to $\\{x, y\\}$. These imply that the language of all conjugacy geodesics in $G(m)$ with respect to $\\{x, y\\}$ is regular."],"url":"http://arxiv.org/abs/2404.17312v1","category":"math.GR"}
{"created":"2024-04-26 10:36:14","title":"$L$-space knots with positive surgeries that are not weakly symplectically fillable","abstract":"In this paper we discuss a general strategy to detect the absence of weakly symplectic fillings of $L$-spaces. We start from a generic $L$-space knot and consider (positive) Dehn surgeries on it. We compute, using arithmetic data depending only on the knot type and the surgery coefficient, the value of the relevant geometric invariants used to obstruct fillability. We also provide a new example of an infinite family of hyperbolic $L$-spaces that do not admit weakly symplectic fillings. These are manifolds that lie inside $\\{\\text{Tight}\\}$ but not inside $\\{\\text{Weakly Fillable}\\}$.","sentences":["In this paper we discuss a general strategy to detect the absence of weakly symplectic fillings of $L$-spaces.","We start from a generic $L$-space knot and consider (positive) Dehn surgeries on it.","We compute, using arithmetic data depending only on the knot type and the surgery coefficient, the value of the relevant geometric invariants used to obstruct fillability.","We also provide a new example of an infinite family of hyperbolic $L$-spaces that do not admit weakly symplectic fillings.","These are manifolds that lie inside $\\{\\text{Tight}\\}$ but not inside $\\{\\text{Weakly Fillable}\\}$."],"url":"http://arxiv.org/abs/2404.17308v1","category":"math.GT"}
{"created":"2024-04-26 10:18:17","title":"Part-Guided 3D RL for Sim2Real Articulated Object Manipulation","abstract":"Manipulating unseen articulated objects through visual feedback is a critical but challenging task for real robots. Existing learning-based solutions mainly focus on visual affordance learning or other pre-trained visual models to guide manipulation policies, which face challenges for novel instances in real-world scenarios. In this paper, we propose a novel part-guided 3D RL framework, which can learn to manipulate articulated objects without demonstrations. We combine the strengths of 2D segmentation and 3D RL to improve the efficiency of RL policy training. To improve the stability of the policy on real robots, we design a Frame-consistent Uncertainty-aware Sampling (FUS) strategy to get a condensed and hierarchical 3D representation. In addition, a single versatile RL policy can be trained on multiple articulated object manipulation tasks simultaneously in simulation and shows great generalizability to novel categories and instances. Experimental results demonstrate the effectiveness of our framework in both simulation and real-world settings. Our code is available at https://github.com/THU-VCLab/Part-Guided-3D-RL-for-Sim2Real-Articulated-Object-Manipulation.","sentences":["Manipulating unseen articulated objects through visual feedback is a critical but challenging task for real robots.","Existing learning-based solutions mainly focus on visual affordance learning or other pre-trained visual models to guide manipulation policies, which face challenges for novel instances in real-world scenarios.","In this paper, we propose a novel part-guided 3D RL framework, which can learn to manipulate articulated objects without demonstrations.","We combine the strengths of 2D segmentation and 3D RL to improve the efficiency of RL policy training.","To improve the stability of the policy on real robots, we design a Frame-consistent Uncertainty-aware Sampling (FUS) strategy to get a condensed and hierarchical 3D representation.","In addition, a single versatile RL policy can be trained on multiple articulated object manipulation tasks simultaneously in simulation and shows great generalizability to novel categories and instances.","Experimental results demonstrate the effectiveness of our framework in both simulation and real-world settings.","Our code is available at https://github.com/THU-VCLab/Part-Guided-3D-RL-for-Sim2Real-Articulated-Object-Manipulation."],"url":"http://arxiv.org/abs/2404.17302v1","category":"cs.RO"}
{"created":"2024-04-26 10:10:48","title":"Canonical vs. Grand Canonical Ensemble for Bosonic Gases under Harmonic Confinement","abstract":"We analyze the general relation between canonical and grand canonical ensembles in the thermodynamic limit. We begin our discussion by deriving, with an alternative approach, some standard results first obtained by Kac and coworkers in the late 1970s. Then, motivated by the Bose-Einstein condensation (BEC) of trapped gases with a fixed number of atoms, which is well described by the canonical ensemble and by the recent groundbreaking experimental realization of BEC with photons in a dye-filled optical micro-cavity under genuine grand canonical conditions, we apply our formalism to a system of non-interacting Bose particles confined in a two-dimensional harmonic trap. We discuss in detail the mathematical origin of the inequivalence of ensembles observed in the condensed phase, giving place to the so-called grand canonical catastrophe of density fluctuations. We also provide explicit analytical expressions for the internal energy and specific heat and compare them with available experimental data. For these quantities, we show the equivalence of ensembles in the thermodynamic limit.","sentences":["We analyze the general relation between canonical and grand canonical ensembles in the thermodynamic limit.","We begin our discussion by deriving, with an alternative approach, some standard results first obtained by Kac and coworkers in the late 1970s.","Then, motivated by the Bose-Einstein condensation (BEC) of trapped gases with a fixed number of atoms, which is well described by the canonical ensemble and by the recent groundbreaking experimental realization of BEC with photons in a dye-filled optical micro-cavity under genuine grand canonical conditions, we apply our formalism to a system of non-interacting Bose particles confined in a two-dimensional harmonic trap.","We discuss in detail the mathematical origin of the inequivalence of ensembles observed in the condensed phase, giving place to the so-called grand canonical catastrophe of density fluctuations.","We also provide explicit analytical expressions for the internal energy and specific heat and compare them with available experimental data.","For these quantities, we show the equivalence of ensembles in the thermodynamic limit."],"url":"http://arxiv.org/abs/2404.17300v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-26 10:06:58","title":"Automatic Target-Less Camera-LiDAR Calibration From Motion and Deep Point Correspondences","abstract":"Sensor setups of robotic platforms commonly include both camera and LiDAR as they provide complementary information. However, fusing these two modalities typically requires a highly accurate calibration between them. In this paper, we propose MDPCalib which is a novel method for camera-LiDAR calibration that requires neither human supervision nor any specific target objects. Instead, we utilize sensor motion estimates from visual and LiDAR odometry as well as deep learning-based 2D-pixel-to-3D-point correspondences that are obtained without in-domain retraining. We represent the camera-LiDAR calibration as a graph optimization problem and minimize the costs induced by constraints from sensor motion and point correspondences. In extensive experiments, we demonstrate that our approach yields highly accurate extrinsic calibration parameters and is robust to random initialization. Additionally, our approach generalizes to a wide range of sensor setups, which we demonstrate by employing it on various robotic platforms including a self-driving perception car, a quadruped robot, and a UAV. To make our calibration method publicly accessible, we release the code on our project website at http://calibration.cs.uni-freiburg.de.","sentences":["Sensor setups of robotic platforms commonly include both camera and LiDAR as they provide complementary information.","However, fusing these two modalities typically requires a highly accurate calibration between them.","In this paper, we propose MDPCalib which is a novel method for camera-LiDAR calibration that requires neither human supervision nor any specific target objects.","Instead, we utilize sensor motion estimates from visual and LiDAR odometry as well as deep learning-based 2D-pixel-to-3D-point correspondences that are obtained without in-domain retraining.","We represent the camera-LiDAR calibration as a graph optimization problem and minimize the costs induced by constraints from sensor motion and point correspondences.","In extensive experiments, we demonstrate that our approach yields highly accurate extrinsic calibration parameters and is robust to random initialization.","Additionally, our approach generalizes to a wide range of sensor setups, which we demonstrate by employing it on various robotic platforms including a self-driving perception car, a quadruped robot, and a UAV.","To make our calibration method publicly accessible, we release the code on our project website at http://calibration.cs.uni-freiburg.de."],"url":"http://arxiv.org/abs/2404.17298v1","category":"cs.RO"}
{"created":"2024-04-26 09:57:27","title":"Generalized quantifiers using team semantics","abstract":"Dependence logic provides an elegant approach for introducing dependencies between variables into the object language of first-order logic. In [1] generalized quantifiers were introduced in this context. However, a satisfactory account was only achieved for monotone increasing generalized quantifiers.   In this paper, we modify the fundamental semantical guideline of dependence logic to create a framework that adequately handles both monotone and non-monotone generalized quantifiers. We demonstrate that this new logic can interpret dependence logic and possesses the same expressive power as existential second-order logic (ESO) on the level of formulas. Additionally, we establish truth conditions for generalized quantifiers and prove that the extended logic remains conservative over first-order logic with generalized quantifiers and is able to express the branching of continuous generalized quantifiers.","sentences":["Dependence logic provides an elegant approach for introducing dependencies between variables into the object language of first-order logic.","In [1] generalized quantifiers were introduced in this context.","However, a satisfactory account was only achieved for monotone increasing generalized quantifiers.   ","In this paper, we modify the fundamental semantical guideline of dependence logic to create a framework that adequately handles both monotone and non-monotone generalized quantifiers.","We demonstrate that this new logic can interpret dependence logic and possesses the same expressive power as existential second-order logic (ESO) on the level of formulas.","Additionally, we establish truth conditions for generalized quantifiers and prove that the extended logic remains conservative over first-order logic with generalized quantifiers and is able to express the branching of continuous generalized quantifiers."],"url":"http://arxiv.org/abs/2404.17295v1","category":"math.LO"}
{"created":"2024-04-26 09:51:24","title":"Lazy Data Practices Harm Fairness Research","abstract":"Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications.   Our analyses identify three main areas of concern: (1) a \\textbf{lack of representation for certain protected attributes} in both data and evaluations; (2) the widespread \\textbf{exclusion of minorities} during data preprocessing; and (3) \\textbf{opaque data processing} threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.","sentences":["Data practices shape research and practice on fairness in machine learning (fair ML).","Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement.","In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings.","We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications.   ","Our analyses identify three main areas of concern: (1) a \\textbf{lack of representation for certain protected attributes} in both data and evaluations; (2) the widespread \\textbf{exclusion of minorities} during data preprocessing; and (3) \\textbf{opaque data processing} threatening the generalization of fairness research.","By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons.","Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges.","To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion.","This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets."],"url":"http://arxiv.org/abs/2404.17293v1","category":"cs.LG"}
{"created":"2024-04-26 09:48:40","title":"Interpreting the power spectral density of a fluctuating colloidal current","abstract":"The transport of molecules through biological and synthetic nanopores is governed by multiple stochastic processes that lead to noisy, fluctuating currents. Disentangling the characteristics of different noise-generating mechanisms is thus central to better understanding molecular transport at a fundamental level. Here, we study current noise experimentally at the single particle level by imaging colloidal particles driven through microfluidic channels by a difference in fluid pressure. In this scenario, currents fluctuate due to the random arrival times of particles into the channel and the distribution of particle speeds within the channel. We find that this results in a characteristic form of the power spectral density of the current, scaling as ~f^0, at low frequencies and ~1/f^2 at high frequencies. To rationalise these scalings, we extend a model for shot noise with a finite transit time, borrowed from electronic circuit theory, to include the experimental distribution of transit times and compare this model to the experimental spectra. We find excellent agreement between the data and model across a range of driving pressures within an advection-dominated regime, thereby establishing concrete links between the power spectra scalings and underlying mechanisms for this experimental system. This paves the way for establishing a more systematic understanding of the links between features of power spectra and underlying molecular mechanisms in driven systems.","sentences":["The transport of molecules through biological and synthetic nanopores is governed by multiple stochastic processes that lead to noisy, fluctuating currents.","Disentangling the characteristics of different noise-generating mechanisms is thus central to better understanding molecular transport at a fundamental level.","Here, we study current noise experimentally at the single particle level by imaging colloidal particles driven through microfluidic channels by a difference in fluid pressure.","In this scenario, currents fluctuate due to the random arrival times of particles into the channel and the distribution of particle speeds within the channel.","We find that this results in a characteristic form of the power spectral density of the current, scaling as ~f^0, at low frequencies and ~1/f^2 at high frequencies.","To rationalise these scalings, we extend a model for shot noise with a finite transit time, borrowed from electronic circuit theory, to include the experimental distribution of transit times and compare this model to the experimental spectra.","We find excellent agreement between the data and model across a range of driving pressures within an advection-dominated regime, thereby establishing concrete links between the power spectra scalings and underlying mechanisms for this experimental system.","This paves the way for establishing a more systematic understanding of the links between features of power spectra and underlying molecular mechanisms in driven systems."],"url":"http://arxiv.org/abs/2404.17291v1","category":"cond-mat.soft"}
{"created":"2024-04-26 09:43:40","title":"ExcluIR: Exclusionary Neural Information Retrieval","abstract":"Exclusion is an important and universal linguistic skill that humans use to express what they do not want. However, in information retrieval community, there is little research on exclusionary retrieval, where users express what they do not want in their queries. In this work, we investigate the scenario of exclusionary retrieval in document retrieval for the first time. We present ExcluIR, a set of resources for exclusionary retrieval, consisting of an evaluation benchmark and a training set for helping retrieval models to comprehend exclusionary queries. The evaluation benchmark includes 3,452 high-quality exclusionary queries, each of which has been manually annotated. The training set contains 70,293 exclusionary queries, each paired with a positive document and a negative document. We conduct detailed experiments and analyses, obtaining three main observations: (1) Existing retrieval models with different architectures struggle to effectively comprehend exclusionary queries; (2) Although integrating our training data can improve the performance of retrieval models on exclusionary retrieval, there still exists a gap compared to human performance; (3) Generative retrieval models have a natural advantage in handling exclusionary queries. To facilitate future research on exclusionary retrieval, we share the benchmark and evaluation scripts on \\url{https://github.com/zwh-sdu/ExcluIR}.","sentences":["Exclusion is an important and universal linguistic skill that humans use to express what they do not want.","However, in information retrieval community, there is little research on exclusionary retrieval, where users express what they do not want in their queries.","In this work, we investigate the scenario of exclusionary retrieval in document retrieval for the first time.","We present ExcluIR, a set of resources for exclusionary retrieval, consisting of an evaluation benchmark and a training set for helping retrieval models to comprehend exclusionary queries.","The evaluation benchmark includes 3,452 high-quality exclusionary queries, each of which has been manually annotated.","The training set contains 70,293 exclusionary queries, each paired with a positive document and a negative document.","We conduct detailed experiments and analyses, obtaining three main observations: (1) Existing retrieval models with different architectures struggle to effectively comprehend exclusionary queries; (2) Although integrating our training data can improve the performance of retrieval models on exclusionary retrieval, there still exists a gap compared to human performance; (3) Generative retrieval models have a natural advantage in handling exclusionary queries.","To facilitate future research on exclusionary retrieval, we share the benchmark and evaluation scripts on \\url{https://github.com/zwh-sdu/ExcluIR}."],"url":"http://arxiv.org/abs/2404.17288v1","category":"cs.IR"}
{"created":"2024-04-26 09:42:46","title":"When to Trust LLMs: Aligning Confidence with Response Quality","abstract":"Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text. This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains. Existing methods, which rely on verbalizing confidence to tell the reliability by inducing top-k responses and sampling-aggregating multiple responses, often fail, due to the lack of objective guidance of confidence. To address this, we propose CONfidence-Quality-ORDerpreserving alignment approach (CONQORD), leveraging reinforcement learning with a tailored dual-component reward function. This function encompasses quality reward and orderpreserving alignment reward functions. Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality. Experiments demonstrate that our CONQORD significantly improves the alignment performance between confidence levels and response accuracy, without causing the model to become over-cautious. Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge. Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness.","sentences":["Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text.","This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains.","Existing methods, which rely on verbalizing confidence to tell the reliability by inducing top-k responses and sampling-aggregating multiple responses, often fail, due to the lack of objective guidance of confidence.","To address this, we propose CONfidence-Quality-ORDerpreserving alignment approach (CONQORD), leveraging reinforcement learning with a tailored dual-component reward function.","This function encompasses quality reward and orderpreserving alignment reward functions.","Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality.","Experiments demonstrate that our CONQORD significantly improves the alignment performance between confidence levels and response accuracy, without causing the model to become over-cautious.","Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge.","Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness."],"url":"http://arxiv.org/abs/2404.17287v1","category":"cs.CL"}
{"created":"2024-04-26 09:37:33","title":"Topological polarization singularities induced by the non-Hermitian Dirac points","abstract":"A Dirac point in the Hermitian photonic system will split into a pair of exceptional points (EPs) or even spawn a ring of EPs if non-Hermiticity is involved. Here, we present a new type of non-Hermitian Dirac point which is situated in the complex plane of eigenfrequency. When there is differential loss, the Dirac point exhibits a dual behavior: it not only splits into a pair of EPs with opposite chirality in the band structure but also induces a pair of circularly polarized states (C points) with opposite handedness in the far-field radiation. Furthermore, breaking the corresponding mirror symmetries enables independent control of these Dirac-point induced C points, facilitating the merging of two C points and generation of unidirectional guided resonances. Our results demonstrate an explicit relation between the band singularities and polarization singularities, and provide a new mechanism to generate unidirectional emission, which can be useful in the band engineering and polarization manipulation.","sentences":["A Dirac point in the Hermitian photonic system will split into a pair of exceptional points (EPs) or even spawn a ring of EPs if non-Hermiticity is involved.","Here, we present a new type of non-Hermitian Dirac point which is situated in the complex plane of eigenfrequency.","When there is differential loss, the Dirac point exhibits a dual behavior: it not only splits into a pair of EPs with opposite chirality in the band structure but also induces a pair of circularly polarized states (C points) with opposite handedness in the far-field radiation.","Furthermore, breaking the corresponding mirror symmetries enables independent control of these Dirac-point induced C points, facilitating the merging of two C points and generation of unidirectional guided resonances.","Our results demonstrate an explicit relation between the band singularities and polarization singularities, and provide a new mechanism to generate unidirectional emission, which can be useful in the band engineering and polarization manipulation."],"url":"http://arxiv.org/abs/2404.17281v1","category":"physics.optics"}
{"created":"2024-04-26 09:32:32","title":"A Notion of Dimension based on Probability on Groups","abstract":"We introduce notions of dimension of an infinite group, or more generally, a metric space, defined using percolation. Roughly speaking, the percolation dimension $pdim(G)$ of a group $G$ is the fastest rate of decay of a symmetric probability measure $\\mu$ on $G$, such that Bernoulli percolation on $G$ with connection probabilities proportional to $\\mu$ behaves like a Poisson branching process with parameter 1 in a sense made precise below. We show that $pdim(G)$ has several natural properties: it is monotone decreasing with respect to subgroups and quotients, and coincides with the growth rate exponent for several classes of groups.","sentences":["We introduce notions of dimension of an infinite group, or more generally, a metric space, defined using percolation.","Roughly speaking, the percolation dimension $pdim(G)$ of a group $G$ is the fastest rate of decay of a symmetric probability measure $\\mu$ on $G$, such that Bernoulli percolation on $G$ with connection probabilities proportional to $\\mu$ behaves like a Poisson branching process with parameter 1 in a sense made precise below.","We show that $pdim(G)$ has several natural properties: it is monotone decreasing with respect to subgroups and quotients, and coincides with the growth rate exponent for several classes of groups."],"url":"http://arxiv.org/abs/2404.17278v1","category":"math.PR"}
{"created":"2024-04-26 09:31:11","title":"Spherical orbits around Kerr-Newman and Ghosh black holes","abstract":"We conduct a comprehensive study on spherical orbits around two types of black holes: Kerr-Newman black holes, which are charged, and Ghosh black holes, which are nonsingular. In this work, we consider both null and timelike cases of orbits. Utilizing the Mino formalism, all analytical solutions for the geodesics governing these orbits can be obtained. It turns out that all spherical photon orbits outside the black hole horizons are unstable. In the extremal cases of both models, we obtain the {\\it photon boomerangs}. The existence of charge in the Kerr-Newman allows the orbits to transition between retrograde and prograde motions, and its increase tends to force the orbits to be more equatorial. On the other hand, the Ghosh black hole, characterized by a regular core and a lack of horizons in certain conditions, presents the possibility of observable stable spherical orbits in the so-called {\\it no-horizon} condition. As the Ghosh parameter $k$ increases, trajectories tend to exhibit larger latitudinal oscillation amplitudes. We observe that as the Ghosh parameter $k$ increases the trajectories tend to have larger latitudinal oscillation amplitudes. Finally, we investigate the existence of {\\it innermost stable spherical orbits} (ISSOs). Both black holes demonstrate the appearance of two branches of ISSO radii as a function of the Carter constant C. However, there are notable differences in their behavior: in the case of the Kerr-Newman black hole, the branches merge at a critical value, beyond which no ISSO exists, while for the Ghosh black hole, the transcendental nature of the metric function causes the branches to become complex at some finite distance.","sentences":["We conduct a comprehensive study on spherical orbits around two types of black holes: Kerr-Newman black holes, which are charged, and Ghosh black holes, which are nonsingular.","In this work, we consider both null and timelike cases of orbits.","Utilizing the Mino formalism, all analytical solutions for the geodesics governing these orbits can be obtained.","It turns out that all spherical photon orbits outside the black hole horizons are unstable.","In the extremal cases of both models, we obtain the {\\it photon boomerangs}.","The existence of charge in the Kerr-Newman allows the orbits to transition between retrograde and prograde motions, and its increase tends to force the orbits to be more equatorial.","On the other hand, the Ghosh black hole, characterized by a regular core and a lack of horizons in certain conditions, presents the possibility of observable stable spherical orbits in the so-called {\\it no-horizon} condition.","As the Ghosh parameter $k$ increases, trajectories tend to exhibit larger latitudinal oscillation amplitudes.","We observe that as the Ghosh parameter $k$ increases the trajectories tend to have larger latitudinal oscillation amplitudes.","Finally, we investigate the existence of {\\it innermost stable spherical orbits} (ISSOs).","Both black holes demonstrate the appearance of two branches of ISSO radii as a function of the Carter constant C. However, there are notable differences in their behavior: in the case of the Kerr-Newman black hole, the branches merge at a critical value, beyond which no ISSO exists, while for the Ghosh black hole, the transcendental nature of the metric function causes the branches to become complex at some finite distance."],"url":"http://arxiv.org/abs/2404.17277v1","category":"gr-qc"}
{"created":"2024-04-26 09:30:55","title":"Efficient Deterministic Renewable Energy Forecasting Guided by Multiple-Location Weather Data","abstract":"Electricity generated from renewable energy sources has been established as an efficient remedy for both energy shortages and the environmental pollution stemming from conventional energy production methods. Solar and wind power are two of the most dominant renewable energy sources. The accurate forecasting of the energy generation of those sources facilitates their integration into electric grids, by minimizing the negative impact of uncertainty regarding their management and operation. This paper proposes a novel methodology for deterministic wind and solar energy generation forecasting for multiple generation sites, utilizing multi-location weather forecasts. The method employs a U-shaped Temporal Convolutional Auto-Encoder (UTCAE) architecture for temporal processing of weather-related and energy-related time-series across each site. The Multi-sized Kernels convolutional Spatio-Temporal Attention (MKST-Attention), inspired by the multi-head scaled-dot product attention mechanism, is also proposed aiming to efficiently transfer temporal patterns from weather data to energy data, without a priori knowledge of the locations of the power stations and the locations of provided weather data. The conducted experimental evaluation on a day-ahead solar and wind energy forecasting scenario on five datasets demonstrated that the proposed method achieves top results, outperforming all competitive time-series forecasting state-of-the-art methods.","sentences":["Electricity generated from renewable energy sources has been established as an efficient remedy for both energy shortages and the environmental pollution stemming from conventional energy production methods.","Solar and wind power are two of the most dominant renewable energy sources.","The accurate forecasting of the energy generation of those sources facilitates their integration into electric grids, by minimizing the negative impact of uncertainty regarding their management and operation.","This paper proposes a novel methodology for deterministic wind and solar energy generation forecasting for multiple generation sites, utilizing multi-location weather forecasts.","The method employs a U-shaped Temporal Convolutional Auto-Encoder (UTCAE) architecture for temporal processing of weather-related and energy-related time-series across each site.","The Multi-sized Kernels convolutional Spatio-Temporal Attention (MKST-Attention), inspired by the multi-head scaled-dot product attention mechanism, is also proposed aiming to efficiently transfer temporal patterns from weather data to energy data, without a priori knowledge of the locations of the power stations and the locations of provided weather data.","The conducted experimental evaluation on a day-ahead solar and wind energy forecasting scenario on five datasets demonstrated that the proposed method achieves top results, outperforming all competitive time-series forecasting state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.17276v1","category":"cs.LG"}
{"created":"2024-04-26 09:25:18","title":"3SHNet: Boosting Image-Sentence Retrieval via Visual Semantic-Spatial Self-Highlighting","abstract":"In this paper, we propose a novel visual Semantic-Spatial Self-Highlighting Network (termed 3SHNet) for high-precision, high-efficiency and high-generalization image-sentence retrieval. 3SHNet highlights the salient identification of prominent objects and their spatial locations within the visual modality, thus allowing the integration of visual semantics-spatial interactions and maintaining independence between two modalities. This integration effectively combines object regions with the corresponding semantic and position layouts derived from segmentation to enhance the visual representation. And the modality-independence guarantees efficiency and generalization. Additionally, 3SHNet utilizes the structured contextual visual scene information from segmentation to conduct the local (region-based) or global (grid-based) guidance and achieve accurate hybrid-level retrieval. Extensive experiments conducted on MS-COCO and Flickr30K benchmarks substantiate the superior performances, inference efficiency and generalization of the proposed 3SHNet when juxtaposed with contemporary state-of-the-art methodologies. Specifically, on the larger MS-COCO 5K test set, we achieve 16.3%, 24.8%, and 18.3% improvements in terms of rSum score, respectively, compared with the state-of-the-art methods using different image representations, while maintaining optimal retrieval efficiency. Moreover, our performance on cross-dataset generalization improves by 18.6%. Data and code are available at https://github.com/XuriGe1995/3SHNet.","sentences":["In this paper, we propose a novel visual Semantic-Spatial Self-Highlighting Network (termed 3SHNet) for high-precision, high-efficiency and high-generalization image-sentence retrieval.","3SHNet highlights the salient identification of prominent objects and their spatial locations within the visual modality, thus allowing the integration of visual semantics-spatial interactions and maintaining independence between two modalities.","This integration effectively combines object regions with the corresponding semantic and position layouts derived from segmentation to enhance the visual representation.","And the modality-independence guarantees efficiency and generalization.","Additionally, 3SHNet utilizes the structured contextual visual scene information from segmentation to conduct the local (region-based) or global (grid-based) guidance and achieve accurate hybrid-level retrieval.","Extensive experiments conducted on MS-COCO and Flickr30K benchmarks substantiate the superior performances, inference efficiency and generalization of the proposed 3SHNet when juxtaposed with contemporary state-of-the-art methodologies.","Specifically, on the larger MS-COCO 5K test set, we achieve 16.3%, 24.8%, and 18.3% improvements in terms of rSum score, respectively, compared with the state-of-the-art methods using different image representations, while maintaining optimal retrieval efficiency.","Moreover, our performance on cross-dataset generalization improves by 18.6%.","Data and code are available at https://github.com/XuriGe1995/3SHNet."],"url":"http://arxiv.org/abs/2404.17273v1","category":"cs.CV"}
{"created":"2024-04-26 09:17:45","title":"Empirical Studies of Propagation Characteristics and Modeling Based on XL-MIMO Channel Measurement: From Far-Field to Near-Field","abstract":"In the sixth-generation (6G), the extremely large-scale multiple-input-multiple-output (XL-MIMO) is considered a promising enabling technology. With the further expansion of array element number and frequency bands, near-field effects will be more likely to occur in 6G communication systems. The near-field radio communications (NFRC) will become crucial in 6G communication systems. It is known that the channel research is very important for the development and performance evaluation of the communication systems. In this paper, we will systematically investigate the channel measurements and modeling for the emerging NFRC. First, the principle design of massive MIMO channel measurement platform are solved. Second, an indoor XL-MIMO channel measurement campaign with 1600 array elements is conducted, and the channel characteristics are extracted and validated in the near-field region. Then, the outdoor XL-MIMO channel measurement campaign with 320 array elements is conducted, and the channel characteristics are extracted and modeled from near-field to far-field (NF-FF) region. The spatial non-stationary characteristics of angular spread at the transmitting end are more important in modeling. We hope that this work will give some reference to the near-field and far-field research for 6G.","sentences":["In the sixth-generation (6G), the extremely large-scale multiple-input-multiple-output (XL-MIMO) is considered a promising enabling technology.","With the further expansion of array element number and frequency bands, near-field effects will be more likely to occur in 6G communication systems.","The near-field radio communications (NFRC) will become crucial in 6G communication systems.","It is known that the channel research is very important for the development and performance evaluation of the communication systems.","In this paper, we will systematically investigate the channel measurements and modeling for the emerging NFRC.","First, the principle design of massive MIMO channel measurement platform are solved.","Second, an indoor XL-MIMO channel measurement campaign with 1600 array elements is conducted, and the channel characteristics are extracted and validated in the near-field region.","Then, the outdoor XL-MIMO channel measurement campaign with 320 array elements is conducted, and the channel characteristics are extracted and modeled from near-field to far-field (NF-FF) region.","The spatial non-stationary characteristics of angular spread at the transmitting end are more important in modeling.","We hope that this work will give some reference to the near-field and far-field research for 6G."],"url":"http://arxiv.org/abs/2404.17270v1","category":"cs.IT"}
{"created":"2024-04-26 09:14:45","title":"Avoiding singularities in Lorentzian-Euclidean black holes: the role of atemporality","abstract":"We investigate a Schwarzschild metric exhibiting a signature change across the event horizon, which gives rise to what we term a Lorentzian-Euclidean black hole. The resulting geometry is regularized by employing the Hadamard partie finie technique, which allows us to prove that the metric represents a solution of vacuum Einstein equations. In this framework, we introduce the concept of atemporality as the dynamical mechanism responsible for the transition from a regime with a real-valued time variable to a new one featuring an imaginary time. We show that this mechanism prevents the occurrence of the singularity and, by means of the regularized Kretschmann invariant, we discuss in which terms atemporality can be considered as the characteristic feature of this black hole.","sentences":["We investigate a Schwarzschild metric exhibiting a signature change across the event horizon, which gives rise to what we term a Lorentzian-Euclidean black hole.","The resulting geometry is regularized by employing the Hadamard partie finie technique, which allows us to prove that the metric represents a solution of vacuum Einstein equations.","In this framework, we introduce the concept of atemporality as the dynamical mechanism responsible for the transition from a regime with a real-valued time variable to a new one featuring an imaginary time.","We show that this mechanism prevents the occurrence of the singularity and, by means of the regularized Kretschmann invariant, we discuss in which terms atemporality can be considered as the characteristic feature of this black hole."],"url":"http://arxiv.org/abs/2404.17267v1","category":"gr-qc"}
{"created":"2024-04-26 09:13:58","title":"Typical behaviour of genuine multimode entanglement of pure Gaussian states","abstract":"Trends of genuine entanglement in Haar uniformly generated multimode pure Gaussian states with fixed average energy per mode are explored. A distance-based metric known as the generalized geometric measure (GGM) is used to quantify genuine entanglement. The GGM of a state is defined as its minimum distance from the set of all non-genuinely entangled states. To begin with, we derive an expression for the Haar averaged value of any function defined on the set of energy-constrained states. Subsequently, we investigate states with a large number of modes and provide a closed-form expression for the Haar averaged GGM in terms of the average energy per mode. Furthermore, we demonstrate that typical states closely approximate their Haar averaged GGM value, with deviation probabilities bounded by an exponentially suppressed limit. We then analyze the GGM content of typical states with a finite number of modes and present the distribution of GGM. Our findings indicate that as the number of modes increases, the distribution shifts towards higher entanglement values and becomes more concentrated. We quantify these features by computing the Haar averaged GGM and the standard deviation of the GGM distribution, revealing that the former increases while the latter decreases with the number of modes.","sentences":["Trends of genuine entanglement in Haar uniformly generated multimode pure Gaussian states with fixed average energy per mode are explored.","A distance-based metric known as the generalized geometric measure (GGM) is used to quantify genuine entanglement.","The GGM of a state is defined as its minimum distance from the set of all non-genuinely entangled states.","To begin with, we derive an expression for the Haar averaged value of any function defined on the set of energy-constrained states.","Subsequently, we investigate states with a large number of modes and provide a closed-form expression for the Haar averaged GGM in terms of the average energy per mode.","Furthermore, we demonstrate that typical states closely approximate their Haar averaged GGM value, with deviation probabilities bounded by an exponentially suppressed limit.","We then analyze the GGM content of typical states with a finite number of modes and present the distribution of GGM.","Our findings indicate that as the number of modes increases, the distribution shifts towards higher entanglement values and becomes more concentrated.","We quantify these features by computing the Haar averaged GGM and the standard deviation of the GGM distribution, revealing that the former increases while the latter decreases with the number of modes."],"url":"http://arxiv.org/abs/2404.17265v1","category":"quant-ph"}
{"created":"2024-04-26 09:03:14","title":"Parametric and inverse analysis of flow inside an obstructed channel under the influence of magnetic field using physics informed neural networks","abstract":"In this study, fluid flow inside of an obstructed channel under the influence of magnetic field has been analyzed using physics informed neural networks(PINNs). Governing equations have been utilized in low-order form and the solution has been obtained in dimensionless form. Geometric and physics-related dimensionless parameters have been used as input variables of the neural network in the learning process. The radius and longitudinal position of the obstruction have also been involved in the learning process and the problem has been solved parametrically. In the successive sections of the study, inverse problem has been a matter of interest, particularly in form of obtaining the Hartmann number using the proposed method. The results have indicated that the employed method determined the Hartmann number with great accuracy and entailed proper results. In this study a thorough exploration of effects of physical and geometric parameters on the magnetically influenced flow through a duct has been carried out. The accuracy of the results obtained from the solving method proposed have been compared to results generated by common methods used in computational fluid mechanics. The parametric solution of the problem can serve as a powerful tool in optimization problems. The method was also applicable to cases where the parameters were outside of defined range, indicating it's generalization capabilities.","sentences":["In this study, fluid flow inside of an obstructed channel under the influence of magnetic field has been analyzed using physics informed neural networks(PINNs).","Governing equations have been utilized in low-order form and the solution has been obtained in dimensionless form.","Geometric and physics-related dimensionless parameters have been used as input variables of the neural network in the learning process.","The radius and longitudinal position of the obstruction have also been involved in the learning process and the problem has been solved parametrically.","In the successive sections of the study, inverse problem has been a matter of interest, particularly in form of obtaining the Hartmann number using the proposed method.","The results have indicated that the employed method determined the Hartmann number with great accuracy and entailed proper results.","In this study a thorough exploration of effects of physical and geometric parameters on the magnetically influenced flow through a duct has been carried out.","The accuracy of the results obtained from the solving method proposed have been compared to results generated by common methods used in computational fluid mechanics.","The parametric solution of the problem can serve as a powerful tool in optimization problems.","The method was also applicable to cases where the parameters were outside of defined range, indicating it's generalization capabilities."],"url":"http://arxiv.org/abs/2404.17261v1","category":"physics.flu-dyn"}
{"created":"2024-04-26 08:58:53","title":"Terrestrial planet formation from a ring: long-term simulations accounting for the giant planet instability","abstract":"The process leading to the formation of the terrestrial planet remains elusive. In a previous publication, we have shown that, if the first generation of planetesimals forms in a ring at about 1 AU and the gas disk's density peaks at the ring location, planetary embryos of a few martian masses can grow and remain in the ring. In this work, we extend our simulations beyond the gas-disk stage, covering 200 Myr and accounting for the phase of giant planet instability, assumed to happen at different times. About half of the simulations form a pair of Venus and Earth analogues and, independently, about 10% form a Mars analogue. We find that the timing of the giant planet instability affects statistically the terrestrial system's excitation state and the timing of the last giant impacts. Hence a late instability (about 60 to 100 Myr after the Solar system's birth) is more consistent with a late Moon-formation time, as suggested by radioactive chronometers. However, the late veneer mass (LVM: mass accreted after the last giant impact) of Earth-sized planets suffering a giant impact after 80 My is usually an order of magnitude lower than the value inferred from geochemistry. In addition, the final angular momentum deficit (AMD) of the terrestrial planets tends to be too high. We tested the effect on the final AMD of the generation of debris during collisions and found that it is too small to change these conclusions. We argue that the best-case scenario is that the Moon-forming event occurred between 50 and 80 My, possibly just following the giant planet instability.","sentences":["The process leading to the formation of the terrestrial planet remains elusive.","In a previous publication, we have shown that, if the first generation of planetesimals forms in a ring at about 1 AU and the gas disk's density peaks at the ring location, planetary embryos of a few martian masses can grow and remain in the ring.","In this work, we extend our simulations beyond the gas-disk stage, covering 200 Myr and accounting for the phase of giant planet instability, assumed to happen at different times.","About half of the simulations form a pair of Venus and Earth analogues and, independently, about 10% form a Mars analogue.","We find that the timing of the giant planet instability affects statistically the terrestrial system's excitation state and the timing of the last giant impacts.","Hence a late instability (about 60 to 100 Myr after the Solar system's birth) is more consistent with a late Moon-formation time, as suggested by radioactive chronometers.","However, the late veneer mass (LVM: mass accreted after the last giant impact) of Earth-sized planets suffering a giant impact after 80 My is usually an order of magnitude lower than the value inferred from geochemistry.","In addition, the final angular momentum deficit (AMD) of the terrestrial planets tends to be too high.","We tested the effect on the final AMD of the generation of debris during collisions and found that it is too small to change these conclusions.","We argue that the best-case scenario is that the Moon-forming event occurred between 50 and 80 My, possibly just following the giant planet instability."],"url":"http://arxiv.org/abs/2404.17259v1","category":"astro-ph.EP"}
{"created":"2024-04-26 08:55:49","title":"Thermodynamics and Dynamic Stability: Extended Theories of Heat Conduction","abstract":"The fundamental dynamic stability of heat conduction theories beyond Fourier is analyzed in the framework of nonequilibrium thermodynamics. It is shown, that the thermodynamic framework, concave entropy and nonnegative entropy production, can ensure the stability of homogeneous thermodynamic equilibrium. Various special heat conduction theories of Extended Thermodynamics are compared and analysed in the general framework.","sentences":["The fundamental dynamic stability of heat conduction theories beyond Fourier is analyzed in the framework of nonequilibrium thermodynamics.","It is shown, that the thermodynamic framework, concave entropy and nonnegative entropy production, can ensure the stability of homogeneous thermodynamic equilibrium.","Various special heat conduction theories of Extended Thermodynamics are compared and analysed in the general framework."],"url":"http://arxiv.org/abs/2404.17257v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-26 08:53:27","title":"Degree bounds for rational generators of invariant fields of finite abelian groups","abstract":"We study degree bounds on rational but not necessarily polynomial generators for the field $\\mathbf{k}(V)^G$ of rational invariants of a linear action of a finite abelian group. We show that lattice-theoretic methods used recently by the author and collaborators to study polynomial generators for the same field largely carry over, after minor modifications to the arguments. It then develops that the specific degree bounds found in that setting also carry over.","sentences":["We study degree bounds on rational but not necessarily polynomial generators for the field $\\mathbf{k}(V)^G$ of rational invariants of a linear action of a finite abelian group.","We show that lattice-theoretic methods used recently by the author and collaborators to study polynomial generators for the same field largely carry over, after minor modifications to the arguments.","It then develops that the specific degree bounds found in that setting also carry over."],"url":"http://arxiv.org/abs/2404.17256v1","category":"math.AC"}
{"created":"2024-04-26 08:51:31","title":"SDFD: Building a Versatile Synthetic Face Image Dataset with Diverse Attributes","abstract":"AI systems rely on extensive training on large datasets to address various tasks. However, image-based systems, particularly those used for demographic attribute prediction, face significant challenges. Many current face image datasets primarily focus on demographic factors such as age, gender, and skin tone, overlooking other crucial facial attributes like hairstyle and accessories. This narrow focus limits the diversity of the data and consequently the robustness of AI systems trained on them. This work aims to address this limitation by proposing a methodology for generating synthetic face image datasets that capture a broader spectrum of facial diversity. Specifically, our approach integrates a systematic prompt formulation strategy, encompassing not only demographics and biometrics but also non-permanent traits like make-up, hairstyle, and accessories. These prompts guide a state-of-the-art text-to-image model in generating a comprehensive dataset of high-quality realistic images and can be used as an evaluation set in face analysis systems. Compared to existing datasets, our proposed dataset proves equally or more challenging in image classification tasks while being much smaller in size.","sentences":["AI systems rely on extensive training on large datasets to address various tasks.","However, image-based systems, particularly those used for demographic attribute prediction, face significant challenges.","Many current face image datasets primarily focus on demographic factors such as age, gender, and skin tone, overlooking other crucial facial attributes like hairstyle and accessories.","This narrow focus limits the diversity of the data and consequently the robustness of AI systems trained on them.","This work aims to address this limitation by proposing a methodology for generating synthetic face image datasets that capture a broader spectrum of facial diversity.","Specifically, our approach integrates a systematic prompt formulation strategy, encompassing not only demographics and biometrics but also non-permanent traits like make-up, hairstyle, and accessories.","These prompts guide a state-of-the-art text-to-image model in generating a comprehensive dataset of high-quality realistic images and can be used as an evaluation set in face analysis systems.","Compared to existing datasets, our proposed dataset proves equally or more challenging in image classification tasks while being much smaller in size."],"url":"http://arxiv.org/abs/2404.17255v1","category":"cs.CV"}
{"created":"2024-04-26 08:50:35","title":"Trinity Detector:text-assisted and attention mechanisms based spectral fusion for diffusion generation image detection","abstract":"Artificial Intelligence Generated Content (AIGC) techniques, represented by text-to-image generation, have led to a malicious use of deep forgeries, raising concerns about the trustworthiness of multimedia content. Adapting traditional forgery detection methods to diffusion models proves challenging. Thus, this paper proposes a forgery detection method explicitly designed for diffusion models called Trinity Detector. Trinity Detector incorporates coarse-grained text features through a CLIP encoder, coherently integrating them with fine-grained artifacts in the pixel domain for comprehensive multimodal detection. To heighten sensitivity to diffusion-generated image features, a Multi-spectral Channel Attention Fusion Unit (MCAF) is designed, extracting spectral inconsistencies through adaptive fusion of diverse frequency bands and further integrating spatial co-occurrence of the two modalities. Extensive experimentation validates that our Trinity Detector method outperforms several state-of-the-art methods, our performance is competitive across all datasets and up to 17.6\\% improvement in transferability in the diffusion datasets.","sentences":["Artificial Intelligence Generated Content (AIGC) techniques, represented by text-to-image generation, have led to a malicious use of deep forgeries, raising concerns about the trustworthiness of multimedia content.","Adapting traditional forgery detection methods to diffusion models proves challenging.","Thus, this paper proposes a forgery detection method explicitly designed for diffusion models called Trinity Detector.","Trinity Detector incorporates coarse-grained text features through a CLIP encoder, coherently integrating them with fine-grained artifacts in the pixel domain for comprehensive multimodal detection.","To heighten sensitivity to diffusion-generated image features, a Multi-spectral Channel Attention Fusion Unit (MCAF) is designed, extracting spectral inconsistencies through adaptive fusion of diverse frequency bands and further integrating spatial co-occurrence of the two modalities.","Extensive experimentation validates that our Trinity Detector method outperforms several state-of-the-art methods, our performance is competitive across all datasets and up to 17.6\\% improvement in transferability in the diffusion datasets."],"url":"http://arxiv.org/abs/2404.17254v1","category":"cs.CV"}
{"created":"2024-04-26 08:42:10","title":"Omega Theorems for Logarithmic Derivatives of Zeta and L-functions Near the 1-line","abstract":"We establish an omega theorem for logarithmic derivative of the Riemann zeta function near the 1-line by resonance method. We show that the inequality $\\left| \\zeta^{\\prime}\\left(\\sigma_A+it\\right)/\\zeta\\left(\\sigma_A+it\\right) \\right| \\geqslant \\left(\\left(e^A-1\\right)/A\\right)\\log_2 T + O\\left(\\log_2 T / \\log_3 T\\right)$ has a solution $t \\in [T^{\\beta}, T]$ for all sufficiently large $T,$ where $\\sigma_A = 1 - A / \\log_2 {T}.$Furthermore, we give a conditional lower bound for the measure of the set of $t$ for which the logarithmic derivative of the Riemann zeta function is large. Moreover, similar results can be generalized to Dirichlet $L$-functions.","sentences":["We establish an omega theorem for logarithmic derivative of the Riemann zeta function near the 1-line by resonance method.","We show that the inequality $\\left| \\zeta^{\\prime}\\left(\\sigma_A+it\\right)/\\zeta\\left(\\sigma_A+it\\right) \\right| \\geqslant \\left(\\left(e^A-1\\right)/A\\right)\\log_2 T + O\\left(\\log_2 T / \\log_3 T\\right)$ has a solution $t \\in [T^{\\beta}, T]$ for all sufficiently large $T,$ where $\\sigma_A = 1 - A / \\log_2 {T}.$Furthermore, we give a conditional lower bound for the measure of the set of $t$ for which the logarithmic derivative of the Riemann zeta function is large.","Moreover, similar results can be generalized to Dirichlet $L$-functions."],"url":"http://arxiv.org/abs/2404.17250v1","category":"math.NT"}
{"created":"2024-04-26 08:41:50","title":"Bayesian and Principal Component Analyses of Neutron Star Properties","abstract":"A Bayesian method is used in this extensive work to generate a large set of minimally constrained equations of state (EOSs) for matters in neutron stars (NS). These EOSs are analyzed for their correlations with key NS properties, such as the tidal deformability, radius, and maximum mass, within the mass range of $1.2-2M_\\odot$. The observed connections between the pressure of $\\beta$-equilibrated matter and the properties of neutron stars at different densities offer significant insights into the behavior of NS matter in a nearly model-independent manner. The study also examines the influence of various factors on the correlation of symmetry energy parameters, such as slope and curvature parameters at saturation density ($\\rho_0=0.16 ~\\text{fm}^{-3}$) with the tidal deformability and radius of neutron stars. This study investigates the robustness of the observed correlations by considering the distributions and interdependence of symmetry energy parameters. Furthermore, the utilization of Principal Component Analysis (PCA) is employed to unveil the complicated relationship between various nuclear matter parameters and properties of neutron stars. This analysis highlights the importance of employing multivariate analysis techniques in order to comprehend the variety in tidal deformability and radius observed across distinct masses of NS. This comprehensive study aims to establish a connection between the parameters of nuclear matter and the properties of neutron stars, providing significant insights into the behavior of NS matter across different circumstances.","sentences":["A Bayesian method is used in this extensive work to generate a large set of minimally constrained equations of state (EOSs) for matters in neutron stars (NS).","These EOSs are analyzed for their correlations with key NS properties, such as the tidal deformability, radius, and maximum mass, within the mass range of $1.2-2M_\\odot$. The observed connections between the pressure of $\\beta$-equilibrated matter and the properties of neutron stars at different densities offer significant insights into the behavior of NS matter in a nearly model-independent manner.","The study also examines the influence of various factors on the correlation of symmetry energy parameters, such as slope and curvature parameters at saturation density ($\\rho_0=0.16 ~\\text{fm}^{-3}$) with the tidal deformability and radius of neutron stars.","This study investigates the robustness of the observed correlations by considering the distributions and interdependence of symmetry energy parameters.","Furthermore, the utilization of Principal Component Analysis (PCA) is employed to unveil the complicated relationship between various nuclear matter parameters and properties of neutron stars.","This analysis highlights the importance of employing multivariate analysis techniques in order to comprehend the variety in tidal deformability and radius observed across distinct masses of NS.","This comprehensive study aims to establish a connection between the parameters of nuclear matter and the properties of neutron stars, providing significant insights into the behavior of NS matter across different circumstances."],"url":"http://arxiv.org/abs/2404.17248v1","category":"nucl-th"}
{"created":"2024-04-26 08:35:46","title":"Parameter Efficient Fine-tuning of Self-supervised ViTs without Catastrophic Forgetting","abstract":"Artificial neural networks often suffer from catastrophic forgetting, where learning new concepts leads to a complete loss of previously acquired knowledge. We observe that this issue is particularly magnified in vision transformers (ViTs), where post-pre-training and fine-tuning on new tasks can significantly degrade the model's original general abilities. For instance, a DINO ViT-Base/16 pre-trained on ImageNet-1k loses over 70% accuracy on ImageNet-1k after just 10 iterations of fine-tuning on CIFAR-100. Overcoming this stability-plasticity dilemma is crucial for enabling ViTs to continuously learn and adapt to new domains while preserving their initial knowledge. In this work, we study two new parameter-efficient fine-tuning strategies: (1)~Block Expansion, and (2) Low-rank adaptation (LoRA). Our experiments reveal that using either Block Expansion or LoRA on self-supervised pre-trained ViTs surpass fully fine-tuned ViTs in new domains while offering significantly greater parameter efficiency. Notably, we find that Block Expansion experiences only a minimal performance drop in the pre-training domain, thereby effectively mitigating catastrophic forgetting in pre-trained ViTs.","sentences":["Artificial neural networks often suffer from catastrophic forgetting, where learning new concepts leads to a complete loss of previously acquired knowledge.","We observe that this issue is particularly magnified in vision transformers (ViTs), where post-pre-training and fine-tuning on new tasks can significantly degrade the model's original general abilities.","For instance, a DINO ViT-Base/16 pre-trained on ImageNet-1k loses over 70% accuracy on ImageNet-1k after just 10 iterations of fine-tuning on CIFAR-100.","Overcoming this stability-plasticity dilemma is crucial for enabling ViTs to continuously learn and adapt to new domains while preserving their initial knowledge.","In this work, we study two new parameter-efficient fine-tuning strategies: (1)~Block Expansion, and (2) Low-rank adaptation (LoRA).","Our experiments reveal that using either Block Expansion or LoRA on self-supervised pre-trained ViTs surpass fully fine-tuned ViTs in new domains while offering significantly greater parameter efficiency.","Notably, we find that Block Expansion experiences only a minimal performance drop in the pre-training domain, thereby effectively mitigating catastrophic forgetting in pre-trained ViTs."],"url":"http://arxiv.org/abs/2404.17245v1","category":"cs.CV"}
{"created":"2024-04-26 08:35:02","title":"Automated Configuration Synthesis for Machine Learning Models: A git-Based Requirement and Architecture Management System","abstract":"This work introduces a tool for generating runtime configurations automatically from textual requirements stored as artifacts in git repositories (a.k.a. T-Reqs) alongside the software code. The tool leverages T-Reqs-modelled architectural description to identify relevant configuration properties for the deployment of artificial intelligence (AI)-enabled software systems. This enables traceable configuration generation, taking into account both functional and non-functional requirements. The resulting configuration specification also includes the dynamic properties that need to be adjusted and the rationale behind their adjustment. We show that this intermediary format can be directly used by the system or adapted for specific targets, for example in order to achieve runtime optimisations in term of ML model size before deployment.","sentences":["This work introduces a tool for generating runtime configurations automatically from textual requirements stored as artifacts in git repositories (a.k.a. T-Reqs) alongside the software code.","The tool leverages T-Reqs-modelled architectural description to identify relevant configuration properties for the deployment of artificial intelligence (AI)-enabled software systems.","This enables traceable configuration generation, taking into account both functional and non-functional requirements.","The resulting configuration specification also includes the dynamic properties that need to be adjusted and the rationale behind their adjustment.","We show that this intermediary format can be directly used by the system or adapted for specific targets, for example in order to achieve runtime optimisations in term of ML model size before deployment."],"url":"http://arxiv.org/abs/2404.17244v1","category":"cs.SE"}
{"created":"2024-04-26 08:31:10","title":"Binarizing Documents by Leveraging both Space and Frequency","abstract":"Document Image Binarization is a well-known problem in Document Analysis and Computer Vision, although it is far from being solved. One of the main challenges of this task is that documents generally exhibit degradations and acquisition artifacts that can greatly vary throughout the page. Nonetheless, even when dealing with a local patch of the document, taking into account the overall appearance of a wide portion of the page can ease the prediction by enriching it with semantic information on the ink and background conditions. In this respect, approaches able to model both local and global information have been proven suitable for this task. In particular, recent applications of Vision Transformer (ViT)-based models, able to model short and long-range dependencies via the attention mechanism, have demonstrated their superiority over standard Convolution-based models, which instead struggle to model global dependencies. In this work, we propose an alternative solution based on the recently introduced Fast Fourier Convolutions, which overcomes the limitation of standard convolutions in modeling global information while requiring fewer parameters than ViTs. We validate the effectiveness of our approach via extensive experimental analysis considering different types of degradations.","sentences":["Document Image Binarization is a well-known problem in Document Analysis and Computer Vision, although it is far from being solved.","One of the main challenges of this task is that documents generally exhibit degradations and acquisition artifacts that can greatly vary throughout the page.","Nonetheless, even when dealing with a local patch of the document, taking into account the overall appearance of a wide portion of the page can ease the prediction by enriching it with semantic information on the ink and background conditions.","In this respect, approaches able to model both local and global information have been proven suitable for this task.","In particular, recent applications of Vision Transformer (ViT)-based models, able to model short and long-range dependencies via the attention mechanism, have demonstrated their superiority over standard Convolution-based models, which instead struggle to model global dependencies.","In this work, we propose an alternative solution based on the recently introduced Fast Fourier Convolutions, which overcomes the limitation of standard convolutions in modeling global information while requiring fewer parameters than ViTs.","We validate the effectiveness of our approach via extensive experimental analysis considering different types of degradations."],"url":"http://arxiv.org/abs/2404.17243v1","category":"cs.CV"}
{"created":"2024-04-26 08:24:25","title":"Broken time reversal symmetry vestigial state for a two-component superconductor in two spatial dimensions","abstract":"We consider the vestigial phase with broken time-reversal symmetry above the superconducting transition temperature of a two-component superconductor in two spatial dimensions. We show that, in contrast to 3D, a vestigial phase is in general allowed within Ginzburg-Landau theory. The vestigial phase occupies an increasing temperature region if the parameters in the Ginzburg-Landau theory gives a larger energy difference between the broken time reversal symmetry phase and the other ordered phase.","sentences":["We consider the vestigial phase with broken time-reversal symmetry above the superconducting transition temperature of a two-component superconductor in two spatial dimensions.","We show that, in contrast to 3D, a vestigial phase is in general allowed within Ginzburg-Landau theory.","The vestigial phase occupies an increasing temperature region if the parameters in the Ginzburg-Landau theory gives a larger energy difference between the broken time reversal symmetry phase and the other ordered phase."],"url":"http://arxiv.org/abs/2404.17239v1","category":"cond-mat.supr-con"}
{"created":"2024-04-26 08:23:36","title":"TruthSR: Trustworthy Sequential Recommender Systems via User-generated Multimodal Content","abstract":"Sequential recommender systems explore users' preferences and behavioral patterns from their historically generated data. Recently, researchers aim to improve sequential recommendation by utilizing massive user-generated multi-modal content, such as reviews, images, etc. This content often contains inevitable noise. Some studies attempt to reduce noise interference by suppressing cross-modal inconsistent information. However, they could potentially constrain the capturing of personalized user preferences. In addition, it is almost impossible to entirely eliminate noise in diverse user-generated multi-modal content. To solve these problems, we propose a trustworthy sequential recommendation method via noisy user-generated multi-modal content. Specifically, we explicitly capture the consistency and complementarity of user-generated multi-modal content to mitigate noise interference. We also achieve the modeling of the user's multi-modal sequential preferences. In addition, we design a trustworthy decision mechanism that integrates subjective user perspective and objective item perspective to dynamically evaluate the uncertainty of prediction results. Experimental evaluation on four widely-used datasets demonstrates the superior performance of our model compared to state-of-the-art methods. The code is released at https://github.com/FairyMeng/TrustSR.","sentences":["Sequential recommender systems explore users' preferences and behavioral patterns from their historically generated data.","Recently, researchers aim to improve sequential recommendation by utilizing massive user-generated multi-modal content, such as reviews, images, etc.","This content often contains inevitable noise.","Some studies attempt to reduce noise interference by suppressing cross-modal inconsistent information.","However, they could potentially constrain the capturing of personalized user preferences.","In addition, it is almost impossible to entirely eliminate noise in diverse user-generated multi-modal content.","To solve these problems, we propose a trustworthy sequential recommendation method via noisy user-generated multi-modal content.","Specifically, we explicitly capture the consistency and complementarity of user-generated multi-modal content to mitigate noise interference.","We also achieve the modeling of the user's multi-modal sequential preferences.","In addition, we design a trustworthy decision mechanism that integrates subjective user perspective and objective item perspective to dynamically evaluate the uncertainty of prediction results.","Experimental evaluation on four widely-used datasets demonstrates the superior performance of our model compared to state-of-the-art methods.","The code is released at https://github.com/FairyMeng/TrustSR."],"url":"http://arxiv.org/abs/2404.17238v1","category":"cs.IR"}
{"created":"2024-04-26 17:17:17","title":"Root-to-Leaf Scheduling in Write-Optimized Trees","abstract":"Write-optimized dictionaries are a class of cache-efficient data structures that buffer updates and apply them in batches to optimize the amortized cache misses per update. For example, a B^epsilon tree inserts updates as messages at the root. B^epsilon trees only move (\"flush\") messages when they have total size close to a cache line, optimizing the amount of work done per cache line written. Thus, recently-inserted messages reside at or near the root and are only flushed down the tree after a sufficient number of new messages arrive. Although this lazy approach works well for many operations, some types of updates do not complete until the update message reaches a leaf. For example, deferred queries and secure deletes must flush through all nodes along their root-to-leaf path before taking effect. What happens when we want to service a large number of (say) secure deletes as quickly as possible? Classic techniques leave us with an unsavory choice. On the one hand, we can group the delete messages using a write-optimized approach and move them down the tree lazily. But then many individual deletes may be left incomplete for an extended period of time, as their messages wait to be grouped with a sufficiently large number of related messages. On the other hand, we can ignore cache efficiency and perform a root-to-leaf flush for each delete. This begins work on individual deletes immediately, but harms system throughput. This paper investigates a new framework for efficiently flushing collections of messages from the root to their leaves in a write-optimized data structure. Our goal is to minimize the average time that messages reach the leaves. We give an algorithm that O(1)-approximates the optimal average completion time in this model. Along the way, we give a new 4-approximation algorithm for scheduling parallel tasks for weighted completion time with tree precedence constraints.","sentences":["Write-optimized dictionaries are a class of cache-efficient data structures that buffer updates and apply them in batches to optimize the amortized cache misses per update.","For example, a B^epsilon tree inserts updates as messages at the root.","B^epsilon trees only move (\"flush\") messages when they have total size close to a cache line, optimizing the amount of work done per cache line written.","Thus, recently-inserted messages reside at or near the root and are only flushed down the tree after a sufficient number of new messages arrive.","Although this lazy approach works well for many operations, some types of updates do not complete until the update message reaches a leaf.","For example, deferred queries and secure deletes must flush through all nodes along their root-to-leaf path before taking effect.","What happens when we want to service a large number of (say) secure deletes as quickly as possible?","Classic techniques leave us with an unsavory choice.","On the one hand, we can group the delete messages using a write-optimized approach and move them down the tree lazily.","But then many individual deletes may be left incomplete for an extended period of time, as their messages wait to be grouped with a sufficiently large number of related messages.","On the other hand, we can ignore cache efficiency and perform a root-to-leaf flush for each delete.","This begins work on individual deletes immediately, but harms system throughput.","This paper investigates a new framework for efficiently flushing collections of messages from the root to their leaves in a write-optimized data structure.","Our goal is to minimize the average time that messages reach the leaves.","We give an algorithm that O(1)-approximates the optimal average completion time in this model.","Along the way, we give a new 4-approximation algorithm for scheduling parallel tasks for weighted completion time with tree precedence constraints."],"url":"http://arxiv.org/abs/2404.17544v1","category":"cs.DS"}
{"created":"2024-04-26 13:38:29","title":"Nonreciprocal pair interactions and emergent activity mediated by scattered waves","abstract":"Particles that scatter waves interact through the waves they scatter. Using acoustically levitated spheres as a model system, we show that wave-mediated pair interactions are nonreciprocal except under a limited set of circumstances. This apparent violation of Newton's third law is allowed because the pair of particles does not constitute a closed system; the sound wave carries away the balance of the system's momentum and angular momentum. The nonreciprocity of wave-mediated interactions is significant because it enables collections of scatterers to transduce energy out of the wave and use it to power their own motion. Such wave-matter composite systems display a form of activity that is an emergent property of their state of organization. We demonstrate emergent activity arising from nonreciprocal wave-mediated interactions through experiments and simulations on trios of acoustically levitated beads.","sentences":["Particles that scatter waves interact through the waves they scatter.","Using acoustically levitated spheres as a model system, we show that wave-mediated pair interactions are nonreciprocal except under a limited set of circumstances.","This apparent violation of Newton's third law is allowed because the pair of particles does not constitute a closed system; the sound wave carries away the balance of the system's momentum and angular momentum.","The nonreciprocity of wave-mediated interactions is significant because it enables collections of scatterers to transduce energy out of the wave and use it to power their own motion.","Such wave-matter composite systems display a form of activity that is an emergent property of their state of organization.","We demonstrate emergent activity arising from nonreciprocal wave-mediated interactions through experiments and simulations on trios of acoustically levitated beads."],"url":"http://arxiv.org/abs/2404.17410v1","category":"cond-mat.soft"}
{"created":"2024-04-26 12:50:14","title":"Performance of CMS muon reconstruction from proton-proton to heavy ion collisions","abstract":"The performance of muon tracking, identification, triggering, momentum resolution, and momentum scale has been studied with the CMS detector at the LHC using data collected at $\\sqrt{s_\\mathrm{NN}}$ = 5.02 TeV in proton-proton (pp) and lead-lead (PbPb) collisions in 2017 and 2018, respectively, and at $\\sqrt{s_\\mathrm{NN}}$ = 8.16 TeV in proton-lead (pPb) collisions in 2016. Muon efficiencies, momentum resolutions, and momentum scales are compared by focusing on how the muon reconstruction performance varies from relatively small occupancy pp collisions to the larger occupancies of pPb collisions and, finally, to the highest track multiplicity PbPb collisions. We find the efficiencies of muon tracking, identification, and triggering to be above 90% throughout most of the track multiplicity range. The momentum resolution and scale are unaffected by the detector occupancy. The excellent muon reconstruction of the CMS detector enables precision studies across all available collision systems.","sentences":["The performance of muon tracking, identification, triggering, momentum resolution, and momentum scale has been studied with the CMS detector at the LHC using data collected at $\\sqrt{s_\\mathrm{NN}}$ = 5.02 TeV in proton-proton (pp) and lead-lead (PbPb) collisions in 2017 and 2018, respectively, and at $\\sqrt{s_\\mathrm{NN}}$ = 8.16 TeV in proton-lead (pPb) collisions in 2016.","Muon efficiencies, momentum resolutions, and momentum scales are compared by focusing on how the muon reconstruction performance varies from relatively small occupancy pp collisions to the larger occupancies of pPb collisions and, finally, to the highest track multiplicity PbPb collisions.","We find the efficiencies of muon tracking, identification, and triggering to be above 90% throughout most of the track multiplicity range.","The momentum resolution and scale are unaffected by the detector occupancy.","The excellent muon reconstruction of the CMS detector enables precision studies across all available collision systems."],"url":"http://arxiv.org/abs/2404.17377v1","category":"hep-ex"}
{"created":"2024-04-26 12:11:58","title":"Archives of Photographic PLates for Astronomical USE (APPLAUSE) Digitisation of astronomical plates and their integration into the International Virtual Observatory","abstract":"The Archives of Photographic PLates for Astronomical USE (APPLAUSE) project is aimed at digitising astronomical photographic plates from three major German plate collections, making them accessible through integration into the International Virtual Observatory (IVO). Photographic plates and related materials (logbooks, envelopes, etc.) were scanned with commercial flatbed scanners. Astrometric and photometric calibrations were carried out with the developed PyPlate software, using Gaia EDR3 data as a reference. The APPLAUSE data publication complies with IVO standards. The latest data release contains images and metadata from 27 plate collections from the partner institutes in Hamburg, Bamberg, and Potsdam, along with digitised archives provided by Tautenburg, Tartu, and Vatican observatories. Altogether, over two billion calibrated measurements extracted from about 70,000 direct photographic plates can readily be used to create long-term light curves. For instance, we constructed the historic light curve of the enigmatic dipping star KIC 8462852. We found no evidence of previously assumed variations on timescales of decades in our light curve. Potential uses of APPLAUSE images for transient sources can be appreciated by following the development of the nova shell of GK Per (1901) over time and the change in brightness of two extragalactic supernovae. The database holds about 10,000 spectral plates. We made use of objective prism plates to follow the temporal changes of Nova DN Gem through 1912 and 1913, highlighting an outburst in early 1913.","sentences":["The Archives of Photographic PLates for Astronomical USE (APPLAUSE) project is aimed at digitising astronomical photographic plates from three major German plate collections, making them accessible through integration into the International Virtual Observatory (IVO).","Photographic plates and related materials (logbooks, envelopes, etc.) were scanned with commercial flatbed scanners.","Astrometric and photometric calibrations were carried out with the developed PyPlate software, using Gaia EDR3 data as a reference.","The APPLAUSE data publication complies with IVO standards.","The latest data release contains images and metadata from 27 plate collections from the partner institutes in Hamburg, Bamberg, and Potsdam, along with digitised archives provided by Tautenburg, Tartu, and Vatican observatories.","Altogether, over two billion calibrated measurements extracted from about 70,000 direct photographic plates can readily be used to create long-term light curves.","For instance, we constructed the historic light curve of the enigmatic dipping star KIC 8462852.","We found no evidence of previously assumed variations on timescales of decades in our light curve.","Potential uses of APPLAUSE images for transient sources can be appreciated by following the development of the nova shell of GK Per (1901) over time and the change in brightness of two extragalactic supernovae.","The database holds about 10,000 spectral plates.","We made use of objective prism plates to follow the temporal changes of Nova DN Gem through 1912 and 1913, highlighting an outburst in early 1913."],"url":"http://arxiv.org/abs/2404.17355v1","category":"astro-ph.IM"}
{"created":"2024-04-26 11:53:43","title":"ANAIS-112 three years data: a sensitive model independent negative test of the DAMA/LIBRA dark matter signal","abstract":"Weakly interacting massive particles (WIMPs) are well-motivated candidates for dark matter. One signature of galactic WIMPs is the annual modulation expected in a detector's interaction rate, which arises from Earth's rotation around the Sun. Over two decades, the DAMA/LIBRA experiment has observed such modulation with 250 kg of NaI(Tl) scintillators, in accordance with WIMP expectations but inconsistent with the negative results of other experiments. The signal, however, depends on the target, so in order to either validate or refute the DAMA signal it is necessary to replicate the experiment using the same material. This is the goal of the ANAIS-112 experiment, currently underway since August 2017 with 112.5 kg of NaI(Tl). In this work, we present a reanalysis of three years of data employing an improved analysis chain to enhance the experimental sensitivity. The results presented here are consistent with the absence of modulation and inconsistent with DAMA's observation at nearly 3$\\sigma$ C.L., with the potential to reach a 5$\\sigma$ level within 8 years of data collection. Additionally, we explore the impact of different scintillation quenching factors in the comparison between ANAIS-112 and DAMA/LIBRA.","sentences":["Weakly interacting massive particles (WIMPs) are well-motivated candidates for dark matter.","One signature of galactic WIMPs is the annual modulation expected in a detector's interaction rate, which arises from Earth's rotation around the Sun.","Over two decades, the DAMA/LIBRA experiment has observed such modulation with 250 kg of NaI(Tl) scintillators, in accordance with WIMP expectations but inconsistent with the negative results of other experiments.","The signal, however, depends on the target, so in order to either validate or refute the DAMA signal it is necessary to replicate the experiment using the same material.","This is the goal of the ANAIS-112 experiment, currently underway since August 2017 with 112.5 kg of NaI(Tl).","In this work, we present a reanalysis of three years of data employing an improved analysis chain to enhance the experimental sensitivity.","The results presented here are consistent with the absence of modulation and inconsistent with DAMA's observation at nearly 3$\\sigma$ C.L., with the potential to reach a 5$\\sigma$ level within 8 years of data collection.","Additionally, we explore the impact of different scintillation quenching factors in the comparison between ANAIS-112 and DAMA/LIBRA."],"url":"http://arxiv.org/abs/2404.17348v1","category":"astro-ph.IM"}
{"created":"2024-04-26 11:51:30","title":"Revealing mode formation in quasi-bound states in the continuum metasurfaces via near-field optical microscopy","abstract":"Photonic metasurfaces offer exceptional control over light at the nanoscale, facilitating applications spanning from biosensing, and nonlinear optics to photocatalysis. Many metasurfaces, especially resonant ones, rely on periodicity for the collective mode to form, which makes them subject to the influences of finite size effects, defects, and edge effects, all of which have considerable negative impact at the application level. These aspects are especially important for quasi-bound state in the continuum (BIC) metasurfaces, for which the collective mode is highly sensitive to perturbations due to high quality factors and strong near-field enhancement. Here, we quantitatively investigate the mode formation in quasi-BIC metasurfaces on the individual resonator level using scattering scanning near-field optical microscopy (s-SNOM) in combination with a new image processing technique. We find that the quasi-BIC mode is formed at a minimum size of 10 x 10-unit cells much smaller than expected from far-field measurements. Furthermore, we show that the coupling direction of the resonators, defects and edge states have pronounced influence on the quasi-BIC mode. This study serves as a link between the far-field and near-field responses of metasurfaces, offering crucial insights for optimizing spatial footprint and active area, holding promise for augmenting applications such as catalysis and biospectroscopy.","sentences":["Photonic metasurfaces offer exceptional control over light at the nanoscale, facilitating applications spanning from biosensing, and nonlinear optics to photocatalysis.","Many metasurfaces, especially resonant ones, rely on periodicity for the collective mode to form, which makes them subject to the influences of finite size effects, defects, and edge effects, all of which have considerable negative impact at the application level.","These aspects are especially important for quasi-bound state in the continuum (BIC) metasurfaces, for which the collective mode is highly sensitive to perturbations due to high quality factors and strong near-field enhancement.","Here, we quantitatively investigate the mode formation in quasi-BIC metasurfaces on the individual resonator level using scattering scanning near-field optical microscopy (s-SNOM) in combination with a new image processing technique.","We find that the quasi-BIC mode is formed at a minimum size of 10 x 10-unit cells much smaller than expected from far-field measurements.","Furthermore, we show that the coupling direction of the resonators, defects and edge states have pronounced influence on the quasi-BIC mode.","This study serves as a link between the far-field and near-field responses of metasurfaces, offering crucial insights for optimizing spatial footprint and active area, holding promise for augmenting applications such as catalysis and biospectroscopy."],"url":"http://arxiv.org/abs/2404.17346v1","category":"physics.optics"}
{"created":"2024-04-26 09:38:17","title":"An improved correction of radial-velocity systematic for the SOPHIE spectrograph","abstract":"High precision spectrographs might exhibit temporal variations of their reference velocity or nightly zero point (NZP). One way to monitor the NZP is to measure bright stars, which are assumed to have an intrinsic radial velocity variation much smaller than the instrument's precision. While this method is effective in most cases, it does not fully propagate the uncertainty arising from NZP variations. We present a new method to correct for NZP variations in radial-velocity time series. This method uses Gaussian Processes based on ancillary information to model these systematic effects. It enables us to propagate the uncertainties of this correction into the overall error budget. Another advantage of this approach is that it relies on ancillary data collected simultaneously with the spectra rather than solely on dedicated observations of constant stars. We applied this method to the SOPHIE spectrograph at the Haute-Provence Observatory using a few instrument's housekeeping data, such as the internal pressure and temperature variations. Our results demonstrate that this method effectively models the red noise of constant stars, even with a limited amount of housekeeping data, while preserving the signals of exoplanets. Using both simulations with mock planets and real data, we found that this method improves the false-alarm probability of detections by several orders of magnitude. By simulating numerous planetary signals, we were able to detect up to 10 percent more planets with small amplitude radial velocity signals. We used this new correction to reanalysed the planetary system around HD158259 and improved the detection of the outermost planets. We also suggest decreasing the observing cadence of the constant stars to optimise telescope time for scientific targets.","sentences":["High precision spectrographs might exhibit temporal variations of their reference velocity or nightly zero point (NZP).","One way to monitor the NZP is to measure bright stars, which are assumed to have an intrinsic radial velocity variation much smaller than the instrument's precision.","While this method is effective in most cases, it does not fully propagate the uncertainty arising from NZP variations.","We present a new method to correct for NZP variations in radial-velocity time series.","This method uses Gaussian Processes based on ancillary information to model these systematic effects.","It enables us to propagate the uncertainties of this correction into the overall error budget.","Another advantage of this approach is that it relies on ancillary data collected simultaneously with the spectra rather than solely on dedicated observations of constant stars.","We applied this method to the SOPHIE spectrograph at the Haute-Provence Observatory using a few instrument's housekeeping data, such as the internal pressure and temperature variations.","Our results demonstrate that this method effectively models the red noise of constant stars, even with a limited amount of housekeeping data, while preserving the signals of exoplanets.","Using both simulations with mock planets and real data, we found that this method improves the false-alarm probability of detections by several orders of magnitude.","By simulating numerous planetary signals, we were able to detect up to 10 percent more planets with small amplitude radial velocity signals.","We used this new correction to reanalysed the planetary system around HD158259 and improved the detection of the outermost planets.","We also suggest decreasing the observing cadence of the constant stars to optimise telescope time for scientific targets."],"url":"http://arxiv.org/abs/2404.17282v1","category":"astro-ph.EP"}
{"created":"2024-04-26 17:56:05","title":"Spectrum occupies pseudospectrum for random matrices with diagonal deformation and variance profile","abstract":"We consider $n\\times n$ non-Hermitian random matrices with independent entries and a variance profile, as well as an additive deterministic diagonal deformation. We show that the support of the asymptotic eigenvalue distribution in the complex plane exactly coincides with the $\\varepsilon$-pseudospectrum in the consecutive limits $n \\to \\infty$ and $\\varepsilon \\to 0$. Furthermore, we provide a description of this support in terms of a single real-valued function on the complex plane. As a level set of this locally real analytic function, the spectral edge is a real analytic variety of dimension at most one.","sentences":["We consider $n\\times n$ non-Hermitian random matrices with independent entries and a variance profile, as well as an additive deterministic diagonal deformation.","We show that the support of the asymptotic eigenvalue distribution in the complex plane exactly coincides with the $\\varepsilon$-pseudospectrum in the consecutive limits $n \\to \\infty$ and $\\varepsilon \\to 0$.","Furthermore, we provide a description of this support in terms of a single real-valued function on the complex plane.","As a level set of this locally real analytic function, the spectral edge is a real analytic variety of dimension at most one."],"url":"http://arxiv.org/abs/2404.17573v1","category":"math.PR"}
{"created":"2024-04-26 17:53:51","title":"The boring history of Gaia BH3 from isolated binary evolution","abstract":"Gaia BH3 is the first observed dormant black hole (BH) with a mass of $\\approx{30}$ M$_\\odot$ and represents the first confirmation that such massive BHs are associated with metal-poor stars. Here, we explore the isolated binary formation channel for Gaia BH3 focusing on the old and metal-poor stellar population of the Milky Way halo. We use our open-source population synthesis code SEVN to evolve $3.2 \\times 10^8$ binaries exploring 16 sets of parameters. We find that systems like Gaia BH3 form preferentially from binaries initially composed of a massive star ($40-60$ M$_\\odot$) and a low mass companion ($<1$ M$_\\odot$) in a wide ($P>10^3$ days) and eccentric orbit ($e>0.6$). Such progenitors do not undergo any Roche-lobe overflow episode during their entire evolution, so that the final orbital properties of the BH-star system are essentially determined at the core collapse of the primary star. Low natal kicks ($\\approx$ 10~km/s) significantly favour the formation of Gaia BH3-like systems, but high velocity kicks up to $\\approx 220$ km/s are also allowed. We estimate the formation efficiency for Gaia BH3-like systems in old ($t>10$ Gyr) and metal-poor ($Z<0.01$) populations to be $4 \\times 10^{-8}$ M$_\\odot^{-1}$, representing $\\approx 3\\%$ of the whole simulated BH-star population. We expect up to $\\sim 3000$ BH-star systems in the Galactic halo formed through isolated evolution, of which $\\sim 100$ are compatible with Gaia BH3 alike. Gaia BH3-like systems represent a common product of isolated binary evolution at low metallicity ($Z<0.01$), but given the steep density profile of the Galactic halo we do not expect more than one in the halo at the observed distance of Gaia BH3. Considering the estimated formation efficiency for both the isolated and dynamical formation channel, we conclude that they are almost equally likely to explain the origin of Gaia BH3.","sentences":["Gaia BH3 is the first observed dormant black hole (BH) with a mass of $\\approx{30}$ M$_\\odot$ and represents the first confirmation that such massive BHs are associated with metal-poor stars.","Here, we explore the isolated binary formation channel for Gaia BH3 focusing on the old and metal-poor stellar population of the Milky Way halo.","We use our open-source population synthesis code SEVN to evolve $3.2 \\times 10^8$ binaries exploring 16 sets of parameters.","We find that systems like Gaia BH3 form preferentially from binaries initially composed of a massive star ($40-60$ M$_\\odot$) and a low mass companion ($<1$ M$_\\odot$) in a wide ($P>10^3$ days) and eccentric orbit ($e>0.6$).","Such progenitors do not undergo any Roche-lobe overflow episode during their entire evolution, so that the final orbital properties of the BH-star system are essentially determined at the core collapse of the primary star.","Low natal kicks ($\\approx$ 10~km/s) significantly favour the formation of Gaia BH3-like systems, but high velocity kicks up to $\\approx 220$ km/s are also allowed.","We estimate the formation efficiency for Gaia BH3-like systems in old ($t>10$ Gyr) and metal-poor ($Z<0.01$) populations to be $4 \\times 10^{-8}$ M$_\\odot^{-1}$, representing $\\approx 3\\%$ of the whole simulated BH-star population.","We expect up to $\\sim 3000$ BH-star systems in the Galactic halo formed through isolated evolution, of which $\\sim 100$ are compatible with Gaia BH3 alike.","Gaia BH3-like systems represent a common product of isolated binary evolution at low metallicity ($Z<0.01$), but given the steep density profile of the Galactic halo we do not expect more than one in the halo at the observed distance of Gaia BH3.","Considering the estimated formation efficiency for both the isolated and dynamical formation channel, we conclude that they are almost equally likely to explain the origin of Gaia BH3."],"url":"http://arxiv.org/abs/2404.17568v1","category":"astro-ph.GA"}
{"created":"2024-04-26 17:47:14","title":"ChangeBind: A Hybrid Change Encoder for Remote Sensing Change Detection","abstract":"Change detection (CD) is a fundamental task in remote sensing (RS) which aims to detect the semantic changes between the same geographical regions at different time stamps. Existing convolutional neural networks (CNNs) based approaches often struggle to capture long-range dependencies. Whereas recent transformer-based methods are prone to the dominant global representation and may limit their capabilities to capture the subtle change regions due to the complexity of the objects in the scene. To address these limitations, we propose an effective Siamese-based framework to encode the semantic changes occurring in the bi-temporal RS images. The main focus of our design is to introduce a change encoder that leverages local and global feature representations to capture both subtle and large change feature information from multi-scale features to precisely estimate the change regions. Our experimental study on two challenging CD datasets reveals the merits of our approach and obtains state-of-the-art performance.","sentences":["Change detection (CD) is a fundamental task in remote sensing (RS) which aims to detect the semantic changes between the same geographical regions at different time stamps.","Existing convolutional neural networks (CNNs) based approaches often struggle to capture long-range dependencies.","Whereas recent transformer-based methods are prone to the dominant global representation and may limit their capabilities to capture the subtle change regions due to the complexity of the objects in the scene.","To address these limitations, we propose an effective Siamese-based framework to encode the semantic changes occurring in the bi-temporal RS images.","The main focus of our design is to introduce a change encoder that leverages local and global feature representations to capture both subtle and large change feature information from multi-scale features to precisely estimate the change regions.","Our experimental study on two challenging CD datasets reveals the merits of our approach and obtains state-of-the-art performance."],"url":"http://arxiv.org/abs/2404.17565v1","category":"cs.CV"}
{"created":"2024-04-26 17:45:32","title":"An exactly solvable model for emergence and scaling laws","abstract":"Deep learning models can exhibit what appears to be a sudden ability to solve a new problem as training time ($T$), training data ($D$), or model size ($N$) increases, a phenomenon known as emergence. In this paper, we present a framework where each new ability (a skill) is represented as a basis function. We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute ($C$). We compare our detailed calculations to direct simulations of a two-layer neural network trained on multitask sparse parity, where the tasks in the dataset are distributed according to a power-law. Our simple model captures, using a single fit parameter, the sigmoidal emergence of multiple new skills as training time, data size or model size increases in the neural network.","sentences":["Deep learning models can exhibit what appears to be a sudden ability to solve a new problem as training time ($T$), training data ($D$), or model size ($N$) increases, a phenomenon known as emergence.","In this paper, we present a framework where each new ability (a skill) is represented as a basis function.","We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute ($C$).","We compare our detailed calculations to direct simulations of a two-layer neural network trained on multitask sparse parity, where the tasks in the dataset are distributed according to a power-law.","Our simple model captures, using a single fit parameter, the sigmoidal emergence of multiple new skills as training time, data size or model size increases in the neural network."],"url":"http://arxiv.org/abs/2404.17563v1","category":"cs.LG"}
{"created":"2024-04-26 17:33:26","title":"Ferrimagnetism of ultracold fermions in a multi-band Hubbard system","abstract":"Strongly correlated materials feature multiple electronic orbitals which are crucial to accurately understand their many-body properties, from cuprate materials to twisted bilayer graphene. In such multi-band models, quantum interference can lead to dispersionless bands whose large degeneracy gives rise to itinerant magnetism even with weak interactions. Here, we report on signatures of a ferrimagnetic state realized in a Lieb lattice at half-filling, characterized by antialigned magnetic moments with antiferromagnetic correlations, concomitant with a finite spin polarization. We demonstrate its robustness when increasing repulsive interactions from the non-interacting to the Heisenberg regime, and study its emergence when continuously tuning the lattice unit cell from a square to a Lieb geometry. Our work paves the way towards exploring exotic phases in related multi-orbital models such as quantum spin liquids in kagome lattices and heavy fermion behavior in Kondo models.","sentences":["Strongly correlated materials feature multiple electronic orbitals which are crucial to accurately understand their many-body properties, from cuprate materials to twisted bilayer graphene.","In such multi-band models, quantum interference can lead to dispersionless bands whose large degeneracy gives rise to itinerant magnetism even with weak interactions.","Here, we report on signatures of a ferrimagnetic state realized in a Lieb lattice at half-filling, characterized by antialigned magnetic moments with antiferromagnetic correlations, concomitant with a finite spin polarization.","We demonstrate its robustness when increasing repulsive interactions from the non-interacting to the Heisenberg regime, and study its emergence when continuously tuning the lattice unit cell from a square to a Lieb geometry.","Our work paves the way towards exploring exotic phases in related multi-orbital models such as quantum spin liquids in kagome lattices and heavy fermion behavior in Kondo models."],"url":"http://arxiv.org/abs/2404.17555v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-26 17:33:24","title":"A Novel Context driven Critical Integrative Levels (CIL) Approach: Advancing Human-Centric and Integrative Lighting Asset Management in Public Libraries with Practical Thresholds","abstract":"This paper proposes the context driven Critical Integrative Levels (CIL), a novel approach to lighting asset management in public libraries that aligns with the transformative vision of human-centric and integrative lighting. This approach encompasses not only the visual aspects of lighting performance but also prioritizes the physiological and psychological well-being of library users. Incorporating a newly defined metric, Mean Time of Exposure (MTOE), the approach quantifies user-light interaction, enabling tailored lighting strategies that respond to diverse activities and needs in library spaces. Case studies demonstrate how the CIL matrix can be practically applied, offering significant improvements over conventional methods by focusing on optimized user experiences from both visual impacts and non-visual effects.","sentences":["This paper proposes the context driven Critical Integrative Levels (CIL), a novel approach to lighting asset management in public libraries that aligns with the transformative vision of human-centric and integrative lighting.","This approach encompasses not only the visual aspects of lighting performance but also prioritizes the physiological and psychological well-being of library users.","Incorporating a newly defined metric, Mean Time of Exposure (MTOE), the approach quantifies user-light interaction, enabling tailored lighting strategies that respond to diverse activities and needs in library spaces.","Case studies demonstrate how the CIL matrix can be practically applied, offering significant improvements over conventional methods by focusing on optimized user experiences from both visual impacts and non-visual effects."],"url":"http://arxiv.org/abs/2404.17554v1","category":"cs.HC"}
{"created":"2024-04-26 17:27:14","title":"Probing the Propeller Regime with Symbiotic X-ray Binaries","abstract":"At the moment, there are two neutron star X-ray binaries with massive red supergiants as donors. De et al. (2023) proposed that the system SWIFT J0850.8-4219 contains a neutron star at the propeller stage. We study this possibility by applying various models of propeller spin-down. We demonstrate that the duration of the propeller stage is very sensitive to the regime of rotational losses. Only in the case of a relatively slow propeller model proposed by Davies and Pringle (1981), the duration of the propeller is long enough to provide a significant probability to observe the system at this stage. Future determination of the system parameters (orbital and spin periods, magnetic field of the compact object, etc.) will allow putting strong constraints on the propeller behavior.","sentences":["At the moment, there are two neutron star X-ray binaries with massive red supergiants as donors.","De et al. (2023) proposed that the system SWIFT J0850.8-4219 contains a neutron star at the propeller stage.","We study this possibility by applying various models of propeller spin-down.","We demonstrate that the duration of the propeller stage is very sensitive to the regime of rotational losses.","Only in the case of a relatively slow propeller model proposed by Davies and Pringle (1981), the duration of the propeller is long enough to provide a significant probability to observe the system at this stage.","Future determination of the system parameters (orbital and spin periods, magnetic field of the compact object, etc.) will allow putting strong constraints on the propeller behavior."],"url":"http://arxiv.org/abs/2404.17549v1","category":"astro-ph.HE"}
{"created":"2024-04-26 17:18:58","title":"Integrating UAV-Enabled Base Stations in 3D Networks: QoS-Aware Joint Fronthaul and Backhaul Design","abstract":"The emerging concept of 3D networks, integrating terrestrial, aerial, and space layers, introduces a novel and complex structure characterized by stations relaying backhaul loads through point-to-point wireless links, forming a wireless 3D backhaul mesh. A key challenge is the strategic placement of aerial platform such as drone base stations (DBSs), considering the locations and service demands of ground nodes and the connectivity to backhaul gateway nodes for core network access. This paper addresses these complexities with a two-fold approach: a novel Agglomerative Hierarchical Clustering (HC) algorithm that optimizes DBS locations to satisfy minimum backhaul adjacency and maximum fronthaul coverage radius requirements; and a Genetic Algorithm (GA) that designs backhaul connections to satisfy the cumulative load across the network and maximize the throughput margin which translates to network resilience to increasing demands. Our results showcase the effectiveness of these algorithms against benchline schemes, offering insights into the operational dynamics of these novel 3D networks.","sentences":["The emerging concept of 3D networks, integrating terrestrial, aerial, and space layers, introduces a novel and complex structure characterized by stations relaying backhaul loads through point-to-point wireless links, forming a wireless 3D backhaul mesh.","A key challenge is the strategic placement of aerial platform such as drone base stations (DBSs), considering the locations and service demands of ground nodes and the connectivity to backhaul gateway nodes for core network access.","This paper addresses these complexities with a two-fold approach: a novel Agglomerative Hierarchical Clustering (HC) algorithm that optimizes DBS locations to satisfy minimum backhaul adjacency and maximum fronthaul coverage radius requirements; and a Genetic Algorithm (GA) that designs backhaul connections to satisfy the cumulative load across the network and maximize the throughput margin which translates to network resilience to increasing demands.","Our results showcase the effectiveness of these algorithms against benchline schemes, offering insights into the operational dynamics of these novel 3D networks."],"url":"http://arxiv.org/abs/2404.17547v1","category":"cs.NI"}
{"created":"2024-04-26 17:13:55","title":"Complexity of Minimizing Regularized Convex Quadratic Functions","abstract":"In this work, we study the iteration complexity of gradient methods minimizing the class of uniformly convex regularized quadratic functions. We prove lower bounds on the functional residual of the form $\\Omega(N^{-2p/(p-2)})$, where $p > 2$ is the power of the regularization term, and $N$ is the number of calls to a first-order oracle. A special case of our problem class is $p=3$, which is the minimization of cubically regularized convex quadratic functions. It naturally appears as a subproblem at each iteration of the cubic Newton method. The corresponding lower bound for $p = 3$ becomes $\\Omega(N^{-6})$. Our result matches the best-known upper bounds on this problem class, rendering a sharp analysis of the minimization of uniformly convex regularized quadratic functions. We also establish new lower bounds on minimizing the gradient norm within our framework.","sentences":["In this work, we study the iteration complexity of gradient methods minimizing the class of uniformly convex regularized quadratic functions.","We prove lower bounds on the functional residual of the form $\\Omega(N^{-2p/(p-2)})$, where $p > 2$ is the power of the regularization term, and $N$ is the number of calls to a first-order oracle.","A special case of our problem class is $p=3$, which is the minimization of cubically regularized convex quadratic functions.","It naturally appears as a subproblem at each iteration of the cubic Newton method.","The corresponding lower bound for $p = 3$ becomes $\\Omega(N^{-6})$.","Our result matches the best-known upper bounds on this problem class, rendering a sharp analysis of the minimization of uniformly convex regularized quadratic functions.","We also establish new lower bounds on minimizing the gradient norm within our framework."],"url":"http://arxiv.org/abs/2404.17543v1","category":"math.OC"}
{"created":"2024-04-26 17:06:23","title":"Multifold topological semimetals","abstract":"The discovery of topological semimetals with multifold band crossings has opened up a new and exciting frontier in the field of topological physics. These materials exhibit large Chern numbers, leading to long double Fermi arcs on their surfaces, which are protected by either crystal symmetries or topological order. The impact of these multifold crossings extends beyond surface science, as they are not constrained by the Poincar\\'e classification of quasiparticles and only need to respect the crystal symmetry of one of the 1651 magnetic space groups. Consequently, we observe the emergence of free fermionic excitations in solid-state systems that have no high-energy counterparts, protected by non-symmorphic symmetries. In this work, we review the recent theoretical and experimental progress made in the field of multifold topological semimetals. We begin with the theoretical prediction of the so-called multifold fermions and discuss the subsequent discoveries of chiral and magnetic topological semimetals. Several experiments that have realized chiral semimetals in spectroscopic measurements are described, and we discuss the future prospects of this field. These exciting developments have the potential to deepen our understanding of the fundamental properties of quantum matter and inspire new technological applications in the future.","sentences":["The discovery of topological semimetals with multifold band crossings has opened up a new and exciting frontier in the field of topological physics.","These materials exhibit large Chern numbers, leading to long double Fermi arcs on their surfaces, which are protected by either crystal symmetries or topological order.","The impact of these multifold crossings extends beyond surface science, as they are not constrained by the Poincar\\'e classification of quasiparticles and only need to respect the crystal symmetry of one of the 1651 magnetic space groups.","Consequently, we observe the emergence of free fermionic excitations in solid-state systems that have no high-energy counterparts, protected by non-symmorphic symmetries.","In this work, we review the recent theoretical and experimental progress made in the field of multifold topological semimetals.","We begin with the theoretical prediction of the so-called multifold fermions and discuss the subsequent discoveries of chiral and magnetic topological semimetals.","Several experiments that have realized chiral semimetals in spectroscopic measurements are described, and we discuss the future prospects of this field.","These exciting developments have the potential to deepen our understanding of the fundamental properties of quantum matter and inspire new technological applications in the future."],"url":"http://arxiv.org/abs/2404.17539v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-26 17:05:28","title":"[OI] fine structure line profiles in Mon R2 and M17 SW: the puzzling nature of cold foreground material identified by [12CII] self-absorption","abstract":"Context. Recent studies of the optical depth comparing [12CII] and [13CII] line profiles in Galactic star-forming regions revealed strong self-absorption in [12CII] by low excitation foreground material, implying a large column density of C+ corresponding to an equivalent AV of a few, up to about 10 mag.   Aims. As the nature and origin of such a large column of cold C+ foreground gas are difficult to explain, it is essential to constrain the physical conditions of this material.   Methods. We conducted high-resolution observations of [OI] 63 um and [OI] 145 um lines in M17 SW and Mon R2. The [OI] 145 um transition traces warm PDR-material, while the [OI] 63 um line traces foreground material as manifested by absorption dips.   Results. Comparison of both [OI] line profiles with [CII] isotopic lines confirms warm PDR-origin background emission and a significant column of cold foreground material causing self-absorption visible in [12CII] and [OI] 63 um profiles. In M17 SW, the C+ and O column densities are comparable for both layers. Mon R2 exhibits larger O columns compared to C+, indicating additional material where the carbon is neutral or in molecular form. Small-scale spatial variation of the foreground absorption profiles and the large column density (around 1E18 cm-2 ) of the foreground material suggest emission from high-density regions associated with the cloud complex, not a uniform diffuse foreground cloud.   Conclusions. The analysis confirms that the previously detected intense [CII] foreground absorption is attributable to a large column of low excitation dense atomic material, where carbon is ionized, and oxygen is in neutral atomic form.","sentences":["Context.","Recent studies of the optical depth comparing [12CII] and [13CII] line profiles in Galactic star-forming regions revealed strong self-absorption in [12CII] by low excitation foreground material, implying a large column density of C+ corresponding to an equivalent AV of a few, up to about 10 mag.   Aims.","As the nature and origin of such a large column of cold C+ foreground gas are difficult to explain, it is essential to constrain the physical conditions of this material.   ","Methods.","We conducted high-resolution observations of [OI] 63 um and [OI] 145 um lines in M17 SW and Mon R2.","The [OI] 145 um transition traces warm PDR-material, while the [OI] 63 um line traces foreground material as manifested by absorption dips.   Results.","Comparison of both [OI] line profiles with [CII] isotopic lines confirms warm PDR-origin background emission and a significant column of cold foreground material causing self-absorption visible in [12CII] and [OI] 63 um profiles.","In M17 SW, the C+ and O column densities are comparable for both layers.","Mon R2 exhibits larger O columns compared to C+, indicating additional material where the carbon is neutral or in molecular form.","Small-scale spatial variation of the foreground absorption profiles and the large column density (around 1E18 cm-2 ) of the foreground material suggest emission from high-density regions associated with the cloud complex, not a uniform diffuse foreground cloud.   ","Conclusions.","The analysis confirms that the previously detected intense [CII] foreground absorption is attributable to a large column of low excitation dense atomic material, where carbon is ionized, and oxygen is in neutral atomic form."],"url":"http://arxiv.org/abs/2404.17538v1","category":"astro-ph.GA"}
{"created":"2024-04-26 16:57:08","title":"Mitigating Collisions in Sidelink NR V2X: A Study on Cooperative Resource Allocation","abstract":"New Radio (NR) Vehicle-to-Everything (V2X) Sidelink (SL), an integral part of the 5G NR standard, is expected to revolutionize the automotive and rail industries by enabling direct and low-latency exchange of critical information between traffic participants independently of cellular networks. However, this advancement depends primarily on efficient SL resource allocation. Mode 2(a) is a well-known method for this purpose, where each node autonomously selects resources. However, this method is prone to packet collisions due to the hidden-node problem. In this paper, we propose a cooperative scheduling method that could potentially address this issue. We describe an extension of Mode 2(a) that allows nodes to share resource allocation information at two hops. Initial simulation results show a promising improvement over Mode 2(a).","sentences":["New Radio (NR) Vehicle-to-Everything (V2X) Sidelink (SL), an integral part of the 5G NR standard, is expected to revolutionize the automotive and rail industries by enabling direct and low-latency exchange of critical information between traffic participants independently of cellular networks.","However, this advancement depends primarily on efficient SL resource allocation.","Mode 2(a) is a well-known method for this purpose, where each node autonomously selects resources.","However, this method is prone to packet collisions due to the hidden-node problem.","In this paper, we propose a cooperative scheduling method that could potentially address this issue.","We describe an extension of Mode 2(a) that allows nodes to share resource allocation information at two hops.","Initial simulation results show a promising improvement over Mode 2(a)."],"url":"http://arxiv.org/abs/2404.17532v1","category":"cs.NI"}
{"created":"2024-04-26 16:46:10","title":"Reduction of the effective population size in a branching particle system in the moderate mutation-selection regime","abstract":"We consider a system of particles performing a one-dimensional dyadic branching Brownian motion with positive drift $\\beta\\in(0,1)$, branching rate 1/2, killed at $L(\\beta)>0$, and reflected at 0. The killing boundary $L(\\beta)$ is chosen so that the total population size is approximately constant, proportional to $N\\in\\mathbb{N}$. This branching system is interpreted as a population accumulating deleterious mutations.   We prove that, when the typical width of the cloud of particles is of order $c\\log(N)$, $c\\in(0,1)$, the demographic fluctuations of the system converge to a Feller diffusion on the time scale $N^{1-c}$. In addition, we show that the limiting genealogy of the system comprises only binary mergers and that these mergers are concentrated in the vicinity of the reflective boundary. This model is a version of the branching Brownian motion with absorption studied by Berestycki, Berestycki and Schweinsberg to describe the effect of natural selection on the genealogy of a population accumulating beneficial mutations. In the latter case, the genealogical structure of the system is described by a Bolthausen-Sznitman coalescent on a logarithmic time scale. In this work, we show that, when the population size in the fittest class is mesoscopic, namely of order $N^{1-c}$, the genealogy of the system is given by a Kingman coalescent on a polynomial time scale.","sentences":["We consider a system of particles performing a one-dimensional dyadic branching Brownian motion with positive drift $\\beta\\in(0,1)$, branching rate 1/2, killed at $L(\\beta)>0$, and reflected at 0.","The killing boundary $L(\\beta)$ is chosen so that the total population size is approximately constant, proportional to $N\\in\\mathbb{N}$.","This branching system is interpreted as a population accumulating deleterious mutations.   ","We prove that, when the typical width of the cloud of particles is of order $c\\log(N)$, $c\\in(0,1)$, the demographic fluctuations of the system converge to a Feller diffusion on the time scale","$N^{1-c}$.","In addition, we show that the limiting genealogy of the system comprises only binary mergers and that these mergers are concentrated in the vicinity of the reflective boundary.","This model is a version of the branching Brownian motion with absorption studied by Berestycki, Berestycki and Schweinsberg to describe the effect of natural selection on the genealogy of a population accumulating beneficial mutations.","In the latter case, the genealogical structure of the system is described by a Bolthausen-Sznitman coalescent on a logarithmic time scale.","In this work, we show that, when the population size in the fittest class is mesoscopic, namely of order $N^{1-c}$, the genealogy of the system is given by a Kingman coalescent on a polynomial time scale."],"url":"http://arxiv.org/abs/2404.17527v1","category":"math.PR"}
{"created":"2024-04-26 16:40:50","title":"Forming Mercury from excited initial conditions","abstract":"Mercury is notoriously difficult to form in solar system simulations, due to its small mass and iron-rich composition. Smooth particle hydrodynamics simulations of collisions have found that a Mercury-like body could be formed by one or multiple giant impacts, but due to the chaotic nature of collisions it is difficult to create a scenario where such impacts will take place. Recent work has found more success forming Mercury analogues by adding additional embryos near Mercury's orbit. In this work, we aim to form Mercury by simulating the formation of the solar system in the presence of the giant planets Jupiter and Saturn. We test out the effect of an inner disk of embryos added on to the commonly-used narrow annulus of initial material. We form Mercury analogues with core-mass fractions (CMF) $> 0.4$ in $\\sim 10\\%$ of our simulations, and twice that number of Mercury analogues form during the formation process, but are unstable and do not last to the end of the simulations. Mercury analogues form at similar rates for both disks with and without an inner component, and most of our Mercury analogues have lower CMF than that of Mercury, $\\sim 0.7$, due to significant accretion of debris material. We suggest that a more in-depth understanding of the fraction of debris mass that is lost to collisional grinding is necessary to understand Mercury's formation, or some additional mechanism is required to stop this debris from accreting.","sentences":["Mercury is notoriously difficult to form in solar system simulations, due to its small mass and iron-rich composition.","Smooth particle hydrodynamics simulations of collisions have found that a Mercury-like body could be formed by one or multiple giant impacts, but due to the chaotic nature of collisions it is difficult to create a scenario where such impacts will take place.","Recent work has found more success forming Mercury analogues by adding additional embryos near Mercury's orbit.","In this work, we aim to form Mercury by simulating the formation of the solar system in the presence of the giant planets Jupiter and Saturn.","We test out the effect of an inner disk of embryos added on to the commonly-used narrow annulus of initial material.","We form Mercury analogues with core-mass fractions (CMF) $> 0.4$ in $\\sim 10\\%$ of our simulations, and twice that number of Mercury analogues form during the formation process, but are unstable and do not last to the end of the simulations.","Mercury analogues form at similar rates for both disks with and without an inner component, and most of our Mercury analogues have lower CMF than that of Mercury, $\\sim 0.7$, due to significant accretion of debris material.","We suggest that a more in-depth understanding of the fraction of debris mass that is lost to collisional grinding is necessary to understand Mercury's formation, or some additional mechanism is required to stop this debris from accreting."],"url":"http://arxiv.org/abs/2404.17523v1","category":"astro-ph.EP"}
{"created":"2024-04-26 16:40:17","title":"Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual and Action Representations","abstract":"Autonomous robotic systems capable of learning novel manipulation tasks are poised to transform industries from manufacturing to service automation. However, modern methods (e.g., VIP and R3M) still face significant hurdles, notably the domain gap among robotic embodiments and the sparsity of successful task executions within specific action spaces, resulting in misaligned and ambiguous task representations. We introduce Ag2Manip (Agent-Agnostic representations for Manipulation), a framework aimed at surmounting these challenges through two key innovations: a novel agent-agnostic visual representation derived from human manipulation videos, with the specifics of embodiments obscured to enhance generalizability; and an agent-agnostic action representation abstracting a robot's kinematics to a universal agent proxy, emphasizing crucial interactions between end-effector and object. Ag2Manip's empirical validation across simulated benchmarks like FrankaKitchen, ManiSkill, and PartManip shows a 325% increase in performance, achieved without domain-specific demonstrations. Ablation studies underline the essential contributions of the visual and action representations to this success. Extending our evaluations to the real world, Ag2Manip significantly improves imitation learning success rates from 50% to 77.5%, demonstrating its effectiveness and generalizability across both simulated and physical environments.","sentences":["Autonomous robotic systems capable of learning novel manipulation tasks are poised to transform industries from manufacturing to service automation.","However, modern methods (e.g., VIP and R3M) still face significant hurdles, notably the domain gap among robotic embodiments and the sparsity of successful task executions within specific action spaces, resulting in misaligned and ambiguous task representations.","We introduce Ag2Manip (Agent-Agnostic representations for Manipulation), a framework aimed at surmounting these challenges through two key innovations: a novel agent-agnostic visual representation derived from human manipulation videos, with the specifics of embodiments obscured to enhance generalizability; and an agent-agnostic action representation abstracting a robot's kinematics to a universal agent proxy, emphasizing crucial interactions between end-effector and object.","Ag2Manip's empirical validation across simulated benchmarks like FrankaKitchen, ManiSkill, and PartManip shows a 325% increase in performance, achieved without domain-specific demonstrations.","Ablation studies underline the essential contributions of the visual and action representations to this success.","Extending our evaluations to the real world, Ag2Manip significantly improves imitation learning success rates from 50% to 77.5%, demonstrating its effectiveness and generalizability across both simulated and physical environments."],"url":"http://arxiv.org/abs/2404.17521v1","category":"cs.RO"}
{"created":"2024-04-26 16:26:11","title":"Bridging the Fairness Divide: Achieving Group and Individual Fairness in Graph Neural Networks","abstract":"Graph neural networks (GNNs) have emerged as a powerful tool for analyzing and learning from complex data structured as graphs, demonstrating remarkable effectiveness in various applications, such as social network analysis, recommendation systems, and drug discovery. However, despite their impressive performance, the fairness problem has increasingly gained attention as a crucial aspect to consider. Existing research in graph learning focuses on either group fairness or individual fairness. However, since each concept provides unique insights into fairness from distinct perspectives, integrating them into a fair graph neural network system is crucial. To the best of our knowledge, no study has yet to comprehensively tackle both individual and group fairness simultaneously. In this paper, we propose a new concept of individual fairness within groups and a novel framework named Fairness for Group and Individual (FairGI), which considers both group fairness and individual fairness within groups in the context of graph learning. FairGI employs the similarity matrix of individuals to achieve individual fairness within groups, while leveraging adversarial learning to address group fairness in terms of both Equal Opportunity and Statistical Parity. The experimental results demonstrate that our approach not only outperforms other state-of-the-art models in terms of group fairness and individual fairness within groups, but also exhibits excellent performance in population-level individual fairness, while maintaining comparable prediction accuracy.","sentences":["Graph neural networks (GNNs) have emerged as a powerful tool for analyzing and learning from complex data structured as graphs, demonstrating remarkable effectiveness in various applications, such as social network analysis, recommendation systems, and drug discovery.","However, despite their impressive performance, the fairness problem has increasingly gained attention as a crucial aspect to consider.","Existing research in graph learning focuses on either group fairness or individual fairness.","However, since each concept provides unique insights into fairness from distinct perspectives, integrating them into a fair graph neural network system is crucial.","To the best of our knowledge, no study has yet to comprehensively tackle both individual and group fairness simultaneously.","In this paper, we propose a new concept of individual fairness within groups and a novel framework named Fairness for Group and Individual (FairGI), which considers both group fairness and individual fairness within groups in the context of graph learning.","FairGI employs the similarity matrix of individuals to achieve individual fairness within groups, while leveraging adversarial learning to address group fairness in terms of both Equal Opportunity and Statistical Parity.","The experimental results demonstrate that our approach not only outperforms other state-of-the-art models in terms of group fairness and individual fairness within groups, but also exhibits excellent performance in population-level individual fairness, while maintaining comparable prediction accuracy."],"url":"http://arxiv.org/abs/2404.17511v1","category":"cs.LG"}
{"created":"2024-04-26 16:25:51","title":"Kerr Nonlinearity Induced Nonreciprocity in dissipatively coupled resonators","abstract":"Nonlinearity induced nonreciprocity is studied in a system comprising two resonators coupled to a one-dimensional waveguide when the linear system does not exhibit nonreciprocity. The analysis is based on the Hamiltonian of the coupled system and includes the dissipative coupling between the waveguide and resonators, along with the input-output relations. We consider a large number of scenarios which can lead to nonreciprocity. We pay special attention to the case when the linear system does not exhibit nonreciprocal behavior. In this case, we show how very significant nonreciprocal behavior can result from Kerr nonlinearities. We find that the bistability of the nonlinear system can aid in achieving large nonreciprocity. Additionally, We bring out nonreciprocity in the excitation of each resonator, which can be monitored independently. Our results highlight the profound influence of nonlinearity on nonreciprocal behavior, offering a new avenue for controlling light propagation in integrated photonic circuits. Nonlinearity induced nonreciprocity would lead to significant nonreciprocity in quantum fluctuations when our system is treated quantum mechanically.","sentences":["Nonlinearity induced nonreciprocity is studied in a system comprising two resonators coupled to a one-dimensional waveguide when the linear system does not exhibit nonreciprocity.","The analysis is based on the Hamiltonian of the coupled system and includes the dissipative coupling between the waveguide and resonators, along with the input-output relations.","We consider a large number of scenarios which can lead to nonreciprocity.","We pay special attention to the case when the linear system does not exhibit nonreciprocal behavior.","In this case, we show how very significant nonreciprocal behavior can result from Kerr nonlinearities.","We find that the bistability of the nonlinear system can aid in achieving large nonreciprocity.","Additionally, We bring out nonreciprocity in the excitation of each resonator, which can be monitored independently.","Our results highlight the profound influence of nonlinearity on nonreciprocal behavior, offering a new avenue for controlling light propagation in integrated photonic circuits.","Nonlinearity induced nonreciprocity would lead to significant nonreciprocity in quantum fluctuations when our system is treated quantum mechanically."],"url":"http://arxiv.org/abs/2404.17510v1","category":"physics.optics"}
{"created":"2024-04-26 16:20:04","title":"Constrained Neural Networks for Interpretable Heuristic Creation to Optimise Computer Algebra Systems","abstract":"We present a new methodology for utilising machine learning technology in symbolic computation research. We explain how a well known human-designed heuristic to make the choice of variable ordering in cylindrical algebraic decomposition may be represented as a constrained neural network. This allows us to then use machine learning methods to further optimise the heuristic, leading to new networks of similar size, representing new heuristics of similar complexity as the original human-designed one. We present this as a form of ante-hoc explainability for use in computer algebra development.","sentences":["We present a new methodology for utilising machine learning technology in symbolic computation research.","We explain how a well known human-designed heuristic to make the choice of variable ordering in cylindrical algebraic decomposition may be represented as a constrained neural network.","This allows us to then use machine learning methods to further optimise the heuristic, leading to new networks of similar size, representing new heuristics of similar complexity as the original human-designed one.","We present this as a form of ante-hoc explainability for use in computer algebra development."],"url":"http://arxiv.org/abs/2404.17508v1","category":"cs.SC"}
{"created":"2024-04-26 16:19:31","title":"Chemotaxis-inspired PDE model for airborne infectious disease transmission: analysis and simulations","abstract":"Partial differential equation (PDE) models for infectious disease have received renewed interest in recent years. Most models of this type extend classical compartmental formulations with additional terms accounting for spatial dynamics, with Fickian diffusion being the most common such term. However, while diffusion may be appropriate for modeling vector-borne diseases, or diseases among plants or wildlife, the spatial propagation of airborne diseases in human populations is heavily dependent on human contact and mobility patterns, which are not necessarily well-described by diffusion. By including an additional chemotaxis-inspired term, in which the infection is propagated along the positive gradient of the susceptible population (from regions of low- to high-density of susceptibles), one may provide a more suitable description of these dynamics. This article introduces and analyzes a mathematical model of infectious disease incorporating a modified chemotaxis-type term. The model is analyzed mathematically and the well-posedness of the resulting PDE system is demonstrated. A series of numerical simulations are provided, demonstrating the ability of the model to naturally capture important phenomena not easily observed in standard diffusion models, including propagation over long spatial distances over short time scales and the emergence of localized infection hotspots","sentences":["Partial differential equation (PDE) models for infectious disease have received renewed interest in recent years.","Most models of this type extend classical compartmental formulations with additional terms accounting for spatial dynamics, with Fickian diffusion being the most common such term.","However, while diffusion may be appropriate for modeling vector-borne diseases, or diseases among plants or wildlife, the spatial propagation of airborne diseases in human populations is heavily dependent on human contact and mobility patterns, which are not necessarily well-described by diffusion.","By including an additional chemotaxis-inspired term, in which the infection is propagated along the positive gradient of the susceptible population (from regions of low- to high-density of susceptibles), one may provide a more suitable description of these dynamics.","This article introduces and analyzes a mathematical model of infectious disease incorporating a modified chemotaxis-type term.","The model is analyzed mathematically and the well-posedness of the resulting PDE system is demonstrated.","A series of numerical simulations are provided, demonstrating the ability of the model to naturally capture important phenomena not easily observed in standard diffusion models, including propagation over long spatial distances over short time scales and the emergence of localized infection hotspots"],"url":"http://arxiv.org/abs/2404.17506v1","category":"math.AP"}
{"created":"2024-04-26 16:15:56","title":"Detailed dynamics of a moving magnetic skyrmion lattice in MnSi observed using a small-angle neutron scattering under an alternating electric current flow","abstract":"Lattice formation of swirling textures is ubiquitous in solid-state materials, such as a magnetic skyrmion lattice in chiral magnets. In the magnetic skyrmion lattices, their moving states and dynamics under external perturbations are still unrevealed, although a detailed understanding of the dynamics is crucial to realizing spintronic applications, such as magnetic domain-wall racetrack memory. Here, we report in detail on the transient state of a moving magnetic skyrmion lattice in bulk single-crystalline MnSi under alternating current (AC) using small-angle neutron scattering. A rotation and concomitant broadening of the spot width in the azimuthal direction of the magnetic skyrmion reflections originating from the plastic deformation of the magnetic skyrmion lattice were found only at low AC frequencies, whereas above the threshold AC frequency (ft ~ 0.12 Hz) the rotation was not observed, and the spot width becomes sharper. The observed complex response of the magnetic skyrmion reflections can be explained by the change in dislocation density in the magnetic skyrmion lattice. At frequencies higher than ft, the magnetic skyrmions oscillate removing the dislocations, indicating that the dislocation density is controlled by the AC frequency.","sentences":["Lattice formation of swirling textures is ubiquitous in solid-state materials, such as a magnetic skyrmion lattice in chiral magnets.","In the magnetic skyrmion lattices, their moving states and dynamics under external perturbations are still unrevealed, although a detailed understanding of the dynamics is crucial to realizing spintronic applications, such as magnetic domain-wall racetrack memory.","Here, we report in detail on the transient state of a moving magnetic skyrmion lattice in bulk single-crystalline MnSi under alternating current (AC) using small-angle neutron scattering.","A rotation and concomitant broadening of the spot width in the azimuthal direction of the magnetic skyrmion reflections originating from the plastic deformation of the magnetic skyrmion lattice were found only at low AC frequencies, whereas above the threshold AC frequency (ft ~ 0.12 Hz) the rotation was not observed, and the spot width becomes sharper.","The observed complex response of the magnetic skyrmion reflections can be explained by the change in dislocation density in the magnetic skyrmion lattice.","At frequencies higher than ft, the magnetic skyrmions oscillate removing the dislocations, indicating that the dislocation density is controlled by the AC frequency."],"url":"http://arxiv.org/abs/2404.17505v1","category":"cond-mat.str-el"}
{"created":"2024-04-26 16:02:44","title":"Internal Pattern Matching in Small Space and Applications","abstract":"In this work, we consider pattern matching variants in small space, that is, in the read-only setting, where we want to bound the space usage on top of storing the strings. Our main contribution is a space-time trade-off for the Internal Pattern Matching (IPM) problem, where the goal is to construct a data structure over a string $S$ of length $n$ that allows one to answer the following type of queries: Compute the occurrences of a fragment $P$ of $S$ inside another fragment $T$ of $S$, provided that $|T| < 2|P|$. For any $\\tau \\in [1 .. n/\\log^2 n]$, we present a nearly-optimal $\\~O(n/\\tau)$-size data structure that can be built in $\\~O(n)$ time using $\\~O(n/\\tau)$ extra space, and answers IPM queries in $O(\\tau+\\log n \\log^3 \\log n)$ time. IPM queries have been identified as a crucial primitive operation for the analysis of algorithms on strings. In particular, the complexities of several recent algorithms for approximate pattern matching are expressed with regards to the number of calls to a small set of primitive operations that include IPM queries; our data structure allows us to port these results to the small-space setting. We further showcase the applicability of our IPM data structure by using it to obtain space-time trade-offs for the longest common substring and circular pattern matching problems in the asymmetric streaming setting.","sentences":["In this work, we consider pattern matching variants in small space, that is, in the read-only setting, where we want to bound the space usage on top of storing the strings.","Our main contribution is a space-time trade-off for the Internal Pattern Matching (IPM) problem, where the goal is to construct a data structure over a string $S$ of length $n$ that allows one to answer the following type of queries: Compute the occurrences of a fragment $P$ of $S$ inside another fragment $T$ of $S$, provided that $|T| < 2|P|$. For any $\\tau \\in [1 .. n/\\log^2 n]$, we present a nearly-optimal $\\~O(n/\\tau)$-size data structure that can be built in $\\~O(n)$ time using $\\~O(n/\\tau)$ extra space, and answers IPM queries in $O(\\tau+\\log n \\log^3 \\log n)$ time.","IPM queries have been identified as a crucial primitive operation for the analysis of algorithms on strings.","In particular, the complexities of several recent algorithms for approximate pattern matching are expressed with regards to the number of calls to a small set of primitive operations that include IPM queries; our data structure allows us to port these results to the small-space setting.","We further showcase the applicability of our IPM data structure by using it to obtain space-time trade-offs for the longest common substring and circular pattern matching problems in the asymmetric streaming setting."],"url":"http://arxiv.org/abs/2404.17502v1","category":"cs.DS"}
{"created":"2024-04-26 16:00:08","title":"Relations between normal state nonreciprocal transport and the superconducting diode effect in the trivial and topological phases","abstract":"Nonreciprocal transport effects can occur in the normal state of conductors and in superconductors when both inversion and time-reversal symmetry are broken. Here, we consider systems where magnetochiral anisotropy (MCA) of the energy spectrum due to an externally applied magnetic field results in a rectification effect in the normal state and a superconducting (SC) diode effect when the system is proximitised by a superconductor. Focussing on nanowire systems, we obtain analytic expressions for both normal state rectification and SC diode effects that reveal the commonalities - as well as differences - between these two phenomena. Furthermore, we consider the nanowire brought into an (almost) helical state in the normal phase or a topological superconducting phase when proximitised. In both cases this reveals that the topology of the system considerably modifies its nonreciprocal transport properties. Our results provide new insights into how to determine the origin of nonreciprocal effects and further evince the strong connection of nonreciprocal transport with the topological properties of a system.","sentences":["Nonreciprocal transport effects can occur in the normal state of conductors and in superconductors when both inversion and time-reversal symmetry are broken.","Here, we consider systems where magnetochiral anisotropy (MCA) of the energy spectrum due to an externally applied magnetic field results in a rectification effect in the normal state and a superconducting (SC) diode effect when the system is proximitised by a superconductor.","Focussing on nanowire systems, we obtain analytic expressions for both normal state rectification and SC diode effects that reveal the commonalities - as well as differences - between these two phenomena.","Furthermore, we consider the nanowire brought into an (almost) helical state in the normal phase or a topological superconducting phase when proximitised.","In both cases this reveals that the topology of the system considerably modifies its nonreciprocal transport properties.","Our results provide new insights into how to determine the origin of nonreciprocal effects and further evince the strong connection of nonreciprocal transport with the topological properties of a system."],"url":"http://arxiv.org/abs/2404.17501v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-26 15:57:06","title":"Quantum Multi-Agent Reinforcement Learning for Aerial Ad-hoc Networks","abstract":"Quantum machine learning (QML) as combination of quantum computing with machine learning (ML) is a promising direction to explore, in particular due to the advances in realizing quantum computers and the hoped-for quantum advantage. A field within QML that is only little approached is quantum multi-agent reinforcement learning (QMARL), despite having shown to be potentially attractive for addressing industrial applications such as factory management, cellular access and mobility cooperation. This paper presents an aerial communication use case and introduces a hybrid quantum-classical (HQC) ML algorithm to solve it. This use case intends to increase the connectivity of flying ad-hoc networks and is solved by an HQC multi-agent proximal policy optimization algorithm in which the core of the centralized critic is replaced with a data reuploading variational quantum circuit. Results show a slight increase in performance for the quantum-enhanced solution with respect to a comparable classical algorithm, earlier reaching convergence, as well as the scalability of such a solution: an increase in the size of the ansatz, and thus also in the number of trainable parameters, leading to better outcomes. These promising results show the potential of QMARL to industrially-relevant complex use cases.","sentences":["Quantum machine learning (QML) as combination of quantum computing with machine learning (ML) is a promising direction to explore, in particular due to the advances in realizing quantum computers and the hoped-for quantum advantage.","A field within QML that is only little approached is quantum multi-agent reinforcement learning (QMARL), despite having shown to be potentially attractive for addressing industrial applications such as factory management, cellular access and mobility cooperation.","This paper presents an aerial communication use case and introduces a hybrid quantum-classical (HQC) ML algorithm to solve it.","This use case intends to increase the connectivity of flying ad-hoc networks and is solved by an HQC multi-agent proximal policy optimization algorithm in which the core of the centralized critic is replaced with a data reuploading variational quantum circuit.","Results show a slight increase in performance for the quantum-enhanced solution with respect to a comparable classical algorithm, earlier reaching convergence, as well as the scalability of such a solution: an increase in the size of the ansatz, and thus also in the number of trainable parameters, leading to better outcomes.","These promising results show the potential of QMARL to industrially-relevant complex use cases."],"url":"http://arxiv.org/abs/2404.17499v1","category":"quant-ph"}
{"created":"2024-04-26 15:46:45","title":"Regular Expressions with Backreferences and Lookaheads Capture NLOG","abstract":"Backreferences and lookaheads are vital features to make classical regular expressions (REGEX) practical. Although these features have been widely used, understanding of the unrestricted combination of them has been limited. Practically, most likely no implementation fully supports them. Theoretically, while some studies have addressed these features separately, few have dared to combine them. In those few studies, it has been made clear that the amalgamation of these features renders REGEX significantly expressive. However, no acceptable expressivity bound for REWBLk$\\unicode{x2014}$REGEX with backreferences and lookaheads$\\unicode{x2014}$has been established.   We elucidate this by establishing that REWBLk coincides with NLOG, the class of languages accepted by log-space nondeterministic Turing machines (NTMs). In translating REWBLk to log-space NTMs, negative lookaheads are the most challenging part since it essentially requires complementing log-space NTMs in nondeterministic log-space. To address this problem, we revisit Immerman$\\unicode{x2013}$Szelepcs\\'enyi theorem. In addition, we employ log-space nested-oracles NTMs to naturally handle nested lookaheads of REWBLk. Utilizing such oracle machines, we also present the new result that the membership problem of REWBLk is PSPACE-complete.","sentences":["Backreferences and lookaheads are vital features to make classical regular expressions (REGEX) practical.","Although these features have been widely used, understanding of the unrestricted combination of them has been limited.","Practically, most likely no implementation fully supports them.","Theoretically, while some studies have addressed these features separately, few have dared to combine them.","In those few studies, it has been made clear that the amalgamation of these features renders REGEX significantly expressive.","However, no acceptable expressivity bound for REWBLk$\\unicode{x2014}$REGEX with backreferences and lookaheads$\\unicode{x2014}$has been established.   ","We elucidate this by establishing that REWBLk coincides with NLOG, the class of languages accepted by log-space nondeterministic Turing machines (NTMs).","In translating REWBLk to log-space NTMs, negative lookaheads are the most challenging part since it essentially requires complementing log-space NTMs in nondeterministic log-space.","To address this problem, we revisit Immerman$\\unicode{x2013}$Szelepcs\\'enyi theorem.","In addition, we employ log-space nested-oracles NTMs to naturally handle nested lookaheads of REWBLk.","Utilizing such oracle machines, we also present the new result that the membership problem of REWBLk is PSPACE-complete."],"url":"http://arxiv.org/abs/2404.17492v1","category":"cs.FL"}
{"created":"2024-04-26 15:40:17","title":"A Survey on Industrial Internet of Things (IIoT) Testbeds for Connectivity Research","abstract":"Industrial Internet of Things (IIoT) technologies have revolutionized industrial processes, enabling smart automation, real-time data analytics, and improved operational efficiency across diverse industry sectors. IIoT testbeds play a critical role in advancing IIoT research and development (R&D) to provide controlled environments for technology evaluation before their real-world deployment. In this article, we conduct a comprehensive literature review on existing IIoT testbeds, aiming to identify benchmark performance, research gaps and explore emerging trends in IIoT systems. We first review the state-of-the-art resource management solutions proposed for IIoT applications. We then categorize the reviewed testbeds according to their deployed communication protocols (including TSN, IEEE 802.15.4, IEEE 802.11 and 5G) and discuss the design and usage of each testbed. Driven by the knowledge gained during this study, we present suggestions and good practices for researchers and practitioners who are planning to design and develop IIoT testbeds for connectivity research.","sentences":["Industrial Internet of Things (IIoT) technologies have revolutionized industrial processes, enabling smart automation, real-time data analytics, and improved operational efficiency across diverse industry sectors.","IIoT testbeds play a critical role in advancing IIoT research and development (R&D) to provide controlled environments for technology evaluation before their real-world deployment.","In this article, we conduct a comprehensive literature review on existing IIoT testbeds, aiming to identify benchmark performance, research gaps and explore emerging trends in IIoT systems.","We first review the state-of-the-art resource management solutions proposed for IIoT applications.","We then categorize the reviewed testbeds according to their deployed communication protocols (including TSN, IEEE 802.15.4, IEEE 802.11 and 5G) and discuss the design and usage of each testbed.","Driven by the knowledge gained during this study, we present suggestions and good practices for researchers and practitioners who are planning to design and develop IIoT testbeds for connectivity research."],"url":"http://arxiv.org/abs/2404.17485v1","category":"cs.NI"}
{"created":"2024-04-26 15:26:43","title":"A multi-agent model of hierarchical decision dynamics","abstract":"Decision making can be difficult when there are many actors (or agents) who may be coordinating or competing to achieve their various ideas of the optimum outcome. Here I present a simple decision making model with an explicitly hierarchical binary-tree structure, and evaluate how this might cooperate to take actions that match its various evaluations of the uncertain state of the world. Key features of agent behaviour are (a) the separation of its decision making process into three distinct steps: observation, judgement, and action; and (b) the evolution of coordination by the sharing of judgements.","sentences":["Decision making can be difficult when there are many actors (or agents) who may be coordinating or competing to achieve their various ideas of the optimum outcome.","Here I present a simple decision making model with an explicitly hierarchical binary-tree structure, and evaluate how this might cooperate to take actions that match its various evaluations of the uncertain state of the world.","Key features of agent behaviour are (a) the separation of its decision making process into three distinct steps: observation, judgement, and action; and (b) the evolution of coordination by the sharing of judgements."],"url":"http://arxiv.org/abs/2404.17477v1","category":"cs.MA"}
{"created":"2024-04-26 15:22:45","title":"Establishing best practices for modeling long duration energy storage in deeply decarbonized energy systems","abstract":"Long duration energy storage (LDES) may become a critical technology for the decarbonization of the power sector, as current commercially available Li-ion battery storage technologies cannot cost-effectively shift energy to address multi-day or seasonal variability in demand and renewable energy availability. LDES is difficult to model in existing energy system planning models (such as electricity system capacity expansion models), as it is much more dependent on an accurate representation of chronology than other resources. Techniques exist for modeling LDES in these planning models; however, it is not known how spatial and temporal resolution affect the performance of these techniques, creating a research gap. In this study we examine what spatial and temporal resolution is necessarily to accurately capture the full value of LDES, in the context of a continent-scale capacity expansion model. We use the results to draw conclusions and present best practices for modelers seeking to accurately model LDES in a macro-energy systems planning context. Our key findings are: 1) modeling LDES with linked representative periods is crucial to capturing its full value, 2) LDES value is highly sensitive to the cost and availability of other resources, and 3) temporal resolution is more important than spatial resolution for capturing the full value of LDES, although how much temporal resolution is needed will depend on the specific model context.","sentences":["Long duration energy storage (LDES) may become a critical technology for the decarbonization of the power sector, as current commercially available Li-ion battery storage technologies cannot cost-effectively shift energy to address multi-day or seasonal variability in demand and renewable energy availability.","LDES is difficult to model in existing energy system planning models (such as electricity system capacity expansion models), as it is much more dependent on an accurate representation of chronology than other resources.","Techniques exist for modeling LDES in these planning models; however, it is not known how spatial and temporal resolution affect the performance of these techniques, creating a research gap.","In this study we examine what spatial and temporal resolution is necessarily to accurately capture the full value of LDES, in the context of a continent-scale capacity expansion model.","We use the results to draw conclusions and present best practices for modelers seeking to accurately model LDES in a macro-energy systems planning context.","Our key findings are: 1) modeling LDES with linked representative periods is crucial to capturing its full value, 2) LDES value is highly sensitive to the cost and availability of other resources, and 3) temporal resolution is more important than spatial resolution for capturing the full value of LDES, although how much temporal resolution is needed will depend on the specific model context."],"url":"http://arxiv.org/abs/2404.17474v1","category":"eess.SY"}
{"created":"2024-04-26 15:15:25","title":"Consistent Second Moment Methods with Scalable Linear Solvers for Radiation Transport","abstract":"Second Moment Methods (SMMs) are developed that are consistent with the Discontinuous Galerkin (DG) spatial discretization of the discrete ordinates (or \\Sn) transport equations. The low-order (LO) diffusion system of equations is discretized with fully consistent \\Pone, Local Discontinuous Galerkin (LDG), and Interior Penalty (IP) methods. A discrete residual approach is used to derive SMM correction terms that make each of the LO systems consistent with the high-order (HO) discretization. We show that the consistent methods are more accurate and have better solution quality than independently discretized LO systems, that they preserve the diffusion limit, and that the LDG and IP consistent SMMs can be scalably solved in parallel on a challenging, multi-material benchmark problem.","sentences":["Second Moment Methods (SMMs) are developed that are consistent with the Discontinuous Galerkin (DG) spatial discretization of the discrete ordinates (or \\Sn) transport equations.","The low-order (LO) diffusion system of equations is discretized with fully consistent \\Pone, Local Discontinuous Galerkin (LDG), and Interior Penalty (IP) methods.","A discrete residual approach is used to derive SMM correction terms that make each of the LO systems consistent with the high-order (HO) discretization.","We show that the consistent methods are more accurate and have better solution quality than independently discretized LO systems, that they preserve the diffusion limit, and that the LDG and IP consistent SMMs can be scalably solved in parallel on a challenging, multi-material benchmark problem."],"url":"http://arxiv.org/abs/2404.17473v1","category":"math.NA"}
{"created":"2024-04-26 15:11:35","title":"MIMO in network simulators: Design, implementation and evaluation of single-user MIMO in ns-3 5G-LENA","abstract":"MIMO technology has been studied in textbooks for several decades, and it has been adopted in 4G and 5G systems. Due to the recent evolution in 5G and beyond networks, designed to cover a wide range of use cases with every time more complex applications, it is essential to have network simulation tools (such as ns-3) to evaluate MIMO performance from the network perspective, before real implementation. Up to date, the well-known ns-3 simulator has been missing the inclusion of single-user MIMO (SU-MIMO) models for 5G. In this paper, we detail the implementation models and provide an exhaustive evaluation of SU-MIMO in the 5G-LENA module of ns-3. As per 3GPP 5G, we adopt a hybrid beamforming architecture and a closed-loop MIMO mechanism and follow all 3GPP specifications for MIMO implementation, including channel state information feedback with precoding matrix indicator and rank indicator reports, and codebook-based precoding following Precoding Type-I (used for SU-MIMO). The simulation models are released in open-source and currently support up to 32 antenna ports and 4 streams per user. The simulation results presented in this paper help in testing and verifying the simulated models, for different multi-antenna array and antenna ports configurations.","sentences":["MIMO technology has been studied in textbooks for several decades, and it has been adopted in 4G and 5G systems.","Due to the recent evolution in 5G and beyond networks, designed to cover a wide range of use cases with every time more complex applications, it is essential to have network simulation tools (such as ns-3) to evaluate MIMO performance from the network perspective, before real implementation.","Up to date, the well-known ns-3 simulator has been missing the inclusion of single-user MIMO (SU-MIMO) models for 5G.","In this paper, we detail the implementation models and provide an exhaustive evaluation of SU-MIMO in the 5G-LENA module of ns-3.","As per 3GPP 5G, we adopt a hybrid beamforming architecture and a closed-loop MIMO mechanism and follow all 3GPP specifications for MIMO implementation, including channel state information feedback with precoding matrix indicator and rank indicator reports, and codebook-based precoding following Precoding Type-I (used for SU-MIMO).","The simulation models are released in open-source and currently support up to 32 antenna ports and 4 streams per user.","The simulation results presented in this paper help in testing and verifying the simulated models, for different multi-antenna array and antenna ports configurations."],"url":"http://arxiv.org/abs/2404.17472v1","category":"cs.NI"}
{"created":"2024-04-26 15:08:57","title":"FTL: Transfer Learning Nonlinear Plasma Dynamic Transitions in Low Dimensional Embeddings via Deep Neural Networks","abstract":"Deep learning algorithms provide a new paradigm to study high-dimensional dynamical behaviors, such as those in fusion plasma systems. Development of novel model reduction methods, coupled with detection of abnormal modes with plasma physics, opens a unique opportunity for building efficient models to identify plasma instabilities for real-time control. Our Fusion Transfer Learning (FTL) model demonstrates success in reconstructing nonlinear kink mode structures by learning from a limited amount of nonlinear simulation data. The knowledge transfer process leverages a pre-trained neural encoder-decoder network, initially trained on linear simulations, to effectively capture nonlinear dynamics. The low-dimensional embeddings extract the coherent structures of interest, while preserving the inherent dynamics of the complex system. Experimental results highlight FTL's capacity to capture transitional behaviors and dynamical features in plasma dynamics -- a task often challenging for conventional methods. The model developed in this study is generalizable and can be extended broadly through transfer learning to address various magnetohydrodynamics (MHD) modes.","sentences":["Deep learning algorithms provide a new paradigm to study high-dimensional dynamical behaviors, such as those in fusion plasma systems.","Development of novel model reduction methods, coupled with detection of abnormal modes with plasma physics, opens a unique opportunity for building efficient models to identify plasma instabilities for real-time control.","Our Fusion Transfer Learning (FTL) model demonstrates success in reconstructing nonlinear kink mode structures by learning from a limited amount of nonlinear simulation data.","The knowledge transfer process leverages a pre-trained neural encoder-decoder network, initially trained on linear simulations, to effectively capture nonlinear dynamics.","The low-dimensional embeddings extract the coherent structures of interest, while preserving the inherent dynamics of the complex system.","Experimental results highlight FTL's capacity to capture transitional behaviors and dynamical features in plasma dynamics -- a task often challenging for conventional methods.","The model developed in this study is generalizable and can be extended broadly through transfer learning to address various magnetohydrodynamics (MHD) modes."],"url":"http://arxiv.org/abs/2404.17466v1","category":"physics.comp-ph"}
{"created":"2024-04-26 14:57:55","title":"Ruffle&Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System","abstract":"Conversational tutoring systems (CTSs) offer learning experiences through interactions based on natural language. They are recognized for promoting cognitive engagement and improving learning outcomes, especially in reasoning tasks. Nonetheless, the cost associated with authoring CTS content is a major obstacle to widespread adoption and to research on effective instructional design. In this paper, we discuss and evaluate a novel type of CTS that leverages recent advances in large language models (LLMs) in two ways: First, the system enables AI-assisted content authoring by inducing an easily editable tutoring script automatically from a lesson text. Second, the system automates the script orchestration in a learning-by-teaching format via two LLM-based agents (Ruffle&Riley) acting as a student and a professor. The system allows for free-form conversations that follow the ITS-typical inner and outer loop structure. We evaluate Ruffle&Riley's ability to support biology lessons in two between-subject online user studies (N = 200) comparing the system to simpler QA chatbots and reading activity. Analyzing system usage patterns, pre/post-test scores and user experience surveys, we find that Ruffle&Riley users report high levels of engagement, understanding and perceive the offered support as helpful. Even though Ruffle&Riley users require more time to complete the activity, we did not find significant differences in short-term learning gains over the reading activity. Our system architecture and user study provide various insights for designers of future CTSs. We further open-source our system to support ongoing research on effective instructional design of LLM-based learning technologies.","sentences":["Conversational tutoring systems (CTSs) offer learning experiences through interactions based on natural language.","They are recognized for promoting cognitive engagement and improving learning outcomes, especially in reasoning tasks.","Nonetheless, the cost associated with authoring CTS content is a major obstacle to widespread adoption and to research on effective instructional design.","In this paper, we discuss and evaluate a novel type of CTS that leverages recent advances in large language models (LLMs) in two ways: First, the system enables AI-assisted content authoring by inducing an easily editable tutoring script automatically from a lesson text.","Second, the system automates the script orchestration in a learning-by-teaching format via two LLM-based agents (Ruffle&Riley) acting as a student and a professor.","The system allows for free-form conversations that follow the ITS-typical inner and outer loop structure.","We evaluate Ruffle&Riley's ability to support biology lessons in two between-subject online user studies (N = 200) comparing the system to simpler QA chatbots and reading activity.","Analyzing system usage patterns, pre/post-test scores and user experience surveys, we find that Ruffle&Riley users report high levels of engagement, understanding and perceive the offered support as helpful.","Even though Ruffle&Riley users require more time to complete the activity, we did not find significant differences in short-term learning gains over the reading activity.","Our system architecture and user study provide various insights for designers of future CTSs.","We further open-source our system to support ongoing research on effective instructional design of LLM-based learning technologies."],"url":"http://arxiv.org/abs/2404.17460v1","category":"cs.CL"}
{"created":"2024-04-26 14:56:49","title":"Quasi particle model vs lattice QCD thermodynamics: extension to $N_f=2+1+1$ flavors and momentum dependent quark masses","abstract":"In the last decade a Quasi-Particle Model ($QPM$) has supplied the basis for the study of HQ production in ultra-relativistic AA collisions, allowing for a phenomenological estimate of the HQ diffusion coefficient $D_s(T)$. Taking advantage of the new lattice QCD results for the Equation of State (EoS) with 2+1+1 dynamical flavors, we extend our $QPM$ approach from $N_f=2+1$ to $N_f=2+1+1$, in which the charm quark is included. Given an effective coupling $g(T)$ fixed by a fit to the lQCD energy density $\\epsilon(T)$, we evaluate the impact of different temperature parametrizations of charm quark mass on EoS and susceptibilities $\\chi_q(T)$ of light, $\\chi_s(T)$ of strange and $\\chi_c(T)$ of charm quarks, the last favouring a charm quark mass increasing toward $T_c$. We also explore the extension of the $QPM$ approach to a more realistic approach, that we label $QPM_p$, in which quark and gluon masses explicitly depend on their momentum converging to the current quark mass at high momenta, as expected from asymptotic free dynamics. The $QPM_p$ is seen to allow for a simultaneous quantitative description not only of the EoS but also of the quark susceptibilities ($\\chi_q(T)$, $\\chi_s(T)$), which instead are underestimated in the simple $QPM$ model. Furthermore, evaluating the spatial diffusion coefficient $2\\pi T D_s(T)$ in the $QPM_p$, we find it is also significantly closer than $QPM$ to the recent lQCD data performed including dynamical fermions. Finally, in a 1+1D expanding system, we evaluate the $R_{AA}(p_T)$ in the $QPM$ and $QPM_p$, finding a significant reduction at low momenta for $QPM_p$ which could lead in a realistic scenario to a better agreement to experimental data.","sentences":["In the last decade a Quasi-Particle Model ($QPM$) has supplied the basis for the study of HQ production in ultra-relativistic AA collisions, allowing for a phenomenological estimate of the HQ diffusion coefficient $D_s(T)$. Taking advantage of the new lattice QCD results for the Equation of State (EoS) with 2+1+1 dynamical flavors, we extend our $QPM$ approach from $N_f=2+1$ to $N_f=2+1+1$, in which the charm quark is included.","Given an effective coupling $g(T)$ fixed by a fit to the lQCD energy density $\\epsilon(T)$, we evaluate the impact of different temperature parametrizations of charm quark mass on EoS and susceptibilities $\\chi_q(T)$ of light, $\\chi_s(T)$ of strange and $\\chi_c(T)$ of charm quarks, the last favouring a charm quark mass increasing toward $T_c$. We also explore the extension of the $QPM$ approach to a more realistic approach, that we label $QPM_p$, in which quark and gluon masses explicitly depend on their momentum converging to the current quark mass at high momenta, as expected from asymptotic free dynamics.","The $QPM_p$ is seen to allow for a simultaneous quantitative description not only of the EoS but also of the quark susceptibilities ($\\chi_q(T)$, $\\chi_s(T)$), which instead are underestimated in the simple $QPM$ model.","Furthermore, evaluating the spatial diffusion coefficient $2\\pi T D_s(T)$ in the $QPM_p$, we find it is also significantly closer than $QPM$ to the recent lQCD data performed including dynamical fermions.","Finally, in a 1+1D expanding system, we evaluate the $R_{AA}(p_T)$ in the $QPM$ and $QPM_p$, finding a significant reduction at low momenta for $QPM_p$ which could lead in a realistic scenario to a better agreement to experimental data."],"url":"http://arxiv.org/abs/2404.17459v1","category":"hep-ph"}
{"created":"2024-04-26 14:55:39","title":"Pullback of symplectic forms to the space of circle patterns","abstract":"We consider circle patterns on surfaces with complex projective structures. We investigate two symplectic forms pulled back to the deformation space of circle patterns. The first one is Goldman's symplectic form on the space of complex projective structures on closed surfaces. The other is the Weil-Petersson symplectic form on the Teichm\\\"uller space of punctured surfaces. We show that their pullbacks to the space of circle patterns coincide. It is applied to prove the smoothness of the deformation space, which is an essential step to the conjecture that the space of circle patterns is homeomorphic to the Teichm\\\"uller space of the closed surface. We further conjecture that the pullback of the symplectic forms is non-degenerate and defines a symplectic structure on the space of circle patterns.","sentences":["We consider circle patterns on surfaces with complex projective structures.","We investigate two symplectic forms pulled back to the deformation space of circle patterns.","The first one is Goldman's symplectic form on the space of complex projective structures on closed surfaces.","The other is the Weil-Petersson symplectic form on the Teichm\\\"uller space of punctured surfaces.","We show that their pullbacks to the space of circle patterns coincide.","It is applied to prove the smoothness of the deformation space, which is an essential step to the conjecture that the space of circle patterns is homeomorphic to the Teichm\\\"uller space of the closed surface.","We further conjecture that the pullback of the symplectic forms is non-degenerate and defines a symplectic structure on the space of circle patterns."],"url":"http://arxiv.org/abs/2404.17458v1","category":"math.GT"}
{"created":"2024-04-26 14:53:22","title":"Vaporization dynamics of a super-heated water-in-oil droplet: modeling and numerical solution","abstract":"The study of vapor bubble growth following droplet vaporization in a superheated liquid involves research areas such as hydrodynamics, heat transfer, mass transfer, and thermodynamics. The interplay between these multiscale aspects is strongly dependent on the geometry, the thermodynamic response, and the local physical properties of the system. To understand the role of each aspect of this complex mechanism we model super-heated droplet vaporization by coupling the equation of motion for bubble growth with the thermodynamics of phase change and heat transfer through the convection-diffusion equation. The semi-analytical model is validated with the analytical description for vapor bubble growth dominated either by inertia (Rayleigh) or by thermal diffusion (Plesset-Zwick), depending on droplet radius and degree of superheat. The effect of a mismatch of the thermal properties between the host liquid and the droplet is shown to be relevant only for low superheating, above which an increase in thermal diffusivity leads to a reduction in the rate of vaporization. At medium to high superheating, the droplet vaporizes completely without relying on thermal diffusion. At the point of complete vaporization, the potential energy within the system drives the bubble overshoots, which vary based on the droplet size and degree of superheat.","sentences":["The study of vapor bubble growth following droplet vaporization in a superheated liquid involves research areas such as hydrodynamics, heat transfer, mass transfer, and thermodynamics.","The interplay between these multiscale aspects is strongly dependent on the geometry, the thermodynamic response, and the local physical properties of the system.","To understand the role of each aspect of this complex mechanism we model super-heated droplet vaporization by coupling the equation of motion for bubble growth with the thermodynamics of phase change and heat transfer through the convection-diffusion equation.","The semi-analytical model is validated with the analytical description for vapor bubble growth dominated either by inertia (Rayleigh) or by thermal diffusion (Plesset-Zwick), depending on droplet radius and degree of superheat.","The effect of a mismatch of the thermal properties between the host liquid and the droplet is shown to be relevant only for low superheating, above which an increase in thermal diffusivity leads to a reduction in the rate of vaporization.","At medium to high superheating, the droplet vaporizes completely without relying on thermal diffusion.","At the point of complete vaporization, the potential energy within the system drives the bubble overshoots, which vary based on the droplet size and degree of superheat."],"url":"http://arxiv.org/abs/2404.17457v1","category":"physics.flu-dyn"}
{"created":"2024-04-26 14:49:17","title":"Averaged observations and turnpike phenomenon for parameter-dependent systems","abstract":"Our main contribution in this article is the achievement of the turnpike property in its integral and exponential forms for parameter-dependent systems with averaged observations in the cost functional. Namely, under suitable assumptions with respect to the matrices that defined the dynamics and the cost functional, we prove that the optimal control and state for the evolutionary problem converge in average to the optimal pair of an associated stationary problem. Moreover, we characterize the closeness between these two optimal solutions, proving that over a large time interval, they are exponentially close.","sentences":["Our main contribution in this article is the achievement of the turnpike property in its integral and exponential forms for parameter-dependent systems with averaged observations in the cost functional.","Namely, under suitable assumptions with respect to the matrices that defined the dynamics and the cost functional, we prove that the optimal control and state for the evolutionary problem converge in average to the optimal pair of an associated stationary problem.","Moreover, we characterize the closeness between these two optimal solutions, proving that over a large time interval, they are exponentially close."],"url":"http://arxiv.org/abs/2404.17455v1","category":"math.OC"}
{"created":"2024-04-26 14:34:05","title":"Spiral flow of quantum quartic oscillator with energy cutoff","abstract":"Theory of the quantum quartic oscillator is developed with close attention to the energy cutoff one needs to impose on the system in order to approximate the smallest eigenvalues and corresponding eigenstates of its Hamiltonian by diagonalizing matrices of limited size. The matrices are obtained by evaluating matrix elements of the Hamiltonian between the associated harmonic-oscillator eigenstates and by correcting the computed matrices to compensate for their limited dimension, using the Wilsonian renormalization-group procedure. The cutoff dependence of the corrected matrices is found to be described by a spiral motion of a three-dimensional vector. This behavior is shown to result from a combination of a limit-cycle and a floating fixed-point behaviors, a distinct feature of the foundational quantum system that warrants further study. A brief discussion of the research directions concerning renormalization of polynomial interactions of degree higher than four, spontaneous symmetry breaking and coupling of more than one oscillator through the near neighbor couplings known in condensed matter and quantum field theory, is included.","sentences":["Theory of the quantum quartic oscillator is developed with close attention to the energy cutoff one needs to impose on the system in order to approximate the smallest eigenvalues and corresponding eigenstates of its Hamiltonian by diagonalizing matrices of limited size.","The matrices are obtained by evaluating matrix elements of the Hamiltonian between the associated harmonic-oscillator eigenstates and by correcting the computed matrices to compensate for their limited dimension, using the Wilsonian renormalization-group procedure.","The cutoff dependence of the corrected matrices is found to be described by a spiral motion of a three-dimensional vector.","This behavior is shown to result from a combination of a limit-cycle and a floating fixed-point behaviors, a distinct feature of the foundational quantum system that warrants further study.","A brief discussion of the research directions concerning renormalization of polynomial interactions of degree higher than four, spontaneous symmetry breaking and coupling of more than one oscillator through the near neighbor couplings known in condensed matter and quantum field theory, is included."],"url":"http://arxiv.org/abs/2404.17446v1","category":"quant-ph"}
{"created":"2024-04-26 14:14:34","title":"Shellability of Kohnert posets","abstract":"In this paper, we are concerned with identifying among the family of posets associated with Kohnert polynomials, those whose order complex has a certain combinatorial property. In particular, for numerous families of Kohnert polynomials, including key polynomials, we determine when the associated Kohnert posets are (EL-)shellable. Interestingly, under certain diagram restrictions, (EL-)shellability of a Kohnert poset is equivalent to multiplcity freeness of the associated Kohnert polynomial.","sentences":["In this paper, we are concerned with identifying among the family of posets associated with Kohnert polynomials, those whose order complex has a certain combinatorial property.","In particular, for numerous families of Kohnert polynomials, including key polynomials, we determine when the associated Kohnert posets are (EL-)shellable.","Interestingly, under certain diagram restrictions, (EL-)shellability of a Kohnert poset is equivalent to multiplcity freeness of the associated Kohnert polynomial."],"url":"http://arxiv.org/abs/2404.17432v1","category":"math.CO"}
{"created":"2024-04-26 13:53:33","title":"Ultrafast Optical Control of Exciton Diffusion in WSe$_2$/Graphene Heterostructures Revealed by Heterodyne Transient Grating Spectroscopy","abstract":"Using heterodyne transient grating spectroscopy, we observe a significant enhancement of exciton diffusion within a monolayer WSe$_2$ stacked on top of graphene. We further demonstrate that the diffusion dynamics can be optically tuned on the ultrafast time scale (i.e. a few picoseconds) by altering the photoexcited charge carrier density in graphene. The results reveal that, on a time scale of a few picoseconds, the effective diffusion constant in the WSe$_2$/graphene heterostructure is approximately 40 cm$^2 $/s, representing a substantial improvement over the 2 cm$^2 $/s typical for an isolated monolayer of WSe$_2$. The enhanced diffusion can be understood in terms of a transient screening of impurities, charge traps, and defect states in WSe$_2$ by photoexcited charge carriers in graphene. Furthermore, we observe that the diffusion within WSe$_2$ is affected by interlayer interactions, like charge transfer, exhibiting different dynamic states depending on the incident excitation fluence. These findings underscore the dynamic nature of screening and diffusion processes in heterostructures of 2D semiconductors and graphene and provide valuable insights for future applications of these systems in ultrafast optoelectronic devices.","sentences":["Using heterodyne transient grating spectroscopy, we observe a significant enhancement of exciton diffusion within a monolayer WSe$_2$ stacked on top of graphene.","We further demonstrate that the diffusion dynamics can be optically tuned on the ultrafast time scale (i.e. a few picoseconds) by altering the photoexcited charge carrier density in graphene.","The results reveal that, on a time scale of a few picoseconds, the effective diffusion constant in the WSe$_2$/graphene heterostructure is approximately 40 cm$^2 $/s, representing a substantial improvement over the 2 cm$^2 $/s typical for an isolated monolayer of WSe$_2$.","The enhanced diffusion can be understood in terms of a transient screening of impurities, charge traps, and defect states in WSe$_2$ by photoexcited charge carriers in graphene.","Furthermore, we observe that the diffusion within WSe$_2$ is affected by interlayer interactions, like charge transfer, exhibiting different dynamic states depending on the incident excitation fluence.","These findings underscore the dynamic nature of screening and diffusion processes in heterostructures of 2D semiconductors and graphene and provide valuable insights for future applications of these systems in ultrafast optoelectronic devices."],"url":"http://arxiv.org/abs/2404.17416v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-26 13:44:06","title":"Multifractal analysis of the power-2-decaying Gauss-like expansion","abstract":"Each real number $x\\in[0,1]$ admits a unique power-2-decaying Gauss-like expansion (P2GLE for short) as $x=\\sum_{i\\in\\mathbb{N}} 2^{-(d_1(x)+d_2(x)+\\cdots+d_i(x))}$, where $d_i(x)\\in\\mathbb{N}$. For any $x\\in(0,1]$, the Khintchine exponent $\\gamma(x)$ is defined by $\\gamma(x):=\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{j=1}^nd_j(x)$ if the limit exists. We investigate the sizes of the level sets $E(\\xi):=\\{x\\in(0,1]:\\gamma(x)=\\xi\\}$ for $\\xi\\geq 1$. Utilizing the Ruelle operator theory, we obtain the Khintchine spectrum $\\xi\\mapsto\\dim_H E(\\xi)$, where $\\dim_H$ denotes the Hausdorff dimension. We establish the remarkable fact that the Khintchine spectrum has exactly one inflection point, which was never proved for the corresponding spectrum in continued fractions. As a direct consequence, we also obtain the Lyapunov spectrum. Furthermore, we find the Hausdorff dimensions of the level sets $\\{x\\in(0,1]:\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{j=1}^{n}\\log(d_j(x))=\\xi\\}$ and $\\{x\\in(0,1]:\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{j=1}^{n}2^{d_j(x)}=\\xi\\}$.","sentences":["Each real number $x\\in[0,1]$ admits a unique power-2-decaying Gauss-like expansion (P2GLE for short) as $x=\\sum_{i\\in\\mathbb{N}} 2^{-(d_1(x)+d_2(x)+\\cdots+d_i(x))}$, where $d_i(x)\\in\\mathbb{N}$. For any $x\\in(0,1]$, the Khintchine exponent $\\gamma(x)$ is defined by $\\gamma(x):=\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{j=1}^nd_j(x)$ if the limit exists.","We investigate the sizes of the level sets $E(\\xi):=\\{x\\in(0,1]:\\gamma(x)=\\xi\\}$ for $\\xi\\geq 1$. Utilizing the Ruelle operator theory, we obtain the Khintchine spectrum $\\xi\\mapsto\\dim_H E(\\xi)$, where $\\dim_H$ denotes the Hausdorff dimension.","We establish the remarkable fact that the Khintchine spectrum has exactly one inflection point, which was never proved for the corresponding spectrum in continued fractions.","As a direct consequence, we also obtain the Lyapunov spectrum.","Furthermore, we find the Hausdorff dimensions of the level sets $\\{x\\in(0,1]:\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{j=1}^{n}\\log(d_j(x))=\\xi\\}$ and $\\{x\\in(0,1]:\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{j=1}^{n}2^{d_j(x)}=\\xi\\}$."],"url":"http://arxiv.org/abs/2404.17414v1","category":"math.DS"}
{"created":"2024-04-26 13:38:45","title":"Low-Complexity Near-Field Channel Estimation for Hybrid RIS Assisted Systems","abstract":"We investigate the channel estimation (CE) problem for hybrid RIS assisted systems and focus on the near-field (NF) regime. Different from their far-field counterparts, NF channels possess a block-sparsity property, which is leveraged in the two developed CE algorithms: (i) boundary estimation and sub-vector recovery (BESVR) and (ii) linear total variation regularization (TVR). In addition, we adopt the alternating direction method of multipliers to reduce their computational complexity. Numerical results show that the linear TVR algorithm outperforms the chosen baseline schemes in terms of normalized mean square error in the high signal-to-noise ratio regime while the BESVR algorithm achieves comparable performance to the baseline schemes but with the added advantage of minimal CPU time.","sentences":["We investigate the channel estimation (CE) problem for hybrid RIS assisted systems and focus on the near-field (NF) regime.","Different from their far-field counterparts, NF channels possess a block-sparsity property, which is leveraged in the two developed CE algorithms: (i) boundary estimation and sub-vector recovery (BESVR) and (ii) linear total variation regularization (TVR).","In addition, we adopt the alternating direction method of multipliers to reduce their computational complexity.","Numerical results show that the linear TVR algorithm outperforms the chosen baseline schemes in terms of normalized mean square error in the high signal-to-noise ratio regime while the BESVR algorithm achieves comparable performance to the baseline schemes but with the added advantage of minimal CPU time."],"url":"http://arxiv.org/abs/2404.17411v1","category":"eess.SP"}
{"created":"2024-04-26 13:32:53","title":"Stability of partially congested travelling wave solutions for the extended Aw-Rascle system","abstract":"We prove the non-linear stability of a class of travelling-wave solutions to the extended Aw-Rascle system with a singular offset function, which is formally equivalent to the compressible pressureless Navier-Stokes system with a singular viscosity. These solutions encode the effect of congestion by connecting a congested left state to an uncongested right state, and may also be viewed as approximations of solutions to the 'hard-congestion model'. By using carefully weighted energy estimates we are able to prove the non-linear stability of viscous shock waves to the Aw-Rascle system under a small zero integral perturbation, which in particular extends previous results that do not handle the case where the viscosity is singular.","sentences":["We prove the non-linear stability of a class of travelling-wave solutions to the extended Aw-Rascle system with a singular offset function, which is formally equivalent to the compressible pressureless Navier-Stokes system with a singular viscosity.","These solutions encode the effect of congestion by connecting a congested left state to an uncongested right state, and may also be viewed as approximations of solutions to the 'hard-congestion model'.","By using carefully weighted energy estimates we are able to prove the non-linear stability of viscous shock waves to the Aw-Rascle system under a small zero integral perturbation, which in particular extends previous results that do not handle the case where the viscosity is singular."],"url":"http://arxiv.org/abs/2404.17406v1","category":"math.AP"}
{"created":"2024-04-26 13:17:17","title":"Onset of global instability in a premixed annular V-flame","abstract":"We investigate self-excited axisymmetric oscillations of a lean premixed methane--air V-flame in a laminar annular jet. The flame is anchored near the rim of the centrebody, forming an inverted cone, while the strongest vorticity is concentrated along the outer shear layer of the annular jet. Consequently, the reaction and vorticity dynamics are largely separated, except where they coalesce near the flame tip. The global eigenmodes corresponding to the linearised reacting flow equations around the steady base state are computed in an axisymmetric setting. We identify an arc branch of eigenmodes exhibiting strong oscillations at the flame tip. The associated eigenvalues are robust with respect to domain truncation and numerical discretisation, and they become destabilised as the Reynolds number increases. The frequency of the leading eigenmode is found to correspond to the Lagrangian disturbance advection time from the nozzle outlet to the flame tip. This linear result suggests a non-local feedback mechanism consistent with the scenario of `intrinsic thermoacoustic instability'. Nonlinear time-resolved simulation further reveals notable hysteresis phenomena in the subcritical regime prior to instability. Hence, even when the flame is linearly stable, perturbations of sufficient amplitude can trigger limit-cycle oscillations and higher-dimensional dynamics sustained by nonlinear feedback. Notably, linear analysis of the subcritical time-averaged limit-cycle state yields eigenvalues that do not match the nonlinear periodic oscillation frequencies, emphasising the essential role of nonlinear harmonic interactions in the system dynamics.","sentences":["We investigate self-excited axisymmetric oscillations of a lean premixed methane--air V-flame in a laminar annular jet.","The flame is anchored near the rim of the centrebody, forming an inverted cone, while the strongest vorticity is concentrated along the outer shear layer of the annular jet.","Consequently, the reaction and vorticity dynamics are largely separated, except where they coalesce near the flame tip.","The global eigenmodes corresponding to the linearised reacting flow equations around the steady base state are computed in an axisymmetric setting.","We identify an arc branch of eigenmodes exhibiting strong oscillations at the flame tip.","The associated eigenvalues are robust with respect to domain truncation and numerical discretisation, and they become destabilised as the Reynolds number increases.","The frequency of the leading eigenmode is found to correspond to the Lagrangian disturbance advection time from the nozzle outlet to the flame tip.","This linear result suggests a non-local feedback mechanism consistent with the scenario of `intrinsic thermoacoustic instability'.","Nonlinear time-resolved simulation further reveals notable hysteresis phenomena in the subcritical regime prior to instability.","Hence, even when the flame is linearly stable, perturbations of sufficient amplitude can trigger limit-cycle oscillations and higher-dimensional dynamics sustained by nonlinear feedback.","Notably, linear analysis of the subcritical time-averaged limit-cycle state yields eigenvalues that do not match the nonlinear periodic oscillation frequencies, emphasising the essential role of nonlinear harmonic interactions in the system dynamics."],"url":"http://arxiv.org/abs/2404.17396v1","category":"physics.flu-dyn"}
{"created":"2024-04-26 13:15:08","title":"Situational Graphs for Robotic First Responders: an application to dismantling drug labs","abstract":"In this work, we support experts in the safety domain with safer dismantling of drug labs, by deploying robots for the initial inspection. Being able to act on the discovered environment is key to enabling this (semi-)autonomous inspection, e.g. to open doors or take a closer at suspicious items. Our approach addresses this with a novel environmental representation, the Behavior-Oriented Situational Graph, where we extend on the classical situational graph by merging a perception-driven backbone with prior actionable knowledge via a situational affordance schema. Linking situations to robot behaviors facilitates both autonomous mission planning and situational understanding of the operator. Planning over the graph is easier and faster, since it directly incorporates actionable information, which is critical for online mission systems. Moreover, the representation allows the human operator to seamlessly transition between different levels of autonomy of the robot, from remote control to behavior execution to full autonomous exploration. We test the effectiveness of our approach in a real-world drug lab scenario at a Dutch police training facility using a mobile Spot robot and use the results to iterate on the system design.","sentences":["In this work, we support experts in the safety domain with safer dismantling of drug labs, by deploying robots for the initial inspection.","Being able to act on the discovered environment is key to enabling this (semi-)autonomous inspection, e.g. to open doors or take a closer at suspicious items.","Our approach addresses this with a novel environmental representation, the Behavior-Oriented Situational Graph, where we extend on the classical situational graph by merging a perception-driven backbone with prior actionable knowledge via a situational affordance schema.","Linking situations to robot behaviors facilitates both autonomous mission planning and situational understanding of the operator.","Planning over the graph is easier and faster, since it directly incorporates actionable information, which is critical for online mission systems.","Moreover, the representation allows the human operator to seamlessly transition between different levels of autonomy of the robot, from remote control to behavior execution to full autonomous exploration.","We test the effectiveness of our approach in a real-world drug lab scenario at a Dutch police training facility using a mobile Spot robot and use the results to iterate on the system design."],"url":"http://arxiv.org/abs/2404.17395v1","category":"cs.RO"}
{"created":"2024-04-26 13:11:01","title":"Equivariant Lagrangian Floer homology via multiplicative flow trees","abstract":"We provide constructions of equivariant Lagrangian Floer homology groups, by constructing and exploiting an $A_\\infty$-module structure on the Floer complex.","sentences":["We provide constructions of equivariant Lagrangian Floer homology groups, by constructing and exploiting an $A_\\infty$-module structure on the Floer complex."],"url":"http://arxiv.org/abs/2404.17393v1","category":"math.SG"}
{"created":"2024-04-26 13:10:38","title":"Hindered settling of log-normally distributed particulate suspensions: theoretical models vs. Stokesian simulations","abstract":"Settling velocity statistics for dilute, non-Brownian suspensions of polydisperse spheres having a log-normal size distribution are analysed by Stokesian Dynamics, as a function of the total volume fraction and width of the size distribution. Several hundred instantaneous configurations are averaged to obtain reliable statistics. Average velocities for each particle class are compared to the models proposed by Batchelor, Richardson & Zaki, Davis & Gecol, and Masliyah-Lockett-Bassoon (MLB). Batchelor's model is shown to give reasonably accurate predictions when the volume fraction is within 5%. Because of its complexity, this model is however hardly used in practice, so lower-order models are needed. We found that while the other hindered settling models can give reasonably accurate predictions of the velocity of the largest particles, all of them overestimate - in certain cases by a large margin - the velocity of the smaller particles. By computing the fluid-particle velocity slip for each particle class and using Batchelor's model, we explain why predicting the lower tail of the particle size distribution is challenging, and propose possible avenues for model improvement. The analysis of velocity fluctuations suggest quantitative similarities between velocity fluctuations in monodisperse and polydisperse suspensions.","sentences":["Settling velocity statistics for dilute, non-Brownian suspensions of polydisperse spheres having a log-normal size distribution are analysed by Stokesian Dynamics, as a function of the total volume fraction and width of the size distribution.","Several hundred instantaneous configurations are averaged to obtain reliable statistics.","Average velocities for each particle class are compared to the models proposed by Batchelor, Richardson & Zaki, Davis & Gecol, and Masliyah-Lockett-Bassoon (MLB).","Batchelor's model is shown to give reasonably accurate predictions when the volume fraction is within 5%.","Because of its complexity, this model is however hardly used in practice, so lower-order models are needed.","We found that while the other hindered settling models can give reasonably accurate predictions of the velocity of the largest particles, all of them overestimate - in certain cases by a large margin - the velocity of the smaller particles.","By computing the fluid-particle velocity slip for each particle class and using Batchelor's model, we explain why predicting the lower tail of the particle size distribution is challenging, and propose possible avenues for model improvement.","The analysis of velocity fluctuations suggest quantitative similarities between velocity fluctuations in monodisperse and polydisperse suspensions."],"url":"http://arxiv.org/abs/2404.17392v1","category":"physics.flu-dyn"}
{"created":"2024-04-26 12:55:05","title":"Adaptive speed planning for Unmanned Vehicle Based on Deep Reinforcement Learning","abstract":"In order to solve the problem of frequent deceleration of unmanned vehicles when approaching obstacles, this article uses a Deep Q-Network (DQN) and its extension, the Double Deep Q-Network (DDQN), to develop a local navigation system that adapts to obstacles while maintaining optimal speed planning. By integrating improved reward functions and obstacle angle determination methods, the system demonstrates significant enhancements in maneuvering capabilities without frequent decelerations. Experiments conducted in simulated environments with varying obstacle densities confirm the effectiveness of the proposed method in achieving more stable and efficient path planning.","sentences":["In order to solve the problem of frequent deceleration of unmanned vehicles when approaching obstacles, this article uses a Deep Q-Network (DQN) and its extension, the Double Deep Q-Network (DDQN), to develop a local navigation system that adapts to obstacles while maintaining optimal speed planning.","By integrating improved reward functions and obstacle angle determination methods, the system demonstrates significant enhancements in maneuvering capabilities without frequent decelerations.","Experiments conducted in simulated environments with varying obstacle densities confirm the effectiveness of the proposed method in achieving more stable and efficient path planning."],"url":"http://arxiv.org/abs/2404.17379v1","category":"cs.RO"}
{"created":"2024-04-26 12:45:16","title":"Walking behavior induced by $\\mathcal{PT}$ symmetry breaking in a non-Hermitian $\\rm XY$ model with clock anisotropy","abstract":"A quantum system governed by a non-Hermitian Hamiltonian may exhibit zero temperature phase transitions that are driven by interactions, just as its Hermitian counterpart, raising the fundamental question how non-Hermiticity affects quantum criticality. In this context we consider a non-Hermitian system consisting of an $\\rm XY$ model with a complex-valued four-state clock interaction that may or may not have parity-time-reversal ($\\mathcal{PT}$) symmetry. When the $\\mathcal{PT}$ symmetry is broken, and time-evolution becomes non-unitary, a scaling behavior similar to the Berezinskii-Kosterlitz-Thouless phase transition ensues, but in a highly unconventional way, as the line of fixed points is absent. From the analysis of the $d$-dimensional RG equations, we obtain that the unconventional behavior in the $\\mathcal{PT}$ broken regime follows from the collision of two fixed points in the $d\\to 2$ limit, leading to walking behavior or pseudocriticality. For $d=2+1$ the near critical behavior is characterized by a correlation length exponent $\\nu=3/8$, a value smaller than the mean-field one. These results are in sharp contrast with the $\\mathcal{PT}$-symmetric case where only one fixed point arises for $2<d<4$ and in $d=1+1$ three lines of fixed points occur with a continuously varying critical exponent $\\nu$.","sentences":["A quantum system governed by a non-Hermitian Hamiltonian may exhibit zero temperature phase transitions that are driven by interactions, just as its Hermitian counterpart, raising the fundamental question how non-Hermiticity affects quantum criticality.","In this context we consider a non-Hermitian system consisting of an $\\rm XY$ model with a complex-valued four-state clock interaction that may or may not have parity-time-reversal ($\\mathcal{PT}$) symmetry.","When the $\\mathcal{PT}$ symmetry is broken, and time-evolution becomes non-unitary, a scaling behavior similar to the Berezinskii-Kosterlitz-Thouless phase transition ensues, but in a highly unconventional way, as the line of fixed points is absent.","From the analysis of the $d$-dimensional RG equations, we obtain that the unconventional behavior in the $\\mathcal{PT}$ broken regime follows from the collision of two fixed points in the $d\\to 2$ limit, leading to walking behavior or pseudocriticality.","For $d=2+1$ the near critical behavior is characterized by a correlation length exponent $\\nu=3/8$, a value smaller than the mean-field one.","These results are in sharp contrast with the $\\mathcal{PT}$-symmetric case where only one fixed point arises for $2<d<4$ and in $d=1+1$ three lines of fixed points occur with a continuously varying critical exponent $\\nu$."],"url":"http://arxiv.org/abs/2404.17373v1","category":"quant-ph"}
{"created":"2024-04-26 12:41:17","title":"An Optimised Brushless DC Motor Control Scheme for Robotics Applications","abstract":"This work aims to develop an integrated control strategy for Brushless Direct Current Motors for a wide range of applications in robotics systems. The controller is suited for both high torque - low speed and high-speed control of the motors. Hardware validation is done by developing a custom BLDC drive system, and the circuit elements are optimised for power efficiency.","sentences":["This work aims to develop an integrated control strategy for Brushless Direct Current Motors for a wide range of applications in robotics systems.","The controller is suited for both high torque - low speed and high-speed control of the motors.","Hardware validation is done by developing a custom BLDC drive system, and the circuit elements are optimised for power efficiency."],"url":"http://arxiv.org/abs/2404.17367v1","category":"cs.RO"}
{"created":"2024-04-26 12:12:05","title":"Phase and amplitude responses for delay equations using harmonic balance","abstract":"Robust delay induced oscillations, common in nature, are often modeled by delay-differential equations (DDEs). Motivated by the success of phase-amplitude reductions for ordinary differential equations with limit cycle oscillations, there is now a growing interest in the development of analogous approaches for DDEs to understand their response to external forcing. When combined with Floquet theory, the fundamental quantities for this reduction are phase and amplitude response functions. Here, we develop a framework for their construction that utilises the method of harmonic balance.","sentences":["Robust delay induced oscillations, common in nature, are often modeled by delay-differential equations (DDEs).","Motivated by the success of phase-amplitude reductions for ordinary differential equations with limit cycle oscillations, there is now a growing interest in the development of analogous approaches for DDEs to understand their response to external forcing.","When combined with Floquet theory, the fundamental quantities for this reduction are phase and amplitude response functions.","Here, we develop a framework for their construction that utilises the method of harmonic balance."],"url":"http://arxiv.org/abs/2404.17356v1","category":"math.DS"}
{"created":"2024-04-26 12:11:47","title":"Pole-skipping for massive fields and the Stueckelberg formalism","abstract":"Pole-skipping refers to the special phenomenon that the pole and the zero of a retarded two-point Green's function coincide at certain points in momentum space. We study the pole-skipping phenomenon in holographic Green's functions of boundary operators that are dual to massive $p$-form fields and the dRGT massive gravitational fields in the AdS black hole background. Pole-skipping points for these systems are computed using the near horizon method. The relation between the pole-skipping points of massive fields and their massless counterparts is revealed. In particular, as the field mass $m$ is varied from zero to non-zero, the pole-skipping phenomenon undergoes an abrupt change with doubled pole-skipping points found in the massive case. This arises from the breaking of gauge invariance due to the mass term and the consequent appearance of more degrees of freedom. We recover the gauge invariance using the Stueckelberg formalism by introducing auxiliary dynamical fields. The extra pole-skipping points are identified to be associated with the Stueckelberg fields. We also observe that, as the mass varies, some pole-skipping points of the wave number $q$ may move from a non-physical region with complex $q$ to a physical region with real $q$.","sentences":["Pole-skipping refers to the special phenomenon that the pole and the zero of a retarded two-point Green's function coincide at certain points in momentum space.","We study the pole-skipping phenomenon in holographic Green's functions of boundary operators that are dual to massive $p$-form fields and the dRGT massive gravitational fields in the AdS black hole background.","Pole-skipping points for these systems are computed using the near horizon method.","The relation between the pole-skipping points of massive fields and their massless counterparts is revealed.","In particular, as the field mass $m$ is varied from zero to non-zero, the pole-skipping phenomenon undergoes an abrupt change with doubled pole-skipping points found in the massive case.","This arises from the breaking of gauge invariance due to the mass term and the consequent appearance of more degrees of freedom.","We recover the gauge invariance using the Stueckelberg formalism by introducing auxiliary dynamical fields.","The extra pole-skipping points are identified to be associated with the Stueckelberg fields.","We also observe that, as the mass varies, some pole-skipping points of the wave number $q$ may move from a non-physical region with complex $q$ to a physical region with real $q$."],"url":"http://arxiv.org/abs/2404.17354v1","category":"hep-th"}
{"created":"2024-04-26 11:39:50","title":"Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak Multi-label Learning","abstract":"Multi-view learning has become a popular research topic in recent years, but research on the cross-application of classic multi-label classification and multi-view learning is still in its early stages. In this paper, we focus on the complex yet highly realistic task of incomplete multi-view weak multi-label learning and propose a masked two-channel decoupling framework based on deep neural networks to solve this problem. The core innovation of our method lies in decoupling the single-channel view-level representation, which is common in deep multi-view learning methods, into a shared representation and a view-proprietary representation. We also design a cross-channel contrastive loss to enhance the semantic property of the two channels. Additionally, we exploit supervised information to design a label-guided graph regularization loss, helping the extracted embedding features preserve the geometric structure among samples. Inspired by the success of masking mechanisms in image and text analysis, we develop a random fragment masking strategy for vector features to improve the learning ability of encoders. Finally, it is important to emphasize that our model is fully adaptable to arbitrary view and label absences while also performing well on the ideal full data. We have conducted sufficient and convincing experiments to confirm the effectiveness and advancement of our model.","sentences":["Multi-view learning has become a popular research topic in recent years, but research on the cross-application of classic multi-label classification and multi-view learning is still in its early stages.","In this paper, we focus on the complex yet highly realistic task of incomplete multi-view weak multi-label learning and propose a masked two-channel decoupling framework based on deep neural networks to solve this problem.","The core innovation of our method lies in decoupling the single-channel view-level representation, which is common in deep multi-view learning methods, into a shared representation and a view-proprietary representation.","We also design a cross-channel contrastive loss to enhance the semantic property of the two channels.","Additionally, we exploit supervised information to design a label-guided graph regularization loss, helping the extracted embedding features preserve the geometric structure among samples.","Inspired by the success of masking mechanisms in image and text analysis, we develop a random fragment masking strategy for vector features to improve the learning ability of encoders.","Finally, it is important to emphasize that our model is fully adaptable to arbitrary view and label absences while also performing well on the ideal full data.","We have conducted sufficient and convincing experiments to confirm the effectiveness and advancement of our model."],"url":"http://arxiv.org/abs/2404.17340v1","category":"cs.CV"}
{"created":"2024-04-26 11:38:55","title":"Towards an Approach to Pattern-based Domain-Specific Requirements Engineering","abstract":"Requirements specification patterns have received much attention as they promise to guide the structured specification of natural language requirements. By using them, the intention is to reduce quality problems related to requirements artifacts. Patterns may need to vary in their syntax (e.g. domain details/ parameter incorporation) and semantics according to the particularities of the application domain. However, pattern-based approaches, such as EARS, are designed domain-independently to facilitate their wide adoption across several domains. Little is yet known about how to adopt the principle idea of pattern-based requirements engineering to cover domain-specificity in requirements engineering and, ideally, integrate requirements engineering activities into quality assurance tasks. In this paper, we propose the Pattern-based Domain-specific Requirements Engineering Approach for the specification of functional and performance requirements in a holistic manner. This approach emerges from an academia-industry collaboration and is our first attempt to frame an approach which allows for analyzing domain knowledge and incorporating it into the requirements engineering process enabling automated checks for requirements quality assurance and computer-aided support for system verification. Our contribution is two-fold: First, we present a solution to pattern-based domain-specific requirements engineering and its exemplary integration into quality assurance techniques. Second, we showcase a proof of concept using a tool implementation for the domain of flight controllers for Unmanned Aerial Vehicles. Both shall allow us to outline next steps in our research agenda and foster discussions in this direction.","sentences":["Requirements specification patterns have received much attention as they promise to guide the structured specification of natural language requirements.","By using them, the intention is to reduce quality problems related to requirements artifacts.","Patterns may need to vary in their syntax (e.g. domain details/ parameter incorporation) and semantics according to the particularities of the application domain.","However, pattern-based approaches, such as EARS, are designed domain-independently to facilitate their wide adoption across several domains.","Little is yet known about how to adopt the principle idea of pattern-based requirements engineering to cover domain-specificity in requirements engineering and, ideally, integrate requirements engineering activities into quality assurance tasks.","In this paper, we propose the Pattern-based Domain-specific Requirements Engineering Approach for the specification of functional and performance requirements in a holistic manner.","This approach emerges from an academia-industry collaboration and is our first attempt to frame an approach which allows for analyzing domain knowledge and incorporating it into the requirements engineering process enabling automated checks for requirements quality assurance and computer-aided support for system verification.","Our contribution is two-fold: First, we present a solution to pattern-based domain-specific requirements engineering and its exemplary integration into quality assurance techniques.","Second, we showcase a proof of concept using a tool implementation for the domain of flight controllers for Unmanned Aerial Vehicles.","Both shall allow us to outline next steps in our research agenda and foster discussions in this direction."],"url":"http://arxiv.org/abs/2404.17338v1","category":"cs.SE"}
{"created":"2024-04-26 11:30:34","title":"Managing Security Evidence in Safety-Critical Organizations","abstract":"With the increasing prevalence of open and connected products, cybersecurity has become a serious issue in safety-critical domains such as the automotive industry. As a result, regulatory bodies have become more stringent in their requirements for cybersecurity, necessitating security assurance for products developed in these domains. In response, companies have implemented new or modified processes to incorporate security into their product development lifecycle, resulting in a large amount of evidence being created to support claims about the achievement of a certain level of security. However, managing evidence is not a trivial task, particularly for complex products and systems. This paper presents a qualitative interview study conducted in six companies on the maturity of managing security evidence in safety-critical organizations. We find that the current maturity of managing security evidence is insufficient for the increasing requirements set by certification authorities and standardization bodies. Organisations currently fail to identify relevant artifacts as security evidence and manage this evidence on an organizational level. One part of the reason are educational gaps, the other a lack of processes. The impact of AI on the management of security evidence is still an open question","sentences":["With the increasing prevalence of open and connected products, cybersecurity has become a serious issue in safety-critical domains such as the automotive industry.","As a result, regulatory bodies have become more stringent in their requirements for cybersecurity, necessitating security assurance for products developed in these domains.","In response, companies have implemented new or modified processes to incorporate security into their product development lifecycle, resulting in a large amount of evidence being created to support claims about the achievement of a certain level of security.","However, managing evidence is not a trivial task, particularly for complex products and systems.","This paper presents a qualitative interview study conducted in six companies on the maturity of managing security evidence in safety-critical organizations.","We find that the current maturity of managing security evidence is insufficient for the increasing requirements set by certification authorities and standardization bodies.","Organisations currently fail to identify relevant artifacts as security evidence and manage this evidence on an organizational level.","One part of the reason are educational gaps, the other a lack of processes.","The impact of AI on the management of security evidence is still an open question"],"url":"http://arxiv.org/abs/2404.17332v1","category":"cs.SE"}
{"created":"2024-04-26 11:28:17","title":"Finite Sample Analysis for a Class of Subspace Identification Methods","abstract":"While subspace identification methods (SIMs) are appealing due to their simple parameterization for MIMO systems and robust numerical realizations, a comprehensive statistical analysis of SIMs remains an open problem, especially in the non-asymptotic regime. In this work, we provide a finite sample analysis for a class of SIMs, which reveals that the convergence rates for estimating Markov parameters and system matrices are $\\mathcal{O}(1/\\sqrt{N})$, in line with classical asymptotic results. Based on the observation that the model format in classical SIMs becomes non-causal because of a projection step, we choose a parsimonious SIM that bypasses the projection step and strictly enforces a causal model to facilitate the analysis, where a bank of ARX models are estimated in parallel. Leveraging recent results from finite sample analysis of an individual ARX model, we obtain an overall error bound of an array of ARX models and proceed to derive error bounds for system matrices via robustness results for the singular value decomposition.","sentences":["While subspace identification methods (SIMs) are appealing due to their simple parameterization for MIMO systems and robust numerical realizations, a comprehensive statistical analysis of SIMs remains an open problem, especially in the non-asymptotic regime.","In this work, we provide a finite sample analysis for a class of SIMs, which reveals that the convergence rates for estimating Markov parameters and system matrices are $\\mathcal{O}(1/\\sqrt{N})$, in line with classical asymptotic results.","Based on the observation that the model format in classical SIMs becomes non-causal because of a projection step, we choose a parsimonious SIM that bypasses the projection step and strictly enforces a causal model to facilitate the analysis, where a bank of ARX models are estimated in parallel.","Leveraging recent results from finite sample analysis of an individual ARX model, we obtain an overall error bound of an array of ARX models and proceed to derive error bounds for system matrices via robustness results for the singular value decomposition."],"url":"http://arxiv.org/abs/2404.17331v1","category":"eess.SY"}
{"created":"2024-04-26 11:20:50","title":"Inductive magnon noise spectroscopy","abstract":"State tomography allows to characterize quantum states, and was recently applied to reveal the dynamic magnetization state of a parametrically driven magnet. The identification of non-classical states, such as squeezed states, relies on a careful analysis of their emission and their distinction from thermal and vacuum fluctuations. A technique allowing to detect equilibrium magnetization fluctuations is a crucial first step in this regard. In this Letter, we show that inductive magnon noise spectroscopy (iMNS) allows to characterize the thermal magnetization fluctuations of a ferromagnetic thin film in a broadband coplanar waveguide-based scheme. Relative to a cold microwave background, the microwaves emitted by the equilibrium magnetization fluctuations can be detected via spectrum analysis. We provide a comprehensive picture of our microwave system by quantitatively modeling its response, including the thermalizing influence of the cables. The model allows for direct comparison to low-power broadband ferromagnetic resonance measurements with excellent agreement, corroborating the equilibrium character of the iMNS measurement by probing the linear response of the equilibrium state. Our work thus demonstrates broadband access to the equilibrium properties of magnetization fluctuations using a purely inductive approach.","sentences":["State tomography allows to characterize quantum states, and was recently applied to reveal the dynamic magnetization state of a parametrically driven magnet.","The identification of non-classical states, such as squeezed states, relies on a careful analysis of their emission and their distinction from thermal and vacuum fluctuations.","A technique allowing to detect equilibrium magnetization fluctuations is a crucial first step in this regard.","In this Letter, we show that inductive magnon noise spectroscopy (iMNS) allows to characterize the thermal magnetization fluctuations of a ferromagnetic thin film in a broadband coplanar waveguide-based scheme.","Relative to a cold microwave background, the microwaves emitted by the equilibrium magnetization fluctuations can be detected via spectrum analysis.","We provide a comprehensive picture of our microwave system by quantitatively modeling its response, including the thermalizing influence of the cables.","The model allows for direct comparison to low-power broadband ferromagnetic resonance measurements with excellent agreement, corroborating the equilibrium character of the iMNS measurement by probing the linear response of the equilibrium state.","Our work thus demonstrates broadband access to the equilibrium properties of magnetization fluctuations using a purely inductive approach."],"url":"http://arxiv.org/abs/2404.17327v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-26 11:14:36","title":"Towards Scalable Multi-Chip Wireless Networks with Near-Field Time Reversal","abstract":"The concept of Wireless Network-on-Chip (WNoC) has emerged as a potential solution to address the escalating communication demands of modern computing systems due to their low-latency, versatility, and reconfigurability. However, for WNoC to fulfill its potential, it is essential to establish multiple high-speed wireless links across chips. Unfortunately, the compact and enclosed nature of computing packages introduces significant challenges in the form of Co-Channel Interference (CCI) and Inter-Symbol Interference (ISI), which not only hinder the deployment of multiple spatial channels but also severely restrict the symbol rate of each individual channel. In this paper, we posit that Time Reversal (TR) could be effective in addressing both impairments in this static scenario thanks to its spatiotemporal focusing capabilities even in the near field. Through comprehensive full-wave simulations and bit error rate analysis in multiple scenarios and at multiple frequency bands, we provide evidence that TR can increase the symbol rate by an order of magnitude, enabling the deployment of multiple concurrent links and achieving aggregate speeds exceeding 100 Gb/s. Finally, we evaluate the impact of reducing the sampling rate of the TR filter on the achievable speeds, paving the way to practical TR-based wireless communications at the chip scale.","sentences":["The concept of Wireless Network-on-Chip (WNoC) has emerged as a potential solution to address the escalating communication demands of modern computing systems due to their low-latency, versatility, and reconfigurability.","However, for WNoC to fulfill its potential, it is essential to establish multiple high-speed wireless links across chips.","Unfortunately, the compact and enclosed nature of computing packages introduces significant challenges in the form of Co-Channel Interference (CCI) and Inter-Symbol Interference (ISI), which not only hinder the deployment of multiple spatial channels but also severely restrict the symbol rate of each individual channel.","In this paper, we posit that Time Reversal (TR) could be effective in addressing both impairments in this static scenario thanks to its spatiotemporal focusing capabilities even in the near field.","Through comprehensive full-wave simulations and bit error rate analysis in multiple scenarios and at multiple frequency bands, we provide evidence that TR can increase the symbol rate by an order of magnitude, enabling the deployment of multiple concurrent links and achieving aggregate speeds exceeding 100 Gb/s. Finally, we evaluate the impact of reducing the sampling rate of the TR filter on the achievable speeds, paving the way to practical TR-based wireless communications at the chip scale."],"url":"http://arxiv.org/abs/2404.17325v1","category":"cs.ET"}
{"created":"2024-04-26 10:53:50","title":"Observation of a Fully-formed Forward--Reverse Shock Pair Due to the Interaction Between Two Coronal Mass Ejections at 0.5 au","abstract":"We report direct observations of a fast magnetosonic forward--reverse shock pair observed by Solar Orbiter on March 8, 2022 at the short heliocentric distance of 0.5 au. The structure, sharing some features with fully-formed stream interaction regions (SIRs), is due to the interaction between two successive coronal mass ejections (CMEs), never previously observed to give rise to a forward--reverse shock pair. The scenario is supported by remote observations from the STEREO-A coronographs, where two candidate eruptions compatible with the in-situ signatures have been found. In the interaction region, we find enhanced energetic particle activity, strong non-radial flow deflections and evidence of magnetic reconnection. At 1~au, well radially-aligned \\textit{Wind} observations reveal a complex event, with characteristic observational signatures of both SIR and CME--CME interaction, thus demonstrating the importance of investigating the complex dynamics governing solar eruptive phenomena.","sentences":["We report direct observations of a fast magnetosonic forward--reverse shock pair observed by Solar Orbiter on March 8, 2022 at the short heliocentric distance of 0.5 au.","The structure, sharing some features with fully-formed stream interaction regions (SIRs), is due to the interaction between two successive coronal mass ejections (CMEs), never previously observed to give rise to a forward--reverse shock pair.","The scenario is supported by remote observations from the STEREO-A coronographs, where two candidate eruptions compatible with the in-situ signatures have been found.","In the interaction region, we find enhanced energetic particle activity, strong non-radial flow deflections and evidence of magnetic reconnection.","At 1~au, well radially-aligned \\textit{Wind} observations reveal a complex event, with characteristic observational signatures of both SIR and CME--CME interaction, thus demonstrating the importance of investigating the complex dynamics governing solar eruptive phenomena."],"url":"http://arxiv.org/abs/2404.17315v1","category":"astro-ph.SR"}
{"created":"2024-04-26 10:45:34","title":"Towards Group-aware Search Success","abstract":"Traditional measures of search success often overlook the varying information needs of different demographic groups. To address this gap, we introduce a novel metric, named Group-aware Search Success (GA-SS). GA-SS redefines search success to ensure that all demographic groups achieve satisfaction from search outcomes. We introduce a comprehensive mathematical framework to calculate GA-SS, incorporating both static and stochastic ranking policies and integrating user browsing models for a more accurate assessment. In addition, we have proposed Group-aware Most Popular Completion (gMPC) ranking model to account for demographic variances in user intent, aligning more closely with the diverse needs of all user groups. We empirically validate our metric and approach with two real-world datasets: one focusing on query auto-completion and the other on movie recommendations, where the results highlight the impact of stochasticity and the complex interplay among various search success metrics. Our findings advocate for a more inclusive approach in measuring search success, as well as inspiring future investigations into the quality of service of search.","sentences":["Traditional measures of search success often overlook the varying information needs of different demographic groups.","To address this gap, we introduce a novel metric, named Group-aware Search Success (GA-SS).","GA-SS redefines search success to ensure that all demographic groups achieve satisfaction from search outcomes.","We introduce a comprehensive mathematical framework to calculate GA-SS, incorporating both static and stochastic ranking policies and integrating user browsing models for a more accurate assessment.","In addition, we have proposed Group-aware Most Popular Completion (gMPC) ranking model to account for demographic variances in user intent, aligning more closely with the diverse needs of all user groups.","We empirically validate our metric and approach with two real-world datasets: one focusing on query auto-completion and the other on movie recommendations, where the results highlight the impact of stochasticity and the complex interplay among various search success metrics.","Our findings advocate for a more inclusive approach in measuring search success, as well as inspiring future investigations into the quality of service of search."],"url":"http://arxiv.org/abs/2404.17313v1","category":"cs.IR"}
{"created":"2024-04-26 10:39:01","title":"Energy Recovery System for Large Telescopes","abstract":"In this paper, a kinetic energy recovery system for large telescopes is presented, with the Atacama Large Aperture Submm Telescope (AtLAST) as a possible target application. The system consists of supercapacitors integrated in the DC-link of motor inverters through a bidirectional DC-DC converter. The optimal system design, based on the energy flow analysis within the telescope's power electronics, is introduced. The proposed system is simulated as part of the telescope's drives, providing not only a significant reduction in energy consumption of the telescope due to motion, but also remarkably reducing (or shaving) grid power peaks. We find that the system presented here can contribute to making both current and future observatories more sustainable.","sentences":["In this paper, a kinetic energy recovery system for large telescopes is presented, with the Atacama Large Aperture Submm Telescope (AtLAST) as a possible target application.","The system consists of supercapacitors integrated in the DC-link of motor inverters through a bidirectional DC-DC converter.","The optimal system design, based on the energy flow analysis within the telescope's power electronics, is introduced.","The proposed system is simulated as part of the telescope's drives, providing not only a significant reduction in energy consumption of the telescope due to motion, but also remarkably reducing (or shaving) grid power peaks.","We find that the system presented here can contribute to making both current and future observatories more sustainable."],"url":"http://arxiv.org/abs/2404.17311v1","category":"astro-ph.IM"}
{"created":"2024-04-26 09:40:23","title":"Machine Learning based prediction of Vanadium Redox Flow Battery temperature rise under different charge-discharge conditions","abstract":"Accurate prediction of battery temperature rise is very essential for designing an efficient thermal management scheme. In this paper, machine learning (ML) based prediction of Vanadium Redox Flow Battery (VRFB) thermal behavior during charge-discharge operation has been demonstrated for the first time. Considering different currents with a specified electrolyte flow rate, the temperature of a kW scale VRFB system is studied through experiments. Three different ML algorithms; Linear Regression (LR), Support Vector Regression (SVR) and Extreme Gradient Boost (XGBoost) have been used for the prediction work. The training and validation of ML algorithms have been done by the practical dataset of a 1kW 6kWh VRFB storage under 40A, 45A, 50A and 60A charge-discharge currents and 10 L min-1 of flow rate. A comparative analysis among the ML algorithms is done in terms of performance metrics such as correlation coefficient (R2), mean absolute error (MAE) and root mean square error (RMSE). It is observed that XGBoost shows the highest accuracy in prediction of around 99%. The ML based prediction results obtained in this work can be very useful for controlling the VRFB temperature rise during operation and act as indicator for further development of an optimized thermal management system.","sentences":["Accurate prediction of battery temperature rise is very essential for designing an efficient thermal management scheme.","In this paper, machine learning (ML) based prediction of Vanadium Redox Flow Battery (VRFB) thermal behavior during charge-discharge operation has been demonstrated for the first time.","Considering different currents with a specified electrolyte flow rate, the temperature of a kW scale VRFB system is studied through experiments.","Three different ML algorithms; Linear Regression (LR), Support Vector Regression (SVR) and Extreme Gradient Boost (XGBoost) have been used for the prediction work.","The training and validation of ML algorithms have been done by the practical dataset of a 1kW 6kWh VRFB storage under 40A, 45A, 50A and 60A charge-discharge currents and 10 L min-1 of flow rate.","A comparative analysis among the ML algorithms is done in terms of performance metrics such as correlation coefficient (R2), mean absolute error (MAE) and root mean square error (RMSE).","It is observed that XGBoost shows the highest accuracy in prediction of around 99%.","The ML based prediction results obtained in this work can be very useful for controlling the VRFB temperature rise during operation and act as indicator for further development of an optimized thermal management system."],"url":"http://arxiv.org/abs/2404.17284v1","category":"cs.LG"}
{"created":"2024-04-26 09:36:49","title":"Device Feature based on Graph Fourier Transformation with Logarithmic Processing For Detection of Replay Speech Attacks","abstract":"The most common spoofing attacks on automatic speaker verification systems are replay speech attacks. Detection of replay speech heavily relies on replay configuration information. Previous studies have shown that graph Fourier transform-derived features can effectively detect replay speech but ignore device and environmental noise effects. In this work, we propose a new feature, the graph frequency device cepstral coefficient, derived from the graph frequency domain using a device-related linear transformation. We also introduce two novel representations: graph frequency logarithmic coefficient and graph frequency logarithmic device coefficient. We evaluate our methods using traditional Gaussian mixture model and light convolutional neural network systems as classifiers. On the ASVspoof 2017 V2, ASVspoof 2019 physical access, and ASVspoof 2021 physical access datasets, our proposed features outperform known front-ends, demonstrating their effectiveness for replay speech detection.","sentences":["The most common spoofing attacks on automatic speaker verification systems are replay speech attacks.","Detection of replay speech heavily relies on replay configuration information.","Previous studies have shown that graph Fourier transform-derived features can effectively detect replay speech but ignore device and environmental noise effects.","In this work, we propose a new feature, the graph frequency device cepstral coefficient, derived from the graph frequency domain using a device-related linear transformation.","We also introduce two novel representations: graph frequency logarithmic coefficient and graph frequency logarithmic device coefficient.","We evaluate our methods using traditional Gaussian mixture model and light convolutional neural network systems as classifiers.","On the ASVspoof 2017 V2, ASVspoof 2019 physical access, and ASVspoof 2021 physical access datasets, our proposed features outperform known front-ends, demonstrating their effectiveness for replay speech detection."],"url":"http://arxiv.org/abs/2404.17280v1","category":"cs.SD"}
{"created":"2024-04-26 09:26:32","title":"Exact and Approximate High-Multiplicity Scheduling on Identical Machines","abstract":"Goemans and Rothvoss (SODA'14) gave a framework for solving problems in time $enc(P)^{2^{O(N)}}enc(Q)^{O(1)}$ that can be described as finding a point in $\\text{int.cone}(P\\cap\\mathbb{Z}^N)\\cap Q$, where $P,Q\\subset\\mathbb{R}^N$ are (bounded) polyhedra. This framework can be used to solve various scheduling problems, but the encoding length $enc(P)$ usually involves large parameters like the makespan. We describe three tools to improve the framework by Goemans and Rothvoss: Problem-specific preprocessing, LP relaxation techniques and a new bound for the number of vertices of the integer hull.   In particular, applied to the classical scheduling problem $P||C_{\\max}$, these tools each improve the running time from $(\\log(C_{\\max}))^{2^{O(d)}} enc(I)^{O(1)}$ to the possibly much better $(\\log(p_{\\max}))^{2^{O(d)}}enc(I)^{O(1)}$. Here, $p_{\\max}$ is the largest processing time, $d$ is the number of different processing times, $C_{\\max}$ is the makespan and $enc(I)$ is the encoding length of the instance. This running time is FPT w.r.t. parameter $d$ if $p_{\\max}$ is given in unary. We obtain similar results for various other problems. Moreover, we show how a balancing result by Govzmann et al. can be used to speed up an additive approximation scheme by Buchem et al. (ICALP'21) in the high-multiplicity setting.   On the complexity side, we use reductions from the literature to provide new parameterized lower bounds for $P||C_{\\max}$ and to show that the improved running time of the additive approximation algorithm is probably optimal. Finally, we show that the big open question asked by Mnich and van Bevern (Comput. Oper. Res. '18) whether $P||C_{\\max}$ is FPT w.r.t. the number of job types $d$ has the same answer as the question whether $Q||C_{\\max}$ is FPT w.r.t. the number of job and machine types $d+\\tau$ (all in high-multiplicity encoding). The same holds for objective $C_{\\min}$.","sentences":["Goemans and Rothvoss (SODA'14) gave a framework for solving problems in time $enc(P)^{2^{O(N)}}enc(Q)^{O(1)}$ that can be described as finding a point in $\\text{int.cone}(P\\cap\\mathbb{Z}^N)\\cap Q$, where $P,Q\\subset\\mathbb{R}^N$ are (bounded) polyhedra.","This framework can be used to solve various scheduling problems, but the encoding length $enc(P)$ usually involves large parameters like the makespan.","We describe three tools to improve the framework by Goemans and Rothvoss: Problem-specific preprocessing, LP relaxation techniques and a new bound for the number of vertices of the integer hull.   ","In particular, applied to the classical scheduling problem $P||C_{\\max}$, these tools each improve the running time from $(\\log(C_{\\max}))^{2^{O(d)}} enc(I)^{O(1)}$ to the possibly much better $(\\log(p_{\\max}))^{2^{O(d)}}enc(I)^{O(1)}$. Here, $p_{\\max}$ is the largest processing time, $d$ is the number of different processing times, $C_{\\max}$ is the makespan and $enc(I)$ is the encoding length of the instance.","This running time is FPT w.r.t. parameter $d$ if $p_{\\max}$ is given in unary.","We obtain similar results for various other problems.","Moreover, we show how a balancing result by Govzmann et al. can be used to speed up an additive approximation scheme by Buchem et al. (ICALP'21) in the high-multiplicity setting.   ","On the complexity side, we use reductions from the literature to provide new parameterized lower bounds for $P||C_{\\max}$ and to show that the improved running time of the additive approximation algorithm is probably optimal.","Finally, we show that the big open question asked by Mnich and van Bevern (Comput.","Oper.","Res. '18) whether $P||C_{\\max}$ is FPT w.r.t.","the number of job types $d$ has the same answer as the question whether $Q||C_{\\max}$ is FPT w.r.t.","the number of job and machine types $d+\\tau$ (all in high-multiplicity encoding).","The same holds for objective $C_{\\min}$."],"url":"http://arxiv.org/abs/2404.17274v1","category":"cs.DS"}
{"created":"2024-04-26 09:14:39","title":"On the Grothendieck duality for the space of holomorphic Sobolev functions","abstract":"We describe the strong dual space $({\\mathcal O}^s (D))^*$ for the space ${\\mathcal O}^s (D) =   H^s (D) \\cap {\\mathcal O} (D)$ of holomorphic functions from the Sobolev space $H^s(D)$, $s \\in \\mathbb Z$, over a bounded simply connected plane domain $D$ with infinitely differential boundary $\\partial D$. We identify the dual space with the space of holomorhic functions on ${\\mathbb C}^n\\setminus \\overline D$ that belong to $H^{1-s} (G\\setminus \\overline D)$ for any bounded domain $G$, containing the compact $\\overline D$, and vanish at the infinity. As a corollary, we obtain a description of the strong dual space $({\\mathcal O}_F (D))^*$ for the space ${\\mathcal O}_F (D)$ of holomorphic functions of finite order of growth in $D$ (here, ${\\mathcal O}_F (D)$ is endowed with the inductive limit topology with respect to the family of spaces ${\\mathcal O}^s (D)$, $s \\in \\mathbb Z$).   In this way we extend the classical Grothendieck-K{\\\"o}the-Sebasti\\~{a}o e Silva duality for the space of holomorphic functions.","sentences":["We describe the strong dual space $({\\mathcal O}^s (D))^*$ for the space ${\\mathcal O}^s (D) =   H^s (D) \\cap {\\mathcal O} (D)$ of holomorphic functions from the Sobolev space $H^s(D)$, $s \\in \\mathbb Z$, over a bounded simply connected plane domain $D$ with infinitely differential boundary $\\partial D$. We identify the dual space with the space of holomorhic functions on ${\\mathbb C}^n\\setminus","\\overline D$ that belong to $H^{1-s} (G\\setminus \\overline D)$ for any bounded domain $G$, containing the compact $\\overline D$, and vanish at the infinity.","As a corollary, we obtain a description of the strong dual space $({\\mathcal O}_F (D))^*$ for the space ${\\mathcal O}_F (D)$ of holomorphic functions of finite order of growth in $D$ (here, ${\\mathcal O}_F (D)$ is endowed with the inductive limit topology with respect to the family of spaces ${\\mathcal O}^s (D)$, $s \\in \\mathbb Z$).   ","In this way we extend the classical Grothendieck-K{\\\"o}the-Sebasti\\~{a}o e Silva duality for the space of holomorphic functions."],"url":"http://arxiv.org/abs/2404.17266v1","category":"math.CV"}
{"created":"2024-04-26 09:10:04","title":"Beyond Efficiency and Convenience. Using Post-growth Values as a Nucleus to Transform Design Education and Society","abstract":"In this position paper we present Municipan, an artefact resulting from a post-growth design experiment, applied in a student design project. In contrast to mainstream human-centered design directed at efficiency and convenience, which we argue leads to deskilling, dependency, and the progression of the climate crisis, we challenged students to envision an opposite user that is willing to invest time and effort and learn new skills. While Municipan is not a direct step towards a postgrowth society, integrating the way it was created in design education can act as a nucleus, bringing forth design professionals inclined to create technologies with potential to gradually transform society towards postgrowth living. Bringing in examples from our own research, we illustrate that designs created in this mindset, such as heating systems that train cold resistance, or navigation systems that train orientation have potential to reskill users, reduce technological dependency and steer consumption within planetary limits.","sentences":["In this position paper we present Municipan, an artefact resulting from a post-growth design experiment, applied in a student design project.","In contrast to mainstream human-centered design directed at efficiency and convenience, which we argue leads to deskilling, dependency, and the progression of the climate crisis, we challenged students to envision an opposite user that is willing to invest time and effort and learn new skills.","While Municipan is not a direct step towards a postgrowth society, integrating the way it was created in design education can act as a nucleus, bringing forth design professionals inclined to create technologies with potential to gradually transform society towards postgrowth living.","Bringing in examples from our own research, we illustrate that designs created in this mindset, such as heating systems that train cold resistance, or navigation systems that train orientation have potential to reskill users, reduce technological dependency and steer consumption within planetary limits."],"url":"http://arxiv.org/abs/2404.17264v1","category":"cs.HC"}
{"created":"2024-04-26 09:05:16","title":"Multiple-Target Detection in Cell-Free Massive MIMO-Assisted ISAC","abstract":"We propose a distributed implementation for integrated sensing and communication (ISAC) backed by a massive multiple input multiple output (CF-mMIMO) architecture without cells. Distributed multi-antenna access points (APs) simultaneously serve communication users (UEs) and emit probing signals towards multiple specified zones for sensing. The APs can switch between communication and sensing modes, and adjust their transmit power based on the network settings and sensing and communication operations' requirements. By considering local partial zero-forcing and maximum-ratio-transmit precoding at the APs for communication and sensing, respectively, we first derive closed-form expressions for the spectral efficiency (SE) of the UEs and the mainlobe-to-average-sidelobe ratio (MASR) of the sensing zones. Then, a joint operation mode selection and power control design problem is formulated to maximize the SE fairness among the UEs, while ensuring specific levels of MASR for sensing zones. The complicated mixed-integer problem is relaxed and solved via successive convex approximation approach. We further propose a low-complexity design, where AP mode selection is designed through a greedy algorithm and then power control is designed based on this chosen mode. Our findings reveal that the proposed scheme can consistently ensure a sensing success rate of $100\\%$ for different network setups with a satisfactory fairness among all UEs.","sentences":["We propose a distributed implementation for integrated sensing and communication (ISAC) backed by a massive multiple input multiple output (CF-mMIMO) architecture without cells.","Distributed multi-antenna access points (APs) simultaneously serve communication users (UEs) and emit probing signals towards multiple specified zones for sensing.","The APs can switch between communication and sensing modes, and adjust their transmit power based on the network settings and sensing and communication operations' requirements.","By considering local partial zero-forcing and maximum-ratio-transmit precoding at the APs for communication and sensing, respectively, we first derive closed-form expressions for the spectral efficiency (SE) of the UEs and the mainlobe-to-average-sidelobe ratio (MASR) of the sensing zones.","Then, a joint operation mode selection and power control design problem is formulated to maximize the SE fairness among the UEs, while ensuring specific levels of MASR for sensing zones.","The complicated mixed-integer problem is relaxed and solved via successive convex approximation approach.","We further propose a low-complexity design, where AP mode selection is designed through a greedy algorithm and then power control is designed based on this chosen mode.","Our findings reveal that the proposed scheme can consistently ensure a sensing success rate of $100\\%$ for different network setups with a satisfactory fairness among all UEs."],"url":"http://arxiv.org/abs/2404.17263v1","category":"cs.IT"}
{"created":"2024-04-26 08:47:28","title":"Weakly Supervised Training for Hologram Verification in Identity Documents","abstract":"We propose a method to remotely verify the authenticity of Optically Variable Devices (OVDs), often referred to as ``holograms'', in identity documents. Our method processes video clips captured with smartphones under common lighting conditions, and is evaluated on two public datasets: MIDV-HOLO and MIDV-2020. Thanks to a weakly-supervised training, we optimize a feature extraction and decision pipeline which achieves a new leading performance on MIDV-HOLO, while maintaining a high recall on documents from MIDV-2020 used as attack samples. It is also the first method, to date, to effectively address the photo replacement attack task, and can be trained on either genuine samples, attack samples, or both for increased performance. By enabling to verify OVD shapes and dynamics with very little supervision, this work opens the way towards the use of massive amounts of unlabeled data to build robust remote identity document verification systems on commodity smartphones. Code is available at https://github.com/EPITAResearchLab/pouliquen.24.icdar","sentences":["We propose a method to remotely verify the authenticity of Optically Variable Devices (OVDs), often referred to as ``holograms'', in identity documents.","Our method processes video clips captured with smartphones under common lighting conditions, and is evaluated on two public datasets: MIDV-HOLO and MIDV-2020.","Thanks to a weakly-supervised training, we optimize a feature extraction and decision pipeline which achieves a new leading performance on MIDV-HOLO, while maintaining a high recall on documents from MIDV-2020 used as attack samples.","It is also the first method, to date, to effectively address the photo replacement attack task, and can be trained on either genuine samples, attack samples, or both for increased performance.","By enabling to verify OVD shapes and dynamics with very little supervision, this work opens the way towards the use of massive amounts of unlabeled data to build robust remote identity document verification systems on commodity smartphones.","Code is available at https://github.com/EPITAResearchLab/pouliquen.24.icdar"],"url":"http://arxiv.org/abs/2404.17253v1","category":"cs.CV"}
{"created":"2024-04-26 08:26:10","title":"Synchronized Stepwise Control of Firing and Learning Thresholds in a Spiking Randomly Connected Neural Network toward Hardware Implementation","abstract":"We propose hardware-oriented models of intrinsic plasticity (IP) and synaptic plasticity (SP) for spiking randomly connected recursive neural network (RNN). Although the potential of RNNs for temporal data processing has been demonstrated, randomness of the network architecture often causes performance degradation. Self-organization mechanism using IP and SP can mitigate the degradation, therefore, we compile these functions in a spiking neuronal model. To implement the function of IP, a variable firing threshold is introduced to each excitatory neuron in the RNN that changes stepwise in accordance with its activity. We also define other thresholds for SP that synchronize with the firing threshold, which determine the direction of stepwise synaptic update that is executed on receiving a pre-synaptic spike. We demonstrate the effectiveness of our model through simulations of temporal data learning and anomaly detection with a spiking RNN using publicly available electrocardiograms. Considering hardware implementation, we employ discretized thresholds and synaptic weights and show that these parameters can be reduced to binary if the RNN architecture is appropriately designed. This contributes to minimization of the circuit of the neuronal system having IP and SP.","sentences":["We propose hardware-oriented models of intrinsic plasticity (IP) and synaptic plasticity (SP) for spiking randomly connected recursive neural network (RNN).","Although the potential of RNNs for temporal data processing has been demonstrated, randomness of the network architecture often causes performance degradation.","Self-organization mechanism using IP and SP can mitigate the degradation, therefore, we compile these functions in a spiking neuronal model.","To implement the function of IP, a variable firing threshold is introduced to each excitatory neuron in the RNN that changes stepwise in accordance with its activity.","We also define other thresholds for SP that synchronize with the firing threshold, which determine the direction of stepwise synaptic update that is executed on receiving a pre-synaptic spike.","We demonstrate the effectiveness of our model through simulations of temporal data learning and anomaly detection with a spiking RNN using publicly available electrocardiograms.","Considering hardware implementation, we employ discretized thresholds and synaptic weights and show that these parameters can be reduced to binary if the RNN architecture is appropriately designed.","This contributes to minimization of the circuit of the neuronal system having IP and SP."],"url":"http://arxiv.org/abs/2404.17241v1","category":"cs.NE"}
{"created":"2024-04-26 17:52:37","title":"Sensitivity-Improved Polarization Maps at 40 GHz with CLASS and WMAP data","abstract":"Improved polarization measurements at frequencies below 70 GHz with degree-level angular resolution are crucial for advancing our understanding of the Galactic synchrotron radiation and the potential polarized anomalous microwave emission and ultimately benefiting the detection of primordial $B$ modes. In this study, we present sensitivity-improved 40 GHz polarization maps obtained by combining the CLASS 40 GHz and WMAP $Q$-band data through a weighted average in the harmonic domain. The decision to include WMAP $Q$-band data stems from similarities in the bandpasses. Leveraging the accurate large-scale measurements from WMAP $Q$ band and the high-sensitivity information from CLASS 40 GHz band at intermediate scales, the noise level at $\\ell\\in[30, 100]$ is reduced by a factor of $2-3$ in the map space. A pixel domain analysis of the polarized synchrotron spectral index ($\\beta_s$) using WMAP $K$ band and the combined maps (mean and 16/84th percentile across the $\\beta_s$ map: $-3.08_{-0.20}^{+0.20}$) reveals a stronger preference for spatial variation (PTE for a uniform $\\beta_s$ hypothesis smaller than 0.001) than the results obtained using WMAP $K$ and $Ka$ bands ($-3.08_{-0.14}^{+0.14}$). The cross-power spectra of the combined maps follow the same trend as other low-frequency data, and validation through simulations indicates negligible bias introduced by the combination method (sub-percent level in the power spectra). The products of this work are publicly available on $\\mathtt{LAMBDA}$.","sentences":["Improved polarization measurements at frequencies below 70 GHz with degree-level angular resolution are crucial for advancing our understanding of the Galactic synchrotron radiation and the potential polarized anomalous microwave emission and ultimately benefiting the detection of primordial $B$ modes.","In this study, we present sensitivity-improved 40 GHz polarization maps obtained by combining the CLASS 40 GHz and WMAP $Q$-band data through a weighted average in the harmonic domain.","The decision to include WMAP $Q$-band data stems from similarities in the bandpasses.","Leveraging the accurate large-scale measurements from WMAP $Q$ band and the high-sensitivity information from CLASS 40 GHz band at intermediate scales, the noise level at $\\ell\\in[30, 100]$ is reduced by a factor of $2-3$ in the map space.","A pixel domain analysis of the polarized synchrotron spectral index ($\\beta_s$) using WMAP $K$ band and the combined maps (mean and 16/84th percentile across the $\\beta_s$ map: $-3.08_{-0.20}^{+0.20}$) reveals a stronger preference for spatial variation (PTE for a uniform $\\beta_s$ hypothesis smaller than 0.001) than the results obtained using WMAP $K$ and $Ka$ bands ($-3.08_{-0.14}^{+0.14}$).","The cross-power spectra of the combined maps follow the same trend as other low-frequency data, and validation through simulations indicates negligible bias introduced by the combination method (sub-percent level in the power spectra).","The products of this work are publicly available on $\\mathtt{LAMBDA}$."],"url":"http://arxiv.org/abs/2404.17567v1","category":"astro-ph.CO"}
{"created":"2024-04-26 16:34:29","title":"An Online Framework for Fitting Fast Transient Lightcurves","abstract":"The identification of extragalactic fast optical transients (eFOTs) as potential multi-messenger sources is one of the main challenges in time-domain astronomy. However, recent developments have allowed for probes of rapidly-evolving transients. With the increasing number of alert streams from optical time-domain surveys, the next paradigm is building technologies to rapidly identify the most interesting transients for follow-up. One effort to make this possible is the fitting of objects to a variety of eFOT lightcurve models such as kilonovae and $\\gamma$-ray burst (GRB) afterglows. In this work, we describe a new framework designed to efficiently fit transients to light curve models and flag them for further follow-up. We describe the pipeline's workflow and a handful of performance metrics, including the nominal sampling time for each model. We highlight as examples ZTF20abwysqy, the shortest long gamma ray burst discovered to date, and ZTF21abotose, a core-collapse supernova initially identified as a potential kilonova candidate.","sentences":["The identification of extragalactic fast optical transients (eFOTs) as potential multi-messenger sources is one of the main challenges in time-domain astronomy.","However, recent developments have allowed for probes of rapidly-evolving transients.","With the increasing number of alert streams from optical time-domain surveys, the next paradigm is building technologies to rapidly identify the most interesting transients for follow-up.","One effort to make this possible is the fitting of objects to a variety of eFOT lightcurve models such as kilonovae and $\\gamma$-ray burst (GRB) afterglows.","In this work, we describe a new framework designed to efficiently fit transients to light curve models and flag them for further follow-up.","We describe the pipeline's workflow and a handful of performance metrics, including the nominal sampling time for each model.","We highlight as examples ZTF20abwysqy, the shortest long gamma ray burst discovered to date, and ZTF21abotose, a core-collapse supernova initially identified as a potential kilonova candidate."],"url":"http://arxiv.org/abs/2404.17515v1","category":"astro-ph.HE"}
{"created":"2024-04-26 16:09:42","title":"Inhomogeneous illuminated image enhancement under extremely low visibility condition","abstract":"Imaging through fog significantly impacts fields such as object detection and recognition. In conditions of extremely low visibility, essential image information can be obscured, rendering standard extraction methods ineffective. Traditional digital processing techniques, such as histogram stretching, aim to mitigate fog effects by enhancing object light contrast diminished by atmospheric scattering. However, these methods often experience reduce effectiveness under inhomogeneous illumination. This paper introduces a novel approach that adaptively filters background illumination under extremely low visibility and preserve only the essential signal information. Additionally, we employ a visual optimization strategy based on image gradients to eliminate grayscale banding. Finally, the image is transformed to achieve high contrast and maintain fidelity to the original information through maximum histogram equalization. Our proposed method significantly enhances signal clarity in conditions of extremely low visibility and outperforms existing algorithms.","sentences":["Imaging through fog significantly impacts fields such as object detection and recognition.","In conditions of extremely low visibility, essential image information can be obscured, rendering standard extraction methods ineffective.","Traditional digital processing techniques, such as histogram stretching, aim to mitigate fog effects by enhancing object light contrast diminished by atmospheric scattering.","However, these methods often experience reduce effectiveness under inhomogeneous illumination.","This paper introduces a novel approach that adaptively filters background illumination under extremely low visibility and preserve only the essential signal information.","Additionally, we employ a visual optimization strategy based on image gradients to eliminate grayscale banding.","Finally, the image is transformed to achieve high contrast and maintain fidelity to the original information through maximum histogram equalization.","Our proposed method significantly enhances signal clarity in conditions of extremely low visibility and outperforms existing algorithms."],"url":"http://arxiv.org/abs/2404.17503v1","category":"cs.CV"}
{"created":"2024-04-26 15:56:08","title":"Learning text-to-video retrieval from image captioning","abstract":"We describe a protocol to study text-to-video retrieval training with unlabeled videos, where we assume (i) no access to labels for any videos, i.e., no access to the set of ground-truth captions, but (ii) access to labeled images in the form of text. Using image expert models is a realistic scenario given that annotating images is cheaper therefore scalable, in contrast to expensive video labeling schemes. Recently, zero-shot image experts such as CLIP have established a new strong baseline for video understanding tasks. In this paper, we make use of this progress and instantiate the image experts from two types of models: a text-to-image retrieval model to provide an initial backbone, and image captioning models to provide supervision signal into unlabeled videos. We show that automatically labeling video frames with image captioning allows text-to-video retrieval training. This process adapts the features to the target domain at no manual annotation cost, consequently outperforming the strong zero-shot CLIP baseline. During training, we sample captions from multiple video frames that best match the visual content, and perform a temporal pooling over frame representations by scoring frames according to their relevance to each caption. We conduct extensive ablations to provide insights and demonstrate the effectiveness of this simple framework by outperforming the CLIP zero-shot baselines on text-to-video retrieval on three standard datasets, namely ActivityNet, MSR-VTT, and MSVD.","sentences":["We describe a protocol to study text-to-video retrieval training with unlabeled videos, where we assume (i) no access to labels for any videos, i.e., no access to the set of ground-truth captions, but (ii) access to labeled images in the form of text.","Using image expert models is a realistic scenario given that annotating images is cheaper therefore scalable, in contrast to expensive video labeling schemes.","Recently, zero-shot image experts such as CLIP have established a new strong baseline for video understanding tasks.","In this paper, we make use of this progress and instantiate the image experts from two types of models: a text-to-image retrieval model to provide an initial backbone, and image captioning models to provide supervision signal into unlabeled videos.","We show that automatically labeling video frames with image captioning allows text-to-video retrieval training.","This process adapts the features to the target domain at no manual annotation cost, consequently outperforming the strong zero-shot CLIP baseline.","During training, we sample captions from multiple video frames that best match the visual content, and perform a temporal pooling over frame representations by scoring frames according to their relevance to each caption.","We conduct extensive ablations to provide insights and demonstrate the effectiveness of this simple framework by outperforming the CLIP zero-shot baselines on text-to-video retrieval on three standard datasets, namely ActivityNet, MSR-VTT, and MSVD."],"url":"http://arxiv.org/abs/2404.17498v1","category":"cs.CV"}
{"created":"2024-04-26 15:51:20","title":"Q-Learning to navigate turbulence without a map","abstract":"We consider the problem of olfactory searches in a turbulent environment. We focus on agents that respond solely to odor stimuli, with no access to spatial perception nor prior information about the odor location. We ask whether navigation strategies to a target can be learned robustly within a sequential decision making framework. We develop a reinforcement learning algorithm using a small set of interpretable olfactory states and train it with realistic turbulent odor cues. By introducing a temporal memory, we demonstrate that two salient features of odor traces, discretized in few olfactory states, are sufficient to learn navigation in a realistic odor plume. Performance is dictated by the sparse nature of turbulent plumes. An optimal memory exists which ignores blanks within the plume and activates a recovery strategy outside the plume. We obtain the best performance by letting agents learn their recovery strategy and show that it is mostly casting cross wind, similar to behavior observed in flying insects. The optimal strategy is robust to substantial changes in the odor plumes, suggesting minor parameter tuning may be sufficient to adapt to different environments.","sentences":["We consider the problem of olfactory searches in a turbulent environment.","We focus on agents that respond solely to odor stimuli, with no access to spatial perception nor prior information about the odor location.","We ask whether navigation strategies to a target can be learned robustly within a sequential decision making framework.","We develop a reinforcement learning algorithm using a small set of interpretable olfactory states and train it with realistic turbulent odor cues.","By introducing a temporal memory, we demonstrate that two salient features of odor traces, discretized in few olfactory states, are sufficient to learn navigation in a realistic odor plume.","Performance is dictated by the sparse nature of turbulent plumes.","An optimal memory exists which ignores blanks within the plume and activates a recovery strategy outside the plume.","We obtain the best performance by letting agents learn their recovery strategy and show that it is mostly casting cross wind, similar to behavior observed in flying insects.","The optimal strategy is robust to substantial changes in the odor plumes, suggesting minor parameter tuning may be sufficient to adapt to different environments."],"url":"http://arxiv.org/abs/2404.17495v1","category":"physics.bio-ph"}
{"created":"2024-04-26 15:34:04","title":"Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation","abstract":"There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using the inverse of probability weighting (IPW). However, due to the numerically unstable IPW weights, they suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator via weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Experimental results show that by effectively correcting the weight values, our method outperforms the existing ones, including traditional weighting schemes.","sentences":["There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes.","Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes.","To avoid losing such predictive feature information, existing methods learn separate feature representations using the inverse of probability weighting (IPW).","However, due to the numerically unstable IPW weights, they suffer from estimation bias under a finite sample setup.","To develop a numerically robust estimator via weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion.","Experimental results show that by effectively correcting the weight values, our method outperforms the existing ones, including traditional weighting schemes."],"url":"http://arxiv.org/abs/2404.17483v1","category":"stat.ML"}
{"created":"2024-04-26 15:09:02","title":"Around the positive graph conjecture","abstract":"A graph $H$ is said to be positive if the homomorphism density $t_H(G)$ is non-negative for all weighted graphs $G$. The positive graph conjecture proposes a characterisation of such graphs, saying that a graph is positive if and only if it is symmetric, in the sense that it is formed by gluing two copies of some subgraph along an independent set. We prove several results relating to this conjecture. First, we make progress towards the conjecture itself by showing that any connected positive graph must have a vertex of even degree. We then make use of this result to identify some new counterexamples to the analogue of Sidorenko's conjecture for hypergraphs. In particular, we show that, for $r$ odd, every $r$-uniform tight cycle is a counterexample, generalising a recent result of Conlon, Lee and Sidorenko that dealt with the case $r=3$. Finally, we relate the positive graph conjecture to the emerging study of graph codes by showing that any positive graph has vanishing graph code density, thereby improving a result of Alon who proved the same result for symmetric graphs. Our proofs make use of a variety of tools and techniques, including the properties of independence polynomials, hypergraph quasirandomness and discrete Fourier analysis.","sentences":["A graph $H$ is said to be positive if the homomorphism density $t_H(G)$ is non-negative for all weighted graphs $G$. The positive graph conjecture proposes a characterisation of such graphs, saying that a graph is positive if and only if it is symmetric, in the sense that it is formed by gluing two copies of some subgraph along an independent set.","We prove several results relating to this conjecture.","First, we make progress towards the conjecture itself by showing that any connected positive graph must have a vertex of even degree.","We then make use of this result to identify some new counterexamples to the analogue of Sidorenko's conjecture for hypergraphs.","In particular, we show that, for $r$ odd, every $r$-uniform tight cycle is a counterexample, generalising a recent result of Conlon, Lee and Sidorenko that dealt with the case $r=3$. Finally, we relate the positive graph conjecture to the emerging study of graph codes by showing that any positive graph has vanishing graph code density, thereby improving a result of Alon who proved the same result for symmetric graphs.","Our proofs make use of a variety of tools and techniques, including the properties of independence polynomials, hypergraph quasirandomness and discrete Fourier analysis."],"url":"http://arxiv.org/abs/2404.17467v1","category":"math.CO"}
{"created":"2024-04-26 14:29:30","title":"Global data-driven determination of baryon transition form factors","abstract":"Hadronic resonances emerge from strong interactions encoding the dynamics of quarks and gluons. The structure of these resonances can be probed by virtual photons parameterized in transition form factors. In this study, twelve $N^*$ and $\\Delta$ transition form factors at the pole are extracted from data with the center-of-mass energy from $\\pi N$ threshold to $1.8\\,{\\rm GeV}$, and the photon virtuality $0\\leq Q^2/{\\rm GeV}^2\\leq 8$. For the first time, these results are determined from a simultaneous analysis of more than one state, i.e., $\\sim 10^5$ $\\pi N$, $\\eta N$, and $K\\Lambda$ electroproduction data. In addition, about $ 5\\cdot 10^4$ data in the hadronic sector as well as photoproduction serve as boundary conditions. For the $\\Delta(1232)$ and $N(1440)$ states our results are in qualitative agreement with previous studies, while the transition form factors at the poles of some higher excited states are estimated for the first time. Realistic uncertainties are determined by further exploring the parameter space.","sentences":["Hadronic resonances emerge from strong interactions encoding the dynamics of quarks and gluons.","The structure of these resonances can be probed by virtual photons parameterized in transition form factors.","In this study, twelve $N^*$ and $\\Delta$ transition form factors at the pole are extracted from data with the center-of-mass energy from $\\pi N$ threshold to $1.8\\,{\\rm GeV}$, and the photon virtuality $0\\leq Q^2/{\\rm GeV}^2\\leq","8$.","For the first time, these results are determined from a simultaneous analysis of more than one state, i.e., $\\sim 10^5$ $\\pi N$, $\\eta N$, and $K\\Lambda$ electroproduction data.","In addition, about $ 5\\cdot 10^4$ data in the hadronic sector as well as photoproduction serve as boundary conditions.","For the $\\Delta(1232)$ and $N(1440)$ states our results are in qualitative agreement with previous studies, while the transition form factors at the poles of some higher excited states are estimated for the first time.","Realistic uncertainties are determined by further exploring the parameter space."],"url":"http://arxiv.org/abs/2404.17444v1","category":"nucl-th"}
{"created":"2024-04-26 14:11:20","title":"Classical echoes of quantum boundary conditions","abstract":"We consider a non-relativistic particle in a one-dimensional box with all possible quantum boundary conditions that make the kinetic-energy operator selfadjoint. We determine the Wigner functions of the corresponding eigenfunctions and analyze in detail their classical limit in the high-energy regime. We show that the quantum boundary conditions split into two classes: all local and regular boundary conditions collapse to the same classical boundary condition, while singular non-local boundary conditions slightly persist in the classical limit.","sentences":["We consider a non-relativistic particle in a one-dimensional box with all possible quantum boundary conditions that make the kinetic-energy operator selfadjoint.","We determine the Wigner functions of the corresponding eigenfunctions and analyze in detail their classical limit in the high-energy regime.","We show that the quantum boundary conditions split into two classes: all local and regular boundary conditions collapse to the same classical boundary condition, while singular non-local boundary conditions slightly persist in the classical limit."],"url":"http://arxiv.org/abs/2404.17430v1","category":"quant-ph"}
{"created":"2024-04-26 14:03:55","title":"Cost-Sensitive Uncertainty-Based Failure Recognition for Object Detection","abstract":"Object detectors in real-world applications often fail to detect objects due to varying factors such as weather conditions and noisy input. Therefore, a process that mitigates false detections is crucial for both safety and accuracy. While uncertainty-based thresholding shows promise, previous works demonstrate an imperfect correlation between uncertainty and detection errors. This hinders ideal thresholding, prompting us to further investigate the correlation and associated cost with different types of uncertainty. We therefore propose a cost-sensitive framework for object detection tailored to user-defined budgets on the two types of errors, missing and false detections. We derive minimum thresholding requirements to prevent performance degradation and define metrics to assess the applicability of uncertainty for failure recognition. Furthermore, we automate and optimize the thresholding process to maximize the failure recognition rate w.r.t. the specified budget. Evaluation on three autonomous driving datasets demonstrates that our approach significantly enhances safety, particularly in challenging scenarios. Leveraging localization aleatoric uncertainty and softmax-based entropy only, our method boosts the failure recognition rate by 36-60\\% compared to conventional approaches. Code is available at https://mos-ks.github.io/publications.","sentences":["Object detectors in real-world applications often fail to detect objects due to varying factors such as weather conditions and noisy input.","Therefore, a process that mitigates false detections is crucial for both safety and accuracy.","While uncertainty-based thresholding shows promise, previous works demonstrate an imperfect correlation between uncertainty and detection errors.","This hinders ideal thresholding, prompting us to further investigate the correlation and associated cost with different types of uncertainty.","We therefore propose a cost-sensitive framework for object detection tailored to user-defined budgets on the two types of errors, missing and false detections.","We derive minimum thresholding requirements to prevent performance degradation and define metrics to assess the applicability of uncertainty for failure recognition.","Furthermore, we automate and optimize the thresholding process to maximize the failure recognition rate w.r.t.","the specified budget.","Evaluation on three autonomous driving datasets demonstrates that our approach significantly enhances safety, particularly in challenging scenarios.","Leveraging localization aleatoric uncertainty and softmax-based entropy only, our method boosts the failure recognition rate by 36-60\\% compared to conventional approaches.","Code is available at https://mos-ks.github.io/publications."],"url":"http://arxiv.org/abs/2404.17427v1","category":"cs.CV"}
{"created":"2024-04-26 13:23:50","title":"Probing intergalactic intergalactic magnetic fields with LOFAR LoTSS DR2 data","abstract":"We use Faraday rotation measurements from the latest catalog LoTSS DR2 from LOFAR to probe intergalactic magnetic fields. To identify the extragalactic component of the observed rotation measure (RM) we use two different techniques: residual rotation measure (RRM) and close radio pairs. For the RRM approach, we conclude that, despite smaller measurement errors in the LOFAR data, robust and conservative treatment of the systematic uncertainties in the Galactic contribution to RM results in the constraint on a homogeneous volume-filling magnetic field at the level 2.4 nG, slightly weaker than previous constraints from NVSS data, and does not allow to probe the presence of over-magnetized bubbles predicted by the AGN feedback model of the IllustrisTNG. Analyzing close radio pairs we found that in only 0.5% of our mock realizations of observed data, the expected contribution from the over-magnetized bubbles does not exceed LoTSS DR2 data.","sentences":["We use Faraday rotation measurements from the latest catalog LoTSS DR2 from LOFAR to probe intergalactic magnetic fields.","To identify the extragalactic component of the observed rotation measure (RM) we use two different techniques: residual rotation measure (RRM) and close radio pairs.","For the RRM approach, we conclude that, despite smaller measurement errors in the LOFAR data, robust and conservative treatment of the systematic uncertainties in the Galactic contribution to RM results in the constraint on a homogeneous volume-filling magnetic field at the level 2.4 nG, slightly weaker than previous constraints from NVSS data, and does not allow to probe the presence of over-magnetized bubbles predicted by the AGN feedback model of the IllustrisTNG.","Analyzing close radio pairs we found that in only 0.5% of our mock realizations of observed data, the expected contribution from the over-magnetized bubbles does not exceed LoTSS DR2 data."],"url":"http://arxiv.org/abs/2404.17402v1","category":"astro-ph.CO"}
{"created":"2024-04-26 13:21:30","title":"Evaluations of Machine Learning Privacy Defenses are Misleading","abstract":"Empirical defenses for machine learning privacy forgo the provable guarantees of differential privacy in the hope of achieving higher utility while resisting realistic adversaries. We identify severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions. In particular, we show that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines. In 5 case studies of empirical privacy defenses, we find that prior evaluations underestimate privacy leakage by an order of magnitude. Under our stronger evaluation, none of the empirical defenses we study are competitive with a properly tuned, high-utility DP-SGD baseline (with vacuous provable guarantees).","sentences":["Empirical defenses for machine learning privacy forgo the provable guarantees of differential privacy in the hope of achieving higher utility while resisting realistic adversaries.","We identify severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions.","In particular, we show that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines.","In 5 case studies of empirical privacy defenses, we find that prior evaluations underestimate privacy leakage by an order of magnitude.","Under our stronger evaluation, none of the empirical defenses we study are competitive with a properly tuned, high-utility DP-SGD baseline (with vacuous provable guarantees)."],"url":"http://arxiv.org/abs/2404.17399v1","category":"cs.CR"}
{"created":"2024-04-26 12:58:31","title":"Statistical theory of the broadband two plasmon decay instability","abstract":"There is renewed interest in direct-drive inertial confinement fusion, following the milestone December 2022 3.15 MJ ignition result on the National Ignition Facility. A key obstacle is the control of the two plasmon decay instability. Here, recent advances in inhomogeneous turbulence theory are applied to the broadband parametric instability problem for the first time. A novel dispersion relation is derived for the two plasmon decay in a uniform plasma valid under broad-bandwidth laser fields with arbitrary power spectra. The effects of temporal incoherence on the instability are then studied. In the limit of large bandwidth, the well-known scaling relations for the growth rate are recovered, but it is shown that the result is more sensitive to the spectral shape of the laser pulse rather than to its coherence time. The range of wavenumbers of the excited plasma waves is shown to be substantially broadened, suggesting that the absolute instability is favoured in regions further away from the quarter critical density. The intermediate bandwidth regime is explored numerically -- the growth rate is reduced to half its monochromatic value for laser intensities of $10^{15} \\, \\text{W}/\\text{cm}^{2}$ and relatively modest bandwidths of $5 \\, \\text{THz}$. The instability-quenching properties of a spectrum of discrete lines spread over some bandwidth have also been studied. The reduction in the growth rate is found to be somewhat lower compared to the continuous case but is still significant, despite the fact that, formally, the coherence time of such a laser pulse is infinite.","sentences":["There is renewed interest in direct-drive inertial confinement fusion, following the milestone December 2022 3.15 MJ ignition result on the National Ignition Facility.","A key obstacle is the control of the two plasmon decay instability.","Here, recent advances in inhomogeneous turbulence theory are applied to the broadband parametric instability problem for the first time.","A novel dispersion relation is derived for the two plasmon decay in a uniform plasma valid under broad-bandwidth laser fields with arbitrary power spectra.","The effects of temporal incoherence on the instability are then studied.","In the limit of large bandwidth, the well-known scaling relations for the growth rate are recovered, but it is shown that the result is more sensitive to the spectral shape of the laser pulse rather than to its coherence time.","The range of wavenumbers of the excited plasma waves is shown to be substantially broadened, suggesting that the absolute instability is favoured in regions further away from the quarter critical density.","The intermediate bandwidth regime is explored numerically -- the growth rate is reduced to half its monochromatic value for laser intensities of $10^{15} \\, \\text{W}/\\text{cm}^{2}$ and relatively modest bandwidths of $5 \\, \\text{THz}$. The instability-quenching properties of a spectrum of discrete lines spread over some bandwidth have also been studied.","The reduction in the growth rate is found to be somewhat lower compared to the continuous case but is still significant, despite the fact that, formally, the coherence time of such a laser pulse is infinite."],"url":"http://arxiv.org/abs/2404.17384v1","category":"physics.plasm-ph"}
{"created":"2024-04-26 12:48:14","title":"Properties of the complementarity set for the cone of copositive matrices","abstract":"For a proper cone $K$ and its dual cone $K^*$ in $\\mathbb R^n$, the complementarity set of $K$ is defined as   ${\\mathbb C}(K)=\\{(x,y): x\\in K,\\; y\\in K^*,\\, x^\\top y=0\\}$. It is known that ${\\mathbb C}(K)$ is an $n$-dimensional manifold in the space $\\mathbb R^{2n}$. If $ K$ is a symmetric cone, points in ${\\mathbb C}(K)$ must satisfy at least $n$ linearly independent bi-linear identities. Since this knowledge comes in handy when optimizing over such cones, it makes sense to search for similar relationships for non-symmetric cones.   In this paper, we study properties of the complementarity set for the dual cones of copositive and completely positive matrices. Despite these cones are of great interest due to their applications in optimization, they have not yet been sufficiently studied.","sentences":["For a proper cone $K$ and its dual cone $K^*$ in $\\mathbb R^n$, the complementarity set of $K$ is defined as   ${\\mathbb C}(K)=\\{(x,y): x\\in K,\\; y\\in K^*,\\,","x^\\top y=0\\}$.","It is known that ${\\mathbb C}(K)$ is an $n$-dimensional manifold in the space $\\mathbb R^{2n}$.","If $ K$ is a symmetric cone, points in ${\\mathbb C}(K)$ must satisfy at least $n$ linearly independent bi-linear identities.","Since this knowledge comes in handy when optimizing over such cones, it makes sense to search for similar relationships for non-symmetric cones.   ","In this paper, we study properties of the complementarity set for the dual cones of copositive and completely positive matrices.","Despite these cones are of great interest due to their applications in optimization, they have not yet been sufficiently studied."],"url":"http://arxiv.org/abs/2404.17375v1","category":"math.OC"}
{"created":"2024-04-26 12:46:31","title":"Inefficient star formation in high Mach number environments. II. Numerical simulations and comparison with analytical models","abstract":"Predicting the star formation rate (SFR) in galaxies is crucial to understand their evolution and morphology. To do so requires a fine understanding of how dense structures of gas are created and collapse. In that, turbulence and gravity play a major role. Within the gravo-turbulent framework, we assume that turbulence shapes the ISM, creating density fluctuations that, if gravitationally unstable, will collapse and form stars. The goal of this work is to quantify how different regimes of turbulence, characterized by the strength and compressibility of the driving, shape the density field. We are interested in the outcome in terms of SFR and how it compares with existing analytical models for the SFR. We run a series of hydrodynamical simulations of turbulent gas. The simulations are first conducted without gravity, so that the density and velocity are shaped by the turbulence driving. Gravity is then switched on, and the SFR is measured and compared with analytical models. The physics included in these simulations is very close to the one assumed in the classical gravo-turbulent SFR analytical models, which makes the comparison straightforward. We found that the existing analytical models convincingly agree with simulations at low Mach number, but we measure a much lower SFR in the simulation with a high Mach number. We develop, in a companion paper, an updated physically-motivated SFR model that reproduces well the inefficient high Mach regime of the simulations. Our work demonstrates that accurate estimations of the turbulent-driven replenishment time of dense structures and the dense gas spatial distribution are necessary to correctly predict the SFR in the high Mach regime. The inefficient high-Mach regime is a possible explanation for the low SFR found in dense and turbulent environments such as the centers of our Milky Way and other galaxies.","sentences":["Predicting the star formation rate (SFR) in galaxies is crucial to understand their evolution and morphology.","To do so requires a fine understanding of how dense structures of gas are created and collapse.","In that, turbulence and gravity play a major role.","Within the gravo-turbulent framework, we assume that turbulence shapes the ISM, creating density fluctuations that, if gravitationally unstable, will collapse and form stars.","The goal of this work is to quantify how different regimes of turbulence, characterized by the strength and compressibility of the driving, shape the density field.","We are interested in the outcome in terms of SFR and how it compares with existing analytical models for the SFR.","We run a series of hydrodynamical simulations of turbulent gas.","The simulations are first conducted without gravity, so that the density and velocity are shaped by the turbulence driving.","Gravity is then switched on, and the SFR is measured and compared with analytical models.","The physics included in these simulations is very close to the one assumed in the classical gravo-turbulent SFR analytical models, which makes the comparison straightforward.","We found that the existing analytical models convincingly agree with simulations at low Mach number, but we measure a much lower SFR in the simulation with a high Mach number.","We develop, in a companion paper, an updated physically-motivated SFR model that reproduces well the inefficient high Mach regime of the simulations.","Our work demonstrates that accurate estimations of the turbulent-driven replenishment time of dense structures and the dense gas spatial distribution are necessary to correctly predict the SFR in the high Mach regime.","The inefficient high-Mach regime is a possible explanation for the low SFR found in dense and turbulent environments such as the centers of our Milky Way and other galaxies."],"url":"http://arxiv.org/abs/2404.17374v1","category":"astro-ph.GA"}
{"created":"2024-04-26 12:16:08","title":"Adversarial Consistency and the Uniqueness of the Adversarial Bayes Classifier","abstract":"Adversarial training is a common technique for learning robust classifiers. Prior work showed that convex surrogate losses are not statistically consistent in the adversarial context -- or in other words, a minimizing sequence of the adversarial surrogate risk will not necessarily minimize the adversarial classification error. We connect the consistency of adversarial surrogate losses to properties of minimizers to the adversarial classification risk, known as \\emph{adversarial Bayes classifiers}. Specifically, under reasonable distributional assumptions, a convex loss is statistically consistent for adversarial learning iff the adversarial Bayes classifier satisfies a certain notion of uniqueness.","sentences":["Adversarial training is a common technique for learning robust classifiers.","Prior work showed that convex surrogate losses are not statistically consistent in the adversarial context -- or in other words, a minimizing sequence of the adversarial surrogate risk will not necessarily minimize the adversarial classification error.","We connect the consistency of adversarial surrogate losses to properties of minimizers to the adversarial classification risk, known as \\emph{adversarial Bayes classifiers}.","Specifically, under reasonable distributional assumptions, a convex loss is statistically consistent for adversarial learning iff the adversarial Bayes classifier satisfies a certain notion of uniqueness."],"url":"http://arxiv.org/abs/2404.17358v1","category":"cs.LG"}
{"created":"2024-04-26 12:07:32","title":"Large-scale atomistic study of plasticity in amorphous gallium oxide with a machine-learning potential","abstract":"Compared to the widely investigated crystalline polymorphs of gallium oxide (Ga2O3), knowledge about its amorphous state is still limited. With the help of a machine-learning interatomic potential, we conducted large-scale atomistic simulations to investigate the glass transition and mechanical behavior of amorphous Ga2O3 (a-Ga2O3). During the quenching simulations, amorphization of gallium oxide melt is observed at ultrahigh cooling rates, including a distinct glass transition. The final densities at room temperature have up to 4% variance compared to experiments. The glass transition temperature is evaluated to range from 1234 K to 1348 K at different cooling rates. Structural analysis of the amorphous structure shows evident similarities in structural properties between a-Ga2O3 and amorphous alumina (a-Al2O3), such as radial distribution function, coordination distribution, and bond angle distribution. An amorphous gallium oxide structure that contains approximately one million atoms is prepared for the tension simulation. A highly plastic behavior is observed at room temperature in the tension simulations, comparable to amorphous alumina. With quantitative characterization methods, we show that a-Ga2O3 can possibly has a higher nucleation rate of localized plastic strain events compared to a-Al2O3, which can increase the material's resistance to shear banding formation during deformation.","sentences":["Compared to the widely investigated crystalline polymorphs of gallium oxide (Ga2O3), knowledge about its amorphous state is still limited.","With the help of a machine-learning interatomic potential, we conducted large-scale atomistic simulations to investigate the glass transition and mechanical behavior of amorphous Ga2O3 (a-Ga2O3).","During the quenching simulations, amorphization of gallium oxide melt is observed at ultrahigh cooling rates, including a distinct glass transition.","The final densities at room temperature have up to 4% variance compared to experiments.","The glass transition temperature is evaluated to range from 1234 K to 1348 K at different cooling rates.","Structural analysis of the amorphous structure shows evident similarities in structural properties between a-Ga2O3 and amorphous alumina (a-Al2O3), such as radial distribution function, coordination distribution, and bond angle distribution.","An amorphous gallium oxide structure that contains approximately one million atoms is prepared for the tension simulation.","A highly plastic behavior is observed at room temperature in the tension simulations, comparable to amorphous alumina.","With quantitative characterization methods, we show that a-Ga2O3 can possibly has a higher nucleation rate of localized plastic strain events compared to a-Al2O3, which can increase the material's resistance to shear banding formation during deformation."],"url":"http://arxiv.org/abs/2404.17353v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-26 11:32:27","title":"From infinite to infinitesimal: Using the Universe as a dataset to probe Casimir corrections to the vacuum energy from fields inhabiting the dark dimension","abstract":"Promptly after high-resolution experiments harbinger the field of precision cosmology low- and high-redshift observations abruptly gave rise to a tension in the measurement of the present-day expansion rate of the Universe ($H_0$) and the clustering of matter ($S_8$). The statistically significant discrepancies between the locally measured values of $H_0$ and $S_8$ and the ones inferred from observations of the cosmic microwave background assuming the canonical $\\Lambda$ cold dark matter (CDM) cosmological model have become a new cornerstone of theoretical physics. $\\Lambda_s$CDM is one of the many beyond Standard Model setups that have been proposed to simultaneously resolve the cosmological tensions. This setup relies on an empirical conjecture, which postulates that $\\Lambda$ switched sign (from negative to positive) at a critical redshift $z_c \\sim 2$. We reexamine a stringy model that can describe the transition in the vacuum energy hypothesized in $\\Lambda_s$CDM. The model makes use of the Casimir forces driven by fields inhabiting the incredible bulk of the dark dimension scenario. Unlike the $\\Lambda_s$CDM setup the model deviates from $\\Lambda$CDM in the early universe due to the existence of relativistic neutrino-like species. Using the Boltzmann solver CLASS in combination with MontePython we confront predictions of the stringy model to experimental data (from the Planck mission, Pantheon+ supernova type Ia, BAO, and KiDS-1000). We show that the string-inspired model provides a satisfactory fit to the data and can resolve the cosmological tensions.","sentences":["Promptly after high-resolution experiments harbinger the field of precision cosmology low- and high-redshift observations abruptly gave rise to a tension in the measurement of the present-day expansion rate of the Universe ($H_0$) and the clustering of matter ($S_8$).","The statistically significant discrepancies between the locally measured values of $H_0$ and $S_8$ and the ones inferred from observations of the cosmic microwave background assuming the canonical $\\Lambda$ cold dark matter (CDM) cosmological model have become a new cornerstone of theoretical physics.","$\\Lambda_s$CDM is one of the many beyond Standard Model setups that have been proposed to simultaneously resolve the cosmological tensions.","This setup relies on an empirical conjecture, which postulates that $\\Lambda$ switched sign (from negative to positive) at a critical redshift $z_c \\sim 2$.","We reexamine a stringy model that can describe the transition in the vacuum energy hypothesized in $\\Lambda_s$CDM.","The model makes use of the Casimir forces driven by fields inhabiting the incredible bulk of the dark dimension scenario.","Unlike the $\\Lambda_s$CDM setup the model deviates from $\\Lambda$CDM in the early universe due to the existence of relativistic neutrino-like species.","Using the Boltzmann solver CLASS in combination with MontePython we confront predictions of the stringy model to experimental data (from the Planck mission, Pantheon+ supernova type Ia, BAO, and KiDS-1000).","We show that the string-inspired model provides a satisfactory fit to the data and can resolve the cosmological tensions."],"url":"http://arxiv.org/abs/2404.17334v1","category":"astro-ph.CO"}
{"created":"2024-04-26 11:22:57","title":"REvoLd: Ultra-Large Library Screening with an Evolutionary Algorithm in Rosetta","abstract":"Ultra-large make-on-demand compound libraries now contain billions of readily available compounds. This represents a golden opportunity for in-silico drug discovery. One challenge, however, is the time and computational cost of an exhaustive screen of such large libraries when receptor flexibility is taken into account. We propose an evolutionary algorithm to search combinatorial make-on-demand chemical space efficiently without enumerating all molecules. We exploit the feature of make-on-demand compound libraries, namely that they are constructed from lists of substrates and chemical reactions. Our novel algorithm RosettaEvolutionaryLigand (REvoLd) explores the vast search space of combinatorial libraries for protein-ligand docking with full ligand and receptor flexibility through RosettaLigand. A benchmark of REvoLd on five drug targets showed improvements in hit rates by factors between 869 and 1,622 compared to random selections. REvoLd is available as an application within the Rosetta software suite.","sentences":["Ultra-large make-on-demand compound libraries now contain billions of readily available compounds.","This represents a golden opportunity for in-silico drug discovery.","One challenge, however, is the time and computational cost of an exhaustive screen of such large libraries when receptor flexibility is taken into account.","We propose an evolutionary algorithm to search combinatorial make-on-demand chemical space efficiently without enumerating all molecules.","We exploit the feature of make-on-demand compound libraries, namely that they are constructed from lists of substrates and chemical reactions.","Our novel algorithm RosettaEvolutionaryLigand (REvoLd) explores the vast search space of combinatorial libraries for protein-ligand docking with full ligand and receptor flexibility through RosettaLigand.","A benchmark of REvoLd on five drug targets showed improvements in hit rates by factors between 869 and 1,622 compared to random selections.","REvoLd is available as an application within the Rosetta software suite."],"url":"http://arxiv.org/abs/2404.17329v1","category":"q-bio.QM"}
{"created":"2024-04-26 10:56:45","title":"Performance Bounds of Near-Field Sensing with Circular Arrays","abstract":"The performance bounds of near-field sensing are studied for circular arrays, focusing on the impact of bandwidth and array size. The closed-form Cramer-Rao bound (CRBs) for angle and distance estimation are derived, revealing the scaling laws of the CRBs with bandwidth and array size. Contrary to expectations, enlarging array size does not always enhance sensing performance. Furthermore, the asymptotic CRBs are analyzed under different conditions, unveiling that the derived expressions include the existing results as special cases. Finally, the derived expressions are validated through numerical results.","sentences":["The performance bounds of near-field sensing are studied for circular arrays, focusing on the impact of bandwidth and array size.","The closed-form Cramer-Rao bound (CRBs) for angle and distance estimation are derived, revealing the scaling laws of the CRBs with bandwidth and array size.","Contrary to expectations, enlarging array size does not always enhance sensing performance.","Furthermore, the asymptotic CRBs are analyzed under different conditions, unveiling that the derived expressions include the existing results as special cases.","Finally, the derived expressions are validated through numerical results."],"url":"http://arxiv.org/abs/2404.17318v1","category":"cs.IT"}
{"created":"2024-04-26 09:56:13","title":"Machine Learning Recognition of hybrid lead halide perovskites and perovskite-related structures out of X-ray diffraction patterns","abstract":"Identification of crystal structures is a crucial stage in the exploration of novel functional materials. This procedure is usually time-consuming and can be false-positive or false-negative. This necessitates a significant level of expert proficiency in the field of crystallography and, especially, requires deep experience in perovskite - related structures of hybrid perovskites. Our work is devoted to the machine learning classification of structure types of hybrid lead halides based on available X-ray diffraction data. Here, we proposed a simple approach to quickly identify of dimensionality of inorganic substructures, types of lead halide polyhedra connectivity and structure types using common powder XRD data and ML - decision tree classification model. The average accuracy of our ML algorithm in predicting the dimensionality of inorganic substructure, type of connection of lead halide and inorganic substructure topology by theoretically calculated XRD pattern among 14 most common structure types reaches 0.86+-0.05, 0.827+-0.028 and 0.71+-0.05, respectively. The validation of our decision tree classification ML model on experimental XRD data shows the accuracies of 1.0 and 0.82 for the dimension and structure type prediction. Thus, our approach can significantly simplify and accelerate the interpretation of highly complicated XRD data for hybrid lead halides.","sentences":["Identification of crystal structures is a crucial stage in the exploration of novel functional materials.","This procedure is usually time-consuming and can be false-positive or false-negative.","This necessitates a significant level of expert proficiency in the field of crystallography and, especially, requires deep experience in perovskite - related structures of hybrid perovskites.","Our work is devoted to the machine learning classification of structure types of hybrid lead halides based on available X-ray diffraction data.","Here, we proposed a simple approach to quickly identify of dimensionality of inorganic substructures, types of lead halide polyhedra connectivity and structure types using common powder XRD data and ML - decision tree classification model.","The average accuracy of our ML algorithm in predicting the dimensionality of inorganic substructure, type of connection of lead halide and inorganic substructure topology by theoretically calculated XRD pattern among 14 most common structure types reaches 0.86+-0.05, 0.827+-0.028 and 0.71+-0.05, respectively.","The validation of our decision tree classification ML model on experimental XRD data shows the accuracies of 1.0 and 0.82 for the dimension and structure type prediction.","Thus, our approach can significantly simplify and accelerate the interpretation of highly complicated XRD data for hybrid lead halides."],"url":"http://arxiv.org/abs/2404.17294v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-26 09:38:27","title":"Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM","abstract":"Retrieval-augmented language models have exhibited promising performance across various areas of natural language processing (NLP), including fact-critical tasks. However, due to the black-box nature of advanced large language models (LLMs) and the non-retrieval-oriented supervision signal of specific tasks, the training of retrieval model faces significant challenges under the setting of black-box LLM. We propose an approach leveraging Fine-grained Feedback with Reinforcement Retrieval (FFRR) to enhance fact-checking on news claims by using black-box LLM. FFRR adopts a two-level strategy to gather fine-grained feedback from the LLM, which serves as a reward for optimizing the retrieval policy, by rating the retrieved documents based on the non-retrieval ground truth of the task. We evaluate our model on two public datasets for real-world news claim verification, and the results demonstrate that FFRR achieves significant improvements over strong LLM-enabled and non-LLM baselines.","sentences":["Retrieval-augmented language models have exhibited promising performance across various areas of natural language processing (NLP), including fact-critical tasks.","However, due to the black-box nature of advanced large language models (LLMs) and the non-retrieval-oriented supervision signal of specific tasks, the training of retrieval model faces significant challenges under the setting of black-box LLM.","We propose an approach leveraging Fine-grained Feedback with Reinforcement Retrieval (FFRR) to enhance fact-checking on news claims by using black-box LLM.","FFRR adopts a two-level strategy to gather fine-grained feedback from the LLM, which serves as a reward for optimizing the retrieval policy, by rating the retrieved documents based on the non-retrieval ground truth of the task.","We evaluate our model on two public datasets for real-world news claim verification, and the results demonstrate that FFRR achieves significant improvements over strong LLM-enabled and non-LLM baselines."],"url":"http://arxiv.org/abs/2404.17283v1","category":"cs.CL"}
{"created":"2024-04-26 09:34:52","title":"Bipartite powers of some classes of bipartite graphs","abstract":"Graph powers are a well-studied concept in graph theory. Analogous to graph powers, Chandran et al.[3] introduced the concept of bipartite powers for bipartite graphs. In this paper, we will demonstrate that some well-known classes of bipartite graphs, namely the interval bigraphs, proper interval bigraphs, and bigraphs of Ferrers dimension 2, are closed under the operation of taking bipartite powers. Finally, we define strongly closed property for bipartite graphs under powers and have shown that the class of chordal bipartite graphs is strongly closed under powers.","sentences":["Graph powers are a well-studied concept in graph theory.","Analogous to graph powers, Chandran et al.[3] introduced the concept of bipartite powers for bipartite graphs.","In this paper, we will demonstrate that some well-known classes of bipartite graphs, namely the interval bigraphs, proper interval bigraphs, and bigraphs of Ferrers dimension 2, are closed under the operation of taking bipartite powers.","Finally, we define strongly closed property for bipartite graphs under powers and have shown that the class of chordal bipartite graphs is strongly closed under powers."],"url":"http://arxiv.org/abs/2404.17279v1","category":"math.CO"}
{"created":"2024-04-26 09:29:55","title":"Adversarial Reweighting with $\u03b1$-Power Maximization for Domain Adaptation","abstract":"The practical Domain Adaptation (DA) tasks, e.g., Partial DA (PDA), open-set DA, universal DA, and test-time adaptation, have gained increasing attention in the machine learning community. In this paper, we propose a novel approach, dubbed Adversarial Reweighting with $\\alpha$-Power Maximization (ARPM), for PDA where the source domain contains private classes absent in target domain. In ARPM, we propose a novel adversarial reweighting model that adversarially learns to reweight source domain data to identify source-private class samples by assigning smaller weights to them, for mitigating potential negative transfer. Based on the adversarial reweighting, we train the transferable recognition model on the reweighted source distribution to be able to classify common class data. To reduce the prediction uncertainty of the recognition model on the target domain for PDA, we present an $\\alpha$-power maximization mechanism in ARPM, which enriches the family of losses for reducing the prediction uncertainty for PDA. Extensive experimental results on five PDA benchmarks, i.e., Office-31, Office-Home, VisDA-2017, ImageNet-Caltech, and DomainNet, show that our method is superior to recent PDA methods. Ablation studies also confirm the effectiveness of components in our approach. To theoretically analyze our method, we deduce an upper bound of target domain expected error for PDA, which is approximately minimized in our approach. We further extend ARPM to open-set DA, universal DA, and test time adaptation, and verify the usefulness through experiments.","sentences":["The practical Domain Adaptation (DA) tasks, e.g., Partial DA (PDA), open-set DA, universal DA, and test-time adaptation, have gained increasing attention in the machine learning community.","In this paper, we propose a novel approach, dubbed Adversarial Reweighting with $\\alpha$-Power Maximization (ARPM), for PDA where the source domain contains private classes absent in target domain.","In ARPM, we propose a novel adversarial reweighting model that adversarially learns to reweight source domain data to identify source-private class samples by assigning smaller weights to them, for mitigating potential negative transfer.","Based on the adversarial reweighting, we train the transferable recognition model on the reweighted source distribution to be able to classify common class data.","To reduce the prediction uncertainty of the recognition model on the target domain for PDA, we present an $\\alpha$-power maximization mechanism in ARPM, which enriches the family of losses for reducing the prediction uncertainty for PDA.","Extensive experimental results on five PDA benchmarks, i.e., Office-31, Office-Home, VisDA-2017, ImageNet-Caltech, and DomainNet, show that our method is superior to recent PDA methods.","Ablation studies also confirm the effectiveness of components in our approach.","To theoretically analyze our method, we deduce an upper bound of target domain expected error for PDA, which is approximately minimized in our approach.","We further extend ARPM to open-set DA, universal DA, and test time adaptation, and verify the usefulness through experiments."],"url":"http://arxiv.org/abs/2404.17275v1","category":"cs.CV"}
{"created":"2024-04-26 09:18:54","title":"To democratize research with sensitive data, we should make synthetic data more accessible","abstract":"For over 30 years, synthetic data has been heralded as a promising solution to make sensitive datasets accessible. However, despite much research effort and several high-profile use-cases, the widespread adoption of synthetic data as a tool for open, accessible, reproducible research with sensitive data is still a distant dream. In this opinion, Erik-Jan van Kesteren, head of the ODISSEI Social Data Science team, argues that in order to progress towards widespread adoption of synthetic data as a privacy enhancing technology, the data science research community should shift focus away from developing better synthesis methods: instead, it should develop accessible tools, educate peers, and publish small-scale case studies.","sentences":["For over 30 years, synthetic data has been heralded as a promising solution to make sensitive datasets accessible.","However, despite much research effort and several high-profile use-cases, the widespread adoption of synthetic data as a tool for open, accessible, reproducible research with sensitive data is still a distant dream.","In this opinion, Erik-Jan van Kesteren, head of the ODISSEI Social Data Science team, argues that in order to progress towards widespread adoption of synthetic data as a privacy enhancing technology, the data science research community should shift focus away from developing better synthesis methods: instead, it should develop accessible tools, educate peers, and publish small-scale case studies."],"url":"http://arxiv.org/abs/2404.17271v1","category":"stat.OT"}
{"created":"2024-04-26 08:41:55","title":"Making Better Use of Unlabelled Data in Bayesian Active Learning","abstract":"Fully supervised models are predominant in Bayesian active learning. We argue that their neglect of the information present in unlabelled data harms not just predictive performance but also decisions about what data to acquire. Our proposed solution is a simple framework for semi-supervised Bayesian active learning. We find it produces better-performing models than either conventional Bayesian active learning or semi-supervised learning with randomly acquired data. It is also easier to scale up than the conventional approach. As well as supporting a shift towards semi-supervised models, our findings highlight the importance of studying models and acquisition methods in conjunction.","sentences":["Fully supervised models are predominant in Bayesian active learning.","We argue that their neglect of the information present in unlabelled data harms not just predictive performance but also decisions about what data to acquire.","Our proposed solution is a simple framework for semi-supervised Bayesian active learning.","We find it produces better-performing models than either conventional Bayesian active learning or semi-supervised learning with randomly acquired data.","It is also easier to scale up than the conventional approach.","As well as supporting a shift towards semi-supervised models, our findings highlight the importance of studying models and acquisition methods in conjunction."],"url":"http://arxiv.org/abs/2404.17249v1","category":"cs.LG"}
{"created":"2024-04-26 08:41:21","title":"Analytical derivation and expansion of the anti-Kibble-Zurek scaling in the transverse field Ising model","abstract":"A defect density which quantifies the deviation from the spin ground state characterizes non-equilibrium dynamics during phase transitions. The widely recognized Kibble-Zurek scaling predicts how the defect density evolves during phase transitions. However, it can be perturbed by noise, leading to anti-Kibble-Zurek scaling. In this research, we analytically investigate the effect of Gaussian white noise on the transition probabilities of the Landau-Zener model. We apply this model to the one-dimensional transverse field Ising model and derive an analytical approximation for the defect density. Our analysis reveals that under small noise conditions, the model follows an anti-Kibble-Zurek scaling. As the noise increases, a new scaling behavior emerges, showing higher accuracy than previously reported. Furthermore, we identify the parameters that optimize the defect density based on the new scaling. This allows for the refinement of optimized parameters with greater precision and provides further validations of previously established scaling.","sentences":["A defect density which quantifies the deviation from the spin ground state characterizes non-equilibrium dynamics during phase transitions.","The widely recognized Kibble-Zurek scaling predicts how the defect density evolves during phase transitions.","However, it can be perturbed by noise, leading to anti-Kibble-Zurek scaling.","In this research, we analytically investigate the effect of Gaussian white noise on the transition probabilities of the Landau-Zener model.","We apply this model to the one-dimensional transverse field Ising model and derive an analytical approximation for the defect density.","Our analysis reveals that under small noise conditions, the model follows an anti-Kibble-Zurek scaling.","As the noise increases, a new scaling behavior emerges, showing higher accuracy than previously reported.","Furthermore, we identify the parameters that optimize the defect density based on the new scaling.","This allows for the refinement of optimized parameters with greater precision and provides further validations of previously established scaling."],"url":"http://arxiv.org/abs/2404.17247v1","category":"quant-ph"}
{"created":"2024-04-26 17:17:20","title":"Towards determining the (2+1)-dimensional Quantum Electrodynamics running coupling with Monte Carlo and quantum computing methods","abstract":"In this paper, we examine a compact $U(1)$ lattice gauge theory in $(2+1)$ dimensions and present a strategy for studying the running coupling and extracting the non-perturbative $\\Lambda$-parameter. To this end, we combine Monte Carlo simulations and quantum computing, where the former can be used to determine the numerical value of the lattice spacing $a$, and the latter allows for reaching the perturbative regime at very small values of the bare coupling and, correspondingly, small values of $a$. The methodology involves a series of sequential steps (i.e., the step scaling function) to bridge results from small lattice spacings to non-perturbative large-scale lattice calculations. To address the model on current and near-future quantum devices, we propose variational Ansatz circuits adapted to gauge degrees of freedom. Focusing on the pure gauge case, we demonstrate that these quantum circuits are able to capture the relevant physics by studying the expectation value of the plaquette operator, for matching with corresponding Monte Carlo simulations. We also present results for the static potential and static force, which can be related to the renormalized coupling. The procedure outlined in this work can be extended to Abelian and non-Abelian lattice gauge theories with matter fields and might provide a way towards studying lattice quantum chromodynamics utilizing both quantum and classical methods.","sentences":["In this paper, we examine a compact $U(1)$ lattice gauge theory in $(2+1)$ dimensions and present a strategy for studying the running coupling and extracting the non-perturbative $\\Lambda$-parameter.","To this end, we combine Monte Carlo simulations and quantum computing, where the former can be used to determine the numerical value of the lattice spacing $a$, and the latter allows for reaching the perturbative regime at very small values of the bare coupling and, correspondingly, small values of $a$. The methodology involves a series of sequential steps (i.e., the step scaling function) to bridge results from small lattice spacings to non-perturbative large-scale lattice calculations.","To address the model on current and near-future quantum devices, we propose variational Ansatz circuits adapted to gauge degrees of freedom.","Focusing on the pure gauge case, we demonstrate that these quantum circuits are able to capture the relevant physics by studying the expectation value of the plaquette operator, for matching with corresponding Monte Carlo simulations.","We also present results for the static potential and static force, which can be related to the renormalized coupling.","The procedure outlined in this work can be extended to Abelian and non-Abelian lattice gauge theories with matter fields and might provide a way towards studying lattice quantum chromodynamics utilizing both quantum and classical methods."],"url":"http://arxiv.org/abs/2404.17545v1","category":"hep-lat"}
{"created":"2024-04-26 14:35:24","title":"The Most Common Habitable Planets III -- Modeling Temperature Forcing and Surface Conditions on Rocky Exoplanets and Exomoons","abstract":"Small rocky planets, as well as larger planets that suffered extensive volatile loss, tend to be drier and have thinner atmospheres as compared to Earth. Such planets probably outnumber worlds better endowed with volatiles, being the most common habitable planets. For the subgroup of fast rotators following eccentric orbits, atmospheres suffer radiative forcing and their heat capacity provides a method for gauging atmospheric thickness and surface conditions. We further explore the model presented in a previous paper and apply it to real and hypothetical exoplanets in the habitable zone of various classes of stars, simulating atmospheric and orbital characteristics. For planetary eccentricities e ~0.3, the forcing-induced hypothetical temperature variation would reach ~80 K for airless planets and ~10 K for planets with substantial atmospheres. For Kepler-186 f and Kepler-442 b, assuming e ~0.1, temperature variations can reach ~24 K. We also consider habitable exomoons in circular orbits around gas giants within the habitable zone, which suffer radiative forcing due to their epicyclic motion. We study several combinations of parameters for the characterization of planets (mass, eccentricity and semi-major axis) and exomoons (mass, orbital radius, albedo and atmospheric characteristics) for different stellar types. For e ~0.3, exomoon temperature varies up to ~90 K, while for ~0.6 variations can reach ~200 K. Such exomoons may plausibly retain their volatiles by continued volcanic activity fueled by tidal dissipation. Although currently undetectable, such effects might be within reach of future ELT-class telescopes and space missions with mid-infrared and coronagraphic capabilities.","sentences":["Small rocky planets, as well as larger planets that suffered extensive volatile loss, tend to be drier and have thinner atmospheres as compared to Earth.","Such planets probably outnumber worlds better endowed with volatiles, being the most common habitable planets.","For the subgroup of fast rotators following eccentric orbits, atmospheres suffer radiative forcing and their heat capacity provides a method for gauging atmospheric thickness and surface conditions.","We further explore the model presented in a previous paper and apply it to real and hypothetical exoplanets in the habitable zone of various classes of stars, simulating atmospheric and orbital characteristics.","For planetary eccentricities e ~0.3, the forcing-induced hypothetical temperature variation would reach ~80 K for airless planets and ~10 K for planets with substantial atmospheres.","For Kepler-186 f and Kepler-442 b, assuming e ~0.1, temperature variations can reach ~24 K. We also consider habitable exomoons in circular orbits around gas giants within the habitable zone, which suffer radiative forcing due to their epicyclic motion.","We study several combinations of parameters for the characterization of planets (mass, eccentricity and semi-major axis) and exomoons (mass, orbital radius, albedo and atmospheric characteristics) for different stellar types.","For e ~0.3, exomoon temperature varies up to ~90 K, while for ~0.6 variations can reach ~200 K. Such exomoons may plausibly retain their volatiles by continued volcanic activity fueled by tidal dissipation.","Although currently undetectable, such effects might be within reach of future ELT-class telescopes and space missions with mid-infrared and coronagraphic capabilities."],"url":"http://arxiv.org/abs/2404.17448v1","category":"astro-ph.EP"}
{"created":"2024-04-26 12:56:16","title":"Frequency-Guided Multi-Level Human Action Anomaly Detection with Normalizing Flows","abstract":"We introduce the task of human action anomaly detection (HAAD), which aims to identify anomalous motions in an unsupervised manner given only the pre-determined normal category of training action samples. Compared to prior human-related anomaly detection tasks which primarily focus on unusual events from videos, HAAD involves the learning of specific action labels to recognize semantically anomalous human behaviors. To address this task, we propose a normalizing flow (NF)-based detection framework where the sample likelihood is effectively leveraged to indicate anomalies. As action anomalies often occur in some specific body parts, in addition to the full-body action feature learning, we incorporate extra encoding streams into our framework for a finer modeling of body subsets. Our framework is thus multi-level to jointly discover global and local motion anomalies. Furthermore, to show awareness of the potentially jittery data during recording, we resort to discrete cosine transformation by converting the action samples from the temporal to the frequency domain to mitigate the issue of data instability. Extensive experimental results on two human action datasets demonstrate that our method outperforms the baselines formed by adapting state-of-the-art human activity AD approaches to our task of HAAD.","sentences":["We introduce the task of human action anomaly detection (HAAD), which aims to identify anomalous motions in an unsupervised manner given only the pre-determined normal category of training action samples.","Compared to prior human-related anomaly detection tasks which primarily focus on unusual events from videos, HAAD involves the learning of specific action labels to recognize semantically anomalous human behaviors.","To address this task, we propose a normalizing flow (NF)-based detection framework where the sample likelihood is effectively leveraged to indicate anomalies.","As action anomalies often occur in some specific body parts, in addition to the full-body action feature learning, we incorporate extra encoding streams into our framework for a finer modeling of body subsets.","Our framework is thus multi-level to jointly discover global and local motion anomalies.","Furthermore, to show awareness of the potentially jittery data during recording, we resort to discrete cosine transformation by converting the action samples from the temporal to the frequency domain to mitigate the issue of data instability.","Extensive experimental results on two human action datasets demonstrate that our method outperforms the baselines formed by adapting state-of-the-art human activity AD approaches to our task of HAAD."],"url":"http://arxiv.org/abs/2404.17381v1","category":"cs.CV"}
{"created":"2024-04-26 12:21:56","title":"Relations between Kondratiev spaces and refined localization Triebel-Lizorkin spaces","abstract":"We investigate the close relation between certain weighted Sobolev spaces (Kondratiev spaces) and refined localization spaces from introduced by Triebel [39,40]. In particular, using a characterization for refined localization spaces from Scharf [32], we considerably improve an embedding from Hansen [17]. This embedding is of special interest in connection with convergence rates for adaptive approximation schemes.","sentences":["We investigate the close relation between certain weighted Sobolev spaces (Kondratiev spaces) and refined localization spaces from introduced by Triebel","[39,40].","In particular, using a characterization for refined localization spaces from Scharf [32], we considerably improve an embedding from Hansen [17].","This embedding is of special interest in connection with convergence rates for adaptive approximation schemes."],"url":"http://arxiv.org/abs/2404.17359v1","category":"math.AP"}
{"created":"2024-04-26 16:37:59","title":"Manin pairs and moment maps revisited","abstract":"The notion of quasi-Poisson $G$-spaces with $D/G$-valued moment maps was introduced by Alekseev and Kosmann-Schwarzbach in 1999. Our main result is a \\emph{Lifting Theorem}, establishing a bijective correspondence between the categories of quasi-Poisson $G$-spaces with $D/G$-valued moment maps and of quasi-Poisson $G\\times G$-spaces with $D$-valued moment maps. Using this result, we give simple constructions of fusion and conjugation for these spaces, and new examples coming from moduli spaces.","sentences":["The notion of quasi-Poisson $G$-spaces with $D/G$-valued moment maps was introduced by Alekseev and Kosmann-Schwarzbach in 1999.","Our main result is a \\emph{Lifting Theorem}, establishing a bijective correspondence between the categories of quasi-Poisson $G$-spaces with $D/G$-valued moment maps and of quasi-Poisson $G\\times G$-spaces with $D$-valued moment maps.","Using this result, we give simple constructions of fusion and conjugation for these spaces, and new examples coming from moduli spaces."],"url":"http://arxiv.org/abs/2404.17518v1","category":"math.DG"}
{"created":"2024-04-26 15:45:31","title":"The CARFAC v2 Cochlear Model in Matlab, NumPy, and JAX","abstract":"The open-source CARFAC (Cascade of Asymmetric Resonators with Fast-Acting Compression) cochlear model is upgraded to version 2, with improvements to the Matlab implementation, and with new Python/NumPy and JAX implementations -- but C++ version changes are still pending. One change addresses the DC (direct current, or zero frequency) quadratic distortion anomaly previously reported; another reduces the neural synchrony at high frequencies; the others have little or no noticeable effect in the default configuration. A new feature allows modeling a reduction of cochlear amplifier function, as a step toward a differentiable parameterized model of hearing impairment. In addition, the integration into the Auditory Model Toolbox (AMT) has been extensively improved, as the prior integration had bugs that made it unsuitable for including CARFAC in multi-model comparisons.","sentences":["The open-source CARFAC (Cascade of Asymmetric Resonators with Fast-Acting Compression) cochlear model is upgraded to version 2, with improvements to the Matlab implementation, and with new Python/NumPy and JAX implementations -- but C++ version changes are still pending.","One change addresses the DC (direct current, or zero frequency) quadratic distortion anomaly previously reported; another reduces the neural synchrony at high frequencies; the others have little or no noticeable effect in the default configuration.","A new feature allows modeling a reduction of cochlear amplifier function, as a step toward a differentiable parameterized model of hearing impairment.","In addition, the integration into the Auditory Model Toolbox (AMT) has been extensively improved, as the prior integration had bugs that made it unsuitable for including CARFAC in multi-model comparisons."],"url":"http://arxiv.org/abs/2404.17490v1","category":"eess.AS"}
{"created":"2024-04-26 15:11:25","title":"Lorentzian homogeneous structures with indecomposable holonomy","abstract":"For a Lorentzian homogeneous space, we study how algebraic conditions on the isotropy group affect the geometry and curvature of the homogeneous space. More specifically, we prove that a Lorentzian locally homogeneous space is locally isometric to a plane wave if it admits an Ambrose--Singer connection with indecomposable, non-irreducible holonomy. This generalises several existing results that require a certain algebraic type of the torsion of the Ambrose--Singer connection and moreover is in analogy to the fact that a Lorentzian homogeneous space with irreducible isotropy has constant sectional curvature.","sentences":["For a Lorentzian homogeneous space, we study how algebraic conditions on the isotropy group affect the geometry and curvature of the homogeneous space.","More specifically, we prove that a Lorentzian locally homogeneous space is locally isometric to a plane wave if it admits an Ambrose--Singer connection with indecomposable, non-irreducible holonomy.","This generalises several existing results that require a certain algebraic type of the torsion of the Ambrose--Singer connection and moreover is in analogy to the fact that a Lorentzian homogeneous space with irreducible isotropy has constant sectional curvature."],"url":"http://arxiv.org/abs/2404.17470v1","category":"math.DG"}
{"created":"2024-04-26 13:35:58","title":"Interior regularity of area minimizing currents within a $C^{2,\u03b1}$-submanifold","abstract":"Given an area-minimizing integral $m$-current in $\\Sigma$, we prove that the Hausdorff dimension of the interior singular set of $T$ cannot exceed $m-2$, provided that $\\Sigma$ is an embedded $(m+\\bar{n})$-submanifold of $\\mathbb{R}^{m+n}$ of class $C^{2,\\alpha}$, where $\\alpha>0$. This result establishes the complete counterpart, in the arbitrary codimension setting, of the interior regularity theory for area-minimizing integral hypercurrents within a Riemannian manifold of class $C^{2,\\alpha}$.","sentences":["Given an area-minimizing integral $m$-current in $\\Sigma$, we prove that the Hausdorff dimension of the interior singular set of $T$ cannot exceed $m-2$, provided that $\\Sigma$ is an embedded $(m+\\bar{n})$-submanifold of $\\mathbb{R}^{m+n}$ of class $C^{2,\\alpha}$, where $\\alpha>0$. This result establishes the complete counterpart, in the arbitrary codimension setting, of the interior regularity theory for area-minimizing integral hypercurrents within a Riemannian manifold of class $C^{2,\\alpha}$."],"url":"http://arxiv.org/abs/2404.17407v1","category":"math.AP"}
{"created":"2024-04-26 13:02:57","title":"Well-posedness and convergence of entropic approximation of semi-geostrophic equations","abstract":"We prove existence and uniqueness of solutions for an entropic version of the semi-geostrophic equations. We also establish convergence to a weak solution of the semi-geostrophic equations as the entropic parameter vanishes. Convergence is also proved for discretizations that can be computed numerically in practice as shown recently in [6].","sentences":["We prove existence and uniqueness of solutions for an entropic version of the semi-geostrophic equations.","We also establish convergence to a weak solution of the semi-geostrophic equations as the entropic parameter vanishes.","Convergence is also proved for discretizations that can be computed numerically in practice as shown recently in [6]."],"url":"http://arxiv.org/abs/2404.17387v1","category":"math.AP"}
{"created":"2024-04-26 12:43:19","title":"Estimating the Robustness Radius for Randomized Smoothing with 100$\\times$ Sample Efficiency","abstract":"Randomized smoothing (RS) has successfully been used to improve the robustness of predictions for deep neural networks (DNNs) by adding random noise to create multiple variations of an input, followed by deciding the consensus. To understand if an RS-enabled DNN is effective in the sampled input domains, it is mandatory to sample data points within the operational design domain, acquire the point-wise certificate regarding robustness radius, and compare it with pre-defined acceptance criteria. Consequently, ensuring that a point-wise robustness certificate for any given data point is obtained relatively cost-effectively is crucial. This work demonstrates that reducing the number of samples by one or two orders of magnitude can still enable the computation of a slightly smaller robustness radius (commonly ~20% radius reduction) with the same confidence. We provide the mathematical foundation for explaining the phenomenon while experimentally showing promising results on the standard CIFAR-10 and ImageNet datasets.","sentences":["Randomized smoothing (RS) has successfully been used to improve the robustness of predictions for deep neural networks (DNNs) by adding random noise to create multiple variations of an input, followed by deciding the consensus.","To understand if an RS-enabled DNN is effective in the sampled input domains, it is mandatory to sample data points within the operational design domain, acquire the point-wise certificate regarding robustness radius, and compare it with pre-defined acceptance criteria.","Consequently, ensuring that a point-wise robustness certificate for any given data point is obtained relatively cost-effectively is crucial.","This work demonstrates that reducing the number of samples by one or two orders of magnitude can still enable the computation of a slightly smaller robustness radius (commonly ~20% radius reduction) with the same confidence.","We provide the mathematical foundation for explaining the phenomenon while experimentally showing promising results on the standard CIFAR-10 and ImageNet datasets."],"url":"http://arxiv.org/abs/2404.17371v1","category":"cs.LG"}
{"created":"2024-04-26 11:50:16","title":"Fast Evaluation of Additive Kernels: Feature Arrangement, Fourier Methods, and Kernel Derivatives","abstract":"One of the main computational bottlenecks when working with kernel based learning is dealing with the large and typically dense kernel matrix. Techniques dealing with fast approximations of the matrix vector product for these kernel matrices typically deteriorate in their performance if the feature vectors reside in higher-dimensional feature spaces. We here present a technique based on the non-equispaced fast Fourier transform (NFFT) with rigorous error analysis. We show that this approach is also well suited to allow the approximation of the matrix that arises when the kernel is differentiated with respect to the kernel hyperparameters; a problem often found in the training phase of methods such as Gaussian processes. We also provide an error analysis for this case. We illustrate the performance of the additive kernel scheme with fast matrix vector products on a number of data sets. Our code is available at https://github.com/wagnertheresa/NFFTAddKer","sentences":["One of the main computational bottlenecks when working with kernel based learning is dealing with the large and typically dense kernel matrix.","Techniques dealing with fast approximations of the matrix vector product for these kernel matrices typically deteriorate in their performance if the feature vectors reside in higher-dimensional feature spaces.","We here present a technique based on the non-equispaced fast Fourier transform (NFFT) with rigorous error analysis.","We show that this approach is also well suited to allow the approximation of the matrix that arises when the kernel is differentiated with respect to the kernel hyperparameters; a problem often found in the training phase of methods such as Gaussian processes.","We also provide an error analysis for this case.","We illustrate the performance of the additive kernel scheme with fast matrix vector products on a number of data sets.","Our code is available at https://github.com/wagnertheresa/NFFTAddKer"],"url":"http://arxiv.org/abs/2404.17344v1","category":"cs.LG"}
{"created":"2024-04-26 11:50:15","title":"A Bionic Natural Language Parser Equivalent to a Pushdown Automaton","abstract":"Assembly Calculus (AC), proposed by Papadimitriou et al., aims to reproduce advanced cognitive functions through simulating neural activities, with several applications based on AC having been developed, including a natural language parser proposed by Mitropolsky et al. However, this parser lacks the ability to handle Kleene closures, preventing it from parsing all regular languages and rendering it weaker than Finite Automata (FA). In this paper, we propose a new bionic natural language parser (BNLP) based on AC and integrates two new biologically rational structures, Recurrent Circuit and Stack Circuit which are inspired by RNN and short-term memory mechanism. In contrast to the original parser, the BNLP can fully handle all regular languages and Dyck languages. Therefore, leveraging the Chomsky-Sch \\H{u}tzenberger theorem, the BNLP which can parse all Context-Free Languages can be constructed. We also formally prove that for any PDA, a Parser Automaton corresponding to BNLP can always be formed, ensuring that BNLP has a description ability equal to that of PDA and addressing the deficiencies of the original parser.","sentences":["Assembly Calculus (AC), proposed by Papadimitriou et al., aims to reproduce advanced cognitive functions through simulating neural activities, with several applications based on AC having been developed, including a natural language parser proposed by Mitropolsky et al.","However, this parser lacks the ability to handle Kleene closures, preventing it from parsing all regular languages and rendering it weaker than Finite Automata (FA).","In this paper, we propose a new bionic natural language parser (BNLP) based on AC and integrates two new biologically rational structures, Recurrent Circuit and Stack Circuit which are inspired by RNN and short-term memory mechanism.","In contrast to the original parser, the BNLP can fully handle all regular languages and Dyck languages.","Therefore, leveraging the Chomsky-Sch \\H{u}tzenberger theorem, the BNLP which can parse all Context-Free Languages can be constructed.","We also formally prove that for any PDA, a Parser Automaton corresponding to BNLP can always be formed, ensuring that BNLP has a description ability equal to that of PDA and addressing the deficiencies of the original parser."],"url":"http://arxiv.org/abs/2404.17343v1","category":"cs.CL"}
{"created":"2024-04-26 11:22:21","title":"Isotope effect in the Morse approximation of the ground state term of hydrogen molecule nH2, n = 1-7. Herzberg anomaly and anharmonicity","abstract":"The influence of the reduced mass on the results of the approximation by the Morse function of the ground electronic state term potential U(r) of hydrogen molecule nH2 for seven symmetrical isotopologues n = 1-7 is investigated. For each isotopologue, two alternative model solutions of the Schroedinger equation are obtained, namely, M1(r) and M2(r) functions, which differ by the selection of the primary fitting parameters in the Morse approximation. The U(r) - M(r) functions are shown to clearly visualize the differences between the shapes of the potential at its approximation. The concept of systematic deviation of Morse functions from the real potential is introduced. This deviation is always present in the approximation of simple terms, for which U(r) curve lies between M1(r) and M2(r) without crossings. In case of nH2, for M1(r) the systematic monotonous deviation increases, reaching 1700-2770 cm-1 (for 7H2-1H2) at the level of the asymptote. For M2(r) the systematic deviation decreases at larger distances: it is dome-shaped with a maximum of ca. 1500 cm-1 in the upper part of the term, after which it falls to zero. The M2(r) curves for different isotopologues diverge by less than 20 cm-1. There are also anomalies, where M1(r) and M2(r) cross U(r), called \"Herzberg anomaly\". For example, at around r = 1.08 A the U(r) - M2(r) function shows anomalous negative values ranging from -125 cm-1 (for 1H2) to -142 cm-1 (for 7H2). The diagnostic value of Herzberg anomaly and anharmonicities for various isotopologues is discussed.","sentences":["The influence of the reduced mass on the results of the approximation by the Morse function of the ground electronic state term potential U(r) of hydrogen molecule nH2 for seven symmetrical isotopologues n = 1-7 is investigated.","For each isotopologue, two alternative model solutions of the Schroedinger equation are obtained, namely, M1(r) and M2(r) functions, which differ by the selection of the primary fitting parameters in the Morse approximation.","The U(r) - M(r) functions are shown to clearly visualize the differences between the shapes of the potential at its approximation.","The concept of systematic deviation of Morse functions from the real potential is introduced.","This deviation is always present in the approximation of simple terms, for which U(r) curve lies between M1(r) and M2(r) without crossings.","In case of nH2, for M1(r)","the systematic monotonous deviation increases, reaching 1700-2770 cm-1 (for 7H2-1H2) at the level of the asymptote.","For M2(r)","the systematic deviation decreases at larger distances: it is dome-shaped with a maximum of ca. 1500 cm-1 in the upper part of the term, after which it falls to zero.","The M2(r) curves for different isotopologues diverge by less than 20 cm-1.","There are also anomalies, where M1(r) and M2(r) cross U(r), called \"Herzberg anomaly\".","For example, at around r = 1.08 A the U(r) - M2(r) function shows anomalous negative values ranging from -125 cm-1 (for 1H2) to -142 cm-1 (for 7H2).","The diagnostic value of Herzberg anomaly and anharmonicities for various isotopologues is discussed."],"url":"http://arxiv.org/abs/2404.17328v1","category":"physics.chem-ph"}
{"created":"2024-04-26 11:10:24","title":"Dense Road Surface Grip Map Prediction from Multimodal Image Data","abstract":"Slippery road weather conditions are prevalent in many regions and cause a regular risk for traffic. Still, there has been less research on how autonomous vehicles could detect slippery driving conditions on the road to drive safely. In this work, we propose a method to predict a dense grip map from the area in front of the car, based on postprocessed multimodal sensor data. We trained a convolutional neural network to predict pixelwise grip values from fused RGB camera, thermal camera, and LiDAR reflectance images, based on weakly supervised ground truth from an optical road weather sensor.   The experiments show that it is possible to predict dense grip values with good accuracy from the used data modalities as the produced grip map follows both ground truth measurements and local weather conditions, such as snowy areas on the road. The model using only the RGB camera or LiDAR reflectance modality provided good baseline results for grip prediction accuracy while using models fusing the RGB camera, thermal camera, and LiDAR modalities improved the grip predictions significantly.","sentences":["Slippery road weather conditions are prevalent in many regions and cause a regular risk for traffic.","Still, there has been less research on how autonomous vehicles could detect slippery driving conditions on the road to drive safely.","In this work, we propose a method to predict a dense grip map from the area in front of the car, based on postprocessed multimodal sensor data.","We trained a convolutional neural network to predict pixelwise grip values from fused RGB camera, thermal camera, and LiDAR reflectance images, based on weakly supervised ground truth from an optical road weather sensor.   ","The experiments show that it is possible to predict dense grip values with good accuracy from the used data modalities as the produced grip map follows both ground truth measurements and local weather conditions, such as snowy areas on the road.","The model using only the RGB camera or LiDAR reflectance modality provided good baseline results for grip prediction accuracy while using models fusing the RGB camera, thermal camera, and LiDAR modalities improved the grip predictions significantly."],"url":"http://arxiv.org/abs/2404.17324v1","category":"cs.CV"}
{"created":"2024-04-26 10:38:17","title":"Image Copy-Move Forgery Detection via Deep PatchMatch and Pairwise Ranking Learning","abstract":"Recent advances in deep learning algorithms have shown impressive progress in image copy-move forgery detection (CMFD). However, these algorithms lack generalizability in practical scenarios where the copied regions are not present in the training images, or the cloned regions are part of the background. Additionally, these algorithms utilize convolution operations to distinguish source and target regions, leading to unsatisfactory results when the target regions blend well with the background. To address these limitations, this study proposes a novel end-to-end CMFD framework that integrates the strengths of conventional and deep learning methods. Specifically, the study develops a deep cross-scale PatchMatch (PM) method that is customized for CMFD to locate copy-move regions. Unlike existing deep models, our approach utilizes features extracted from high-resolution scales to seek explicit and reliable point-to-point matching between source and target regions. Furthermore, we propose a novel pairwise rank learning framework to separate source and target regions. By leveraging the strong prior of point-to-point matches, the framework can identify subtle differences and effectively discriminate between source and target regions, even when the target regions blend well with the background. Our framework is fully differentiable and can be trained end-to-end. Comprehensive experimental results highlight the remarkable generalizability of our scheme across various copy-move scenarios, significantly outperforming existing methods.","sentences":["Recent advances in deep learning algorithms have shown impressive progress in image copy-move forgery detection (CMFD).","However, these algorithms lack generalizability in practical scenarios where the copied regions are not present in the training images, or the cloned regions are part of the background.","Additionally, these algorithms utilize convolution operations to distinguish source and target regions, leading to unsatisfactory results when the target regions blend well with the background.","To address these limitations, this study proposes a novel end-to-end CMFD framework that integrates the strengths of conventional and deep learning methods.","Specifically, the study develops a deep cross-scale PatchMatch (PM) method that is customized for CMFD to locate copy-move regions.","Unlike existing deep models, our approach utilizes features extracted from high-resolution scales to seek explicit and reliable point-to-point matching between source and target regions.","Furthermore, we propose a novel pairwise rank learning framework to separate source and target regions.","By leveraging the strong prior of point-to-point matches, the framework can identify subtle differences and effectively discriminate between source and target regions, even when the target regions blend well with the background.","Our framework is fully differentiable and can be trained end-to-end.","Comprehensive experimental results highlight the remarkable generalizability of our scheme across various copy-move scenarios, significantly outperforming existing methods."],"url":"http://arxiv.org/abs/2404.17310v1","category":"cs.CV"}
{"created":"2024-04-26 10:31:38","title":"An optimal control study for a two-strain SEIR epidemic model with saturated incidence rates and treatment","abstract":"This work will study an optimal control problem describing the two-strain SEIR epidemic model. The studied model is in the form of six nonlinear differential equations illustrating the dynamics of the susceptibles and the exposed, the infected, and the recovered individuals. The exposed and the infected compartments are each divided into two sub-classes representing the first and the second strain. The model includes two saturated rates and two treatments for each strain. We begin our study by showing the well-posedness of our problem. The basic reproduction number is calculated and depends mainly on the reproduction numbers of the first and second strains. The global stability of the disease-free equilibrium is fulfilled. The optimal control study is achieved by using the Pontryagin minimum principle. Numerical simulations have shown the importance of therapy in minimizing the infection's effect. By administrating suitable therapies, the disease's severity decreases considerably. The estimation of parameters as well as a comparison study with COVID-19 clinical data are fulfilled. It was shown that the mathematical model results fits well the clinical data.","sentences":["This work will study an optimal control problem describing the two-strain SEIR epidemic model.","The studied model is in the form of six nonlinear differential equations illustrating the dynamics of the susceptibles and the exposed, the infected, and the recovered individuals.","The exposed and the infected compartments are each divided into two sub-classes representing the first and the second strain.","The model includes two saturated rates and two treatments for each strain.","We begin our study by showing the well-posedness of our problem.","The basic reproduction number is calculated and depends mainly on the reproduction numbers of the first and second strains.","The global stability of the disease-free equilibrium is fulfilled.","The optimal control study is achieved by using the Pontryagin minimum principle.","Numerical simulations have shown the importance of therapy in minimizing the infection's effect.","By administrating suitable therapies, the disease's severity decreases considerably.","The estimation of parameters as well as a comparison study with COVID-19 clinical data are fulfilled.","It was shown that the mathematical model results fits well the clinical data."],"url":"http://arxiv.org/abs/2404.17305v1","category":"q-bio.PE"}
{"created":"2024-04-26 10:25:12","title":"An Introduction to Adjoint Problems","abstract":"Originally published as a Supplemental Appendix to Adjoint Equations in Stability Analysis, Annu. Rev. Fluid Mech. 46:493-517 (2014)","sentences":["Originally published as a Supplemental Appendix to Adjoint Equations in Stability Analysis, Annu.","Rev. Fluid Mech.","46:493-517 (2014)"],"url":"http://arxiv.org/abs/2404.17304v1","category":"physics.flu-dyn"}
{"created":"2024-04-26 09:49:32","title":"The Inefficiency of Genetic Programming for Symbolic Regression -- Extended Version","abstract":"We analyse the search behaviour of genetic programming for symbolic regression in practically relevant but limited settings, allowing exhaustive enumeration of all solutions. This enables us to quantify the success probability of finding the best possible expressions, and to compare the search efficiency of genetic programming to random search in the space of semantically unique expressions. This analysis is made possible by improved algorithms for equality saturation, which we use to improve the Exhaustive Symbolic Regression algorithm; this produces the set of semantically unique expression structures, orders of magnitude smaller than the full symbolic regression search space. We compare the efficiency of random search in the set of unique expressions and genetic programming. For our experiments we use two real-world datasets where symbolic regression has been used to produce well-fitting univariate expressions: the Nikuradse dataset of flow in rough pipes and the Radial Acceleration Relation of galaxy dynamics. The results show that genetic programming in such limited settings explores only a small fraction of all unique expressions, and evaluates expressions repeatedly that are congruent to already visited expressions.","sentences":["We analyse the search behaviour of genetic programming for symbolic regression in practically relevant but limited settings, allowing exhaustive enumeration of all solutions.","This enables us to quantify the success probability of finding the best possible expressions, and to compare the search efficiency of genetic programming to random search in the space of semantically unique expressions.","This analysis is made possible by improved algorithms for equality saturation, which we use to improve the Exhaustive Symbolic Regression algorithm; this produces the set of semantically unique expression structures, orders of magnitude smaller than the full symbolic regression search space.","We compare the efficiency of random search in the set of unique expressions and genetic programming.","For our experiments we use two real-world datasets where symbolic regression has been used to produce well-fitting univariate expressions: the Nikuradse dataset of flow in rough pipes and the Radial Acceleration Relation of galaxy dynamics.","The results show that genetic programming in such limited settings explores only a small fraction of all unique expressions, and evaluates expressions repeatedly that are congruent to already visited expressions."],"url":"http://arxiv.org/abs/2404.17292v1","category":"cs.NE"}
{"created":"2024-04-26 09:22:39","title":"Measurement of $\u03a9^0_{\\rm c}$ baryon production and branching-fraction ratio ${\\rm BR(\u03a9^0_c \\rightarrow \u03a9^- e^+\u03bd_e)} / {\\rm BR(\u03a9^0_c \\rightarrow \u03a9^- \u03c0^+)}$ in pp collisions at $\\sqrt{s}$ = 13 TeV","abstract":"The inclusive production of the charm-strange baryon $\\Omega^{0}_{\\rm c}$ is measured for the first time via its semileptonic decay into $\\Omega^{-}\\rm e^{+}\\nu_{e}$ at midrapidity ($|y|<0.8$) in proton$-$proton (pp) collisions at the centre-of-mass energy $\\sqrt{s}=13$ TeV with the ALICE detector at the LHC. The transverse momentum ($p_{\\rm T}$) differential cross section multiplied by the branching ratio is presented in the interval $2<p_{\\rm T}<12~{\\rm GeV}/c$. The branching-fraction ratio ${\\rm BR}(\\Omega^0_{\\rm c} \\rightarrow \\Omega^{-}{\\rm e}^{+}\\nu_{\\rm e})/ {\\rm BR}(\\Omega^0_{\\rm c} \\rightarrow \\Omega^{-}{\\pi}^{+})$ is measured to be 1.12 $\\pm$ 0.22 (stat.) $\\pm$ 0.27 (syst.). Comparisons with other experimental measurements, as well as with theoretical calculations, are presented.","sentences":["The inclusive production of the charm-strange baryon $\\Omega^{0}_{\\rm c}$ is measured for the first time via its semileptonic decay into $\\Omega^{-}\\rm e^{+}\\nu_{e}$ at midrapidity ($|y|<0.8$) in proton$-$proton (pp) collisions at the centre-of-mass energy $\\sqrt{s}=13$ TeV with the ALICE detector at the LHC.","The transverse momentum ($p_{\\rm T}$) differential cross section multiplied by the branching ratio is presented in the interval $2<p_{\\rm T}<12~{\\rm GeV}/c$.","The branching-fraction ratio ${\\rm BR}(\\Omega^0_{\\rm c} \\rightarrow \\Omega^{-}{\\rm e}^{+}\\nu_{\\rm e})/ {\\rm BR}(\\Omega^0_{\\rm c} \\rightarrow \\Omega^{-}{\\pi}^{+})$ is measured to be 1.12 $\\pm$ 0.22 (stat.)","$\\pm$ 0.27 (syst.).","Comparisons with other experimental measurements, as well as with theoretical calculations, are presented."],"url":"http://arxiv.org/abs/2404.17272v1","category":"hep-ex"}
{"created":"2024-04-26 08:47:28","title":"Comparison of self-supervised in-domain and supervised out-domain transfer learning for bird species recognition","abstract":"Transferring the weights of a pre-trained model to assist another task has become a crucial part of modern deep learning, particularly in data-scarce scenarios. Pre-training refers to the initial step of training models outside the current task of interest, typically on another dataset. It can be done via supervised models using human-annotated datasets or self-supervised models trained on unlabeled datasets. In both cases, many pre-trained models are available to fine-tune for the task of interest. Interestingly, research has shown that pre-trained models from ImageNet can be helpful for audio tasks despite being trained on image datasets. Hence, it's unclear whether in-domain models would be advantageous compared to competent out-domain models, such as convolutional neural networks from ImageNet. Our experiments will demonstrate the usefulness of in-domain models and datasets for bird species recognition by leveraging VICReg, a recent and powerful self-supervised method.","sentences":["Transferring the weights of a pre-trained model to assist another task has become a crucial part of modern deep learning, particularly in data-scarce scenarios.","Pre-training refers to the initial step of training models outside the current task of interest, typically on another dataset.","It can be done via supervised models using human-annotated datasets or self-supervised models trained on unlabeled datasets.","In both cases, many pre-trained models are available to fine-tune for the task of interest.","Interestingly, research has shown that pre-trained models from ImageNet can be helpful for audio tasks despite being trained on image datasets.","Hence, it's unclear whether in-domain models would be advantageous compared to competent out-domain models, such as convolutional neural networks from ImageNet.","Our experiments will demonstrate the usefulness of in-domain models and datasets for bird species recognition by leveraging VICReg, a recent and powerful self-supervised method."],"url":"http://arxiv.org/abs/2404.17252v1","category":"cs.LG"}
{"created":"2024-04-26 17:30:36","title":"A Semi-Automatic Approach to Create Large Gender- and Age-Balanced Speaker Corpora: Usefulness of Speaker Diarization & Identification","abstract":"This paper presents a semi-automatic approach to create a diachronic corpus of voices balanced for speaker's age, gender, and recording period, according to 32 categories (2 genders, 4 age ranges and 4 recording periods). Corpora were selected at French National Institute of Audiovisual (INA) to obtain at least 30 speakers per category (a total of 960 speakers; only 874 have be found yet). For each speaker, speech excerpts were extracted from audiovisual documents using an automatic pipeline consisting of speech detection, background music and overlapped speech removal and speaker diarization, used to present clean speaker segments to human annotators identifying target speakers. This pipeline proved highly effective, cutting down manual processing by a factor of ten. Evaluation of the quality of the automatic processing and of the final output is provided. It shows the automatic processing compare to up-to-date process, and that the output provides high quality speech for most of the selected excerpts. This method shows promise for creating large corpora of known target speakers.","sentences":["This paper presents a semi-automatic approach to create a diachronic corpus of voices balanced for speaker's age, gender, and recording period, according to 32 categories (2 genders, 4 age ranges and 4 recording periods).","Corpora were selected at French National Institute of Audiovisual (INA) to obtain at least 30 speakers per category (a total of 960 speakers; only 874 have be found yet).","For each speaker, speech excerpts were extracted from audiovisual documents using an automatic pipeline consisting of speech detection, background music and overlapped speech removal and speaker diarization, used to present clean speaker segments to human annotators identifying target speakers.","This pipeline proved highly effective, cutting down manual processing by a factor of ten.","Evaluation of the quality of the automatic processing and of the final output is provided.","It shows the automatic processing compare to up-to-date process, and that the output provides high quality speech for most of the selected excerpts.","This method shows promise for creating large corpora of known target speakers."],"url":"http://arxiv.org/abs/2404.17552v1","category":"eess.AS"}
{"created":"2024-04-26 16:39:18","title":"Interpreting Deepcode, a learned feedback code","abstract":"Deep learning methods have recently been used to construct non-linear codes for the additive white Gaussian noise (AWGN) channel with feedback. However, there is limited understanding of how these black-box-like codes with many learned parameters use feedback. This study aims to uncover the fundamental principles underlying the first deep-learned feedback code, known as Deepcode, which is based on an RNN architecture. Our interpretable model based on Deepcode is built by analyzing the influence length of inputs and approximating the non-linear dynamics of the original black-box RNN encoder. Numerical experiments demonstrate that our interpretable model -- which includes both an encoder and a decoder -- achieves comparable performance to Deepcode while offering an interpretation of how it employs feedback for error correction.","sentences":["Deep learning methods have recently been used to construct non-linear codes for the additive white Gaussian noise (AWGN) channel with feedback.","However, there is limited understanding of how these black-box-like codes with many learned parameters use feedback.","This study aims to uncover the fundamental principles underlying the first deep-learned feedback code, known as Deepcode, which is based on an RNN architecture.","Our interpretable model based on Deepcode is built by analyzing the influence length of inputs and approximating the non-linear dynamics of the original black-box RNN encoder.","Numerical experiments demonstrate that our interpretable model -- which includes both an encoder and a decoder -- achieves comparable performance to Deepcode while offering an interpretation of how it employs feedback for error correction."],"url":"http://arxiv.org/abs/2404.17519v1","category":"cs.IT"}
{"created":"2024-04-26 16:19:55","title":"HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts","abstract":"In an era where the volume of data drives the effectiveness of self-supervised learning, the specificity and clarity of data semantics play a crucial role in model training. Addressing this, we introduce HYPerbolic Entailment filtering (HYPE), a novel methodology designed to meticulously extract modality-wise meaningful and well-aligned data from extensive, noisy image-text pair datasets. Our approach leverages hyperbolic embeddings and the concept of entailment cones to evaluate and filter out samples with meaningless or underspecified semantics, focusing on enhancing the specificity of each data sample. HYPE not only demonstrates a significant improvement in filtering efficiency but also sets a new state-of-the-art in the DataComp benchmark when combined with existing filtering techniques. This breakthrough showcases the potential of HYPE to refine the data selection process, thereby contributing to the development of more accurate and efficient self-supervised learning models. Additionally, the image specificity $\\epsilon_{i}$ can be independently applied to induce an image-only dataset from an image-text or image-only data pool for training image-only self-supervised models and showed superior performance when compared to the dataset induced by CLIP score.","sentences":["In an era where the volume of data drives the effectiveness of self-supervised learning, the specificity and clarity of data semantics play a crucial role in model training.","Addressing this, we introduce HYPerbolic Entailment filtering (HYPE), a novel methodology designed to meticulously extract modality-wise meaningful and well-aligned data from extensive, noisy image-text pair datasets.","Our approach leverages hyperbolic embeddings and the concept of entailment cones to evaluate and filter out samples with meaningless or underspecified semantics, focusing on enhancing the specificity of each data sample.","HYPE not only demonstrates a significant improvement in filtering efficiency but also sets a new state-of-the-art in the DataComp benchmark when combined with existing filtering techniques.","This breakthrough showcases the potential of HYPE to refine the data selection process, thereby contributing to the development of more accurate and efficient self-supervised learning models.","Additionally, the image specificity $\\epsilon_{i}$ can be independently applied to induce an image-only dataset from an image-text or image-only data pool for training image-only self-supervised models and showed superior performance when compared to the dataset induced by CLIP score."],"url":"http://arxiv.org/abs/2404.17507v1","category":"cs.CV"}
{"created":"2024-04-26 15:08:27","title":"Fast Abstracts and Student Forum Proceedings -- EDCC 2024 -- 19th European Dependable Computing Conference","abstract":"The goal of the Fast Abstracts track is to bring together researchers and practitioners working on dependable computing to discuss work in progress or opinion pieces. Contributions are welcome from academia and industry. Fast Abstracts aim to serve as a rapid and flexible mechanism to: (i) Report on current work that may or may not be complete; (ii) Introduce new ideas to the community; (iii) State positions on controversial issues or open problems; (iv) Share lessons learnt from real-word dependability engineering; and (v) Debunk or question results from other papers based on contra-indications. The Student Forum aims at creating a vibrant and friendly environment where students can present and discuss their work, and exchange ideas and experiences with other students, researchers and industry. One of the key goals of the Forum is to provide students with feedback on their preliminary results that might help with their future research directions.","sentences":["The goal of the Fast Abstracts track is to bring together researchers and practitioners working on dependable computing to discuss work in progress or opinion pieces.","Contributions are welcome from academia and industry.","Fast Abstracts aim to serve as a rapid and flexible mechanism to: (i) Report on current work that may or may not be complete; (ii) Introduce new ideas to the community; (iii) State positions on controversial issues or open problems; (iv) Share lessons learnt from real-word dependability engineering; and (v) Debunk or question results from other papers based on contra-indications.","The Student Forum aims at creating a vibrant and friendly environment where students can present and discuss their work, and exchange ideas and experiences with other students, researchers and industry.","One of the key goals of the Forum is to provide students with feedback on their preliminary results that might help with their future research directions."],"url":"http://arxiv.org/abs/2404.17465v1","category":"cs.SE"}
{"created":"2024-04-26 14:47:40","title":"A Continuous Relaxation for Discrete Bayesian Optimization","abstract":"To optimize efficiently over discrete data and with only few available target observations is a challenge in Bayesian optimization. We propose a continuous relaxation of the objective function and show that inference and optimization can be computationally tractable. We consider in particular the optimization domain where very few observations and strict budgets exist; motivated by optimizing protein sequences for expensive to evaluate bio-chemical properties. The advantages of our approach are two-fold: the problem is treated in the continuous setting, and available prior knowledge over sequences can be incorporated directly. More specifically, we utilize available and learned distributions over the problem domain for a weighting of the Hellinger distance which yields a covariance function. We show that the resulting acquisition function can be optimized with both continuous or discrete optimization algorithms and empirically assess our method on two bio-chemical sequence optimization tasks.","sentences":["To optimize efficiently over discrete data and with only few available target observations is a challenge in Bayesian optimization.","We propose a continuous relaxation of the objective function and show that inference and optimization can be computationally tractable.","We consider in particular the optimization domain where very few observations and strict budgets exist; motivated by optimizing protein sequences for expensive to evaluate bio-chemical properties.","The advantages of our approach are two-fold: the problem is treated in the continuous setting, and available prior knowledge over sequences can be incorporated directly.","More specifically, we utilize available and learned distributions over the problem domain for a weighting of the Hellinger distance which yields a covariance function.","We show that the resulting acquisition function can be optimized with both continuous or discrete optimization algorithms and empirically assess our method on two bio-chemical sequence optimization tasks."],"url":"http://arxiv.org/abs/2404.17452v1","category":"cs.LG"}
{"created":"2024-04-26 14:10:55","title":"Separation capacity of linear reservoirs with random connectivity matrix","abstract":"We argue that the success of reservoir computing lies within the separation capacity of the reservoirs and show that the expected separation capacity of random linear reservoirs is fully characterised by the spectral decomposition of an associated generalised matrix of moments. Of particular interest are reservoirs with Gaussian matrices that are either symmetric or whose entries are all independent. In the symmetric case, we prove that the separation capacity always deteriorates with time; while for short inputs, separation with large reservoirs is best achieved when the entries of the matrix are scaled with a factor $\\rho_T/\\sqrt{N}$, where $N$ is the dimension of the reservoir and $\\rho_T$ depends on the maximum length of the input time series. In the i.i.d. case, we establish that optimal separation with large reservoirs is consistently achieved when the entries of the reservoir matrix are scaled with the exact factor $1/\\sqrt{N}$. We further give upper bounds on the quality of separation in function of the length of the time series. We complement this analysis with an investigation of the likelihood of this separation and the impact of the chosen architecture on separation consistency.","sentences":["We argue that the success of reservoir computing lies within the separation capacity of the reservoirs and show that the expected separation capacity of random linear reservoirs is fully characterised by the spectral decomposition of an associated generalised matrix of moments.","Of particular interest are reservoirs with Gaussian matrices that are either symmetric or whose entries are all independent.","In the symmetric case, we prove that the separation capacity always deteriorates with time; while for short inputs, separation with large reservoirs is best achieved when the entries of the matrix are scaled with a factor $\\rho_T/\\sqrt{N}$, where $N$ is the dimension of the reservoir and $\\rho_T$ depends on the maximum length of the input time series.","In the i.i.d. case, we establish that optimal separation with large reservoirs is consistently achieved when the entries of the reservoir matrix are scaled with the exact factor $1/\\sqrt{N}$. We further give upper bounds on the quality of separation in function of the length of the time series.","We complement this analysis with an investigation of the likelihood of this separation and the impact of the chosen architecture on separation consistency."],"url":"http://arxiv.org/abs/2404.17429v1","category":"stat.ML"}
{"created":"2024-04-26 13:22:28","title":"Evaluation of Geographical Distortions in Language Models: A Crucial Step Towards Equitable Representations","abstract":"Language models now constitute essential tools for improving efficiency for many professional tasks such as writing, coding, or learning. For this reason, it is imperative to identify inherent biases. In the field of Natural Language Processing, five sources of bias are well-identified: data, annotation, representation, models, and research design. This study focuses on biases related to geographical knowledge. We explore the connection between geography and language models by highlighting their tendency to misrepresent spatial information, thus leading to distortions in the representation of geographical distances. This study introduces four indicators to assess these distortions, by comparing geographical and semantic distances. Experiments are conducted from these four indicators with ten widely used language models. Results underscore the critical necessity of inspecting and rectifying spatial biases in language models to ensure accurate and equitable representations.","sentences":["Language models now constitute essential tools for improving efficiency for many professional tasks such as writing, coding, or learning.","For this reason, it is imperative to identify inherent biases.","In the field of Natural Language Processing, five sources of bias are well-identified: data, annotation, representation, models, and research design.","This study focuses on biases related to geographical knowledge.","We explore the connection between geography and language models by highlighting their tendency to misrepresent spatial information, thus leading to distortions in the representation of geographical distances.","This study introduces four indicators to assess these distortions, by comparing geographical and semantic distances.","Experiments are conducted from these four indicators with ten widely used language models.","Results underscore the critical necessity of inspecting and rectifying spatial biases in language models to ensure accurate and equitable representations."],"url":"http://arxiv.org/abs/2404.17401v1","category":"cs.CL"}
{"created":"2024-04-26 17:59:22","title":"Quantum Optimization for the Maximum Cut Problem on a Superconducting Quantum Computer","abstract":"Achieving high-quality solutions faster than classical solvers on computationally hard problems is a challenge for quantum optimization to deliver utility. Using a superconducting quantum computer, we experimentally investigate the performance of a hybrid quantum-classical algorithm inspired by semidefinite programming approaches for solving the maximum cut problem on 3-regular graphs up to several thousand variables. We leverage the structure of the input problems to address sizes beyond what current quantum machines can naively handle. We attain an average performance of 99% over a random ensemble of thousands of problem instances. We benchmark the quantum solver against similarly high-performing classical heuristics, including the Gurobi optimizer, simulated annealing, and the Burer-Monteiro algorithm. A runtime analysis shows that the quantum solver on large-scale problems is competitive against Gurobi but short of others. We explore multiple leads to close the gap and discuss prospects for a practical quantum speedup.","sentences":["Achieving high-quality solutions faster than classical solvers on computationally hard problems is a challenge for quantum optimization to deliver utility.","Using a superconducting quantum computer, we experimentally investigate the performance of a hybrid quantum-classical algorithm inspired by semidefinite programming approaches for solving the maximum cut problem on 3-regular graphs up to several thousand variables.","We leverage the structure of the input problems to address sizes beyond what current quantum machines can naively handle.","We attain an average performance of 99% over a random ensemble of thousands of problem instances.","We benchmark the quantum solver against similarly high-performing classical heuristics, including the Gurobi optimizer, simulated annealing, and the Burer-Monteiro algorithm.","A runtime analysis shows that the quantum solver on large-scale problems is competitive against Gurobi but short of others.","We explore multiple leads to close the gap and discuss prospects for a practical quantum speedup."],"url":"http://arxiv.org/abs/2404.17579v1","category":"quant-ph"}
{"created":"2024-04-26 17:40:20","title":"Exploiting many-body localization for scalable variational quantum simulation","abstract":"Variational quantum algorithms have emerged as a promising approach to achieving practical quantum advantages using near-term quantum devices. Despite their potential, the scalability of these algorithms poses a significant challenge. This is largely attributed to the \"barren plateau\" phenomenon, which persists even in the absence of noise. In this work, we explore the many-body localization (MBL)-thermalization phase transitions within a framework of Floquet-initialized variational quantum circuits and investigate how MBL could be used to avoid barren plateaus. The phase transitions are observed through calculations of the inverse participation ratio, the entanglement entropy, and a metric termed \\text{low-weight stabilizer R\\'enyi entropy}. A critical element of our study involves the experimental validation of the phase transitions using the 127-qubit IBMQ Brisbane quantum processor. By initializing the circuit in the MBL phase and employing an easily preparable initial state, we find it is possible to prevent the formation of a unitary 2-design, resulting in an output state with entanglement that follows an area- rather than a volume-law, and which circumvents barren plateaus throughout the optimization. Utilizing this methodology, we successfully determine the ground states of various model Hamiltonians across different phases and show that the resources required for the optimization are significantly reduced. These results provide new insights into the interplay between MBL and quantum computing and suggest that the role of MBL states should be considered in the design of quantum algorithms.","sentences":["Variational quantum algorithms have emerged as a promising approach to achieving practical quantum advantages using near-term quantum devices.","Despite their potential, the scalability of these algorithms poses a significant challenge.","This is largely attributed to the \"barren plateau\" phenomenon, which persists even in the absence of noise.","In this work, we explore the many-body localization (MBL)-thermalization phase transitions within a framework of Floquet-initialized variational quantum circuits and investigate how MBL could be used to avoid barren plateaus.","The phase transitions are observed through calculations of the inverse participation ratio, the entanglement entropy, and a metric termed \\text{low-weight stabilizer R\\'enyi entropy}.","A critical element of our study involves the experimental validation of the phase transitions using the 127-qubit IBMQ Brisbane quantum processor.","By initializing the circuit in the MBL phase and employing an easily preparable initial state, we find it is possible to prevent the formation of a unitary 2-design, resulting in an output state with entanglement that follows an area- rather than a volume-law, and which circumvents barren plateaus throughout the optimization.","Utilizing this methodology, we successfully determine the ground states of various model Hamiltonians across different phases and show that the resources required for the optimization are significantly reduced.","These results provide new insights into the interplay between MBL and quantum computing and suggest that the role of MBL states should be considered in the design of quantum algorithms."],"url":"http://arxiv.org/abs/2404.17560v1","category":"quant-ph"}
{"created":"2024-04-26 16:23:53","title":"Understanding the Cluster LP for Correlation Clustering","abstract":"In the classic Correlation Clustering problem introduced by Bansal, Blum, and Chawla~(FOCS 2002), the input is a complete graph where edges are labeled either $+$ or $-$, and the goal is to find a partition of the vertices that minimizes the sum of the +edges across parts plus the sum of the -edges within parts. In recent years, Chawla, Makarychev, Schramm and Yaroslavtsev~(STOC 2015) gave a 2.06-approximation by providing a near-optimal rounding of the standard LP, and Cohen-Addad, Lee, Li, and Newman~(FOCS 2022, 2023) finally bypassed the integrality gap of 2 for this LP giving a $1.73$-approximation for the problem.   In order to create a simple and unified framework for Correlation Clustering similar to those for {\\em typical} approximate optimization tasks, we propose the {\\em cluster LP} as a strong linear program that might tightly capture the approximability of Correlation Clustering. It unifies all the previous relaxations for the problem.   We demonstrate the power of the cluster LP by presenting a simple rounding algorithm, and providing two analyses, one analytically proving a 1.49-approximation and the other solving a factor-revealing SDP to show a 1.437-approximation. Both proofs introduce principled methods by which to analyze the performance of the algorithm, resulting in a significantly improved approximation guarantee.   Finally, we prove an integrality gap of $4/3$ for the cluster LP, showing our 1.437-upper bound cannot be drastically improved. Our gap instance directly inspires an improved NP-hardness of approximation with a ratio $24/23 \\approx 1.042$; no explicit hardness ratio was known before.","sentences":["In the classic Correlation Clustering problem introduced by Bansal, Blum, and Chawla~(FOCS 2002), the input is a complete graph where edges are labeled either $+$ or $-$, and the goal is to find a partition of the vertices that minimizes the sum of the +edges across parts plus the sum of the -edges within parts.","In recent years, Chawla, Makarychev, Schramm and Yaroslavtsev~(STOC 2015) gave a 2.06-approximation by providing a near-optimal rounding of the standard LP, and Cohen-Addad, Lee, Li, and Newman~(FOCS 2022, 2023) finally bypassed the integrality gap of 2 for this LP giving a $1.73$-approximation for the problem.   ","In order to create a simple and unified framework for Correlation Clustering similar to those for {\\em typical} approximate optimization tasks, we propose the {\\em cluster LP} as a strong linear program that might tightly capture the approximability of Correlation Clustering.","It unifies all the previous relaxations for the problem.   ","We demonstrate the power of the cluster LP by presenting a simple rounding algorithm, and providing two analyses, one analytically proving a 1.49-approximation and the other solving a factor-revealing SDP to show a 1.437-approximation.","Both proofs introduce principled methods by which to analyze the performance of the algorithm, resulting in a significantly improved approximation guarantee.   ","Finally, we prove an integrality gap of $4/3$ for the cluster LP, showing our 1.437-upper bound cannot be drastically improved.","Our gap instance directly inspires an improved NP-hardness of approximation with a ratio $24/23 \\approx 1.042$; no explicit hardness ratio was known before."],"url":"http://arxiv.org/abs/2404.17509v1","category":"cs.DS"}
{"created":"2024-04-26 14:21:05","title":"Exploring Wireless Channels in Rural Areas: A Comprehensive Measurement Study","abstract":"The study of wireless channel behavior has been an active research topic for many years. However, there exists a noticeable scarcity of studies focusing on wireless channel characteristics in rural areas. With the advancement of smart agriculture practices in rural regions, there has been an increasing demand for affordable, high-capacity, and low-latency wireless networks to support various precision agriculture applications such as plant phenotyping, livestock health monitoring, and agriculture automation. To address this research gap, we conducted a channel measurement study on multiple wireless frequency bands at various crop and livestock farms near Ames, Iowa, based on Iowa State University~(ISU)'s ARA Wireless Living lab - one of the NSF PAWR platforms. We specifically investigate the impact of weather conditions, humidity, temperature, and farm buildings on wireless channel behavior. The resulting measurement dataset, which will soon be made publicly accessible, represents a valuable resource for researchers interested in wireless channel prediction and optimization.","sentences":["The study of wireless channel behavior has been an active research topic for many years.","However, there exists a noticeable scarcity of studies focusing on wireless channel characteristics in rural areas.","With the advancement of smart agriculture practices in rural regions, there has been an increasing demand for affordable, high-capacity, and low-latency wireless networks to support various precision agriculture applications such as plant phenotyping, livestock health monitoring, and agriculture automation.","To address this research gap, we conducted a channel measurement study on multiple wireless frequency bands at various crop and livestock farms near Ames, Iowa, based on Iowa State University~(ISU)'s ARA Wireless Living lab - one of the NSF PAWR platforms.","We specifically investigate the impact of weather conditions, humidity, temperature, and farm buildings on wireless channel behavior.","The resulting measurement dataset, which will soon be made publicly accessible, represents a valuable resource for researchers interested in wireless channel prediction and optimization."],"url":"http://arxiv.org/abs/2404.17434v1","category":"cs.NI"}
{"created":"2024-04-26 14:14:24","title":"Quantum Information Engines: Assessing Time, Cost and Performance Criteria","abstract":"In this study, we investigate the crucial role of measurement time ($t_m$), information gain and energy consumption in information engines (IEs) utilizing a von-Neumann measurement model. These important measurement parameters allow us to analyze the efficiency and power output of these devices. As the measurement time increases, the information gain and subsequently the extracted work also increase. However, there is a corresponding increase in the energetic cost. The efficiency of converting information into free energy diminishes as $t_m$ approaches both 0 and infinity, peaking at intermediate values of $t_m$. The power output (work extracted per times) also reaches a maximum at specific operational time regimes. By considering the product of efficiency and power as a performance metric, we can identify the optimal operating conditions for the IE.","sentences":["In this study, we investigate the crucial role of measurement time ($t_m$), information gain and energy consumption in information engines (IEs) utilizing a von-Neumann measurement model.","These important measurement parameters allow us to analyze the efficiency and power output of these devices.","As the measurement time increases, the information gain and subsequently the extracted work also increase.","However, there is a corresponding increase in the energetic cost.","The efficiency of converting information into free energy diminishes as $t_m$ approaches both 0 and infinity, peaking at intermediate values of $t_m$. The power output (work extracted per times) also reaches a maximum at specific operational time regimes.","By considering the product of efficiency and power as a performance metric, we can identify the optimal operating conditions for the IE."],"url":"http://arxiv.org/abs/2404.17431v1","category":"quant-ph"}
{"created":"2024-04-26 09:46:00","title":"The asymptotic behaviour of the Ces\u00e0ro operator","abstract":"We study the asymptotic behaviour of orbits $(T^nx)_{n\\ge0}$ of the classical Ces\\`aro operator $T$ for sequences $x$ in the Banach space $c$ of convergent sequences. We give new non-probabilistic proofs, based on the Katznelson--Tzafriri theorem and one of its quantified variants, of results which characterise the set of sequences $x\\in c$ that lead to convergent orbits and, for sequences satisfying a simple additional condition, we provide a rate of convergence. These results are then shown, again by operator-theoretic techniques, to be optimal in different ways. Finally, we study the asymptotic behaviour of the Ces\\`aro operator defined on spaces of continuous functions, establishing new and improved results in this setting, too.","sentences":["We study the asymptotic behaviour of orbits $(T^nx)_{n\\ge0}$ of the classical Ces\\`aro operator $T$ for sequences $x$ in the Banach space $c$ of convergent sequences.","We give new non-probabilistic proofs, based on the Katznelson--Tzafriri theorem and one of its quantified variants, of results which characterise the set of sequences $x\\in c$ that lead to convergent orbits and, for sequences satisfying a simple additional condition, we provide a rate of convergence.","These results are then shown, again by operator-theoretic techniques, to be optimal in different ways.","Finally, we study the asymptotic behaviour of the Ces\\`aro operator defined on spaces of continuous functions, establishing new and improved results in this setting, too."],"url":"http://arxiv.org/abs/2404.17289v1","category":"math.FA"}
{"created":"2024-04-26 09:16:02","title":"Clustering of Motion Trajectories by a Distance Measure Based on Semantic Features","abstract":"Clustering of motion trajectories is highly relevant for human-robot interactions as it allows the anticipation of human motions, fast reaction to those, as well as the recognition of explicit gestures. Further, it allows automated analysis of recorded motion data. Many clustering algorithms for trajectories build upon distance metrics that are based on pointwise Euclidean distances. However, our work indicates that focusing on salient characteristics is often sufficient. We present a novel distance measure for motion plans consisting of state and control trajectories that is based on a compressed representation built from their main features. This approach allows a flexible choice of feature classes relevant to the respective task. The distance measure is used in agglomerative hierarchical clustering. We compare our method with the widely used dynamic time warping algorithm on test sets of motion plans for the Furuta pendulum and the Manutec robot arm and on real-world data from a human motion dataset. The proposed method demonstrates slight advantages in clustering and strong advantages in runtime, especially for long trajectories.","sentences":["Clustering of motion trajectories is highly relevant for human-robot interactions as it allows the anticipation of human motions, fast reaction to those, as well as the recognition of explicit gestures.","Further, it allows automated analysis of recorded motion data.","Many clustering algorithms for trajectories build upon distance metrics that are based on pointwise Euclidean distances.","However, our work indicates that focusing on salient characteristics is often sufficient.","We present a novel distance measure for motion plans consisting of state and control trajectories that is based on a compressed representation built from their main features.","This approach allows a flexible choice of feature classes relevant to the respective task.","The distance measure is used in agglomerative hierarchical clustering.","We compare our method with the widely used dynamic time warping algorithm on test sets of motion plans for the Furuta pendulum and the Manutec robot arm and on real-world data from a human motion dataset.","The proposed method demonstrates slight advantages in clustering and strong advantages in runtime, especially for long trajectories."],"url":"http://arxiv.org/abs/2404.17269v1","category":"cs.RO"}
{"created":"2024-04-26 08:42:59","title":"Camera Motion Estimation from RGB-D-Inertial Scene Flow","abstract":"In this paper, we introduce a novel formulation for camera motion estimation that integrates RGB-D images and inertial data through scene flow. Our goal is to accurately estimate the camera motion in a rigid 3D environment, along with the state of the inertial measurement unit (IMU). Our proposed method offers the flexibility to operate as a multi-frame optimization or to marginalize older data, thus effectively utilizing past measurements. To assess the performance of our method, we conducted evaluations using both synthetic data from the ICL-NUIM dataset and real data sequences from the OpenLORIS-Scene dataset. Our results show that the fusion of these two sensors enhances the accuracy of camera motion estimation when compared to using only visual data.","sentences":["In this paper, we introduce a novel formulation for camera motion estimation that integrates RGB-D images and inertial data through scene flow.","Our goal is to accurately estimate the camera motion in a rigid 3D environment, along with the state of the inertial measurement unit (IMU).","Our proposed method offers the flexibility to operate as a multi-frame optimization or to marginalize older data, thus effectively utilizing past measurements.","To assess the performance of our method, we conducted evaluations using both synthetic data from the ICL-NUIM dataset and real data sequences from the OpenLORIS-Scene dataset.","Our results show that the fusion of these two sensors enhances the accuracy of camera motion estimation when compared to using only visual data."],"url":"http://arxiv.org/abs/2404.17251v1","category":"cs.CV"}
{"created":"2024-04-26 08:29:04","title":"The role of quantum and classical correlations in shrinking algorithms for optimization","abstract":"The benefit of quantum computing for solving combinatorial optimization problems (COPs) constitutes an open research question. In this work, we study the performance of a shrinking algorithm for COPs. The algorithm leverages correlations extracted from quantum or classical subroutines to recursively simplify the problem. We compare the performance of the algorithm equipped with correlations from the quantum approximate optimization algorithm (QAOA) as well as the classical linear programming (LP) and semi-definite programming (SDP) relaxations. This allows us to benchmark the utility of QAOA correlations against established classical relaxation algorithms. We apply the recursive algorithm to MaxCut problem instances with up to a hundred vertices at different graph densities. Our results indicate that LP outperforms all other approaches for low-density instances, while SDP excels for high-density problems. Moreover, the shrinking algorithm proves to be a viable alternative to established methods of rounding LP and SDP relaxations. In addition, the recursive shrinking algorithm outperforms its bare counterparts for all three types of correlations, i.e., LP with spanning tree rounding, the Goemans-Williamson algorithm, and conventional QAOA. While the lowest depth QAOA consistently yields worse results than the SDP, our tensor network experiments show that the performance increases significantly for deeper QAOA circuits.","sentences":["The benefit of quantum computing for solving combinatorial optimization problems (COPs) constitutes an open research question.","In this work, we study the performance of a shrinking algorithm for COPs.","The algorithm leverages correlations extracted from quantum or classical subroutines to recursively simplify the problem.","We compare the performance of the algorithm equipped with correlations from the quantum approximate optimization algorithm (QAOA) as well as the classical linear programming (LP) and semi-definite programming (SDP) relaxations.","This allows us to benchmark the utility of QAOA correlations against established classical relaxation algorithms.","We apply the recursive algorithm to MaxCut problem instances with up to a hundred vertices at different graph densities.","Our results indicate that LP outperforms all other approaches for low-density instances, while SDP excels for high-density problems.","Moreover, the shrinking algorithm proves to be a viable alternative to established methods of rounding LP and SDP relaxations.","In addition, the recursive shrinking algorithm outperforms its bare counterparts for all three types of correlations, i.e., LP with spanning tree rounding, the Goemans-Williamson algorithm, and conventional QAOA.","While the lowest depth QAOA consistently yields worse results than the SDP, our tensor network experiments show that the performance increases significantly for deeper QAOA circuits."],"url":"http://arxiv.org/abs/2404.17242v1","category":"quant-ph"}
{"created":"2024-04-26 17:56:45","title":"\"Unification\" of BSM Searches and SM Measurements: the case of lepton$+MET$ and $m_W$","abstract":"We develop the idea that the unprecedented precision in Standard Model (SM) measurements, with further improvement at the HL-LHC, enables new searches for physics Beyond the Standard Model (BSM).As an illustration, we demonstrate that the measured kinematic distributions of the lepton$+MET$ final state not only determine the mass of the $W$ boson, but are also sensitive to light new physics. Such a search for new physics thus requires a simultaneous fit to the BSM and SM parameters, \"unifying\" searches and measurements at the LHC and Tevatron. In this paper, we complete the program initiated in our earlier work arXiv:2310.13687. In particular, we analyze ($i$) novel decay modes of the $W$ boson with a neutrinophilic invisible scalar or with a heavy neutrino; ($ii$) modified production of $W$ bosons, namely, associated with a hadrophilic invisible $Z^\\prime$ gauge boson; and ($iii$) scenarios without an on-shell $W$ boson, such as slepton-sneutrino production in the Minimal Supersymmetric Standard Model (MSSM). Here, we complement our previous MSSM analysis in arXiv:2310.13687 by considering a different kinematic region. Our results highlight that new physics can still be directly discovered at the LHC, including light new physics,via SM precision measurements. Furthermore, we illustrate that such BSM signals are subtle, yet potentially large enough to affect the precision measurements of SM parameters themselves, such as the $W$ boson mass.","sentences":["We develop the idea that the unprecedented precision in Standard Model (SM) measurements, with further improvement at the HL-LHC, enables new searches for physics Beyond the Standard Model (BSM).As an illustration, we demonstrate that the measured kinematic distributions of the lepton$+MET$ final state not only determine the mass of the $W$ boson, but are also sensitive to light new physics.","Such a search for new physics thus requires a simultaneous fit to the BSM and SM parameters, \"unifying\" searches and measurements at the LHC and Tevatron.","In this paper, we complete the program initiated in our earlier work arXiv:2310.13687.","In particular, we analyze ($i$) novel decay modes of the $W$ boson with a neutrinophilic invisible scalar or with a heavy neutrino; ($ii$) modified production of $W$ bosons, namely, associated with a hadrophilic invisible $Z^\\prime$ gauge boson; and ($iii$) scenarios without an on-shell $W$ boson, such as slepton-sneutrino production in the Minimal Supersymmetric Standard Model (MSSM).","Here, we complement our previous MSSM analysis in arXiv:2310.13687 by considering a different kinematic region.","Our results highlight that new physics can still be directly discovered at the LHC, including light new physics,via SM precision measurements.","Furthermore, we illustrate that such BSM signals are subtle, yet potentially large enough to affect the precision measurements of SM parameters themselves, such as the $W$ boson mass."],"url":"http://arxiv.org/abs/2404.17574v1","category":"hep-ph"}
{"created":"2024-04-26 17:38:04","title":"Decoherence in Neutrino Oscillation at the ESSnuSB Experiment","abstract":"Neutrino oscillation experiments provide a unique window in exploring several new physics scenarios beyond the standard three flavour. One such scenario is quantum decoherence in neutrino oscillation which tends to destroy the interference pattern of neutrinos reaching the far detector from the source. In this work, we study the decoherence in neutrino oscillation in the context of the ESSnuSB experiment. We consider the energy-independent decoherence parameter and derive the analytical expressions for P$_{\\mu e}$ and P$_{\\mu \\mu}$ probabilities in vacuum. We have computed the capability of ESSnuSB to put bounds on the decoherence parameters namely, $\\Gamma_{21}$ and $\\Gamma_{32}$ and found that the constraints on $\\Gamma_{21}$ are competitive compared to the DUNE bounds and better than the current T2K and MINOS ones. We have also investigated the impact of decoherence on the ESSnuSB measurement of the Dirac CP phase $\\delta_{\\rm CP}$ and concluded that it remains robust in the presence of new physics.","sentences":["Neutrino oscillation experiments provide a unique window in exploring several new physics scenarios beyond the standard three flavour.","One such scenario is quantum decoherence in neutrino oscillation which tends to destroy the interference pattern of neutrinos reaching the far detector from the source.","In this work, we study the decoherence in neutrino oscillation in the context of the ESSnuSB experiment.","We consider the energy-independent decoherence parameter and derive the analytical expressions for P$_{\\mu e}$ and P$_{\\mu \\mu}$ probabilities in vacuum.","We have computed the capability of ESSnuSB to put bounds on the decoherence parameters namely, $\\Gamma_{21}$ and $\\Gamma_{32}$ and found that the constraints on $\\Gamma_{21}$ are competitive compared to the DUNE bounds and better than the current T2K and MINOS ones.","We have also investigated the impact of decoherence on the ESSnuSB measurement of the Dirac CP phase $\\delta_{\\rm CP}$ and concluded that it remains robust in the presence of new physics."],"url":"http://arxiv.org/abs/2404.17559v1","category":"hep-ex"}
{"created":"2024-04-26 16:42:06","title":"QCD analysis of $xF_3$ structure functions in deep-inelastic scattering: Mellin transform by Gegenbauer polynomial up to N$^3$LO approximation","abstract":"This paper provides a thorough examination of the $xF_3$ structure functions in deep-inelastic scattering through a comprehensive QCD analysis. Our approach harnesses sophisticated mathematical techniques, namely the Mellin transform combined with Gegenbauer polynomials. We have employed the Jacobi polynomials approach for analysis, conducting investigations at three levels of precision: Next-to-Leading Order (NLO), Next-to-Next-to-Leading Order (N$^2$LO), and Next-Next-Next-to-Leading Order (N$^3$LO). We have performed a comparison of our sets of valence-quark parton distribution functions with those of recent research groups, specifically CT18 and MSHT20 at NLO and N$^2$LO, and MSTH23 at N$^3$LO, which are concurrent with our current analysis. The combination of Mellin transforms with Gegenbauer polynomials proves to be a powerful tool for investigating the $xF_3$ structure functions in deep-inelastic scattering and the results obtained from our analysis demonstrate a favorable alignment with experimental data.","sentences":["This paper provides a thorough examination of the $xF_3$ structure functions in deep-inelastic scattering through a comprehensive QCD analysis.","Our approach harnesses sophisticated mathematical techniques, namely the Mellin transform combined with Gegenbauer polynomials.","We have employed the Jacobi polynomials approach for analysis, conducting investigations at three levels of precision: Next-to-Leading Order (NLO), Next-to-Next-to-Leading Order (N$^2$LO), and Next-Next-Next-to-Leading Order (N$^3$LO).","We have performed a comparison of our sets of valence-quark parton distribution functions with those of recent research groups, specifically CT18 and MSHT20 at NLO and N$^2$LO, and MSTH23 at N$^3$LO, which are concurrent with our current analysis.","The combination of Mellin transforms with Gegenbauer polynomials proves to be a powerful tool for investigating the $xF_3$ structure functions in deep-inelastic scattering and the results obtained from our analysis demonstrate a favorable alignment with experimental data."],"url":"http://arxiv.org/abs/2404.17526v1","category":"hep-ph"}
{"created":"2024-04-26 15:48:41","title":"The death of Vulcan: NEID reveals the planet candidate orbiting HD 26965 is stellar activity","abstract":"We revisit the long-studied radial velocity (RV) target HD26965 using recent observations from the NASA-NSF 'NEID' precision Doppler facility. Leveraging a suite of classical activity indicators, combined with line-by-line RV analyses, we demonstrate that the claimed 45-day signal previously identified as a planet candidate is most likely an activity-induced signal. Correlating the bulk (spectrally-averaged) RV with canonical line activity indicators confirms a multi-day 'lag' between the observed activity indicator time series and the measured RV. When accounting for this lag, we show that much of the observed RV signal can be removed by a linear detrending of the data. Investigating activity at the line-by-line level, we find a depth-dependent correlation between individual line RVs and the bulk RVs, further indicative of periodic suppression of convective blueshift causing the observed RV variability, rather than an orbiting planet. We conclude that the combined evidence of the activity correlations and depth dependence is consistent with a radial velocity signature dominated by a rotationally-modulated activity signal at a period of $\\sim$42 days. We hypothesize that this activity signature is due to a combination of spots and convective blueshift suppression. The tools applied in our analysis are broadly applicable to other stars, and could help paint a more comprehensive picture of the manifestations of stellar activity in future Doppler RV surveys.","sentences":["We revisit the long-studied radial velocity (RV) target HD26965 using recent observations from the NASA-NSF 'NEID' precision Doppler facility.","Leveraging a suite of classical activity indicators, combined with line-by-line RV analyses, we demonstrate that the claimed 45-day signal previously identified as a planet candidate is most likely an activity-induced signal.","Correlating the bulk (spectrally-averaged) RV with canonical line activity indicators confirms a multi-day 'lag' between the observed activity indicator time series and the measured RV.","When accounting for this lag, we show that much of the observed RV signal can be removed by a linear detrending of the data.","Investigating activity at the line-by-line level, we find a depth-dependent correlation between individual line RVs and the bulk RVs, further indicative of periodic suppression of convective blueshift causing the observed RV variability, rather than an orbiting planet.","We conclude that the combined evidence of the activity correlations and depth dependence is consistent with a radial velocity signature dominated by a rotationally-modulated activity signal at a period of $\\sim$42 days.","We hypothesize that this activity signature is due to a combination of spots and convective blueshift suppression.","The tools applied in our analysis are broadly applicable to other stars, and could help paint a more comprehensive picture of the manifestations of stellar activity in future Doppler RV surveys."],"url":"http://arxiv.org/abs/2404.17494v1","category":"astro-ph.EP"}
{"created":"2024-04-26 15:23:56","title":"Nuclear suppression of coherent $J/\u03c8$ photoproduction in heavy-ion UPCs and leading twist nuclear shadowing","abstract":"We determine the nuclear suppression factor $S_{Pb}(x)$, where $x=M_{J/\\psi}^2/W_{\\gamma p}^2$ with $M_{J/\\psi}$ the $J/\\psi$ mass and $W_{\\gamma p}$ the photon-nucleon energy, for the cross section of coherent $J/\\psi$ photoproduction in heavy-ion ultraperipheral collisions (UPCs) at the Large Hadron Collider (LHC) and Relativistic Heavy Ion Collider (RHIC) by performing the $\\chi^2$ fit to all available data on the cross section $d\\sigma^{AA \\to J/\\psi AA}/dy$ as a function of the $J/\\psi$ rapidity $y$ and the photoproduction cross section $\\sigma^{\\gamma A \\to J/\\psi A}(W_{\\gamma p})$ as a function of $W_{\\gamma p}$. We find that while the $d\\sigma^{AA \\to J/\\psi AA}/dy$ data alone constrain $S_{Pb}(x)$ for $x \\geq 10^{-3}$, the combined $d\\sigma^{AA \\to J/\\psi AA}/dy$ and $\\sigma^{\\gamma A \\to J/\\psi A}(W_{\\gamma p})$ data allow us to determine $S_{Pb}(x)$ in the wide interval $10^{-5} < x < 0.05$. In particular, the data favor $S_{Pb}(x)$, which decreases with a decrease of $x$ in the $10^{-4} < x < 0.01$ interval, and can be both decreasing or constant for $x< 10^{-4}$. Identifying $S_{Pb}(x)$ with the ratio of the gluon distributions in Pb and the proton $R_g(x,Q_0^2)=g_A(x,Q_0^2)/[A g_p(x,Q_0^2)]$, we demonstrate that the leading twist approximation (LTA) for nuclear shadowing provides a good description of all the data on $d\\sigma^{AA \\to J/\\psi AA}/dy$ and $\\sigma^{\\gamma A \\to J/\\psi A}(W_{\\gamma p})$ as well as on the experimental values for $S_{Pb}(x)$ derived from $\\sigma^{\\gamma A \\to J/\\psi A}(W_{\\gamma p})$. We also show that modern nuclear PDFs reasonably reproduce $S_{Pb}(x)$ as well.","sentences":["We determine the nuclear suppression factor $S_{Pb}(x)$, where $x=M_{J/\\psi}^2/W_{\\gamma p}^2$ with $M_{J/\\psi}$ the $J/\\psi$ mass and $W_{\\gamma p}$ the photon-nucleon energy, for the cross section of coherent $J/\\psi$ photoproduction in heavy-ion ultraperipheral collisions (UPCs) at the Large Hadron Collider (LHC) and Relativistic Heavy Ion Collider (RHIC) by performing the $\\chi^2$ fit to all available data on the cross section $d\\sigma^{AA \\to J/\\psi AA}/dy$ as a function of the $J/\\psi$ rapidity $y$ and the photoproduction cross section $\\sigma^{\\gamma A \\to J/\\psi A}(W_{\\gamma p})$ as a function of $W_{\\gamma p}$. We find that while the $d\\sigma^{AA \\to J/\\psi AA}/dy$ data alone constrain $S_{Pb}(x)$ for $x \\geq 10^{-3}$, the combined $d\\sigma^{AA \\to J/\\psi AA}/dy$ and $\\sigma^{\\gamma A \\to J/\\psi A}(W_{\\gamma p})$ data allow us to determine $S_{Pb}(x)$ in the wide interval $10^{-5} <","x < 0.05$.","In particular, the data favor $S_{Pb}(x)$, which decreases with a decrease of $x$ in the $10^{-4} < x < 0.01$ interval, and can be both decreasing or constant for $x< 10^{-4}$. Identifying $S_{Pb}(x)$ with the ratio of the gluon distributions in Pb and the proton $R_g(x,Q_0^2)=g_A(x,Q_0^2)/[A g_p(x,Q_0^2)]$, we demonstrate that the leading twist approximation (LTA) for nuclear shadowing provides a good description of all the data on $d\\sigma^{AA \\to J/\\psi AA}/dy$ and $\\sigma^{\\gamma A \\to J/\\psi A}(W_{\\gamma p})$ as well as on the experimental values for $S_{Pb}(x)$ derived from $\\sigma^{\\gamma A \\to J/\\psi","A}(W_{\\gamma p})$. We also show that modern nuclear PDFs reasonably reproduce $S_{Pb}(x)$ as well."],"url":"http://arxiv.org/abs/2404.17476v1","category":"hep-ph"}
{"created":"2024-04-26 14:47:44","title":"A quasi-linear model of electromagnetic turbulent transport and its application to flux-driven transport predictions for STEP","abstract":"A quasi-linear reduced transport model is developed from a database of high-$\\beta$ electromagnetic nonlinear gyrokinetic simulations performed with Spherical Tokamak for Energy Production (STEP) relevant parameters. The quasi-linear model is fully electromagnetic and accounts for the effect of equilibrium flow shear using a novel approach. Its flux predictions are shown to agree quantitatively with predictions from local nonlinear gyrokinetic simulations across a broad range of STEP-relevant local equilibria. This reduced transport model is implemented in the T3D transport solver that is used to perform the first flux-driven simulations for STEP to account for transport from hybrid-KBM turbulence, which dominates over a wide region of the core plasma. Nonlinear gyrokinetic simulations of the final transport steady state from T3D return turbulent fluxes that are consistent with the reduced model, indicating that the quasi-linear model may also be appropriate for describing the transport steady state. Within the assumption considered here, our simulations support the existence of a transport steady state in STEP with a fusion power comparable to that in the burning flat-top of the conceptual design, but do not demonstrate how this state can be accessed.","sentences":["A quasi-linear reduced transport model is developed from a database of high-$\\beta$ electromagnetic nonlinear gyrokinetic simulations performed with Spherical Tokamak for Energy Production (STEP) relevant parameters.","The quasi-linear model is fully electromagnetic and accounts for the effect of equilibrium flow shear using a novel approach.","Its flux predictions are shown to agree quantitatively with predictions from local nonlinear gyrokinetic simulations across a broad range of STEP-relevant local equilibria.","This reduced transport model is implemented in the T3D transport solver that is used to perform the first flux-driven simulations for STEP to account for transport from hybrid-KBM turbulence, which dominates over a wide region of the core plasma.","Nonlinear gyrokinetic simulations of the final transport steady state from T3D return turbulent fluxes that are consistent with the reduced model, indicating that the quasi-linear model may also be appropriate for describing the transport steady state.","Within the assumption considered here, our simulations support the existence of a transport steady state in STEP with a fusion power comparable to that in the burning flat-top of the conceptual design, but do not demonstrate how this state can be accessed."],"url":"http://arxiv.org/abs/2404.17453v1","category":"physics.plasm-ph"}
{"created":"2024-04-26 14:30:18","title":"Metrology of microwave fields based on trap-loss spectroscopy with cold Rydberg atoms","abstract":"We demonstrate a new approach for the metrology of microwave fields based on the trap-loss-spectroscopy of cold Rydberg atoms in a magneto-optical trap. Compared to state-of-the-art sensors using room-temperature vapors, cold atoms allow longer interaction times, better isolation from the environment and a reduced Doppler effect. Our approach is particularly simple as the detection relies on fluorescence measurements only. Moreover, our signal is well described by a two-level model across a broad measurement range, allowing in principle to reconstruct the amplitude and the frequency of the microwave field simultaneously without the need for an external reference field. We report on a scale factor linearity at the percent level and no noticeable drifts over two hours, paving the way for new applications of cold Rydberg atoms in metrology such as calibrating blackbody shifts in state-of-the-art optical clocks, monitoring the Earth cryosphere from space, measuring the cosmic microwave background or searching for dark matter.","sentences":["We demonstrate a new approach for the metrology of microwave fields based on the trap-loss-spectroscopy of cold Rydberg atoms in a magneto-optical trap.","Compared to state-of-the-art sensors using room-temperature vapors, cold atoms allow longer interaction times, better isolation from the environment and a reduced Doppler effect.","Our approach is particularly simple as the detection relies on fluorescence measurements only.","Moreover, our signal is well described by a two-level model across a broad measurement range, allowing in principle to reconstruct the amplitude and the frequency of the microwave field simultaneously without the need for an external reference field.","We report on a scale factor linearity at the percent level and no noticeable drifts over two hours, paving the way for new applications of cold Rydberg atoms in metrology such as calibrating blackbody shifts in state-of-the-art optical clocks, monitoring the Earth cryosphere from space, measuring the cosmic microwave background or searching for dark matter."],"url":"http://arxiv.org/abs/2404.17445v1","category":"physics.atom-ph"}
{"created":"2024-04-26 14:27:00","title":"Disentangling left and right-handed neutrino effects in $B\\rightarrow K^{(*)}\u03bd\u03bd$","abstract":"The first observation of $\\mathcal{B}\\left(B^+\\rightarrow K^+\\nu\\nu\\right)$ by the Belle II experiment lies almost $3\\sigma$ away from the Standard Model expectation. In this letter we study this result in the SMEFT, extended by a light right-handed neutrino. We explore the correlations between the measured decay rate and other observables, such as $\\mathcal{B}\\left(B\\rightarrow K^*\\nu\\nu\\right)$ and $F_L\\left(B\\rightarrow K^*\\nu\\nu\\right)$, showing that they could disentangle among scenarios involving left-handed neutrinos and those with the right-handed ones. Furthermore, we find that the high-$p_T$ tails of Drell-Yan processes studied at LHC provide important constraints that help us exclude some of the scenarios consistent with the Belle II result.","sentences":["The first observation of $\\mathcal{B}\\left(B^+\\rightarrow K^+\\nu\\nu\\right)$ by the Belle II experiment lies almost $3\\sigma$ away from the Standard Model expectation.","In this letter we study this result in the SMEFT, extended by a light right-handed neutrino.","We explore the correlations between the measured decay rate and other observables, such as $\\mathcal{B}\\left(B\\rightarrow K^*\\nu\\nu\\right)$ and $F_L\\left(B\\rightarrow K^*\\nu\\nu\\right)$, showing that they could disentangle among scenarios involving left-handed neutrinos and those with the right-handed ones.","Furthermore, we find that the high-$p_T$ tails of Drell-Yan processes studied at LHC provide important constraints that help us exclude some of the scenarios consistent with the Belle II result."],"url":"http://arxiv.org/abs/2404.17440v1","category":"hep-ph"}
{"created":"2024-04-26 14:21:43","title":"NH$_3$ adsorption and competition with H$_2$O on a hydroxylated aluminosilicate surface","abstract":"The interaction between ammonia (NH$_3$) and (alumino)silicates is of fundamental and applied importance, yet the specifics of NH$_3$ adsorption on silicate surfaces remain largely unexplored, mainly because of experimental challenges related to their electrically insulating nature. An example of this knowledge gap is evident in the context of ice nucleation on silicate dust, wherein the role of NH$_3$ for ice nucleation remains debated. This study explores the fundamentals of the interaction between NH$_3$ and microcline feldspar (KAlSi$_3$O$_8$), a common aluminosilicate with outstanding ice nucleation abilities. Atomically resolved non-contact atomic force microscopy, x-ray photoelectron spectroscopy, and density functional theory-based calculations elucidate the adsorption geometry of NH$_3$ on the lowest-energy surface of microcline, the (001) facet, and its interplay with surface hydroxyls and molecular water. NH$_3$ and H$_2$O are found to adsorb molecularly in the same adsorption sites, creating H-bonds with the proximate surface silanol (Si-OH) and aluminol (Al-OH) groups. Despite the closely matched adsorption energies of the two molecules, NH$_3$ readily yields to replacement by H$_2$O, challenging the notion that ice nucleation on microcline proceeds via the creation of an ordered H$_2$O layer atop pre-adsorbed NH$_3$ molecules.","sentences":["The interaction between ammonia (NH$_3$) and (alumino)silicates is of fundamental and applied importance, yet the specifics of NH$_3$ adsorption on silicate surfaces remain largely unexplored, mainly because of experimental challenges related to their electrically insulating nature.","An example of this knowledge gap is evident in the context of ice nucleation on silicate dust, wherein the role of NH$_3$ for ice nucleation remains debated.","This study explores the fundamentals of the interaction between NH$_3$ and microcline feldspar (KAlSi$_3$O$_8$), a common aluminosilicate with outstanding ice nucleation abilities.","Atomically resolved non-contact atomic force microscopy, x-ray photoelectron spectroscopy, and density functional theory-based calculations elucidate the adsorption geometry of NH$_3$ on the lowest-energy surface of microcline, the (001) facet, and its interplay with surface hydroxyls and molecular water.","NH$_3$ and H$_2$O are found to adsorb molecularly in the same adsorption sites, creating H-bonds with the proximate surface silanol (Si-OH) and aluminol (Al-OH) groups.","Despite the closely matched adsorption energies of the two molecules, NH$_3$ readily yields to replacement by H$_2$O, challenging the notion that ice nucleation on microcline proceeds via the creation of an ordered H$_2$O layer atop pre-adsorbed NH$_3$ molecules."],"url":"http://arxiv.org/abs/2404.17436v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-26 14:01:14","title":"Weyl nodes in Ce$_3$Bi$_4$Pd$_3$ revealed by dynamical mean-field theory","abstract":"Experimental studies have found unusual transport properties in Ce$_3$Bi$_4$Pd$_3$ which are potentially a consequence of the interplay between band-structure topology and electronic correlations. Based on these measurements, the existence of Weyl points in strongly renormalized, flat quasiparticle bands has been postulated. However, so far, there has been neither a direct spectroscopic observation of these, nor a calculation from first principles that would confirm their existence close to the Fermi energy. Here, we present density functional theory (DFT) and dynamical mean field theory (DMFT) calculations and study the low-energy excitations and their topological properties. We find that the Kondo effect promotes two out of the six angular momentum $J=5/2$ states, with the other four pushed to higher energies. Further, we find Weyl nodes close to the Fermi energy as previously suggested for explaining the observed giant spontaneous Hall effect in Ce$_3$Bi$_4$Pd$_3$, as well as nodal lines.","sentences":["Experimental studies have found unusual transport properties in Ce$_3$Bi$_4$Pd$_3$ which are potentially a consequence of the interplay between band-structure topology and electronic correlations.","Based on these measurements, the existence of Weyl points in strongly renormalized, flat quasiparticle bands has been postulated.","However, so far, there has been neither a direct spectroscopic observation of these, nor a calculation from first principles that would confirm their existence close to the Fermi energy.","Here, we present density functional theory (DFT) and dynamical mean field theory (DMFT) calculations and study the low-energy excitations and their topological properties.","We find that the Kondo effect promotes two out of the six angular momentum $J=5/2$ states, with the other four pushed to higher energies.","Further, we find Weyl nodes close to the Fermi energy as previously suggested for explaining the observed giant spontaneous Hall effect in Ce$_3$Bi$_4$Pd$_3$, as well as nodal lines."],"url":"http://arxiv.org/abs/2404.17425v1","category":"cond-mat.str-el"}
{"created":"2024-04-26 13:55:32","title":"Indirect detection of dark matter absorption in the Galactic Center","abstract":"We consider the nuclear absorption of dark matter as an alternative to the typical indirect detection search channels of dark matter decay or annihilation. In this scenario, an atomic nucleus transitions to an excited state by absorbing a pseudoscalar dark matter particle and promptly emits a photon as it transitions back to its ground state. The nuclear excitation of carbon and oxygen in the Galactic Center would produce a discrete photon spectrum in the $\\mathcal{O}(10)$ MeV range that could be detected by gamma-ray telescopes. Using the \\texttt{BIGSTICK} large-scale shell-model code, we calculate the excitation energies of carbon and oxygen. We constrain the dark matter-nucleus coupling for current COMPTEL data, and provide projections for future experiments AMEGO-X, e-ASTROGAM, and GRAMS for dark matter masses from $\\sim$ 10 to 30 MeV. We find the excitation process to be very sensitive to the dark matter mass and find that the future experiments considered would improve constraints on the dark matter-nucleus coupling within an order of magnitude.","sentences":["We consider the nuclear absorption of dark matter as an alternative to the typical indirect detection search channels of dark matter decay or annihilation.","In this scenario, an atomic nucleus transitions to an excited state by absorbing a pseudoscalar dark matter particle and promptly emits a photon as it transitions back to its ground state.","The nuclear excitation of carbon and oxygen in the Galactic Center would produce a discrete photon spectrum in the $\\mathcal{O}(10)$ MeV range that could be detected by gamma-ray telescopes.","Using the \\texttt{BIGSTICK} large-scale shell-model code, we calculate the excitation energies of carbon and oxygen.","We constrain the dark matter-nucleus coupling for current COMPTEL data, and provide projections for future experiments AMEGO-X, e-ASTROGAM, and GRAMS for dark matter masses from $\\sim$ 10 to 30 MeV.","We find the excitation process to be very sensitive to the dark matter mass and find that the future experiments considered would improve constraints on the dark matter-nucleus coupling within an order of magnitude."],"url":"http://arxiv.org/abs/2404.17418v1","category":"hep-ph"}
{"created":"2024-04-26 13:27:04","title":"Analyzing the Accessibility of GitHub Repositories for PyPI and NPM Libraries","abstract":"Industrial applications heavily rely on open-source software (OSS) libraries, which provide various benefits. But, they can also present a substantial risk if a vulnerability or attack arises and the community fails to promptly address the issue and release a fix due to inactivity. To be able to monitor the activities of such communities, a comprehensive list of repositories for the libraries of an ecosystem must be accessible. Based on these repositories, integrated libraries of an application can be monitored to observe whether they are adequately maintained. In this descriptive study, we analyze the accessibility of GitHub repositories for PyPI and NPM libraries. For all available libraries, we extract assigned repository URLs, direct dependencies and use the page rank algorithm to comprehensively analyze the ecosystems from a library and dependency chain perspective. For invalid repository URLs, we derive potential reasons. Both ecosystems show varying accessibility to GitHub repository URLs, depending on the page rank score of the analyzed libraries. For individual libraries, up to 73.8% of PyPI and up to 69.4% of NPM libraries have repository URLs. Within dependency chains, up to 80.1% of PyPI libraries have URLs, while up to 81.1% for NPM. That means, most libraries, especially the ones of increasing importance, can be monitored on GitHub. Among the most common reasons for invalid repository URLs is no URLs being assigned at all, which amounts up to 17.9% for PyPI and up to 39.6% for NPM. Package maintainers should address this issue and update the repository information to enable monitoring of their libraries.","sentences":["Industrial applications heavily rely on open-source software (OSS) libraries, which provide various benefits.","But, they can also present a substantial risk if a vulnerability or attack arises and the community fails to promptly address the issue and release a fix due to inactivity.","To be able to monitor the activities of such communities, a comprehensive list of repositories for the libraries of an ecosystem must be accessible.","Based on these repositories, integrated libraries of an application can be monitored to observe whether they are adequately maintained.","In this descriptive study, we analyze the accessibility of GitHub repositories for PyPI and NPM libraries.","For all available libraries, we extract assigned repository URLs, direct dependencies and use the page rank algorithm to comprehensively analyze the ecosystems from a library and dependency chain perspective.","For invalid repository URLs, we derive potential reasons.","Both ecosystems show varying accessibility to GitHub repository URLs, depending on the page rank score of the analyzed libraries.","For individual libraries, up to 73.8% of PyPI and up to 69.4% of NPM libraries have repository URLs.","Within dependency chains, up to 80.1% of PyPI libraries have URLs, while up to 81.1% for NPM.","That means, most libraries, especially the ones of increasing importance, can be monitored on GitHub.","Among the most common reasons for invalid repository URLs is no URLs being assigned at all, which amounts up to 17.9% for PyPI and up to 39.6% for NPM.","Package maintainers should address this issue and update the repository information to enable monitoring of their libraries."],"url":"http://arxiv.org/abs/2404.17403v1","category":"cs.SE"}
{"created":"2024-04-26 11:51:01","title":"The SRG/eROSITA All-Sky Survey: Exploring halo assembly bias with X-ray selected superclusters","abstract":"We explore halo assembly bias on cluster scales using large samples of superclusters. Leveraging the largest-ever X-ray galaxy cluster and supercluster samples obtained from the first SRG/eROSITA all-sky survey, we construct two subsamples of galaxy clusters which consist of supercluster members (SC) and isolated clusters (ISO) respectively. After correcting the selection effects on redshift, mass, and survey depth, we compute the excess in the concentration of the intracluster gas of isolated clusters with respect to supercluster members, defined as $\\delta c_{\\rm gas} \\equiv c_{\\rm gas,ISO}/c_{\\rm gas,SC}-1$, to investigate the environmental effect on the concentration of clusters, an inference of halo assembly bias on cluster scales. We find that the average gas mass concentration of isolated clusters is a few percent higher than that of supercluster members, with a significance of $2\\sigma$. The result on $\\delta c_{\\rm gas}$ varies with the overdensity ratio $f$ in supercluster identification, cluster mass proxies, and mass ranges, but remains positive in all the measurements. We measure slightly larger $\\delta c_{\\rm gas}$ when adopting a higher $f$ in supercluster identification. $\\delta c_{\\rm gas}$ is also larger for low-mass clusters, and almost negligible for high-mass clusters. We perform weak lensing analyses to compare the total mass concentration of the two classes and find a similar trend as obtained from gas mass concentration. Our results are consistent with the prediction of HAB on cluster scales, where halos located in denser environments are less concentrated, and this trend is stronger for low-mass halos than for massive halos. These phenomena can be interpreted by the fact that clusters in denser environments such as superclusters have experienced more mergers than isolated clusters in their assembling history.","sentences":["We explore halo assembly bias on cluster scales using large samples of superclusters.","Leveraging the largest-ever X-ray galaxy cluster and supercluster samples obtained from the first SRG/eROSITA all-sky survey, we construct two subsamples of galaxy clusters which consist of supercluster members (SC) and isolated clusters (ISO) respectively.","After correcting the selection effects on redshift, mass, and survey depth, we compute the excess in the concentration of the intracluster gas of isolated clusters with respect to supercluster members, defined as $\\delta c_{\\rm gas} \\equiv c_{\\rm gas,ISO}/c_{\\rm gas,SC}-1$, to investigate the environmental effect on the concentration of clusters, an inference of halo assembly bias on cluster scales.","We find that the average gas mass concentration of isolated clusters is a few percent higher than that of supercluster members, with a significance of $2\\sigma$. The result on $\\delta c_{\\rm gas}$ varies with the overdensity ratio $f$ in supercluster identification, cluster mass proxies, and mass ranges, but remains positive in all the measurements.","We measure slightly larger $\\delta c_{\\rm gas}$ when adopting a higher $f$ in supercluster identification.","$\\delta c_{\\rm gas}$ is also larger for low-mass clusters, and almost negligible for high-mass clusters.","We perform weak lensing analyses to compare the total mass concentration of the two classes and find a similar trend as obtained from gas mass concentration.","Our results are consistent with the prediction of HAB on cluster scales, where halos located in denser environments are less concentrated, and this trend is stronger for low-mass halos than for massive halos.","These phenomena can be interpreted by the fact that clusters in denser environments such as superclusters have experienced more mergers than isolated clusters in their assembling history."],"url":"http://arxiv.org/abs/2404.17345v1","category":"astro-ph.CO"}
{"created":"2024-04-26 11:43:43","title":"Free curves in Fano hypersurfaces must have high degree","abstract":"The purpose of this note is to show that the minimal $e$ for which every smooth Fano hypersurface of dimension $n$ contains a free rational curve of degree at most $e$ cannot be bounded by a linear function in $n$ when the base field has positive characteristic. This is done by providing a super-linear bound on the minimal possible degree of a free curve in certain Fermat hypersurfaces.","sentences":["The purpose of this note is to show that the minimal $e$ for which every smooth Fano hypersurface of dimension $n$ contains a free rational curve of degree at most $e$ cannot be bounded by a linear function in $n$ when the base field has positive characteristic.","This is done by providing a super-linear bound on the minimal possible degree of a free curve in certain Fermat hypersurfaces."],"url":"http://arxiv.org/abs/2404.17341v1","category":"math.AG"}
{"created":"2024-04-26 11:39:08","title":"Identical particles as a genuine non-local resource","abstract":"All particles of the same type are indistinguishable, according to a fundamental quantum principle. This entails a description of many-particle states using symmetrised or anti-symmetrised wave functions, which turn out to be formally entangled. However, the measurement of individual particles is hampered by a mode description in the second-quantised theory that masks this entanglement. Is it nonetheless possible to use such states as a resource in Bell-type experiments? More specifically, which states of identical particles can demonstrate non-local correlations in passive linear optical setups that are considered purely classical component of the experiment? Here, the problem is fully solved for multi-particle states with a definite number of identical particles. We show that all fermion states and most boson states provide a sufficient quantum resource to exhibit non-locality in classical optical setups. The only exception is a special class of boson states that are reducible to a single mode, which turns out to be locally simulable for any passive linear optical experiment. This finding highlights the connection between the basic concept of particle indistinguishability and Bell non-locality, which can be observed by classical means for almost every state of identical particles.","sentences":["All particles of the same type are indistinguishable, according to a fundamental quantum principle.","This entails a description of many-particle states using symmetrised or anti-symmetrised wave functions, which turn out to be formally entangled.","However, the measurement of individual particles is hampered by a mode description in the second-quantised theory that masks this entanglement.","Is it nonetheless possible to use such states as a resource in Bell-type experiments?","More specifically, which states of identical particles can demonstrate non-local correlations in passive linear optical setups that are considered purely classical component of the experiment?","Here, the problem is fully solved for multi-particle states with a definite number of identical particles.","We show that all fermion states and most boson states provide a sufficient quantum resource to exhibit non-locality in classical optical setups.","The only exception is a special class of boson states that are reducible to a single mode, which turns out to be locally simulable for any passive linear optical experiment.","This finding highlights the connection between the basic concept of particle indistinguishability and Bell non-locality, which can be observed by classical means for almost every state of identical particles."],"url":"http://arxiv.org/abs/2404.17339v1","category":"quant-ph"}
{"created":"2024-04-26 10:01:28","title":"Bayesian insights in Tycho supernova remnant : a detailed mapping of ejecta properties","abstract":"While Tycho's supernova remnant is one of the most studied type Ia Galactic supernova remnants, a global view of the physical properties of its ejecta is lacking, to understand its mysteries. In particular, the spatial distribution of the Si-rich ejecta line-of-sight velocity presents a large-scale unexplained asymmetry, with the north dominantly blueshifted and the south redshifted. To investigate the origin of this line-of-sight velocity asymmetry in the ejecta, we carry out a detailed X-ray spatially-resolved spectral analysis of the entire shocked ejecta in Tycho's SNR to determine the physical properties of its various components. This study is based on the archival deep X-ray observations from the Chandra space telescope. The spatially-resolved spectral analysis in 211 regions over the entire SNR is based on a tesselation method applied to the line-of-sight velocity map. A Bayesian tool is used to conduct the fitting, using a nested sampling algorithm. It allows us to obtain a complete view of the statistical landscape. We provide maps of the physical parameters of the various components across the SNR ejecta. The Doppler shift map confirms spectrally the large-scale north-south asymmetry in the line-of-sight velocity. We reveal different spatial distributions of temperature and ionization time for IMEs and for iron-rich ejecta, but none of these maps shows structure associated to the large-scale north-south asymmetry in the line-of-sight velocity distribution. The abundance maps show spatial variations, depending on the element, perhaps due to an origin in different layers during the explosion. We compare these abundances with some nucleosynthesis models. In addition, we observe for the first time an emission line at 0.654 keV possibly related to oxygen. Its spatial distribution differs from the other elements, so that this line may arise in the ambient medium.","sentences":["While Tycho's supernova remnant is one of the most studied type Ia Galactic supernova remnants, a global view of the physical properties of its ejecta is lacking, to understand its mysteries.","In particular, the spatial distribution of the Si-rich ejecta line-of-sight velocity presents a large-scale unexplained asymmetry, with the north dominantly blueshifted and the south redshifted.","To investigate the origin of this line-of-sight velocity asymmetry in the ejecta, we carry out a detailed X-ray spatially-resolved spectral analysis of the entire shocked ejecta in Tycho's SNR to determine the physical properties of its various components.","This study is based on the archival deep X-ray observations from the Chandra space telescope.","The spatially-resolved spectral analysis in 211 regions over the entire SNR is based on a tesselation method applied to the line-of-sight velocity map.","A Bayesian tool is used to conduct the fitting, using a nested sampling algorithm.","It allows us to obtain a complete view of the statistical landscape.","We provide maps of the physical parameters of the various components across the SNR ejecta.","The Doppler shift map confirms spectrally the large-scale north-south asymmetry in the line-of-sight velocity.","We reveal different spatial distributions of temperature and ionization time for IMEs and for iron-rich ejecta, but none of these maps shows structure associated to the large-scale north-south asymmetry in the line-of-sight velocity distribution.","The abundance maps show spatial variations, depending on the element, perhaps due to an origin in different layers during the explosion.","We compare these abundances with some nucleosynthesis models.","In addition, we observe for the first time an emission line at 0.654 keV possibly related to oxygen.","Its spatial distribution differs from the other elements, so that this line may arise in the ambient medium."],"url":"http://arxiv.org/abs/2404.17296v1","category":"astro-ph.HE"}
{"created":"2024-04-26 09:42:26","title":"Black Hole Singularity from OPE","abstract":"Eternal asymptotically AdS black holes are dual to thermofield double states in the boundary CFT. It has long been known that black hole singularities have certain signatures in boundary thermal two-point functions related to null geodesics bouncing off the singularities (bouncing geodesics). In this paper we shed light on the manifestations of black hole singularities in the dual CFT. We decompose the boundary CFT correlator of scalar operators using the Operator Product Expansion (OPE) and focus on the contributions from the identity, the stress tensor, and its products. We show that this part of the correlator develops singularities precisely at the points that are connected by bulk bouncing geodesics. Black hole singularities are thus encoded in the analytic behavior of the boundary correlators determined by multiple stress tensor exchanges. Furthermore, we show that in the limit where the conformal dimension of the operators is large, the sum of multi-stress-tensor contributions develops a branch point singularity as predicted by the geodesic analysis. We also argue that the appearance of complexified geodesics, which play an important role in computing the full correlator, is related to the contributions of the double-trace operators in the boundary CFT.","sentences":["Eternal asymptotically AdS black holes are dual to thermofield double states in the boundary CFT.","It has long been known that black hole singularities have certain signatures in boundary thermal two-point functions related to null geodesics bouncing off the singularities (bouncing geodesics).","In this paper we shed light on the manifestations of black hole singularities in the dual CFT.","We decompose the boundary CFT correlator of scalar operators using the Operator Product Expansion (OPE) and focus on the contributions from the identity, the stress tensor, and its products.","We show that this part of the correlator develops singularities precisely at the points that are connected by bulk bouncing geodesics.","Black hole singularities are thus encoded in the analytic behavior of the boundary correlators determined by multiple stress tensor exchanges.","Furthermore, we show that in the limit where the conformal dimension of the operators is large, the sum of multi-stress-tensor contributions develops a branch point singularity as predicted by the geodesic analysis.","We also argue that the appearance of complexified geodesics, which play an important role in computing the full correlator, is related to the contributions of the double-trace operators in the boundary CFT."],"url":"http://arxiv.org/abs/2404.17286v1","category":"hep-th"}
{"created":"2024-04-26 09:41:31","title":"Influence of ammonia-water fog formation on ammonia dispersion from a liquid spill","abstract":"Ammonia is expected to play an important role in the green transition, both as a hydrogen carrier and a zero-emission fuel. The use of refrigerated ammonia is attractive due to its relatively high volumetric energy density and increased safety compared to pressurized solutions. Ammonia is highly toxic, and with new applications and increased global demand come stricter requirements for safe handling. Cold gaseous ammonia following a spill of refrigerated ammonia will in contact with humid air cause fog formation. In an environment rich in ammonia, these droplets will due to ammonia's strong hygroscopicity consist of considerable amounts of liquid ammonia as well as water. Fog formation affects the ammonia-air density and thus influences the dispersion dynamics, with a potentially significant impact on hazardous zones. In this work, we present a CFD model including an ammonia-water fog formation model based on accurate thermodynamics. This includes modeling the vapor-liquid equilibrium and accounting for the exothermic mixing of ammonia and water. We apply this CFD model to relevant cases and demonstrate the significant impact of the fog. We analyze the effect of varying relative humidity, fog visibility, influence of wind, and pool evaporation rate. Finally, we model the experimental Red Squirrel test 1F.","sentences":["Ammonia is expected to play an important role in the green transition, both as a hydrogen carrier and a zero-emission fuel.","The use of refrigerated ammonia is attractive due to its relatively high volumetric energy density and increased safety compared to pressurized solutions.","Ammonia is highly toxic, and with new applications and increased global demand come stricter requirements for safe handling.","Cold gaseous ammonia following a spill of refrigerated ammonia will in contact with humid air cause fog formation.","In an environment rich in ammonia, these droplets will due to ammonia's strong hygroscopicity consist of considerable amounts of liquid ammonia as well as water.","Fog formation affects the ammonia-air density and thus influences the dispersion dynamics, with a potentially significant impact on hazardous zones.","In this work, we present a CFD model including an ammonia-water fog formation model based on accurate thermodynamics.","This includes modeling the vapor-liquid equilibrium and accounting for the exothermic mixing of ammonia and water.","We apply this CFD model to relevant cases and demonstrate the significant impact of the fog.","We analyze the effect of varying relative humidity, fog visibility, influence of wind, and pool evaporation rate.","Finally, we model the experimental Red Squirrel test 1F."],"url":"http://arxiv.org/abs/2404.17285v1","category":"physics.flu-dyn"}
{"created":"2024-04-26 09:15:37","title":"A two parameter family of lightcone-like hyperbolic string vertices","abstract":"We introduce a two parameter family of string field theory vertices, which we refer to as hyperbolic Kaku vertices. It is defined in terms of hyperbolic metrics on the Riemann surface, but the geometry is allowed to depend on inputs of the states. The vertices are defined for both open and closed strings. In either case, the family contains the hyperbolic vertices. Then we show that the open string lightcone vertex is obtained as the flat limit of the hyperbolic Kaku vertices. The open string Kaku vertices, which interpolate between the Witten vertex and the open string lightcone vertex, is also obtained as a flat limit. We use the same limit on the case of closed strings to define the closed string Kaku vertices: a one parameter family of vertices that interpolates between the polyhedral vertices - which are covariant, but not cubic - and the closed string lightcone vertex - which is cubic, but not Lorentz covariant.","sentences":["We introduce a two parameter family of string field theory vertices, which we refer to as hyperbolic Kaku vertices.","It is defined in terms of hyperbolic metrics on the Riemann surface, but the geometry is allowed to depend on inputs of the states.","The vertices are defined for both open and closed strings.","In either case, the family contains the hyperbolic vertices.","Then we show that the open string lightcone vertex is obtained as the flat limit of the hyperbolic Kaku vertices.","The open string Kaku vertices, which interpolate between the Witten vertex and the open string lightcone vertex, is also obtained as a flat limit.","We use the same limit on the case of closed strings to define the closed string Kaku vertices: a one parameter family of vertices that interpolates between the polyhedral vertices - which are covariant, but not cubic - and the closed string lightcone vertex - which is cubic, but not Lorentz covariant."],"url":"http://arxiv.org/abs/2404.17268v1","category":"hep-th"}
{"created":"2024-04-26 08:38:55","title":"Canonical interpretation of the newly observed $J^P =1^+$ structure $X(2085)$","abstract":"Inspired by the newly observed $X(2085)$ by the BESIII Collaboration, we study the strong decay behaviors of excited axialvector strange mesons within the quark pair creation model. Our results indicate that the $K_1(1793)/K_1(1861)$ can be regarded as the same $K_1(2P)$ state, and the $K_1(1911)$ is assigned as the $K_1(2P^\\prime)$ state. Considering the mass, spin-parity, and decay behaviors, we interpret the newly observed $X(2085)$ as the radially excited $K_1(3P)$ state, which mainly decays into the $\\rho(1450) K$, $\\omega(1420)K$, $\\pi K^*(1410)$, $\\rho K_1(1270)$, and $\\rho K^*(892)$ final states. Also, the width of $K_1(3P^\\prime)$ state is predicted to be about 300 MeV, which can be searched for by future experiments. We expect that present calculations can help us to better understand the nature of the $X(2085)$ structure.","sentences":["Inspired by the newly observed $X(2085)$ by the BESIII Collaboration, we study the strong decay behaviors of excited axialvector strange mesons within the quark pair creation model.","Our results indicate that the $K_1(1793)/K_1(1861)$ can be regarded as the same $K_1(2P)$ state, and the $K_1(1911)$ is assigned as the $K_1(2P^\\prime)$ state.","Considering the mass, spin-parity, and decay behaviors, we interpret the newly observed $X(2085)$ as the radially excited $K_1(3P)$ state, which mainly decays into the $\\rho(1450) K$, $\\omega(1420)K$, $\\pi K^*(1410)$, $\\rho K_1(1270)$, and $\\rho K^*(892)$ final states.","Also, the width of $K_1(3P^\\prime)$ state is predicted to be about 300 MeV, which can be searched for by future experiments.","We expect that present calculations can help us to better understand the nature of the $X(2085)$ structure."],"url":"http://arxiv.org/abs/2404.17246v1","category":"hep-ph"}
{"created":"2024-04-26 08:25:25","title":"TALYS calculation and a short review of the experimental status of proton capture studies on p-nuclei: A guide to future investigation","abstract":"TALYS calculations were performed to obtain the theoretical proton capture cross-sections on the p-nuclei. A short review on the status of related experimental studies was also conducted. Some basic properties such as Q-values, Coulomb barrier, Gamow peak, Gamow Window, and decay properties of the parent and daughter nuclei were studied. Various experimental parameters, e.g., beam energy, beam current, targets, and detectors, used in experimental investigations reported in the literature, were tabulated. The results of the TALYS calculations in the Gamow region were compared with the corresponding experimental values wherever available. This study is expected to facilitate the planning of future experiments.","sentences":["TALYS calculations were performed to obtain the theoretical proton capture cross-sections on the p-nuclei.","A short review on the status of related experimental studies was also conducted.","Some basic properties such as Q-values, Coulomb barrier, Gamow peak, Gamow Window, and decay properties of the parent and daughter nuclei were studied.","Various experimental parameters, e.g., beam energy, beam current, targets, and detectors, used in experimental investigations reported in the literature, were tabulated.","The results of the TALYS calculations in the Gamow region were compared with the corresponding experimental values wherever available.","This study is expected to facilitate the planning of future experiments."],"url":"http://arxiv.org/abs/2404.17240v1","category":"nucl-th"}
