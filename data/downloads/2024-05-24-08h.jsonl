{"created":"2024-05-23 17:59:58","title":"Replica Wormholes and Entanglement Islands in the Karch-Randall Braneworld","abstract":"The Karch-Randall braneworld provides a natural set-up to study the Hawking radiation from a black hole using holographic tools. Such a black hole lives on a brane and is highly quantum yet has a holographic dual as a higher dimensional classical theory that lives in the ambient space. Moreover, such a black hole is coupled to a nongravitational bath which is absorbing its Hawking radiation. This allows us to compute the entropy of the Hawking radiation by studying the bath using the quantum extremal surface prescription. The quantum extremal surface geometrizes into a Ryu-Takayanagi surface in the ambient space. The topological phase transition of the Ryu-Takayanagi surface in time from connecting different portions of the bath to the one connecting the bath and the brane gives the Page curve of the Hawking radiation that is consistent with unitarity. Nevertheless, there doesn't exit a derivation of the quantum extremal surface prescription and its geometrization in the Karch-Randall braneworld. In this paper, we fill this gap. We mainly focus on the case that the ambient space is (2+1)-dimensional for which explicit computations can be done in each description of the set-up. We show that the topological phase transition of the Ryu-Takayanagi surface corresponds to the formation of the replica wormhole on the Karch-Randall brane as the dominate contribution to the replica path integral. We comment on higher dimensional cases for which we argue that in generic situations the replica wormhole configuration is off-shell due to the Witten-Yau theorem. This necessitates the inclusion of smooth constrained saddles into the gravitational path integral for appropriate questions.","sentences":["The Karch-Randall braneworld provides a natural set-up to study the Hawking radiation from a black hole using holographic tools.","Such a black hole lives on a brane and is highly quantum yet has a holographic dual as a higher dimensional classical theory that lives in the ambient space.","Moreover, such a black hole is coupled to a nongravitational bath which is absorbing its Hawking radiation.","This allows us to compute the entropy of the Hawking radiation by studying the bath using the quantum extremal surface prescription.","The quantum extremal surface geometrizes into a Ryu-Takayanagi surface in the ambient space.","The topological phase transition of the Ryu-Takayanagi surface in time from connecting different portions of the bath to the one connecting the bath and the brane gives the Page curve of the Hawking radiation that is consistent with unitarity.","Nevertheless, there doesn't exit a derivation of the quantum extremal surface prescription and its geometrization in the Karch-Randall braneworld.","In this paper, we fill this gap.","We mainly focus on the case that the ambient space is (2+1)-dimensional for which explicit computations can be done in each description of the set-up.","We show that the topological phase transition of the Ryu-Takayanagi surface corresponds to the formation of the replica wormhole on the Karch-Randall brane as the dominate contribution to the replica path integral.","We comment on higher dimensional cases for which we argue that in generic situations the replica wormhole configuration is off-shell due to the Witten-Yau theorem.","This necessitates the inclusion of smooth constrained saddles into the gravitational path integral for appropriate questions."],"url":"http://arxiv.org/abs/2405.14872v1","category":"hep-th"}
{"created":"2024-05-23 17:59:57","title":"An Empirical Study of Training State-of-the-Art LiDAR Segmentation Models","abstract":"In the rapidly evolving field of autonomous driving, precise segmentation of LiDAR data is crucial for understanding complex 3D environments. Traditional approaches often rely on disparate, standalone codebases, hindering unified advancements and fair benchmarking across models. To address these challenges, we introduce MMDetection3D-lidarseg, a comprehensive toolbox designed for the efficient training and evaluation of state-of-the-art LiDAR segmentation models. We support a wide range of segmentation models and integrate advanced data augmentation techniques to enhance robustness and generalization. Additionally, the toolbox provides support for multiple leading sparse convolution backends, optimizing computational efficiency and performance. By fostering a unified framework, MMDetection3D-lidarseg streamlines development and benchmarking, setting new standards for research and application. Our extensive benchmark experiments on widely-used datasets demonstrate the effectiveness of the toolbox. The codebase and trained models have been publicly available, promoting further research and innovation in the field of LiDAR segmentation for autonomous driving.","sentences":["In the rapidly evolving field of autonomous driving, precise segmentation of LiDAR data is crucial for understanding complex 3D environments.","Traditional approaches often rely on disparate, standalone codebases, hindering unified advancements and fair benchmarking across models.","To address these challenges, we introduce MMDetection3D-lidarseg, a comprehensive toolbox designed for the efficient training and evaluation of state-of-the-art LiDAR segmentation models.","We support a wide range of segmentation models and integrate advanced data augmentation techniques to enhance robustness and generalization.","Additionally, the toolbox provides support for multiple leading sparse convolution backends, optimizing computational efficiency and performance.","By fostering a unified framework, MMDetection3D-lidarseg streamlines development and benchmarking, setting new standards for research and application.","Our extensive benchmark experiments on widely-used datasets demonstrate the effectiveness of the toolbox.","The codebase and trained models have been publicly available, promoting further research and innovation in the field of LiDAR segmentation for autonomous driving."],"url":"http://arxiv.org/abs/2405.14870v1","category":"cs.CV"}
{"created":"2024-05-23 17:59:56","title":"PuzzleAvatar: Assembling 3D Avatars from Personal Albums","abstract":"Generating personalized 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if a user could just upload their personal \"OOTD\" (Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel \"Album2Human\" task by developing PuzzleAvatar, a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, while bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into (separate) learned tokens and instilling these cues into the VLM. In effect, we exploit the learned tokens as \"puzzle pieces\" from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total of nearly 1K OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and strong robustness. Our model and data will be public.","sentences":["Generating personalized 3D avatars is crucial for AR/VR.","However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people.","Methods for faithful reconstruction typically require full-body images in controlled settings.","What if a user could just upload their personal \"OOTD\" (Outfit Of The Day) photo collection and get a faithful avatar in return?","The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle).","We address this novel \"Album2Human\" task by developing PuzzleAvatar, a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, while bypassing the challenging estimation of body and camera pose.","To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into (separate) learned tokens and instilling these cues into the VLM.","In effect, we exploit the learned tokens as \"puzzle pieces\" from which we assemble a faithful, personalized 3D avatar.","Importantly, we can customize avatars by simply inter-changing tokens.","As a benchmark for this new task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total of nearly 1K OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies.","Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and strong robustness.","Our model and data will be public."],"url":"http://arxiv.org/abs/2405.14869v1","category":"cs.CV"}
{"created":"2024-05-23 17:59:52","title":"Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis","abstract":"Accurate reconstruction of complex dynamic scenes from just a single viewpoint continues to be a challenging task in computer vision. Current dynamic novel view synthesis methods typically require videos from many different camera viewpoints, necessitating careful recording setups, and significantly restricting their utility in the wild as well as in terms of embodied AI applications. In this paper, we propose $\\textbf{GCD}$, a controllable monocular dynamic view synthesis pipeline that leverages large-scale diffusion priors to, given a video of any scene, generate a synchronous video from any other chosen perspective, conditioned on a set of relative camera pose parameters. Our model does not require depth as input, and does not explicitly model 3D scene geometry, instead performing end-to-end video-to-video translation in order to achieve its goal efficiently. Despite being trained on synthetic multi-view video data only, zero-shot real-world generalization experiments show promising results in multiple domains, including robotics, object permanence, and driving environments. We believe our framework can potentially unlock powerful applications in rich dynamic scene understanding, perception for robotics, and interactive 3D video viewing experiences for virtual reality.","sentences":["Accurate reconstruction of complex dynamic scenes from just a single viewpoint continues to be a challenging task in computer vision.","Current dynamic novel view synthesis methods typically require videos from many different camera viewpoints, necessitating careful recording setups, and significantly restricting their utility in the wild as well as in terms of embodied AI applications.","In this paper, we propose $\\textbf{GCD}$, a controllable monocular dynamic view synthesis pipeline that leverages large-scale diffusion priors to, given a video of any scene, generate a synchronous video from any other chosen perspective, conditioned on a set of relative camera pose parameters.","Our model does not require depth as input, and does not explicitly model 3D scene geometry, instead performing end-to-end video-to-video translation in order to achieve its goal efficiently.","Despite being trained on synthetic multi-view video data only, zero-shot real-world generalization experiments show promising results in multiple domains, including robotics, object permanence, and driving environments.","We believe our framework can potentially unlock powerful applications in rich dynamic scene understanding, perception for robotics, and interactive 3D video viewing experiences for virtual reality."],"url":"http://arxiv.org/abs/2405.14868v1","category":"cs.CV"}
{"created":"2024-05-23 17:59:49","title":"Improved Distribution Matching Distillation for Fast Image Synthesis","abstract":"Recent approaches have shown promises distilling diffusion models into efficient one-step generators. Among them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution, without enforcing a one-to-one correspondence with the sampling trajectories of their teachers. However, to ensure stable training, DMD requires an additional regression loss computed using a large set of noise-image pairs generated by the teacher with many steps of a deterministic sampler. This is costly for large-scale text-to-image synthesis and limits the student's quality, tying it too closely to the teacher's original sampling paths. We introduce DMD2, a set of techniques that lift this limitation and improve DMD training. First, we eliminate the regression loss and the need for expensive dataset construction. We show that the resulting instability is due to the fake critic not estimating the distribution of generated samples accurately and propose a two time-scale update rule as a remedy. Second, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images. This lets us train the student model on real data, mitigating the imperfect real score estimation from the teacher model, and enhancing quality. Lastly, we modify the training procedure to enable multi-step sampling. We identify and address the training-inference input mismatch problem in this setting, by simulating inference-time generator samples during training time. Taken together, our improvements set new benchmarks in one-step image generation, with FID scores of 1.28 on ImageNet-64x64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500X reduction in inference cost. Further, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods.","sentences":["Recent approaches have shown promises distilling diffusion models into efficient one-step generators.","Among them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution, without enforcing a one-to-one correspondence with the sampling trajectories of their teachers.","However, to ensure stable training, DMD requires an additional regression loss computed using a large set of noise-image pairs generated by the teacher with many steps of a deterministic sampler.","This is costly for large-scale text-to-image synthesis and limits the student's quality, tying it too closely to the teacher's original sampling paths.","We introduce DMD2, a set of techniques that lift this limitation and improve DMD training.","First, we eliminate the regression loss and the need for expensive dataset construction.","We show that the resulting instability is due to the fake critic not estimating the distribution of generated samples accurately and propose a two time-scale update rule as a remedy.","Second, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images.","This lets us train the student model on real data, mitigating the imperfect real score estimation from the teacher model, and enhancing quality.","Lastly, we modify the training procedure to enable multi-step sampling.","We identify and address the training-inference input mismatch problem in this setting, by simulating inference-time generator samples during training time.","Taken together, our improvements set new benchmarks in one-step image generation, with FID scores of 1.28 on ImageNet-64x64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500X reduction in inference cost.","Further, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods."],"url":"http://arxiv.org/abs/2405.14867v1","category":"cs.CV"}
{"created":"2024-05-23 17:59:45","title":"Tele-Aloha: A Low-budget and High-authenticity Telepresence System Using Sparse RGB Cameras","abstract":"In this paper, we present a low-budget and high-authenticity bidirectional telepresence system, Tele-Aloha, targeting peer-to-peer communication scenarios. Compared to previous systems, Tele-Aloha utilizes only four sparse RGB cameras, one consumer-grade GPU, and one autostereoscopic screen to achieve high-resolution (2048x2048), real-time (30 fps), low-latency (less than 150ms) and robust distant communication. As the core of Tele-Aloha, we propose an efficient novel view synthesis algorithm for upper-body. Firstly, we design a cascaded disparity estimator for obtaining a robust geometry cue. Additionally a neural rasterizer via Gaussian Splatting is introduced to project latent features onto target view and to decode them into a reduced resolution. Further, given the high-quality captured data, we leverage weighted blending mechanism to refine the decoded image into the final resolution of 2K. Exploiting world-leading autostereoscopic display and low-latency iris tracking, users are able to experience a strong three-dimensional sense even without any wearable head-mounted display device. Altogether, our telepresence system demonstrates the sense of co-presence in real-life experiments, inspiring the next generation of communication.","sentences":["In this paper, we present a low-budget and high-authenticity bidirectional telepresence system, Tele-Aloha, targeting peer-to-peer communication scenarios.","Compared to previous systems, Tele-Aloha utilizes only four sparse RGB cameras, one consumer-grade GPU, and one autostereoscopic screen to achieve high-resolution (2048x2048), real-time (30 fps), low-latency (less than 150ms) and robust distant communication.","As the core of Tele-Aloha, we propose an efficient novel view synthesis algorithm for upper-body.","Firstly, we design a cascaded disparity estimator for obtaining a robust geometry cue.","Additionally a neural rasterizer via Gaussian Splatting is introduced to project latent features onto target view and to decode them into a reduced resolution.","Further, given the high-quality captured data, we leverage weighted blending mechanism to refine the decoded image into the final resolution of 2K. Exploiting world-leading autostereoscopic display and low-latency iris tracking, users are able to experience a strong three-dimensional sense even without any wearable head-mounted display device.","Altogether, our telepresence system demonstrates the sense of co-presence in real-life experiments, inspiring the next generation of communication."],"url":"http://arxiv.org/abs/2405.14866v1","category":"cs.CV"}
{"created":"2024-05-23 17:59:40","title":"Video Diffusion Models are Training-free Motion Interpreter and Controller","abstract":"Video generation primarily aims to model authentic and customized motion across frames, making understanding and controlling the motion a crucial topic. Most diffusion-based studies on video motion focus on motion customization with training-based paradigms, which, however, demands substantial training resources and necessitates retraining for diverse models. Crucially, these approaches do not explore how video diffusion models encode cross-frame motion information in their features, lacking interpretability and transparency in their effectiveness. To answer this question, this paper introduces a novel perspective to understand, localize, and manipulate motion-aware features in video diffusion models. Through analysis using Principal Component Analysis (PCA), our work discloses that robust motion-aware feature already exists in video diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating content correlation information and filtering motion channels. MOFT provides a distinct set of benefits, including the ability to encode comprehensive motion information with clear interpretability, extraction without the need for training, and generalizability across diverse architectures. Leveraging MOFT, we propose a novel training-free video motion control framework. Our method demonstrates competitive performance in generating natural and faithful motion, providing architecture-agnostic insights and applicability in a variety of downstream tasks.","sentences":["Video generation primarily aims to model authentic and customized motion across frames, making understanding and controlling the motion a crucial topic.","Most diffusion-based studies on video motion focus on motion customization with training-based paradigms, which, however, demands substantial training resources and necessitates retraining for diverse models.","Crucially, these approaches do not explore how video diffusion models encode cross-frame motion information in their features, lacking interpretability and transparency in their effectiveness.","To answer this question, this paper introduces a novel perspective to understand, localize, and manipulate motion-aware features in video diffusion models.","Through analysis using Principal Component Analysis (PCA), our work discloses that robust motion-aware feature already exists in video diffusion models.","We present a new MOtion FeaTure (MOFT) by eliminating content correlation information and filtering motion channels.","MOFT provides a distinct set of benefits, including the ability to encode comprehensive motion information with clear interpretability, extraction without the need for training, and generalizability across diverse architectures.","Leveraging MOFT, we propose a novel training-free video motion control framework.","Our method demonstrates competitive performance in generating natural and faithful motion, providing architecture-agnostic insights and applicability in a variety of downstream tasks."],"url":"http://arxiv.org/abs/2405.14864v1","category":"cs.CV"}
{"created":"2024-05-23 17:59:26","title":"A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns","abstract":"Cross-domain alignment refers to the task of mapping a concept from one domain to another. For example, ``If a \\textit{doctor} were a \\textit{color}, what color would it be?''. This seemingly peculiar task is designed to investigate how people represent concrete and abstract concepts through their mappings between categories and their reasoning processes over those mappings. In this paper, we adapt this task from cognitive science to evaluate the conceptualization and reasoning abilities of large language models (LLMs) through a behavioral study. We examine several LLMs by prompting them with a cross-domain mapping task and analyzing their responses at both the population and individual levels. Additionally, we assess the models' ability to reason about their predictions by analyzing and categorizing their explanations for these mappings. The results reveal several similarities between humans' and models' mappings and explanations, suggesting that models represent concepts similarly to humans. This similarity is evident not only in the model representation but also in their behavior. Furthermore, the models mostly provide valid explanations and deploy reasoning paths that are similar to those of humans.","sentences":["Cross-domain alignment refers to the task of mapping a concept from one domain to another.","For example, ``If a \\textit{doctor} were a \\textit{color}, what color would it be?''.","This seemingly peculiar task is designed to investigate how people represent concrete and abstract concepts through their mappings between categories and their reasoning processes over those mappings.","In this paper, we adapt this task from cognitive science to evaluate the conceptualization and reasoning abilities of large language models (LLMs) through a behavioral study.","We examine several LLMs by prompting them with a cross-domain mapping task and analyzing their responses at both the population and individual levels.","Additionally, we assess the models' ability to reason about their predictions by analyzing and categorizing their explanations for these mappings.","The results reveal several similarities between humans' and models' mappings and explanations, suggesting that models represent concepts similarly to humans.","This similarity is evident not only in the model representation but also in their behavior.","Furthermore, the models mostly provide valid explanations and deploy reasoning paths that are similar to those of humans."],"url":"http://arxiv.org/abs/2405.14863v1","category":"cs.CL"}
{"created":"2024-05-23 17:59:22","title":"Bitune: Bidirectional Instruction-Tuning","abstract":"We introduce Bitune, a method that improves instruction-tuning of pretrained decoder-only large language models, leading to consistent gains on downstream tasks. Bitune applies both causal and bidirectional attention to the prompt, to obtain a better representation of the query or instruction. We realize this by introducing two sets of parameters, for which we apply parameter-efficient finetuning techniques. These causal and bidirectional features are then combined into a weighted average with trainable coefficients, which is subsequently used to generate new tokens. We demonstrate significant improvements in zero-shot performance on commonsense reasoning, arithmetic, and language understanding tasks, while extensive ablation studies validate the role of each component and demonstrate the method's agnosticism to different PEFT techniques.","sentences":["We introduce Bitune, a method that improves instruction-tuning of pretrained decoder-only large language models, leading to consistent gains on downstream tasks.","Bitune applies both causal and bidirectional attention to the prompt, to obtain a better representation of the query or instruction.","We realize this by introducing two sets of parameters, for which we apply parameter-efficient finetuning techniques.","These causal and bidirectional features are then combined into a weighted average with trainable coefficients, which is subsequently used to generate new tokens.","We demonstrate significant improvements in zero-shot performance on commonsense reasoning, arithmetic, and language understanding tasks, while extensive ablation studies validate the role of each component and demonstrate the method's agnosticism to different PEFT techniques."],"url":"http://arxiv.org/abs/2405.14862v1","category":"cs.CL"}
{"created":"2024-05-23 17:59:10","title":"Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models","abstract":"This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.","sentences":["This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions.","Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper.","For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable.","We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps.","This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design.","All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner."],"url":"http://arxiv.org/abs/2405.14861v1","category":"cs.LG"}
{"created":"2024-05-23 17:58:03","title":"Semantica: An Adaptable Image-Conditioned Diffusion Model","abstract":"We investigate the task of adapting image generative models to different datasets without finetuneing. To this end, we introduce Semantica, an image-conditioned diffusion model capable of generating images based on the semantics of a conditioning image. Semantica is trained exclusively on web-scale image pairs, that is it receives a random image from a webpage as conditional input and models another random image from the same webpage. Our experiments highlight the expressivity of pretrained image encoders and necessity of semantic-based data filtering in achieving high-quality image generation. Once trained, it can adaptively generate new images from a dataset by simply using images from that dataset as input. We study the transfer properties of Semantica on ImageNet, LSUN Churches, LSUN Bedroom and SUN397.","sentences":["We investigate the task of adapting image generative models to different datasets without finetuneing.","To this end, we introduce Semantica, an image-conditioned diffusion model capable of generating images based on the semantics of a conditioning image.","Semantica is trained exclusively on web-scale image pairs, that is it receives a random image from a webpage as conditional input and models another random image from the same webpage.","Our experiments highlight the expressivity of pretrained image encoders and necessity of semantic-based data filtering in achieving high-quality image generation.","Once trained, it can adaptively generate new images from a dataset by simply using images from that dataset as input.","We study the transfer properties of Semantica on ImageNet, LSUN Churches, LSUN Bedroom and SUN397."],"url":"http://arxiv.org/abs/2405.14857v1","category":"cs.CV"}
{"created":"2024-05-23 17:57:50","title":"Synergistic Global-space Camera and Human Reconstruction from Videos","abstract":"Remarkable strides have been made in reconstructing static scenes or human bodies from monocular videos. Yet, the two problems have largely been approached independently, without much synergy. Most visual SLAM methods can only reconstruct camera trajectories and scene structures up to scale, while most HMR methods reconstruct human meshes in metric scale but fall short in reasoning with cameras and scenes. This work introduces Synergistic Camera and Human Reconstruction (SynCHMR) to marry the best of both worlds. Specifically, we design Human-aware Metric SLAM to reconstruct metric-scale camera poses and scene point clouds using camera-frame HMR as a strong prior, addressing depth, scale, and dynamic ambiguities. Conditioning on the dense scene recovered, we further learn a Scene-aware SMPL Denoiser to enhance world-frame HMR by incorporating spatio-temporal coherency and dynamic scene constraints. Together, they lead to consistent reconstructions of camera trajectories, human meshes, and dense scene point clouds in a common world frame. Project page: https://paulchhuang.github.io/synchmr","sentences":["Remarkable strides have been made in reconstructing static scenes or human bodies from monocular videos.","Yet, the two problems have largely been approached independently, without much synergy.","Most visual SLAM methods can only reconstruct camera trajectories and scene structures up to scale, while most HMR methods reconstruct human meshes in metric scale but fall short in reasoning with cameras and scenes.","This work introduces Synergistic Camera and Human Reconstruction (SynCHMR) to marry the best of both worlds.","Specifically, we design Human-aware Metric SLAM to reconstruct metric-scale camera poses and scene point clouds using camera-frame HMR as a strong prior, addressing depth, scale, and dynamic ambiguities.","Conditioning on the dense scene recovered, we further learn a Scene-aware SMPL Denoiser to enhance world-frame HMR by incorporating spatio-temporal coherency and dynamic scene constraints.","Together, they lead to consistent reconstructions of camera trajectories, human meshes, and dense scene point clouds in a common world frame.","Project page: https://paulchhuang.github.io/synchmr"],"url":"http://arxiv.org/abs/2405.14855v1","category":"cs.CV"}
{"created":"2024-05-23 17:57:24","title":"TerDiT: Ternary Diffusion Models with Transformers","abstract":"Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs). Among these diffusion models, diffusion transformers have demonstrated superior image generation capabilities, boosting lower FID scores and higher scalability. However, deploying large-scale DiT models can be expensive due to their extensive parameter numbers. Although existing research has explored efficient deployment techniques for diffusion models such as model quantization, there is still little work concerning DiT-based models. To tackle this research gap, in this paper, we propose TerDiT, a quantization-aware training (QAT) and efficient deployment scheme for ternary diffusion models with transformers. We focus on the ternarization of DiT networks and scale model sizes from 600M to 4.2B. Our work contributes to the exploration of efficient deployment strategies for large-scale DiT models, demonstrating the feasibility of training extremely low-bit diffusion transformer models from scratch while maintaining competitive image generation capacities compared to full-precision models. Code will be available at https://github.com/Lucky-Lance/TerDiT.","sentences":["Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs).","Among these diffusion models, diffusion transformers have demonstrated superior image generation capabilities, boosting lower FID scores and higher scalability.","However, deploying large-scale DiT models can be expensive due to their extensive parameter numbers.","Although existing research has explored efficient deployment techniques for diffusion models such as model quantization, there is still little work concerning DiT-based models.","To tackle this research gap, in this paper, we propose TerDiT, a quantization-aware training (QAT) and efficient deployment scheme for ternary diffusion models with transformers.","We focus on the ternarization of DiT networks and scale model sizes from 600M to 4.2B. Our work contributes to the exploration of efficient deployment strategies for large-scale DiT models, demonstrating the feasibility of training extremely low-bit diffusion transformer models from scratch while maintaining competitive image generation capacities compared to full-precision models.","Code will be available at https://github.com/Lucky-Lance/TerDiT."],"url":"http://arxiv.org/abs/2405.14854v1","category":"cs.CV"}
{"created":"2024-05-23 17:57:14","title":"Privileged Sensing Scaffolds Reinforcement Learning","abstract":"We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon \"sensory scaffolding\": observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose \"Scaffolder\", a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new \"S3\" suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/","sentences":["We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone.","We call this phenomenon \"sensory scaffolding\": observation streams that are not needed by a master might yet aid a novice learner.","We consider such sensory scaffolding setups for training artificial agents.","For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors.","For these settings, we propose \"Scaffolder\", a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy.","For evaluating sensory scaffolding agents, we design a new \"S3\" suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups.","Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more.","Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors.","Website: https://penn-pal-lab.github.io/scaffolder/"],"url":"http://arxiv.org/abs/2405.14853v1","category":"cs.LG"}
{"created":"2024-05-23 17:57:04","title":"PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression","abstract":"There has been significant interest in \"extreme\" compression of large language models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices. Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs. We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases. On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama 2 family models at 2 bits per parameter.","sentences":["There has been significant interest in \"extreme\" compression of large language models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices.","Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off.","State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting.","In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs.","We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases.","On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral.","Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama 2 family models at 2 bits per parameter."],"url":"http://arxiv.org/abs/2405.14852v1","category":"cs.LG"}
{"created":"2024-05-23 17:56:52","title":"Domain Wall Magnetic Tunnel Junction Reliable Integrate and Fire Neuron","abstract":"In spiking neural networks, neuron dynamics are described by the biologically realistic integrate-and-fire model that captures membrane potential accumulation and above-threshold firing behaviors. Among the hardware implementations of integrate-and-fire neuron devices, one important feature, reset, has been largely ignored. Here, we present the design and fabrication of a magnetic domain wall and magnetic tunnel junction based artificial integrate-and-fire neuron device that achieves reliable reset at the end of the integrate-fire cycle. We demonstrate the domain propagation in the domain wall racetrack (integration), reading using a magnetic tunnel junction (fire), and reset as the domain is ejected from the racetrack, showing the artificial neuron can be operated continuously over 100 integrate-fire-reset cycles. Both pulse amplitude and pulse number encoding is demonstrated. The device data is applied on an image classification task using a spiking neural network and shown to have comparable performance to an ideal leaky, integrate-and-fire neural network. These results achieve the first demonstration of reliable integrate-fire-reset in domain wall-magnetic tunnel junction-based neuron devices and shows the promise of spintronics for neuromorphic computing.","sentences":["In spiking neural networks, neuron dynamics are described by the biologically realistic integrate-and-fire model that captures membrane potential accumulation and above-threshold firing behaviors.","Among the hardware implementations of integrate-and-fire neuron devices, one important feature, reset, has been largely ignored.","Here, we present the design and fabrication of a magnetic domain wall and magnetic tunnel junction based artificial integrate-and-fire neuron device that achieves reliable reset at the end of the integrate-fire cycle.","We demonstrate the domain propagation in the domain wall racetrack (integration), reading using a magnetic tunnel junction (fire), and reset as the domain is ejected from the racetrack, showing the artificial neuron can be operated continuously over 100 integrate-fire-reset cycles.","Both pulse amplitude and pulse number encoding is demonstrated.","The device data is applied on an image classification task using a spiking neural network and shown to have comparable performance to an ideal leaky, integrate-and-fire neural network.","These results achieve the first demonstration of reliable integrate-fire-reset in domain wall-magnetic tunnel junction-based neuron devices and shows the promise of spintronics for neuromorphic computing."],"url":"http://arxiv.org/abs/2405.14851v1","category":"cs.NE"}
{"created":"2024-05-23 17:56:04","title":"Electric, thermal, and thermoelectric magnetoconductivity for Weyl/multi-Weyl semimetals in planar Hall set-ups induced by the combined effects of topology and strain","abstract":"We continue our investigation of the response tensors in planar Hall (or planar thermal Hall) configurations where a three-dimensional Weyl/multi-Weyl semimetal is subjected to the combined influence of an electric field $\\mathbf E $ (or temperature gradient $\\nabla_{\\mathbf r } T$) and an effective magnetic field $\\mathbf B_\\chi $, generalizing the considerations of Phys. Rev. B 108 (2023) 155132 and Physica E 159 (2024) 115914. The electromagnetic fields are oriented at a generic angle with respect to each other, thus leading to the possibility of having a collinear component, which does not arise in a Hall set-up. The net effective magnetic field $\\mathbf B_\\chi $ consists of two parts - (a) an actual/physical magnetic field $\\mathbf B $ applied externally; and (b) an emergent magnetic field $\\mathbf B_5 $ which quantifies the elastic deformations of the sample. $\\mathbf B_5 $ is an axial pseudomagnetic field because it couples to conjugate nodal points with opposite chiralities with opposite signs. Using a semiclassical Boltzmann formalism, we derive the generic expressions for the response tensors, including the effects of the Berry curvature (BC) and the orbital magnetic moment (OMM), which arise due to a nontrivial topology of the bandstructures. We elucidate the interplay of the BC-only and the OMM-dependent parts in the longitudinal and transverse (or Hall) components of the electric, thermal, and thermoelectric response tensors. Especially, for the planar transverse components of the response tensors, the OMM part acts exclusively in opposition (sync) with the BC-only part for the Weyl (multi-Weyl) semimetals.","sentences":["We continue our investigation of the response tensors in planar Hall (or planar thermal Hall) configurations where a three-dimensional Weyl/multi-Weyl semimetal is subjected to the combined influence of an electric field $\\mathbf E $ (or temperature gradient $\\nabla_{\\mathbf r } T$) and an effective magnetic field $\\mathbf B_\\chi $, generalizing the considerations of Phys.","Rev. B 108 (2023) 155132 and Physica E 159 (2024) 115914.","The electromagnetic fields are oriented at a generic angle with respect to each other, thus leading to the possibility of having a collinear component, which does not arise in a Hall set-up.","The net effective magnetic field $\\mathbf B_\\chi $ consists of two parts - (a) an actual/physical magnetic field $\\mathbf B $ applied externally; and (b) an emergent magnetic field $\\mathbf B_5 $ which quantifies the elastic deformations of the sample.","$\\mathbf B_5 $ is an axial pseudomagnetic field because it couples to conjugate nodal points with opposite chiralities with opposite signs.","Using a semiclassical Boltzmann formalism, we derive the generic expressions for the response tensors, including the effects of the Berry curvature (BC) and the orbital magnetic moment (OMM), which arise due to a nontrivial topology of the bandstructures.","We elucidate the interplay of the BC-only and the OMM-dependent parts in the longitudinal and transverse (or Hall) components of the electric, thermal, and thermoelectric response tensors.","Especially, for the planar transverse components of the response tensors, the OMM part acts exclusively in opposition (sync) with the BC-only part for the Weyl (multi-Weyl) semimetals."],"url":"http://arxiv.org/abs/2405.14844v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 17:56:01","title":"Accretion onto supermassive black hole binaries","abstract":"In this chapter, we give an overview of our current understanding of the physics of accreting massive black hole binaries (MBHBs), with a special focus on the latest developments in numerical simulations and General-Relativistic Magnetohydrodynamics (GRMHD) simulations in particular. We give a self-contained global picture of how to model accretion onto MBHBs, analyzing different aspects of the system such as the dynamics of the circumbinary disk, mini-disks, outflows, the role of magnetic fields, and electromagnetic signatures. We discuss important questions and open problems related to these systems, what are the advantages and disadvantages of the different numerical approaches, and what robust knowledge we have built from simulations.","sentences":["In this chapter, we give an overview of our current understanding of the physics of accreting massive black hole binaries (MBHBs), with a special focus on the latest developments in numerical simulations and General-Relativistic Magnetohydrodynamics (GRMHD) simulations in particular.","We give a self-contained global picture of how to model accretion onto MBHBs, analyzing different aspects of the system such as the dynamics of the circumbinary disk, mini-disks, outflows, the role of magnetic fields, and electromagnetic signatures.","We discuss important questions and open problems related to these systems, what are the advantages and disadvantages of the different numerical approaches, and what robust knowledge we have built from simulations."],"url":"http://arxiv.org/abs/2405.14843v1","category":"astro-ph.HE"}
{"created":"2024-05-23 17:55:02","title":"A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis","abstract":"While deep networks have achieved broad success in analyzing natural images, when applied to medical scans, they often fail in unexcepted situations. We investigate this challenge and focus on model sensitivity to domain shifts, such as data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc, in the context of chest X-rays and skin lesion images. A key finding we show empirically is that existing visual backbones lack an appropriate prior from the architecture for reliable generalization in these settings. Taking inspiration from medical training, we propose giving deep networks a prior grounded in explicit medical knowledge communicated in natural language. To this end, we introduce Knowledge-enhanced Bottlenecks (KnoBo), a class of concept bottleneck models that incorporates knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed. KnoBo uses retrieval-augmented language models to design an appropriate concept space paired with an automatic training procedure for recognizing the concept. We evaluate different resources of knowledge and recognition architectures on a broad range of domain shifts across 20 datasets. In our comprehensive evaluation with two imaging modalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4% on average. Finally, evaluations reveal that PubMed is a promising resource for making medical models less sensitive to domain shift, outperforming other resources on both diversity of information and final prediction performance.","sentences":["While deep networks have achieved broad success in analyzing natural images, when applied to medical scans, they often fail in unexcepted situations.","We investigate this challenge and focus on model sensitivity to domain shifts, such as data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc, in the context of chest X-rays and skin lesion images.","A key finding we show empirically is that existing visual backbones lack an appropriate prior from the architecture for reliable generalization in these settings.","Taking inspiration from medical training, we propose giving deep networks a prior grounded in explicit medical knowledge communicated in natural language.","To this end, we introduce Knowledge-enhanced Bottlenecks (KnoBo), a class of concept bottleneck models that incorporates knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed.","KnoBo uses retrieval-augmented language models to design an appropriate concept space paired with an automatic training procedure for recognizing the concept.","We evaluate different resources of knowledge and recognition architectures on a broad range of domain shifts across 20 datasets.","In our comprehensive evaluation with two imaging modalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4% on average.","Finally, evaluations reveal that PubMed is a promising resource for making medical models less sensitive to domain shift, outperforming other resources on both diversity of information and final prediction performance."],"url":"http://arxiv.org/abs/2405.14839v1","category":"cs.CV"}
{"created":"2024-05-23 17:54:14","title":"From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step","abstract":"When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.","sentences":["When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs.","In this paper, we investigate if models can be taught to internalize these CoT steps.","To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model.","This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance.","Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication.","Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps."],"url":"http://arxiv.org/abs/2405.14838v1","category":"cs.CL"}
{"created":"2024-05-23 17:51:05","title":"Analysis of Atom-level pretraining with QM data for Graph Neural Networks Molecular property models","abstract":"Despite the rapid and significant advancements in deep learning for Quantitative Structure-Activity Relationship (QSAR) models, the challenge of learning robust molecular representations that effectively generalize in real-world scenarios to novel compounds remains an elusive and unresolved task. This study examines how atom-level pretraining with quantum mechanics (QM) data can mitigate violations of assumptions regarding the distributional similarity between training and test data and therefore improve performance and generalization in downstream tasks. In the public dataset Therapeutics Data Commons (TDC), we show how pretraining on atom-level QM improves performance overall and makes the activation of the features distributes more Gaussian-like which results in a representation that is more robust to distribution shifts. To the best of our knowledge, this is the first time that hidden state molecular representations are analyzed to compare the effects of molecule-level and atom-level pretraining on QM data.","sentences":["Despite the rapid and significant advancements in deep learning for Quantitative Structure-Activity Relationship (QSAR) models, the challenge of learning robust molecular representations that effectively generalize in real-world scenarios to novel compounds remains an elusive and unresolved task.","This study examines how atom-level pretraining with quantum mechanics (QM) data can mitigate violations of assumptions regarding the distributional similarity between training and test data and therefore improve performance and generalization in downstream tasks.","In the public dataset Therapeutics Data Commons (TDC), we show how pretraining on atom-level QM improves performance overall and makes the activation of the features distributes more Gaussian-like which results in a representation that is more robust to distribution shifts.","To the best of our knowledge, this is the first time that hidden state molecular representations are analyzed to compare the effects of molecule-level and atom-level pretraining on QM data."],"url":"http://arxiv.org/abs/2405.14837v1","category":"cs.LG"}
{"created":"2024-05-23 17:50:34","title":"Polynomial Pass Semi-Streaming Lower Bounds for K-Cores and Degeneracy","abstract":"The following question arises naturally in the study of graph streaming algorithms:   \"Is there any graph problem which is \"not too hard\", in that it can be solved efficiently with total communication (nearly) linear in the number $n$ of vertices, and for which, nonetheless, any streaming algorithm with $\\tilde{O}(n)$ space (i.e., a semi-streaming algorithm) needs a polynomial $n^{\\Omega(1)}$ number of passes?\"   Assadi, Chen, and Khanna [STOC 2019] were the first to prove that this is indeed the case. However, the lower bounds that they obtained are for rather non-standard graph problems.   Our first main contribution is to present the first polynomial-pass lower bounds for natural \"not too hard\" graph problems studied previously in the streaming model: $k$-cores and degeneracy. We devise a novel communication protocol for both problems with near-linear communication, thus showing that $k$-cores and degeneracy are natural examples of \"not too hard\" problems. Indeed, previous work have developed single-pass semi-streaming algorithms for approximating these problems. In contrast, we prove that any semi-streaming algorithm for exactly solving these problems requires (almost) $\\Omega(n^{1/3})$ passes.   Our second main contribution is improved round-communication lower bounds for the underlying communication problems at the basis of these reductions:   * We improve the previous lower bound of Assadi, Chen, and Khanna for hidden pointer chasing (HPC) to achieve optimal bounds.   * We observe that all current reductions from HPC can also work with a generalized version of this problem that we call MultiHPC, and prove an even stronger and optimal lower bound for this generalization.   These two results collectively allow us to improve the resulting pass lower bounds for semi-streaming algorithms by a polynomial factor, namely, from $n^{1/5}$ to $n^{1/3}$ passes.","sentences":["The following question arises naturally in the study of graph streaming algorithms:   \"Is there any graph problem which is \"not too hard\", in that it can be solved efficiently with total communication (nearly) linear in the number $n$ of vertices, and for which, nonetheless, any streaming algorithm with $\\tilde{O}(n)$ space (i.e., a semi-streaming algorithm) needs a polynomial $n^{\\Omega(1)}$ number of passes?\"   Assadi, Chen, and","Khanna [STOC 2019] were the first to prove that this is indeed the case.","However, the lower bounds that they obtained are for rather non-standard graph problems.   ","Our first main contribution is to present the first polynomial-pass lower bounds for natural \"not too hard\" graph problems studied previously in the streaming model: $k$-cores and degeneracy.","We devise a novel communication protocol for both problems with near-linear communication, thus showing that $k$-cores and degeneracy are natural examples of \"not too hard\" problems.","Indeed, previous work have developed single-pass semi-streaming algorithms for approximating these problems.","In contrast, we prove that any semi-streaming algorithm for exactly solving these problems requires (almost) $\\Omega(n^{1/3})$ passes.   ","Our second main contribution is improved round-communication lower bounds for the underlying communication problems at the basis of these reductions:   ","*","We improve the previous lower bound of Assadi, Chen, and Khanna for hidden pointer chasing (HPC) to achieve optimal bounds.   ","* We observe that all current reductions from HPC can also work with a generalized version of this problem that we call MultiHPC, and prove an even stronger and optimal lower bound for this generalization.   ","These two results collectively allow us to improve the resulting pass lower bounds for semi-streaming algorithms by a polynomial factor, namely, from $n^{1/5}$ to $n^{1/3}$ passes."],"url":"http://arxiv.org/abs/2405.14835v1","category":"cs.DS"}
{"created":"2024-05-23 17:50:20","title":"A central limit theorem for coefficients of L-functions in short intervals","abstract":"Assuming the generalized Lindel\\\"{o}f hypothesis (GLH), a weak generalized Ramanujan conjecture and a Rankin--Selberg type partial sum estimate, we show that the sum of coefficients of a general $L$-function in short intervals of appropriate length has a Gaussian limiting distribution. The novelty lies in the degree aspect under GLH. In particular, this generalizes Hughes--Rudnick's result on lattice point counts in thin annuli.","sentences":["Assuming the generalized Lindel\\\"{o}f hypothesis (GLH), a weak generalized Ramanujan conjecture and a Rankin--Selberg type partial sum estimate, we show that the sum of coefficients of a general $L$-function in short intervals of appropriate length has a Gaussian limiting distribution.","The novelty lies in the degree aspect under GLH.","In particular, this generalizes Hughes--Rudnick's result on lattice point counts in thin annuli."],"url":"http://arxiv.org/abs/2405.14834v1","category":"math.NT"}
{"created":"2024-05-23 17:49:37","title":"Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer","abstract":"Generating high-quality 3D assets from text and images has long been challenging, primarily due to the absence of scalable 3D representations capable of capturing intricate geometry distributions. In this work, we introduce Direct3D, a native 3D generative model scalable to in-the-wild input images, without requiring a multiview diffusion model or SDS optimization. Our approach comprises two primary components: a Direct 3D Variational Auto-Encoder (D3D-VAE) and a Direct 3D Diffusion Transformer (D3D-DiT). D3D-VAE efficiently encodes high-resolution 3D shapes into a compact and continuous latent triplane space. Notably, our method directly supervises the decoded geometry using a semi-continuous surface sampling strategy, diverging from previous methods relying on rendered images as supervision signals. D3D-DiT models the distribution of encoded 3D latents and is specifically designed to fuse positional information from the three feature maps of the triplane latent, enabling a native 3D generative model scalable to large-scale 3D datasets. Additionally, we introduce an innovative image-to-3D generation pipeline incorporating semantic and pixel-level image conditions, allowing the model to produce 3D shapes consistent with the provided conditional image input. Extensive experiments demonstrate the superiority of our large-scale pre-trained Direct3D over previous image-to-3D approaches, achieving significantly better generation quality and generalization ability, thus establishing a new state-of-the-art for 3D content creation. Project page: https://nju-3dv.github.io/projects/Direct3D/.","sentences":["Generating high-quality 3D assets from text and images has long been challenging, primarily due to the absence of scalable 3D representations capable of capturing intricate geometry distributions.","In this work, we introduce Direct3D, a native 3D generative model scalable to in-the-wild input images, without requiring a multiview diffusion model or SDS optimization.","Our approach comprises two primary components: a Direct 3D Variational Auto-Encoder (D3D-VAE) and a Direct 3D Diffusion Transformer (D3D-DiT).","D3D-VAE efficiently encodes high-resolution 3D shapes into a compact and continuous latent triplane space.","Notably, our method directly supervises the decoded geometry using a semi-continuous surface sampling strategy, diverging from previous methods relying on rendered images as supervision signals.","D3D-DiT models the distribution of encoded 3D latents and is specifically designed to fuse positional information from the three feature maps of the triplane latent, enabling a native 3D generative model scalable to large-scale 3D datasets.","Additionally, we introduce an innovative image-to-3D generation pipeline incorporating semantic and pixel-level image conditions, allowing the model to produce 3D shapes consistent with the provided conditional image input.","Extensive experiments demonstrate the superiority of our large-scale pre-trained Direct3D over previous image-to-3D approaches, achieving significantly better generation quality and generalization ability, thus establishing a new state-of-the-art for 3D content creation.","Project page: https://nju-3dv.github.io/projects/Direct3D/."],"url":"http://arxiv.org/abs/2405.14832v1","category":"cs.CV"}
{"created":"2024-05-23 17:47:55","title":"HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models","abstract":"In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.","sentences":["In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting.","Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training.","In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences.","HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory.","We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%.","Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains.","Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods.","Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG."],"url":"http://arxiv.org/abs/2405.14831v1","category":"cs.CL"}
{"created":"2024-05-23 17:46:23","title":"Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models","abstract":"Recent advances in text-to-image (T2I) diffusion models have facilitated creative and photorealistic image synthesis. By varying the random seeds, we can generate various images for a fixed text prompt. Technically, the seed controls the initial noise and, in multi-step diffusion inference, the noise used for reparameterization at intermediate timesteps in the reverse diffusion process. However, the specific impact of the random seed on the generated images remains relatively unexplored. In this work, we conduct a large-scale scientific study into the impact of random seeds during diffusion inference. Remarkably, we reveal that the best 'golden' seed achieved an impressive FID of 21.60, compared to the worst 'inferior' seed's FID of 31.97. Additionally, a classifier can predict the seed number used to generate an image with over 99.9% accuracy in just a few epochs, establishing that seeds are highly distinguishable based on generated images. Encouraged by these findings, we examined the influence of seeds on interpretable visual dimensions. We find that certain seeds consistently produce grayscale images, prominent sky regions, or image borders. Seeds also affect image composition, including object location, size, and depth. Moreover, by leveraging these 'golden' seeds, we demonstrate improved image generation such as high-fidelity inference and diversified sampling. Our investigation extends to inpainting tasks, where we uncover some seeds that tend to insert unwanted text artifacts. Overall, our extensive analyses highlight the importance of selecting good seeds and offer practical utility for image generation.","sentences":["Recent advances in text-to-image (T2I) diffusion models have facilitated creative and photorealistic image synthesis.","By varying the random seeds, we can generate various images for a fixed text prompt.","Technically, the seed controls the initial noise and, in multi-step diffusion inference, the noise used for reparameterization at intermediate timesteps in the reverse diffusion process.","However, the specific impact of the random seed on the generated images remains relatively unexplored.","In this work, we conduct a large-scale scientific study into the impact of random seeds during diffusion inference.","Remarkably, we reveal that the best 'golden' seed achieved an impressive FID of 21.60, compared to the worst 'inferior' seed's FID of 31.97.","Additionally, a classifier can predict the seed number used to generate an image with over 99.9% accuracy in just a few epochs, establishing that seeds are highly distinguishable based on generated images.","Encouraged by these findings, we examined the influence of seeds on interpretable visual dimensions.","We find that certain seeds consistently produce grayscale images, prominent sky regions, or image borders.","Seeds also affect image composition, including object location, size, and depth.","Moreover, by leveraging these 'golden' seeds, we demonstrate improved image generation such as high-fidelity inference and diversified sampling.","Our investigation extends to inpainting tasks, where we uncover some seeds that tend to insert unwanted text artifacts.","Overall, our extensive analyses highlight the importance of selecting good seeds and offer practical utility for image generation."],"url":"http://arxiv.org/abs/2405.14828v1","category":"cs.CV"}
{"created":"2024-05-23 17:39:09","title":"PaGoDA: Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher","abstract":"To accelerate sampling, diffusion models (DMs) are often distilled into generators that directly map noise to data in a single step. In this approach, the resolution of the generator is fundamentally limited by that of the teacher DM. To overcome this limitation, we propose Progressive Growing of Diffusion Autoencoder (PaGoDA), a technique to progressively grow the resolution of the generator beyond that of the original teacher DM. Our key insight is that a pre-trained, low-resolution DM can be used to deterministically encode high-resolution data to a structured latent space by solving the PF-ODE forward in time (data-to-noise), starting from an appropriately down-sampled image. Using this frozen encoder in an auto-encoder framework, we train a decoder by progressively growing its resolution. From the nature of progressively growing decoder, PaGoDA avoids re-training teacher/student models when we upsample the student model, making the whole training pipeline much cheaper. In experiments, we used our progressively growing decoder to upsample from the pre-trained model's 64x64 resolution to generate 512x512 samples, achieving 2x faster inference compared to single-step distilled Stable Diffusion like LCM. PaGoDA also achieved state-of-the-art FIDs on ImageNet across all resolutions from 64x64 to 512x512. Additionally, we demonstrated PaGoDA's effectiveness in solving inverse problems and enabling controllable generation.","sentences":["To accelerate sampling, diffusion models (DMs) are often distilled into generators that directly map noise to data in a single step.","In this approach, the resolution of the generator is fundamentally limited by that of the teacher DM.","To overcome this limitation, we propose Progressive Growing of Diffusion Autoencoder (PaGoDA), a technique to progressively grow the resolution of the generator beyond that of the original teacher DM.","Our key insight is that a pre-trained, low-resolution DM can be used to deterministically encode high-resolution data to a structured latent space by solving the PF-ODE forward in time (data-to-noise), starting from an appropriately down-sampled image.","Using this frozen encoder in an auto-encoder framework, we train a decoder by progressively growing its resolution.","From the nature of progressively growing decoder, PaGoDA avoids re-training teacher/student models when we upsample the student model, making the whole training pipeline much cheaper.","In experiments, we used our progressively growing decoder to upsample from the pre-trained model's 64x64 resolution to generate 512x512 samples, achieving 2x faster inference compared to single-step distilled Stable Diffusion like LCM.","PaGoDA also achieved state-of-the-art FIDs on ImageNet across all resolutions from 64x64 to 512x512.","Additionally, we demonstrated PaGoDA's effectiveness in solving inverse problems and enabling controllable generation."],"url":"http://arxiv.org/abs/2405.14822v1","category":"cs.CV"}
{"created":"2024-05-23 17:36:40","title":"Pathwise uniqueness in infinite dimension under weak structure conditions","abstract":"Let $U,H$ be two separable Hilbert spaces and $T>0$. We consider an SDE which evolves in the Hilbert space $H$ of the form \\begin{align} dX(t)=AX(t)dt+\\widetilde{\\mathscr L}B(X(t))dt+GdW(t), \\quad t\\in[0,T], \\quad X(0)=x \\in H, \\end{align} where $A:D(A)\\subseteq H\\to H$ is the infinitesimal generator of a strongly continuous semigroup $(e^{tA})_{t\\geq0}$, $W=(W(t))_{t\\geq0}$ is a $U$-cylindrical Wiener process defined on a normal filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\in [0,T]},\\mathbb{P})$, $B:H\\to H$ is a bounded and $\\theta$-H\\\"older continuous function, for some suitable $\\theta\\in(0,1)$, and $\\widetilde{\\mathscr L}:H\\to H$ and $G:U\\to H$ are linear bounded operators. We prove that, under suitable assumptions on the coefficients, the weak mild solution to the equation depends on the initial datum in a Lipschitz way. This implies that pathwise uniqueness holds true. Here, the presence of the operator $\\Lambda$ plays a crucial role. In particular the conditions assumed on the coefficients cover the stochastic damped wave equation in dimension $1$ and the stochastic damped Euler--Bernoulli Beam equation upto dimension $3$ even in the hyperbolic case.","sentences":["Let $U,H$ be two separable Hilbert spaces and $T>0$. We consider an SDE which evolves in the Hilbert space $H$ of the form \\begin{align} dX(t)=AX(t)dt+\\widetilde{\\mathscr L}B(X(t))dt+GdW(t), \\quad t\\in[0,T], \\quad X(0)=x \\in H, \\end{align} where $A:D(A)\\subseteq H\\to H$ is the infinitesimal generator of a strongly continuous semigroup $(e^{tA})_{t\\geq0}$, $W=(W(t))_{t\\geq0}$ is a $U$-cylindrical Wiener process defined on a normal filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\in [0,T]},\\mathbb{P})$, $B:H\\to H$ is a bounded and $\\theta$-H\\\"older continuous function, for some suitable $\\theta\\in(0,1)$, and $\\widetilde{\\mathscr L}:H\\to H$ and $G:U\\to H$ are linear bounded operators.","We prove that, under suitable assumptions on the coefficients, the weak mild solution to the equation depends on the initial datum in a Lipschitz way.","This implies that pathwise uniqueness holds true.","Here, the presence of the operator $\\Lambda$ plays a crucial role.","In particular the conditions assumed on the coefficients cover the stochastic damped wave equation in dimension $1$ and the stochastic damped Euler--Bernoulli Beam equation upto dimension $3$ even in the hyperbolic case."],"url":"http://arxiv.org/abs/2405.14819v1","category":"math.PR"}
{"created":"2024-05-23 17:30:43","title":"Gravitational-wave dark siren cosmology systematics from galaxy weighting","abstract":"The discovery of GW170817 provided the first direct gravitational-wave measurement of the Hubble constant, $H_0$, demonstrating the potential power of standard-siren cosmology. The dark siren approach can be utilized for gravitational-wave sources in the absence of an electromagnetic counterpart: one considers all galaxies contained within the localization volume as potential hosts. When statistically averaging over the potential host galaxies, one can weight them by their luminosities to account for physically-motivated prescriptions (e.g., tracing star formation or stellar mass). Using mock galaxy catalogs, we explore the impact of these weightings on the measurement of $H_0$, focusing on the bias in $H_0$ inference that results from incorrectly-weighted prescriptions. We find that assuming an incorrect galaxy host probability can lead to significant biases in $H_0$, up to about five times off from typical values inferred by current experiments. These biases are due to inconsistent galaxy weighted redshift distributions as well as preferentially selecting the incorrect host during inference. The magnitudes of these biases are influenced by the galaxy number density along each line of sight, the uncertainty in the measurement of the gravitational-wave luminosity distance, and correlations in the parameter space of galaxies. These biases may be overcome with future GW detectors that contain better GW localization, using a strategic choice of weighting prescription, or with increasing the SNR cut. We propose using hierarchical inference as a diagnosis of incorrectly-weighted prescriptions, which can further be used to simultaneously infer the correct weighting scheme and cosmological parameters.","sentences":["The discovery of GW170817 provided the first direct gravitational-wave measurement of the Hubble constant, $H_0$, demonstrating the potential power of standard-siren cosmology.","The dark siren approach can be utilized for gravitational-wave sources in the absence of an electromagnetic counterpart: one considers all galaxies contained within the localization volume as potential hosts.","When statistically averaging over the potential host galaxies, one can weight them by their luminosities to account for physically-motivated prescriptions (e.g., tracing star formation or stellar mass).","Using mock galaxy catalogs, we explore the impact of these weightings on the measurement of $H_0$, focusing on the bias in $H_0$ inference that results from incorrectly-weighted prescriptions.","We find that assuming an incorrect galaxy host probability can lead to significant biases in $H_0$, up to about five times off from typical values inferred by current experiments.","These biases are due to inconsistent galaxy weighted redshift distributions as well as preferentially selecting the incorrect host during inference.","The magnitudes of these biases are influenced by the galaxy number density along each line of sight, the uncertainty in the measurement of the gravitational-wave luminosity distance, and correlations in the parameter space of galaxies.","These biases may be overcome with future GW detectors that contain better GW localization, using a strategic choice of weighting prescription, or with increasing the SNR cut.","We propose using hierarchical inference as a diagnosis of incorrectly-weighted prescriptions, which can further be used to simultaneously infer the correct weighting scheme and cosmological parameters."],"url":"http://arxiv.org/abs/2405.14818v1","category":"astro-ph.CO"}
{"created":"2024-05-23 17:27:31","title":"Accelerated First-Principles Exploration of Structure and Reactivity in Graphene Oxide","abstract":"Graphene oxide (GO) materials are widely studied, and yet their atomic-scale structures remain to be fully understood. Here we show that the chemical and configurational space of GO can be rapidly explored by advanced machine-learning methods, combining on-the-fly acceleration for first-principles molecular dynamics with message-passing neural-network potentials. The first step allows for the rapid sampling of chemical structures with very little prior knowledge required; the second step affords state-of-the-art accuracy and predictive power. We apply the method to the thermal reduction of GO, which we describe in a realistic (ten-nanometre scale) structural model. Our simulations are consistent with recent experimental findings and help to rationalise them in atomistic and mechanistic detail. More generally, our work provides a platform for routine, accurate, and predictive simulations of diverse carbonaceous materials.","sentences":["Graphene oxide (GO) materials are widely studied, and yet their atomic-scale structures remain to be fully understood.","Here we show that the chemical and configurational space of GO can be rapidly explored by advanced machine-learning methods, combining on-the-fly acceleration for first-principles molecular dynamics with message-passing neural-network potentials.","The first step allows for the rapid sampling of chemical structures with very little prior knowledge required; the second step affords state-of-the-art accuracy and predictive power.","We apply the method to the thermal reduction of GO, which we describe in a realistic (ten-nanometre scale) structural model.","Our simulations are consistent with recent experimental findings and help to rationalise them in atomistic and mechanistic detail.","More generally, our work provides a platform for routine, accurate, and predictive simulations of diverse carbonaceous materials."],"url":"http://arxiv.org/abs/2405.14814v1","category":"physics.chem-ph"}
{"created":"2024-05-23 17:23:30","title":"Scalable Optimization in the Modular Norm","abstract":"To improve performance in contemporary deep learning, one is interested in scaling up the neural network in terms of both the number and the size of the layers. When ramping up the width of a single layer, graceful scaling of training has been linked to the need to normalize the weights and their updates in the \"natural norm\" particular to that layer. In this paper, we significantly generalize this idea by defining the modular norm, which is the natural norm on the full weight space of any neural network architecture. The modular norm is defined recursively in tandem with the network architecture itself. We show that the modular norm has several promising applications. On the practical side, the modular norm can be used to normalize the updates of any base optimizer so that the learning rate becomes transferable across width and depth. This means that the user does not need to compute optimizer-specific scale factors in order to scale training. On the theoretical side, we show that for any neural network built from \"well-behaved\" atomic modules, the gradient of the network is Lipschitz-continuous in the modular norm, with the Lipschitz constant admitting a simple recursive formula. This characterization opens the door to porting standard ideas in optimization theory over to deep learning. We have created a Python package called Modula that automatically normalizes weight updates in the modular norm of the architecture. The package is available via \"pip install modula\" with source code at https://github.com/jxbz/modula.","sentences":["To improve performance in contemporary deep learning, one is interested in scaling up the neural network in terms of both the number and the size of the layers.","When ramping up the width of a single layer, graceful scaling of training has been linked to the need to normalize the weights and their updates in the \"natural norm\" particular to that layer.","In this paper, we significantly generalize this idea by defining the modular norm, which is the natural norm on the full weight space of any neural network architecture.","The modular norm is defined recursively in tandem with the network architecture itself.","We show that the modular norm has several promising applications.","On the practical side, the modular norm can be used to normalize the updates of any base optimizer so that the learning rate becomes transferable across width and depth.","This means that the user does not need to compute optimizer-specific scale factors in order to scale training.","On the theoretical side, we show that for any neural network built from \"well-behaved\" atomic modules, the gradient of the network is Lipschitz-continuous in the modular norm, with the Lipschitz constant admitting a simple recursive formula.","This characterization opens the door to porting standard ideas in optimization theory over to deep learning.","We have created a Python package called Modula that automatically normalizes weight updates in the modular norm of the architecture.","The package is available via \"pip install modula\" with source code at https://github.com/jxbz/modula."],"url":"http://arxiv.org/abs/2405.14813v1","category":"cs.LG"}
{"created":"2024-05-23 17:19:21","title":"Prospective and retrospective coding in cortical neurons","abstract":"Brains can process sensory information from different modalities at astonishing speed, this is surprising as already the integration of inputs through the membrane causes a delayed response. Neuronal recordings in vitro reveal a possible explanation for the fast processing through an advancement of the output firing rates of individual neurons with respect to the input, a concept which we refer to as prospective coding. The underlying mechanisms of prospective coding, however, is not completely understood. We propose a mechanistic explanation for individual neurons advancing their output on the level of single action potentials and instantaneous firing rates. Using the Hodgkin-Huxley model, we show that the spike generation mechanism can be the source for prospective (advanced) or retrospective (delayed) responses with respect the underlying somatic voltage. A simplified Hodgkin-Huxley model identifies the sodium inactivation as a source for the prospective firing, controlling the timing of the neuron's output as a function the voltage and its derivative. We also consider a slower spike-frequency adaptation as a mechanisms that generates prospective firings to inputs that undergo slow temporal modulations. In general, we show that adaptation processes at different time scales can cause advanced neuronal responses to time varying inputs that are modulated on the corresponding time scales.","sentences":["Brains can process sensory information from different modalities at astonishing speed, this is surprising as already the integration of inputs through the membrane causes a delayed response.","Neuronal recordings in vitro reveal a possible explanation for the fast processing through an advancement of the output firing rates of individual neurons with respect to the input, a concept which we refer to as prospective coding.","The underlying mechanisms of prospective coding, however, is not completely understood.","We propose a mechanistic explanation for individual neurons advancing their output on the level of single action potentials and instantaneous firing rates.","Using the Hodgkin-Huxley model, we show that the spike generation mechanism can be the source for prospective (advanced) or retrospective (delayed) responses with respect the underlying somatic voltage.","A simplified Hodgkin-Huxley model identifies the sodium inactivation as a source for the prospective firing, controlling the timing of the neuron's output as a function the voltage and its derivative.","We also consider a slower spike-frequency adaptation as a mechanisms that generates prospective firings to inputs that undergo slow temporal modulations.","In general, we show that adaptation processes at different time scales can cause advanced neuronal responses to time varying inputs that are modulated on the corresponding time scales."],"url":"http://arxiv.org/abs/2405.14810v1","category":"q-bio.NC"}
{"created":"2024-05-23 17:18:46","title":"Implicit Personalization in Language Models: A Systematic Study","abstract":"Implicit Personalization (IP) is a phenomenon of language models inferring a user's background from the implicit cues in the input prompts and tailoring the response based on this inference. While previous work has touched upon various instances of this problem, there lacks a unified framework to study this behavior. This work systematically studies IP through a rigorous mathematical formulation, a multi-perspective moral reasoning framework, and a set of case studies. Our theoretical foundation for IP relies on a structural causal model and introduces a novel method, indirect intervention, to estimate the causal effect of a mediator variable that cannot be directly intervened upon. Beyond the technical approach, we also introduce a set of moral reasoning principles based on three schools of moral philosophy to study when IP may or may not be ethically appropriate. Equipped with both mathematical and ethical insights, we present three diverse case studies illustrating the varied nature of the IP problem and offer recommendations for future research. Our code and data are at https://github.com/jiarui-liu/IP.","sentences":["Implicit Personalization (IP) is a phenomenon of language models inferring a user's background from the implicit cues in the input prompts and tailoring the response based on this inference.","While previous work has touched upon various instances of this problem, there lacks a unified framework to study this behavior.","This work systematically studies IP through a rigorous mathematical formulation, a multi-perspective moral reasoning framework, and a set of case studies.","Our theoretical foundation for IP relies on a structural causal model and introduces a novel method, indirect intervention, to estimate the causal effect of a mediator variable that cannot be directly intervened upon.","Beyond the technical approach, we also introduce a set of moral reasoning principles based on three schools of moral philosophy to study when IP may or may not be ethically appropriate.","Equipped with both mathematical and ethical insights, we present three diverse case studies illustrating the varied nature of the IP problem and offer recommendations for future research.","Our code and data are at https://github.com/jiarui-liu/IP."],"url":"http://arxiv.org/abs/2405.14808v1","category":"cs.CL"}
{"created":"2024-05-23 17:15:41","title":"Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics","abstract":"Extracting scientific understanding from particle-physics experiments requires solving diverse learning problems with high precision and good data efficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a new multi-purpose architecture for high-energy physics. L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics. At the same time, the architecture is a Transformer, which makes it versatile and scalable to large systems. L-GATr is first demonstrated on regression and classification tasks from particle physics. We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching. Across our experiments, L-GATr is on par with or outperforms strong domain-specific baselines.","sentences":["Extracting scientific understanding from particle-physics experiments requires solving diverse learning problems with high precision and good data efficiency.","We propose the Lorentz Geometric Algebra Transformer (L-GATr), a new multi-purpose architecture for high-energy physics.","L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics.","At the same time, the architecture is a Transformer, which makes it versatile and scalable to large systems.","L-GATr is first demonstrated on regression and classification tasks from particle physics.","We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching.","Across our experiments, L-GATr is on par with or outperforms strong domain-specific baselines."],"url":"http://arxiv.org/abs/2405.14806v1","category":"physics.data-an"}
{"created":"2024-05-23 17:15:32","title":"A Seifert algorithm for integral homology spheres","abstract":"From classical knot theory we know that every knot in $S^3$ is the boundary of an oriented, embedded surface. A standard demonstration of this fact achieved by elementary technique comes from taking a regular projection of any knot and employing Seifert's constructive algorithm. In this note we give a natural generalization of Seifert's algorithm to any closed integral homology 3-sphere. The starting point of our algorithm is presenting the handle structure of a Heegaard splitting of a given integral homology sphere as a planar diagram on the boundary of a $3$-ball. (For a well known example of such a planar presentation, see the Poincar\\'e homology sphere planar presentation in {\\em Knots and Links} by D. Rolfsen \\cite{Rolfsen}.) An oriented link can then be represented by the regular projection of an oriented $k$-strand tangle. From there we give a natural way to find a ``Seifert circle\" and associated half-twisted bands.","sentences":["From classical knot theory we know that every knot in $S^3$ is the boundary of an oriented, embedded surface.","A standard demonstration of this fact achieved by elementary technique comes from taking a regular projection of any knot and employing Seifert's constructive algorithm.","In this note we give a natural generalization of Seifert's algorithm to any closed integral homology 3-sphere.","The starting point of our algorithm is presenting the handle structure of a Heegaard splitting of a given integral homology sphere as a planar diagram on the boundary of a $3$-ball.","(For a well known example of such a planar presentation, see the Poincar\\'e homology sphere planar presentation in {\\em Knots and Links} by D. Rolfsen \\cite{Rolfsen}.)","An oriented link can then be represented by the regular projection of an oriented $k$-strand tangle.","From there we give a natural way to find a ``Seifert circle\" and associated half-twisted bands."],"url":"http://arxiv.org/abs/2405.14805v1","category":"math.GT"}
{"created":"2024-05-23 17:13:50","title":"Can LLMs Solve longer Math Word Problems Better?","abstract":"Math Word Problems (MWPs) are crucial for evaluating the capability of Large Language Models (LLMs), with current research primarily focusing on questions with concise contexts. However, as real-world math problems often involve complex circumstances, LLMs' ability to solve long MWPs is vital for their applications in these scenarios, yet remains under-explored. This study pioneers the exploration of Context Length Generalizability (CoLeG), the ability of LLMs to solve long MWPs. We introduce Extended Grade-School Math (E-GSM), a collection of MWPs with lengthy narratives. Two novel metrics are proposed to assess the efficacy and resilience of LLMs in solving these problems. Our examination of existing zero-shot prompting techniques and both proprietary and open-source LLMs reveals a general deficiency in CoLeG. To alleviate these challenges, we propose distinct approaches for different categories of LLMs. For proprietary LLMs, a new instructional prompt is proposed to mitigate the influence of long context. For open-source LLMs, a new data augmentation task is developed to improve CoLeG. Our comprehensive results demonstrate the effectiveness of our proposed methods, showing not only improved performance on E-GSM but also generalizability across several other MWP benchmarks. Our findings pave the way for future research in employing LLMs for complex, real-world applications, offering practical solutions to current limitations and opening avenues for further exploration of model generalizability and training methodologies.","sentences":["Math Word Problems (MWPs) are crucial for evaluating the capability of Large Language Models (LLMs), with current research primarily focusing on questions with concise contexts.","However, as real-world math problems often involve complex circumstances, LLMs' ability to solve long MWPs is vital for their applications in these scenarios, yet remains under-explored.","This study pioneers the exploration of Context Length Generalizability (CoLeG), the ability of LLMs to solve long MWPs.","We introduce Extended Grade-School Math (E-GSM), a collection of MWPs with lengthy narratives.","Two novel metrics are proposed to assess the efficacy and resilience of LLMs in solving these problems.","Our examination of existing zero-shot prompting techniques and both proprietary and open-source LLMs reveals a general deficiency in CoLeG. To alleviate these challenges, we propose distinct approaches for different categories of LLMs.","For proprietary LLMs, a new instructional prompt is proposed to mitigate the influence of long context.","For open-source LLMs, a new data augmentation task is developed to improve CoLeG. Our comprehensive results demonstrate the effectiveness of our proposed methods, showing not only improved performance on E-GSM but also generalizability across several other MWP benchmarks.","Our findings pave the way for future research in employing LLMs for complex, real-world applications, offering practical solutions to current limitations and opening avenues for further exploration of model generalizability and training methodologies."],"url":"http://arxiv.org/abs/2405.14804v1","category":"cs.CL"}
{"created":"2024-05-23 17:13:45","title":"On the Value of the Cosmological Constant in Entropic Gravity","abstract":"We explicitly calculate the value of the cosmological constant, based on the recently developed theory connecting entropic gravity with quantum-events, induced by transactions, called transactional gravity. We suggest a novel interpretation of the cosmological constant and rigorously show its inverse proportionality to the squared radius of the causal universe.","sentences":["We explicitly calculate the value of the cosmological constant, based on the recently developed theory connecting entropic gravity with quantum-events, induced by transactions, called transactional gravity.","We suggest a novel interpretation of the cosmological constant and rigorously show its inverse proportionality to the squared radius of the causal universe."],"url":"http://arxiv.org/abs/2405.14803v1","category":"gr-qc"}
{"created":"2024-05-23 17:12:22","title":"Fast Denoising Diffusion Probabilistic Models for Medical Image-to-Image Generation","abstract":"Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented success in computer vision. However, they remain underutilized in medical imaging, a field crucial for disease diagnosis and treatment planning. This is primarily due to the high computational cost associated with (1) the use of large number of time steps (e.g., 1,000) in diffusion processes and (2) the increased dimensionality of medical images, which are often 3D or 4D. Training a diffusion model on medical images typically takes days to weeks, while sampling each image volume takes minutes to hours. To address this challenge, we introduce Fast-DDPM, a simple yet effective approach capable of improving training speed, sampling speed, and generation quality simultaneously. Unlike DDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains and samples using only 10 time steps. The key to our method lies in aligning the training and sampling procedures. We introduced two efficient noise schedulers with 10 time steps: one with uniform time step sampling and another with non-uniform sampling. We evaluated Fast-DDPM across three medical image-to-image generation tasks: multi-image super-resolution, image denoising, and image-to-image translation. Fast-DDPM outperformed DDPM and current state-of-the-art methods based on convolutional networks and generative adversarial networks in all tasks. Additionally, Fast-DDPM reduced training time by a factor of 5 and sampling time by a factor of 100 compared to DDPM. Our code is publicly available at: https://github.com/mirthAI/Fast-DDPM.","sentences":["Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented success in computer vision.","However, they remain underutilized in medical imaging, a field crucial for disease diagnosis and treatment planning.","This is primarily due to the high computational cost associated with (1) the use of large number of time steps (e.g., 1,000) in diffusion processes and (2) the increased dimensionality of medical images, which are often 3D or 4D. Training a diffusion model on medical images typically takes days to weeks, while sampling each image volume takes minutes to hours.","To address this challenge, we introduce Fast-DDPM, a simple yet effective approach capable of improving training speed, sampling speed, and generation quality simultaneously.","Unlike DDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains and samples using only 10 time steps.","The key to our method lies in aligning the training and sampling procedures.","We introduced two efficient noise schedulers with 10 time steps: one with uniform time step sampling and another with non-uniform sampling.","We evaluated Fast-DDPM across three medical image-to-image generation tasks: multi-image super-resolution, image denoising, and image-to-image translation.","Fast-DDPM outperformed DDPM and current state-of-the-art methods based on convolutional networks and generative adversarial networks in all tasks.","Additionally, Fast-DDPM reduced training time by a factor of 5 and sampling time by a factor of 100 compared to DDPM.","Our code is publicly available at: https://github.com/mirthAI/Fast-DDPM."],"url":"http://arxiv.org/abs/2405.14802v1","category":"eess.IV"}
{"created":"2024-05-23 17:12:06","title":"Pseudospin density wave intability in 2D electron bilayers","abstract":"We investigate the instability of layer pseudospin paramagnetic (PSP) state to the formation of pseudospin density wave (PSDW) in two-dimensional (2D) electron bilayers, analogous to the formation of Overhauser spin density wave (SDW) in a single-layer 2D electron gas (2DEG) with spin 1/2. Our comprehensive study on phase diagrams, based on the self-consistent Hartree-Fock (HF) theory, reveals that the PSDW has a lower energy than both PSP and pseudospin ferromagnetic (PSF) states near the PSP-PSF phase transition boundary. When the two layers are populated by the same number of electrons, the PSDW momentum $Q_c \\sim 2k_F$ near the PSP-PSDW boundary, where $k_F= (2\\pi n)^{1/2}$ is the Fermi momentum characterized by the density in one of the two layers, and $Q_c$ decreases as the system transitions to the PSF regime. Extending the HF study to the case of unequal layer densities, the PSP phase is unstable to PSDW for small density imbalances, with momentum $Q_c \\sim k_{F,t} + k_{F,b}$, where $k_{F,t}$ and $k_{F,b}$ are Fermi momenta of top and bottom layers, respectively. In PSDW regime, the ground state stability, defined by the energy difference between PSDW and the second lowest-energy state, is one order of magnitude lower than that in PSF regime, and decreases with increasing layer separation $d$. Furthermore, incorporating RPA static screening with the Hubbard-type local field correction leads to disappearance of both SDW and PSDW phases, and pushes the phase boundaries of paramagnetic to ferromagnetic transitions to larger $r_s$ values. Our study on PSDW in 2D electron bilayers is equally applicable to 2D hole bilayers. The idea of pursuing PSDW is, in general, relevant across various 2D bilayer systems, not limited to the parabolic model that we investigate in this paper, and provides a new possibility of exploring novel coherent phases.","sentences":["We investigate the instability of layer pseudospin paramagnetic (PSP) state to the formation of pseudospin density wave (PSDW) in two-dimensional (2D) electron bilayers, analogous to the formation of Overhauser spin density wave (SDW) in a single-layer 2D electron gas (2DEG) with spin 1/2.","Our comprehensive study on phase diagrams, based on the self-consistent Hartree-Fock (HF) theory, reveals that the PSDW has a lower energy than both PSP and pseudospin ferromagnetic (PSF) states near the PSP-PSF phase transition boundary.","When the two layers are populated by the same number of electrons, the PSDW momentum $Q_c \\sim 2k_F$ near the PSP-PSDW boundary, where $k_F= (2\\pi n)^{1/2}$ is the Fermi momentum characterized by the density in one of the two layers, and $Q_c$ decreases as the system transitions to the PSF regime.","Extending the HF study to the case of unequal layer densities, the PSP phase is unstable to PSDW for small density imbalances, with momentum $Q_c \\sim k_{F,t} + k_{F,b}$, where $k_{F,t}$ and $k_{F,b}$ are Fermi momenta of top and bottom layers, respectively.","In PSDW regime, the ground state stability, defined by the energy difference between PSDW and the second lowest-energy state, is one order of magnitude lower than that in PSF regime, and decreases with increasing layer separation $d$.","Furthermore, incorporating RPA static screening with the Hubbard-type local field correction leads to disappearance of both SDW and PSDW phases, and pushes the phase boundaries of paramagnetic to ferromagnetic transitions to larger $r_s$ values.","Our study on PSDW in 2D electron bilayers is equally applicable to 2D hole bilayers.","The idea of pursuing PSDW is, in general, relevant across various 2D bilayer systems, not limited to the parabolic model that we investigate in this paper, and provides a new possibility of exploring novel coherent phases."],"url":"http://arxiv.org/abs/2405.14801v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 17:09:51","title":"Membership Inference on Text-to-Image Diffusion Models via Conditional Likelihood Discrepancy","abstract":"Text-to-image diffusion models have achieved tremendous success in the field of controllable image generation, while also coming along with issues of privacy leakage and data copyrights. Membership inference arises in these contexts as a potential auditing method for detecting unauthorized data usage. While some efforts have been made on diffusion models, they are not applicable to text-to-image diffusion models due to the high computation overhead and enhanced generalization capabilities. In this paper, we first identify a conditional overfitting phenomenon in text-to-image diffusion models, indicating that these models tend to overfit the conditional distribution of images given the text rather than the marginal distribution of images. Based on this observation, we derive an analytical indicator, namely Conditional Likelihood Discrepancy (CLiD), to perform membership inference. This indicator reduces the stochasticity in estimating the memorization of individual samples. Experimental results demonstrate that our method significantly outperforms previous methods across various data distributions and scales. Additionally, our method shows superior resistance to overfitting mitigation strategies such as early stopping and data augmentation.","sentences":["Text-to-image diffusion models have achieved tremendous success in the field of controllable image generation, while also coming along with issues of privacy leakage and data copyrights.","Membership inference arises in these contexts as a potential auditing method for detecting unauthorized data usage.","While some efforts have been made on diffusion models, they are not applicable to text-to-image diffusion models due to the high computation overhead and enhanced generalization capabilities.","In this paper, we first identify a conditional overfitting phenomenon in text-to-image diffusion models, indicating that these models tend to overfit the conditional distribution of images given the text rather than the marginal distribution of images.","Based on this observation, we derive an analytical indicator, namely Conditional Likelihood Discrepancy (CLiD), to perform membership inference.","This indicator reduces the stochasticity in estimating the memorization of individual samples.","Experimental results demonstrate that our method significantly outperforms previous methods across various data distributions and scales.","Additionally, our method shows superior resistance to overfitting mitigation strategies such as early stopping and data augmentation."],"url":"http://arxiv.org/abs/2405.14800v1","category":"cs.CR"}
{"created":"2024-05-23 17:07:20","title":"Exploring modified Kaniadakis entropy: MOND theory and the Bekenstein bound conjecture","abstract":"We examine the potential of Kaniadakis entropy to describe black hole entropy, proposing a modified version accounting for black hole thermodynamics. Additionally, we discuss the Modified Newtonian Dynamics (MOND) theory, a modification of Newton's second law aimed at explaining galaxy rotation curves without resorting to dark matter. Furthermore, we consider the Bekenstein bound conjecture, which imposes an upper limit on the entropy of confined quantum systems. We analyze this conjecture in the context of a modified Kaniadakis entropy and find that it holds for typical values of $\\kappa$, as evidenced by our numerical investigation and displaying a figure. Our exploration underscores the potential of modified Kaniadakis statistics in understanding diverse physical phenomena, from gravitational systems to quantum mechanics, offering a promising direction for future research at the intersection of statistical mechanics and other important areas of physics as well.","sentences":["We examine the potential of Kaniadakis entropy to describe black hole entropy, proposing a modified version accounting for black hole thermodynamics.","Additionally, we discuss the Modified Newtonian Dynamics (MOND) theory, a modification of Newton's second law aimed at explaining galaxy rotation curves without resorting to dark matter.","Furthermore, we consider the Bekenstein bound conjecture, which imposes an upper limit on the entropy of confined quantum systems.","We analyze this conjecture in the context of a modified Kaniadakis entropy and find that it holds for typical values of $\\kappa$, as evidenced by our numerical investigation and displaying a figure.","Our exploration underscores the potential of modified Kaniadakis statistics in understanding diverse physical phenomena, from gravitational systems to quantum mechanics, offering a promising direction for future research at the intersection of statistical mechanics and other important areas of physics as well."],"url":"http://arxiv.org/abs/2405.14799v1","category":"gr-qc"}
{"created":"2024-05-23 17:06:46","title":"Generative Plant Growth Simulation from Sequence-Informed Environmental Conditions","abstract":"A plant growth simulation can be characterized as a reconstructed visual representation of a plant or plant system. The phenotypic characteristics and plant structures are controlled by the scene environment and other contextual attributes. Considering the temporal dependencies and compounding effects of various factors on growth trajectories, we formulate a probabilistic approach to the simulation task by solving a frame synthesis and pattern recognition problem. We introduce a Sequence-Informed Plant Growth Simulation framework (SI-PGS) that employs a conditional generative model to implicitly learn a distribution of possible plant representations within a dynamic scene from a fusion of low dimensional temporal sensor and context data. Methods such as controlled latent sampling and recurrent output connections are used to improve coherence in plant structures between frames of predictions. In this work, we demonstrate that SI-PGS is able to capture temporal dependencies and continuously generate realistic frames of a plant scene.","sentences":["A plant growth simulation can be characterized as a reconstructed visual representation of a plant or plant system.","The phenotypic characteristics and plant structures are controlled by the scene environment and other contextual attributes.","Considering the temporal dependencies and compounding effects of various factors on growth trajectories, we formulate a probabilistic approach to the simulation task by solving a frame synthesis and pattern recognition problem.","We introduce a Sequence-Informed Plant Growth Simulation framework (SI-PGS) that employs a conditional generative model to implicitly learn a distribution of possible plant representations within a dynamic scene from a fusion of low dimensional temporal sensor and context data.","Methods such as controlled latent sampling and recurrent output connections are used to improve coherence in plant structures between frames of predictions.","In this work, we demonstrate that SI-PGS is able to capture temporal dependencies and continuously generate realistic frames of a plant scene."],"url":"http://arxiv.org/abs/2405.14796v1","category":"cs.CV"}
{"created":"2024-05-23 17:05:13","title":"RetAssist: Facilitating Vocabulary Learners with Generative Images in Story Retelling Practices","abstract":"Reading and repeatedly retelling a short story is a common and effective approach to learning the meanings and usages of target words. However, learners often struggle with comprehending, recalling, and retelling the story contexts of these target words. Inspired by the Cognitive Theory of Multimedia Learning, we propose a computational workflow to generate relevant images paired with stories. Based on the workflow, we work with learners and teachers to iteratively design an interactive vocabulary learning system named RetAssist. It can generate sentence-level images of a story to facilitate the understanding and recall of the target words in the story retelling practices. Our within-subjects study (N=24) shows that compared to a baseline system without generative images, RetAssist significantly improves learners' fluency in expressing with target words. Participants also feel that RetAssist eases their learning workload and is more useful. We discuss insights into leveraging text-to-image generative models to support learning tasks.","sentences":["Reading and repeatedly retelling a short story is a common and effective approach to learning the meanings and usages of target words.","However, learners often struggle with comprehending, recalling, and retelling the story contexts of these target words.","Inspired by the Cognitive Theory of Multimedia Learning, we propose a computational workflow to generate relevant images paired with stories.","Based on the workflow, we work with learners and teachers to iteratively design an interactive vocabulary learning system named RetAssist.","It can generate sentence-level images of a story to facilitate the understanding and recall of the target words in the story retelling practices.","Our within-subjects study (N=24) shows that compared to a baseline system without generative images, RetAssist significantly improves learners' fluency in expressing with target words.","Participants also feel that RetAssist eases their learning workload and is more useful.","We discuss insights into leveraging text-to-image generative models to support learning tasks."],"url":"http://arxiv.org/abs/2405.14794v1","category":"cs.HC"}
{"created":"2024-05-23 17:04:04","title":"SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow","abstract":"We introduce SEA-RAFT, a more simple, efficient, and accurate RAFT for optical flow. Compared with RAFT, SEA-RAFT is trained with a new loss (mixture of Laplace). It directly regresses an initial flow for faster convergence in iterative refinements and introduces rigid-motion pre-training to improve generalization. SEA-RAFT achieves state-of-the-art accuracy on the Spring benchmark with a 3.69 endpoint-error (EPE) and a 0.36 1-pixel outlier rate (1px), representing 22.9% and 17.8% error reduction from best published results. In addition, SEA-RAFT obtains the best cross-dataset generalization on KITTI and Spring. With its high efficiency, SEA-RAFT operates at least 2.3x faster than existing methods while maintaining competitive performance. The code is publicly available at https://github.com/princeton-vl/SEA-RAFT.","sentences":["We introduce SEA-RAFT, a more simple, efficient, and accurate RAFT for optical flow.","Compared with RAFT, SEA-RAFT is trained with a new loss (mixture of Laplace).","It directly regresses an initial flow for faster convergence in iterative refinements and introduces rigid-motion pre-training to improve generalization.","SEA-RAFT achieves state-of-the-art accuracy on the Spring benchmark with a 3.69 endpoint-error (EPE) and a 0.36 1-pixel outlier rate (1px), representing 22.9% and 17.8% error reduction from best published results.","In addition, SEA-RAFT obtains the best cross-dataset generalization on KITTI and Spring.","With its high efficiency, SEA-RAFT operates at least 2.3x faster than existing methods while maintaining competitive performance.","The code is publicly available at https://github.com/princeton-vl/SEA-RAFT."],"url":"http://arxiv.org/abs/2405.14793v1","category":"cs.CV"}
{"created":"2024-05-23 17:01:53","title":"Tetrahedron Instantons on Orbifolds","abstract":"Given a homomorphism $\\tau$ from a finite group $\\mathsf{\\Gamma}$ to $\\mathsf{SU}(4)$ with image $\\mathsf{\\Gamma}^\\tau$, we construct a cohomological gauge theory on a noncommutative resolution of the quotient singularity $\\mathbb{C}^4/\\mathsf{\\Gamma}^\\tau$ whose BRST fixed points are $\\mathsf{\\Gamma}$-invariant tetrahedron instantons on a generally non-effective orbifold. The partition function computes the expectation values of complex codimension one defect operators in rank $r$ cohomological Donaldson-Thomas theory on a flat gerbe over the quotient stack $[\\mathbb{C}^4/\\,\\mathsf{\\Gamma}^\\tau]$. We describe the generalized ADHM parametrization of the tetrahedron instanton moduli space, and evaluate the orbifold partition functions through virtual torus localization. If $\\mathsf{\\Gamma}$ is an abelian group the partition function is expressed as a combinatorial series over arrays of $\\mathsf{\\Gamma}$-coloured plane partitions, while if $\\mathsf{\\Gamma}$ is non-abelian the partition function localizes onto a sum over torus-invariant connected components of the moduli space labelled by lower-dimensional partitions. When $\\mathsf{\\Gamma}=\\mathbb{Z}_n$ is a finite abelian subgroup of $\\mathsf{SL}(2,\\mathbb{C})$, we exhibit the reduction of Donaldson-Thomas theory on the toric Calabi-Yau four-orbifold $\\mathbb{C}^2/\\,\\mathsf{\\Gamma}\\times\\mathbb{C}^2$ to the cohomological field theory of tetrahedron instantons, from which we express the partition function as a closed infinite product formula. We also use the crepant resolution correpondence to derive a closed formula for the partition function on any polyhedral singularity.","sentences":["Given a homomorphism $\\tau$ from a finite group $\\mathsf{\\Gamma}$ to $\\mathsf{SU}(4)$ with image $\\mathsf{\\Gamma}^\\tau$, we construct a cohomological gauge theory on a noncommutative resolution of the quotient singularity $\\mathbb{C}^4/\\mathsf{\\Gamma}^\\tau$ whose BRST fixed points are $\\mathsf{\\Gamma}$-invariant tetrahedron instantons on a generally non-effective orbifold.","The partition function computes the expectation values of complex codimension one defect operators in rank $r$ cohomological Donaldson-Thomas theory on a flat gerbe over the quotient stack $","[\\mathbb{C}^4/\\,\\mathsf{\\Gamma}^\\tau]$. We describe the generalized ADHM parametrization of the tetrahedron instanton moduli space, and evaluate the orbifold partition functions through virtual torus localization.","If $\\mathsf{\\Gamma}$ is an abelian group the partition function is expressed as a combinatorial series over arrays of $\\mathsf{\\Gamma}$-coloured plane partitions, while if $\\mathsf{\\Gamma}$ is non-abelian the partition function localizes onto a sum over torus-invariant connected components of the moduli space labelled by lower-dimensional partitions.","When $\\mathsf{\\Gamma}=\\mathbb{Z}_n$ is a finite abelian subgroup of $\\mathsf{SL}(2,\\mathbb{C})$, we exhibit the reduction of Donaldson-Thomas theory on the toric Calabi-Yau four-orbifold $\\mathbb{C}^2/\\,\\mathsf{\\Gamma}\\times\\mathbb{C}^2$ to the cohomological field theory of tetrahedron instantons, from which we express the partition function as a closed infinite product formula.","We also use the crepant resolution correpondence to derive a closed formula for the partition function on any polyhedral singularity."],"url":"http://arxiv.org/abs/2405.14792v1","category":"hep-th"}
{"created":"2024-05-23 17:00:15","title":"DIDI: Diffusion-Guided Diversity for Offline Behavioral Generation","abstract":"In this paper, we propose a novel approach called DIffusion-guided DIversity (DIDI) for offline behavioral generation. The goal of DIDI is to learn a diverse set of skills from a mixture of label-free offline data. We achieve this by leveraging diffusion probabilistic models as priors to guide the learning process and regularize the policy. By optimizing a joint objective that incorporates diversity and diffusion-guided regularization, we encourage the emergence of diverse behaviors while maintaining the similarity to the offline data. Experimental results in four decision-making domains (Push, Kitchen, Humanoid, and D4RL tasks) show that DIDI is effective in discovering diverse and discriminative skills. We also introduce skill stitching and skill interpolation, which highlight the generalist nature of the learned skill space. Further, by incorporating an extrinsic reward function, DIDI enables reward-guided behavior generation, facilitating the learning of diverse and optimal behaviors from sub-optimal data.","sentences":["In this paper, we propose a novel approach called DIffusion-guided DIversity (DIDI) for offline behavioral generation.","The goal of DIDI is to learn a diverse set of skills from a mixture of label-free offline data.","We achieve this by leveraging diffusion probabilistic models as priors to guide the learning process and regularize the policy.","By optimizing a joint objective that incorporates diversity and diffusion-guided regularization, we encourage the emergence of diverse behaviors while maintaining the similarity to the offline data.","Experimental results in four decision-making domains (Push, Kitchen, Humanoid, and D4RL tasks) show that DIDI is effective in discovering diverse and discriminative skills.","We also introduce skill stitching and skill interpolation, which highlight the generalist nature of the learned skill space.","Further, by incorporating an extrinsic reward function, DIDI enables reward-guided behavior generation, facilitating the learning of diverse and optimal behaviors from sub-optimal data."],"url":"http://arxiv.org/abs/2405.14790v1","category":"cs.LG"}
{"created":"2024-05-23 16:57:57","title":"A Bayesian Approach to Estimate Causal Peer Influence Accounting for Latent Network Homophily","abstract":"Researchers have focused on understanding how individual's behavior is influenced by the behaviors of their peers in observational studies of social networks. Identifying and estimating causal peer influence, however, is challenging due to confounding by homophily, where people tend to connect with those who share similar characteristics with them. Moreover, since all the attributes driving homophily are generally not always observed and act as unobserved confounders, identifying and estimating causal peer influence becomes infeasible using standard causal identification assumptions. In this paper, we address this challenge by leveraging latent locations inferred from the network itself to disentangle homophily from causal peer influence, and we extend this approach to multiple networks by adopting a Bayesian hierarchical modeling framework. To accommodate the nonlinear dependency of peer influence on individual behavior, we employ a Bayesian nonparametric method, specifically Bayesian Additive Regression Trees (BART), and we propose a Bayesian framework that accounts for the uncertainty in inferring latent locations. We assess the operating characteristics of the estimator via extensive simulation study. Finally, we apply our method to estimate causal peer influence in advice-seeking networks of teachers in secondary schools, in order to assess whether the teachers' belief about mathematics education is influenced by the beliefs of their peers from whom they receive advice. Our results suggest that, overlooking latent homophily can lead to either underestimation or overestimation of causal peer influence, accompanied by considerable estimation uncertainty.","sentences":["Researchers have focused on understanding how individual's behavior is influenced by the behaviors of their peers in observational studies of social networks.","Identifying and estimating causal peer influence, however, is challenging due to confounding by homophily, where people tend to connect with those who share similar characteristics with them.","Moreover, since all the attributes driving homophily are generally not always observed and act as unobserved confounders, identifying and estimating causal peer influence becomes infeasible using standard causal identification assumptions.","In this paper, we address this challenge by leveraging latent locations inferred from the network itself to disentangle homophily from causal peer influence, and we extend this approach to multiple networks by adopting a Bayesian hierarchical modeling framework.","To accommodate the nonlinear dependency of peer influence on individual behavior, we employ a Bayesian nonparametric method, specifically Bayesian Additive Regression Trees (BART), and we propose a Bayesian framework that accounts for the uncertainty in inferring latent locations.","We assess the operating characteristics of the estimator via extensive simulation study.","Finally, we apply our method to estimate causal peer influence in advice-seeking networks of teachers in secondary schools, in order to assess whether the teachers' belief about mathematics education is influenced by the beliefs of their peers from whom they receive advice.","Our results suggest that, overlooking latent homophily can lead to either underestimation or overestimation of causal peer influence, accompanied by considerable estimation uncertainty."],"url":"http://arxiv.org/abs/2405.14789v1","category":"stat.AP"}
{"created":"2024-05-23 16:57:54","title":"Masked Image Modelling for retinal OCT understanding","abstract":"This work explores the effectiveness of masked image modelling for learning representations of retinal OCT images. To this end, we leverage Masked Autoencoders (MAE), a simple and scalable method for self-supervised learning, to obtain a powerful and general representation for OCT images by training on 700K OCT images from 41K patients collected under real world clinical settings. We also provide the first extensive evaluation for a model of OCT on a challenging battery of 6 downstream tasks. Our model achieves strong performance when fully finetuned but can also serve as a versatile frozen feature extractor for many tasks using lightweight adapters. Furthermore, we propose an extension of the MAE pretraining to fuse OCT with an auxiliary modality, namely, IR fundus images and learn a joint model for both. We demonstrate our approach improves performance on a multimodal downstream application. Our experiments utilize most publicly available OCT datasets, thus enabling future comparisons. Our code and model weights are publicly available https://github.com/TheoPis/MIM_OCT.","sentences":["This work explores the effectiveness of masked image modelling for learning representations of retinal OCT images.","To this end, we leverage Masked Autoencoders (MAE), a simple and scalable method for self-supervised learning, to obtain a powerful and general representation for OCT images by training on 700K OCT images from 41K patients collected under real world clinical settings.","We also provide the first extensive evaluation for a model of OCT on a challenging battery of 6 downstream tasks.","Our model achieves strong performance when fully finetuned but can also serve as a versatile frozen feature extractor for many tasks using lightweight adapters.","Furthermore, we propose an extension of the MAE pretraining to fuse OCT with an auxiliary modality, namely, IR fundus images and learn a joint model for both.","We demonstrate our approach improves performance on a multimodal downstream application.","Our experiments utilize most publicly available OCT datasets, thus enabling future comparisons.","Our code and model weights are publicly available https://github.com/TheoPis/MIM_OCT."],"url":"http://arxiv.org/abs/2405.14788v1","category":"cs.CV"}
{"created":"2024-05-23 16:57:29","title":"Construction and sampling of alloy cluster expansions -- A tutorial","abstract":"Crystalline alloys and related mixed systems make up a large family of materials with high tunability which have been proposed as the solution to a large number of energy related materials design problems. Due to the presence of chemical order and disorder in these systems, neither experimental efforts nor ab-initio computational methods alone are sufficient to span the inherently large configuration space. Therefore, fast and accurate models are necessary. To this end, cluster expansions have been widely and successfully used for the past decades. Cluster expansions are generalized Ising models designed to predict the energy of any atomic configuration of a system after training on a small subset of the available configurations. Constructing and sampling a cluster expansion consists of multiple steps that have to be performed with care. In this tutorial, we provide a comprehensive guide to this process, highlighting important considerations and potential pitfalls. The tutorial consists of three parts, starting with cluster expansion construction for a relatively simple system, continuing with strategies for more challenging systems such as surfaces and closing with examples of Monte Carlo sampling of cluster expansions to study order-disorder transitions and phase diagrams.","sentences":["Crystalline alloys and related mixed systems make up a large family of materials with high tunability which have been proposed as the solution to a large number of energy related materials design problems.","Due to the presence of chemical order and disorder in these systems, neither experimental efforts nor ab-initio computational methods alone are sufficient to span the inherently large configuration space.","Therefore, fast and accurate models are necessary.","To this end, cluster expansions have been widely and successfully used for the past decades.","Cluster expansions are generalized Ising models designed to predict the energy of any atomic configuration of a system after training on a small subset of the available configurations.","Constructing and sampling a cluster expansion consists of multiple steps that have to be performed with care.","In this tutorial, we provide a comprehensive guide to this process, highlighting important considerations and potential pitfalls.","The tutorial consists of three parts, starting with cluster expansion construction for a relatively simple system, continuing with strategies for more challenging systems such as surfaces and closing with examples of Monte Carlo sampling of cluster expansions to study order-disorder transitions and phase diagrams."],"url":"http://arxiv.org/abs/2405.14787v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 16:49:09","title":"Unified Neural Backdoor Removal with Only Few Clean Samples through Unlearning and Relearning","abstract":"The application of deep neural network models in various security-critical applications has raised significant security concerns, particularly the risk of backdoor attacks. Neural backdoors pose a serious security threat as they allow attackers to maliciously alter model behavior. While many defenses have been explored, existing approaches are often bounded by model-specific constraints, or necessitate complex alterations to the training process, or fall short against diverse backdoor attacks. In this work, we introduce a novel method for comprehensive and effective elimination of backdoors, called ULRL (short for UnLearn and ReLearn for backdoor removal). ULRL requires only a small set of clean samples and works effectively against all kinds of backdoors. It first applies unlearning for identifying suspicious neurons and then targeted neural weight tuning for backdoor mitigation (i.e., by promoting significant weight deviation on the suspicious neurons). Evaluated against 12 different types of backdoors, ULRL is shown to significantly outperform state-of-the-art methods in eliminating backdoors whilst preserving the model utility.","sentences":["The application of deep neural network models in various security-critical applications has raised significant security concerns, particularly the risk of backdoor attacks.","Neural backdoors pose a serious security threat as they allow attackers to maliciously alter model behavior.","While many defenses have been explored, existing approaches are often bounded by model-specific constraints, or necessitate complex alterations to the training process, or fall short against diverse backdoor attacks.","In this work, we introduce a novel method for comprehensive and effective elimination of backdoors, called ULRL (short for UnLearn and ReLearn for backdoor removal).","ULRL requires only a small set of clean samples and works effectively against all kinds of backdoors.","It first applies unlearning for identifying suspicious neurons and then targeted neural weight tuning for backdoor mitigation (i.e., by promoting significant weight deviation on the suspicious neurons).","Evaluated against 12 different types of backdoors, ULRL is shown to significantly outperform state-of-the-art methods in eliminating backdoors whilst preserving the model utility."],"url":"http://arxiv.org/abs/2405.14781v1","category":"cs.CR"}
{"created":"2024-05-23 16:48:06","title":"Metric Flow Matching for Smooth Interpolations on the Data Manifold","abstract":"Matching objectives underpin the success of modern generative models and rely on constructing conditional paths that transform a source distribution into a target distribution. Despite being a fundamental building block, conditional paths have been designed principally under the assumption of Euclidean geometry, resulting in straight interpolations. However, this can be particularly restrictive for tasks such as trajectory inference, where straight paths might lie outside the data manifold, thus failing to capture the underlying dynamics giving rise to the observed marginals. In this paper, we propose Metric Flow Matching (MFM), a novel simulation-free framework for conditional flow matching where interpolants are approximate geodesics learned by minimizing the kinetic energy of a data-induced Riemannian metric. This way, the generative model matches vector fields on the data manifold, which corresponds to lower uncertainty and more meaningful interpolations. We prescribe general metrics to instantiate MFM, independent of the task, and test it on a suite of challenging problems including LiDAR navigation, unpaired image translation, and modeling cellular dynamics. We observe that MFM outperforms the Euclidean baselines, particularly achieving SOTA on single-cell trajectory prediction.","sentences":["Matching objectives underpin the success of modern generative models and rely on constructing conditional paths that transform a source distribution into a target distribution.","Despite being a fundamental building block, conditional paths have been designed principally under the assumption of Euclidean geometry, resulting in straight interpolations.","However, this can be particularly restrictive for tasks such as trajectory inference, where straight paths might lie outside the data manifold, thus failing to capture the underlying dynamics giving rise to the observed marginals.","In this paper, we propose Metric Flow Matching (MFM), a novel simulation-free framework for conditional flow matching where interpolants are approximate geodesics learned by minimizing the kinetic energy of a data-induced Riemannian metric.","This way, the generative model matches vector fields on the data manifold, which corresponds to lower uncertainty and more meaningful interpolations.","We prescribe general metrics to instantiate MFM, independent of the task, and test it on a suite of challenging problems including LiDAR navigation, unpaired image translation, and modeling cellular dynamics.","We observe that MFM outperforms the Euclidean baselines, particularly achieving SOTA on single-cell trajectory prediction."],"url":"http://arxiv.org/abs/2405.14780v1","category":"cs.LG"}
{"created":"2024-05-23 16:45:52","title":"Optimal Rates for Vector-Valued Spectral Regularization Learning Algorithms","abstract":"We study theoretical properties of a broad class of regularized algorithms with vector-valued output. These spectral algorithms include kernel ridge regression, kernel principal component regression, various implementations of gradient descent and many more. Our contributions are twofold. First, we rigorously confirm the so-called saturation effect for ridge regression with vector-valued output by deriving a novel lower bound on learning rates; this bound is shown to be suboptimal when the smoothness of the regression function exceeds a certain level. Second, we present the upper bound for the finite sample risk general vector-valued spectral algorithms, applicable to both well-specified and misspecified scenarios (where the true regression function lies outside of the hypothesis space) which is minimax optimal in various regimes. All of our results explicitly allow the case of infinite-dimensional output variables, proving consistency of recent practical applications.","sentences":["We study theoretical properties of a broad class of regularized algorithms with vector-valued output.","These spectral algorithms include kernel ridge regression, kernel principal component regression, various implementations of gradient descent and many more.","Our contributions are twofold.","First, we rigorously confirm the so-called saturation effect for ridge regression with vector-valued output by deriving a novel lower bound on learning rates; this bound is shown to be suboptimal when the smoothness of the regression function exceeds a certain level.","Second, we present the upper bound for the finite sample risk general vector-valued spectral algorithms, applicable to both well-specified and misspecified scenarios (where the true regression function lies outside of the hypothesis space) which is minimax optimal in various regimes.","All of our results explicitly allow the case of infinite-dimensional output variables, proving consistency of recent practical applications."],"url":"http://arxiv.org/abs/2405.14778v1","category":"stat.ML"}
{"created":"2024-05-23 16:35:52","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models","abstract":"Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code will be released at https://github.com/zjunlp/EasyEdit.","sentences":["Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing.","Where the updated knowledge resides in memories is a fundamental question for model editing.","In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings.","For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality).","For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization).","Therefore, we propose WISE to bridge the gap between memories.","In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge.","We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query.","For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts.","Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral.","Code will be released at https://github.com/zjunlp/EasyEdit."],"url":"http://arxiv.org/abs/2405.14768v1","category":"cs.CL"}
{"created":"2024-05-23 16:30:51","title":"Neural Pfaffians: Solving Many Many-Electron Schr\u00f6dinger Equations","abstract":"Neural wave functions accomplished unprecedented accuracies in approximating the ground state of many-electron systems, though at a high computational cost. Recent works proposed amortizing the cost by learning generalized wave functions across different structures and compounds instead of solving each problem independently. Enforcing the permutation antisymmetry of electrons in such generalized neural wave functions remained challenging as existing methods require discrete orbital selection via non-learnable hand-crafted algorithms. This work tackles the problem by defining overparametrized, fully learnable neural wave functions suitable for generalization across molecules. We achieve this by relying on Pfaffians rather than Slater determinants. The Pfaffian allows us to enforce the antisymmetry on arbitrary electronic systems without any constraint on electronic spin configurations or molecular structure. Our empirical evaluation finds that a single neural Pfaffian calculates the ground state and ionization energies with chemical accuracy across various systems. On the TinyMol dataset, we outperform the `gold-standard' CCSD(T) CBS reference energies by 1.9m$E_h$ and reduce energy errors compared to previous generalized neural wave functions by up to an order of magnitude.","sentences":["Neural wave functions accomplished unprecedented accuracies in approximating the ground state of many-electron systems, though at a high computational cost.","Recent works proposed amortizing the cost by learning generalized wave functions across different structures and compounds instead of solving each problem independently.","Enforcing the permutation antisymmetry of electrons in such generalized neural wave functions remained challenging as existing methods require discrete orbital selection via non-learnable hand-crafted algorithms.","This work tackles the problem by defining overparametrized, fully learnable neural wave functions suitable for generalization across molecules.","We achieve this by relying on Pfaffians rather than Slater determinants.","The Pfaffian allows us to enforce the antisymmetry on arbitrary electronic systems without any constraint on electronic spin configurations or molecular structure.","Our empirical evaluation finds that a single neural Pfaffian calculates the ground state and ionization energies with chemical accuracy across various systems.","On the TinyMol dataset, we outperform the `gold-standard' CCSD(T) CBS reference energies by 1.9m$E_h$ and reduce energy errors compared to previous generalized neural wave functions by up to an order of magnitude."],"url":"http://arxiv.org/abs/2405.14762v1","category":"cs.LG"}
{"created":"2024-05-23 16:30:26","title":"Effective & Ethical Mentorship in Physics and Astronomy through Grassroots Organizations","abstract":"Effective and ethical mentorship practices are crucial to improving recruitment and retention especially for historically minoritized groups (HMGs). Spectrum is a diversity, inclusion, equity, and accessibility (DEIA) grassroots organization committed to empowering equitable excellence through sustainable change. By improving transparency and DEIA within the fields of physics and astronomy, we can empower the next generation of diverse scientists and increase field retention. Starting within our home department at George Mason University and moving outwards, we ensure our students leave as advocates for DEIA and AJEDI (access, justice, equity, diversity, and inclusion) through education and mentorship. Spectrum is providing professionally trained peer mentors to aid students in all facets of their academic and personal lives. Although the peer mentoring program existed since the creation of Spectrum in Spring 2020, we have recently developed and implemented a formal mentorship training for both student and faculty mentors thus increasing the quality, trustworthiness, and confidence of our mentors. Using the latest mentorship research available, this training is developed by Spectrum for George Mason University, with the ability to implement the training at any institution.","sentences":["Effective and ethical mentorship practices are crucial to improving recruitment and retention especially for historically minoritized groups (HMGs).","Spectrum is a diversity, inclusion, equity, and accessibility (DEIA) grassroots organization committed to empowering equitable excellence through sustainable change.","By improving transparency and DEIA within the fields of physics and astronomy, we can empower the next generation of diverse scientists and increase field retention.","Starting within our home department at George Mason University and moving outwards, we ensure our students leave as advocates for DEIA and AJEDI (access, justice, equity, diversity, and inclusion) through education and mentorship.","Spectrum is providing professionally trained peer mentors to aid students in all facets of their academic and personal lives.","Although the peer mentoring program existed since the creation of Spectrum in Spring 2020, we have recently developed and implemented a formal mentorship training for both student and faculty mentors thus increasing the quality, trustworthiness, and confidence of our mentors.","Using the latest mentorship research available, this training is developed by Spectrum for George Mason University, with the ability to implement the training at any institution."],"url":"http://arxiv.org/abs/2405.14761v1","category":"physics.ed-ph"}
{"created":"2024-05-23 16:30:23","title":"Einstein manifolds with optical geometries of Kerr type","abstract":"We classify the Ricci flat Lorentzian $n$-manifolds satisfying three particular conditions, encoding and combining some crucial features of the Kerr metrics and the Robinson-Trautman optical structures. We prove that if $n > 4$, there is no Lorentzian manifold satisfying the considered conditions, while for $n = 4$ there are two large classes of such manifolds. Each class consists of manifolds fibering over open Riemann surfaces, equipped with a metric of constant Gaussian curvature $\\kappa = 1$ or $\\kappa = -1$. The first class properly includes a three parameter family of metrics admitting real analytic extensions to $(\\mathbb R^3 \\setminus\\{0\\}) \\times \\mathbb R = (S^2 \\times \\mathbb R_+) \\times \\mathbb R$ (all others develop singularities) and all such extensions are isometric to the well-known Kerr metrics. The three parameters correspond to the three space-like components of the angular momentum of the gravitational field. The second class contains a subclass of metrics defined on $\\big(\\mathbb D\\times \\mathbb R_+\\big)\\times \\mathbb R$, where $\\mathbb D$ is the Lobachevsky Poincar\\'e disc. This subclass is in bijection with the holomorphic functions on $\\mathbb D$ satisfying an appropriate open condition. These and other results are obtained as consequences of a very simple method of generating a number of explicit examples of Ricci flat Lorentzian manifolds.","sentences":["We classify the Ricci flat Lorentzian $n$-manifolds satisfying three particular conditions, encoding and combining some crucial features of the Kerr metrics and the Robinson-Trautman optical structures.","We prove that if $n > 4$, there is no Lorentzian manifold satisfying the considered conditions, while for $n = 4$ there are two large classes of such manifolds.","Each class consists of manifolds fibering over open Riemann surfaces, equipped with a metric of constant Gaussian curvature $\\kappa = 1$ or $\\kappa = -1$.","The first class properly includes a three parameter family of metrics admitting real analytic extensions to $(\\mathbb R^3 \\setminus\\{0\\})","\\times \\mathbb R = (S^2 \\times \\mathbb R_+)","\\times \\mathbb R$ (all others develop singularities) and all such extensions are isometric to the well-known Kerr metrics.","The three parameters correspond to the three space-like components of the angular momentum of the gravitational field.","The second class contains a subclass of metrics defined on $\\big(\\mathbb D\\times \\mathbb R_+\\big)\\times \\mathbb R$, where $\\mathbb D$ is the Lobachevsky Poincar\\'e disc.","This subclass is in bijection with the holomorphic functions on $\\mathbb D$ satisfying an appropriate open condition.","These and other results are obtained as consequences of a very simple method of generating a number of explicit examples of Ricci flat Lorentzian manifolds."],"url":"http://arxiv.org/abs/2405.14760v1","category":"math.DG"}
{"created":"2024-05-23 16:29:29","title":"Axioms for AI Alignment from Human Feedback","abstract":"In the context of reinforcement learning from human feedback (RLHF), the reward function is generally derived from maximum likelihood estimation of a random utility model based on pairwise comparisons made by humans. The problem of learning a reward function is one of preference aggregation that, we argue, largely falls within the scope of social choice theory. From this perspective, we can evaluate different aggregation methods via established axioms, examining whether these methods meet or fail well-known standards. We demonstrate that both the Bradley-Terry-Luce Model and its broad generalizations fail to meet basic axioms. In response, we develop novel rules for learning reward functions with strong axiomatic guarantees. A key innovation from the standpoint of social choice is that our problem has a linear structure, which greatly restricts the space of feasible rules and leads to a new paradigm that we call linear social choice.","sentences":["In the context of reinforcement learning from human feedback (RLHF), the reward function is generally derived from maximum likelihood estimation of a random utility model based on pairwise comparisons made by humans.","The problem of learning a reward function is one of preference aggregation that, we argue, largely falls within the scope of social choice theory.","From this perspective, we can evaluate different aggregation methods via established axioms, examining whether these methods meet or fail well-known standards.","We demonstrate that both the Bradley-Terry-Luce Model and its broad generalizations fail to meet basic axioms.","In response, we develop novel rules for learning reward functions with strong axiomatic guarantees.","A key innovation from the standpoint of social choice is that our problem has a linear structure, which greatly restricts the space of feasible rules and leads to a new paradigm that we call linear social choice."],"url":"http://arxiv.org/abs/2405.14758v1","category":"cs.GT"}
{"created":"2024-05-23 16:19:32","title":"A Transformer-Based Approach for Smart Invocation of Automatic Code Completion","abstract":"Transformer-based language models are highly effective for code completion, with much research dedicated to enhancing the content of these completions. Despite their effectiveness, these models come with high operational costs and can be intrusive, especially when they suggest too often and interrupt developers who are concentrating on their work. Current research largely overlooks how these models interact with developers in practice and neglects to address when a developer should receive completion suggestions. To tackle this issue, we developed a machine learning model that can accurately predict when to invoke a code completion tool given the code context and available telemetry data.   To do so, we collect a dataset of 200k developer interactions with our cross-IDE code completion plugin and train several invocation filtering models. Our results indicate that our small-scale transformer model significantly outperforms the baseline while maintaining low enough latency. We further explore the search space for integrating additional telemetry data into a pre-trained transformer directly and obtain promising results. To further demonstrate our approach's practical potential, we deployed the model in an online environment with 34 developers and provided real-world insights based on 74k actual invocations.","sentences":["Transformer-based language models are highly effective for code completion, with much research dedicated to enhancing the content of these completions.","Despite their effectiveness, these models come with high operational costs and can be intrusive, especially when they suggest too often and interrupt developers who are concentrating on their work.","Current research largely overlooks how these models interact with developers in practice and neglects to address when a developer should receive completion suggestions.","To tackle this issue, we developed a machine learning model that can accurately predict when to invoke a code completion tool given the code context and available telemetry data.   ","To do so, we collect a dataset of 200k developer interactions with our cross-IDE code completion plugin and train several invocation filtering models.","Our results indicate that our small-scale transformer model significantly outperforms the baseline while maintaining low enough latency.","We further explore the search space for integrating additional telemetry data into a pre-trained transformer directly and obtain promising results.","To further demonstrate our approach's practical potential, we deployed the model in an online environment with 34 developers and provided real-world insights based on 74k actual invocations."],"url":"http://arxiv.org/abs/2405.14753v1","category":"cs.SE"}
{"created":"2024-05-23 16:18:09","title":"Qudit generalization of the qubit echo and its application to a qutrit-based Toffoli gate","abstract":"The fidelity of certain gates on noisy quantum computers may be improved when they are implemented using more than two levels of the involved transmons. The main impediments to achieving this potential are the dynamic gate phase errors that cannot be corrected via calibration. The standard tool for countering such phase errors in two-level qubits is the echo protocol, often referred to as the dynamical decoupling sequence, where the evolution of a qubit is punctuated by an even number of X gates. We introduce basis cycling, which is a direct generalization of the qubit echo to general qudits, and provide a framework in which to design gate sequences to produce desired effects using this technique. We then apply basis cycling to a Toffoli gate decomposition incorporating a qutrit and obtain a CCZ gate fidelity of 93.8 +- 0.1 via quantum process tomography on an IBM quantum computer. The gate fidelity remains stable without recalibration even while the resonant frequency of the qutrit fluctuates, highlighting the dynamical nature of phase error cancellation through basis cycling. Our results demonstrate that one of the biggest difficulties in implementing qudit-based gate decompositions on superconducting quantum computers can be systematically overcome when certain conditions are met, and thus open a path toward fulfilling the promise of qudits as gate optimization agents.","sentences":["The fidelity of certain gates on noisy quantum computers may be improved when they are implemented using more than two levels of the involved transmons.","The main impediments to achieving this potential are the dynamic gate phase errors that cannot be corrected via calibration.","The standard tool for countering such phase errors in two-level qubits is the echo protocol, often referred to as the dynamical decoupling sequence, where the evolution of a qubit is punctuated by an even number of X gates.","We introduce basis cycling, which is a direct generalization of the qubit echo to general qudits, and provide a framework in which to design gate sequences to produce desired effects using this technique.","We then apply basis cycling to a Toffoli gate decomposition incorporating a qutrit and obtain a CCZ gate fidelity of 93.8 +- 0.1 via quantum process tomography on an IBM quantum computer.","The gate fidelity remains stable without recalibration even while the resonant frequency of the qutrit fluctuates, highlighting the dynamical nature of phase error cancellation through basis cycling.","Our results demonstrate that one of the biggest difficulties in implementing qudit-based gate decompositions on superconducting quantum computers can be systematically overcome when certain conditions are met, and thus open a path toward fulfilling the promise of qudits as gate optimization agents."],"url":"http://arxiv.org/abs/2405.14752v1","category":"quant-ph"}
{"created":"2024-05-23 16:17:16","title":"Extreme Solar Flare Prediction Using Residual Networks with HMI Magnetograms and Intensitygrams","abstract":"Solar flares, especially C, M, and X class, pose significant risks to satellite operations, communication systems, and power grids. We present a novel approach for predicting extreme solar flares using HMI intensitygrams and magnetograms. By detecting sunspots from intensitygrams and extracting magnetic field patches from magnetograms, we train a Residual Network (ResNet) to classify extreme class flares. Our model demonstrates high accuracy, offering a robust tool for predicting extreme solar flares and improving space weather forecasting. Additionally, we show that HMI magnetograms provide more useful data for deep learning compared to other SDO AIA images by better capturing features critical for predicting flare magnitudes. This study underscores the importance of identifying magnetic fields in solar flare prediction, marking a significant advancement in solar activity prediction with practical implications for mitigating space weather impacts.","sentences":["Solar flares, especially C, M, and X class, pose significant risks to satellite operations, communication systems, and power grids.","We present a novel approach for predicting extreme solar flares using HMI intensitygrams and magnetograms.","By detecting sunspots from intensitygrams and extracting magnetic field patches from magnetograms, we train a Residual Network (ResNet) to classify extreme class flares.","Our model demonstrates high accuracy, offering a robust tool for predicting extreme solar flares and improving space weather forecasting.","Additionally, we show that HMI magnetograms provide more useful data for deep learning compared to other SDO AIA images by better capturing features critical for predicting flare magnitudes.","This study underscores the importance of identifying magnetic fields in solar flare prediction, marking a significant advancement in solar activity prediction with practical implications for mitigating space weather impacts."],"url":"http://arxiv.org/abs/2405.14750v1","category":"astro-ph.SR"}
{"created":"2024-05-23 16:16:58","title":"Policy Gradient Methods for Risk-Sensitive Distributional Reinforcement Learning with Provable Convergence","abstract":"Risk-sensitive reinforcement learning (RL) is crucial for maintaining reliable performance in many high-stakes applications. While most RL methods aim to learn a point estimate of the random cumulative cost, distributional RL (DRL) seeks to estimate the entire distribution of it. The distribution provides all necessary information about the cost and leads to a unified framework for handling various risk measures in a risk-sensitive setting. However, developing policy gradient methods for risk-sensitive DRL is inherently more complex as it pertains to finding the gradient of a probability measure. This paper introduces a policy gradient method for risk-sensitive DRL with general coherent risk measures, where we provide an analytical form of the probability measure's gradient. We further prove the local convergence of the proposed algorithm under mild smoothness assumptions. For practical use, we also design a categorical distributional policy gradient algorithm (CDPG) based on categorical distributional policy evaluation and trajectory-based gradient estimation. Through experiments on a stochastic cliff-walking environment, we illustrate the benefits of considering a risk-sensitive setting in DRL.","sentences":["Risk-sensitive reinforcement learning (RL) is crucial for maintaining reliable performance in many high-stakes applications.","While most RL methods aim to learn a point estimate of the random cumulative cost, distributional RL (DRL) seeks to estimate the entire distribution of it.","The distribution provides all necessary information about the cost and leads to a unified framework for handling various risk measures in a risk-sensitive setting.","However, developing policy gradient methods for risk-sensitive DRL is inherently more complex as it pertains to finding the gradient of a probability measure.","This paper introduces a policy gradient method for risk-sensitive DRL with general coherent risk measures, where we provide an analytical form of the probability measure's gradient.","We further prove the local convergence of the proposed algorithm under mild smoothness assumptions.","For practical use, we also design a categorical distributional policy gradient algorithm (CDPG) based on categorical distributional policy evaluation and trajectory-based gradient estimation.","Through experiments on a stochastic cliff-walking environment, we illustrate the benefits of considering a risk-sensitive setting in DRL."],"url":"http://arxiv.org/abs/2405.14749v1","category":"cs.LG"}
{"created":"2024-05-23 16:16:00","title":"MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs","abstract":"Predicting future values in multivariate time series is vital across various domains. This work explores the use of large language models (LLMs) for this task. However, LLMs typically handle one-dimensional data. We introduce MultiCast, a zero-shot LLM-based approach for multivariate time series forecasting. It allows LLMs to receive multivariate time series as input, through three novel token multiplexing solutions that effectively reduce dimensionality while preserving key repetitive patterns. Additionally, a quantization scheme helps LLMs to better learn these patterns, while significantly reducing token use for practical applications. We showcase the performance of our approach in terms of RMSE and execution time against state-of-the-art approaches on three real-world datasets.","sentences":["Predicting future values in multivariate time series is vital across various domains.","This work explores the use of large language models (LLMs) for this task.","However, LLMs typically handle one-dimensional data.","We introduce MultiCast, a zero-shot LLM-based approach for multivariate time series forecasting.","It allows LLMs to receive multivariate time series as input, through three novel token multiplexing solutions that effectively reduce dimensionality while preserving key repetitive patterns.","Additionally, a quantization scheme helps LLMs to better learn these patterns, while significantly reducing token use for practical applications.","We showcase the performance of our approach in terms of RMSE and execution time against state-of-the-art approaches on three real-world datasets."],"url":"http://arxiv.org/abs/2405.14748v1","category":"cs.LG"}
{"created":"2024-05-23 16:15:17","title":"TopoLogic: An Interpretable Pipeline for Lane Topology Reasoning on Driving Scenes","abstract":"As an emerging task that integrates perception and reasoning, topology reasoning in autonomous driving scenes has recently garnered widespread attention. However, existing work often emphasizes \"perception over reasoning\": they typically boost reasoning performance by enhancing the perception of lanes and directly adopt MLP to learn lane topology from lane query. This paradigm overlooks the geometric features intrinsic to the lanes themselves and are prone to being influenced by inherent endpoint shifts in lane detection.   To tackle this issue, we propose an interpretable method for lane topology reasoning based on lane geometric distance and lane query similarity, named TopoLogic.   This method mitigates the impact of endpoint shifts in geometric space, and introduces explicit similarity calculation in semantic space as a complement. By integrating results from both spaces, our methods provides more comprehensive information for lane topology.   Ultimately, our approach significantly outperforms the existing state-of-the-art methods on the mainstream benchmark OpenLane-V2 (23.9 v.s. 10.9 in TOP$_{ll}$ and 44.1 v.s. 39.8 in OLS on subset_A. Additionally, our proposed geometric distance topology reasoning method can be incorporated into well-trained models without re-training, significantly boost the performance of lane topology reasoning. The code is released at https://github.com/Franpin/TopoLogic.","sentences":["As an emerging task that integrates perception and reasoning, topology reasoning in autonomous driving scenes has recently garnered widespread attention.","However, existing work often emphasizes \"perception over reasoning\": they typically boost reasoning performance by enhancing the perception of lanes and directly adopt MLP to learn lane topology from lane query.","This paradigm overlooks the geometric features intrinsic to the lanes themselves and are prone to being influenced by inherent endpoint shifts in lane detection.   ","To tackle this issue, we propose an interpretable method for lane topology reasoning based on lane geometric distance and lane query similarity, named TopoLogic.   ","This method mitigates the impact of endpoint shifts in geometric space, and introduces explicit similarity calculation in semantic space as a complement.","By integrating results from both spaces, our methods provides more comprehensive information for lane topology.   ","Ultimately, our approach significantly outperforms the existing state-of-the-art methods on the mainstream benchmark OpenLane-V2 (23.9 v.s. 10.9 in TOP$_{ll}$ and 44.1 v.s. 39.8 in OLS on subset_A. Additionally, our proposed geometric distance topology reasoning method can be incorporated into well-trained models without re-training, significantly boost the performance of lane topology reasoning.","The code is released at https://github.com/Franpin/TopoLogic."],"url":"http://arxiv.org/abs/2405.14747v1","category":"cs.CV"}
{"created":"2024-05-23 16:14:16","title":"AnyLoss: Transforming Classification Metrics into Loss Functions","abstract":"Many evaluation metrics can be used to assess the performance of models in binary classification tasks. However, most of them are derived from a confusion matrix in a non-differentiable form, making it very difficult to generate a differentiable loss function that could directly optimize them. The lack of solutions to bridge this challenge not only hinders our ability to solve difficult tasks, such as imbalanced learning, but also requires the deployment of computationally expensive hyperparameter search processes in model selection. In this paper, we propose a general-purpose approach that transforms any confusion matrix-based metric into a loss function, \\textit{AnyLoss}, that is available in optimization processes. To this end, we use an approximation function to make a confusion matrix represented in a differentiable form, and this approach enables any confusion matrix-based metric to be directly used as a loss function. The mechanism of the approximation function is provided to ensure its operability and the differentiability of our loss functions is proved by suggesting their derivatives. We conduct extensive experiments under diverse neural networks with many datasets, and we demonstrate their general availability to target any confusion matrix-based metrics. Our method, especially, shows outstanding achievements in dealing with imbalanced datasets, and its competitive learning speed, compared to multiple baseline models, underscores its efficiency.","sentences":["Many evaluation metrics can be used to assess the performance of models in binary classification tasks.","However, most of them are derived from a confusion matrix in a non-differentiable form, making it very difficult to generate a differentiable loss function that could directly optimize them.","The lack of solutions to bridge this challenge not only hinders our ability to solve difficult tasks, such as imbalanced learning, but also requires the deployment of computationally expensive hyperparameter search processes in model selection.","In this paper, we propose a general-purpose approach that transforms any confusion matrix-based metric into a loss function, \\textit{AnyLoss}, that is available in optimization processes.","To this end, we use an approximation function to make a confusion matrix represented in a differentiable form, and this approach enables any confusion matrix-based metric to be directly used as a loss function.","The mechanism of the approximation function is provided to ensure its operability and the differentiability of our loss functions is proved by suggesting their derivatives.","We conduct extensive experiments under diverse neural networks with many datasets, and we demonstrate their general availability to target any confusion matrix-based metrics.","Our method, especially, shows outstanding achievements in dealing with imbalanced datasets, and its competitive learning speed, compared to multiple baseline models, underscores its efficiency."],"url":"http://arxiv.org/abs/2405.14745v1","category":"cs.LG"}
{"created":"2024-05-23 16:13:33","title":"Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View","abstract":"Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can LLM Agents leverage hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties. Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.","sentences":["Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM agents remains under-explored.","As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can LLM Agents leverage hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence?","In this paper, we probe the irrational behavior among contemporary LLM agents by melding practical social science experiments with theoretical insights.","Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases.","Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties.","Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents."],"url":"http://arxiv.org/abs/2405.14744v1","category":"cs.CY"}
{"created":"2024-05-23 16:12:33","title":"Iterative Causal Segmentation: Filling the Gap between Market Segmentation and Marketing Strategy","abstract":"The field of causal Machine Learning (ML) has made significant strides in recent years. Notable breakthroughs include methods such as meta learners (arXiv:1706.03461v6) and heterogeneous doubly robust estimators (arXiv:2004.14497) introduced in the last five years. Despite these advancements, the field still faces challenges, particularly in managing tightly coupled systems where both the causal treatment variable and a confounding covariate must serve as key decision-making indicators. This scenario is common in applications of causal ML for marketing, such as marketing segmentation and incremental marketing uplift. In this work, we present our formally proven algorithm, iterative causal segmentation, to address this issue.","sentences":["The field of causal Machine Learning (ML) has made significant strides in recent years.","Notable breakthroughs include methods such as meta learners (arXiv:1706.03461v6) and heterogeneous doubly robust estimators (arXiv:2004.14497) introduced in the last five years.","Despite these advancements, the field still faces challenges, particularly in managing tightly coupled systems where both the causal treatment variable and a confounding covariate must serve as key decision-making indicators.","This scenario is common in applications of causal ML for marketing, such as marketing segmentation and incremental marketing uplift.","In this work, we present our formally proven algorithm, iterative causal segmentation, to address this issue."],"url":"http://arxiv.org/abs/2405.14743v1","category":"cs.LG"}
{"created":"2024-05-23 16:08:04","title":"HC-GAE: The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning","abstract":"Graph Auto-Encoders (GAEs) are powerful tools for graph representation learning. In this paper, we develop a novel Hierarchical Cluster-based GAE (HC-GAE), that can learn effective structural characteristics for graph data analysis. To this end, during the encoding process, we commence by utilizing the hard node assignment to decompose a sample graph into a family of separated subgraphs. We compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. On the other hand, during the decoding process, we adopt the soft node assignment to reconstruct the original graph structure by expanding the coarsened nodes. By hierarchically performing the above compressing procedure during the decoding process as well as the expanding procedure during the decoding process, the proposed HC-GAE can effectively extract bidirectionally hierarchical structural features of the original sample graph. Furthermore, we re-design the loss function that can integrate the information from either the encoder or the decoder. Since the associated graph convolution operation of the proposed HC-GAE is restricted in each individual separated subgraph and cannot propagate the node information between different subgraphs, the proposed HC-GAE can significantly reduce the over-smoothing problem arising in the classical convolution-based GAEs. The proposed HC-GAE can generate effective representations for either node classification or graph classification, and the experiments demonstrate the effectiveness on real-world datasets.","sentences":["Graph Auto-Encoders (GAEs) are powerful tools for graph representation learning.","In this paper, we develop a novel Hierarchical Cluster-based GAE (HC-GAE), that can learn effective structural characteristics for graph data analysis.","To this end, during the encoding process, we commence by utilizing the hard node assignment to decompose a sample graph into a family of separated subgraphs.","We compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph.","On the other hand, during the decoding process, we adopt the soft node assignment to reconstruct the original graph structure by expanding the coarsened nodes.","By hierarchically performing the above compressing procedure during the decoding process as well as the expanding procedure during the decoding process, the proposed HC-GAE can effectively extract bidirectionally hierarchical structural features of the original sample graph.","Furthermore, we re-design the loss function that can integrate the information from either the encoder or the decoder.","Since the associated graph convolution operation of the proposed HC-GAE is restricted in each individual separated subgraph and cannot propagate the node information between different subgraphs, the proposed HC-GAE can significantly reduce the over-smoothing problem arising in the classical convolution-based GAEs.","The proposed HC-GAE can generate effective representations for either node classification or graph classification, and the experiments demonstrate the effectiveness on real-world datasets."],"url":"http://arxiv.org/abs/2405.14742v1","category":"cs.LG"}
{"created":"2024-05-23 16:05:10","title":"Bagging Improves Generalization Exponentially","abstract":"Bagging is a popular ensemble technique to improve the accuracy of machine learning models. It hinges on the well-established rationale that, by repeatedly retraining on resampled data, the aggregated model exhibits lower variance and hence higher stability, especially for discontinuous base learners. In this paper, we provide a new perspective on bagging: By suitably aggregating the base learners at the parametrization instead of the output level, bagging improves generalization performances exponentially, a strength that is significantly more powerful than variance reduction. More precisely, we show that for general stochastic optimization problems that suffer from slowly (i.e., polynomially) decaying generalization errors, bagging can effectively reduce these errors to an exponential decay. Moreover, this power of bagging is agnostic to the solution schemes, including common empirical risk minimization, distributionally robust optimization, and various regularizations. We demonstrate how bagging can substantially improve generalization performances in a range of examples involving heavy-tailed data that suffer from intrinsically slow rates.","sentences":["Bagging is a popular ensemble technique to improve the accuracy of machine learning models.","It hinges on the well-established rationale that, by repeatedly retraining on resampled data, the aggregated model exhibits lower variance and hence higher stability, especially for discontinuous base learners.","In this paper, we provide a new perspective on bagging: By suitably aggregating the base learners at the parametrization instead of the output level, bagging improves generalization performances exponentially, a strength that is significantly more powerful than variance reduction.","More precisely, we show that for general stochastic optimization problems that suffer from slowly (i.e., polynomially) decaying generalization errors, bagging can effectively reduce these errors to an exponential decay.","Moreover, this power of bagging is agnostic to the solution schemes, including common empirical risk minimization, distributionally robust optimization, and various regularizations.","We demonstrate how bagging can substantially improve generalization performances in a range of examples involving heavy-tailed data that suffer from intrinsically slow rates."],"url":"http://arxiv.org/abs/2405.14741v1","category":"math.OC"}
{"created":"2024-05-23 16:04:42","title":"FLoRA: Low-Rank Core Space for N-dimension","abstract":"Adapting pre-trained foundation models for various downstream tasks has been prevalent in artificial intelligence. Due to the vast number of tasks and high costs, adjusting all parameters becomes unfeasible. To mitigate this, several fine-tuning techniques have been developed to update the pre-trained model weights in a more resource-efficient manner, such as through low-rank adjustments. Yet, almost all of these methods focus on linear weights, neglecting the intricacies of parameter spaces in higher dimensions like 4D. Alternatively, some methods can be adapted for high-dimensional parameter space by compressing changes in the original space into two dimensions and then employing low-rank matrix decomposition. However, these approaches destructs the structural integrity of the involved high-dimensional spaces. To tackle the diversity of dimensional spaces across different foundation models and provide a more precise representation of the changes within these spaces, this paper introduces a generalized parameter-efficient fine-tuning framework, FLoRA, designed for various dimensional parameter space. Specifically, utilizing Tucker decomposition, FLoRA asserts that changes in each dimensional parameter space are based on a low-rank core space which maintains the consistent topological structure with the original space. It then models the changes through this core space alongside corresponding weights to reconstruct alterations in the original space. FLoRA effectively preserves the structural integrity of the change of original N-dimensional parameter space, meanwhile decomposes it via low-rank tensor decomposition. Extensive experiments on computer vision, natural language processing and multi-modal tasks validate FLoRA's effectiveness. Codes are available at https://github.com/SJTU-DeepVisionLab/FLoRA.","sentences":["Adapting pre-trained foundation models for various downstream tasks has been prevalent in artificial intelligence.","Due to the vast number of tasks and high costs, adjusting all parameters becomes unfeasible.","To mitigate this, several fine-tuning techniques have been developed to update the pre-trained model weights in a more resource-efficient manner, such as through low-rank adjustments.","Yet, almost all of these methods focus on linear weights, neglecting the intricacies of parameter spaces in higher dimensions like 4D. Alternatively, some methods can be adapted for high-dimensional parameter space by compressing changes in the original space into two dimensions and then employing low-rank matrix decomposition.","However, these approaches destructs the structural integrity of the involved high-dimensional spaces.","To tackle the diversity of dimensional spaces across different foundation models and provide a more precise representation of the changes within these spaces, this paper introduces a generalized parameter-efficient fine-tuning framework, FLoRA, designed for various dimensional parameter space.","Specifically, utilizing Tucker decomposition, FLoRA asserts that changes in each dimensional parameter space are based on a low-rank core space which maintains the consistent topological structure with the original space.","It then models the changes through this core space alongside corresponding weights to reconstruct alterations in the original space.","FLoRA effectively preserves the structural integrity of the change of original N-dimensional parameter space, meanwhile decomposes it via low-rank tensor decomposition.","Extensive experiments on computer vision, natural language processing and multi-modal tasks validate FLoRA's effectiveness.","Codes are available at https://github.com/SJTU-DeepVisionLab/FLoRA."],"url":"http://arxiv.org/abs/2405.14739v1","category":"cs.CV"}
{"created":"2024-05-23 16:02:30","title":"GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost","abstract":"Recent advancements in dataset distillation have demonstrated the significant benefits of employing soft labels generated by pre-trained teacher models. In this paper, we introduce a novel perspective by emphasizing the full utilization of labels. We first conduct a comprehensive comparison of various loss functions for soft label utilization in dataset distillation, revealing that the model trained on the synthetic dataset exhibits high sensitivity to the choice of loss function for soft label utilization. This finding highlights the necessity of a universal loss function for training models on synthetic datasets. Building on these insights, we introduce an extremely simple yet surprisingly effective plug-and-play approach, GIFT, which encompasses soft label refinement and a cosine similarity-based loss function to efficiently leverage full label information. Extensive experiments demonstrate that GIFT consistently enhances the state-of-the-art dataset distillation methods across various scales datasets without incurring additional computational costs. For instance, on ImageNet-1K with IPC = 10, GIFT improves the SOTA method RDED by 3.9% and 1.8% on ConvNet and ResNet-18, respectively. Code: https://github.com/LINs-lab/GIFT.","sentences":["Recent advancements in dataset distillation have demonstrated the significant benefits of employing soft labels generated by pre-trained teacher models.","In this paper, we introduce a novel perspective by emphasizing the full utilization of labels.","We first conduct a comprehensive comparison of various loss functions for soft label utilization in dataset distillation, revealing that the model trained on the synthetic dataset exhibits high sensitivity to the choice of loss function for soft label utilization.","This finding highlights the necessity of a universal loss function for training models on synthetic datasets.","Building on these insights, we introduce an extremely simple yet surprisingly effective plug-and-play approach, GIFT, which encompasses soft label refinement and a cosine similarity-based loss function to efficiently leverage full label information.","Extensive experiments demonstrate that GIFT consistently enhances the state-of-the-art dataset distillation methods across various scales datasets without incurring additional computational costs.","For instance, on ImageNet-1K with IPC = 10, GIFT improves the SOTA method RDED by 3.9% and 1.8% on ConvNet and ResNet-18, respectively.","Code: https://github.com/LINs-lab/GIFT."],"url":"http://arxiv.org/abs/2405.14736v1","category":"cs.CV"}
{"created":"2024-05-23 16:01:48","title":"Generalized all-optical complex exponential operator","abstract":"Euler's formula, an extraordinary mathematical formula, establishes a vital link between complex-valued operations and trigonometric functions, finding widespread application in various fields. With the end of Moore's Law, electronic computing methods are encountering developmental bottlenecks. With its enviable potential, optical computing has successfully achieved high-speed operation of designed complex numbers. However, the challenge of processing and manipulating arbitrary complex numbers persists. This study introduces a generalized complex exponential operator (GCEO), utilizing a diffractive optical neural network (DONN) for the computation of the complex exponential through Euler's formula. Experiments validate a series of complex exponential calculations using the GCEO. The GCEO has demonstrated generalizability and can compute inputs of any precision within an appropriate error margin. The proposed operator highlights the immense potential of DONN in optical computation and is poised to significantly contribute to the development of computational methods for optoelectronic integration.","sentences":["Euler's formula, an extraordinary mathematical formula, establishes a vital link between complex-valued operations and trigonometric functions, finding widespread application in various fields.","With the end of Moore's Law, electronic computing methods are encountering developmental bottlenecks.","With its enviable potential, optical computing has successfully achieved high-speed operation of designed complex numbers.","However, the challenge of processing and manipulating arbitrary complex numbers persists.","This study introduces a generalized complex exponential operator (GCEO), utilizing a diffractive optical neural network (DONN) for the computation of the complex exponential through Euler's formula.","Experiments validate a series of complex exponential calculations using the GCEO.","The GCEO has demonstrated generalizability and can compute inputs of any precision within an appropriate error margin.","The proposed operator highlights the immense potential of DONN in optical computation and is poised to significantly contribute to the development of computational methods for optoelectronic integration."],"url":"http://arxiv.org/abs/2405.14735v1","category":"physics.optics"}
{"created":"2024-05-23 16:01:46","title":"SimPO: Simple Preference Optimization with a Reference-Free Reward","abstract":"Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further enhancing the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models like Mistral and Llama3. We evaluated on extensive instruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the recent challenging Arena-Hard benchmark. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44.7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 33.8 win rate on Arena-Hard -- making it the strongest 8B open-source model.","sentences":["Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability.","In this work, we propose SimPO, a simpler yet more effective approach.","The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward.","This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient.","Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further enhancing the algorithm's performance.","We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models like Mistral and Llama3.","We evaluated on extensive instruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the recent challenging Arena-Hard benchmark.","Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length.","Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard.","Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44.7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 33.8 win rate on Arena-Hard -- making it the strongest 8B open-source model."],"url":"http://arxiv.org/abs/2405.14734v1","category":"cs.CL"}
{"created":"2024-05-23 15:55:38","title":"Intervention and Conditioning in Causal Bayesian Networks","abstract":"Causal models are crucial for understanding complex systems and identifying causal relationships among variables. Even though causal models are extremely popular, conditional probability calculation of formulas involving interventions pose significant challenges. In case of Causal Bayesian Networks (CBNs), Pearl assumes autonomy of mechanisms that determine interventions to calculate a range of probabilities. We show that by making simple yet often realistic independence assumptions, it is possible to uniquely estimate the probability of an interventional formula (including the well-studied notions of probability of sufficiency and necessity). We discuss when these assumptions are appropriate. Importantly, in many cases of interest, when the assumptions are appropriate, these probability estimates can be evaluated using observational data, which carries immense significance in scenarios where conducting experiments is impractical or unfeasible.","sentences":["Causal models are crucial for understanding complex systems and identifying causal relationships among variables.","Even though causal models are extremely popular, conditional probability calculation of formulas involving interventions pose significant challenges.","In case of Causal Bayesian Networks (CBNs), Pearl assumes autonomy of mechanisms that determine interventions to calculate a range of probabilities.","We show that by making simple yet often realistic independence assumptions, it is possible to uniquely estimate the probability of an interventional formula (including the well-studied notions of probability of sufficiency and necessity).","We discuss when these assumptions are appropriate.","Importantly, in many cases of interest, when the assumptions are appropriate, these probability estimates can be evaluated using observational data, which carries immense significance in scenarios where conducting experiments is impractical or unfeasible."],"url":"http://arxiv.org/abs/2405.14728v1","category":"cs.AI"}
{"created":"2024-05-23 15:51:24","title":"CAPE: Context-Adaptive Positional Encoding for Length Extrapolation","abstract":"Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be context-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Context-Adaptive Positional Encoding (CAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that CAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.","sentences":["Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization.","Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences.","However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility.","Hence, we expect that the desired positional encoding should be context-adaptive and can be dynamically adjusted with the given attention.","In this paper, we propose a Context-Adaptive Positional Encoding (CAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors.","Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that CAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant.","The model visualization suggests that our model can keep both local and anti-local information.","Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method."],"url":"http://arxiv.org/abs/2405.14722v1","category":"cs.CL"}
{"created":"2024-05-23 15:48:46","title":"Decision-Focused Forecasting: Decision Losses for Multistage Optimisation","abstract":"Decision-focused learning has emerged as a promising approach for decision making under uncertainty by training the upstream predictive aspect of the pipeline with respect to the quality of the downstream decisions. Most existing work has focused on single stage problems. Many real-world decision problems are more appropriately modelled using multistage optimisation as contextual information such as prices or demand is revealed over time and decisions now have a bearing on future decisions. We propose decision-focused forecasting, a multiple-implicitlayer model which in its training accounts for the intertemporal decision effects of forecasts using differentiable optimisation. The recursive model reflects a fully differentiable multistage optimisation approach. We present an analysis of the gradients produced by this model showing the adjustments made to account for the state-path caused by forecasting. We demonstrate an application of the model to an energy storage arbitrage task and report that our model outperforms existing approaches.","sentences":["Decision-focused learning has emerged as a promising approach for decision making under uncertainty by training the upstream predictive aspect of the pipeline with respect to the quality of the downstream decisions.","Most existing work has focused on single stage problems.","Many real-world decision problems are more appropriately modelled using multistage optimisation as contextual information such as prices or demand is revealed over time and decisions now have a bearing on future decisions.","We propose decision-focused forecasting, a multiple-implicitlayer model which in its training accounts for the intertemporal decision effects of forecasts using differentiable optimisation.","The recursive model reflects a fully differentiable multistage optimisation approach.","We present an analysis of the gradients produced by this model showing the adjustments made to account for the state-path caused by forecasting.","We demonstrate an application of the model to an energy storage arbitrage task and report that our model outperforms existing approaches."],"url":"http://arxiv.org/abs/2405.14719v1","category":"math.OC"}
{"created":"2024-05-23 15:48:38","title":"StyleX: A Trainable Metric for X-ray Style Distances","abstract":"The progression of X-ray technology introduces diverse image styles that need to be adapted to the preferences of radiologists. To support this task, we introduce a novel deep learning-based metric that quantifies style differences of non-matching image pairs. At the heart of our metric is an encoder capable of generating X-ray image style representations. This encoder is trained without any explicit knowledge of style distances by exploiting Simple Siamese learning. During inference, the style representations produced by the encoder are used to calculate a distance metric for non-matching image pairs. Our experiments investigate the proposed concept for a disclosed reproducible and a proprietary image processing pipeline along two dimensions: First, we use a t-distributed stochastic neighbor embedding (t-SNE) analysis to illustrate that the encoder outputs provide meaningful and discriminative style representations. Second, the proposed metric calculated from the encoder outputs is shown to quantify style distances for non-matching pairs in good alignment with the human perception. These results confirm that our proposed method is a promising technique to quantify style differences, which can be used for guided style selection as well as automatic optimization of image pipeline parameters.","sentences":["The progression of X-ray technology introduces diverse image styles that need to be adapted to the preferences of radiologists.","To support this task, we introduce a novel deep learning-based metric that quantifies style differences of non-matching image pairs.","At the heart of our metric is an encoder capable of generating X-ray image style representations.","This encoder is trained without any explicit knowledge of style distances by exploiting Simple Siamese learning.","During inference, the style representations produced by the encoder are used to calculate a distance metric for non-matching image pairs.","Our experiments investigate the proposed concept for a disclosed reproducible and a proprietary image processing pipeline along two dimensions:","First, we use a t-distributed stochastic neighbor embedding (t-SNE) analysis to illustrate that the encoder outputs provide meaningful and discriminative style representations.","Second, the proposed metric calculated from the encoder outputs is shown to quantify style distances for non-matching pairs in good alignment with the human perception.","These results confirm that our proposed method is a promising technique to quantify style differences, which can be used for guided style selection as well as automatic optimization of image pipeline parameters."],"url":"http://arxiv.org/abs/2405.14718v1","category":"cs.CV"}
{"created":"2024-05-23 15:46:42","title":"HTN-Based Tutors: A New Intelligent Tutoring Framework Based on Hierarchical Task Networks","abstract":"Intelligent tutors have shown success in delivering a personalized and adaptive learning experience. However, there exist challenges regarding the granularity of knowledge in existing frameworks and the resulting instructions they can provide. To address these issues, we propose HTN-based tutors, a new intelligent tutoring framework that represents expert models using Hierarchical Task Networks (HTNs). Like other tutoring frameworks, it allows flexible encoding of different problem-solving strategies while providing the additional benefit of a hierarchical knowledge organization. We leverage the latter to create tutors that can adapt the granularity of their scaffolding. This organization also aligns well with the compositional nature of skills.","sentences":["Intelligent tutors have shown success in delivering a personalized and adaptive learning experience.","However, there exist challenges regarding the granularity of knowledge in existing frameworks and the resulting instructions they can provide.","To address these issues, we propose HTN-based tutors, a new intelligent tutoring framework that represents expert models using Hierarchical Task Networks (HTNs).","Like other tutoring frameworks, it allows flexible encoding of different problem-solving strategies while providing the additional benefit of a hierarchical knowledge organization.","We leverage the latter to create tutors that can adapt the granularity of their scaffolding.","This organization also aligns well with the compositional nature of skills."],"url":"http://arxiv.org/abs/2405.14716v1","category":"cs.AI"}
{"created":"2024-05-23 15:46:35","title":"Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models","abstract":"Modern retrieval systems often struggle with upgrading to new and more powerful models due to the incompatibility of embeddings between the old and new models. This necessitates a costly process known as backfilling, which involves re-computing the embeddings for a large number of data samples. In vision, Backward-compatible Training (BT) has been proposed to ensure that the new model aligns with the old model's embeddings. This paper extends the concept of vision-only BT to the field of cross-modal retrieval, marking the first attempt to address Cross-modal BT (XBT). Our goal is to achieve backward-compatibility between Vision-Language Pretraining (VLP) models, such as CLIP, for the cross-modal retrieval task. To address XBT challenges, we propose an efficient solution: a projection module that maps the new model's embeddings to those of the old model. This module, pretrained solely with text data, significantly reduces the number of image-text pairs required for XBT learning, and, once it is pretrained, it avoids using the old model during training. Furthermore, we utilize parameter-efficient training strategies that improve efficiency and preserve the off-the-shelf new model's knowledge by avoiding any modifications. Experimental results on cross-modal retrieval datasets demonstrate the effectiveness of XBT and its potential to enable backfill-free upgrades when a new VLP model emerges.","sentences":["Modern retrieval systems often struggle with upgrading to new and more powerful models due to the incompatibility of embeddings between the old and new models.","This necessitates a costly process known as backfilling, which involves re-computing the embeddings for a large number of data samples.","In vision, Backward-compatible Training (BT) has been proposed to ensure that the new model aligns with the old model's embeddings.","This paper extends the concept of vision-only BT to the field of cross-modal retrieval, marking the first attempt to address Cross-modal BT (XBT).","Our goal is to achieve backward-compatibility between Vision-Language Pretraining (VLP) models, such as CLIP, for the cross-modal retrieval task.","To address XBT challenges, we propose an efficient solution: a projection module that maps the new model's embeddings to those of the old model.","This module, pretrained solely with text data, significantly reduces the number of image-text pairs required for XBT learning, and, once it is pretrained, it avoids using the old model during training.","Furthermore, we utilize parameter-efficient training strategies that improve efficiency and preserve the off-the-shelf new model's knowledge by avoiding any modifications.","Experimental results on cross-modal retrieval datasets demonstrate the effectiveness of XBT and its potential to enable backfill-free upgrades when a new VLP model emerges."],"url":"http://arxiv.org/abs/2405.14715v1","category":"cs.CV"}
{"created":"2024-05-23 15:46:10","title":"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces","abstract":"Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.","sentences":["Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills.","Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces.","To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles.","Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs.","However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level.","The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface.","A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design.","Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption."],"url":"http://arxiv.org/abs/2405.14713v1","category":"cs.HC"}
{"created":"2024-05-23 15:45:43","title":"Evolution and learning in differentiable robots","abstract":"The automatic design of robots has existed for 30 years but has been constricted by serial non-differentiable design evaluations, premature convergence to simple bodies or clumsy behaviors, and a lack of sim2real transfer to physical machines. Thus, here we employ massively-parallel differentiable simulations to rapidly and simultaneously optimize individual neural control of behavior across a large population of candidate body plans and return a fitness score for each design based on the performance of its fully optimized behavior. Non-differentiable changes to the mechanical structure of each robot in the population -- mutations that rearrange, combine, add, or remove body parts -- were applied by a genetic algorithm in an outer loop of search, generating a continuous flow of novel morphologies with highly-coordinated and graceful behaviors honed by gradient descent. This enabled the exploration of several orders-of-magnitude more designs than all previous methods, despite the fact that robots here have the potential to be much more complex, in terms of number of independent motors, than those in prior studies. We found that evolution reliably produces ``increasingly differentiable'' robots: body plans that smooth the loss landscape in which learning operates and thereby provide better training paths toward performant behaviors. Finally, one of the highly differentiable morphologies discovered in simulation was realized as a physical robot and shown to retain its optimized behavior. This provides a cyberphysical platform to investigate the relationship between evolution and learning in biological systems and broadens our understanding of how a robot's physical structure can influence the ability to train policies for it. Videos and code at https://sites.google.com/view/eldir.","sentences":["The automatic design of robots has existed for 30 years but has been constricted by serial non-differentiable design evaluations, premature convergence to simple bodies or clumsy behaviors, and a lack of sim2real transfer to physical machines.","Thus, here we employ massively-parallel differentiable simulations to rapidly and simultaneously optimize individual neural control of behavior across a large population of candidate body plans and return a fitness score for each design based on the performance of its fully optimized behavior.","Non-differentiable changes to the mechanical structure of each robot in the population -- mutations that rearrange, combine, add, or remove body parts -- were applied by a genetic algorithm in an outer loop of search, generating a continuous flow of novel morphologies with highly-coordinated and graceful behaviors honed by gradient descent.","This enabled the exploration of several orders-of-magnitude more designs than all previous methods, despite the fact that robots here have the potential to be much more complex, in terms of number of independent motors, than those in prior studies.","We found that evolution reliably produces ``increasingly differentiable'' robots: body plans that smooth the loss landscape in which learning operates and thereby provide better training paths toward performant behaviors.","Finally, one of the highly differentiable morphologies discovered in simulation was realized as a physical robot and shown to retain its optimized behavior.","This provides a cyberphysical platform to investigate the relationship between evolution and learning in biological systems and broadens our understanding of how a robot's physical structure can influence the ability to train policies for it.","Videos and code at https://sites.google.com/view/eldir."],"url":"http://arxiv.org/abs/2405.14712v1","category":"cs.RO"}
{"created":"2024-05-23 15:43:05","title":"Sums of four polygonal numbers: precise formulas","abstract":"In this paper we give unified formulas for the numbers of representations of positive integers as sums of four generalized $m$-gonal numbers, and as restricted sums of four squares under a linear condition, respectively. These formulas are given as $\\mathbb{Z}$-linear combinations of Hurwitz class numbers. As applications, we prove several Zhi-Wei Sun's conjectures. As by-products, we obtain formulas for expressing the Fourier coefficients of $\\vartheta(\\tau,z)^4$, $\\eta(\\tau)^{12}$, $\\eta(\\tau)^4$ and $\\eta(\\tau)^8\\eta(2\\tau)^8$ in terms of the Hurwitz class numbers, respectively. The proof is based on the theory of Jacobi forms.","sentences":["In this paper we give unified formulas for the numbers of representations of positive integers as sums of four generalized $m$-gonal numbers, and as restricted sums of four squares under a linear condition, respectively.","These formulas are given as $\\mathbb{Z}$-linear combinations of Hurwitz class numbers.","As applications, we prove several Zhi-Wei Sun's conjectures.","As by-products, we obtain formulas for expressing the Fourier coefficients of $\\vartheta(\\tau,z)^4$, $\\eta(\\tau)^{12}$, $\\eta(\\tau)^4$ and $\\eta(\\tau)^8\\eta(2\\tau)^8$ in terms of the Hurwitz class numbers, respectively.","The proof is based on the theory of Jacobi forms."],"url":"http://arxiv.org/abs/2405.14710v1","category":"math.NT"}
{"created":"2024-05-23 15:42:34","title":"OpFlowTalker: Realistic and Natural Talking Face Generation via Optical Flow Guidance","abstract":"Creating realistic, natural, and lip-readable talking face videos remains a formidable challenge. Previous research primarily concentrated on generating and aligning single-frame images while overlooking the smoothness of frame-to-frame transitions and temporal dependencies. This often compromised visual quality and effects in practical settings, particularly when handling complex facial data and audio content, which frequently led to semantically incongruent visual illusions. Specifically, synthesized videos commonly featured disorganized lip movements, making them difficult to understand and recognize. To overcome these limitations, this paper introduces the application of optical flow to guide facial image generation, enhancing inter-frame continuity and semantic consistency. We propose \"OpFlowTalker\", a novel approach that utilizes predicted optical flow changes from audio inputs rather than direct image predictions. This method smooths image transitions and aligns changes with semantic content. Moreover, it employs a sequence fusion technique to replace the independent generation of single frames, thus preserving contextual information and maintaining temporal coherence. We also developed an optical flow synchronization module that regulates both full-face and lip movements, optimizing visual synthesis by balancing regional dynamics. Furthermore, we introduce a Visual Text Consistency Score (VTCS) that accurately measures lip-readability in synthesized videos. Extensive empirical evidence validates the effectiveness of our approach.","sentences":["Creating realistic, natural, and lip-readable talking face videos remains a formidable challenge.","Previous research primarily concentrated on generating and aligning single-frame images while overlooking the smoothness of frame-to-frame transitions and temporal dependencies.","This often compromised visual quality and effects in practical settings, particularly when handling complex facial data and audio content, which frequently led to semantically incongruent visual illusions.","Specifically, synthesized videos commonly featured disorganized lip movements, making them difficult to understand and recognize.","To overcome these limitations, this paper introduces the application of optical flow to guide facial image generation, enhancing inter-frame continuity and semantic consistency.","We propose \"OpFlowTalker\", a novel approach that utilizes predicted optical flow changes from audio inputs rather than direct image predictions.","This method smooths image transitions and aligns changes with semantic content.","Moreover, it employs a sequence fusion technique to replace the independent generation of single frames, thus preserving contextual information and maintaining temporal coherence.","We also developed an optical flow synchronization module that regulates both full-face and lip movements, optimizing visual synthesis by balancing regional dynamics.","Furthermore, we introduce a Visual Text Consistency Score (VTCS) that accurately measures lip-readability in synthesized videos.","Extensive empirical evidence validates the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.14709v1","category":"cs.CV"}
{"created":"2024-05-23 15:41:35","title":"Artificial Intelligence (AI) in Legal Data Mining","abstract":"Despite the availability of vast amounts of data, legal data is often unstructured, making it difficult even for law practitioners to ingest and comprehend the same. It is important to organise the legal information in a way that is useful for practitioners and downstream automation tasks. The word ontology was used by Greek philosophers to discuss concepts of existence, being, becoming and reality. Today, scientists use this term to describe the relation between concepts, data, and entities. A great example for a working ontology was developed by Dhani and Bhatt. This ontology deals with Indian court cases on intellectual property rights (IPR) The future of legal ontologies is likely to be handled by computer experts and legal experts alike.","sentences":["Despite the availability of vast amounts of data, legal data is often unstructured, making it difficult even for law practitioners to ingest and comprehend the same.","It is important to organise the legal information in a way that is useful for practitioners and downstream automation tasks.","The word ontology was used by Greek philosophers to discuss concepts of existence, being, becoming and reality.","Today, scientists use this term to describe the relation between concepts, data, and entities.","A great example for a working ontology was developed by Dhani and Bhatt.","This ontology deals with Indian court cases on intellectual property rights (IPR)","The future of legal ontologies is likely to be handled by computer experts and legal experts alike."],"url":"http://arxiv.org/abs/2405.14707v1","category":"cs.AI"}
{"created":"2024-05-23 15:40:37","title":"Loschmidt echo, emerging dual unitarity and scaling of generalized temporal entropies after quenches to the critical point","abstract":"We discuss how all relevant objects involved in the calculation of the Loschmidt echo of a product state after a quench to a conformal invariant critical point can be predicted by using conformal field theories. We check such prediction with tensor networks finding excellent agreement. In particular we are able to predict and confirm that in such an out-of-equilibrium scenario, we observe an emerging dual-unitarity of the evolution. We also show how to extract the universal information of the underlying CFT from such out-of-equilibrium protocol, including, the central charge, the operator content, and the generalized temporal entropies. In particular we show that using state-of-the art tensor networks algorithms such calculations only require resources that increase polynomially with the duration of the quench, thus providing an example of numerically efficiently solvable out-of-equilibrium scenario.","sentences":["We discuss how all relevant objects involved in the calculation of the Loschmidt echo of a product state after a quench to a conformal invariant critical point can be predicted by using conformal field theories.","We check such prediction with tensor networks finding excellent agreement.","In particular we are able to predict and confirm that in such an out-of-equilibrium scenario, we observe an emerging dual-unitarity of the evolution.","We also show how to extract the universal information of the underlying CFT from such out-of-equilibrium protocol, including, the central charge, the operator content, and the generalized temporal entropies.","In particular we show that using state-of-the art tensor networks algorithms such calculations only require resources that increase polynomially with the duration of the quench, thus providing an example of numerically efficiently solvable out-of-equilibrium scenario."],"url":"http://arxiv.org/abs/2405.14706v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-23 15:39:43","title":"Learning Multi-dimensional Human Preference for Text-to-Image Generation","abstract":"Current metrics for text-to-image models typically rely on statistical metrics which inadequately represent the real preference of humans. Although recent work attempts to learn these preferences via human annotated images, they reduce the rich tapestry of human preference to a single overall score. However, the preference results vary when humans evaluate images with different aspects. Therefore, to learn the multi-dimensional human preferences, we propose the Multi-dimensional Preference Score (MPS), the first multi-dimensional preference scoring model for the evaluation of text-to-image models. The MPS introduces the preference condition module upon CLIP model to learn these diverse preferences. It is trained based on our Multi-dimensional Human Preference (MHP) Dataset, which comprises 918,315 human preference choices across four dimensions (i.e., aesthetics, semantic alignment, detail quality and overall assessment) on 607,541 images. The images are generated by a wide range of latest text-to-image models. The MPS outperforms existing scoring methods across 3 datasets in 4 dimensions, enabling it a promising metric for evaluating and improving text-to-image generation.","sentences":["Current metrics for text-to-image models typically rely on statistical metrics which inadequately represent the real preference of humans.","Although recent work attempts to learn these preferences via human annotated images, they reduce the rich tapestry of human preference to a single overall score.","However, the preference results vary when humans evaluate images with different aspects.","Therefore, to learn the multi-dimensional human preferences, we propose the Multi-dimensional Preference Score (MPS), the first multi-dimensional preference scoring model for the evaluation of text-to-image models.","The MPS introduces the preference condition module upon CLIP model to learn these diverse preferences.","It is trained based on our Multi-dimensional Human Preference (MHP) Dataset, which comprises 918,315 human preference choices across four dimensions (i.e., aesthetics, semantic alignment, detail quality and overall assessment) on 607,541 images.","The images are generated by a wide range of latest text-to-image models.","The MPS outperforms existing scoring methods across 3 datasets in 4 dimensions, enabling it a promising metric for evaluating and improving text-to-image generation."],"url":"http://arxiv.org/abs/2405.14705v1","category":"cs.CV"}
{"created":"2024-05-23 15:37:06","title":"G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models","abstract":"Worldwide geolocalization aims to locate the precise location at the coordinate level of photos taken anywhere on the Earth. It is very challenging due to 1) the difficulty of capturing subtle location-aware visual semantics, and 2) the heterogeneous geographical distribution of image data. As a result, existing studies have clear limitations when scaled to a worldwide context. They may easily confuse distant images with similar visual contents, or cannot adapt to various locations worldwide with different amounts of relevant data. To resolve these limitations, we propose G3, a novel framework based on Retrieval-Augmented Generation (RAG). In particular, G3 consists of three steps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to optimize both retrieval and generation phases of worldwide geolocalization. During Geo-alignment, our solution jointly learns expressive multi-modal representations for images, GPS and textual descriptions, which allows us to capture location-aware semantics for retrieving nearby images for a given query. During Geo-diversification, we leverage a prompt ensembling method that is robust to inconsistent retrieval performance for different image queries. Finally, we combine both retrieved and generated GPS candidates in Geo-verification for location prediction. Experiments on two well-established datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other state-of-the-art methods.","sentences":["Worldwide geolocalization aims to locate the precise location at the coordinate level of photos taken anywhere on the Earth.","It is very challenging due to 1) the difficulty of capturing subtle location-aware visual semantics, and 2) the heterogeneous geographical distribution of image data.","As a result, existing studies have clear limitations when scaled to a worldwide context.","They may easily confuse distant images with similar visual contents, or cannot adapt to various locations worldwide with different amounts of relevant data.","To resolve these limitations, we propose G3, a novel framework based on Retrieval-Augmented Generation (RAG).","In particular, G3 consists of three steps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to optimize both retrieval and generation phases of worldwide geolocalization.","During Geo-alignment, our solution jointly learns expressive multi-modal representations for images, GPS and textual descriptions, which allows us to capture location-aware semantics for retrieving nearby images for a given query.","During Geo-diversification, we leverage a prompt ensembling method that is robust to inconsistent retrieval performance for different image queries.","Finally, we combine both retrieved and generated GPS candidates in Geo-verification for location prediction.","Experiments on two well-established datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.14702v1","category":"cs.CV"}
{"created":"2024-05-23 15:35:48","title":"High Fidelity Scene Text Synthesis","abstract":"Scene text synthesis involves rendering specified texts onto arbitrary images. Current methods typically formulate this task in an end-to-end manner but lack effective character-level guidance during training. Besides, their text encoders, pre-trained on a single font type, struggle to adapt to the diverse font styles encountered in practical applications. Consequently, these methods suffer from character distortion, repetition, and absence, particularly in polystylistic scenarios. To this end, this paper proposes DreamText for high-fidelity scene text synthesis. Our key idea is to reconstruct the diffusion training process, introducing more refined guidance tailored to this task, to expose and rectify the model's attention at the character level and strengthen its learning of text regions. This transformation poses a hybrid optimization challenge, involving both discrete and continuous variables. To effectively tackle this challenge, we employ a heuristic alternate optimization strategy. Meanwhile, we jointly train the text encoder and generator to comprehensively learn and utilize the diverse font present in the training dataset. This joint training is seamlessly integrated into the alternate optimization process, fostering a synergistic relationship between learning character embedding and re-estimating character attention. Specifically, in each step, we first encode potential character-generated position information from cross-attention maps into latent character masks. These masks are then utilized to update the representation of specific characters in the current step, which, in turn, enables the generator to correct the character's attention in the subsequent steps. Both qualitative and quantitative results demonstrate the superiority of our method to the state of the art.","sentences":["Scene text synthesis involves rendering specified texts onto arbitrary images.","Current methods typically formulate this task in an end-to-end manner but lack effective character-level guidance during training.","Besides, their text encoders, pre-trained on a single font type, struggle to adapt to the diverse font styles encountered in practical applications.","Consequently, these methods suffer from character distortion, repetition, and absence, particularly in polystylistic scenarios.","To this end, this paper proposes DreamText for high-fidelity scene text synthesis.","Our key idea is to reconstruct the diffusion training process, introducing more refined guidance tailored to this task, to expose and rectify the model's attention at the character level and strengthen its learning of text regions.","This transformation poses a hybrid optimization challenge, involving both discrete and continuous variables.","To effectively tackle this challenge, we employ a heuristic alternate optimization strategy.","Meanwhile, we jointly train the text encoder and generator to comprehensively learn and utilize the diverse font present in the training dataset.","This joint training is seamlessly integrated into the alternate optimization process, fostering a synergistic relationship between learning character embedding and re-estimating character attention.","Specifically, in each step, we first encode potential character-generated position information from cross-attention maps into latent character masks.","These masks are then utilized to update the representation of specific characters in the current step, which, in turn, enables the generator to correct the character's attention in the subsequent steps.","Both qualitative and quantitative results demonstrate the superiority of our method to the state of the art."],"url":"http://arxiv.org/abs/2405.14701v1","category":"cs.CV"}
{"created":"2024-05-23 15:31:18","title":"A Declarative System for Optimizing AI Workloads","abstract":"Modern AI models provide the key to a long-standing dream: processing analytical queries about almost any kind of data. Until recently, it was difficult and expensive to extract facts from company documents, data from scientific papers, or insights from image and video corpora. Today's models can accomplish these tasks with high accuracy. However, a programmer who wants to answer a substantive AI-powered query must orchestrate large numbers of models, prompts, and data operations. For even a single query, the programmer has to make a vast number of decisions such as the choice of model, the right inference method, the most cost-effective inference hardware, the ideal prompt design, and so on. The optimal set of decisions can change as the query changes and as the rapidly-evolving technical landscape shifts. In this paper we present Palimpzest, a system that enables anyone to process AI-powered analytical queries simply by defining them in a declarative language. The system uses its cost optimization framework -- which explores the search space of AI models, prompting techniques, and related foundation model optimizations -- to implement the query with the best trade-offs between runtime, financial cost, and output data quality. We describe the workload of AI-powered analytics tasks, the optimization methods that Palimpzest uses, and the prototype system itself. We evaluate Palimpzest on tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching. We show that even our simple prototype offers a range of appealing plans, including one that is 3.3x faster, 2.9x cheaper, and offers better data quality than the baseline method. With parallelism enabled, Palimpzest can produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a single-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the baseline. These require no additional work by the user.","sentences":["Modern AI models provide the key to a long-standing dream: processing analytical queries about almost any kind of data.","Until recently, it was difficult and expensive to extract facts from company documents, data from scientific papers, or insights from image and video corpora.","Today's models can accomplish these tasks with high accuracy.","However, a programmer who wants to answer a substantive AI-powered query must orchestrate large numbers of models, prompts, and data operations.","For even a single query, the programmer has to make a vast number of decisions such as the choice of model, the right inference method, the most cost-effective inference hardware, the ideal prompt design, and so on.","The optimal set of decisions can change as the query changes and as the rapidly-evolving technical landscape shifts.","In this paper we present Palimpzest, a system that enables anyone to process AI-powered analytical queries simply by defining them in a declarative language.","The system uses its cost optimization framework -- which explores the search space of AI models, prompting techniques, and related foundation model optimizations -- to implement the query with the best trade-offs between runtime, financial cost, and output data quality.","We describe the workload of AI-powered analytics tasks, the optimization methods that Palimpzest uses, and the prototype system itself.","We evaluate Palimpzest on tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching.","We show that even our simple prototype offers a range of appealing plans, including one that is 3.3x faster, 2.9x cheaper, and offers better data quality than the baseline method.","With parallelism enabled, Palimpzest can produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a single-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the baseline.","These require no additional work by the user."],"url":"http://arxiv.org/abs/2405.14696v1","category":"cs.CL"}
{"created":"2024-05-23 15:29:12","title":"Necessity of Quantizable Geometry for Quantum Gravity","abstract":"In this paper, Dirac Quantization of $3D$ gravity in the first-order formalism is attempted where instead of quantizing the connection and triad fields, the connection and the triad 1-forms themselves are quantized. The exterior derivative operator on the space of differential forms is treated as the `time' derivative to compute the momenta conjugate to these 1-forms. This manner of quantization allows one to compute the transition amplitude in $3D$ gravity which has a close, but not exact, match with the transition amplitude computed via LQG techniques. This inconsistency is interpreted as being due to the non-quantizable nature of differential geometry.","sentences":["In this paper, Dirac Quantization of $3D$ gravity in the first-order formalism is attempted where instead of quantizing the connection and triad fields, the connection and the triad 1-forms themselves are quantized.","The exterior derivative operator on the space of differential forms is treated as the `time' derivative to compute the momenta conjugate to these 1-forms.","This manner of quantization allows one to compute the transition amplitude in $3D$ gravity which has a close, but not exact, match with the transition amplitude computed via LQG techniques.","This inconsistency is interpreted as being due to the non-quantizable nature of differential geometry."],"url":"http://arxiv.org/abs/2405.14692v1","category":"gr-qc"}
{"created":"2024-05-23 15:27:18","title":"CityGPT: Towards Urban IoT Learning, Analysis and Interaction with Multi-Agent System","abstract":"The spatiotemporal data generated by massive sensors in the Internet of Things (IoT) is extremely dynamic, heterogeneous, large scale and time-dependent. It poses great challenges (e.g. accuracy, reliability, and stability) in real-time analysis and decision making for different IoT applications. The complexity of IoT data prevents the common people from gaining a deeper understanding of it. Agentized systems help address the lack of data insight for the common people. We propose a generic framework, namely CityGPT, to facilitate the learning and analysis of IoT time series with an end-to-end paradigm. CityGPT employs three agents to accomplish the spatiotemporal analysis of IoT data. The requirement agent facilitates user inputs based on natural language. Then, the analysis tasks are decomposed into temporal and spatial analysis processes, completed by corresponding data analysis agents (temporal and spatial agents). Finally, the spatiotemporal fusion agent visualizes the system's analysis results by receiving analysis results from data analysis agents and invoking sub-visualization agents, and can provide corresponding textual descriptions based on user demands. To increase the insight for common people using our framework, we have agnentized the framework, facilitated by a large language model (LLM), to increase the data comprehensibility. Our evaluation results on real-world data with different time dependencies show that the CityGPT framework can guarantee robust performance in IoT computing.","sentences":["The spatiotemporal data generated by massive sensors in the Internet of Things (IoT) is extremely dynamic, heterogeneous, large scale and time-dependent.","It poses great challenges (e.g. accuracy, reliability, and stability) in real-time analysis and decision making for different IoT applications.","The complexity of IoT data prevents the common people from gaining a deeper understanding of it.","Agentized systems help address the lack of data insight for the common people.","We propose a generic framework, namely CityGPT, to facilitate the learning and analysis of IoT time series with an end-to-end paradigm.","CityGPT employs three agents to accomplish the spatiotemporal analysis of IoT data.","The requirement agent facilitates user inputs based on natural language.","Then, the analysis tasks are decomposed into temporal and spatial analysis processes, completed by corresponding data analysis agents (temporal and spatial agents).","Finally, the spatiotemporal fusion agent visualizes the system's analysis results by receiving analysis results from data analysis agents and invoking sub-visualization agents, and can provide corresponding textual descriptions based on user demands.","To increase the insight for common people using our framework, we have agnentized the framework, facilitated by a large language model (LLM), to increase the data comprehensibility.","Our evaluation results on real-world data with different time dependencies show that the CityGPT framework can guarantee robust performance in IoT computing."],"url":"http://arxiv.org/abs/2405.14691v1","category":"cs.AI"}
{"created":"2024-05-23 15:25:56","title":"Cascade of phase transitions in the training of Energy-based models","abstract":"In this paper, we investigate the feature encoding process in a prototypical energy-based generative model, the Restricted Boltzmann Machine (RBM). We start with an analytical investigation using simplified architectures and data structures, and end with numerical analysis of real trainings on real datasets. Our study tracks the evolution of the model's weight matrix through its singular value decomposition, revealing a series of phase transitions associated to a progressive learning of the principal modes of the empirical probability distribution. The model first learns the center of mass of the modes and then progressively resolve all modes through a cascade of phase transitions. We first describe this process analytically in a controlled setup that allows us to study analytically the training dynamics. We then validate our theoretical results by training the Bernoulli-Bernoulli RBM on real data sets. By using data sets of increasing dimension, we show that learning indeed leads to sharp phase transitions in the high-dimensional limit. Moreover, we propose and test a mean-field finite-size scaling hypothesis. This shows that the first phase transition is in the same universality class of the one we studied analytically, and which is reminiscent of the mean-field paramagnetic-to-ferromagnetic phase transition.","sentences":["In this paper, we investigate the feature encoding process in a prototypical energy-based generative model, the Restricted Boltzmann Machine (RBM).","We start with an analytical investigation using simplified architectures and data structures, and end with numerical analysis of real trainings on real datasets.","Our study tracks the evolution of the model's weight matrix through its singular value decomposition, revealing a series of phase transitions associated to a progressive learning of the principal modes of the empirical probability distribution.","The model first learns the center of mass of the modes and then progressively resolve all modes through a cascade of phase transitions.","We first describe this process analytically in a controlled setup that allows us to study analytically the training dynamics.","We then validate our theoretical results by training the Bernoulli-Bernoulli RBM on real data sets.","By using data sets of increasing dimension, we show that learning indeed leads to sharp phase transitions in the high-dimensional limit.","Moreover, we propose and test a mean-field finite-size scaling hypothesis.","This shows that the first phase transition is in the same universality class of the one we studied analytically, and which is reminiscent of the mean-field paramagnetic-to-ferromagnetic phase transition."],"url":"http://arxiv.org/abs/2405.14689v1","category":"cs.LG"}
{"created":"2024-05-23 15:17:21","title":"Turing instabilities for three interacting species","abstract":"In this paper, I prove necessary and sufficient conditions for the existence of Turing instabilities in a general system with three interacting species. Turing instabilities describe situations when a stable steady state of a reaction system (ordinary differential equation) becomes an unstable homogeneous steady state of the corresponding reaction-diffusion system (partial differential equation). Similarly to a well-known inequality condition for Turing instabilities in a system with two species, I find a set of inequality conditions for a system with three species. Furthermore, I distinguish conditions for the Turing instability when spatial perturbations grow steadily and the Turing-Hopf instability when spatial perturbations grow and oscillate in time simultaneously.","sentences":["In this paper, I prove necessary and sufficient conditions for the existence of Turing instabilities in a general system with three interacting species.","Turing instabilities describe situations when a stable steady state of a reaction system (ordinary differential equation) becomes an unstable homogeneous steady state of the corresponding reaction-diffusion system (partial differential equation).","Similarly to a well-known inequality condition for Turing instabilities in a system with two species, I find a set of inequality conditions for a system with three species.","Furthermore, I distinguish conditions for the Turing instability when spatial perturbations grow steadily and the Turing-Hopf instability when spatial perturbations grow and oscillate in time simultaneously."],"url":"http://arxiv.org/abs/2405.14682v1","category":"nlin.PS"}
{"created":"2024-05-23 15:15:17","title":"Recursive PAC-Bayes: A Frequentist Approach to Sequential Prior Updates with No Information Loss","abstract":"PAC-Bayesian analysis is a frequentist framework for incorporating prior knowledge into learning. It was inspired by Bayesian learning, which allows sequential data processing and naturally turns posteriors from one processing step into priors for the next. However, despite two and a half decades of research, the ability to update priors sequentially without losing confidence information along the way remained elusive for PAC-Bayes. While PAC-Bayes allows construction of data-informed priors, the final confidence intervals depend only on the number of points that were not used for the construction of the prior, whereas confidence information in the prior, which is related to the number of points used to construct the prior, is lost. This limits the possibility and benefit of sequential prior updates, because the final bounds depend only on the size of the final batch.   We present a novel and, in retrospect, surprisingly simple and powerful PAC-Bayesian procedure that allows sequential prior updates with no information loss. The procedure is based on a novel decomposition of the expected loss of randomized classifiers. The decomposition rewrites the loss of the posterior as an excess loss relative to a downscaled loss of the prior plus the downscaled loss of the prior, which is bounded recursively. As a side result, we also present a generalization of the split-kl and PAC-Bayes-split-kl inequalities to discrete random variables, which we use for bounding the excess losses, and which can be of independent interest. In empirical evaluation the new procedure significantly outperforms state-of-the-art.","sentences":["PAC-Bayesian analysis is a frequentist framework for incorporating prior knowledge into learning.","It was inspired by Bayesian learning, which allows sequential data processing and naturally turns posteriors from one processing step into priors for the next.","However, despite two and a half decades of research, the ability to update priors sequentially without losing confidence information along the way remained elusive for PAC-Bayes.","While PAC-Bayes allows construction of data-informed priors, the final confidence intervals depend only on the number of points that were not used for the construction of the prior, whereas confidence information in the prior, which is related to the number of points used to construct the prior, is lost.","This limits the possibility and benefit of sequential prior updates, because the final bounds depend only on the size of the final batch.   ","We present a novel and, in retrospect, surprisingly simple and powerful PAC-Bayesian procedure that allows sequential prior updates with no information loss.","The procedure is based on a novel decomposition of the expected loss of randomized classifiers.","The decomposition rewrites the loss of the posterior as an excess loss relative to a downscaled loss of the prior plus the downscaled loss of the prior, which is bounded recursively.","As a side result, we also present a generalization of the split-kl and PAC-Bayes-split-kl inequalities to discrete random variables, which we use for bounding the excess losses, and which can be of independent interest.","In empirical evaluation the new procedure significantly outperforms state-of-the-art."],"url":"http://arxiv.org/abs/2405.14681v1","category":"cs.LG"}
{"created":"2024-05-23 15:14:11","title":"Uniform diameter estimates for Kaehler metrics","abstract":"We prove a uniform diameter estimate and a uniform local non-collapsing of volumes for a large family of Kaehler metrics generalizing those obtained recently by Guo-Phong-Song-Sturm to the extent that no assumption on the uniform (pointwise) non-vanishing of volume forms is required. We treat also similar questions in the singular setting where the cohomology classes are allowed to degenerate to non-big classes. This again extends results by the latter authors where only the non-collapsing case was considered.","sentences":["We prove a uniform diameter estimate and a uniform local non-collapsing of volumes for a large family of Kaehler metrics generalizing those obtained recently by Guo-Phong-Song-Sturm to the extent that no assumption on the uniform (pointwise) non-vanishing of volume forms is required.","We treat also similar questions in the singular setting where the cohomology classes are allowed to degenerate to non-big classes.","This again extends results by the latter authors where only the non-collapsing case was considered."],"url":"http://arxiv.org/abs/2405.14680v1","category":"math.DG"}
{"created":"2024-05-23 15:13:40","title":"Leveraging Electric Guitar Tones and Effects to Improve Robustness in Guitar Tablature Transcription Modeling","abstract":"Guitar tablature transcription (GTT) aims at automatically generating symbolic representations from real solo guitar performances. Due to its applications in education and musicology, GTT has gained traction in recent years. However, GTT robustness has been limited due to the small size of available datasets. Researchers have recently used synthetic data that simulates guitar performances using pre-recorded or computer-generated tones and can be automatically generated at large scales. The present study complements these efforts by demonstrating that GTT robustness can be improved by including synthetic training data created using recordings of real guitar tones played with different audio effects. We evaluate our approach on a new evaluation dataset with professional solo guitar performances that we composed and collected, featuring a wide array of tones, chords, and scales.","sentences":["Guitar tablature transcription (GTT) aims at automatically generating symbolic representations from real solo guitar performances.","Due to its applications in education and musicology, GTT has gained traction in recent years.","However, GTT robustness has been limited due to the small size of available datasets.","Researchers have recently used synthetic data that simulates guitar performances using pre-recorded or computer-generated tones and can be automatically generated at large scales.","The present study complements these efforts by demonstrating that GTT robustness can be improved by including synthetic training data created using recordings of real guitar tones played with different audio effects.","We evaluate our approach on a new evaluation dataset with professional solo guitar performances that we composed and collected, featuring a wide array of tones, chords, and scales."],"url":"http://arxiv.org/abs/2405.14679v1","category":"cs.SD"}
{"created":"2024-05-23 15:13:33","title":"Measuring data types","abstract":"In this article, we combine Sweedler's classic theory of measuring coalgebras -- by which $k$-algebras are enriched in $k$-coalgebras for $k$ a field -- with the theory of W-types -- by which the categorical semantics of inductive data types in functional programming languages are understood. In our main theorem, we find that under some hypotheses, algebras of an endofunctor are enriched in coalgebras of the same endofunctor, and we find polynomial endofunctors provide many interesting examples of this phenomenon. We then generalize the notion of initial algebra of an endofunctor using this enrichment, thus generalizing the notion of W-type. This article is an extended version of arXiv:2303.16793, it adds expository introductions to the original theories of measuring coalgebras and W-types along with some improvements to the main theory and many explicitly worked examples.","sentences":["In this article, we combine Sweedler's classic theory of measuring coalgebras -- by which $k$-algebras are enriched in $k$-coalgebras for $k$ a field -- with the theory of W-types -- by which the categorical semantics of inductive data types in functional programming languages are understood.","In our main theorem, we find that under some hypotheses, algebras of an endofunctor are enriched in coalgebras of the same endofunctor, and we find polynomial endofunctors provide many interesting examples of this phenomenon.","We then generalize the notion of initial algebra of an endofunctor using this enrichment, thus generalizing the notion of W-type.","This article is an extended version of arXiv:2303.16793, it adds expository introductions to the original theories of measuring coalgebras and W-types along with some improvements to the main theory and many explicitly worked examples."],"url":"http://arxiv.org/abs/2405.14678v1","category":"math.CT"}
{"created":"2024-05-23 15:12:15","title":"RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance","abstract":"Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at https://github.com/feifeiobama/RectifID.","sentences":["Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem.","The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases.","To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation.","Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators.","Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee.","The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects.","Code is available at https://github.com/feifeiobama/RectifID."],"url":"http://arxiv.org/abs/2405.14677v1","category":"cs.CV"}
{"created":"2024-05-23 15:11:23","title":"Drones Help Drones: A Collaborative Framework for Multi-Drone Object Trajectory Prediction and Beyond","abstract":"Collaborative trajectory prediction can comprehensively forecast the future motion of objects through multi-view complementary information. However, it encounters two main challenges in multi-drone collaboration settings. The expansive aerial observations make it difficult to generate precise Bird's Eye View (BEV) representations. Besides, excessive interactions can not meet real-time prediction requirements within the constrained drone-based communication bandwidth. To address these problems, we propose a novel framework named \"Drones Help Drones\" (DHD). Firstly, we incorporate the ground priors provided by the drone's inclined observation to estimate the distance between objects and drones, leading to more precise BEV generation. Secondly, we design a selective mechanism based on the local feature discrepancy to prioritize the critical information contributing to prediction tasks during inter-drone interactions. Additionally, we create the first dataset for multi-drone collaborative prediction, named \"Air-Co-Pred\", and conduct quantitative and qualitative experiments to validate the effectiveness of our DHD framework.The results demonstrate that compared to state-of-the-art approaches, DHD reduces position deviation in BEV representations by over 20% and requires only a quarter of the transmission ratio for interactions while achieving comparable prediction performance. Moreover, DHD also shows promising generalization to the collaborative 3D object detection in CoPerception-UAVs.","sentences":["Collaborative trajectory prediction can comprehensively forecast the future motion of objects through multi-view complementary information.","However, it encounters two main challenges in multi-drone collaboration settings.","The expansive aerial observations make it difficult to generate precise Bird's Eye View (BEV) representations.","Besides, excessive interactions can not meet real-time prediction requirements within the constrained drone-based communication bandwidth.","To address these problems, we propose a novel framework named \"Drones Help Drones\" (DHD).","Firstly, we incorporate the ground priors provided by the drone's inclined observation to estimate the distance between objects and drones, leading to more precise BEV generation.","Secondly, we design a selective mechanism based on the local feature discrepancy to prioritize the critical information contributing to prediction tasks during inter-drone interactions.","Additionally, we create the first dataset for multi-drone collaborative prediction, named \"Air-Co-Pred\", and conduct quantitative and qualitative experiments to validate the effectiveness of our DHD framework.","The results demonstrate that compared to state-of-the-art approaches, DHD reduces position deviation in BEV representations by over 20% and requires only a quarter of the transmission ratio for interactions while achieving comparable prediction performance.","Moreover, DHD also shows promising generalization to the collaborative 3D object detection in CoPerception-UAVs."],"url":"http://arxiv.org/abs/2405.14674v1","category":"cs.CV"}
{"created":"2024-05-23 15:06:02","title":"Efficiency for Free: Ideal Data Are Transportable Representations","abstract":"Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution. Existing paradigms tackle the issue of learning efficiency over massive datasets from the perspective of self-supervised learning and dataset distillation independently, while neglecting the untapped potential of accelerating representation learning from an intermediate standpoint. In this work, we delve into defining the ideal data properties from both optimization and generalization perspectives. We propose that model-generated representations, despite being trained on diverse tasks and architectures, converge to a shared linear space, facilitating effective linear transport between models. Furthermore, we demonstrate that these representations exhibit properties conducive to the formation of ideal data. The theoretical/empirical insights therein inspire us to propose a Representation Learning Accelerator (ReLA), which leverages a task- and architecture-agnostic, yet publicly available, free model to form a dynamic data subset and thus accelerate (self-)supervised learning. For instance, employing a CLIP ViT B/16 as a prior model for dynamic data generation, ReLA-aided BYOL can train a ResNet-50 from scratch with 50% of ImageNet-1K, yielding performance surpassing that of training on the full dataset. Additionally, employing a ResNet-18 pre-trained on CIFAR-10 can enhance ResNet-50 training on 10% of ImageNet-1K, resulting in a 7.7% increase in accuracy.","sentences":["Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution.","Existing paradigms tackle the issue of learning efficiency over massive datasets from the perspective of self-supervised learning and dataset distillation independently, while neglecting the untapped potential of accelerating representation learning from an intermediate standpoint.","In this work, we delve into defining the ideal data properties from both optimization and generalization perspectives.","We propose that model-generated representations, despite being trained on diverse tasks and architectures, converge to a shared linear space, facilitating effective linear transport between models.","Furthermore, we demonstrate that these representations exhibit properties conducive to the formation of ideal data.","The theoretical/empirical insights therein inspire us to propose a Representation Learning Accelerator (ReLA), which leverages a task- and architecture-agnostic, yet publicly available, free model to form a dynamic data subset and thus accelerate (self-)supervised learning.","For instance, employing a CLIP ViT B/16 as a prior model for dynamic data generation, ReLA-aided BYOL can train a ResNet-50 from scratch with 50% of ImageNet-1K, yielding performance surpassing that of training on the full dataset.","Additionally, employing a ResNet-18 pre-trained on CIFAR-10 can enhance ResNet-50 training on 10% of ImageNet-1K, resulting in a 7.7% increase in accuracy."],"url":"http://arxiv.org/abs/2405.14669v1","category":"cs.LG"}
{"created":"2024-05-23 15:03:16","title":"Giant splitting of the hydrogen rotational eigenenergies in the C$_2$ filled ice","abstract":"Hydrogen hydrates present a rich phase diagram influenced by both pressure and temperature, with the so-called C$_2$ phase emerging prominently above 2.5 GPa. In this phase, hydrogen molecules are densely packed within a cubic ice-like lattice and the interaction with the surrounding water molecules profoundly affects their quantum rotational dynamics. Herein, we delve into this intricate interplay by directly solving the Schr\\\"{o}dinger's equation for a quantum H$_2$ rotor in the C$_2$ crystal field at finite temperature, generated through Density Functional Theory. Our calculations reveal a giant energy splitting relative to the magnetic quantum number of $\\pm$3.2 meV for $l=1$. Employing inelastic neutron scattering, we experimentally measure the energy levels of H$_2$ within the C$_2$ phase at 6.0 and 3.4 GPa and low temperatures, finding remarkable agreement with our theoretical predictions. These findings underscore the pivotal role of hydrogen--water interactions in dictating the rotational behavior of the hydrogen molecules within the C$_2$ phase and indicate heightened induced-dipole interactions compared to other hydrogen hydrates.","sentences":["Hydrogen hydrates present a rich phase diagram influenced by both pressure and temperature, with the so-called C$_2$ phase emerging prominently above 2.5 GPa.","In this phase, hydrogen molecules are densely packed within a cubic ice-like lattice and the interaction with the surrounding water molecules profoundly affects their quantum rotational dynamics.","Herein, we delve into this intricate interplay by directly solving the Schr\\\"{o}dinger's equation for a quantum H$_2$ rotor in the C$_2$ crystal field at finite temperature, generated through Density Functional Theory.","Our calculations reveal a giant energy splitting relative to the magnetic quantum number of $\\pm$3.2 meV for $l=1$. Employing inelastic neutron scattering, we experimentally measure the energy levels of H$_2$ within the C$_2$ phase at 6.0 and 3.4 GPa and low temperatures, finding remarkable agreement with our theoretical predictions.","These findings underscore the pivotal role of hydrogen--water interactions in dictating the rotational behavior of the hydrogen molecules within the C$_2$ phase and indicate heightened induced-dipole interactions compared to other hydrogen hydrates."],"url":"http://arxiv.org/abs/2405.14665v1","category":"cond-mat.soft"}
{"created":"2024-05-23 15:02:11","title":"Fisher Flow Matching for Generative Modeling over Discrete Data","abstract":"Generative modeling over discrete data has recently seen numerous success stories, with applications spanning language modeling, biological sequence design, and graph-structured molecular data. The predominant generative modeling paradigm for discrete data is still autoregressive, with more recent alternatives based on diffusion or flow-matching falling short of their impressive performance in continuous data settings, such as image or video generation. In this work, we introduce Fisher-Flow, a novel flow-matching model for discrete data. Fisher-Flow takes a manifestly geometric perspective by considering categorical distributions over discrete data as points residing on a statistical manifold equipped with its natural Riemannian metric: the $\\textit{Fisher-Rao metric}$. As a result, we demonstrate discrete data itself can be continuously reparameterised to points on the positive orthant of the $d$-hypersphere $\\mathbb{S}^d_+$, which allows us to define flows that map any source distribution to target in a principled manner by transporting mass along (closed-form) geodesics of $\\mathbb{S}^d_+$. Furthermore, the learned flows in Fisher-Flow can be further bootstrapped by leveraging Riemannian optimal transport leading to improved training dynamics. We prove that the gradient flow induced by Fisher-Flow is optimal in reducing the forward KL divergence.   We evaluate Fisher-Flow on an array of synthetic and diverse real-world benchmarks, including designing DNA Promoter, and DNA Enhancer sequences. Empirically, we find that Fisher-Flow improves over prior diffusion and flow-matching models on these benchmarks.","sentences":["Generative modeling over discrete data has recently seen numerous success stories, with applications spanning language modeling, biological sequence design, and graph-structured molecular data.","The predominant generative modeling paradigm for discrete data is still autoregressive, with more recent alternatives based on diffusion or flow-matching falling short of their impressive performance in continuous data settings, such as image or video generation.","In this work, we introduce Fisher-Flow, a novel flow-matching model for discrete data.","Fisher-Flow takes a manifestly geometric perspective by considering categorical distributions over discrete data as points residing on a statistical manifold equipped with its natural Riemannian metric: the $\\textit{Fisher-Rao metric}$. As a result, we demonstrate discrete data itself can be continuously reparameterised to points on the positive orthant of the $d$-hypersphere $\\mathbb{S}^d_+$, which allows us to define flows that map any source distribution to target in a principled manner by transporting mass along (closed-form) geodesics of $\\mathbb{S}^d_+$. Furthermore, the learned flows in Fisher-Flow can be further bootstrapped by leveraging Riemannian optimal transport leading to improved training dynamics.","We prove that the gradient flow induced by Fisher-Flow is optimal in reducing the forward KL divergence.   ","We evaluate Fisher-Flow on an array of synthetic and diverse real-world benchmarks, including designing DNA Promoter, and DNA Enhancer sequences.","Empirically, we find that Fisher-Flow improves over prior diffusion and flow-matching models on these benchmarks."],"url":"http://arxiv.org/abs/2405.14664v1","category":"cs.LG"}
{"created":"2024-05-23 14:59:59","title":"Tuning monolayer superconductivity in twisted NbSe$_2$ graphene heterostructures","abstract":"The recent advent of artificial structures has triggered the emergence of fascinating phenomena that could not exist in natural compounds. A prime example is twisted multilayers, i.e., moir\\'e superlattices represented by magic-angle twisted bilayer graphene (MATBG). As in the case of MATBG, unconventional band hybridization can induce a new type of superconductivity: artificial band engineering by twist induces properties different from the original systems. Here, we apply this perspective to a monolayer superconductor NbSe$_2$ stacked with a twist on doped graphene. We show that the superconducting states of the NbSe$_2$ layer change dramatically by varying the twist angle. Our result shows that twist tuning, in addition to substrate effects, will provide a strategy for designing monolayer superconductors with high controllability.","sentences":["The recent advent of artificial structures has triggered the emergence of fascinating phenomena that could not exist in natural compounds.","A prime example is twisted multilayers, i.e., moir\\'e superlattices represented by magic-angle twisted bilayer graphene (MATBG).","As in the case of MATBG, unconventional band hybridization can induce a new type of superconductivity: artificial band engineering by twist induces properties different from the original systems.","Here, we apply this perspective to a monolayer superconductor NbSe$_2$ stacked with a twist on doped graphene.","We show that the superconducting states of the NbSe$_2$ layer change dramatically by varying the twist angle.","Our result shows that twist tuning, in addition to substrate effects, will provide a strategy for designing monolayer superconductors with high controllability."],"url":"http://arxiv.org/abs/2405.14661v1","category":"cond-mat.supr-con"}
{"created":"2024-05-23 14:57:52","title":"Implicit In-context Learning","abstract":"In-context Learning (ICL) empowers large language models (LLMs) to adapt to unseen tasks during inference by prefixing a few demonstration examples prior to test queries. Despite its versatility, ICL incurs substantial computational and memory overheads compared to zero-shot learning and is susceptible to the selection and order of demonstration examples. In this work, we introduce Implicit In-context Learning (I2CL), an innovative paradigm that addresses the challenges associated with traditional ICL by absorbing demonstration examples within the activation space. I2CL first generates a condensed vector representation, namely a context vector, from the demonstration examples. It then integrates the context vector during inference by injecting a linear combination of the context vector and query activations into the model's residual streams. Empirical evaluation on nine real-world tasks across three model architectures demonstrates that I2CL achieves few-shot performance with zero-shot cost and exhibits robustness against the variation of demonstration examples. Furthermore, I2CL facilitates a novel representation of \"task-ids\", enhancing task similarity detection and enabling effective transfer learning. We provide a comprehensive analysis of I2CL, offering deeper insights into its mechanisms and broader implications for ICL. The source code is available at: https://github.com/LzVv123456/I2CL.","sentences":["In-context Learning (ICL) empowers large language models (LLMs) to adapt to unseen tasks during inference by prefixing a few demonstration examples prior to test queries.","Despite its versatility, ICL incurs substantial computational and memory overheads compared to zero-shot learning and is susceptible to the selection and order of demonstration examples.","In this work, we introduce Implicit In-context Learning (I2CL), an innovative paradigm that addresses the challenges associated with traditional ICL by absorbing demonstration examples within the activation space.","I2CL first generates a condensed vector representation, namely a context vector, from the demonstration examples.","It then integrates the context vector during inference by injecting a linear combination of the context vector and query activations into the model's residual streams.","Empirical evaluation on nine real-world tasks across three model architectures demonstrates that I2CL achieves few-shot performance with zero-shot cost and exhibits robustness against the variation of demonstration examples.","Furthermore, I2CL facilitates a novel representation of \"task-ids\", enhancing task similarity detection and enabling effective transfer learning.","We provide a comprehensive analysis of I2CL, offering deeper insights into its mechanisms and broader implications for ICL.","The source code is available at: https://github.com/LzVv123456/I2CL."],"url":"http://arxiv.org/abs/2405.14660v1","category":"cs.LG"}
{"created":"2024-05-23 14:57:28","title":"Albanese fibrations of surfaces with low slope","abstract":"Let $S$ be a minimal irregular surface of general type, whose Albanese map induces a fibration $f:\\,S \\to C$ of genus $g$.We prove a linear upper bound on the genus $g$ if $K_S^2\\leq 4\\chi(\\mathcal{O}_S)$. Examples are constructed showing that the above linear upper bound is sharp.We also construct a sequence of surfaces $S_n$ of general type with $K_{S_n}^2/\\chi(\\mathcal{O}_{S_n})>4$ and with an Albanese fibration $f_n$, such that the genus $g_n$ of a general fiber of $f_n$ increases quadratically with $\\chi(\\mathcal{O}_{S_n})$, and that $K_{S_n}^2/\\chi(\\mathcal{O}_{S_n})$ can be arbitrarily close to $4$.","sentences":["Let $S$ be a minimal irregular surface of general type, whose Albanese map induces a fibration $f:\\,S \\to C$ of genus $g$.We prove a linear upper bound on the genus $g$ if $K_S^2\\leq 4\\chi(\\mathcal{O}_S)$. Examples are constructed showing that the above linear upper bound is sharp.","We also construct a sequence of surfaces $S_n$ of general type with $K_{S_n}^2/\\chi(\\mathcal{O}_{S_n})>4$ and with an Albanese fibration $f_n$, such that the genus $g_n$ of a general fiber of $f_n$ increases quadratically with $\\chi(\\mathcal{O}_{S_n})$, and that $K_{S_n}^2/\\chi(\\mathcal{O}_{S_n})$ can be arbitrarily close to $4$."],"url":"http://arxiv.org/abs/2405.14659v1","category":"math.AG"}
{"created":"2024-05-23 14:56:48","title":"Proper affine deformations of positive representations","abstract":"We define for every positive Anosov representation of a nonabelian free group into $\\mathrm{SO}(2n,2n-1)$ a family of $\\mathbb{R}^{4n-1}$-valued cocycles which induce proper affine actions on $\\mathbb{R}^{4n-1}$. We construct fundamental domains in $\\mathbb{R}^{4n-1}$ bounded by generalized crooked planes for these affine actions, and deduce that the quotient manifolds are homeomorphic to handlebodies.","sentences":["We define for every positive Anosov representation of a nonabelian free group into $\\mathrm{SO}(2n,2n-1)$ a family of $\\mathbb{R}^{4n-1}$-valued cocycles which induce proper affine actions on $\\mathbb{R}^{4n-1}$. We construct fundamental domains in $\\mathbb{R}^{4n-1}$ bounded by generalized crooked planes for these affine actions, and deduce that the quotient manifolds are homeomorphic to handlebodies."],"url":"http://arxiv.org/abs/2405.14658v1","category":"math.DG"}
{"created":"2024-05-23 14:54:18","title":"Vacuum energy density for interacting real and complex scalar fields in a Lorentz symmetry violation scenario","abstract":"In this paper the vacuum energy density and generation of topological mass are investigated for a system of a real and complex scalar fields interacting with each other. In addition to that, it is also included the quartic self-interaction for each one of the fields. The condition imposed on the real field is the periodic condition, while the complex field obey a quasi-periodic condition. The system is placed in a scenario where the CPT-even aether-type Lorentz symmetry violation takes place. We allow that the Lorentz violation affects the fields with different intensities. The vacuum energy density, its loop correction, and the topological mass are evaluated analytically. It is also discussed the possibility of different vacuum states and their corresponding stability requirements, which depends on the conditions imposed on the fields, the interaction coupling constants and also the Lorentz violation parameters. The formalism used here to perform this investigation is the effective potential one, which is written as a loop expansion via path integral in quantum field theory.","sentences":["In this paper the vacuum energy density and generation of topological mass are investigated for a system of a real and complex scalar fields interacting with each other.","In addition to that, it is also included the quartic self-interaction for each one of the fields.","The condition imposed on the real field is the periodic condition, while the complex field obey a quasi-periodic condition.","The system is placed in a scenario where the CPT-even aether-type Lorentz symmetry violation takes place.","We allow that the Lorentz violation affects the fields with different intensities.","The vacuum energy density, its loop correction, and the topological mass are evaluated analytically.","It is also discussed the possibility of different vacuum states and their corresponding stability requirements, which depends on the conditions imposed on the fields, the interaction coupling constants and also the Lorentz violation parameters.","The formalism used here to perform this investigation is the effective potential one, which is written as a loop expansion via path integral in quantum field theory."],"url":"http://arxiv.org/abs/2405.14656v1","category":"hep-th"}
{"created":"2024-05-23 14:53:54","title":"Multi-turn Reinforcement Learning from Preference Human Feedback","abstract":"Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. In the tabular setting, we present a novel mirror-descent-based policy optimization algorithm for the general multi-turn preference-based RL problem, and prove its convergence to Nash equilibrium. To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent guides a student in learning a random topic, and show that a deep RL variant of our algorithm outperforms RLHF baselines. Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks.","Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal.","In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations.","In the tabular setting, we present a novel mirror-descent-based policy optimization algorithm for the general multi-turn preference-based RL problem, and prove its convergence to Nash equilibrium.","To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent guides a student in learning a random topic, and show that a deep RL variant of our algorithm outperforms RLHF baselines.","Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal."],"url":"http://arxiv.org/abs/2405.14655v1","category":"cs.LG"}
{"created":"2024-05-23 14:53:52","title":"Efficient Medical Question Answering with Knowledge-Augmented Question Generation","abstract":"In the expanding field of language model applications, medical knowledge representation remains a significant challenge due to the specialized nature of the domain. Large language models, such as GPT-4, obtain reasonable scores on medical question answering tasks, but smaller models are far behind. In this work, we introduce a method to improve the proficiency of a small language model in the medical domain by employing a two-fold approach. We first fine-tune the model on a corpus of medical textbooks. Then, we use GPT-4 to generate questions similar to the downstream task, prompted with textbook knowledge, and use them to fine-tune the model. Additionally, we introduce ECN-QA, a novel medical question answering dataset containing ``progressive questions'' composed of related sequential questions. We show the benefits of our training strategy on this dataset. The study's findings highlight the potential of small language models in the medical domain when appropriately fine-tuned. The code and weights are available at https://github.com/raidium-med/MQG.","sentences":["In the expanding field of language model applications, medical knowledge representation remains a significant challenge due to the specialized nature of the domain.","Large language models, such as GPT-4, obtain reasonable scores on medical question answering tasks, but smaller models are far behind.","In this work, we introduce a method to improve the proficiency of a small language model in the medical domain by employing a two-fold approach.","We first fine-tune the model on a corpus of medical textbooks.","Then, we use GPT-4 to generate questions similar to the downstream task, prompted with textbook knowledge, and use them to fine-tune the model.","Additionally, we introduce ECN-QA, a novel medical question answering dataset containing ``progressive questions'' composed of related sequential questions.","We show the benefits of our training strategy on this dataset.","The study's findings highlight the potential of small language models in the medical domain when appropriately fine-tuned.","The code and weights are available at https://github.com/raidium-med/MQG."],"url":"http://arxiv.org/abs/2405.14654v1","category":"cs.CL"}
{"created":"2024-05-23 14:53:40","title":"Inflation in a scalar-vector gravity theory","abstract":"We study the possibility that inflation is driven by a scalar field together with a vector field minimally coupled to gravity. By assuming an effective potential that incorporates both fields into the action, we explore two distinct scenarios: one where the fields interact and another where they do not. In this context, we find different analytical solutions to the background scalar-vector fields dynamics during the inflationary scenario considering the slow-roll approximation. Besides, general conditions required for these models of two fields to be realizable are determined and discussed. From the cosmological perturbations, we consider a local field rotation, and then we determine these perturbations (scalar and tensor) during inflation, and we also utilize recent cosmological observations for constraining the parameter-space in these scalar-vector inflationary models.","sentences":["We study the possibility that inflation is driven by a scalar field together with a vector field minimally coupled to gravity.","By assuming an effective potential that incorporates both fields into the action, we explore two distinct scenarios: one where the fields interact and another where they do not.","In this context, we find different analytical solutions to the background scalar-vector fields dynamics during the inflationary scenario considering the slow-roll approximation.","Besides, general conditions required for these models of two fields to be realizable are determined and discussed.","From the cosmological perturbations, we consider a local field rotation, and then we determine these perturbations (scalar and tensor) during inflation, and we also utilize recent cosmological observations for constraining the parameter-space in these scalar-vector inflationary models."],"url":"http://arxiv.org/abs/2405.14653v1","category":"gr-qc"}
{"created":"2024-05-23 14:48:15","title":"Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models","abstract":"The automatic evaluation of natural language generation (NLG) systems presents a long-lasting challenge. Recent studies have highlighted various neural metrics that align well with human evaluations. Yet, the robustness of these evaluators against adversarial perturbations remains largely under-explored due to the unique challenges in obtaining adversarial data for different NLG evaluation tasks. To address the problem, we introduce AdvEval, a novel black-box adversarial framework against NLG evaluators. AdvEval is specially tailored to generate data that yield strong disagreements between human and victim evaluators. Specifically, inspired by the recent success of large language models (LLMs) in text generation and evaluation, we adopt strong LLMs as both the data generator and gold evaluator. Adversarial data are automatically optimized with feedback from the gold and victim evaluator. We conduct experiments on 12 victim evaluators and 11 NLG datasets, spanning tasks including dialogue, summarization, and question evaluation. The results show that AdvEval can lead to significant performance degradation of various victim metrics, thereby validating its efficacy.","sentences":["The automatic evaluation of natural language generation (NLG) systems presents a long-lasting challenge.","Recent studies have highlighted various neural metrics that align well with human evaluations.","Yet, the robustness of these evaluators against adversarial perturbations remains largely under-explored due to the unique challenges in obtaining adversarial data for different NLG evaluation tasks.","To address the problem, we introduce AdvEval, a novel black-box adversarial framework against NLG evaluators.","AdvEval is specially tailored to generate data that yield strong disagreements between human and victim evaluators.","Specifically, inspired by the recent success of large language models (LLMs) in text generation and evaluation, we adopt strong LLMs as both the data generator and gold evaluator.","Adversarial data are automatically optimized with feedback from the gold and victim evaluator.","We conduct experiments on 12 victim evaluators and 11 NLG datasets, spanning tasks including dialogue, summarization, and question evaluation.","The results show that AdvEval can lead to significant performance degradation of various victim metrics, thereby validating its efficacy."],"url":"http://arxiv.org/abs/2405.14646v1","category":"cs.CL"}
{"created":"2024-05-23 14:43:05","title":"K-factor Evaluation in a Hybrid Reverberation Chamber plus CATR OTA Testing Setup","abstract":"This paper investigates achieving diverse K-factors using a Reverberation Chamber (RC) with a Compact Antenna Test Range (CATR) system. It explores six hybrid \"RC plus CATR\" configurations involving different excitations of the Rich Isotropic Multipath (RIMP) field and CATR-generated plane waves, with some setups including absorbers. A fixed horn antenna points towards the CATR in all configurations. The study found that the null hypothesis of Rayleigh or Rician probability distributions for the received signal envelope could not be rejected, with RIMP setups primarily conforming to Rayleigh distribution and all setups showing Rician distribution. Various K-factors were obtained, but no generalizable method for achieving the desired K-factor was identified. The paper also estimates the K-factor as a function of frequency in the 24.25-29.5 GHz band. Smaller K-factors exhibit larger fluctuations, while larger K-factors remain relatively stable, with consistent fluctuations across the frequency range.","sentences":["This paper investigates achieving diverse K-factors using a Reverberation Chamber (RC) with a Compact Antenna Test Range (CATR) system.","It explores six hybrid \"RC plus CATR\" configurations involving different excitations of the Rich Isotropic Multipath (RIMP) field and CATR-generated plane waves, with some setups including absorbers.","A fixed horn antenna points towards the CATR in all configurations.","The study found that the null hypothesis of Rayleigh or Rician probability distributions for the received signal envelope could not be rejected, with RIMP setups primarily conforming to Rayleigh distribution and all setups showing Rician distribution.","Various K-factors were obtained, but no generalizable method for achieving the desired K-factor was identified.","The paper also estimates the K-factor as a function of frequency in the 24.25-29.5 GHz band.","Smaller K-factors exhibit larger fluctuations, while larger K-factors remain relatively stable, with consistent fluctuations across the frequency range."],"url":"http://arxiv.org/abs/2405.14640v1","category":"eess.SY"}
{"created":"2024-05-23 14:42:11","title":"Screened Scalar Fields in the Laboratory and the Solar System","abstract":"The last few decades have provided abundant evidence for physics beyond the two standard models of particle physics and cosmology. As is now known, the by far largest part of our universe's matter/energy content lies in the `dark' and consists of dark energy and dark matter. Despite intensive efforts on the experimental as well as the theoretical side, the origins of both are still completely unknown. Screened scalar fields have been hypothesized as potential candidates for dark energy or dark matter. Among these, some of the most prominent models are the chameleon, symmetron, and environment-dependent dilaton. In this article, we present a summary containing the most recent experimental constraints on the parameters of these three models. For this, experimental results have been employed from the qBOUNCE collaboration, neutron interferometry, and Lunar Laser Ranging (LLR), among others. In addition, constraints are forecast for the Casimir And Non Newtonian force EXperiment (CANNEX). Combining these results with previous ones, this article collects the most up-to-date constraints on the three considered screened scalar field models.","sentences":["The last few decades have provided abundant evidence for physics beyond the two standard models of particle physics and cosmology.","As is now known, the by far largest part of our universe's matter/energy content lies in the `dark' and consists of dark energy and dark matter.","Despite intensive efforts on the experimental as well as the theoretical side, the origins of both are still completely unknown.","Screened scalar fields have been hypothesized as potential candidates for dark energy or dark matter.","Among these, some of the most prominent models are the chameleon, symmetron, and environment-dependent dilaton.","In this article, we present a summary containing the most recent experimental constraints on the parameters of these three models.","For this, experimental results have been employed from the qBOUNCE collaboration, neutron interferometry, and Lunar Laser Ranging (LLR), among others.","In addition, constraints are forecast for the Casimir And Non Newtonian force EXperiment (CANNEX).","Combining these results with previous ones, this article collects the most up-to-date constraints on the three considered screened scalar field models."],"url":"http://arxiv.org/abs/2405.14638v1","category":"gr-qc"}
{"created":"2024-05-23 14:41:54","title":"On the role of semismoothness in nonsmooth numerical analysis: Theory","abstract":"For the numerical solution of nonsmooth problems, sometimes it is not necessary that an exact subgradient/generalized Jacobian is at our disposal, but that a certain semismoothness property is fulfilled. In this paper we consider not only semismoothness of nonsmooth real- and vector-valued mappings, but also its interplay with the semismoothness$^*$ property for multifunctions. In particular, we are interested in the semismoothness of solution maps to parametric semismooth$^*$ inclusions. Our results are expressed in terms of suitable generalized derivatives of the set-valued part, i.e., by limiting coderivatives or by SC (subspace containing) derivatives. As a byproduct we identify a class of multifunctions having the remarkable property that they are strictly proto-differentiable almost everywhere (with respect to some Hausdorff measure) on their graph.","sentences":["For the numerical solution of nonsmooth problems, sometimes it is not necessary that an exact subgradient/generalized Jacobian is at our disposal, but that a certain semismoothness property is fulfilled.","In this paper we consider not only semismoothness of nonsmooth real- and vector-valued mappings, but also its interplay with the semismoothness$^*$ property for multifunctions.","In particular, we are interested in the semismoothness of solution maps to parametric semismooth$^*$ inclusions.","Our results are expressed in terms of suitable generalized derivatives of the set-valued part, i.e., by limiting coderivatives or by SC (subspace containing) derivatives.","As a byproduct we identify a class of multifunctions having the remarkable property that they are strictly proto-differentiable almost everywhere (with respect to some Hausdorff measure) on their graph."],"url":"http://arxiv.org/abs/2405.14637v1","category":"math.OC"}
{"created":"2024-05-23 14:39:35","title":"Reinforcement Learning for Fine-tuning Text-to-speech Diffusion Models","abstract":"Recent advancements in generative models have sparked significant interest within the machine learning community. Particularly, diffusion models have demonstrated remarkable capabilities in synthesizing images and speech. Studies such as those by Lee et al. [19], Black et al. [4], Wang et al. [36], and Fan et al. [8] illustrate that Reinforcement Learning with Human Feedback (RLHF) can enhance diffusion models for image synthesis. However, due to architectural differences between these models and those employed in speech synthesis, it remains uncertain whether RLHF could similarly benefit speech synthesis models. In this paper, we explore the practical application of RLHF to diffusion-based text-to-speech synthesis, leveraging the mean opinion score (MOS) as predicted by UTokyo-SaruLab MOS prediction system [29] as a proxy loss. We introduce diffusion model loss-guided RL policy optimization (DLPO) and compare it against other RLHF approaches, employing the NISQA speech quality and naturalness assessment model [21] and human preference experiments for further evaluation. Our results show that RLHF can enhance diffusion-based text-to-speech synthesis models, and, moreover, DLPO can better improve diffusion models in generating natural and high quality speech audios.","sentences":["Recent advancements in generative models have sparked significant interest within the machine learning community.","Particularly, diffusion models have demonstrated remarkable capabilities in synthesizing images and speech.","Studies such as those by Lee et al.","[19], Black et al.","[4], Wang et al.","[36], and Fan et al.","[8] illustrate that Reinforcement Learning with Human Feedback (RLHF) can enhance diffusion models for image synthesis.","However, due to architectural differences between these models and those employed in speech synthesis, it remains uncertain whether RLHF could similarly benefit speech synthesis models.","In this paper, we explore the practical application of RLHF to diffusion-based text-to-speech synthesis, leveraging the mean opinion score (MOS) as predicted by UTokyo-SaruLab MOS prediction system [29] as a proxy loss.","We introduce diffusion model loss-guided RL policy optimization (DLPO) and compare it against other RLHF approaches, employing the NISQA speech quality and naturalness assessment model","[21] and human preference experiments for further evaluation.","Our results show that RLHF can enhance diffusion-based text-to-speech synthesis models, and, moreover, DLPO can better improve diffusion models in generating natural and high quality speech audios."],"url":"http://arxiv.org/abs/2405.14632v1","category":"cs.LG"}
{"created":"2024-05-23 14:35:56","title":"Which Experiences Are Influential for RL Agents? Efficiently Estimating The Influence of Experiences","abstract":"In reinforcement learning (RL) with experience replay, experiences stored in a replay buffer influence the RL agent's performance. Information about the influence of these experiences is valuable for various purposes, such as identifying experiences that negatively influence poorly performing RL agents. One method for estimating the influence of experiences is the leave-one-out (LOO) method. However, this method is usually computationally prohibitive. In this paper, we present Policy Iteration with Turn-over Dropout (PIToD), which efficiently estimates the influence of experiences. We evaluate how accurately PIToD estimates the influence of experiences and its efficiency compared to LOO. We then apply PIToD to amend poorly performing RL agents, i.e., we use PIToD to estimate negatively influential experiences for the RL agents and to delete the influence of these experiences. We show that RL agents' performance is significantly improved via amendments with PIToD.","sentences":["In reinforcement learning (RL) with experience replay, experiences stored in a replay buffer influence the RL agent's performance.","Information about the influence of these experiences is valuable for various purposes, such as identifying experiences that negatively influence poorly performing RL agents.","One method for estimating the influence of experiences is the leave-one-out (LOO) method.","However, this method is usually computationally prohibitive.","In this paper, we present Policy Iteration with Turn-over Dropout (PIToD), which efficiently estimates the influence of experiences.","We evaluate how accurately PIToD estimates the influence of experiences and its efficiency compared to LOO.","We then apply PIToD to amend poorly performing RL agents, i.e., we use PIToD to estimate negatively influential experiences for the RL agents and to delete the influence of these experiences.","We show that RL agents' performance is significantly improved via amendments with PIToD."],"url":"http://arxiv.org/abs/2405.14629v1","category":"cs.LG"}
{"created":"2024-05-23 14:31:53","title":"U-TELL: Unsupervised Task Expert Lifelong Learning","abstract":"Continual learning (CL) models are designed to learn new tasks arriving sequentially without re-training the network. However, real-world ML applications have very limited label information and these models suffer from catastrophic forgetting. To address these issues, we propose an unsupervised CL model with task experts called Unsupervised Task Expert Lifelong Learning (U-TELL) to continually learn the data arriving in a sequence addressing catastrophic forgetting. During training of U-TELL, we introduce a new expert on arrival of a new task. Our proposed architecture has task experts, a structured data generator and a task assigner. Each task expert is composed of 3 blocks; i) a variational autoencoder to capture the task distribution and perform data abstraction, ii) a k-means clustering module, and iii) a structure extractor to preserve latent task data signature. During testing, task assigner selects a suitable expert to perform clustering. U-TELL does not store or replay task samples, instead, we use generated structured samples to train the task assigner. We compared U-TELL with five SOTA unsupervised CL methods. U-TELL outperformed all baselines on seven benchmarks and one industry dataset for various CL scenarios with a training time over 6 times faster than the best performing baseline.","sentences":["Continual learning (CL) models are designed to learn new tasks arriving sequentially without re-training the network.","However, real-world ML applications have very limited label information and these models suffer from catastrophic forgetting.","To address these issues, we propose an unsupervised CL model with task experts called Unsupervised Task Expert Lifelong Learning (U-TELL) to continually learn the data arriving in a sequence addressing catastrophic forgetting.","During training of U-TELL, we introduce a new expert on arrival of a new task.","Our proposed architecture has task experts, a structured data generator and a task assigner.","Each task expert is composed of 3 blocks; i) a variational autoencoder to capture the task distribution and perform data abstraction, ii) a k-means clustering module, and iii) a structure extractor to preserve latent task data signature.","During testing, task assigner selects a suitable expert to perform clustering.","U-TELL does not store or replay task samples, instead, we use generated structured samples to train the task assigner.","We compared U-TELL with five SOTA unsupervised CL methods.","U-TELL outperformed all baselines on seven benchmarks and one industry dataset for various CL scenarios with a training time over 6 times faster than the best performing baseline."],"url":"http://arxiv.org/abs/2405.14623v1","category":"cs.LG"}
{"created":"2024-05-23 14:30:33","title":"Calibrated Self-Rewarding Vision Language Models","abstract":"Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning. Our data and code are available at https://github.com/YiyangZhou/CSR.","sentences":["Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning.","Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs.","This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality.","Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization.","These approaches may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable.","Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning.","In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input.","Empirical results demonstrate that CSR enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%.","Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm.","Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning.","Our data and code are available at https://github.com/YiyangZhou/CSR."],"url":"http://arxiv.org/abs/2405.14622v1","category":"cs.LG"}
{"created":"2024-05-23 14:29:31","title":"Blade: A package for block-triangular form improved Feynman integrals decomposition","abstract":"In this article, we present the package Blade as the first implementation of the block-triangular form improved Feynman integral reduction method. The block-triangular form has orders of magnitude fewer equations compared to the plain integration-by-parts system, allowing for strictly block-by-block solutions. This results in faster evaluations and reduced resource consumption. We elucidate the algorithms involved in obtaining the block-triangular form along with their implementations. Additionally, we introduce novel algorithms for finding the canonical form and symmetry relations of Feynman integrals, as well as for performing spanning-sector reduction. Our benchmarks for various state-of-the-art problems demonstrate that Blade is remarkably competitive among existing reduction tools. Furthermore, the Blade package offers several distinctive features, including support for complex kinematic variables or masses, user-defined Feynman prescriptions for each propagator, and general integrands.","sentences":["In this article, we present the package Blade as the first implementation of the block-triangular form improved Feynman integral reduction method.","The block-triangular form has orders of magnitude fewer equations compared to the plain integration-by-parts system, allowing for strictly block-by-block solutions.","This results in faster evaluations and reduced resource consumption.","We elucidate the algorithms involved in obtaining the block-triangular form along with their implementations.","Additionally, we introduce novel algorithms for finding the canonical form and symmetry relations of Feynman integrals, as well as for performing spanning-sector reduction.","Our benchmarks for various state-of-the-art problems demonstrate that Blade is remarkably competitive among existing reduction tools.","Furthermore, the Blade package offers several distinctive features, including support for complex kinematic variables or masses, user-defined Feynman prescriptions for each propagator, and general integrands."],"url":"http://arxiv.org/abs/2405.14621v1","category":"hep-ph"}
{"created":"2024-05-23 14:28:41","title":"Generating Exceptional Behavior Tests with Reasoning Augmented Large Language Models","abstract":"Many popular programming languages, including C#, Java, and Python, support exceptions. Exceptions are thrown during program execution if an unwanted event happens, e.g., a method is invoked with an illegal argument value. Software developers write exceptional behavior tests (EBTs) to check that their code detects unwanted events and throws appropriate exceptions. Prior research studies have shown the importance of EBTs, but those studies also highlighted that developers put most of their efforts on \"happy paths\", e.g., paths without unwanted events. To help developers fill the gap, we present the first framework, dubbed EXLONG, that automatically generates EBTs. EXLONG is a large language model instruction-tuned from CodeLlama and embeds reasoning about traces that lead to throw statements, conditional expressions that guard throw statements, and non-exceptional behavior tests that execute similar traces. We compare EXLONG with the state-of-the-art models for test generation (CAT-LM) and one of the strongest foundation models (GPT3.5), as well as with analysis-based tools for test generation (Randoop and EvoSuite). Our results show that EXLONG outperforms existing models and tools. Furthermore, we contributed several pull requests to open-source projects and 23 EBTs generated by EXLONG were already accepted.","sentences":["Many popular programming languages, including C#, Java, and Python, support exceptions.","Exceptions are thrown during program execution if an unwanted event happens, e.g., a method is invoked with an illegal argument value.","Software developers write exceptional behavior tests (EBTs) to check that their code detects unwanted events and throws appropriate exceptions.","Prior research studies have shown the importance of EBTs, but those studies also highlighted that developers put most of their efforts on \"happy paths\", e.g., paths without unwanted events.","To help developers fill the gap, we present the first framework, dubbed EXLONG, that automatically generates EBTs.","EXLONG is a large language model instruction-tuned from CodeLlama and embeds reasoning about traces that lead to throw statements, conditional expressions that guard throw statements, and non-exceptional behavior tests that execute similar traces.","We compare EXLONG with the state-of-the-art models for test generation (CAT-LM) and one of the strongest foundation models (GPT3.5), as well as with analysis-based tools for test generation (Randoop and EvoSuite).","Our results show that EXLONG outperforms existing models and tools.","Furthermore, we contributed several pull requests to open-source projects and 23 EBTs generated by EXLONG were already accepted."],"url":"http://arxiv.org/abs/2405.14619v1","category":"cs.SE"}
{"created":"2024-05-23 14:27:12","title":"Towards an Optimal Staking Design: Balancing Security, User Growth, and Token Appreciation","abstract":"This paper examines the economic and security implications of Proof-of-Stake (POS) designs, providing a survey of POS design choices and their underlying economic principles in prominent POS-blockchains. The paper argues that POS-blockchains are essentially platforms that connect three groups of agents: users, validators, and investors. To meet the needs of these groups, blockchains must balance trade-offs between security, user adoption, and investment into the protocol. We focus on the security aspect and identify two different strategies: increasing the quality of validators (static security) vs. increasing the quantity of stakes (dynamic security). We argue that quality comes at the cost of quantity, identifying a trade-off between the two strategies when designing POS systems. We test our qualitative findings using panel analysis on collected data. The analysis indicates that enhancing the quality of the validator set through security measures like slashing and minimum staking amounts may decrease dynamic security. Further, the analysis reveals a strategic divergence among blockchains, highlighting the absence of a single, universally optimal staking design solution. The optimal design hinges upon a platform's specific objectives and its developmental stage. This research compels blockchain developers to meticulously assess the trade-offs outlined in this paper when developing their staking designs.","sentences":["This paper examines the economic and security implications of Proof-of-Stake (POS) designs, providing a survey of POS design choices and their underlying economic principles in prominent POS-blockchains.","The paper argues that POS-blockchains are essentially platforms that connect three groups of agents: users, validators, and investors.","To meet the needs of these groups, blockchains must balance trade-offs between security, user adoption, and investment into the protocol.","We focus on the security aspect and identify two different strategies: increasing the quality of validators (static security) vs. increasing the quantity of stakes (dynamic security).","We argue that quality comes at the cost of quantity, identifying a trade-off between the two strategies when designing POS systems.","We test our qualitative findings using panel analysis on collected data.","The analysis indicates that enhancing the quality of the validator set through security measures like slashing and minimum staking amounts may decrease dynamic security.","Further, the analysis reveals a strategic divergence among blockchains, highlighting the absence of a single, universally optimal staking design solution.","The optimal design hinges upon a platform's specific objectives and its developmental stage.","This research compels blockchain developers to meticulously assess the trade-offs outlined in this paper when developing their staking designs."],"url":"http://arxiv.org/abs/2405.14617v1","category":"econ.GN"}
{"created":"2024-05-23 14:27:07","title":"TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting","abstract":"Time series forecasting is widely used in extensive applications, such as traffic planning and weather forecasting. However, real-world time series usually present intricate temporal variations, making forecasting extremely challenging. Going beyond the mainstream paradigms of plain decomposition and multiperiodicity analysis, we analyze temporal variations in a novel view of multiscale-mixing, which is based on an intuitive but important observation that time series present distinct patterns in different sampling scales. The microscopic and the macroscopic information are reflected in fine and coarse scales respectively, and thereby complex variations can be inherently disentangled. Based on this observation, we propose TimeMixer as a fully MLP-based architecture with Past-Decomposable-Mixing (PDM) and Future-Multipredictor-Mixing (FMM) blocks to take full advantage of disentangled multiscale series in both past extraction and future prediction phases. Concretely, PDM applies the decomposition to multiscale series and further mixes the decomposed seasonal and trend components in fine-to-coarse and coarse-to-fine directions separately, which successively aggregates the microscopic seasonal and macroscopic trend information. FMM further ensembles multiple predictors to utilize complementary forecasting capabilities in multiscale observations. Consequently, TimeMixer is able to achieve consistent state-of-the-art performances in both long-term and short-term forecasting tasks with favorable run-time efficiency.","sentences":["Time series forecasting is widely used in extensive applications, such as traffic planning and weather forecasting.","However, real-world time series usually present intricate temporal variations, making forecasting extremely challenging.","Going beyond the mainstream paradigms of plain decomposition and multiperiodicity analysis, we analyze temporal variations in a novel view of multiscale-mixing, which is based on an intuitive but important observation that time series present distinct patterns in different sampling scales.","The microscopic and the macroscopic information are reflected in fine and coarse scales respectively, and thereby complex variations can be inherently disentangled.","Based on this observation, we propose TimeMixer as a fully MLP-based architecture with Past-Decomposable-Mixing (PDM) and Future-Multipredictor-Mixing (FMM) blocks to take full advantage of disentangled multiscale series in both past extraction and future prediction phases.","Concretely, PDM applies the decomposition to multiscale series and further mixes the decomposed seasonal and trend components in fine-to-coarse and coarse-to-fine directions separately, which successively aggregates the microscopic seasonal and macroscopic trend information.","FMM further ensembles multiple predictors to utilize complementary forecasting capabilities in multiscale observations.","Consequently, TimeMixer is able to achieve consistent state-of-the-art performances in both long-term and short-term forecasting tasks with favorable run-time efficiency."],"url":"http://arxiv.org/abs/2405.14616v1","category":"cs.LG"}
{"created":"2024-05-23 14:26:27","title":"Search for inhomogeneous Meissner screening in Nb induced by low-temperature surface treatments","abstract":"Empirical surface treatments, such as low-temperature baking (LTB) in a gaseous atmosphere or in vacuum, are important for the surface preparation of Nb superconducting radio frequency (SRF) cavities. These treatments inhomogeneously dope the first $\\sim$50 nm of Nb's subsurface and are expected to impart depth-dependent characteristics to its Meissner response; however, direct evidence supporting this remains elusive, suggesting the effect is subtle. In this work, we revisit the Meissner profile data for several LTB treatments obtained from low-energy muon spin rotation (LE-$\\mu$SR) experiments [A. Romanenko et al., Appl. Phys. Lett. 104, 072601 (2014) and R. M. L. McFadden et al., Phys. Rev. Appl. 19, 044018 (2023)], and search for signatures of inhomogeneous field screening. Using a generalized London expression with a recently proposed empirical model for a depth-dependent magnetic penetration depth $\\lambda(z)$, we obtain improved fits to the Meissner data, revealing that the presence of a non-superconducting surface \"dead layer\" $d \\geq 25$ nm is a strong indicator of a reduced supercurrent density at shallow subsurface depths. Our analysis supports the notion that vacuum annealing at 120 $^{\\circ}$C for 48 h induces a depth-dependent Meissner response, which has consequences for Nb's ability to maintain a magnetic-flux-free state. Evidence of similar behavior from a \"nitrogen infusion\" treatment is less compelling. Suggestions for further investigation into the matter are provided.","sentences":["Empirical surface treatments, such as low-temperature baking (LTB) in a gaseous atmosphere or in vacuum, are important for the surface preparation of Nb superconducting radio frequency (SRF) cavities.","These treatments inhomogeneously dope the first $\\sim$50 nm of Nb's subsurface and are expected to impart depth-dependent characteristics to its Meissner response; however, direct evidence supporting this remains elusive, suggesting the effect is subtle.","In this work, we revisit the Meissner profile data for several LTB treatments obtained from low-energy muon spin rotation (LE-$\\mu$SR) experiments [A. Romanenko et al., Appl.","Phys.","Lett.","104, 072601 (2014) and R. M. L. McFadden et al., Phys.","Rev. Appl.","19, 044018 (2023)], and search for signatures of inhomogeneous field screening.","Using a generalized London expression with a recently proposed empirical model for a depth-dependent magnetic penetration depth $\\lambda(z)$, we obtain improved fits to the Meissner data, revealing that the presence of a non-superconducting surface \"dead layer\" $d \\geq 25$ nm is a strong indicator of a reduced supercurrent density at shallow subsurface depths.","Our analysis supports the notion that vacuum annealing at 120 $^{\\circ}$C for 48 h induces a depth-dependent Meissner response, which has consequences for Nb's ability to maintain a magnetic-flux-free state.","Evidence of similar behavior from a \"nitrogen infusion\" treatment is less compelling.","Suggestions for further investigation into the matter are provided."],"url":"http://arxiv.org/abs/2405.14615v1","category":"cond-mat.supr-con"}
{"created":"2024-05-23 14:26:04","title":"Push and Pull: A Framework for Measuring Attentional Agency","abstract":"We propose a framework for measuring attentional agency - the ability to allocate one's attention according to personal desires, goals, and intentions - on digital platforms. Platforms extend people's limited powers of attention by extrapolating their preferences to large collections of previously unconsidered informational objects. However, platforms typically also allow people to influence one another's attention. We introduce a formal framework for measuring how much a given platform empowers people to both pull information into their own attentional field and push information into the attentional fields of others. We also use these definitions to shed light on the implications of generative foundation models, which enable users to bypass the implicit \"attentional bargain\" that underlies embedded advertising and other methods for capturing economic value from informational goods. We conclude with a set of policy strategies that can be used to understand and reshape the distribution of attentional agency online.","sentences":["We propose a framework for measuring attentional agency - the ability to allocate one's attention according to personal desires, goals, and intentions - on digital platforms.","Platforms extend people's limited powers of attention by extrapolating their preferences to large collections of previously unconsidered informational objects.","However, platforms typically also allow people to influence one another's attention.","We introduce a formal framework for measuring how much a given platform empowers people to both pull information into their own attentional field and push information into the attentional fields of others.","We also use these definitions to shed light on the implications of generative foundation models, which enable users to bypass the implicit \"attentional bargain\" that underlies embedded advertising and other methods for capturing economic value from informational goods.","We conclude with a set of policy strategies that can be used to understand and reshape the distribution of attentional agency online."],"url":"http://arxiv.org/abs/2405.14614v1","category":"cs.CY"}
{"created":"2024-05-23 14:24:23","title":"Explaining Multi-modal Large Language Models by Analyzing their Vision Perception","abstract":"Multi-modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and generating content across various modalities, such as images and text. However, their interpretability remains a challenge, hindering their adoption in critical applications. This research proposes a novel approach to enhance the interpretability of MLLMs by focusing on the image embedding component. We combine an open-world localization model with a MLLM, thus creating a new architecture able to simultaneously produce text and object localization outputs from the same vision embedding. The proposed architecture greatly promotes interpretability, enabling us to design a novel saliency map to explain any output token, to identify model hallucinations, and to assess model biases through semantic adversarial perturbations.","sentences":["Multi-modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and generating content across various modalities, such as images and text.","However, their interpretability remains a challenge, hindering their adoption in critical applications.","This research proposes a novel approach to enhance the interpretability of MLLMs by focusing on the image embedding component.","We combine an open-world localization model with a MLLM, thus creating a new architecture able to simultaneously produce text and object localization outputs from the same vision embedding.","The proposed architecture greatly promotes interpretability, enabling us to design a novel saliency map to explain any output token, to identify model hallucinations, and to assess model biases through semantic adversarial perturbations."],"url":"http://arxiv.org/abs/2405.14612v1","category":"cs.CV"}
{"created":"2024-05-23 14:24:11","title":"Is the EJRA proportionate and therefore justified? A critical review of the EJRA policy at Cambridge","abstract":"This paper critically evaluates the HESA (Higher Education Statistics Agency) Data Report for the Employer Justified Retirement Age (EJRA) Review Group at the University of Cambridge (Cambridge 2024), identifying significant methodological flaws and misinterpretations. Our analysis reveals issues such as application of data filters only to the Cambridge sample, inconsistent variable treatment, and erroneous statistical conclusions. The Report suggests EJRA increased job creation rates at Cambridge, but we show Cambridge consistently had lower job creation rates for Established Academic Careers compared to other Russell Group universities, both before and after EJRA implementation in 2011. This suggests that EJRA is not a significant factor driving job creation rates. Since other universities without an EJRA exhibit higher job creation rates, this suggests job creation can be sustained without such a policy. We conclude that the EJRA did not achieve its intended goal of increasing opportunities for young academics and may have exacerbated existing disparities compared to other leading universities. We recommend EJRA be abolished at Cambridge since it does not meet its justified aims and could be viewed as unlawful age discrimination.","sentences":["This paper critically evaluates the HESA (Higher Education Statistics Agency) Data Report for the Employer Justified Retirement Age (EJRA) Review Group at the University of Cambridge (Cambridge 2024), identifying significant methodological flaws and misinterpretations.","Our analysis reveals issues such as application of data filters only to the Cambridge sample, inconsistent variable treatment, and erroneous statistical conclusions.","The Report suggests EJRA increased job creation rates at Cambridge, but we show Cambridge consistently had lower job creation rates for Established Academic Careers compared to other Russell Group universities, both before and after EJRA implementation in 2011.","This suggests that EJRA is not a significant factor driving job creation rates.","Since other universities without an EJRA exhibit higher job creation rates, this suggests job creation can be sustained without such a policy.","We conclude that the EJRA did not achieve its intended goal of increasing opportunities for young academics and may have exacerbated existing disparities compared to other leading universities.","We recommend EJRA be abolished at Cambridge since it does not meet its justified aims and could be viewed as unlawful age discrimination."],"url":"http://arxiv.org/abs/2405.14611v1","category":"econ.GN"}
{"created":"2024-05-23 14:21:35","title":"ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification","abstract":"Multivariate time series classification (MTSC) has attracted significant research attention due to its diverse real-world applications. Recently, exploiting transformers for MTSC has achieved state-of-the-art performance. However, existing methods focus on generic features, providing a comprehensive understanding of data, but they ignore class-specific features crucial for learning the representative characteristics of each class. This leads to poor performance in the case of imbalanced datasets or datasets with similar overall patterns but differing in minor class-specific details. In this paper, we propose a novel Shapelet Transformer (ShapeFormer), which comprises class-specific and generic transformer modules to capture both of these features. In the class-specific module, we introduce the discovery method to extract the discriminative subsequences of each class (i.e. shapelets) from the training set. We then propose a Shapelet Filter to learn the difference features between these shapelets and the input time series. We found that the difference feature for each shapelet contains important class-specific features, as it shows a significant distinction between its class and others. In the generic module, convolution filters are used to extract generic features that contain information to distinguish among all classes. For each module, we employ the transformer encoder to capture the correlation between their features. As a result, the combination of two transformer modules allows our model to exploit the power of both types of features, thereby enhancing the classification performance. Our experiments on 30 UEA MTSC datasets demonstrate that ShapeFormer has achieved the highest accuracy ranking compared to state-of-the-art methods. The code is available at https://github.com/xuanmay2701/shapeformer.","sentences":["Multivariate time series classification (MTSC) has attracted significant research attention due to its diverse real-world applications.","Recently, exploiting transformers for MTSC has achieved state-of-the-art performance.","However, existing methods focus on generic features, providing a comprehensive understanding of data, but they ignore class-specific features crucial for learning the representative characteristics of each class.","This leads to poor performance in the case of imbalanced datasets or datasets with similar overall patterns but differing in minor class-specific details.","In this paper, we propose a novel Shapelet Transformer (ShapeFormer), which comprises class-specific and generic transformer modules to capture both of these features.","In the class-specific module, we introduce the discovery method to extract the discriminative subsequences of each class (i.e. shapelets) from the training set.","We then propose a Shapelet Filter to learn the difference features between these shapelets and the input time series.","We found that the difference feature for each shapelet contains important class-specific features, as it shows a significant distinction between its class and others.","In the generic module, convolution filters are used to extract generic features that contain information to distinguish among all classes.","For each module, we employ the transformer encoder to capture the correlation between their features.","As a result, the combination of two transformer modules allows our model to exploit the power of both types of features, thereby enhancing the classification performance.","Our experiments on 30 UEA MTSC datasets demonstrate that ShapeFormer has achieved the highest accuracy ranking compared to state-of-the-art methods.","The code is available at https://github.com/xuanmay2701/shapeformer."],"url":"http://arxiv.org/abs/2405.14608v1","category":"cs.LG"}
{"created":"2024-05-23 14:19:52","title":"Discontinuous transition to chaos in a canonical random neural network","abstract":"We study a paradigmatic random recurrent neural network introduced by Sompolinsky, Crisanti, and Sommers (SCS). In the infinite size limit, this system exhibits a direct transition from a homogeneous rest state to chaotic behavior, with the Lyapunov exponent gradually increasing from zero. We generalize the SCS model considering odd saturating nonlinear transfer functions, beyond the usual choice $\\phi(x)=\\tanh x$. A discontinuous transition to chaos occurs whenever the slope of $\\phi$ at 0 is a local minimum (i.e.,~for $\\phi'''(0)>0$). Chaos appears out of the blue, by an attractor-repeller fold. Accordingly, the Lyapunov exponent stays away from zero at the birth of chaos.","sentences":["We study a paradigmatic random recurrent neural network introduced by Sompolinsky, Crisanti, and Sommers (SCS).","In the infinite size limit, this system exhibits a direct transition from a homogeneous rest state to chaotic behavior, with the Lyapunov exponent gradually increasing from zero.","We generalize the SCS model considering odd saturating nonlinear transfer functions, beyond the usual choice $\\phi(x)=\\tanh x$.","A discontinuous transition to chaos occurs whenever the slope of $\\phi$ at 0 is a local minimum (i.e.,~for $\\phi'''(0)>0$).","Chaos appears out of the blue, by an attractor-repeller fold.","Accordingly, the Lyapunov exponent stays away from zero at the birth of chaos."],"url":"http://arxiv.org/abs/2405.14607v1","category":"nlin.CD"}
{"created":"2024-05-23 14:19:21","title":"Logical Characterizations of Recurrent Graph Neural Networks with Reals and Floats","abstract":"In pioneering work from 2019, Barcel\\'o and coauthors identified logics that precisely match the expressive power of constant iteration-depth graph neural networks (GNNs) relative to properties definable in first-order logic. In this article, we give exact logical characterizations of recurrent GNNs in two scenarios: (1) in the setting with floating-point numbers and (2) with reals. For floats, the formalism matching recurrent GNNs is a rule-based modal logic with counting, while for reals we use a suitable infinitary modal logic, also with counting. These results give exact matches between logics and GNNs in the recurrent setting without relativising to a background logic in either case, but using some natural assumptions about floating-point arithmetic. Applying our characterizations, we also prove that, relative to graph properties definable in monadic second-order logic (MSO), our infinitary and rule-based logics are equally expressive. This implies that recurrent GNNs with reals and floats have the same expressive power over MSO-definable properties and shows that, for such properties, also recurrent GNNs with reals are characterized by a (finitary!) rule-based modal logic. In the general case, in contrast, the expressive power with floats is weaker than with reals. In addition to logic-oriented results, we also characterize recurrent GNNs, with both reals and floats, via distributed automata, drawing links to distributed computing models.","sentences":["In pioneering work from 2019, Barcel\\'o and coauthors identified logics that precisely match the expressive power of constant iteration-depth graph neural networks (GNNs) relative to properties definable in first-order logic.","In this article, we give exact logical characterizations of recurrent GNNs in two scenarios: (1) in the setting with floating-point numbers and (2) with reals.","For floats, the formalism matching recurrent GNNs is a rule-based modal logic with counting, while for reals we use a suitable infinitary modal logic, also with counting.","These results give exact matches between logics and GNNs in the recurrent setting without relativising to a background logic in either case, but using some natural assumptions about floating-point arithmetic.","Applying our characterizations, we also prove that, relative to graph properties definable in monadic second-order logic (MSO), our infinitary and rule-based logics are equally expressive.","This implies that recurrent GNNs with reals and floats have the same expressive power over MSO-definable properties and shows that, for such properties, also recurrent GNNs with reals are characterized by a (finitary!)","rule-based modal logic.","In the general case, in contrast, the expressive power with floats is weaker than with reals.","In addition to logic-oriented results, we also characterize recurrent GNNs, with both reals and floats, via distributed automata, drawing links to distributed computing models."],"url":"http://arxiv.org/abs/2405.14606v1","category":"cs.LO"}
{"created":"2024-05-23 14:17:29","title":"A Watermark for Low-entropy and Unbiased Generation in Large Language Models","abstract":"Recent advancements in large language models (LLMs) have highlighted the risk of misuse, raising concerns about accurately detecting LLM-generated content. A viable solution for the detection problem is to inject imperceptible identifiers into LLMs, known as watermarks. Previous work demonstrates that unbiased watermarks ensure unforgeability and preserve text quality by maintaining the expectation of the LLM output probability distribution. However, previous unbiased watermarking methods are impractical for local deployment because they rely on accesses to white-box LLMs and input prompts during detection. Moreover, these methods fail to provide statistical guarantees for the type II error of watermark detection. This study proposes the Sampling One Then Accepting (STA-1) method, an unbiased watermark that does not require access to LLMs nor prompts during detection and has statistical guarantees for the type II error. Moreover, we propose a novel tradeoff between watermark strength and text quality in unbiased watermarks. We show that in low-entropy scenarios, unbiased watermarks face a tradeoff between watermark strength and the risk of unsatisfactory outputs. Experimental results on low-entropy and high-entropy datasets demonstrate that STA-1 achieves text quality and watermark strength comparable to existing unbiased watermarks, with a low risk of unsatisfactory outputs. Implementation codes for this study are available online.","sentences":["Recent advancements in large language models (LLMs) have highlighted the risk of misuse, raising concerns about accurately detecting LLM-generated content.","A viable solution for the detection problem is to inject imperceptible identifiers into LLMs, known as watermarks.","Previous work demonstrates that unbiased watermarks ensure unforgeability and preserve text quality by maintaining the expectation of the LLM output probability distribution.","However, previous unbiased watermarking methods are impractical for local deployment because they rely on accesses to white-box LLMs and input prompts during detection.","Moreover, these methods fail to provide statistical guarantees for the type II error of watermark detection.","This study proposes the Sampling One Then Accepting (STA-1) method, an unbiased watermark that does not require access to LLMs nor prompts during detection and has statistical guarantees for the type II error.","Moreover, we propose a novel tradeoff between watermark strength and text quality in unbiased watermarks.","We show that in low-entropy scenarios, unbiased watermarks face a tradeoff between watermark strength and the risk of unsatisfactory outputs.","Experimental results on low-entropy and high-entropy datasets demonstrate that STA-1 achieves text quality and watermark strength comparable to existing unbiased watermarks, with a low risk of unsatisfactory outputs.","Implementation codes for this study are available online."],"url":"http://arxiv.org/abs/2405.14604v1","category":"cs.CL"}
{"created":"2024-05-23 14:16:46","title":"A FAIR and Free Prompt-based Research Assistant","abstract":"This demo will present the Research Assistant (RA) tool developed to assist with six main types of research tasks defined as standardized instruction templates, instantiated with user input, applied finally as prompts to well-known--for their sophisticated natural language processing abilities--AI tools, such as ChatGPT (https://chat.openai.com/) and Gemini (https://gemini.google.com/app). The six research tasks addressed by RA are: creating FAIR research comparisons, ideating research topics, drafting grant applications, writing scientific blogs, aiding preliminary peer reviews, and formulating enhanced literature search queries. RA's reliance on generative AI tools like ChatGPT or Gemini means the same research task assistance can be offered in any scientific discipline. We demonstrate its versatility by sharing RA outputs in Computer Science, Virology, and Climate Science, where the output with the RA tool assistance mirrored that from a domain expert who performed the same research task.","sentences":["This demo will present the Research Assistant (RA) tool developed to assist with six main types of research tasks defined as standardized instruction templates, instantiated with user input, applied finally as prompts to well-known--for their sophisticated natural language processing abilities--AI tools, such as ChatGPT (https://chat.openai.com/) and Gemini (https://gemini.google.com/app).","The six research tasks addressed by RA are: creating FAIR research comparisons, ideating research topics, drafting grant applications, writing scientific blogs, aiding preliminary peer reviews, and formulating enhanced literature search queries.","RA's reliance on generative AI tools like ChatGPT or Gemini means the same research task assistance can be offered in any scientific discipline.","We demonstrate its versatility by sharing RA outputs in Computer Science, Virology, and Climate Science, where the output with the RA tool assistance mirrored that from a domain expert who performed the same research task."],"url":"http://arxiv.org/abs/2405.14601v1","category":"cs.CL"}
{"created":"2024-05-23 14:16:44","title":"Discretization of continuous input spaces in the hippocampal autoencoder","abstract":"The hippocampus has been associated with both spatial cognition and episodic memory formation, but integrating these functions into a unified framework remains challenging. Here, we demonstrate that forming discrete memories of visual events in sparse autoencoder neurons can produce spatial tuning similar to hippocampal place cells. We then show that the resulting very high-dimensional code enables neurons to discretize and tile the underlying image space with minimal overlap. Additionally, we extend our results to the auditory domain, showing that neurons similarly tile the frequency space in an experience-dependent manner. Lastly, we show that reinforcement learning agents can effectively perform various visuo-spatial cognitive tasks using these sparse, very high-dimensional representations.","sentences":["The hippocampus has been associated with both spatial cognition and episodic memory formation, but integrating these functions into a unified framework remains challenging.","Here, we demonstrate that forming discrete memories of visual events in sparse autoencoder neurons can produce spatial tuning similar to hippocampal place cells.","We then show that the resulting very high-dimensional code enables neurons to discretize and tile the underlying image space with minimal overlap.","Additionally, we extend our results to the auditory domain, showing that neurons similarly tile the frequency space in an experience-dependent manner.","Lastly, we show that reinforcement learning agents can effectively perform various visuo-spatial cognitive tasks using these sparse, very high-dimensional representations."],"url":"http://arxiv.org/abs/2405.14600v1","category":"cs.AI"}
{"created":"2024-05-23 14:14:27","title":"Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields","abstract":"Deep learning has revolutionized the field of computer vision by introducing large scale neural networks with millions of parameters. Training these networks requires massive datasets and leads to intransparent models that can fail to generalize. At the other extreme, models designed from partial differential equations (PDEs) embed specialized domain knowledge into mathematical equations and usually rely on few manually chosen hyperparameters. This makes them transparent by construction and if designed and calibrated carefully, they can generalize well to unseen scenarios. In this paper, we show how to bring model- and data-driven approaches together by combining the explicit PDE-based approaches with convolutional neural networks to obtain the best of both worlds. We illustrate a joint architecture for the task of inpainting optical flow fields and show that the combination of model- and data-driven modeling leads to an effective architecture. Our model outperforms both fully explicit and fully data-driven baselines in terms of reconstruction quality, robustness and amount of required training data. Averaging the endpoint error across different mask densities, our method outperforms the explicit baselines by 11-27%, the GAN baseline by 47% and the Probabilisitic Diffusion baseline by 42%. With that, our method sets a new state of the art for inpainting of optical flow fields from random masks.","sentences":["Deep learning has revolutionized the field of computer vision by introducing large scale neural networks with millions of parameters.","Training these networks requires massive datasets and leads to intransparent models that can fail to generalize.","At the other extreme, models designed from partial differential equations (PDEs) embed specialized domain knowledge into mathematical equations and usually rely on few manually chosen hyperparameters.","This makes them transparent by construction and if designed and calibrated carefully, they can generalize well to unseen scenarios.","In this paper, we show how to bring model- and data-driven approaches together by combining the explicit PDE-based approaches with convolutional neural networks to obtain the best of both worlds.","We illustrate a joint architecture for the task of inpainting optical flow fields and show that the combination of model- and data-driven modeling leads to an effective architecture.","Our model outperforms both fully explicit and fully data-driven baselines in terms of reconstruction quality, robustness and amount of required training data.","Averaging the endpoint error across different mask densities, our method outperforms the explicit baselines by 11-27%, the GAN baseline by 47% and the Probabilisitic Diffusion baseline by 42%.","With that, our method sets a new state of the art for inpainting of optical flow fields from random masks."],"url":"http://arxiv.org/abs/2405.14599v1","category":"cs.CV"}
{"created":"2024-05-23 14:13:16","title":"Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation","abstract":"In recent years, with the realistic generation results and a wide range of personalized applications, diffusion-based generative models gain huge attention in both visual and audio generation areas. Compared to the considerable advancements of text2image or text2audio generation, research in audio2visual or visual2audio generation has been relatively slow. The recent audio-visual generation methods usually resort to huge large language model or composable diffusion models. Instead of designing another giant model for audio-visual generation, in this paper we take a step back showing a simple and lightweight generative transformer, which is not fully investigated in multi-modal generation, can achieve excellent results on image2audio generation. The transformer operates in the discrete audio and visual Vector-Quantized GAN space, and is trained in the mask denoising manner. After training, the classifier-free guidance could be deployed off-the-shelf achieving better performance, without any extra training or modification. Since the transformer model is modality symmetrical, it could also be directly deployed for audio2image generation and co-generation. In the experiments, we show that our simple method surpasses recent image2audio generation methods. Generated audio samples can be found at https://docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ","sentences":["In recent years, with the realistic generation results and a wide range of personalized applications, diffusion-based generative models gain huge attention in both visual and audio generation areas.","Compared to the considerable advancements of text2image or text2audio generation, research in audio2visual or visual2audio generation has been relatively slow.","The recent audio-visual generation methods usually resort to huge large language model or composable diffusion models.","Instead of designing another giant model for audio-visual generation, in this paper we take a step back showing a simple and lightweight generative transformer, which is not fully investigated in multi-modal generation, can achieve excellent results on image2audio generation.","The transformer operates in the discrete audio and visual Vector-Quantized GAN space, and is trained in the mask denoising manner.","After training, the classifier-free guidance could be deployed off-the-shelf achieving better performance, without any extra training or modification.","Since the transformer model is modality symmetrical, it could also be directly deployed for audio2image generation and co-generation.","In the experiments, we show that our simple method surpasses recent image2audio generation methods.","Generated audio samples can be found at https://docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ"],"url":"http://arxiv.org/abs/2405.14598v1","category":"cs.CV"}
{"created":"2024-05-23 14:12:58","title":"Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs","abstract":"We introduce Integer Scale, a novel post-training quantization scheme for large language models that effectively resolves the inference bottleneck in current fine-grained quantization approaches while maintaining similar accuracies. Integer Scale is a free lunch as it requires no extra calibration or fine-tuning which will otherwise incur additional costs. It can be used plug-and-play for most fine-grained quantization methods. Its integration results in at most 1.85x end-to-end speed boost over the original counterpart with comparable accuracy. Additionally, due to the orchestration of the proposed Integer Scale and fine-grained quantization, we resolved the quantization difficulty for Mixtral-8x7B and LLaMA-3 models with negligible performance degradation, and it comes with an end-to-end speed boost of 2.13x, and 2.31x compared with their FP16 versions respectively.","sentences":["We introduce Integer Scale, a novel post-training quantization scheme for large language models that effectively resolves the inference bottleneck in current fine-grained quantization approaches while maintaining similar accuracies.","Integer Scale is a free lunch as it requires no extra calibration or fine-tuning which will otherwise incur additional costs.","It can be used plug-and-play for most fine-grained quantization methods.","Its integration results in at most 1.85x end-to-end speed boost over the original counterpart with comparable accuracy.","Additionally, due to the orchestration of the proposed Integer Scale and fine-grained quantization, we resolved the quantization difficulty for Mixtral-8x7B and LLaMA-3 models with negligible performance degradation, and it comes with an end-to-end speed boost of 2.13x, and 2.31x compared with their FP16 versions respectively."],"url":"http://arxiv.org/abs/2405.14597v1","category":"cs.LG"}
{"created":"2024-05-23 14:11:03","title":"Elastic Locomotion with Mixed Second-order Differentiation","abstract":"We present a framework of elastic locomotion, which allows users to enliven an elastic body to produce interesting locomotion by prescribing its high-level kinematics. We formulate this problem as an inverse simulation problem and seek the optimal muscle activations to drive the body to complete the desired actions. We employ the interior-point method to model wide-area contacts between the body and the environment with logarithmic barrier penalties. The core of our framework is a mixed second-order differentiation algorithm. By combining both analytic differentiation and numerical differentiation modalities, a general-purpose second-order differentiation scheme is made possible. Specifically, we augment complex-step finite difference (CSFD) with reverse automatic differentiation (AD). We treat AD as a generic function, mapping a computing procedure to its derivative w.r.t. output loss, and promote CSFD along the AD computation. To this end, we carefully implement all the arithmetics used in elastic locomotion, from elementary functions to linear algebra and matrix operation for CSFD promotion. With this novel differentiation tool, elastic locomotion can directly exploit Newton's method and use its strong second-order convergence to find the needed activations at muscle fibers. This is not possible with existing first-order inverse or differentiable simulation techniques. We showcase a wide range of interesting locomotions of soft bodies and creatures to validate our method.","sentences":["We present a framework of elastic locomotion, which allows users to enliven an elastic body to produce interesting locomotion by prescribing its high-level kinematics.","We formulate this problem as an inverse simulation problem and seek the optimal muscle activations to drive the body to complete the desired actions.","We employ the interior-point method to model wide-area contacts between the body and the environment with logarithmic barrier penalties.","The core of our framework is a mixed second-order differentiation algorithm.","By combining both analytic differentiation and numerical differentiation modalities, a general-purpose second-order differentiation scheme is made possible.","Specifically, we augment complex-step finite difference (CSFD) with reverse automatic differentiation (AD).","We treat AD as a generic function, mapping a computing procedure to its derivative w.r.t.","output loss, and promote CSFD along the AD computation.","To this end, we carefully implement all the arithmetics used in elastic locomotion, from elementary functions to linear algebra and matrix operation for CSFD promotion.","With this novel differentiation tool, elastic locomotion can directly exploit Newton's method and use its strong second-order convergence to find the needed activations at muscle fibers.","This is not possible with existing first-order inverse or differentiable simulation techniques.","We showcase a wide range of interesting locomotions of soft bodies and creatures to validate our method."],"url":"http://arxiv.org/abs/2405.14595v1","category":"cs.GR"}
{"created":"2024-05-23 14:00:26","title":"Top-Down Partitioning for Efficient List-Wise Ranking","abstract":"Large Language Models (LLMs) have significantly impacted many facets of natural language processing and information retrieval. Unlike previous encoder-based approaches, the enlarged context window of these generative models allows for ranking multiple documents at once, commonly called list-wise ranking. However, there are still limits to the number of documents that can be ranked in a single inference of the model, leading to the broad adoption of a sliding window approach to identify the k most relevant items in a ranked list. We argue that the sliding window approach is not well-suited for list-wise re-ranking because it (1) cannot be parallelized in its current form, (2) leads to redundant computational steps repeatedly re-scoring the best set of documents as it works its way up the initial ranking, and (3) prioritizes the lowest-ranked documents for scoring rather than the highest-ranked documents by taking a bottom-up approach. Motivated by these shortcomings and an initial study that shows list-wise rankers are biased towards relevant documents at the start of their context window, we propose a novel algorithm that partitions a ranking to depth k and processes documents top-down. Unlike sliding window approaches, our algorithm is inherently parallelizable due to the use of a pivot element, which can be compared to documents down to an arbitrary depth concurrently. In doing so, we reduce the number of expected inference calls by around 33% when ranking at depth 100 while matching the performance of prior approaches across multiple strong re-rankers.","sentences":["Large Language Models (LLMs) have significantly impacted many facets of natural language processing and information retrieval.","Unlike previous encoder-based approaches, the enlarged context window of these generative models allows for ranking multiple documents at once, commonly called list-wise ranking.","However, there are still limits to the number of documents that can be ranked in a single inference of the model, leading to the broad adoption of a sliding window approach to identify the k most relevant items in a ranked list.","We argue that the sliding window approach is not well-suited for list-wise re-ranking because it (1) cannot be parallelized in its current form, (2) leads to redundant computational steps repeatedly re-scoring the best set of documents as it works its way up the initial ranking, and (3) prioritizes the lowest-ranked documents for scoring rather than the highest-ranked documents by taking a bottom-up approach.","Motivated by these shortcomings and an initial study that shows list-wise rankers are biased towards relevant documents at the start of their context window, we propose a novel algorithm that partitions a ranking to depth k and processes documents top-down.","Unlike sliding window approaches, our algorithm is inherently parallelizable due to the use of a pivot element, which can be compared to documents down to an arbitrary depth concurrently.","In doing so, we reduce the number of expected inference calls by around 33% when ranking at depth 100 while matching the performance of prior approaches across multiple strong re-rankers."],"url":"http://arxiv.org/abs/2405.14589v1","category":"cs.IR"}
{"created":"2024-05-23 13:55:11","title":"SE3D: A Framework For Saliency Method Evaluation In 3D Imaging","abstract":"For more than a decade, deep learning models have been dominating in various 2D imaging tasks. Their application is now extending to 3D imaging, with 3D Convolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and CT scans, with significant implications for fields such as autonomous driving and medical imaging. In these critical settings, explaining the model's decisions is fundamental. Despite recent advances in Explainable Artificial Intelligence, however, little effort has been devoted to explaining 3D CNNs, and many works explain these models via inadequate extensions of 2D saliency methods.   One fundamental limitation to the development of 3D saliency methods is the lack of a benchmark to quantitatively assess them on 3D data. To address this issue, we propose SE3D: a framework for Saliency method Evaluation in 3D imaging. We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and evaluation metrics to assess saliency methods for 3D CNNs. We evaluate both state-of-the-art saliency methods designed for 3D data and extensions of popular 2D saliency methods to 3D. Our experiments show that 3D saliency methods do not provide explanations of sufficient quality, and that there is margin for future improvements and safer applications of 3D CNNs in critical fields.","sentences":["For more than a decade, deep learning models have been dominating in various 2D imaging tasks.","Their application is now extending to 3D imaging, with 3D Convolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and CT scans, with significant implications for fields such as autonomous driving and medical imaging.","In these critical settings, explaining the model's decisions is fundamental.","Despite recent advances in Explainable Artificial Intelligence, however, little effort has been devoted to explaining 3D CNNs, and many works explain these models via inadequate extensions of 2D saliency methods.   ","One fundamental limitation to the development of 3D saliency methods is the lack of a benchmark to quantitatively assess them on 3D data.","To address this issue, we propose SE3D: a framework for Saliency method Evaluation in 3D imaging.","We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and evaluation metrics to assess saliency methods for 3D CNNs.","We evaluate both state-of-the-art saliency methods designed for 3D data and extensions of popular 2D saliency methods to 3D.","Our experiments show that 3D saliency methods do not provide explanations of sufficient quality, and that there is margin for future improvements and safer applications of 3D CNNs in critical fields."],"url":"http://arxiv.org/abs/2405.14584v1","category":"cs.CV"}
{"created":"2024-05-23 13:53:50","title":"PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible Poses","abstract":"In this paper, we introduce PoseCrafter, a one-shot method for personalized video generation following the control of flexible poses. Built upon Stable Diffusion and ControlNet, we carefully design an inference process to produce high-quality videos without the corresponding ground-truth frames. First, we select an appropriate reference frame from the training video and invert it to initialize all latent variables for generation. Then, we insert the corresponding training pose into the target pose sequences to enhance faithfulness through a trained temporal attention module. Furthermore, to alleviate the face and hand degradation resulting from discrepancies between poses of training videos and inference poses, we implement simple latent editing through an affine transformation matrix involving facial and hand landmarks. Extensive experiments on several datasets demonstrate that PoseCrafter achieves superior results to baselines pre-trained on a vast collection of videos under 8 commonly used metrics. Besides, PoseCrafter can follow poses from different individuals or artificial edits and simultaneously retain the human identity in an open-domain training video.","sentences":["In this paper, we introduce PoseCrafter, a one-shot method for personalized video generation following the control of flexible poses.","Built upon Stable Diffusion and ControlNet, we carefully design an inference process to produce high-quality videos without the corresponding ground-truth frames.","First, we select an appropriate reference frame from the training video and invert it to initialize all latent variables for generation.","Then, we insert the corresponding training pose into the target pose sequences to enhance faithfulness through a trained temporal attention module.","Furthermore, to alleviate the face and hand degradation resulting from discrepancies between poses of training videos and inference poses, we implement simple latent editing through an affine transformation matrix involving facial and hand landmarks.","Extensive experiments on several datasets demonstrate that PoseCrafter achieves superior results to baselines pre-trained on a vast collection of videos under 8 commonly used metrics.","Besides, PoseCrafter can follow poses from different individuals or artificial edits and simultaneously retain the human identity in an open-domain training video."],"url":"http://arxiv.org/abs/2405.14582v1","category":"cs.CV"}
{"created":"2024-05-23 13:53:17","title":"LDM: Large Tensorial SDF Model for Textured Mesh Generation","abstract":"Previous efforts have managed to generate production-ready 3D assets from text or images. However, these methods primarily employ NeRF or 3D Gaussian representations, which are not adept at producing smooth, high-quality geometries required by modern rendering pipelines. In this paper, we propose LDM, a novel feed-forward framework capable of generating high-fidelity, illumination-decoupled textured mesh from a single image or text prompts. We firstly utilize a multi-view diffusion model to generate sparse multi-view inputs from single images or text prompts, and then a transformer-based model is trained to predict a tensorial SDF field from these sparse multi-view image inputs. Finally, we employ a gradient-based mesh optimization layer to refine this model, enabling it to produce an SDF field from which high-quality textured meshes can be extracted. Extensive experiments demonstrate that our method can generate diverse, high-quality 3D mesh assets with corresponding decomposed RGB textures within seconds.","sentences":["Previous efforts have managed to generate production-ready 3D assets from text or images.","However, these methods primarily employ NeRF or 3D Gaussian representations, which are not adept at producing smooth, high-quality geometries required by modern rendering pipelines.","In this paper, we propose LDM, a novel feed-forward framework capable of generating high-fidelity, illumination-decoupled textured mesh from a single image or text prompts.","We firstly utilize a multi-view diffusion model to generate sparse multi-view inputs from single images or text prompts, and then a transformer-based model is trained to predict a tensorial SDF field from these sparse multi-view image inputs.","Finally, we employ a gradient-based mesh optimization layer to refine this model, enabling it to produce an SDF field from which high-quality textured meshes can be extracted.","Extensive experiments demonstrate that our method can generate diverse, high-quality 3D mesh assets with corresponding decomposed RGB textures within seconds."],"url":"http://arxiv.org/abs/2405.14580v1","category":"cs.GR"}
{"created":"2024-05-23 13:51:55","title":"Representation noising effectively prevents harmful fine-tuning on LLMs","abstract":"Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes. Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning. In this work, we propose Representation Noising (RepNoise), a defence mechanism that is effective even when attackers have access to the weights and the defender no longer has any control. RepNoise works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the effectiveness of our defence lies in its \"depth\": the degree to which information about harmful representations is removed across all layers of the LLM.","sentences":["Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes.","Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs).","While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning.","In this work, we propose Representation Noising (RepNoise), a defence mechanism that is effective even when attackers have access to the weights and the defender no longer has any control.","RepNoise works by removing information about harmful representations such that it is difficult to recover them during fine-tuning.","Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process.","Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks.","We provide empirical evidence that the effectiveness of our defence lies in its \"depth\": the degree to which information about harmful representations is removed across all layers of the LLM."],"url":"http://arxiv.org/abs/2405.14577v1","category":"cs.CL"}
{"created":"2024-05-23 13:50:04","title":"Finding bifurcations in mathematical epidemiology via reaction network methods","abstract":"Mathematical Epidemiology (ME) shares with Chemical Reaction Network Theory (CRNT) the basic mathematical structure of its dynamical systems. Despite this central similarity, methods from CRNT have been seldom applied to solving problems in ME. We explore here the applicability of CRNT methods to find bifurcations at endemic equilibria of ME models.   In particular, we adapt three CRNT methods to the features of ME. Firstly, we prove that essentially all ME models admit Hopf bifurcations for certain monotone choices of the interaction functions. Secondly, we offer a parametrization of equilibria Jacobians of ME systems where few interactions are not in mass action form. Thirdly, we show that periodic oscillations in closed systems imply periodic oscillations when demography is added.   Finally, we apply such results to two families of networks: a general SIR model with a nonlinear treatment rate and a recent SIRnS model with a gradual increase in infectiousness. We give necessary and sufficient conditions for the occurrence of bifurcations at endemic equilibria of both families.","sentences":["Mathematical Epidemiology (ME) shares with Chemical Reaction Network Theory (CRNT) the basic mathematical structure of its dynamical systems.","Despite this central similarity, methods from CRNT have been seldom applied to solving problems in ME.","We explore here the applicability of CRNT methods to find bifurcations at endemic equilibria of ME models.   ","In particular, we adapt three CRNT methods to the features of ME.","Firstly, we prove that essentially all ME models admit Hopf bifurcations for certain monotone choices of the interaction functions.","Secondly, we offer a parametrization of equilibria Jacobians of ME systems where few interactions are not in mass action form.","Thirdly, we show that periodic oscillations in closed systems imply periodic oscillations when demography is added.   ","Finally, we apply such results to two families of networks: a general SIR model with a nonlinear treatment rate and a recent SIRnS model with a gradual increase in infectiousness.","We give necessary and sufficient conditions for the occurrence of bifurcations at endemic equilibria of both families."],"url":"http://arxiv.org/abs/2405.14576v1","category":"math.DS"}
{"created":"2024-05-23 13:50:01","title":"Share-Based Fairness for Arbitrary Entitlements","abstract":"We consider the problem of fair allocation of indivisible items to agents that have arbitrary entitlements to the items. Every agent $i$ has a valuation function $v_i$ and an entitlement $b_i$, where entitlements sum up to~1. Which allocation should one choose in situations in which agents fail to agree on one acceptable fairness notion? We study this problem in the case in which each agent focuses on the value she gets, and fairness notions are restricted to be {\\em share based}. A {\\em share} $s$ is an function that maps every $(v_i,b_i)$ to a value $s(v_i,b_i)$, representing the minimal value $i$ should get, and $s$ is {\\em feasible} if it is always possible to give every agent $i$ value of at least $s(v_i,b_i)$.   Our main result is that for additive valuations over goods there is an allocation that gives every agent at least half her share value, regardless of which feasible share-based fairness notion the agent wishes to use. Moreover, the ratio of half is best possible. More generally, we provide tight characterizations of what can be achieved, both ex-post (as single allocations) and ex-ante (as expected values of distributions of allocations), both for goods and for chores. We also show that for chores one can achieve the ex-ante and ex-post guarantees simultaneously (a ``best of both world\" result), whereas for goods one cannot.","sentences":["We consider the problem of fair allocation of indivisible items to agents that have arbitrary entitlements to the items.","Every agent $i$ has a valuation function $v_i$ and an entitlement $b_i$, where entitlements sum up to~1.","Which allocation should one choose in situations in which agents fail to agree on one acceptable fairness notion?","We study this problem in the case in which each agent focuses on the value she gets, and fairness notions are restricted to be {\\em share based}.","A {\\em share} $s$ is an function that maps every $(v_i,b_i)$ to a value $s(v_i,b_i)$, representing the minimal value $i$ should get, and $s$ is {\\em feasible} if it is always possible to give every agent $i$ value of at least $s(v_i,b_i)$.   Our main result is that for additive valuations over goods there is an allocation that gives every agent at least half her share value, regardless of which feasible share-based fairness notion the agent wishes to use.","Moreover, the ratio of half is best possible.","More generally, we provide tight characterizations of what can be achieved, both ex-post (as single allocations) and ex-ante (as expected values of distributions of allocations), both for goods and for chores.","We also show that for chores one can achieve the ex-ante and ex-post guarantees simultaneously (a ``best of both world\" result), whereas for goods one cannot."],"url":"http://arxiv.org/abs/2405.14575v1","category":"cs.GT"}
{"created":"2024-05-23 13:49:37","title":"Learning with Fitzpatrick Losses","abstract":"Fenchel-Young losses are a family of convex loss functions, encompassing the squared, logistic and sparsemax losses, among others. Each Fenchel-Young loss is implicitly associated with a link function, for mapping model outputs to predictions. For instance, the logistic loss is associated with the soft argmax link function. Can we build new loss functions associated with the same link function as Fenchel-Young losses? In this paper, we introduce Fitzpatrick losses, a new family of convex loss functions based on the Fitzpatrick function. A well-known theoretical tool in maximal monotone operator theory, the Fitzpatrick function naturally leads to a refined Fenchel-Young inequality, making Fitzpatrick losses tighter than Fenchel-Young losses, while maintaining the same link function for prediction. As an example, we introduce the Fitzpatrick logistic loss and the Fitzpatrick sparsemax loss, counterparts of the logistic and the sparsemax losses. This yields two new tighter losses associated with the soft argmax and the sparse argmax, two of the most ubiquitous output layers used in machine learning. We study in details the properties of Fitzpatrick losses and in particular, we show that they can be seen as Fenchel-Young losses using a modified, target-dependent generating function. We demonstrate the effectiveness of Fitzpatrick losses for label proportion estimation.","sentences":["Fenchel-Young losses are a family of convex loss functions, encompassing the squared, logistic and sparsemax losses, among others.","Each Fenchel-Young loss is implicitly associated with a link function, for mapping model outputs to predictions.","For instance, the logistic loss is associated with the soft argmax link function.","Can we build new loss functions associated with the same link function as Fenchel-Young losses?","In this paper, we introduce Fitzpatrick losses, a new family of convex loss functions based on the Fitzpatrick function.","A well-known theoretical tool in maximal monotone operator theory, the Fitzpatrick function naturally leads to a refined Fenchel-Young inequality, making Fitzpatrick losses tighter than Fenchel-Young losses, while maintaining the same link function for prediction.","As an example, we introduce the Fitzpatrick logistic loss and the Fitzpatrick sparsemax loss, counterparts of the logistic and the sparsemax losses.","This yields two new tighter losses associated with the soft argmax and the sparse argmax, two of the most ubiquitous output layers used in machine learning.","We study in details the properties of Fitzpatrick losses and in particular, we show that they can be seen as Fenchel-Young losses using a modified, target-dependent generating function.","We demonstrate the effectiveness of Fitzpatrick losses for label proportion estimation."],"url":"http://arxiv.org/abs/2405.14574v1","category":"stat.ML"}
{"created":"2024-05-23 13:48:54","title":"AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents","abstract":"Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. Yet, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functioning Android environment that provides reward signals for 116 programmatic task workflows across 20 real world Android applications. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and realistic suite of tasks. Reward signals are derived from the computer's system state, making them durable across task variations and extensible across different apps. To demonstrate AndroidWorld's benefits and mode of operation, we introduce a new computer control agent, M3A. M3A can complete 30.6% of the AndroidWorld's tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-domain agents. Finally, we conduct a robustness analysis by testing M3A against a range of task variations on a representative subset of tasks, demonstrating that variations in task parameters can significantly alter the complexity of a task and therefore an agent's performance, highlighting the importance of testing agents under diverse conditions. AndroidWorld and the experiments in this paper are available at https://github.com/google-research/android_world.","sentences":["Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility.","Yet, progress in this field will be driven by realistic and reproducible benchmarks.","We present AndroidWorld, a fully functioning Android environment that provides reward signals for 116 programmatic task workflows across 20 real world Android applications.","Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and realistic suite of tasks.","Reward signals are derived from the computer's system state, making them durable across task variations and extensible across different apps.","To demonstrate AndroidWorld's benefits and mode of operation, we introduce a new computer control agent, M3A. M3A can complete 30.6% of the AndroidWorld's tasks, leaving ample room for future work.","Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-domain agents.","Finally, we conduct a robustness analysis by testing M3A against a range of task variations on a representative subset of tasks, demonstrating that variations in task parameters can significantly alter the complexity of a task and therefore an agent's performance, highlighting the importance of testing agents under diverse conditions.","AndroidWorld and the experiments in this paper are available at https://github.com/google-research/android_world."],"url":"http://arxiv.org/abs/2405.14573v1","category":"cs.AI"}
{"created":"2024-05-23 13:46:25","title":"A general method for the development of constrained codes","abstract":"Nowadays there are several classes of constrained codes intended for different applications. The following two large classes can be distinguished. The first class contains codes with local constraints; for example, the source data must be encoded by binary sequences containing no sub-words 00 and 111. The second class contains codes with global constraints; for example, the code-words must be binary sequences of certain even length with half zeros and half ones.It is important to note that often the necessary codes must fulfill some requirements of both classes.   In this paper we propose a general polynomial complexity method for constructing codes for both classes, as well as for combinations thereof. The proposed method uses the enumerative Cover's code, but the main difference between known applications of this code is that the known algorithms require the use of combinatorial formulae when applied, whereas the proposed method calculates all parameters on-the-fly using a polynomial complexity algorithm.","sentences":["Nowadays there are several classes of constrained codes intended for different applications.","The following two large classes can be distinguished.","The first class contains codes with local constraints; for example, the source data must be encoded by binary sequences containing no sub-words 00 and 111.","The second class contains codes with global constraints; for example, the code-words must be binary sequences of certain even length with half zeros and half ones.","It is important to note that often the necessary codes must fulfill some requirements of both classes.   ","In this paper we propose a general polynomial complexity method for constructing codes for both classes, as well as for combinations thereof.","The proposed method uses the enumerative Cover's code, but the main difference between known applications of this code is that the known algorithms require the use of combinatorial formulae when applied, whereas the proposed method calculates all parameters on-the-fly using a polynomial complexity algorithm."],"url":"http://arxiv.org/abs/2405.14570v1","category":"cs.IT"}
{"created":"2024-05-23 13:44:48","title":"PrivCirNet: Efficient Private Inference via Block Circulant Transformation","abstract":"Homomorphic encryption (HE)-based deep neural network (DNN) inference protects data and model privacy but suffers from significant computation overhead. We observe transforming the DNN weights into circulant matrices converts general matrix-vector multiplications into HE-friendly 1-dimensional convolutions, drastically reducing the HE computation cost. Hence, in this paper, we propose \\method, a protocol/network co-optimization framework based on block circulant transformation. At the protocol level, PrivCirNet customizes the HE encoding algorithm that is fully compatible with the block circulant transformation and reduces the computation latency in proportion to the block size. At the network level, we propose a latency-aware formulation to search for the layer-wise block size assignment based on second-order information. PrivCirNet also leverages layer fusion to further reduce the inference cost. We compare PrivCirNet with the state-of-the-art HE-based framework Bolt (IEEE S\\&P 2024) and the HE-friendly pruning method SpENCNN (ICML 2023). For ResNet-18 and Vision Transformer (ViT) on Tiny ImageNet, PrivCirNet reduces latency by $5.0\\times$ and $1.3\\times$ with iso-accuracy over Bolt, respectively, and improves accuracy by $4.1\\%$ and $12\\%$ over SpENCNN, respectively. For MobileNetV2 on ImageNet, PrivCirNet achieves $1.7\\times$ lower latency and $4.2\\%$ better accuracy over Bolt and SpENCNN, respectively. Our code and checkpoints are available in the supplementary materials.","sentences":["Homomorphic encryption (HE)-based deep neural network (DNN) inference protects data and model privacy but suffers from significant computation overhead.","We observe transforming the DNN weights into circulant matrices converts general matrix-vector multiplications into HE-friendly 1-dimensional convolutions, drastically reducing the HE computation cost.","Hence, in this paper, we propose \\method, a protocol/network co-optimization framework based on block circulant transformation.","At the protocol level, PrivCirNet customizes the HE encoding algorithm that is fully compatible with the block circulant transformation and reduces the computation latency in proportion to the block size.","At the network level, we propose a latency-aware formulation to search for the layer-wise block size assignment based on second-order information.","PrivCirNet also leverages layer fusion to further reduce the inference cost.","We compare PrivCirNet with the state-of-the-art HE-based framework Bolt (IEEE S\\&P 2024) and the HE-friendly pruning method SpENCNN (ICML 2023).","For ResNet-18 and Vision Transformer (ViT) on Tiny ImageNet, PrivCirNet reduces latency by $5.0\\times$ and $1.3\\times$ with iso-accuracy over Bolt, respectively, and improves accuracy by $4.1\\%$ and $12\\%$ over SpENCNN, respectively.","For MobileNetV2 on ImageNet, PrivCirNet achieves $1.7\\times$ lower latency and $4.2\\%$ better accuracy over Bolt and SpENCNN, respectively.","Our code and checkpoints are available in the supplementary materials."],"url":"http://arxiv.org/abs/2405.14569v1","category":"cs.CR"}
{"created":"2024-05-23 13:43:51","title":"Tug-of-war games related to oblique derivative boundary value problems with the normalized $p$-Laplacian","abstract":"In this paper, we are concerned with game-theoretic interpretations to the following oblique derivative boundary value problem \\begin{align*} \\left\\{ \\begin{array}{ll} \\Delta_{p}^{N}u=0 & \\textrm{in $ \\Omega$,}\\\\ \\langle \\beta , Du \\rangle + \\gamma u = \\gamma G & \\textrm{on $ \\partial \\Omega$,}\\\\ \\end{array} \\right. \\end{align*} where $\\Delta_{p}^{N}$ is the normalized $p$-Laplacian. This problem can be regarded as a generalized version of the Robin boundary value problem for the Laplace equations. We construct several types of stochastic games associated with this problem by using `shrinking tug-of-war'. For the value functions of such games, we investigate the properties such as existence, uniqueness, regularity and convergence.","sentences":["In this paper, we are concerned with game-theoretic interpretations to the following oblique derivative boundary value problem \\begin{align*} \\left\\{ \\begin{array}{ll} \\Delta_{p}^{N}u=0 & \\textrm{in $ \\Omega$,}\\\\ \\langle \\beta , Du \\rangle + \\gamma u = \\gamma G & \\textrm{on $ \\partial \\Omega$,}\\\\ \\end{array} \\right.","\\end{align*} where $\\Delta_{p}^{N}$ is the normalized $p$-Laplacian.","This problem can be regarded as a generalized version of the Robin boundary value problem for the Laplace equations.","We construct several types of stochastic games associated with this problem by using `shrinking tug-of-war'.","For the value functions of such games, we investigate the properties such as existence, uniqueness, regularity and convergence."],"url":"http://arxiv.org/abs/2405.14568v1","category":"math.AP"}
{"created":"2024-05-23 13:43:29","title":"EHRMamba: Towards Generalizable and Scalable Foundation Models for Electronic Health Records","abstract":"Transformers have significantly advanced the modeling of Electronic Health Records (EHR), yet their deployment in real-world healthcare is limited by several key challenges. Firstly, the quadratic computational cost and insufficient context length of these models pose significant obstacles for hospitals in processing the extensive medical histories typical in EHR data. Additionally, existing models employ separate finetuning for each clinical task, complicating maintenance in healthcare environments. Moreover, these models focus exclusively on either clinical prediction or EHR forecasting, lacking the flexibility to perform well across both. To overcome these limitations, we introduce EHRMamba, a robust foundation model built on the Mamba architecture. EHRMamba can process sequences up to four times longer than previous models due to its linear computational cost. We also introduce a novel approach to Multitask Prompted Finetuning (MTF) for EHR data, which enables EHRMamba to simultaneously learn multiple clinical tasks in a single finetuning phase, significantly enhancing deployment and cross-task generalization. Furthermore, our model leverages the HL7 FHIR data standard to simplify integration into existing hospital systems. Alongside EHRMamba, we open-source Odyssey, a toolkit designed to support the development and deployment of EHR foundation models, with an emphasis on data standardization and interpretability. Our evaluations on the MIMIC-IV dataset demonstrate that EHRMamba advances state-of-the-art performance across 6 major clinical tasks and excels in EHR forecasting, marking a significant leap forward in the field.","sentences":["Transformers have significantly advanced the modeling of Electronic Health Records (EHR), yet their deployment in real-world healthcare is limited by several key challenges.","Firstly, the quadratic computational cost and insufficient context length of these models pose significant obstacles for hospitals in processing the extensive medical histories typical in EHR data.","Additionally, existing models employ separate finetuning for each clinical task, complicating maintenance in healthcare environments.","Moreover, these models focus exclusively on either clinical prediction or EHR forecasting, lacking the flexibility to perform well across both.","To overcome these limitations, we introduce EHRMamba, a robust foundation model built on the Mamba architecture.","EHRMamba can process sequences up to four times longer than previous models due to its linear computational cost.","We also introduce a novel approach to Multitask Prompted Finetuning (MTF) for EHR data, which enables EHRMamba to simultaneously learn multiple clinical tasks in a single finetuning phase, significantly enhancing deployment and cross-task generalization.","Furthermore, our model leverages the HL7 FHIR data standard to simplify integration into existing hospital systems.","Alongside EHRMamba, we open-source Odyssey, a toolkit designed to support the development and deployment of EHR foundation models, with an emphasis on data standardization and interpretability.","Our evaluations on the MIMIC-IV dataset demonstrate that EHRMamba advances state-of-the-art performance across 6 major clinical tasks and excels in EHR forecasting, marking a significant leap forward in the field."],"url":"http://arxiv.org/abs/2405.14567v1","category":"cs.LG"}
{"created":"2024-05-23 13:43:01","title":"Task-Based Design and Policy Co-Optimization for Tendon-driven Underactuated Kinematic Chains","abstract":"Underactuated manipulators reduce the number of bulky motors, thereby enabling compact and mechanically robust designs. However, fewer actuators than joints means that the manipulator can only access a specific manifold within the joint space, which is particular to a given hardware configuration and can be low-dimensional and/or discontinuous. Determining an appropriate set of hardware parameters for this class of mechanisms, therefore, is difficult - even for traditional task-based co-optimization methods. In this paper, our goal is to implement a task-based design and policy co-optimization method for underactuated, tendon-driven manipulators. We first formulate a general model for an underactuated, tendon-driven transmission. We then use this model to co-optimize a three-link, two-actuator kinematic chain using reinforcement learning. We demonstrate that our optimized tendon transmission and control policy can be transferred reliably to physical hardware with real-world reaching experiments.","sentences":["Underactuated manipulators reduce the number of bulky motors, thereby enabling compact and mechanically robust designs.","However, fewer actuators than joints means that the manipulator can only access a specific manifold within the joint space, which is particular to a given hardware configuration and can be low-dimensional and/or discontinuous.","Determining an appropriate set of hardware parameters for this class of mechanisms, therefore, is difficult - even for traditional task-based co-optimization methods.","In this paper, our goal is to implement a task-based design and policy co-optimization method for underactuated, tendon-driven manipulators.","We first formulate a general model for an underactuated, tendon-driven transmission.","We then use this model to co-optimize a three-link, two-actuator kinematic chain using reinforcement learning.","We demonstrate that our optimized tendon transmission and control policy can be transferred reliably to physical hardware with real-world reaching experiments."],"url":"http://arxiv.org/abs/2405.14566v1","category":"cs.RO"}
{"created":"2024-05-23 13:41:17","title":"Concept Visualization: Explaining the CLIP Multi-modal Embedding Using WordNet","abstract":"Advances in multi-modal embeddings, and in particular CLIP, have recently driven several breakthroughs in Computer Vision (CV). CLIP has shown impressive performance on a variety of tasks, yet, its inherently opaque architecture may hinder the application of models employing CLIP as backbone, especially in fields where trust and model explainability are imperative, such as in the medical domain. Current explanation methodologies for CV models rely on Saliency Maps computed through gradient analysis or input perturbation. However, these Saliency Maps can only be computed to explain classes relevant to the end task, often smaller in scope than the backbone training classes. In the context of models implementing CLIP as their vision backbone, a substantial portion of the information embedded within the learned representations is thus left unexplained.   In this work, we propose Concept Visualization (ConVis), a novel saliency methodology that explains the CLIP embedding of an image by exploiting the multi-modal nature of the embeddings. ConVis makes use of lexical information from WordNet to compute task-agnostic Saliency Maps for any concept, not limited to concepts the end model was trained on. We validate our use of WordNet via an out of distribution detection experiment, and test ConVis on an object localization benchmark, showing that Concept Visualizations correctly identify and localize the image's semantic content. Additionally, we perform a user study demonstrating that our methodology can give users insight on the model's functioning.","sentences":["Advances in multi-modal embeddings, and in particular CLIP, have recently driven several breakthroughs in Computer Vision (CV).","CLIP has shown impressive performance on a variety of tasks, yet, its inherently opaque architecture may hinder the application of models employing CLIP as backbone, especially in fields where trust and model explainability are imperative, such as in the medical domain.","Current explanation methodologies for CV models rely on Saliency Maps computed through gradient analysis or input perturbation.","However, these Saliency Maps can only be computed to explain classes relevant to the end task, often smaller in scope than the backbone training classes.","In the context of models implementing CLIP as their vision backbone, a substantial portion of the information embedded within the learned representations is thus left unexplained.   ","In this work, we propose Concept Visualization (ConVis), a novel saliency methodology that explains the CLIP embedding of an image by exploiting the multi-modal nature of the embeddings.","ConVis makes use of lexical information from WordNet to compute task-agnostic Saliency Maps for any concept, not limited to concepts the end model was trained on.","We validate our use of WordNet via an out of distribution detection experiment, and test ConVis on an object localization benchmark, showing that Concept Visualizations correctly identify and localize the image's semantic content.","Additionally, we perform a user study demonstrating that our methodology can give users insight on the model's functioning."],"url":"http://arxiv.org/abs/2405.14563v1","category":"cs.CV"}
{"created":"2024-05-23 13:35:53","title":"Deep Learning Classification of Photoplethysmogram Signal for Hypertension Levels","abstract":"Continuous photoplethysmography (PPG)-based blood pressure monitoring is necessary for healthcare and fitness applications. In Artificial Intelligence (AI), signal classification levels with the machine and deep learning arrangements need to be explored further. Techniques based on time-frequency spectra, such as Short-time Fourier Transform (STFT), have been used to address the challenges of motion artifact correction. Therefore, the proposed study works with PPG signals of more than 200 patients (650+ signal samples) with hypertension, using STFT with various Neural Networks (Convolution Neural Network (CNN), Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (Bi-LSTM), followed by machine learning classifiers, such as, Support Vector Machine (SVM) and Random Forest (RF). The classification has been done for two categories: Prehypertension (normal levels) and Hypertension (includes Stage I and Stage II). Various performance metrics have been obtained with two batch sizes of 3 and 16 for the fusion of the neural networks. With precision and specificity of 100% and recall of 82.1%, the LSTM model provides the best results among all combinations of Neural Networks. However, the maximum accuracy of 71.9% is achieved by the LSTM-CNN model. Further stacked Ensemble method has been used to achieve 100% accuracy for Meta-LSTM-RF, Meta- LSTM-CNN-RF and Meta- STFT-CNN-SVM.","sentences":["Continuous photoplethysmography (PPG)-based blood pressure monitoring is necessary for healthcare and fitness applications.","In Artificial Intelligence (AI), signal classification levels with the machine and deep learning arrangements need to be explored further.","Techniques based on time-frequency spectra, such as Short-time Fourier Transform (STFT), have been used to address the challenges of motion artifact correction.","Therefore, the proposed study works with PPG signals of more than 200 patients (650+ signal samples) with hypertension, using STFT with various Neural Networks (Convolution Neural Network (CNN), Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (Bi-LSTM), followed by machine learning classifiers, such as, Support Vector Machine (SVM) and Random Forest (RF).","The classification has been done for two categories: Prehypertension (normal levels) and Hypertension (includes Stage I and Stage II).","Various performance metrics have been obtained with two batch sizes of 3 and 16 for the fusion of the neural networks.","With precision and specificity of 100% and recall of 82.1%, the LSTM model provides the best results among all combinations of Neural Networks.","However, the maximum accuracy of 71.9% is achieved by the LSTM-CNN model.","Further stacked Ensemble method has been used to achieve 100% accuracy for Meta-LSTM-RF, Meta- LSTM-CNN-RF and Meta- STFT-CNN-SVM."],"url":"http://arxiv.org/abs/2405.14556v1","category":"cs.CV"}
{"created":"2024-05-23 13:35:34","title":"Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models","abstract":"Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: \\textit{representative bias}, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and \\textit{affinity bias}, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.","sentences":["Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives.","This study addresses two such biases within LLMs: \\textit{representative bias}, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and \\textit{affinity bias}, reflecting the models' evaluative preferences for specific narratives or viewpoints.","We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases.","Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men.","Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'.","This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions."],"url":"http://arxiv.org/abs/2405.14555v1","category":"cs.CL"}
{"created":"2024-05-23 13:32:07","title":"UDKAG: Augmenting Large Vision-Language Models with Up-to-Date Knowledge","abstract":"Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases. For example, if a LVLM was released on January 2024, and it wouldn't know the detailed plot of the new movie Dune 2, which wasn't released until February 2024. To solve the problem, a promising solution is to provide LVLMs with up-to-date knowledge via internet search during inference, i.e., internet-augmented generation (IAG), which is already integrated in some closed-source commercial LVLMs such as GPT-4V. However, the specific mechanics underpinning them remain a mystery. In this paper, we propose a plug-and-play framework, for augmenting existing LVLMs in handling visual question answering (VQA) about up-to-date knowledge, dubbed UDKAG. A hierarchical filtering model is trained to effectively and efficiently find the most helpful content from the websites returned by a search engine to prompt LVLMs with up-to-date knowledge. To train the model and evaluate our framework's performance, we propose a pipeline to automatically generate news-related VQA samples to construct a dataset, dubbed UDK-VQA. A multi-model voting mechanism is introduced to label the usefulness of website/content for VQA samples to construct the training set. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4V by about 25% in accuracy.","sentences":["Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases.","For example, if a LVLM was released on January 2024, and it wouldn't know the detailed plot of the new movie Dune 2, which wasn't released until February 2024.","To solve the problem, a promising solution is to provide LVLMs with up-to-date knowledge via internet search during inference, i.e., internet-augmented generation (IAG), which is already integrated in some closed-source commercial LVLMs such as GPT-4V. However, the specific mechanics underpinning them remain a mystery.","In this paper, we propose a plug-and-play framework, for augmenting existing LVLMs in handling visual question answering (VQA) about up-to-date knowledge, dubbed UDKAG.","A hierarchical filtering model is trained to effectively and efficiently find the most helpful content from the websites returned by a search engine to prompt LVLMs with up-to-date knowledge.","To train the model and evaluate our framework's performance, we propose a pipeline to automatically generate news-related VQA samples to construct a dataset, dubbed UDK-VQA.","A multi-model voting mechanism is introduced to label the usefulness of website/content for VQA samples to construct the training set.","Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4V by about 25% in accuracy."],"url":"http://arxiv.org/abs/2405.14554v1","category":"cs.CV"}
{"created":"2024-05-23 13:31:53","title":"Nonvanishing and Abundance for cones of movable divisors","abstract":"Let $\\overline{\\mathrm{Mov}}^k(X)$ be the closure of the cone $\\mathrm{Mov}^k(X)$ generated by classes of effective divisors on a projective variety $X$ with stable base locus of codimension at least $k+1$. We propose a generalized version of the Log Nonvanishing Conjecture and of the Log Abundance Conjecture for a klt pair $(X,\\Delta)$, that is: if $K_X+\\Delta \\in \\overline{\\mathrm{Mov}}^{k}(X)$, then $K_X+\\Delta \\in \\mathrm{Mov}^{k}(X)$. Moreover, we prove that if the Log Minimal Model Program, the Log Nonvanishing, and the Log Abundance hold, then so does our conjecture.","sentences":["Let $\\overline{\\mathrm{Mov}}^k(X)$ be the closure of the cone $\\mathrm{Mov}^k(X)$ generated by classes of effective divisors on a projective variety $X$ with stable base locus of codimension at least $k+1$. We propose a generalized version of the Log Nonvanishing Conjecture and of the Log Abundance Conjecture for a klt pair $(X,\\Delta)$, that is: if $K_X+\\Delta \\in \\overline{\\mathrm{Mov}}^{k}(X)$, then $K_X+\\Delta \\in \\mathrm{Mov}^{k}(X)$.","Moreover, we prove that if the Log Minimal Model Program, the Log Nonvanishing, and the Log Abundance hold, then so does our conjecture."],"url":"http://arxiv.org/abs/2405.14553v1","category":"math.AG"}
{"created":"2024-05-23 13:31:38","title":"Design and Development of a Roaming Wireless Safety Emergency Stop","abstract":"Modern manufacturing is characterized by a high degree of automation, with autonomous systems also frequently being used. In such environments human intervention in the event of malfunctions or maintenance becomes a rare but also necessary task. When human workers are no longer an integral part of the production process, but only intervene when necessary, e.g., in the case of unexpected machine behavior, appropriate safety solutions will become even more important. This work describes a wireless communication system enabling a flexible and safe emergency stop function for multiple automation cells. A portable emergency stop switch allows seamless transition between different wireless cells, ensuring functional safety. The communication protocol combines IO-Link Wireless features with the safety requirements already implemented in IO-Link Safety. Security requirements are fulfilled through encryption and authentication. The IO-Link Wireless roaming functionality is used to extend the system across several manufacturing cells. An experimental setup confirms the suitability of the system for various applications. The results demonstrate the effectiveness of the handover mechanism and evaluate the potential of the system to improve flexibility, availability and security in dynamic production environments. Future extensions could include the use of AI based evaluation of the radio signals for an intelligent cell handover.","sentences":["Modern manufacturing is characterized by a high degree of automation, with autonomous systems also frequently being used.","In such environments human intervention in the event of malfunctions or maintenance becomes a rare but also necessary task.","When human workers are no longer an integral part of the production process, but only intervene when necessary, e.g., in the case of unexpected machine behavior, appropriate safety solutions will become even more important.","This work describes a wireless communication system enabling a flexible and safe emergency stop function for multiple automation cells.","A portable emergency stop switch allows seamless transition between different wireless cells, ensuring functional safety.","The communication protocol combines IO-Link Wireless features with the safety requirements already implemented in IO-Link Safety.","Security requirements are fulfilled through encryption and authentication.","The IO-Link Wireless roaming functionality is used to extend the system across several manufacturing cells.","An experimental setup confirms the suitability of the system for various applications.","The results demonstrate the effectiveness of the handover mechanism and evaluate the potential of the system to improve flexibility, availability and security in dynamic production environments.","Future extensions could include the use of AI based evaluation of the radio signals for an intelligent cell handover."],"url":"http://arxiv.org/abs/2405.14552v1","category":"eess.SY"}
{"created":"2024-05-23 13:28:10","title":"Rapid modelling of reactive transport in porous media using machine learning: limitations and solutions","abstract":"Reactive transport in porous media plays a pivotal role in subsurface reservoir processes, influencing fluid properties and geochemical characteristics. However, coupling fluid flow and transport with geochemical reactions is computationally intensive, requiring geochemical calculations at each grid cell and each time step within a discretized simulation domain. Although recent advancements have integrated machine learning techniques as surrogates for geochemical simulations, ensuring computational efficiency and accuracy remains a challenge. This chapter investigates machine learning models as replacements for a geochemical module in a reactive transport in porous media simulation. We test this approach on a well-documented cation exchange problem. While the surrogate models excel in isolated predictions, they fall short in rollout predictions over successive time steps. By introducing modifications, including physics-based constraints and tailored dataset generation strategies, we show that machine learning surrogates can achieve accurate rollout predictions. Our findings emphasize that, when judiciously designed, machine learning surrogates can substantially expedite the cation exchange problem without compromising accuracy, offering significant potential for a range of reactive transport applications.","sentences":["Reactive transport in porous media plays a pivotal role in subsurface reservoir processes, influencing fluid properties and geochemical characteristics.","However, coupling fluid flow and transport with geochemical reactions is computationally intensive, requiring geochemical calculations at each grid cell and each time step within a discretized simulation domain.","Although recent advancements have integrated machine learning techniques as surrogates for geochemical simulations, ensuring computational efficiency and accuracy remains a challenge.","This chapter investigates machine learning models as replacements for a geochemical module in a reactive transport in porous media simulation.","We test this approach on a well-documented cation exchange problem.","While the surrogate models excel in isolated predictions, they fall short in rollout predictions over successive time steps.","By introducing modifications, including physics-based constraints and tailored dataset generation strategies, we show that machine learning surrogates can achieve accurate rollout predictions.","Our findings emphasize that, when judiciously designed, machine learning surrogates can substantially expedite the cation exchange problem without compromising accuracy, offering significant potential for a range of reactive transport applications."],"url":"http://arxiv.org/abs/2405.14548v1","category":"cs.CE"}
{"created":"2024-05-23 13:25:41","title":"Causal Effect Identification in a Sub-Population with Latent Variables","abstract":"The s-ID problem seeks to compute a causal effect in a specific sub-population from the observational data pertaining to the same sub population (Abouei et al., 2023). This problem has been addressed when all the variables in the system are observable. In this paper, we consider an extension of the s-ID problem that allows for the presence of latent variables. To tackle the challenges induced by the presence of latent variables in a sub-population, we first extend the classical relevant graphical definitions, such as c-components and Hedges, initially defined for the so-called ID problem (Pearl, 1995; Tian & Pearl, 2002), to their new counterparts. Subsequently, we propose a sound algorithm for the s-ID problem with latent variables.","sentences":["The s-ID problem seeks to compute a causal effect in a specific sub-population from the observational data pertaining to the same sub population (Abouei et al., 2023).","This problem has been addressed when all the variables in the system are observable.","In this paper, we consider an extension of the s-ID problem that allows for the presence of latent variables.","To tackle the challenges induced by the presence of latent variables in a sub-population, we first extend the classical relevant graphical definitions, such as c-components and Hedges, initially defined for the so-called ID problem (Pearl, 1995; Tian & Pearl, 2002), to their new counterparts.","Subsequently, we propose a sound algorithm for the s-ID problem with latent variables."],"url":"http://arxiv.org/abs/2405.14547v1","category":"cs.LG"}
{"created":"2024-05-23 13:22:17","title":"Regressor-free Molecule Generation to Support Drug Response Prediction","abstract":"Drug response prediction (DRP) is a crucial phase in drug discovery, and the most important metric for its evaluation is the IC50 score. DRP results are heavily dependent on the quality of the generated molecules. Existing molecule generation methods typically employ classifier-based guidance, enabling sampling within the IC50 classification range. However, these methods fail to ensure the sampling space range's effectiveness, generating numerous ineffective molecules. Through experimental and theoretical study, we hypothesize that conditional generation based on the target IC50 score can obtain a more effective sampling space. As a result, we introduce regressor-free guidance molecule generation to ensure sampling within a more effective space and support DRP. Regressor-free guidance combines a diffusion model's score estimation with a regression controller model's gradient based on number labels. To effectively map regression labels between drugs and cell lines, we design a common-sense numerical knowledge graph that constrains the order of text representations. Experimental results on the real-world dataset for the DRP task demonstrate our method's effectiveness in drug discovery. The code is available at:https://anonymous.4open.science/r/RMCD-DBD1.","sentences":["Drug response prediction (DRP) is a crucial phase in drug discovery, and the most important metric for its evaluation is the IC50 score.","DRP results are heavily dependent on the quality of the generated molecules.","Existing molecule generation methods typically employ classifier-based guidance, enabling sampling within the IC50 classification range.","However, these methods fail to ensure the sampling space range's effectiveness, generating numerous ineffective molecules.","Through experimental and theoretical study, we hypothesize that conditional generation based on the target IC50 score can obtain a more effective sampling space.","As a result, we introduce regressor-free guidance molecule generation to ensure sampling within a more effective space and support DRP.","Regressor-free guidance combines a diffusion model's score estimation with a regression controller model's gradient based on number labels.","To effectively map regression labels between drugs and cell lines, we design a common-sense numerical knowledge graph that constrains the order of text representations.","Experimental results on the real-world dataset for the DRP task demonstrate our method's effectiveness in drug discovery.","The code is available at:https://anonymous.4open.science/r/RMCD-DBD1."],"url":"http://arxiv.org/abs/2405.14536v1","category":"q-bio.MN"}
{"created":"2024-05-23 13:20:24","title":"Exploring Alignment in Shared Cross-lingual Spaces","abstract":"Despite their remarkable ability to capture linguistic nuances across diverse languages, questions persist regarding the degree of alignment between languages in multilingual embeddings. Drawing inspiration from research on high-dimensional representations in neural language models, we employ clustering to uncover latent concepts within multilingual models. Our analysis focuses on quantifying the \\textit{alignment} and \\textit{overlap} of these concepts across various languages within the latent space. To this end, we introduce two metrics \\CA{} and \\CO{} aimed at quantifying these aspects, enabling a deeper exploration of multilingual embeddings. Our study encompasses three multilingual models (\\texttt{mT5}, \\texttt{mBERT}, and \\texttt{XLM-R}) and three downstream tasks (Machine Translation, Named Entity Recognition, and Sentiment Analysis). Key findings from our analysis include: i) deeper layers in the network demonstrate increased cross-lingual \\textit{alignment} due to the presence of language-agnostic concepts, ii) fine-tuning of the models enhances \\textit{alignment} within the latent space, and iii) such task-specific calibration helps in explaining the emergence of zero-shot capabilities in the models.\\footnote{The code is available at \\url{https://github.com/baselmousi/multilingual-latent-concepts}}","sentences":["Despite their remarkable ability to capture linguistic nuances across diverse languages, questions persist regarding the degree of alignment between languages in multilingual embeddings.","Drawing inspiration from research on high-dimensional representations in neural language models, we employ clustering to uncover latent concepts within multilingual models.","Our analysis focuses on quantifying the \\textit{alignment} and \\textit{overlap} of these concepts across various languages within the latent space.","To this end, we introduce two metrics \\CA{} and \\CO{} aimed at quantifying these aspects, enabling a deeper exploration of multilingual embeddings.","Our study encompasses three multilingual models (\\texttt{mT5}, \\texttt{mBERT}, and \\texttt{XLM-R}) and three downstream tasks (Machine Translation, Named Entity Recognition, and Sentiment Analysis).","Key findings from our analysis include: i) deeper layers in the network demonstrate increased cross-lingual \\textit{alignment} due to the presence of language-agnostic concepts, ii) fine-tuning of the models enhances \\textit{alignment} within the latent space, and iii) such task-specific calibration helps in explaining the emergence of zero-shot capabilities in the models.\\footnote{The code is available at \\url{https://github.com/baselmousi/multilingual-latent-concepts}}"],"url":"http://arxiv.org/abs/2405.14535v1","category":"cs.CL"}
{"created":"2024-05-23 13:19:56","title":"Synchrotron polarization with a partially random magnetic field: general approach, and application to X-ray polarization from SNRs","abstract":"Diagnostics based on the polarization properties of the synchrotron emission can provide precious information on both the ordered structure and the random level of the magnetic field. While this issue has been already analysed in the radio, the polarization data recently obtained by the mission IXPE have shown the need to extend this analysis to the X-rays. While our immediate target are young SNRs, the scope of this analysis is wider. We aim at extending the analysis to particle energy distributions more complex than a power law, as well as to investigate a wider range of cases involving a composition of ordered and random magnetic fields. Since only in a limited number of cases an analytical approach is possible, we have devised to this purpose an optimised numerical scheme, and we have directly used it to investigate particle energy distributions in the form of a power law with an exponential, or super-exponential cutoff. We have also considered a general combination of a ordered field plus an anisotropic random component. We have shown that the previously derived analytic formulae, valid for power-law distributions, may be good approximations of the polarization degree also in the more general case with a cutoff, as typically seen in X-rays. We have explicitly analysed the young SNRs SN 1006, Tycho and Cas A. In particular, for SN 1006, we have proved the consistency between the radio and X-ray polarization degrees, favouring the case of predominantly random field, with an anisotropic distribution. In addition, for the power-law case, we have investigated the effect of a compression on both ordered and random magnetic field components, aimed at describing the mid-age radio SNRs. This work allows a more efficient exploitation of radio and X-ray measurements of the synchrotron polarization, and is addressed to present observations with IXPE as well as to future projects.","sentences":["Diagnostics based on the polarization properties of the synchrotron emission can provide precious information on both the ordered structure and the random level of the magnetic field.","While this issue has been already analysed in the radio, the polarization data recently obtained by the mission IXPE have shown the need to extend this analysis to the X-rays.","While our immediate target are young SNRs, the scope of this analysis is wider.","We aim at extending the analysis to particle energy distributions more complex than a power law, as well as to investigate a wider range of cases involving a composition of ordered and random magnetic fields.","Since only in a limited number of cases an analytical approach is possible, we have devised to this purpose an optimised numerical scheme, and we have directly used it to investigate particle energy distributions in the form of a power law with an exponential, or super-exponential cutoff.","We have also considered a general combination of a ordered field plus an anisotropic random component.","We have shown that the previously derived analytic formulae, valid for power-law distributions, may be good approximations of the polarization degree also in the more general case with a cutoff, as typically seen in X-rays.","We have explicitly analysed the young SNRs SN 1006, Tycho and Cas A. In particular, for SN 1006, we have proved the consistency between the radio and X-ray polarization degrees, favouring the case of predominantly random field, with an anisotropic distribution.","In addition, for the power-law case, we have investigated the effect of a compression on both ordered and random magnetic field components, aimed at describing the mid-age radio SNRs.","This work allows a more efficient exploitation of radio and X-ray measurements of the synchrotron polarization, and is addressed to present observations with IXPE as well as to future projects."],"url":"http://arxiv.org/abs/2405.14534v1","category":"astro-ph.HE"}
{"created":"2024-05-23 13:15:24","title":"Multistable Shape from Shading Emerges from Patch Diffusion","abstract":"Models for monocular shape reconstruction of surfaces with diffuse reflection -- shape from shading -- ought to produce distributions of outputs, because there are fundamental mathematical ambiguities of both continuous (e.g., bas-relief) and discrete (e.g., convex/concave) varieties which are also experienced by humans. Yet, the outputs of current models are limited to point estimates or tight distributions around single modes, which prevent them from capturing these effects. We introduce a model that reconstructs a multimodal distribution of shapes from a single shading image, which aligns with the human experience of multistable perception. We train a small denoising diffusion process to generate surface normal fields from $16\\times 16$ patches of synthetic images of everyday 3D objects. We deploy this model patch-wise at multiple scales, with guidance from inter-patch shape consistency constraints. Despite its relatively small parameter count and predominantly bottom-up structure, we show that multistable shape explanations emerge from this model for ''ambiguous'' test images that humans experience as being multistable. At the same time, the model produces veridical shape estimates for object-like images that include distinctive occluding contours and appear less ambiguous. This may inspire new architectures for stochastic 3D shape perception that are more efficient and better aligned with human experience.","sentences":["Models for monocular shape reconstruction of surfaces with diffuse reflection -- shape from shading -- ought to produce distributions of outputs, because there are fundamental mathematical ambiguities of both continuous (e.g., bas-relief) and discrete (e.g., convex/concave) varieties which are also experienced by humans.","Yet, the outputs of current models are limited to point estimates or tight distributions around single modes, which prevent them from capturing these effects.","We introduce a model that reconstructs a multimodal distribution of shapes from a single shading image, which aligns with the human experience of multistable perception.","We train a small denoising diffusion process to generate surface normal fields from $16\\times 16$ patches of synthetic images of everyday 3D objects.","We deploy this model patch-wise at multiple scales, with guidance from inter-patch shape consistency constraints.","Despite its relatively small parameter count and predominantly bottom-up structure, we show that multistable shape explanations emerge from this model for ''ambiguous'' test images that humans experience as being multistable.","At the same time, the model produces veridical shape estimates for object-like images that include distinctive occluding contours and appear less ambiguous.","This may inspire new architectures for stochastic 3D shape perception that are more efficient and better aligned with human experience."],"url":"http://arxiv.org/abs/2405.14530v1","category":"cs.CV"}
{"created":"2024-05-23 13:11:49","title":"ArchesWeather: An efficient AI weather forecasting model at 1.5\u00b0 resolution","abstract":"One of the guiding principles for designing AI-based weather forecasting systems is to embed physical constraints as inductive priors in the neural network architecture. A popular prior is locality, where the atmospheric data is processed with local neural interactions, like 3D convolutions or 3D local attention windows as in Pangu-Weather. On the other hand, some works have shown great success in weather forecasting without this locality principle, at the cost of a much higher parameter count.   In this paper, we show that the 3D local processing in Pangu-Weather is computationally sub-optimal. We design ArchesWeather, a transformer model that combines 2D attention with a column-wise attention-based feature interaction module, and demonstrate that this design improves forecasting skill.   ArchesWeather is trained at 1.5{\\deg} resolution and 24h lead time, with a training budget of a few GPU-days and a lower inference cost than competing methods. An ensemble of two of our best models shows competitive RMSE scores with the IFS HRES and outperforms the 1.4{\\deg} 50-members NeuralGCM ensemble for one day ahead forecasting.   Code and models will be made publicly available at https://github.com/gcouairon/ArchesWeather.","sentences":["One of the guiding principles for designing AI-based weather forecasting systems is to embed physical constraints as inductive priors in the neural network architecture.","A popular prior is locality, where the atmospheric data is processed with local neural interactions, like 3D convolutions or 3D local attention windows as in Pangu-Weather.","On the other hand, some works have shown great success in weather forecasting without this locality principle, at the cost of a much higher parameter count.   ","In this paper, we show that the 3D local processing in Pangu-Weather is computationally sub-optimal.","We design ArchesWeather, a transformer model that combines 2D attention with a column-wise attention-based feature interaction module, and demonstrate that this design improves forecasting skill.   ","ArchesWeather is trained at 1.5{\\deg} resolution and 24h lead time, with a training budget of a few GPU-days and a lower inference cost than competing methods.","An ensemble of two of our best models shows competitive RMSE scores with the IFS HRES and outperforms the 1.4{\\deg} 50-members NeuralGCM ensemble for one day ahead forecasting.   ","Code and models will be made publicly available at https://github.com/gcouairon/ArchesWeather."],"url":"http://arxiv.org/abs/2405.14527v1","category":"cs.LG"}
{"created":"2024-05-23 13:03:26","title":"Explaining Black-box Model Predictions via Two-level Nested Feature Attributions with Consistency Property","abstract":"Techniques that explain the predictions of black-box machine learning models are crucial to make the models transparent, thereby increasing trust in AI systems. The input features to the models often have a nested structure that consists of high- and low-level features, and each high-level feature is decomposed into multiple low-level features. For such inputs, both high-level feature attributions (HiFAs) and low-level feature attributions (LoFAs) are important for better understanding the model's decision. In this paper, we propose a model-agnostic local explanation method that effectively exploits the nested structure of the input to estimate the two-level feature attributions simultaneously. A key idea of the proposed method is to introduce the consistency property that should exist between the HiFAs and LoFAs, thereby bridging the separate optimization problems for estimating them. Thanks to this consistency property, the proposed method can produce HiFAs and LoFAs that are both faithful to the black-box models and consistent with each other, using a smaller number of queries to the models. In experiments on image classification in multiple instance learning and text classification using language models, we demonstrate that the HiFAs and LoFAs estimated by the proposed method are accurate, faithful to the behaviors of the black-box models, and provide consistent explanations.","sentences":["Techniques that explain the predictions of black-box machine learning models are crucial to make the models transparent, thereby increasing trust in AI systems.","The input features to the models often have a nested structure that consists of high- and low-level features, and each high-level feature is decomposed into multiple low-level features.","For such inputs, both high-level feature attributions (HiFAs) and low-level feature attributions (LoFAs) are important for better understanding the model's decision.","In this paper, we propose a model-agnostic local explanation method that effectively exploits the nested structure of the input to estimate the two-level feature attributions simultaneously.","A key idea of the proposed method is to introduce the consistency property that should exist between the HiFAs and LoFAs, thereby bridging the separate optimization problems for estimating them.","Thanks to this consistency property, the proposed method can produce HiFAs and LoFAs that are both faithful to the black-box models and consistent with each other, using a smaller number of queries to the models.","In experiments on image classification in multiple instance learning and text classification using language models, we demonstrate that the HiFAs and LoFAs estimated by the proposed method are accurate, faithful to the behaviors of the black-box models, and provide consistent explanations."],"url":"http://arxiv.org/abs/2405.14522v1","category":"cs.LG"}
{"created":"2024-05-23 13:03:23","title":"Synthetic Data Generation for Intersectional Fairness by Leveraging Hierarchical Group Structure","abstract":"In this paper, we introduce a data augmentation approach specifically tailored to enhance intersectional fairness in classification tasks. Our method capitalizes on the hierarchical structure inherent to intersectionality, by viewing groups as intersections of their parent categories. This perspective allows us to augment data for smaller groups by learning a transformation function that combines data from these parent groups. Our empirical analysis, conducted on four diverse datasets including both text and images, reveals that classifiers trained with this data augmentation approach achieve superior intersectional fairness and are more robust to ``leveling down'' when compared to methods optimizing traditional group fairness metrics.","sentences":["In this paper, we introduce a data augmentation approach specifically tailored to enhance intersectional fairness in classification tasks.","Our method capitalizes on the hierarchical structure inherent to intersectionality, by viewing groups as intersections of their parent categories.","This perspective allows us to augment data for smaller groups by learning a transformation function that combines data from these parent groups.","Our empirical analysis, conducted on four diverse datasets including both text and images, reveals that classifiers trained with this data augmentation approach achieve superior intersectional fairness and are more robust to ``leveling down'' when compared to methods optimizing traditional group fairness metrics."],"url":"http://arxiv.org/abs/2405.14521v1","category":"cs.LG"}
{"created":"2024-05-23 13:02:30","title":"Ghost-Stereo: GhostNet-based Cost Volume Enhancement and Aggregation for Stereo Matching Networks","abstract":"Depth estimation based on stereo matching is a classic but popular computer vision problem, which has a wide range of real-world applications. Current stereo matching methods generally adopt the deep Siamese neural network architecture, and have achieved impressing performance by constructing feature matching cost volumes and using 3D convolutions for cost aggregation. However, most existing methods suffer from large number of parameters and slow running time due to the sequential use of 3D convolutions. In this paper, we propose Ghost-Stereo, a novel end-to-end stereo matching network. The feature extraction part of the network uses the GhostNet to form a U-shaped structure. The core of Ghost-Stereo is a GhostNet feature-based cost volume enhancement (Ghost-CVE) module and a GhostNet-inspired lightweight cost volume aggregation (Ghost-CVA) module. For the Ghost-CVE part, cost volumes are constructed and fused by the GhostNet-based features to enhance the spatial context awareness. For the Ghost-CVA part, a lightweight 3D convolution bottleneck block based on the GhostNet is proposed to reduce the computational complexity in this module. By combining with the context and geometry fusion module, a classical hourglass-shaped cost volume aggregate structure is constructed. Ghost-Stereo achieves a comparable performance than state-of-the-art real-time methods on several publicly benchmarks, and shows a better generalization ability.","sentences":["Depth estimation based on stereo matching is a classic but popular computer vision problem, which has a wide range of real-world applications.","Current stereo matching methods generally adopt the deep Siamese neural network architecture, and have achieved impressing performance by constructing feature matching cost volumes and using 3D convolutions for cost aggregation.","However, most existing methods suffer from large number of parameters and slow running time due to the sequential use of 3D convolutions.","In this paper, we propose Ghost-Stereo, a novel end-to-end stereo matching network.","The feature extraction part of the network uses the GhostNet to form a U-shaped structure.","The core of Ghost-Stereo is a GhostNet feature-based cost volume enhancement (Ghost-CVE) module and a GhostNet-inspired lightweight cost volume aggregation (Ghost-CVA) module.","For the Ghost-CVE part, cost volumes are constructed and fused by the GhostNet-based features to enhance the spatial context awareness.","For the Ghost-CVA part, a lightweight 3D convolution bottleneck block based on the GhostNet is proposed to reduce the computational complexity in this module.","By combining with the context and geometry fusion module, a classical hourglass-shaped cost volume aggregate structure is constructed.","Ghost-Stereo achieves a comparable performance than state-of-the-art real-time methods on several publicly benchmarks, and shows a better generalization ability."],"url":"http://arxiv.org/abs/2405.14520v1","category":"cs.CV"}
{"created":"2024-05-23 12:55:50","title":"Small Banach bundles and modules","abstract":"We characterize those (continuously-normed) Banach bundles $\\mathcal{E}\\to X$ with compact Hausdorff base whose spaces $\\Gamma(\\mathcal{E})$ of global continuous sections are topologically finitely-generated over the function algebra $C(X)$, answering a question of I. Gogi\\'c's and extending analogous work for metrizable $X$. Conditions equivalent to topological finite generation include: (a) the requirement that $\\mathcal{E}$ be locally trivial and of finite type along locally closed and relatively $F_{\\sigma}$ strata in a finite stratification of $X$; (b) the decomposability of arbitrary elements in $\\ell^p(\\Gamma(\\mathcal{E}))$, $1\\le p<\\infty$ as sums of $\\le N$ products in $\\ell^p(C(X))\\cdot \\Gamma(\\mathcal{E})$ for some fixed $N$; (c) the analogous decomposability requirement for maximal Banach-module tensor products $F\\widehat{\\otimes}_{C(X)}\\Gamma(\\mathcal{E})$ or (d) equivalently, only for $F=\\ell^1(C(X))$.","sentences":["We characterize those (continuously-normed)","Banach bundles $\\mathcal{E}\\to X$ with compact Hausdorff base whose spaces $\\Gamma(\\mathcal{E})$ of global continuous sections are topologically finitely-generated over the function algebra $C(X)$, answering a question of I. Gogi\\'c's and extending analogous work for metrizable $X$. Conditions equivalent to topological finite generation include: (a) the requirement that $\\mathcal{E}$ be locally trivial and of finite type along locally closed and relatively $F_{\\sigma}$ strata in a finite stratification of $X$; (b) the decomposability of arbitrary elements in $\\ell^p(\\Gamma(\\mathcal{E}))$, $1\\le p<\\infty$ as sums of $\\le N$ products in $\\ell^p(C(X))\\cdot \\Gamma(\\mathcal{E})$ for some fixed $N$; (c) the analogous decomposability requirement for maximal Banach-module tensor products $F\\widehat{\\otimes}_{C(X)}\\Gamma(\\mathcal{E})$ or (d) equivalently, only for $F=\\ell^1(C(X))$."],"url":"http://arxiv.org/abs/2405.14518v1","category":"math.FA"}
{"created":"2024-05-23 12:54:25","title":"Identity Inference from CLIP Models using Only Textual Data","abstract":"The widespread usage of large-scale multimodal models like CLIP has heightened concerns about the leakage of personally identifiable information (PII). Existing methods for identity inference in CLIP models, i.e., to detect the presence of a person's PII used for training a CLIP model, require querying the model with full PII, including textual descriptions of the person and corresponding images (e.g., the name and the face photo of the person). However, this may lead to potential privacy breach of the image, as it may have not been seen by the target model yet. Additionally, traditional membership inference attacks (MIAs) train shadow models to mimic the behaviors of the target model, which incurs high computational costs, especially for large CLIP models. To address these challenges, we propose a textual unimodal detector (TUNI) in CLIP models, a novel method for ID inference that 1) queries the target model with only text data; and 2) does not require training shadow models. Firstly, we develop a feature extraction algorithm, guided by the CLIP model, to extract features from a text description. TUNI starts with randomly generating textual gibberish that were clearly not utilized for training, and leverages their feature vectors to train a system of anomaly detectors. During inference, the feature vector of each test text is fed into the anomaly detectors to determine if the person's PII is in the training set (abnormal) or not (normal). Moreover, TUNI can be further strengthened integrating real images associated with the tested individuals, if available at the detector. Extensive experiments of TUNI across various CLIP model architectures and datasets demonstrate its superior performance over baselines, albeit with only text data.","sentences":["The widespread usage of large-scale multimodal models like CLIP has heightened concerns about the leakage of personally identifiable information (PII).","Existing methods for identity inference in CLIP models, i.e., to detect the presence of a person's PII used for training a CLIP model, require querying the model with full PII, including textual descriptions of the person and corresponding images (e.g., the name and the face photo of the person).","However, this may lead to potential privacy breach of the image, as it may have not been seen by the target model yet.","Additionally, traditional membership inference attacks (MIAs) train shadow models to mimic the behaviors of the target model, which incurs high computational costs, especially for large CLIP models.","To address these challenges, we propose a textual unimodal detector (TUNI) in CLIP models, a novel method for ID inference that 1) queries the target model with only text data; and 2) does not require training shadow models.","Firstly, we develop a feature extraction algorithm, guided by the CLIP model, to extract features from a text description.","TUNI starts with randomly generating textual gibberish that were clearly not utilized for training, and leverages their feature vectors to train a system of anomaly detectors.","During inference, the feature vector of each test text is fed into the anomaly detectors to determine if the person's PII is in the training set (abnormal) or not (normal).","Moreover, TUNI can be further strengthened integrating real images associated with the tested individuals, if available at the detector.","Extensive experiments of TUNI across various CLIP model architectures and datasets demonstrate its superior performance over baselines, albeit with only text data."],"url":"http://arxiv.org/abs/2405.14517v1","category":"cs.LG"}
{"created":"2024-05-23 12:53:50","title":"Towards Realistic Long-tailed Semi-supervised Learning in an Open World","abstract":"Open-world long-tailed semi-supervised learning (OLSSL) has increasingly attracted attention. However, existing OLSSL algorithms generally assume that the distributions between known and novel categories are nearly identical. Against this backdrop, we construct a more \\emph{Realistic Open-world Long-tailed Semi-supervised Learning} (\\textbf{ROLSSL}) setting where there is no premise on the distribution relationships between known and novel categories. Furthermore, even within the known categories, the number of labeled samples is significantly smaller than that of the unlabeled samples, as acquiring valid annotations is often prohibitively costly in the real world. Under the proposed ROLSSL setting, we propose a simple yet potentially effective solution called dual-stage post-hoc logit adjustments. The proposed approach revisits the logit adjustment strategy by considering the relationships among the frequency of samples, the total number of categories, and the overall size of data. Then, it estimates the distribution of unlabeled data for both known and novel categories to dynamically readjust the corresponding predictive probabilities, effectively mitigating category bias during the learning of known and novel classes with more selective utilization of imbalanced unlabeled data. Extensive experiments on datasets such as CIFAR100 and ImageNet100 have demonstrated performance improvements of up to 50.1\\%, validating the superiority of our proposed method and establishing a strong baseline for this task. For further researches, the anonymous link to the experimental code is at \\href{https://github.com/heyuanpengpku/ROLSSL}{\\textcolor{brightpink}{https://github.com/heyuanpengpku/ROLSSL}}","sentences":["Open-world long-tailed semi-supervised learning (OLSSL) has increasingly attracted attention.","However, existing OLSSL algorithms generally assume that the distributions between known and novel categories are nearly identical.","Against this backdrop, we construct a more \\emph{Realistic Open-world Long-tailed Semi-supervised Learning} (\\textbf{ROLSSL}) setting where there is no premise on the distribution relationships between known and novel categories.","Furthermore, even within the known categories, the number of labeled samples is significantly smaller than that of the unlabeled samples, as acquiring valid annotations is often prohibitively costly in the real world.","Under the proposed ROLSSL setting, we propose a simple yet potentially effective solution called dual-stage post-hoc logit adjustments.","The proposed approach revisits the logit adjustment strategy by considering the relationships among the frequency of samples, the total number of categories, and the overall size of data.","Then, it estimates the distribution of unlabeled data for both known and novel categories to dynamically readjust the corresponding predictive probabilities, effectively mitigating category bias during the learning of known and novel classes with more selective utilization of imbalanced unlabeled data.","Extensive experiments on datasets such as CIFAR100 and ImageNet100 have demonstrated performance improvements of up to 50.1\\%, validating the superiority of our proposed method and establishing a strong baseline for this task.","For further researches, the anonymous link to the experimental code is at \\href{https://github.com/heyuanpengpku/ROLSSL}{\\textcolor{brightpink}{https://github.com/heyuanpengpku/ROLSSL}}"],"url":"http://arxiv.org/abs/2405.14516v1","category":"cs.CV"}
{"created":"2024-05-23 12:51:54","title":"Quantum Mpemba Effect in Random Circuits","abstract":"The essence of the Mpemba effect is that non-equilibrium systems may relax faster the further they are from their equilibrium configuration. In the quantum realm, this phenomenon arises in the dynamics of closed systems, where it is witnessed by fundamental features such as symmetry and entanglement. Here, we study the quantum Mpemba effect in charge-preserving random circuits on qudits via entanglement asymmetry, combining extensive numerical simulations and analytical mapping to a classical statistical mechanics problem. We show that the more asymmetric certain classes of initial states (tilted ferromagnets) are, the faster they restore symmetry and reach the grand-canonical ensemble. Conversely, other classes of states (tilted antiferromagnets) do not show the Mpemba effect. Our analysis is based on minimal principles -- locality, unitarity, and symmetry. Consequently, our results represent a significant advancement in clarifying the emergence of Mpemba physics in generic systems, including Hamiltonian and Floquet quantum circuits.","sentences":["The essence of the Mpemba effect is that non-equilibrium systems may relax faster the further they are from their equilibrium configuration.","In the quantum realm, this phenomenon arises in the dynamics of closed systems, where it is witnessed by fundamental features such as symmetry and entanglement.","Here, we study the quantum Mpemba effect in charge-preserving random circuits on qudits via entanglement asymmetry, combining extensive numerical simulations and analytical mapping to a classical statistical mechanics problem.","We show that the more asymmetric certain classes of initial states (tilted ferromagnets) are, the faster they restore symmetry and reach the grand-canonical ensemble.","Conversely, other classes of states (tilted antiferromagnets) do not show the Mpemba effect.","Our analysis is based on minimal principles -- locality, unitarity, and symmetry.","Consequently, our results represent a significant advancement in clarifying the emergence of Mpemba physics in generic systems, including Hamiltonian and Floquet quantum circuits."],"url":"http://arxiv.org/abs/2405.14514v1","category":"quant-ph"}
{"created":"2024-05-23 12:47:26","title":"Neutron phase filtering for separating phase- and attenuation signal in aluminium and anodic aluminium oxide","abstract":"Neutron imaging has gained significant importance as a material characterisation technique and is particularly useful to visualise hydrogenous materials in objects opaque to other radiations. Particular fields of application include investigations of hydrogen in metals as well as metal corrosion, thanks to the fact that neutrons can penetrate metals better than e.g. X-rays and are at the same time highly sensitive to hydrogen. However at interfaces for example those that are prone to corrosion, refraction effects sometimes obscure the attenuation image, which is used to for hydrogen quantification. Refraction, as a differential phase effect, diverts the neutron beam away from the interface in the image which leads to intensity gain and intensity loss regions, which are superimposed to the attenuation image, thus obscuring the interface region and hindering quantitative analyses of e.g. hydrogen content in the vicinity of the interface or in an oxide layer. For corresponding effects in X-ray imaging, a phase filter approach was developed and is generally based on transport-of-intensity considerations. Here, we compare such an approach, that has been adapted to neutrons, with another simulation-based assessment using the ray-tracing software McStas. The latter appears superior and promising for future extensions which enable fitting forward models via simulations in order to separate phase and attenuation effects and thus pave the way for overcoming quantitative limitations at refracting interfaces.","sentences":["Neutron imaging has gained significant importance as a material characterisation technique and is particularly useful to visualise hydrogenous materials in objects opaque to other radiations.","Particular fields of application include investigations of hydrogen in metals as well as metal corrosion, thanks to the fact that neutrons can penetrate metals better than e.g. X-rays and are at the same time highly sensitive to hydrogen.","However at interfaces for example those that are prone to corrosion, refraction effects sometimes obscure the attenuation image, which is used to for hydrogen quantification.","Refraction, as a differential phase effect, diverts the neutron beam away from the interface in the image which leads to intensity gain and intensity loss regions, which are superimposed to the attenuation image, thus obscuring the interface region and hindering quantitative analyses of e.g. hydrogen content in the vicinity of the interface or in an oxide layer.","For corresponding effects in X-ray imaging, a phase filter approach was developed and is generally based on transport-of-intensity considerations.","Here, we compare such an approach, that has been adapted to neutrons, with another simulation-based assessment using the ray-tracing software McStas.","The latter appears superior and promising for future extensions which enable fitting forward models via simulations in order to separate phase and attenuation effects and thus pave the way for overcoming quantitative limitations at refracting interfaces."],"url":"http://arxiv.org/abs/2405.14510v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 12:44:51","title":"SIAVC: Semi-Supervised Framework for Industrial Accident Video Classification","abstract":"Semi-supervised learning suffers from the imbalance of labeled and unlabeled training data in the video surveillance scenario. In this paper, we propose a new semi-supervised learning method called SIAVC for industrial accident video classification. Specifically, we design a video augmentation module called the Super Augmentation Block (SAB). SAB adds Gaussian noise and randomly masks video frames according to historical loss on the unlabeled data for model optimization. Then, we propose a Video Cross-set Augmentation Module (VCAM) to generate diverse pseudo-label samples from the high-confidence unlabeled samples, which alleviates the mismatch of sampling experience and provides high-quality training data. Additionally, we construct a new industrial accident surveillance video dataset with frame-level annotation, namely ECA9, to evaluate our proposed method. Compared with the state-of-the-art semi-supervised learning based methods, SIAVC demonstrates outstanding video classification performance, achieving 88.76\\% and 89.13\\% accuracy on ECA9 and Fire Detection datasets, respectively. The source code and the constructed dataset ECA9 will be released in \\url{https://github.com/AlchemyEmperor/SIAVC}.","sentences":["Semi-supervised learning suffers from the imbalance of labeled and unlabeled training data in the video surveillance scenario.","In this paper, we propose a new semi-supervised learning method called SIAVC for industrial accident video classification.","Specifically, we design a video augmentation module called the Super Augmentation Block (SAB).","SAB adds Gaussian noise and randomly masks video frames according to historical loss on the unlabeled data for model optimization.","Then, we propose a Video Cross-set Augmentation Module (VCAM) to generate diverse pseudo-label samples from the high-confidence unlabeled samples, which alleviates the mismatch of sampling experience and provides high-quality training data.","Additionally, we construct a new industrial accident surveillance video dataset with frame-level annotation, namely ECA9, to evaluate our proposed method.","Compared with the state-of-the-art semi-supervised learning based methods, SIAVC demonstrates outstanding video classification performance, achieving 88.76\\% and 89.13\\% accuracy on ECA9 and Fire Detection datasets, respectively.","The source code and the constructed dataset ECA9 will be released in \\url{https://github.com/AlchemyEmperor/SIAVC}."],"url":"http://arxiv.org/abs/2405.14506v1","category":"cs.CV"}
{"created":"2024-05-23 12:43:06","title":"Explainable automatic industrial carbon footprint estimation from bank transaction classification using natural language processing","abstract":"Concerns about the effect of greenhouse gases have motivated the development of certification protocols to quantify the industrial carbon footprint (CF). These protocols are manual, work-intensive, and expensive. All of the above have led to a shift towards automatic data-driven approaches to estimate the CF, including Machine Learning (ML) solutions. Unfortunately, the decision-making processes involved in these solutions lack transparency from the end user's point of view, who must blindly trust their outcomes compared to intelligible traditional manual approaches. In this research, manual and automatic methodologies for CF estimation were reviewed, taking into account their transparency limitations. This analysis led to the proposal of a new explainable ML solution for automatic CF calculations through bank transaction classification. Consideration should be given to the fact that no previous research has considered the explainability of bank transaction classification for this purpose. For classification, different ML models have been employed based on their promising performance in the literature, such as Support Vector Machine, Random Forest, and Recursive Neural Networks. The results obtained were in the 90 % range for accuracy, precision, and recall evaluation metrics. From their decision paths, the proposed solution estimates the CO2 emissions associated with bank transactions. The explainability methodology is based on an agnostic evaluation of the influence of the input terms extracted from the descriptions of transactions using locally interpretable models. The explainability terms were automatically validated using a similarity metric over the descriptions of the target categories. Conclusively, the explanation performance is satisfactory in terms of the proximity of the explanations to the associated activity sector descriptions.","sentences":["Concerns about the effect of greenhouse gases have motivated the development of certification protocols to quantify the industrial carbon footprint (CF).","These protocols are manual, work-intensive, and expensive.","All of the above have led to a shift towards automatic data-driven approaches to estimate the CF, including Machine Learning (ML) solutions.","Unfortunately, the decision-making processes involved in these solutions lack transparency from the end user's point of view, who must blindly trust their outcomes compared to intelligible traditional manual approaches.","In this research, manual and automatic methodologies for CF estimation were reviewed, taking into account their transparency limitations.","This analysis led to the proposal of a new explainable ML solution for automatic CF calculations through bank transaction classification.","Consideration should be given to the fact that no previous research has considered the explainability of bank transaction classification for this purpose.","For classification, different ML models have been employed based on their promising performance in the literature, such as Support Vector Machine, Random Forest, and Recursive Neural Networks.","The results obtained were in the 90 % range for accuracy, precision, and recall evaluation metrics.","From their decision paths, the proposed solution estimates the CO2 emissions associated with bank transactions.","The explainability methodology is based on an agnostic evaluation of the influence of the input terms extracted from the descriptions of transactions using locally interpretable models.","The explainability terms were automatically validated using a similarity metric over the descriptions of the target categories.","Conclusively, the explanation performance is satisfactory in terms of the proximity of the explanations to the associated activity sector descriptions."],"url":"http://arxiv.org/abs/2405.14505v1","category":"cs.CL"}
{"created":"2024-05-23 12:39:49","title":"Enhanced Spatiotemporal Prediction Using Physical-guided And Frequency-enhanced Recurrent Neural Networks","abstract":"Spatiotemporal prediction plays an important role in solving natural problems and processing video frames, especially in weather forecasting and human action recognition. Recent advances attempt to incorporate prior physical knowledge into the deep learning framework to estimate the unknown governing partial differential equations (PDEs), which have shown promising results in spatiotemporal prediction tasks. However, previous approaches only restrict neural network architectures or loss functions to acquire physical or PDE features, which decreases the representative capacity of a neural network. Meanwhile, the updating process of the physical state cannot be effectively estimated. To solve the above mentioned problems, this paper proposes a physical-guided neural network, which utilizes the frequency-enhanced Fourier module and moment loss to strengthen the model's ability to estimate the spatiotemporal dynamics. Furthermore, we propose an adaptive second-order Runge-Kutta method with physical constraints to model the physical states more precisely. We evaluate our model on both spatiotemporal and video prediction tasks. The experimental results show that our model outperforms state-of-the-art methods and performs best in several datasets, with a much smaller parameter count.","sentences":["Spatiotemporal prediction plays an important role in solving natural problems and processing video frames, especially in weather forecasting and human action recognition.","Recent advances attempt to incorporate prior physical knowledge into the deep learning framework to estimate the unknown governing partial differential equations (PDEs), which have shown promising results in spatiotemporal prediction tasks.","However, previous approaches only restrict neural network architectures or loss functions to acquire physical or PDE features, which decreases the representative capacity of a neural network.","Meanwhile, the updating process of the physical state cannot be effectively estimated.","To solve the above mentioned problems, this paper proposes a physical-guided neural network, which utilizes the frequency-enhanced Fourier module and moment loss to strengthen the model's ability to estimate the spatiotemporal dynamics.","Furthermore, we propose an adaptive second-order Runge-Kutta method with physical constraints to model the physical states more precisely.","We evaluate our model on both spatiotemporal and video prediction tasks.","The experimental results show that our model outperforms state-of-the-art methods and performs best in several datasets, with a much smaller parameter count."],"url":"http://arxiv.org/abs/2405.14504v1","category":"cs.CV"}
{"created":"2024-05-23 12:31:21","title":"A rolling horizon heuristic approach for a multi-stage stochastic waste collection problem","abstract":"In this paper we present a multi-stage stochastic optimization model to solve an inventory routing problem for recyclable waste collection. The objective is the maximization of the total expected profit of the waste collection company. The decisions are related to the selection of the bins to be visited and the corresponding routing plan in a predefined time horizon. Stochasticity in waste accumulation is modeled through scenario trees generated via conditional density estimation and dynamic stochastic approximation techniques. The proposed formulation is solved through a rolling horizon approach, providing a worst-case analysis on its performance. Extensive computational experiments are carried out on small- and large-sized instances based on real data provided by a large Portuguese waste collection company. The impact of stochasticity on waste generation is examined through stochastic measures, and the performance of the rolling horizon approach is evaluated. Some managerial insights on different configurations of the instances are finally discussed.","sentences":["In this paper we present a multi-stage stochastic optimization model to solve an inventory routing problem for recyclable waste collection.","The objective is the maximization of the total expected profit of the waste collection company.","The decisions are related to the selection of the bins to be visited and the corresponding routing plan in a predefined time horizon.","Stochasticity in waste accumulation is modeled through scenario trees generated via conditional density estimation and dynamic stochastic approximation techniques.","The proposed formulation is solved through a rolling horizon approach, providing a worst-case analysis on its performance.","Extensive computational experiments are carried out on small- and large-sized instances based on real data provided by a large Portuguese waste collection company.","The impact of stochasticity on waste generation is examined through stochastic measures, and the performance of the rolling horizon approach is evaluated.","Some managerial insights on different configurations of the instances are finally discussed."],"url":"http://arxiv.org/abs/2405.14499v1","category":"math.OC"}
{"created":"2024-05-23 12:29:25","title":"Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment","abstract":"In this work, we tackle the problem of domain generalization for object detection, specifically focusing on the scenario where only a single source domain is available. We propose an effective approach that involves two key steps: diversifying the source domain and aligning detections based on class prediction confidence and localization. Firstly, we demonstrate that by carefully selecting a set of augmentations, a base detector can outperform existing methods for single domain generalization by a good margin. This highlights the importance of domain diversification in improving the performance of object detectors. Secondly, we introduce a method to align detections from multiple views, considering both classification and localization outputs. This alignment procedure leads to better generalized and well-calibrated object detector models, which are crucial for accurate decision-making in safety-critical applications. Our approach is detector-agnostic and can be seamlessly applied to both single-stage and two-stage detectors. To validate the effectiveness of our proposed methods, we conduct extensive experiments and ablations on challenging domain-shift scenarios. The results consistently demonstrate the superiority of our approach compared to existing methods. Our code and models are available at: https://github.com/msohaildanish/DivAlign","sentences":["In this work, we tackle the problem of domain generalization for object detection, specifically focusing on the scenario where only a single source domain is available.","We propose an effective approach that involves two key steps: diversifying the source domain and aligning detections based on class prediction confidence and localization.","Firstly, we demonstrate that by carefully selecting a set of augmentations, a base detector can outperform existing methods for single domain generalization by a good margin.","This highlights the importance of domain diversification in improving the performance of object detectors.","Secondly, we introduce a method to align detections from multiple views, considering both classification and localization outputs.","This alignment procedure leads to better generalized and well-calibrated object detector models, which are crucial for accurate decision-making in safety-critical applications.","Our approach is detector-agnostic and can be seamlessly applied to both single-stage and two-stage detectors.","To validate the effectiveness of our proposed methods, we conduct extensive experiments and ablations on challenging domain-shift scenarios.","The results consistently demonstrate the superiority of our approach compared to existing methods.","Our code and models are available at: https://github.com/msohaildanish/DivAlign"],"url":"http://arxiv.org/abs/2405.14497v1","category":"cs.CV"}
{"created":"2024-05-23 12:28:16","title":"Hybrid Global Causal Discovery with Local Search","abstract":"Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task. Methods based on functional causal models can identify a unique graph, but either suffer from the curse of dimensionality or impose strong parametric assumptions. To address these challenges, we propose a novel hybrid approach for global causal discovery in observational data that leverages local causal substructures. We first present a topological sorting algorithm that leverages ancestral relationships in linear structural equation models to establish a compact top-down hierarchical ordering, encoding more causal information than linear orderings produced by existing methods. We demonstrate that this approach generalizes to nonlinear settings with arbitrary noise. We then introduce a nonparametric constraint-based algorithm that prunes spurious edges by searching for local conditioning sets, achieving greater accuracy than current methods. We provide theoretical guarantees for correctness and worst-case polynomial time complexities, with empirical validation on synthetic data.","sentences":["Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task.","Methods based on functional causal models can identify a unique graph, but either suffer from the curse of dimensionality or impose strong parametric assumptions.","To address these challenges, we propose a novel hybrid approach for global causal discovery in observational data that leverages local causal substructures.","We first present a topological sorting algorithm that leverages ancestral relationships in linear structural equation models to establish a compact top-down hierarchical ordering, encoding more causal information than linear orderings produced by existing methods.","We demonstrate that this approach generalizes to nonlinear settings with arbitrary noise.","We then introduce a nonparametric constraint-based algorithm that prunes spurious edges by searching for local conditioning sets, achieving greater accuracy than current methods.","We provide theoretical guarantees for correctness and worst-case polynomial time complexities, with empirical validation on synthetic data."],"url":"http://arxiv.org/abs/2405.14496v1","category":"cs.LG"}
{"created":"2024-05-23 12:25:31","title":"Minimum Consistent Subset in Interval Graphs and Circle Graphs","abstract":"In a connected simple graph G = (V,E), each vertex of V is colored by a color from the set of colors C={c1, c2,..., c_{\\alpha}}$. We take a subset S of V, such that for every vertex v in V\\S, at least one vertex of the same color is present in its set of nearest neighbors in S. We refer to such a S as a consistent subset. The Minimum Consistent Subset (MCS) problem is the computation of a consistent subset of the minimum size. It is established that MCS is NP-complete for general graphs, including planar graphs. We expand our study to interval graphs and circle graphs in an attempt to gain a complete understanding of the computational complexity of the \\mcs problem across various graph classes.   This work introduces an (4\\alpha+ 2)- approximation algorithm for MCS in interval graphs where \\alpha is the number of colors in the interval graphs. Later, we show that in circle graphs, MCS is APX-hard.","sentences":["In a connected simple graph G = (V,E), each vertex of V is colored by a color from the set of colors C={c1, c2,...,","c_{\\alpha}}$.","We take a subset S of V, such that for every vertex v in V\\S, at least one vertex of the same color is present in its set of nearest neighbors in S. We refer to such a S as a consistent subset.","The Minimum Consistent Subset (MCS) problem is the computation of a consistent subset of the minimum size.","It is established that MCS is NP-complete for general graphs, including planar graphs.","We expand our study to interval graphs and circle graphs in an attempt to gain a complete understanding of the computational complexity of the \\mcs problem across various graph classes.   ","This work introduces an (4\\alpha+ 2)- approximation algorithm for MCS in interval graphs where \\alpha is the number of colors in the interval graphs.","Later, we show that in circle graphs, MCS is APX-hard."],"url":"http://arxiv.org/abs/2405.14493v1","category":"cs.CG"}
{"created":"2024-05-23 12:24:38","title":"Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models","abstract":"The advancement of large language models has significantly improved natural language processing. However, challenges such as jailbreaks (prompt injections that cause an LLM to follow instructions contrary to its intended use), hallucinations (generating incorrect or misleading information), and comprehension errors remain prevalent. In this report, we present a comparative analysis of the performance of fifteen distinct models, with each model undergoing a standardized test comprising 38 queries across three key metrics: jailbreaks, hallucinations, and comprehension errors. The models are assessed based on the total occurrences of jailbreaks, hallucinations, and comprehension errors. Our work exposes these models' inherent vulnerabilities and challenges the notion of human-level language comprehension of these models. We have empirically analysed the impact of non-standard Unicode characters on LLMs and their safeguarding mechanisms on the best-performing LLMs, including GPT-4, Gemini 1.5 Pro, LlaMA-3-70B, and Claude 3 Opus. By incorporating alphanumeric symbols from Unicode outside the standard Latin block and variants of characters in other languages, we observed a reduction in the efficacy of guardrails implemented through Reinforcement Learning Human Feedback (RLHF). Consequently, these models exhibit heightened vulnerability to content policy breaches and prompt leakage. Our study also suggests a need to incorporate non-standard Unicode text in LLM training data to enhance the capabilities of these models.","sentences":["The advancement of large language models has significantly improved natural language processing.","However, challenges such as jailbreaks (prompt injections that cause an LLM to follow instructions contrary to its intended use), hallucinations (generating incorrect or misleading information), and comprehension errors remain prevalent.","In this report, we present a comparative analysis of the performance of fifteen distinct models, with each model undergoing a standardized test comprising 38 queries across three key metrics: jailbreaks, hallucinations, and comprehension errors.","The models are assessed based on the total occurrences of jailbreaks, hallucinations, and comprehension errors.","Our work exposes these models' inherent vulnerabilities and challenges the notion of human-level language comprehension of these models.","We have empirically analysed the impact of non-standard Unicode characters on LLMs and their safeguarding mechanisms on the best-performing LLMs, including GPT-4, Gemini 1.5 Pro, LlaMA-3-70B, and Claude 3 Opus.","By incorporating alphanumeric symbols from Unicode outside the standard Latin block and variants of characters in other languages, we observed a reduction in the efficacy of guardrails implemented through Reinforcement Learning Human Feedback (RLHF).","Consequently, these models exhibit heightened vulnerability to content policy breaches and prompt leakage.","Our study also suggests a need to incorporate non-standard Unicode text in LLM training data to enhance the capabilities of these models."],"url":"http://arxiv.org/abs/2405.14490v1","category":"cs.CL"}
{"created":"2024-05-23 12:24:01","title":"End-to-End User-Defined Keyword Spotting using Shifted Delta Coefficients","abstract":"Identifying user-defined keywords is crucial for personalizing interactions with smart devices. Previous approaches of user-defined keyword spotting (UDKWS) have relied on short-term spectral features such as mel frequency cepstral coefficients (MFCC) to detect the spoken keyword. However, these features may face challenges in accurately identifying closely related pronunciation of audio-text pairs, due to their limited capability in capturing the temporal dynamics of the speech signal. To address this challenge, we propose to use shifted delta coefficients (SDC) which help in capturing pronunciation variability (transition between connecting phonemes) by incorporating long-term temporal information. The performance of the SDC feature is compared with various baseline features across four different datasets using a cross-attention based end-to-end system. Additionally, various configurations of SDC are explored to find the suitable temporal context for the UDKWS task. The experimental results reveal that the SDC feature outperforms the MFCC baseline feature, exhibiting an improvement of 8.32% in area under the curve (AUC) and 8.69% in terms of equal error rate (EER) on the challenging Libriphrase-hard dataset. Moreover, the proposed approach demonstrated superior performance when compared to state-of-the-art UDKWS techniques.","sentences":["Identifying user-defined keywords is crucial for personalizing interactions with smart devices.","Previous approaches of user-defined keyword spotting (UDKWS) have relied on short-term spectral features such as mel frequency cepstral coefficients (MFCC) to detect the spoken keyword.","However, these features may face challenges in accurately identifying closely related pronunciation of audio-text pairs, due to their limited capability in capturing the temporal dynamics of the speech signal.","To address this challenge, we propose to use shifted delta coefficients (SDC) which help in capturing pronunciation variability (transition between connecting phonemes) by incorporating long-term temporal information.","The performance of the SDC feature is compared with various baseline features across four different datasets using a cross-attention based end-to-end system.","Additionally, various configurations of SDC are explored to find the suitable temporal context for the UDKWS task.","The experimental results reveal that the SDC feature outperforms the MFCC baseline feature, exhibiting an improvement of 8.32% in area under the curve (AUC) and 8.69% in terms of equal error rate (EER) on the challenging Libriphrase-hard dataset.","Moreover, the proposed approach demonstrated superior performance when compared to state-of-the-art UDKWS techniques."],"url":"http://arxiv.org/abs/2405.14489v1","category":"cs.SD"}
{"created":"2024-05-23 12:19:07","title":"A Comprehensive Overview of Large Language Models (LLMs) for Cyber Defences: Opportunities and Directions","abstract":"The recent progression of Large Language Models (LLMs) has witnessed great success in the fields of data-centric applications. LLMs trained on massive textual datasets showed ability to encode not only context but also ability to provide powerful comprehension to downstream tasks. Interestingly, Generative Pre-trained Transformers utilised this ability to bring AI a step closer to human being replacement in at least datacentric applications. Such power can be leveraged to identify anomalies of cyber threats, enhance incident response, and automate routine security operations. We provide an overview for the recent activities of LLMs in cyber defence sections, as well as categorization for the cyber defence sections such as threat intelligence, vulnerability assessment, network security, privacy preserving, awareness and training, automation, and ethical guidelines. Fundamental concepts of the progression of LLMs from Transformers, Pre-trained Transformers, and GPT is presented. Next, the recent works of each section is surveyed with the related strengths and weaknesses. A special section about the challenges and directions of LLMs in cyber security is provided. Finally, possible future research directions for benefiting from LLMs in cyber security is discussed.","sentences":["The recent progression of Large Language Models (LLMs) has witnessed great success in the fields of data-centric applications.","LLMs trained on massive textual datasets showed ability to encode not only context but also ability to provide powerful comprehension to downstream tasks.","Interestingly, Generative Pre-trained Transformers utilised this ability to bring AI a step closer to human being replacement in at least datacentric applications.","Such power can be leveraged to identify anomalies of cyber threats, enhance incident response, and automate routine security operations.","We provide an overview for the recent activities of LLMs in cyber defence sections, as well as categorization for the cyber defence sections such as threat intelligence, vulnerability assessment, network security, privacy preserving, awareness and training, automation, and ethical guidelines.","Fundamental concepts of the progression of LLMs from Transformers, Pre-trained Transformers, and GPT is presented.","Next, the recent works of each section is surveyed with the related strengths and weaknesses.","A special section about the challenges and directions of LLMs in cyber security is provided.","Finally, possible future research directions for benefiting from LLMs in cyber security is discussed."],"url":"http://arxiv.org/abs/2405.14487v1","category":"cs.CR"}
{"created":"2024-05-23 12:18:11","title":"RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models","abstract":"Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at https://github.com/amazon-science/RefChecker","sentences":["Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate.","This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations.","In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference.","We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases.","We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs.","RefChecker supports both proprietary and open-source models as the extractor and checker.","Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims.","RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments.","This work is open sourced at https://github.com/amazon-science/RefChecker"],"url":"http://arxiv.org/abs/2405.14486v1","category":"cs.CL"}
{"created":"2024-05-23 12:15:55","title":"Novel semi-explicit symplectic schemes for nonseparable stochastic Hamiltonian systems","abstract":"In this manuscript, we propose efficient stochastic semi-explicit symplectic schemes tailored for nonseparable stochastic Hamiltonian systems (SHSs). These semi-explicit symplectic schemes are constructed by introducing augmented Hamiltonians and using symmetric projection. In the case of the artificial restraint in augmented Hamiltonians being zero, the proposed schemes also preserve quadratic invariants, making them suitable for developing semi-explicit charge-preserved multi-symplectic schemes for stochastic cubic Schr\\\"odinger equations with multiplicative noise. Through numerical experiments that validate theoretical results, we demonstrate that the proposed stochastic semi-explicit symplectic scheme, which features a straightforward Newton iteration solver, outperforms the traditional stochastic midpoint scheme in terms of effectiveness and accuracy.","sentences":["In this manuscript, we propose efficient stochastic semi-explicit symplectic schemes tailored for nonseparable stochastic Hamiltonian systems (SHSs).","These semi-explicit symplectic schemes are constructed by introducing augmented Hamiltonians and using symmetric projection.","In the case of the artificial restraint in augmented Hamiltonians being zero, the proposed schemes also preserve quadratic invariants, making them suitable for developing semi-explicit charge-preserved multi-symplectic schemes for stochastic cubic Schr\\\"odinger equations with multiplicative noise.","Through numerical experiments that validate theoretical results, we demonstrate that the proposed stochastic semi-explicit symplectic scheme, which features a straightforward Newton iteration solver, outperforms the traditional stochastic midpoint scheme in terms of effectiveness and accuracy."],"url":"http://arxiv.org/abs/2405.14484v1","category":"math.NA"}
{"created":"2024-05-23 12:06:49","title":"Theory of Generalized Landau Levels and Implication for non-Abelian States","abstract":"Quantum geometry is a fundamental concept to characterize the local properties of quantum states. It is recently demonstrated that saturating certain quantum geometric bounds allows a topological Chern band to share many essential features with the lowest Landau level, facilitating fractionalized phases in moir\\'e flat bands. In this work, we systematically extend the consequence and universality of saturated geometric bounds to arbitrary Landau levels by introducing a set of single-particle states, which we term as ``generalized Landau levels''. These generalized Landau levels exhibit exactly quantized values of integrated trace of quantum metric determined by their corresponding Landau level indices, regardless of the nonuniformity of their quantum geometric quantities. We derive all geometric quantities for individual and multiple generalized Landau levels, discuss their relations, and understand them in light of the theory of holomorphic curves and moving frames. We further propose a model by superposing few generalized Landau levels which is supposed to capture a large portion of the single-particle Hilbert space of a generic Chern band analogous to the first Landau level. Using this model, we employ exact diagonalization to identify a single-particle geometric criterion for permitting the non-Abelian Moore-Read phase, which is potentially useful for future engineering of moir\\'e materials and beyond. We use a double twisted bilayer graphene model with only adjacent layer hopping term to show the existence of first generalized Landau level type narrow band and zero-field Moore-Read state at the second magic angle which serves as a promising starting point for more detailed future studies. We expect that generalized Landau levels will serve as a systematic tool for analyzing topological Chern bands and fractionalized phases therein.","sentences":["Quantum geometry is a fundamental concept to characterize the local properties of quantum states.","It is recently demonstrated that saturating certain quantum geometric bounds allows a topological Chern band to share many essential features with the lowest Landau level, facilitating fractionalized phases in moir\\'e flat bands.","In this work, we systematically extend the consequence and universality of saturated geometric bounds to arbitrary Landau levels by introducing a set of single-particle states, which we term as ``generalized Landau levels''.","These generalized Landau levels exhibit exactly quantized values of integrated trace of quantum metric determined by their corresponding Landau level indices, regardless of the nonuniformity of their quantum geometric quantities.","We derive all geometric quantities for individual and multiple generalized Landau levels, discuss their relations, and understand them in light of the theory of holomorphic curves and moving frames.","We further propose a model by superposing few generalized Landau levels which is supposed to capture a large portion of the single-particle Hilbert space of a generic Chern band analogous to the first Landau level.","Using this model, we employ exact diagonalization to identify a single-particle geometric criterion for permitting the non-Abelian Moore-Read phase, which is potentially useful for future engineering of moir\\'e materials and beyond.","We use a double twisted bilayer graphene model with only adjacent layer hopping term to show the existence of first generalized Landau level type narrow band and zero-field Moore-Read state at the second magic angle which serves as a promising starting point for more detailed future studies.","We expect that generalized Landau levels will serve as a systematic tool for analyzing topological Chern bands and fractionalized phases therein."],"url":"http://arxiv.org/abs/2405.14479v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 12:06:10","title":"SLIFER: Investigating Performance and Robustness of Malware Detection Pipelines","abstract":"As a result of decades of research, Windows malware detection is approached through a plethora of techniques. However, there is an ongoing mismatch between academia -- which pursues an optimal performances in terms of detection rate and low false alarms -- and the requirements of real-world scenarios. In particular, academia focuses on combining static and dynamic analysis within a single or ensemble of models, falling into several pitfalls like (i) firing dynamic analysis without considering the computational burden it requires; (ii) discarding impossible-to-analyse samples; and (iii) analysing robustness against adversarial attacks without considering that malware detectors are complemented with more non-machine-learning components. Thus, in this paper we propose SLIFER, a novel Windows malware detection pipeline sequentially leveraging both static and dynamic analysis, interrupting computations as soon as one module triggers an alarm, requiring dynamic analysis only when needed. Contrary to the state of the art, we investigate how to deal with samples resistance to analysis, showing how much they impact performances, concluding that it is better to flag them as legitimate to not drastically increase false alarms. Lastly, we perform a robustness evaluation of SLIFER leveraging content-injections attacks, and we show that, counter-intuitively, attacks are blocked more by YARA rules than dynamic analysis due to byte artifacts created while optimizing the adversarial strategy.","sentences":["As a result of decades of research, Windows malware detection is approached through a plethora of techniques.","However, there is an ongoing mismatch between academia -- which pursues an optimal performances in terms of detection rate and low false alarms -- and the requirements of real-world scenarios.","In particular, academia focuses on combining static and dynamic analysis within a single or ensemble of models, falling into several pitfalls like (i) firing dynamic analysis without considering the computational burden it requires; (ii) discarding impossible-to-analyse samples; and (iii) analysing robustness against adversarial attacks without considering that malware detectors are complemented with more non-machine-learning components.","Thus, in this paper we propose SLIFER, a novel Windows malware detection pipeline sequentially leveraging both static and dynamic analysis, interrupting computations as soon as one module triggers an alarm, requiring dynamic analysis only when needed.","Contrary to the state of the art, we investigate how to deal with samples resistance to analysis, showing how much they impact performances, concluding that it is better to flag them as legitimate to not drastically increase false alarms.","Lastly, we perform a robustness evaluation of SLIFER leveraging content-injections attacks, and we show that, counter-intuitively, attacks are blocked more by YARA rules than dynamic analysis due to byte artifacts created while optimizing the adversarial strategy."],"url":"http://arxiv.org/abs/2405.14478v1","category":"cs.CR"}
{"created":"2024-05-23 12:06:00","title":"LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models","abstract":"Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a family of autoencoders for LDMs that leverage the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality. We also investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality. Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM).","sentences":["Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored.","In this paper, we introduce LiteVAE, a family of autoencoders for LDMs that leverage the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality.","We also investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality.","Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM)."],"url":"http://arxiv.org/abs/2405.14477v1","category":"cs.LG"}
{"created":"2024-05-23 12:04:51","title":"MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes","abstract":"While controllable generative models for images and videos have achieved remarkable success, high-quality models for 3D scenes, particularly in unbounded scenarios like autonomous driving, remain underdeveloped due to high data acquisition costs. In this paper, we introduce MagicDrive3D, a novel pipeline for controllable 3D street scene generation that supports multi-condition control, including BEV maps, 3D objects, and text descriptions. Unlike previous methods that reconstruct before training the generative models, MagicDrive3D first trains a video generation model and then reconstructs from the generated data. This innovative approach enables easily controllable generation and static scene acquisition, resulting in high-quality scene reconstruction. To address the minor errors in generated content, we propose deformable Gaussian splatting with monocular depth initialization and appearance modeling to manage exposure discrepancies across viewpoints. Validated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality 3D driving scenes that support any-view rendering and enhance downstream tasks like BEV segmentation. Our results demonstrate the framework's superior performance, showcasing its transformative potential for autonomous driving simulation and beyond.","sentences":["While controllable generative models for images and videos have achieved remarkable success, high-quality models for 3D scenes, particularly in unbounded scenarios like autonomous driving, remain underdeveloped due to high data acquisition costs.","In this paper, we introduce MagicDrive3D, a novel pipeline for controllable 3D street scene generation that supports multi-condition control, including BEV maps, 3D objects, and text descriptions.","Unlike previous methods that reconstruct before training the generative models, MagicDrive3D first trains a video generation model and then reconstructs from the generated data.","This innovative approach enables easily controllable generation and static scene acquisition, resulting in high-quality scene reconstruction.","To address the minor errors in generated content, we propose deformable Gaussian splatting with monocular depth initialization and appearance modeling to manage exposure discrepancies across viewpoints.","Validated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality 3D driving scenes that support any-view rendering and enhance downstream tasks like BEV segmentation.","Our results demonstrate the framework's superior performance, showcasing its transformative potential for autonomous driving simulation and beyond."],"url":"http://arxiv.org/abs/2405.14475v1","category":"cs.CV"}
{"created":"2024-05-23 12:04:46","title":"Time Cell Inspired Temporal Codebook in Spiking Neural Networks for Enhanced Image Generation","abstract":"This paper presents a novel approach leveraging Spiking Neural Networks (SNNs) to construct a Variational Quantized Autoencoder (VQ-VAE) with a temporal codebook inspired by hippocampal time cells. This design captures and utilizes temporal dependencies, significantly enhancing the generative capabilities of SNNs. Neuroscientific research has identified hippocampal \"time cells\" that fire sequentially during temporally structured experiences. Our temporal codebook emulates this behavior by triggering the activation of time cell populations based on similarity measures as input stimuli pass through it. We conducted extensive experiments on standard benchmark datasets, including MNIST, FashionMNIST, CIFAR10, CelebA, and downsampled LSUN Bedroom, to validate our model's performance. Furthermore, we evaluated the effectiveness of the temporal codebook on neuromorphic datasets NMNIST and DVS-CIFAR10, and demonstrated the model's capability with high-resolution datasets such as CelebA-HQ, LSUN Bedroom, and LSUN Church. The experimental results indicate that our method consistently outperforms existing SNN-based generative models across multiple datasets, achieving state-of-the-art performance. Notably, our approach excels in generating high-resolution and temporally consistent data, underscoring the crucial role of temporal information in SNN-based generative modeling.","sentences":["This paper presents a novel approach leveraging Spiking Neural Networks (SNNs) to construct a Variational Quantized Autoencoder (VQ-VAE) with a temporal codebook inspired by hippocampal time cells.","This design captures and utilizes temporal dependencies, significantly enhancing the generative capabilities of SNNs.","Neuroscientific research has identified hippocampal \"time cells\" that fire sequentially during temporally structured experiences.","Our temporal codebook emulates this behavior by triggering the activation of time cell populations based on similarity measures as input stimuli pass through it.","We conducted extensive experiments on standard benchmark datasets, including MNIST, FashionMNIST, CIFAR10, CelebA, and downsampled LSUN Bedroom, to validate our model's performance.","Furthermore, we evaluated the effectiveness of the temporal codebook on neuromorphic datasets NMNIST and DVS-CIFAR10, and demonstrated the model's capability with high-resolution datasets such as CelebA-HQ, LSUN Bedroom, and LSUN Church.","The experimental results indicate that our method consistently outperforms existing SNN-based generative models across multiple datasets, achieving state-of-the-art performance.","Notably, our approach excels in generating high-resolution and temporally consistent data, underscoring the crucial role of temporal information in SNN-based generative modeling."],"url":"http://arxiv.org/abs/2405.14474v1","category":"cs.NE"}
{"created":"2024-05-23 12:02:54","title":"Poisson Variational Autoencoder","abstract":"Variational autoencoders (VAE) employ Bayesian inference to interpret sensory inputs, mirroring processes that occur in primate vision across both ventral (Higgins et al., 2021) and dorsal (Vafaii et al., 2023) pathways. Despite their success, traditional VAEs rely on continuous latent variables, which deviates sharply from the discrete nature of biological neurons. Here, we developed the Poisson VAE (P-VAE), a novel architecture that combines principles of predictive coding with a VAE that encodes inputs into discrete spike counts. Combining Poisson-distributed latent variables with predictive coding introduces a metabolic cost term in the model loss function, suggesting a relationship with sparse coding which we verify empirically. Additionally, we analyze the geometry of learned representations, contrasting the P-VAE to alternative VAE models. We find that the P-VAEencodes its inputs in relatively higher dimensions, facilitating linear separability of categories in a downstream classification task with a much better (5x) sample efficiency. Our work provides an interpretable computational framework to study brain-like sensory processing and paves the way for a deeper understanding of perception as an inferential process.","sentences":["Variational autoencoders (VAE) employ Bayesian inference to interpret sensory inputs, mirroring processes that occur in primate vision across both ventral (Higgins et al., 2021) and dorsal (Vafaii et al., 2023) pathways.","Despite their success, traditional VAEs rely on continuous latent variables, which deviates sharply from the discrete nature of biological neurons.","Here, we developed the Poisson VAE (P-VAE), a novel architecture that combines principles of predictive coding with a VAE that encodes inputs into discrete spike counts.","Combining Poisson-distributed latent variables with predictive coding introduces a metabolic cost term in the model loss function, suggesting a relationship with sparse coding which we verify empirically.","Additionally, we analyze the geometry of learned representations, contrasting the P-VAE to alternative VAE models.","We find that the P-VAEencodes its inputs in relatively higher dimensions, facilitating linear separability of categories in a downstream classification task with a much better (5x) sample efficiency.","Our work provides an interpretable computational framework to study brain-like sensory processing and paves the way for a deeper understanding of perception as an inferential process."],"url":"http://arxiv.org/abs/2405.14473v1","category":"cs.LG"}
{"created":"2024-05-23 12:00:35","title":"SolNet: Open-source deep learning models for photovoltaic power forecasting across the globe","abstract":"Deep learning models have gained increasing prominence in recent years in the field of solar pho-tovoltaic (PV) forecasting. One drawback of these models is that they require a lot of high-quality data to perform well. This is often infeasible in practice, due to poor measurement infrastructure in legacy systems and the rapid build-up of new solar systems across the world. This paper proposes SolNet: a novel, general-purpose, multivariate solar power forecaster, which addresses these challenges by using a two-step forecasting pipeline which incorporates transfer learning from abundant synthetic data generated from PVGIS, before fine-tuning on observational data. Using actual production data from hundreds of sites in the Netherlands, Australia and Belgium, we show that SolNet improves forecasting performance over data-scarce settings as well as baseline models. We find transfer learning benefits to be the strongest when only limited observational data is available. At the same time we provide several guidelines and considerations for transfer learning practitioners, as our results show that weather data, seasonal patterns, amount of synthetic data and possible mis-specification in source location, can have a major impact on the results. The SolNet models created in this way are applicable for any land-based solar photovoltaic system across the planet where simulated and observed data can be combined to obtain improved forecasting capabilities.","sentences":["Deep learning models have gained increasing prominence in recent years in the field of solar pho-tovoltaic (PV) forecasting.","One drawback of these models is that they require a lot of high-quality data to perform well.","This is often infeasible in practice, due to poor measurement infrastructure in legacy systems and the rapid build-up of new solar systems across the world.","This paper proposes SolNet: a novel, general-purpose, multivariate solar power forecaster, which addresses these challenges by using a two-step forecasting pipeline which incorporates transfer learning from abundant synthetic data generated from PVGIS, before fine-tuning on observational data.","Using actual production data from hundreds of sites in the Netherlands, Australia and Belgium, we show that SolNet improves forecasting performance over data-scarce settings as well as baseline models.","We find transfer learning benefits to be the strongest when only limited observational data is available.","At the same time we provide several guidelines and considerations for transfer learning practitioners, as our results show that weather data, seasonal patterns, amount of synthetic data and possible mis-specification in source location, can have a major impact on the results.","The SolNet models created in this way are applicable for any land-based solar photovoltaic system across the planet where simulated and observed data can be combined to obtain improved forecasting capabilities."],"url":"http://arxiv.org/abs/2405.14472v1","category":"eess.SP"}
{"created":"2024-05-23 11:56:05","title":"Generalization of Hamiltonian algorithms","abstract":"The paper proves generalization results for a class of stochastic learning algorithms. The method applies whenever the algorithm generates an absolutely continuous distribution relative to some a-priori measure and the Radon Nikodym derivative has subgaussian concentration. Applications are bounds for the Gibbs algorithm and randomizations of stable deterministic algorithms as well as PAC-Bayesian bounds with data-dependent priors.","sentences":["The paper proves generalization results for a class of stochastic learning algorithms.","The method applies whenever the algorithm generates an absolutely continuous distribution relative to some a-priori measure and the Radon Nikodym derivative has subgaussian concentration.","Applications are bounds for the Gibbs algorithm and randomizations of stable deterministic algorithms as well as PAC-Bayesian bounds with data-dependent priors."],"url":"http://arxiv.org/abs/2405.14469v1","category":"cs.LG"}
{"created":"2024-05-23 11:54:27","title":"Segformer++: Efficient Token-Merging Strategies for High-Resolution Semantic Segmentation","abstract":"Utilizing transformer architectures for semantic segmentation of high-resolution images is hindered by the attention's quadratic computational complexity in the number of tokens. A solution to this challenge involves decreasing the number of tokens through token merging, which has exhibited remarkable enhancements in inference speed, training efficiency, and memory utilization for image classification tasks. In this paper, we explore various token merging strategies within the framework of the Segformer architecture and perform experiments on multiple semantic segmentation and human pose estimation datasets. Notably, without model re-training, we, for example, achieve an inference acceleration of 61% on the Cityscapes dataset while maintaining the mIoU performance. Consequently, this paper facilitates the deployment of transformer-based architectures on resource-constrained devices and in real-time applications.","sentences":["Utilizing transformer architectures for semantic segmentation of high-resolution images is hindered by the attention's quadratic computational complexity in the number of tokens.","A solution to this challenge involves decreasing the number of tokens through token merging, which has exhibited remarkable enhancements in inference speed, training efficiency, and memory utilization for image classification tasks.","In this paper, we explore various token merging strategies within the framework of the Segformer architecture and perform experiments on multiple semantic segmentation and","human pose estimation datasets.","Notably, without model re-training, we, for example, achieve an inference acceleration of 61% on the Cityscapes dataset while maintaining the mIoU performance.","Consequently, this paper facilitates the deployment of transformer-based architectures on resource-constrained devices and in real-time applications."],"url":"http://arxiv.org/abs/2405.14467v1","category":"cs.CV"}
{"created":"2024-05-23 11:54:04","title":"Foams with flat connections and algebraic K-theory","abstract":"This paper proposes a connection between algebraic K-theory and foam cobordisms, where foams are stratified manifolds with singularities of a prescribed form. We consider $n$-dimensional foams equipped with a flat bundle of finitely-generated projective $R$-modules over each facet of the foam, together with gluing conditions along the subfoam of singular points. In a suitable sense which will become clear, a vertex (or the smallest stratum) of an $n$-dimensional foam replaces an $(n+1)$-simplex with a total ordering of vertices. We show that the first K-theory group of a ring $R$ can be identified with the cobordism group of decorated 1-foams embedded in the plane. A similar relation between the $n$-th algebraic K-theory group of a ring $R$ and the cobordism group of decorated $n$-foams embedded in $\\mathbb{R}^{n+1}$ is expected for $n>1$. An analogous correspondence is proposed for arbitrary exact categories. Modifying the embedding and other conditions on the foams may lead to new flavors of K-theory groups.","sentences":["This paper proposes a connection between algebraic K-theory and foam cobordisms, where foams are stratified manifolds with singularities of a prescribed form.","We consider $n$-dimensional foams equipped with a flat bundle of finitely-generated projective $R$-modules over each facet of the foam, together with gluing conditions along the subfoam of singular points.","In a suitable sense which will become clear, a vertex (or the smallest stratum) of an $n$-dimensional foam replaces an $(n+1)$-simplex with a total ordering of vertices.","We show that the first K-theory group of a ring $R$ can be identified with the cobordism group of decorated 1-foams embedded in the plane.","A similar relation between the $n$-th algebraic K-theory group of a ring $R$ and the cobordism group of decorated $n$-foams embedded in $\\mathbb{R}^{n+1}$ is expected for $n>1$. An analogous correspondence is proposed for arbitrary exact categories.","Modifying the embedding and other conditions on the foams may lead to new flavors of K-theory groups."],"url":"http://arxiv.org/abs/2405.14465v1","category":"math.KT"}
{"created":"2024-05-23 11:52:28","title":"Epistemic EFX Allocations Exist for Monotone Valuations","abstract":"We study the fundamental problem of fairly dividing a set of indivisible items among agents with (general) monotone valuations. The notion of envy-freeness up to any item (EFX) is considered to be one of the most fascinating fairness concepts in this line of work. Unfortunately, despite significant efforts, existence of EFX allocations is a major open problem in fair division, thereby making the study of approximations and relaxations of EFX a natural line of research. Recently, Caragiannis et al. introduced a promising relaxation of EFX, called epistemic EFX (EEFX). We say an allocation to be EEFX if, for every agent, it is possible to shuffle the items in the remaining bundles so that she becomes \"EFX-satisfied\". Caragiannis et al. prove existence and polynomial-time computability of EEFX allocations for additive valuations. A natural question asks what happens when we consider valuations more general than additive?   We address this important open question and answer it affirmatively by establishing the existence of EEFX allocations for an arbitrary number of agents with general monotone valuations. To the best of our knowledge, EEFX is the only known relaxation of EFX to have such strong existential guarantees. Furthermore, we complement our existential result by proving computational and information-theoretic lower bounds. We prove that even for an arbitrary number of (more than one) agents with identical submodular valuations, it is PLS-hard to compute EEFX allocations and it requires exponentially-many value queries to do so.","sentences":["We study the fundamental problem of fairly dividing a set of indivisible items among agents with (general) monotone valuations.","The notion of envy-freeness up to any item (EFX) is considered to be one of the most fascinating fairness concepts in this line of work.","Unfortunately, despite significant efforts, existence of EFX allocations is a major open problem in fair division, thereby making the study of approximations and relaxations of EFX a natural line of research.","Recently, Caragiannis et al. introduced a promising relaxation of EFX, called epistemic EFX (EEFX).","We say an allocation to be EEFX if, for every agent, it is possible to shuffle the items in the remaining bundles so that she becomes \"EFX-satisfied\".","Caragiannis et al. prove existence and polynomial-time computability of EEFX allocations for additive valuations.","A natural question asks what happens when we consider valuations more general than additive?   ","We address this important open question and answer it affirmatively by establishing the existence of EEFX allocations for an arbitrary number of agents with general monotone valuations.","To the best of our knowledge, EEFX is the only known relaxation of EFX to have such strong existential guarantees.","Furthermore, we complement our existential result by proving computational and information-theoretic lower bounds.","We prove that even for an arbitrary number of (more than one) agents with identical submodular valuations, it is PLS-hard to compute EEFX allocations and it requires exponentially-many value queries to do so."],"url":"http://arxiv.org/abs/2405.14463v1","category":"cs.GT"}
{"created":"2024-05-23 11:51:34","title":"Fermat Principle and weak deflection angle from Lindstedt-Poincare method","abstract":"The Fermat principle is used for the analysis of light propagation in the metric of a black hole. It is shown how to apply the Lindstedt-Poincare method of solving perturbatively the nonlinear oscillations to the description of light trajectories in weak deflection regime.","sentences":["The Fermat principle is used for the analysis of light propagation in the metric of a black hole.","It is shown how to apply the Lindstedt-Poincare method of solving perturbatively the nonlinear oscillations to the description of light trajectories in weak deflection regime."],"url":"http://arxiv.org/abs/2405.14462v1","category":"gr-qc"}
{"created":"2024-05-23 11:44:29","title":"YOLOv10: Real-Time End-to-End Object Detection","abstract":"Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8$\\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8$\\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\\% less latency and 25\\% fewer parameters for the same performance.","sentences":["Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance.","Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress.","However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency.","Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability.","It renders the suboptimal efficiency, along with considerable potential for performance improvements.","In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture.","To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously.","Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs.","We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability.","The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10.","Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales.","For example, our YOLOv10-S is 1.8$\\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8$\\times$ smaller number of parameters and FLOPs.","Compared with YOLOv9-C, YOLOv10-B has 46\\% less latency and 25\\% fewer parameters for the same performance."],"url":"http://arxiv.org/abs/2405.14458v1","category":"cs.CV"}
{"created":"2024-05-23 11:38:38","title":"Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model","abstract":"Machine learning models can be trained with formal privacy guarantees via differentially private optimizers such as DP-SGD. In this work, we study such privacy guarantees when the adversary only accesses the final model, i.e., intermediate model updates are not released. In the existing literature, this hidden state threat model exhibits a significant gap between the lower bound provided by empirical privacy auditing and the theoretical upper bound provided by privacy accounting. To challenge this gap, we propose to audit this threat model with adversaries that craft a gradient sequence to maximize the privacy loss of the final model without accessing intermediate models. We demonstrate experimentally how this approach consistently outperforms prior attempts at auditing the hidden state model. When the crafted gradient is inserted at every optimization step, our results imply that releasing only the final model does not amplify privacy, providing a novel negative result. On the other hand, when the crafted gradient is not inserted at every step, we show strong evidence that a privacy amplification phenomenon emerges in the general non-convex setting (albeit weaker than in convex regimes), suggesting that existing privacy upper bounds can be improved.","sentences":["Machine learning models can be trained with formal privacy guarantees via differentially private optimizers such as DP-SGD.","In this work, we study such privacy guarantees when the adversary only accesses the final model, i.e., intermediate model updates are not released.","In the existing literature, this hidden state threat model exhibits a significant gap between the lower bound provided by empirical privacy auditing and the theoretical upper bound provided by privacy accounting.","To challenge this gap, we propose to audit this threat model with adversaries that craft a gradient sequence to maximize the privacy loss of the final model without accessing intermediate models.","We demonstrate experimentally how this approach consistently outperforms prior attempts at auditing the hidden state model.","When the crafted gradient is inserted at every optimization step, our results imply that releasing only the final model does not amplify privacy, providing a novel negative result.","On the other hand, when the crafted gradient is not inserted at every step, we show strong evidence that a privacy amplification phenomenon emerges in the general non-convex setting (albeit weaker than in convex regimes), suggesting that existing privacy upper bounds can be improved."],"url":"http://arxiv.org/abs/2405.14457v1","category":"cs.LG"}
{"created":"2024-05-23 11:37:17","title":"TIGER: Text-Instructed 3D Gaussian Retrieval and Coherent Editing","abstract":"Editing objects within a scene is a critical functionality required across a broad spectrum of applications in computer vision and graphics. As 3D Gaussian Splatting (3DGS) emerges as a frontier in scene representation, the effective modification of 3D Gaussian scenes has become increasingly vital. This process entails accurately retrieve the target objects and subsequently performing modifications based on instructions. Though available in pieces, existing techniques mainly embed sparse semantics into Gaussians for retrieval, and rely on an iterative dataset update paradigm for editing, leading to over-smoothing or inconsistency issues. To this end, this paper proposes a systematic approach, namely TIGER, for coherent text-instructed 3D Gaussian retrieval and editing. In contrast to the top-down language grounding approach for 3D Gaussians, we adopt a bottom-up language aggregation strategy to generate a denser language embedded 3D Gaussians that supports open-vocabulary retrieval. To overcome the over-smoothing and inconsistency issues in editing, we propose a Coherent Score Distillation (CSD) that aggregates a 2D image editing diffusion model and a multi-view diffusion model for score distillation, producing multi-view consistent editing with much finer details. In various experiments, we demonstrate that our TIGER is able to accomplish more consistent and realistic edits than prior work.","sentences":["Editing objects within a scene is a critical functionality required across a broad spectrum of applications in computer vision and graphics.","As 3D Gaussian Splatting (3DGS) emerges as a frontier in scene representation, the effective modification of 3D Gaussian scenes has become increasingly vital.","This process entails accurately retrieve the target objects and subsequently performing modifications based on instructions.","Though available in pieces, existing techniques mainly embed sparse semantics into Gaussians for retrieval, and rely on an iterative dataset update paradigm for editing, leading to over-smoothing or inconsistency issues.","To this end, this paper proposes a systematic approach, namely TIGER, for coherent text-instructed 3D Gaussian retrieval and editing.","In contrast to the top-down language grounding approach for 3D Gaussians, we adopt a bottom-up language aggregation strategy to generate a denser language embedded 3D Gaussians that supports open-vocabulary retrieval.","To overcome the over-smoothing and inconsistency issues in editing, we propose a Coherent Score Distillation (CSD) that aggregates a 2D image editing diffusion model and a multi-view diffusion model for score distillation, producing multi-view consistent editing with much finer details.","In various experiments, we demonstrate that our TIGER is able to accomplish more consistent and realistic edits than prior work."],"url":"http://arxiv.org/abs/2405.14455v1","category":"cs.CV"}
{"created":"2024-05-23 11:32:46","title":"JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression","abstract":"Neural Radiance Field (NeRF) excels in photo-realistically static scenes, inspiring numerous efforts to facilitate volumetric videos. However, rendering dynamic and long-sequence radiance fields remains challenging due to the significant data required to represent volumetric videos. In this paper, we propose a novel end-to-end joint optimization scheme of dynamic NeRF representation and compression, called JointRF, thus achieving significantly improved quality and compression efficiency against the previous methods. Specifically, JointRF employs a compact residual feature grid and a coefficient feature grid to represent the dynamic NeRF. This representation handles large motions without compromising quality while concurrently diminishing temporal redundancy. We also introduce a sequential feature compression subnetwork to further reduce spatial-temporal redundancy. Finally, the representation and compression subnetworks are end-to-end trained combined within the JointRF. Extensive experiments demonstrate that JointRF can achieve superior compression performance across various datasets.","sentences":["Neural Radiance Field (NeRF) excels in photo-realistically static scenes, inspiring numerous efforts to facilitate volumetric videos.","However, rendering dynamic and long-sequence radiance fields remains challenging due to the significant data required to represent volumetric videos.","In this paper, we propose a novel end-to-end joint optimization scheme of dynamic NeRF representation and compression, called JointRF, thus achieving significantly improved quality and compression efficiency against the previous methods.","Specifically, JointRF employs a compact residual feature grid and a coefficient feature grid to represent the dynamic NeRF.","This representation handles large motions without compromising quality while concurrently diminishing temporal redundancy.","We also introduce a sequential feature compression subnetwork to further reduce spatial-temporal redundancy.","Finally, the representation and compression subnetworks are end-to-end trained combined within the JointRF.","Extensive experiments demonstrate that JointRF can achieve superior compression performance across various datasets."],"url":"http://arxiv.org/abs/2405.14452v1","category":"cs.CV"}
{"created":"2024-05-23 11:29:33","title":"Adversarial Schr\u00f6dinger Bridge Matching","abstract":"The Schr\\\"odinger Bridge (SB) problem offers a powerful framework for combining optimal transport and diffusion models. A promising recent approach to solve the SB problem is the Iterative Markovian Fitting (IMF) procedure, which alternates between Markovian and reciprocal projections of continuous-time stochastic processes. However, the model built by the IMF procedure has a long inference time due to using many steps of numerical solvers for stochastic differential equations. To address this limitation, we propose a novel Discrete-time IMF (D-IMF) procedure in which learning of stochastic processes is replaced by learning just a few transition probabilities in discrete time. Its great advantage is that in practice it can be naturally implemented using the Denoising Diffusion GAN (DD-GAN), an already well-established adversarial generative modeling technique. We show that our D-IMF procedure can provide the same quality of unpaired domain translation as the IMF, using only several generation steps instead of hundreds.","sentences":["The Schr\\\"odinger Bridge (SB) problem offers a powerful framework for combining optimal transport and diffusion models.","A promising recent approach to solve the SB problem is the Iterative Markovian Fitting (IMF) procedure, which alternates between Markovian and reciprocal projections of continuous-time stochastic processes.","However, the model built by the IMF procedure has a long inference time due to using many steps of numerical solvers for stochastic differential equations.","To address this limitation, we propose a novel Discrete-time IMF (D-IMF) procedure in which learning of stochastic processes is replaced by learning just a few transition probabilities in discrete time.","Its great advantage is that in practice it can be naturally implemented using the Denoising Diffusion GAN (DD-GAN), an already well-established adversarial generative modeling technique.","We show that our D-IMF procedure can provide the same quality of unpaired domain translation as the IMF, using only several generation steps instead of hundreds."],"url":"http://arxiv.org/abs/2405.14449v1","category":"cs.LG"}
{"created":"2024-05-23 11:28:07","title":"What can be the limit in the CLT for a field of martingale differences?","abstract":"The now classical convergence in distribution theorem for well normalized sums ofstationary martingale increments has been extended to multi-indexed martingaleincrements (see Voln\\'{y} (2019) and references in there). In the presentarticle we make progress in the identification of the limit law.In dimension one, as soon as the stationary martingale increments form an ergodic process, the limit law is normal, and it is stillthe case for multi-indexed martingale increments when one of the processes defined by one coordinate of the{\\it multidimensional time} is ergodic. In the general case, the limit may be non normal.The dynamical properties of the $\\mathbb{Z}^d$-measure preserving action associatedto the stationary random field allows us to give a necessary and sufficient conditionfor the existence of a non-normal limit law, in terms of entropy of some random processes.The identification of a {\\it natural} factor on which the $\\mathbb{Z}^d$-action is {\\it of product type","sentences":["The now classical convergence in distribution theorem for well normalized sums ofstationary martingale increments has been extended to multi-indexed martingaleincrements (see Voln\\'{y} (2019) and references in there).","In the presentarticle we make progress in the identification of the limit law.","In dimension one, as soon as the stationary martingale increments form an ergodic process, the limit law is normal, and it is stillthe case for multi-indexed martingale increments when one of the processes defined by one coordinate of the{\\it multidimensional time} is ergodic.","In the general case, the limit may be non normal.","The dynamical properties of the $\\mathbb{Z}^d$-measure preserving action associatedto the stationary random field allows us to give a necessary and sufficient conditionfor the existence of a non-normal limit law, in terms of entropy of some random processes.","The identification of a {\\it natural} factor on which the $\\mathbb{Z}^d$-action is {\\it of product type"],"url":"http://arxiv.org/abs/2405.14447v1","category":"math.DS"}
{"created":"2024-05-23 17:44:02","title":"An augmented Lagrangian trust-region method with inexact gradient evaluations to accelerate constrained optimization problems using model hyperreduction","abstract":"We present an augmented Lagrangian trust-region method to efficiently solve constrained optimization problems governed by large-scale nonlinear systems with application to partial differential equation-constrained optimization. At each major augmented Lagrangian iteration, the expensive optimization subproblem involving the full nonlinear system is replaced by an empirical quadrature-based hyperreduced model constructed on-the-fly. To ensure convergence of these inexact augmented Lagrangian subproblems, we develop a bound-constrained trust-region method that allows for inexact gradient evaluations, and specialize it to our specific setting that leverages hyperreduced models. This approach circumvents a traditional training phase because the models are built on-the-fly in accordance with the requirements of the trust-region convergence theory. Two numerical experiments (constrained aerodynamic shape design) demonstrate the convergence and efficiency of the proposed work. A speedup of 12.7x (for all computational costs, even costs traditionally considered \"offline\" such as snapshot collection and data compression) relative to a standard optimization approach that does not leverage model reduction is shown.","sentences":["We present an augmented Lagrangian trust-region method to efficiently solve constrained optimization problems governed by large-scale nonlinear systems with application to partial differential equation-constrained optimization.","At each major augmented Lagrangian iteration, the expensive optimization subproblem involving the full nonlinear system is replaced by an empirical quadrature-based hyperreduced model constructed on-the-fly.","To ensure convergence of these inexact augmented Lagrangian subproblems, we develop a bound-constrained trust-region method that allows for inexact gradient evaluations, and specialize it to our specific setting that leverages hyperreduced models.","This approach circumvents a traditional training phase because the models are built on-the-fly in accordance with the requirements of the trust-region convergence theory.","Two numerical experiments (constrained aerodynamic shape design) demonstrate the convergence and efficiency of the proposed work.","A speedup of 12.7x (for all computational costs, even costs traditionally considered \"offline\" such as snapshot collection and data compression) relative to a standard optimization approach that does not leverage model reduction is shown."],"url":"http://arxiv.org/abs/2405.14827v1","category":"math.OC"}
{"created":"2024-05-23 16:32:42","title":"Multi-Messenger Emission Characteristics of Blazars","abstract":"Multi-Messenger observations and theory of astrophysical objects is fast becoming a critical research area in the astrophysics scientific community. In particular, point-like objects like that of BL Lac, flat spectrum radio quasars (FSRQ), and blazar candidates of uncertain type (BCU) are of distinct interest among those who look at the synchrotron, Compton, neutrino, and cosmic ray emissions sourced from compact objects. Notably, there is also much interest in the correlation between multi-frequency observations of blazars and neutrino surveys on source demographics. In this review we look at such multi-frequency and multi-physics correlations of the radio, X-ray, and $\\gamma$-ray fluxes of different classes of blazars from a collection of survey catalogues. This multi-physics survey of blazars shows that there are characteristic cross-correlations in the spectra of blazars when considering their multi-frequency and multi-messenger emission. Accompanying this will be a review of cosmic ray and neutrino emissions from blazars and their characteristics.","sentences":["Multi-Messenger observations and theory of astrophysical objects is fast becoming a critical research area in the astrophysics scientific community.","In particular, point-like objects like that of BL Lac, flat spectrum radio quasars (FSRQ), and blazar candidates of uncertain type (BCU) are of distinct interest among those who look at the synchrotron, Compton, neutrino, and cosmic ray emissions sourced from compact objects.","Notably, there is also much interest in the correlation between multi-frequency observations of blazars and neutrino surveys on source demographics.","In this review we look at such multi-frequency and multi-physics correlations of the radio, X-ray, and $\\gamma$-ray fluxes of different classes of blazars from a collection of survey catalogues.","This multi-physics survey of blazars shows that there are characteristic cross-correlations in the spectra of blazars when considering their multi-frequency and multi-messenger emission.","Accompanying this will be a review of cosmic ray and neutrino emissions from blazars and their characteristics."],"url":"http://arxiv.org/abs/2405.14764v1","category":"astro-ph.HE"}
{"created":"2024-05-23 16:29:02","title":"Searches for violation of Lorentz invariance in $\\mathrm{t\\bar{t}}$ production using dilepton events in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search for violation of Lorentz invariance in the production of top quark pairs ($\\mathrm{t\\bar{t}}$) is presented. The measured normalized differential $\\mathrm{t\\bar{t}}$ production cross section, as function of the sidereal time, is examined for potential modulations induced by Lorentz-invariance breaking operators in an effective field theory extension of the standard model (SM). The cross section is measured from collision events collected by the CMS detector at a center-of-mass-energy of 13 TeV, corresponding to an integrated luminosity of 77.8 fb$^{-1}$, and containing one electron and one muon. The results are found to be compatible with zero, in agreement with the SM, and are used to bound the Lorentz-violating couplings to be in ranges of 1 - 8 $\\times$ 10$^{-3}$ at 68% confidence level. This is the first precision test of the isotropy in special relativity with top quarks at the LHC, restricting further the bounds on such couplings by up two orders of magnitude with respect to previous searches conducted at the Tevatron.","sentences":["A search for violation of Lorentz invariance in the production of top quark pairs ($\\mathrm{t\\bar{t}}$) is presented.","The measured normalized differential $\\mathrm{t\\bar{t}}$ production cross section, as function of the sidereal time, is examined for potential modulations induced by Lorentz-invariance breaking operators in an effective field theory extension of the standard model (SM).","The cross section is measured from collision events collected by the CMS detector at a center-of-mass-energy of 13 TeV, corresponding to an integrated luminosity of 77.8 fb$^{-1}$, and containing one electron and one muon.","The results are found to be compatible with zero, in agreement with the SM, and are used to bound the Lorentz-violating couplings to be in ranges of 1 - 8 $\\times$ 10$^{-3}$ at 68% confidence level.","This is the first precision test of the isotropy in special relativity with top quarks at the LHC, restricting further the bounds on such couplings by up two orders of magnitude with respect to previous searches conducted at the Tevatron."],"url":"http://arxiv.org/abs/2405.14757v1","category":"hep-ex"}
{"created":"2024-05-23 15:26:13","title":"Multilevel functional data analysis modeling of human glucose response to meal intake","abstract":"Glucose meal response information collected via Continuous Glucose Monitoring (CGM) is relevant to the assessment of individual metabolic status and the support of personalized diet prescriptions. However, the complexity of the data produced by CGM monitors pushes the limits of existing analytic methods. CGM data often exhibits substantial within-person variability and has a natural multilevel structure. This research is motivated by the analysis of CGM data from individuals without diabetes in the AEGIS study. The dataset includes detailed information on meal timing and nutrition for each individual over different days. The primary focus of this study is to examine CGM glucose responses following patients' meals and explore the time-dependent associations with dietary and patient characteristics. Motivated by this problem, we propose a new analytical framework based on multilevel functional models, including a new functional mixed R-square coefficient. The use of these models illustrates 3 key points: (i) The importance of analyzing glucose responses across the entire functional domain when making diet recommendations; (ii) The differential metabolic responses between normoglycemic and prediabetic patients, particularly with regards to lipid intake; (iii) The importance of including random, person-level effects when modelling this scientific problem.","sentences":["Glucose meal response information collected via Continuous Glucose Monitoring (CGM) is relevant to the assessment of individual metabolic status and the support of personalized diet prescriptions.","However, the complexity of the data produced by CGM monitors pushes the limits of existing analytic methods.","CGM data often exhibits substantial within-person variability and has a natural multilevel structure.","This research is motivated by the analysis of CGM data from individuals without diabetes in the AEGIS study.","The dataset includes detailed information on meal timing and nutrition for each individual over different days.","The primary focus of this study is to examine CGM glucose responses following patients' meals and explore the time-dependent associations with dietary and patient characteristics.","Motivated by this problem, we propose a new analytical framework based on multilevel functional models, including a new functional mixed R-square coefficient.","The use of these models illustrates 3 key points: (i) The importance of analyzing glucose responses across the entire functional domain when making diet recommendations; (ii) The differential metabolic responses between normoglycemic and prediabetic patients, particularly with regards to lipid intake; (iii) The importance of including random, person-level effects when modelling this scientific problem."],"url":"http://arxiv.org/abs/2405.14690v1","category":"q-bio.QM"}
{"created":"2024-05-23 14:32:09","title":"Test of light-lepton universality in $\u03c4$ decays with the Belle II experiment","abstract":"We present a measurement of the ratio $R_\\mu = \\mathcal{B}(\\tau^-\\to \\mu^-\\bar\\nu_\\mu\\nu_\\tau) / \\mathcal{B}(\\tau^-\\to e^-\\bar\\nu_e\\nu_\\tau)$ of branching fractions $\\mathcal{B}$ of the $\\tau$ lepton decaying to muons or electrons using data collected with the Belle II detector at the SuperKEKB $e^+e^-$ collider. The sample has an integrated luminosity of 362 fb$^{-1}$ at a centre-of-mass energy of 10.58 GeV. Using an optimised event selection, a binned maximum likelihood fit is performed using the momentum spectra of the electron and muon candidates. The result, $R_\\mu = 0.9675 \\pm 0.0007 \\pm 0.0036$, where the first uncertainty is statistical and the second is systematic, is the most precise to date. It provides a stringent test of the light-lepton universality, translating to a ratio of the couplings of the muon and electron to the $W$ boson in $\\tau$ decays of $0.9974 \\pm 0.0019$, in agreement with the standard model expectation of unity.","sentences":["We present a measurement of the ratio $R_\\mu = \\mathcal{B}(\\tau^-\\to \\mu^-\\bar\\nu_\\mu\\nu_\\tau) /","\\mathcal{B}(\\tau^-\\to e^-\\bar\\nu_e\\nu_\\tau)$ of branching fractions $\\mathcal{B}$ of the $\\tau$ lepton decaying to muons or electrons using data collected with the Belle II detector at the SuperKEKB $e^+e^-$ collider.","The sample has an integrated luminosity of 362 fb$^{-1}$","at a centre-of-mass energy of 10.58 GeV. Using an optimised event selection, a binned maximum likelihood fit is performed using the momentum spectra of the electron and muon candidates.","The result, $R_\\mu = 0.9675 \\pm 0.0007 \\pm 0.0036$, where the first uncertainty is statistical and the second is systematic, is the most precise to date.","It provides a stringent test of the light-lepton universality, translating to a ratio of the couplings of the muon and electron to the $W$ boson in $\\tau$ decays of $0.9974 \\pm 0.0019$, in agreement with the standard model expectation of unity."],"url":"http://arxiv.org/abs/2405.14625v1","category":"hep-ex"}
{"created":"2024-05-23 13:46:35","title":"Charm fragmentation fractions and ${\\rm c\\overline{c}}$ cross section in p$-$Pb collisions at $\\sqrt{s_{\\rm NN}}=5.02$ TeV","abstract":"The total charm-quark production cross section per unit of rapidity $\\mathrm{d}\\sigma({\\rm c\\overline{c}})/\\mathrm{d}y$, and the fragmentation fractions of charm quarks to different charm-hadron species $f(\\mathrm{c}\\rightarrow {\\rm h_{c}})$, are measured for the first time in p$-$Pb collisions at $\\sqrt{s_\\mathrm{NN}} = 5.02$ TeV at midrapidity ($-0.96<y<0.04$ in the centre-of-mass frame) using data collected by ALICE at the CERN LHC. The results are obtained based on all the available measurements of prompt production of ground-state charm-hadron species: $\\mathrm{D}^{0}$, $\\mathrm{D}^{+}$, $\\mathrm{D}_\\mathrm{s}^{+}$, and $\\mathrm{J/\\psi}$ mesons, and $\\Lambda_\\mathrm{c}^{+}$ and $\\Xi_{\\rm c}^{0}$ baryons. The resulting cross section is $\\mathrm{d}\\sigma({\\rm c\\overline{c}})/\\mathrm{d}y =219.6 \\pm 6.3\\;(\\mathrm{stat.}) {\\;}_{-11.8}^{+10.5}\\;(\\mathrm{syst.}) {\\;}_{-2.9}^{+7.6}\\;(\\mathrm{extr.})\\pm 5.4\\;(\\mathrm{BR})\\pm 4.6\\;(\\mathrm{lumi.}) \\pm 19.5\\;(\\text{rapidity shape})+15.0\\;(\\Omega_{\\rm c}^{0})$ mb, which is consistent with a binary scaling of pQCD calculations from pp collisions. The measured fragmentation fractions are compatible with those measured in pp collisions at $\\sqrt{s} = 5.02$ and $13$ TeV, showing an increase in the relative production rates of charm baryons with respect to charm mesons in pp and p$-$Pb collisions compared with $\\mathrm{e^{+}e^{-}}$ and $\\mathrm{e^{-}p}$ collisions. The $p_\\mathrm{T}$-integrated nuclear modification factor of charm quarks, $R_\\mathrm{pPb}({\\rm c\\overline{c}})= 0.91 \\pm 0.04\\;{\\rm (stat.)}{}^{+0.08}_{-0.09}\\;{\\rm (syst.)}{}^{+0.04}_{-0.03}\\;{\\rm (extr.)}{}\\pm 0.03\\;{\\rm (lumi.)}$, is found to be consistent with unity and with theoretical predictions including nuclear modifications of the parton distribution functions.","sentences":["The total charm-quark production cross section per unit of rapidity $\\mathrm{d}\\sigma({\\rm c\\overline{c}})/\\mathrm{d}y$, and the fragmentation fractions of charm quarks to different charm-hadron species $f(\\mathrm{c}\\rightarrow {\\rm h_{c}})$, are measured for the first time in p$-$Pb collisions at $\\sqrt{s_\\mathrm{NN}} = 5.02$ TeV at midrapidity ($-0.96<y<0.04$ in the centre-of-mass frame) using data collected by ALICE at the CERN LHC.","The results are obtained based on all the available measurements of prompt production of ground-state charm-hadron species: $\\mathrm{D}^{0}$, $\\mathrm{D}^{+}$, $\\mathrm{D}_\\mathrm{s}^{+}$, and $\\mathrm{J/\\psi}$ mesons, and $\\Lambda_\\mathrm{c}^{+}$ and $\\Xi_{\\rm c}^{0}$ baryons.","The resulting cross section is $\\mathrm{d}\\sigma({\\rm c\\overline{c}})/\\mathrm{d}y =219.6 \\pm 6.3\\;(\\mathrm{stat.})","{\\;}_{-11.8}^{+10.5}\\;(\\mathrm{syst.})","{\\;}_{-2.9}^{+7.6}\\;(\\mathrm{extr.})\\pm 5.4\\;(\\mathrm{BR})\\pm 4.6\\;(\\mathrm{lumi.})","\\pm 19.5\\;(\\text{rapidity shape})+15.0\\;(\\Omega_{\\rm c}^{0})$ mb, which is consistent with a binary scaling of pQCD calculations from pp collisions.","The measured fragmentation fractions are compatible with those measured in pp collisions at $\\sqrt{s} = 5.02$ and $13$ TeV, showing an increase in the relative production rates of charm baryons with respect to charm mesons in pp and p$-$Pb collisions compared with $\\mathrm{e^{+}e^{-}}$ and $\\mathrm{e^{-}p}$ collisions.","The $p_\\mathrm{T}$-integrated nuclear modification factor of charm quarks, $R_\\mathrm{pPb}({\\rm c\\overline{c}})= 0.91 \\pm 0.04\\;{\\rm (stat.)}{}^{+0.08}_{-0.09}\\;{\\rm (syst.)}{}^{+0.04}_{-0.03}\\;{\\rm (extr.)}{}\\pm 0.03\\;{\\rm (lumi.)}$, is found to be consistent with unity and with theoretical predictions including nuclear modifications of the parton distribution functions."],"url":"http://arxiv.org/abs/2405.14571v1","category":"nucl-ex"}
{"created":"2024-05-23 13:22:59","title":"This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian Optimization","abstract":"Bayesian Optimization (BO) has proven to be very successful at optimizing a static, noisy, costly-to-evaluate black-box function $f : \\mathcal{S} \\to \\mathbb{R}$. However, optimizing a black-box which is also a function of time (i.e., a dynamic function) $f : \\mathcal{S} \\times \\mathcal{T} \\to \\mathbb{R}$ remains a challenge, since a dynamic Bayesian Optimization (DBO) algorithm has to keep track of the optimum over time. This changes the nature of the optimization problem in at least three aspects: (i) querying an arbitrary point in $\\mathcal{S} \\times \\mathcal{T}$ is impossible, (ii) past observations become less and less relevant for keeping track of the optimum as time goes by and (iii) the DBO algorithm must have a high sampling frequency so it can collect enough relevant observations to keep track of the optimum through time. In this paper, we design a Wasserstein distance-based criterion able to quantify the relevancy of an observation with respect to future predictions. Then, we leverage this criterion to build W-DBO, a DBO algorithm able to remove irrelevant observations from its dataset on the fly, thus maintaining simultaneously a good predictive performance and a high sampling frequency, even in continuous-time optimization tasks with unknown horizon. Numerical experiments establish the superiority of W-DBO, which outperforms state-of-the-art methods by a comfortable margin.","sentences":["Bayesian Optimization (BO) has proven to be very successful at optimizing a static, noisy, costly-to-evaluate black-box function $f : \\mathcal{S} \\to \\mathbb{R}$. However, optimizing a black-box which is also a function of time (i.e., a dynamic function) $f : \\mathcal{S} \\times \\mathcal{T} \\to \\mathbb{R}$ remains a challenge, since a dynamic Bayesian Optimization (DBO) algorithm has to keep track of the optimum over time.","This changes the nature of the optimization problem in at least three aspects: (i) querying an arbitrary point in $\\mathcal{S} \\times \\mathcal{T}$ is impossible, (ii) past observations become less and less relevant for keeping track of the optimum as time goes by and (iii) the DBO algorithm must have a high sampling frequency so it can collect enough relevant observations to keep track of the optimum through time.","In this paper, we design a Wasserstein distance-based criterion able to quantify the relevancy of an observation with respect to future predictions.","Then, we leverage this criterion to build W-DBO, a DBO algorithm able to remove irrelevant observations from its dataset on the fly, thus maintaining simultaneously a good predictive performance and a high sampling frequency, even in continuous-time optimization tasks with unknown horizon.","Numerical experiments establish the superiority of W-DBO, which outperforms state-of-the-art methods by a comfortable margin."],"url":"http://arxiv.org/abs/2405.14540v1","category":"stat.ML"}
{"created":"2024-05-23 12:26:34","title":"Spin-orbit torque-assisted detection of the canted phase of magnetization in a CoTb-based ferrimagnet","abstract":"The utilization of ferrimagnets in spintronic applications purportedly offers the potential for high-speed and energy-efficient switching of magnetic states, coupled with their notable stability. This collection of characteristics is commonly observed in ferrimagnetic materials that are in close proximity to states of magnetic or angular momentum compensation. This is owing to the presence of two antiparallel ordered magnetic sublattices that exhibit differing responses to variations in temperature or composition. In the vicinity of the magnetic compensation state, an external field has the capacity to disrupt the collinearity between the sublattices, resulting in the codirectionality of the magnetic projections. The existence of the canted phase has been extensively described theoretically, but its experimental investigation remains limited. Here, a violation of antiferromagnetic ordering is detected through a change in the direction of the effective field induced by the spin-orbital torque, without altering the dominant characteristics of the ferrimagnetic structure. This effect is observed both during external heating of the sample and as a consequence of Joule heating with an increase in the transmitted current. In the examined structure of W/Co70Tb30/Ru, the canted phase region is observed at approximately room temperature and at external fields on the order of 0.1 T. Through macrospin modeling and analytical explanations, a correlation between anisotropy, interlattice exchange interaction, and the presence of the canted phase region is established.","sentences":["The utilization of ferrimagnets in spintronic applications purportedly offers the potential for high-speed and energy-efficient switching of magnetic states, coupled with their notable stability.","This collection of characteristics is commonly observed in ferrimagnetic materials that are in close proximity to states of magnetic or angular momentum compensation.","This is owing to the presence of two antiparallel ordered magnetic sublattices that exhibit differing responses to variations in temperature or composition.","In the vicinity of the magnetic compensation state, an external field has the capacity to disrupt the collinearity between the sublattices, resulting in the codirectionality of the magnetic projections.","The existence of the canted phase has been extensively described theoretically, but its experimental investigation remains limited.","Here, a violation of antiferromagnetic ordering is detected through a change in the direction of the effective field induced by the spin-orbital torque, without altering the dominant characteristics of the ferrimagnetic structure.","This effect is observed both during external heating of the sample and as a consequence of Joule heating with an increase in the transmitted current.","In the examined structure of W/Co70Tb30/Ru, the canted phase region is observed at approximately room temperature and at external fields on the order of 0.1 T. Through macrospin modeling and analytical explanations, a correlation between anisotropy, interlattice exchange interaction, and the presence of the canted phase region is established."],"url":"http://arxiv.org/abs/2405.14495v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 12:26:25","title":"Entrywise error bounds for low-rank approximations of kernel matrices","abstract":"In this paper, we derive entrywise error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries. Our error bounds fill this gap. A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory. Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets.","sentences":["In this paper, we derive entrywise error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition).","While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries.","Our error bounds fill this gap.","A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory.","Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2405.14494v1","category":"math.ST"}
{"created":"2024-05-23 12:15:08","title":"On resonance coupling in spiral arms: Patterns for flat rotation curve","abstract":"To address questions about the physical nature and origin of spiral arms in galaxies, it is necessary to measure their dynamical properties, such as the angular speed $\\Omega_p$ or the corotation radius. Observations suggest that galaxies may contain several independent spiral patterns simultaneously. It was shown that so-called non-linear resonance coupling plays an important role in such systems. We aim to identify cases of independent spiral patterns for galaxies with a flat rotation curve, and to investigate what relative pattern velocities $\\Omega^{out}_{p}/\\Omega^{in}_{p}$ they could have in principle for all possible cases of coupling between the main resonances. We solve equations for the main resonance positions (1:1, 2:1, 4:1) and estimate the ratio $\\varpi$ of the corotation radii for two subsequent patterns. For six close galaxies with flat rotation curves, we collect the measurements of the corotation radii in the literature, using at least three different methods in each case for credibility, and measure the $\\varpi$ ratio. We find $\\varpi$ ratios for all possible cases for the main resonances. For three cases we get $\\varpi>3$, meaning that it will be difficult to fit two or even more spiral patterns in the disc. These ratios have been used to derive the wind up time for spirals, estimated to be several galactic rotations. We find that three pairs of coupling cases, including vastly acknowledged in galaxies $OLR_{in}=CR_{out} \\& CR_{in}=IUHR_{out}$, have very close $\\varpi$ ratios, hence should be found simultaneously, as observed. We find strongly confirmed apparent resonance coupling for six galaxies, and show that the observed $\\varpi$ is in agreement with theory. In two of them we identify a previously unreported form of simultaneous coupling, namely $OLR_{in}=OUHR_{out} \\& OUHR_{in}=CR_{out}$, which was also predicted from the proximity of $\\varpi$.","sentences":["To address questions about the physical nature and origin of spiral arms in galaxies, it is necessary to measure their dynamical properties, such as the angular speed $\\Omega_p$ or the corotation radius.","Observations suggest that galaxies may contain several independent spiral patterns simultaneously.","It was shown that so-called non-linear resonance coupling plays an important role in such systems.","We aim to identify cases of independent spiral patterns for galaxies with a flat rotation curve, and to investigate what relative pattern velocities $\\Omega^{out}_{p}/\\Omega^{in}_{p}$ they could have in principle for all possible cases of coupling between the main resonances.","We solve equations for the main resonance positions (1:1, 2:1, 4:1) and estimate the ratio $\\varpi$ of the corotation radii for two subsequent patterns.","For six close galaxies with flat rotation curves, we collect the measurements of the corotation radii in the literature, using at least three different methods in each case for credibility, and measure the $\\varpi$ ratio.","We find $\\varpi$ ratios for all possible cases for the main resonances.","For three cases we get $\\varpi>3$, meaning that it will be difficult to fit two or even more spiral patterns in the disc.","These ratios have been used to derive the wind up time for spirals, estimated to be several galactic rotations.","We find that three pairs of coupling cases, including vastly acknowledged in galaxies $OLR_{in}=CR_{out} \\& CR_{in}=IUHR_{out}$, have very close $\\varpi$ ratios, hence should be found simultaneously, as observed.","We find strongly confirmed apparent resonance coupling for six galaxies, and show that the observed $\\varpi$ is in agreement with theory.","In two of them we identify a previously unreported form of simultaneous coupling, namely $OLR_{in}=OUHR_{out} \\& OUHR_{in}=CR_{out}$, which was also predicted from the proximity of $\\varpi$."],"url":"http://arxiv.org/abs/2405.14483v1","category":"astro-ph.GA"}
{"created":"2024-05-23 11:57:02","title":"Adaptive sampling with PIXL on the Mars Perseverance rover","abstract":"Planetary rovers can use onboard data analysis to adapt their measurement plan on the fly, improving the science value of data collected between commands from Earth. This paper describes the implementation of an adaptive sampling algorithm used by PIXL, the X-ray fluorescence spectrometer of the Mars 2020 Perseverance rover. PIXL is deployed using the rover arm to measure X-ray spectra of rocks with a scan density of several thousand points over an area of typically 5 x 7 mm. The adaptive sampling algorithm is programmed to recognize points of interest and to increase the signal-to-noise ratio at those locations by performing longer integrations. Two approaches are used to formulate the sampling rules based on past quantification data: 1) Expressions that isolate particular regions within a ternary compositional diagram, and 2) Machine learning rules that threshold for a high weight percent of particular compounds. The design of the rulesets are outlined and the performance of the algorithm is quantified using measurements from the surface of Mars. To our knowledge, PIXL's adaptive sampling represents the first autonomous decision-making based on real-time compositional analysis by a spacecraft on the surface of another planet.","sentences":["Planetary rovers can use onboard data analysis to adapt their measurement plan on the fly, improving the science value of data collected between commands from Earth.","This paper describes the implementation of an adaptive sampling algorithm used by PIXL, the X-ray fluorescence spectrometer of the Mars 2020 Perseverance rover.","PIXL is deployed using the rover arm to measure X-ray spectra of rocks with a scan density of several thousand points over an area of typically 5 x 7 mm.","The adaptive sampling algorithm is programmed to recognize points of interest and to increase the signal-to-noise ratio at those locations by performing longer integrations.","Two approaches are used to formulate the sampling rules based on past quantification data: 1) Expressions that isolate particular regions within a ternary compositional diagram, and","2) Machine learning rules that threshold for a high weight percent of particular compounds.","The design of the rulesets are outlined and the performance of the algorithm is quantified using measurements from the surface of Mars.","To our knowledge, PIXL's adaptive sampling represents the first autonomous decision-making based on real-time compositional analysis by a spacecraft on the surface of another planet."],"url":"http://arxiv.org/abs/2405.14471v1","category":"astro-ph.EP"}
{"created":"2024-05-23 11:25:19","title":"Worldwide Federated Training of Language Models","abstract":"The reliance of language model training on massive amounts of computation and vast datasets scraped from potentially low-quality, copyrighted, or sensitive data has come into question practically, legally, and ethically. Federated learning provides a plausible alternative by enabling previously untapped data to be voluntarily gathered from collaborating organizations. However, when scaled globally, federated learning requires collaboration across heterogeneous legal, security, and privacy regimes while accounting for the inherent locality of language data; this further exacerbates the established challenge of federated statistical heterogeneity. We propose a Worldwide Federated Language Model Training~(WorldLM) system based on federations of federations, where each federation has the autonomy to account for factors such as its industry, operating jurisdiction, or competitive environment. WorldLM enables such autonomy in the presence of statistical heterogeneity via partial model localization by allowing sub-federations to attentively aggregate key layers from their constituents. Furthermore, it can adaptively share information across federations via residual layer embeddings. Evaluations of language modeling on naturally heterogeneous datasets show that WorldLM outperforms standard federations by up to $1.91\\times$, approaches the personalized performance of fully local models, and maintains these advantages under privacy-enhancing techniques.","sentences":["The reliance of language model training on massive amounts of computation and vast datasets scraped from potentially low-quality, copyrighted, or sensitive data has come into question practically, legally, and ethically.","Federated learning provides a plausible alternative by enabling previously untapped data to be voluntarily gathered from collaborating organizations.","However, when scaled globally, federated learning requires collaboration across heterogeneous legal, security, and privacy regimes while accounting for the inherent locality of language data; this further exacerbates the established challenge of federated statistical heterogeneity.","We propose a Worldwide Federated Language Model Training~(WorldLM) system based on federations of federations, where each federation has the autonomy to account for factors such as its industry, operating jurisdiction, or competitive environment.","WorldLM enables such autonomy in the presence of statistical heterogeneity via partial model localization by allowing sub-federations to attentively aggregate key layers from their constituents.","Furthermore, it can adaptively share information across federations via residual layer embeddings.","Evaluations of language modeling on naturally heterogeneous datasets show that WorldLM outperforms standard federations by up to $1.91\\times$, approaches the personalized performance of fully local models, and maintains these advantages under privacy-enhancing techniques."],"url":"http://arxiv.org/abs/2405.14446v1","category":"cs.LG"}
{"created":"2024-05-23 11:24:23","title":"Exploring the use of a Large Language Model for data extraction in systematic reviews: a rapid feasibility study","abstract":"This paper describes a rapid feasibility study of using GPT-4, a large language model (LLM), to (semi)automate data extraction in systematic reviews. Despite the recent surge of interest in LLMs there is still a lack of understanding of how to design LLM-based automation tools and how to robustly evaluate their performance. During the 2023 Evidence Synthesis Hackathon we conducted two feasibility studies. Firstly, to automatically extract study characteristics from human clinical, animal, and social science domain studies. We used two studies from each category for prompt-development; and ten for evaluation. Secondly, we used the LLM to predict Participants, Interventions, Controls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP dataset. Overall, results indicated an accuracy of around 80%, with some variability between domains (82% for human clinical, 80% for animal, and 72% for studies of human social sciences). Causal inference methods and study design were the data extraction items with the most errors. In the PICO study, participants and intervention/control showed high accuracy (>80%), outcomes were more challenging. Evaluation was done manually; scoring methods such as BLEU and ROUGE showed limited value. We observed variability in the LLMs predictions and changes in response quality. This paper presents a template for future evaluations of LLMs in the context of data extraction for systematic review automation. Our results show that there might be value in using LLMs, for example as second or third reviewers. However, caution is advised when integrating models such as GPT-4 into tools. Further research on stability and reliability in practical settings is warranted for each type of data that is processed by the LLM.","sentences":["This paper describes a rapid feasibility study of using GPT-4, a large language model (LLM), to (semi)automate data extraction in systematic reviews.","Despite the recent surge of interest in LLMs there is still a lack of understanding of how to design LLM-based automation tools and how to robustly evaluate their performance.","During the 2023 Evidence Synthesis Hackathon we conducted two feasibility studies.","Firstly, to automatically extract study characteristics from human clinical, animal, and social science domain studies.","We used two studies from each category for prompt-development; and ten for evaluation.","Secondly, we used the LLM to predict Participants, Interventions, Controls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP dataset.","Overall, results indicated an accuracy of around 80%, with some variability between domains (82% for human clinical, 80% for animal, and 72% for studies of human social sciences).","Causal inference methods and study design were the data extraction items with the most errors.","In the PICO study, participants and intervention/control showed high accuracy (>80%), outcomes were more challenging.","Evaluation was done manually; scoring methods such as BLEU and ROUGE showed limited value.","We observed variability in the LLMs predictions and changes in response quality.","This paper presents a template for future evaluations of LLMs in the context of data extraction for systematic review automation.","Our results show that there might be value in using LLMs, for example as second or third reviewers.","However, caution is advised when integrating models such as GPT-4 into tools.","Further research on stability and reliability in practical settings is warranted for each type of data that is processed by the LLM."],"url":"http://arxiv.org/abs/2405.14445v1","category":"cs.CL"}
{"created":"2024-05-23 11:05:42","title":"LARS-VSA: A Vector Symbolic Architecture For Learning with Abstract Rules","abstract":"Human cognition excels at symbolic reasoning, deducing abstract rules from limited samples. This has been explained using symbolic and connectionist approaches, inspiring the development of a neuro-symbolic architecture that combines both paradigms. In parallel, recent studies have proposed the use of a \"relational bottleneck\" that separates object-level features from abstract rules, allowing learning from limited amounts of data . While powerful, it is vulnerable to the curse of compositionality meaning that object representations with similar features tend to interfere with each other. In this paper, we leverage hyperdimensional computing, which is inherently robust to such interference to build a compositional architecture. We adapt the \"relational bottleneck\" strategy to a high-dimensional space, incorporating explicit vector binding operations between symbols and relational representations. Additionally, we design a novel high-dimensional attention mechanism that leverages this relational representation. Our system benefits from the low overhead of operations in hyperdimensional space, making it significantly more efficient than the state of the art when evaluated on a variety of test datasets, while maintaining higher or equal accuracy.","sentences":["Human cognition excels at symbolic reasoning, deducing abstract rules from limited samples.","This has been explained using symbolic and connectionist approaches, inspiring the development of a neuro-symbolic architecture that combines both paradigms.","In parallel, recent studies have proposed the use of a \"relational bottleneck\" that separates object-level features from abstract rules, allowing learning from limited amounts of data .","While powerful, it is vulnerable to the curse of compositionality meaning that object representations with similar features tend to interfere with each other.","In this paper, we leverage hyperdimensional computing, which is inherently robust to such interference to build a compositional architecture.","We adapt the \"relational bottleneck\" strategy to a high-dimensional space, incorporating explicit vector binding operations between symbols and relational representations.","Additionally, we design a novel high-dimensional attention mechanism that leverages this relational representation.","Our system benefits from the low overhead of operations in hyperdimensional space, making it significantly more efficient than the state of the art when evaluated on a variety of test datasets, while maintaining higher or equal accuracy."],"url":"http://arxiv.org/abs/2405.14436v1","category":"cs.AI"}
{"created":"2024-05-23 11:00:19","title":"RaFe: Ranking Feedback Improves Query Rewriting for RAG","abstract":"As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose ours, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, ours~provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that ours~can obtain better performance than baselines.","sentences":["As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA.","Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting.","However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting.","In this paper, we propose ours, a framework for training query rewriting models free of annotations.","By leveraging a publicly available reranker, ours~provides feedback aligned well with the rewriting objectives.","Experimental results demonstrate that ours~can obtain better performance than baselines."],"url":"http://arxiv.org/abs/2405.14431v1","category":"cs.CL"}
{"created":"2024-05-23 11:00:07","title":"PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models","abstract":"This paper introduces PipeFusion, a novel approach that harnesses multi-GPU parallelism to address the high computational and latency challenges of generating high-resolution images with diffusion transformers (DiT) models. PipeFusion splits images into patches and distributes the network layers across multiple devices. It employs a pipeline parallel manner to orchestrate communication and computations. By leveraging the high similarity between the input from adjacent diffusion steps, PipeFusion eliminates the waiting time in the pipeline by reusing the one-step stale feature maps to provide context for the current step. Our experiments demonstrate that it can generate higher image resolution where existing DiT parallel approaches meet OOM. PipeFusion significantly reduces the required communication bandwidth, enabling DiT inference to be hosted on GPUs connected via PCIe rather than the more costly NVLink infrastructure, which substantially lowers the overall operational expenses for serving DiT models. Our code is publicly available at https://github.com/PipeFusion/PipeFusion.","sentences":["This paper introduces PipeFusion, a novel approach that harnesses multi-GPU parallelism to address the high computational and latency challenges of generating high-resolution images with diffusion transformers (DiT) models.","PipeFusion splits images into patches and distributes the network layers across multiple devices.","It employs a pipeline parallel manner to orchestrate communication and computations.","By leveraging the high similarity between the input from adjacent diffusion steps, PipeFusion eliminates the waiting time in the pipeline by reusing the one-step stale feature maps to provide context for the current step.","Our experiments demonstrate that it can generate higher image resolution where existing DiT parallel approaches meet OOM.","PipeFusion significantly reduces the required communication bandwidth, enabling DiT inference to be hosted on GPUs connected via PCIe rather than the more costly NVLink infrastructure, which substantially lowers the overall operational expenses for serving DiT models.","Our code is publicly available at https://github.com/PipeFusion/PipeFusion."],"url":"http://arxiv.org/abs/2405.14430v1","category":"cs.CV"}
{"created":"2024-05-23 10:51:25","title":"A hybrid systems framework for data-based adaptive control of linear time-varying systems","abstract":"We consider the data-driven stabilization of discrete-time linear time-varying systems. The controller is defined as a linear state-feedback law whose gain is adapted to the plant changes through a data-based event-triggering rule. To do so, we monitor the evolution of a data-based Lyapunov function along the solution. When this Lyapunov function does not satisfy a designed desirable condition, an episode is triggered to update the controller gain and the corresponding Lyapunov function using the last collected data. The resulting closed-loop dynamics hence exhibits both physical jumps, due to the system dynamics, and episodic jumps, which naturally leads to a hybrid discrete-time system. We leverage the inherent robustness of the controller and provide general conditions under which various stability notions can be established for the system. Two notable cases where these conditions are satisfied are treated, and numerical results illustrating the relevance of the approach are discussed.","sentences":["We consider the data-driven stabilization of discrete-time linear time-varying systems.","The controller is defined as a linear state-feedback law whose gain is adapted to the plant changes through a data-based event-triggering rule.","To do so, we monitor the evolution of a data-based Lyapunov function along the solution.","When this Lyapunov function does not satisfy a designed desirable condition, an episode is triggered to update the controller gain and the corresponding Lyapunov function using the last collected data.","The resulting closed-loop dynamics hence exhibits both physical jumps, due to the system dynamics, and episodic jumps, which naturally leads to a hybrid discrete-time system.","We leverage the inherent robustness of the controller and provide general conditions under which various stability notions can be established for the system.","Two notable cases where these conditions are satisfied are treated, and numerical results illustrating the relevance of the approach are discussed."],"url":"http://arxiv.org/abs/2405.14426v1","category":"eess.SY"}
{"created":"2024-05-23 10:48:11","title":"The interplay between liquid-liquid and ferroelectric phase transitions in supercooled water","abstract":"The distinctive characteristics of water, evident in its thermodynamic anomalies, have implications across disciplines from biology to geophysics. Considered a valid hypothesis to rationalize its unique properties, a liquid-liquid phase transition in water's supercooled regime has nowadays been observed in several molecular dynamics simulations and is being actively researched experimentally. Here, we highlight novel and far-reaching implications of water: the interplay between the liquid-liquid and a ferroelectric phase transition. Our results are based on the analysis of extensive molecular dynamics simulations and are explained in the context of a classical density functional theory in mean-field approximation valid for a polar liquid. The theory underpins the potential role of ferroelectricity in promoting the liquid-liquid phase transition. The existence of ferroelectric order in supercooled low-density liquid water is confirmed by the observation in molecular dynamics simulations of collective modes in polarization fluctuations dynamics, traceable to spontaneous breaking of continuous rotational symmetry. Our work opens the door to new experimental investigations of the static and dynamic behavior of water polarization.","sentences":["The distinctive characteristics of water, evident in its thermodynamic anomalies, have implications across disciplines from biology to geophysics.","Considered a valid hypothesis to rationalize its unique properties, a liquid-liquid phase transition in water's supercooled regime has nowadays been observed in several molecular dynamics simulations and is being actively researched experimentally.","Here, we highlight novel and far-reaching implications of water: the interplay between the liquid-liquid and a ferroelectric phase transition.","Our results are based on the analysis of extensive molecular dynamics simulations and are explained in the context of a classical density functional theory in mean-field approximation valid for a polar liquid.","The theory underpins the potential role of ferroelectricity in promoting the liquid-liquid phase transition.","The existence of ferroelectric order in supercooled low-density liquid water is confirmed by the observation in molecular dynamics simulations of collective modes in polarization fluctuations dynamics, traceable to spontaneous breaking of continuous rotational symmetry.","Our work opens the door to new experimental investigations of the static and dynamic behavior of water polarization."],"url":"http://arxiv.org/abs/2405.14424v1","category":"cond-mat.soft"}
{"created":"2024-05-23 10:43:20","title":"Unraveling overoptimism and publication bias in ML-driven science","abstract":"Machine Learning (ML) is increasingly used across many disciplines with impressive reported results across many domain areas. However, recent studies suggest that the published performance of ML models are often overoptimistic and not reflective of true accuracy were these models to be deployed. Validity concerns are underscored by findings of a concerning inverse relationship between sample size and reported accuracy in published ML models across several domains. This is in contrast with the theory of learning curves in ML, where we expect accuracy to improve or stay the same with increasing sample size. This paper investigates the factors contributing to overoptimistic accuracy reports in ML-based science, focusing on data leakage and publication bias. Our study introduces a novel stochastic model for observed accuracy, integrating parametric learning curves and the above biases. We then construct an estimator based on this model that corrects for these biases in observed data. Theoretical and empirical results demonstrate that this framework can estimate the underlying learning curve that gives rise to the observed overoptimistic results, thereby providing more realistic performance assessments of ML performance from a collection of published results. We apply the model to various meta-analyses in the digital health literature, including neuroimaging-based and speech-based classifications of several neurological conditions. Our results indicate prevalent overoptimism across these fields and we estimate the inherent limits of ML-based prediction in each domain.","sentences":["Machine Learning (ML) is increasingly used across many disciplines with impressive reported results across many domain areas.","However, recent studies suggest that the published performance of ML models are often overoptimistic and not reflective of true accuracy were these models to be deployed.","Validity concerns are underscored by findings of a concerning inverse relationship between sample size and reported accuracy in published ML models across several domains.","This is in contrast with the theory of learning curves in ML, where we expect accuracy to improve or stay the same with increasing sample size.","This paper investigates the factors contributing to overoptimistic accuracy reports in ML-based science, focusing on data leakage and publication bias.","Our study introduces a novel stochastic model for observed accuracy, integrating parametric learning curves and the above biases.","We then construct an estimator based on this model that corrects for these biases in observed data.","Theoretical and empirical results demonstrate that this framework can estimate the underlying learning curve that gives rise to the observed overoptimistic results, thereby providing more realistic performance assessments of ML performance from a collection of published results.","We apply the model to various meta-analyses in the digital health literature, including neuroimaging-based and speech-based classifications of several neurological conditions.","Our results indicate prevalent overoptimism across these fields and we estimate the inherent limits of ML-based prediction in each domain."],"url":"http://arxiv.org/abs/2405.14422v1","category":"cs.LG"}
{"created":"2024-05-23 10:39:33","title":"Motion-based video compression for resource-constrained camera traps","abstract":"Field-captured video allows for detailed studies of spatiotemporal aspects of animal locomotion, decision-making, and environmental interactions. However, despite the affordability of data capture with mass-produced hardware, storage, processing, and transmission overheads pose a significant hurdle to acquiring high-resolution video from field-deployed camera traps. Therefore, efficient compression algorithms are crucial for monitoring with camera traps that have limited access to power, storage, and bandwidth. In this article, we introduce a new motion analysis-based video compression algorithm designed to run on camera trap devices. We implemented and tested this algorithm using a case study of insect-pollinator motion tracking. The algorithm identifies and stores only image regions depicting motion relevant to pollination monitoring, reducing the overall data size by an average of 84% across a diverse set of test datasets while retaining the information necessary for relevant behavioural analysis. The methods outlined in this paper facilitate the broader application of computer vision-enabled, low-powered camera trap devices for remote, in-situ video-based animal motion monitoring.","sentences":["Field-captured video allows for detailed studies of spatiotemporal aspects of animal locomotion, decision-making, and environmental interactions.","However, despite the affordability of data capture with mass-produced hardware, storage, processing, and transmission overheads pose a significant hurdle to acquiring high-resolution video from field-deployed camera traps.","Therefore, efficient compression algorithms are crucial for monitoring with camera traps that have limited access to power, storage, and bandwidth.","In this article, we introduce a new motion analysis-based video compression algorithm designed to run on camera trap devices.","We implemented and tested this algorithm using a case study of insect-pollinator motion tracking.","The algorithm identifies and stores only image regions depicting motion relevant to pollination monitoring, reducing the overall data size by an average of 84% across a diverse set of test datasets while retaining the information necessary for relevant behavioural analysis.","The methods outlined in this paper facilitate the broader application of computer vision-enabled, low-powered camera trap devices for remote, in-situ video-based animal motion monitoring."],"url":"http://arxiv.org/abs/2405.14419v1","category":"cs.CV"}
{"created":"2024-05-23 10:35:08","title":"Proving Theorems Recursively","abstract":"Recent advances in automated theorem proving leverages language models to explore expanded search spaces by step-by-step proof generation. However, such approaches are usually based on short-sighted heuristics (e.g., log probability or value function scores) that potentially lead to suboptimal or even distracting subgoals, preventing us from finding longer proofs. To address this challenge, we propose POETRY (PrOvE Theorems RecursivelY), which proves theorems in a recursive, level-by-level manner in the Isabelle theorem prover. Unlike previous step-by-step methods, POETRY searches for a verifiable sketch of the proof at each level and focuses on solving the current level's theorem or conjecture. Detailed proofs of intermediate conjectures within the sketch are temporarily replaced by a placeholder tactic called sorry, deferring their proofs to subsequent levels. This approach allows the theorem to be tackled incrementally by outlining the overall theorem at the first level and then solving the intermediate conjectures at deeper levels. Experiments are conducted on the miniF2F and PISA datasets and significant performance gains are observed in our POETRY approach over state-of-the-art methods. POETRY on miniF2F achieves an average proving success rate improvement of 5.1%. Moreover, we observe a substantial increase in the maximum proof length found by POETRY, from 10 to 26.","sentences":["Recent advances in automated theorem proving leverages language models to explore expanded search spaces by step-by-step proof generation.","However, such approaches are usually based on short-sighted heuristics (e.g., log probability or value function scores) that potentially lead to suboptimal or even distracting subgoals, preventing us from finding longer proofs.","To address this challenge, we propose POETRY (PrOvE Theorems RecursivelY), which proves theorems in a recursive, level-by-level manner in the Isabelle theorem prover.","Unlike previous step-by-step methods, POETRY searches for a verifiable sketch of the proof at each level and focuses on solving the current level's theorem or conjecture.","Detailed proofs of intermediate conjectures within the sketch are temporarily replaced by a placeholder tactic called sorry, deferring their proofs to subsequent levels.","This approach allows the theorem to be tackled incrementally by outlining the overall theorem at the first level and then solving the intermediate conjectures at deeper levels.","Experiments are conducted on the miniF2F and PISA datasets and significant performance gains are observed in our POETRY approach over state-of-the-art methods.","POETRY on miniF2F achieves an average proving success rate improvement of 5.1%.","Moreover, we observe a substantial increase in the maximum proof length found by POETRY, from 10 to 26."],"url":"http://arxiv.org/abs/2405.14414v1","category":"cs.AI"}
{"created":"2024-05-23 10:32:38","title":"Large Language Models for Explainable Decisions in Dynamic Digital Twins","abstract":"Dynamic data-driven Digital Twins (DDTs) can enable informed decision-making and provide an optimisation platform for the underlying system. By leveraging principles of Dynamic Data-Driven Applications Systems (DDDAS), DDTs can formulate computational modalities for feedback loops, model updates and decision-making, including autonomous ones. However, understanding autonomous decision-making often requires technical and domain-specific knowledge. This paper explores using large language models (LLMs) to provide an explainability platform for DDTs, generating natural language explanations of the system's decision-making by leveraging domain-specific knowledge bases. A case study from smart agriculture is presented.","sentences":["Dynamic data-driven Digital Twins (DDTs) can enable informed decision-making and provide an optimisation platform for the underlying system.","By leveraging principles of Dynamic Data-Driven Applications Systems (DDDAS), DDTs can formulate computational modalities for feedback loops, model updates and decision-making, including autonomous ones.","However, understanding autonomous decision-making often requires technical and domain-specific knowledge.","This paper explores using large language models (LLMs) to provide an explainability platform for DDTs, generating natural language explanations of the system's decision-making by leveraging domain-specific knowledge bases.","A case study from smart agriculture is presented."],"url":"http://arxiv.org/abs/2405.14411v1","category":"cs.AI"}
{"created":"2024-05-23 10:19:10","title":"Endowing Interpretability for Neural Cognitive Diagnosis by Efficient Kolmogorov-Arnold Networks","abstract":"In the realm of intelligent education, cognitive diagnosis plays a crucial role in subsequent recommendation tasks attributed to the revealed students' proficiency in knowledge concepts. Although neural network-based neural cognitive diagnosis models (CDMs) have exhibited significantly better performance than traditional models, neural cognitive diagnosis is criticized for the poor model interpretability due to the multi-layer perception (MLP) employed, even with the monotonicity assumption. Therefore, this paper proposes to empower the interpretability of neural cognitive diagnosis models through efficient kolmogorov-arnold networks (KANs), named KAN2CD, where KANs are designed to enhance interpretability in two manners. Specifically, in the first manner, KANs are directly used to replace the used MLPs in existing neural CDMs; while in the second manner, the student embedding, exercise embedding, and concept embedding are directly processed by several KANs, and then their outputs are further combined and learned in a unified KAN to get final predictions. To overcome the problem of training KANs slowly, we modify the implementation of original KANs to accelerate the training. Experiments on four real-world datasets show that the proposed KA2NCD exhibits better performance than traditional CDMs, and the proposed KA2NCD still has a bit of performance leading even over the existing neural CDMs. More importantly, the learned structures of KANs enable the proposed KA2NCD to hold as good interpretability as traditional CDMs, which is superior to existing neural CDMs. Besides, the training cost of the proposed KA2NCD is competitive to existing models.","sentences":["In the realm of intelligent education, cognitive diagnosis plays a crucial role in subsequent recommendation tasks attributed to the revealed students' proficiency in knowledge concepts.","Although neural network-based neural cognitive diagnosis models (CDMs) have exhibited significantly better performance than traditional models, neural cognitive diagnosis is criticized for the poor model interpretability due to the multi-layer perception (MLP) employed, even with the monotonicity assumption.","Therefore, this paper proposes to empower the interpretability of neural cognitive diagnosis models through efficient kolmogorov-arnold networks (KANs), named KAN2CD, where KANs are designed to enhance interpretability in two manners.","Specifically, in the first manner, KANs are directly used to replace the used MLPs in existing neural CDMs; while in the second manner, the student embedding, exercise embedding, and concept embedding are directly processed by several KANs, and then their outputs are further combined and learned in a unified KAN to get final predictions.","To overcome the problem of training KANs slowly, we modify the implementation of original KANs to accelerate the training.","Experiments on four real-world datasets show that the proposed KA2NCD exhibits better performance than traditional CDMs, and the proposed KA2NCD still has a bit of performance leading even over the existing neural CDMs.","More importantly, the learned structures of KANs enable the proposed KA2NCD to hold as good interpretability as traditional CDMs, which is superior to existing neural CDMs.","Besides, the training cost of the proposed KA2NCD is competitive to existing models."],"url":"http://arxiv.org/abs/2405.14399v1","category":"cs.LG"}
{"created":"2024-05-23 10:15:29","title":"SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network","abstract":"Surface electromyography (sEMG) based gesture recognition offers a natural and intuitive interaction modality for wearable devices. Despite significant advancements in sEMG-based gesture-recognition models, existing methods often suffer from high computational latency and increased energy consumption. Additionally, the inherent instability of sEMG signals, combined with their sensitivity to distribution shifts in real-world settings, compromises model robustness.   To tackle these challenges, we propose a novel SpGesture framework based on Spiking Neural Networks, which possesses several unique merits compared with existing methods: (1) Robustness: By utilizing membrane potential as a memory list, we pioneer the introduction of Source-Free Domain Adaptation into SNN for the first time. This enables SpGesture to mitigate the accuracy degradation caused by distribution shifts. (2) High Accuracy: With a novel Spiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent sEMG features, leading to a notable rise in system accuracy. To validate SpGesture's performance, we collected a new sEMG gesture dataset which has different forearm postures, where SpGesture achieved the highest accuracy among the baselines ($89.26\\%$). Moreover, the actual deployment on the CPU demonstrated a system latency below 100ms, well within real-time requirements. This impressive performance showcases SpGesture's potential to enhance the applicability of sEMG in real-world scenarios. The code is available at https://anonymous.4open.science/r/SpGesture.","sentences":["Surface electromyography (sEMG) based gesture recognition offers a natural and intuitive interaction modality for wearable devices.","Despite significant advancements in sEMG-based gesture-recognition models, existing methods often suffer from high computational latency and increased energy consumption.","Additionally, the inherent instability of sEMG signals, combined with their sensitivity to distribution shifts in real-world settings, compromises model robustness.   ","To tackle these challenges, we propose a novel SpGesture framework based on Spiking Neural Networks, which possesses several unique merits compared with existing methods: (1) Robustness: By utilizing membrane potential as a memory list, we pioneer the introduction of Source-Free Domain Adaptation into SNN for the first time.","This enables SpGesture to mitigate the accuracy degradation caused by distribution shifts.","(2) High Accuracy: With a novel Spiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent sEMG features, leading to a notable rise in system accuracy.","To validate SpGesture's performance, we collected a new sEMG gesture dataset which has different forearm postures, where SpGesture achieved the highest accuracy among the baselines ($89.26\\%$).","Moreover, the actual deployment on the CPU demonstrated a system latency below 100ms, well within real-time requirements.","This impressive performance showcases SpGesture's potential to enhance the applicability of sEMG in real-world scenarios.","The code is available at https://anonymous.4open.science/r/SpGesture."],"url":"http://arxiv.org/abs/2405.14398v1","category":"cs.HC"}
{"created":"2024-05-23 10:12:03","title":"Instruction Tuning With Loss Over Instructions","abstract":"Instruction tuning plays a crucial role in shaping the outputs of language models (LMs) to desired styles. In this work, we propose a simple yet effective method, Instruction Modelling (IM), which trains LMs by applying a loss function to the instruction and prompt part rather than solely to the output part. Through experiments across 21 diverse benchmarks, we show that, in many scenarios, IM can effectively improve the LM performance on both NLP tasks (e.g., MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks (e.g., MT-Bench and AlpacaEval). Remarkably, in the most advantageous case, IM boosts model performance on AlpacaEval 1.0 by over 100%. We identify two key factors influencing the effectiveness of IM: (1) The ratio between instruction length and output length in the training data; and (2) The number of training examples. We observe that IM is especially beneficial when trained on datasets with lengthy instructions paired with brief outputs, or under the Superficial Alignment Hypothesis (SAH) where a small amount of training examples are used for instruction tuning. Further analysis substantiates our hypothesis that the improvement can be attributed to reduced overfitting to instruction tuning datasets. Our work provides practical guidance for instruction tuning LMs, especially in low-resource scenarios.","sentences":["Instruction tuning plays a crucial role in shaping the outputs of language models (LMs) to desired styles.","In this work, we propose a simple yet effective method, Instruction Modelling (IM), which trains LMs by applying a loss function to the instruction and prompt part rather than solely to the output part.","Through experiments across 21 diverse benchmarks, we show that, in many scenarios, IM can effectively improve the LM performance on both NLP tasks (e.g., MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks (e.g., MT-Bench and AlpacaEval).","Remarkably, in the most advantageous case, IM boosts model performance on AlpacaEval 1.0 by over 100%.","We identify two key factors influencing the effectiveness of IM: (1) The ratio between instruction length and output length in the training data; and (2) The number of training examples.","We observe that IM is especially beneficial when trained on datasets with lengthy instructions paired with brief outputs, or under the Superficial Alignment Hypothesis (SAH) where a small amount of training examples are used for instruction tuning.","Further analysis substantiates our hypothesis that the improvement can be attributed to reduced overfitting to instruction tuning datasets.","Our work provides practical guidance for instruction tuning LMs, especially in low-resource scenarios."],"url":"http://arxiv.org/abs/2405.14394v1","category":"cs.CL"}
{"created":"2024-05-23 10:07:21","title":"Explainable Few-shot Knowledge Tracing","abstract":"Knowledge tracing (KT), aiming to mine students' mastery of knowledge by their exercise records and predict their performance on future test questions, is a critical task in educational assessment. While researchers achieved tremendous success with the rapid development of deep learning techniques, current knowledge tracing tasks fall into the cracks from real-world teaching scenarios. Relying heavily on extensive student data and solely predicting numerical performances differs from the settings where teachers assess students' knowledge state from limited practices and provide explanatory feedback. To fill this gap, we explore a new task formulation: Explainable Few-shot Knowledge Tracing. By leveraging the powerful reasoning and generation abilities of large language models (LLMs), we then propose a cognition-guided framework that can track the student knowledge from a few student records while providing natural language explanations. Experimental results from three widely used datasets show that LLMs can perform comparable or superior to competitive deep knowledge tracing methods. We also discuss potential directions and call for future improvements in relevant topics.","sentences":["Knowledge tracing (KT), aiming to mine students' mastery of knowledge by their exercise records and predict their performance on future test questions, is a critical task in educational assessment.","While researchers achieved tremendous success with the rapid development of deep learning techniques, current knowledge tracing tasks fall into the cracks from real-world teaching scenarios.","Relying heavily on extensive student data and solely predicting numerical performances differs from the settings where teachers assess students' knowledge state from limited practices and provide explanatory feedback.","To fill this gap, we explore a new task formulation: Explainable Few-shot Knowledge Tracing.","By leveraging the powerful reasoning and generation abilities of large language models (LLMs), we then propose a cognition-guided framework that can track the student knowledge from a few student records while providing natural language explanations.","Experimental results from three widely used datasets show that LLMs can perform comparable or superior to competitive deep knowledge tracing methods.","We also discuss potential directions and call for future improvements in relevant topics."],"url":"http://arxiv.org/abs/2405.14391v1","category":"cs.AI"}
{"created":"2024-05-23 10:04:56","title":"stl2vec: Semantic and Interpretable Vector Representation of Temporal Logic","abstract":"Integrating symbolic knowledge and data-driven learning algorithms is a longstanding challenge in Artificial Intelligence. Despite the recognized importance of this task, a notable gap exists due to the discreteness of symbolic representations and the continuous nature of machine-learning computations. One of the desired bridges between these two worlds would be to define semantically grounded vector representation (feature embedding) of logic formulae, thus enabling to perform continuous learning and optimization in the semantic space of formulae. We tackle this goal for knowledge expressed in Signal Temporal Logic (STL) and devise a method to compute continuous embeddings of formulae with several desirable properties: the embedding (i) is finite-dimensional, (ii) faithfully reflects the semantics of the formulae, (iii) does not require any learning but instead is defined from basic principles, (iv) is interpretable. Another significant contribution lies in demonstrating the efficacy of the approach in two tasks: learning model checking, where we predict the probability of requirements being satisfied in stochastic processes; and integrating the embeddings into a neuro-symbolic framework, to constrain the output of a deep-learning generative model to comply to a given logical specification.","sentences":["Integrating symbolic knowledge and data-driven learning algorithms is a longstanding challenge in Artificial Intelligence.","Despite the recognized importance of this task, a notable gap exists due to the discreteness of symbolic representations and the continuous nature of machine-learning computations.","One of the desired bridges between these two worlds would be to define semantically grounded vector representation (feature embedding) of logic formulae, thus enabling to perform continuous learning and optimization in the semantic space of formulae.","We tackle this goal for knowledge expressed in Signal Temporal Logic (STL) and devise a method to compute continuous embeddings of formulae with several desirable properties: the embedding (i) is finite-dimensional, (ii) faithfully reflects the semantics of the formulae, (iii) does not require any learning but instead is defined from basic principles, (iv) is interpretable.","Another significant contribution lies in demonstrating the efficacy of the approach in two tasks: learning model checking, where we predict the probability of requirements being satisfied in stochastic processes; and integrating the embeddings into a neuro-symbolic framework, to constrain the output of a deep-learning generative model to comply to a given logical specification."],"url":"http://arxiv.org/abs/2405.14389v1","category":"cs.AI"}
{"created":"2024-05-23 10:02:13","title":"Emotion Identification for French in Written Texts: Considering their Modes of Expression as a Step Towards Text Complexity Analysis","abstract":"The objective of this paper is to predict (A) whether a sentence in a written text expresses an emotion, (B) the mode(s) in which it is expressed, (C) whether it is basic or complex, and (D) its emotional category.   One of our major contributions, through a dataset and a model, is to integrate the fact that an emotion can be expressed in different modes: from a direct mode, essentially lexicalized, to a more indirect mode, where emotions will only be suggested, a mode that NLP approaches generally don't take into account.   Another originality is that the scope is on written texts, as opposed usual work focusing on conversational (often multi-modal) data. In this context, modes of expression are seen as a factor towards the automatic analysis of complexity in texts.   Experiments on French texts show acceptable results compared to the human annotators' agreement, and outperforming results compared to using a large language model with in-context learning (i.e. no fine-tuning).","sentences":["The objective of this paper is to predict (A) whether a sentence in a written text expresses an emotion, (B) the mode(s) in which it is expressed, (C) whether it is basic or complex, and (D) its emotional category.   ","One of our major contributions, through a dataset and a model, is to integrate the fact that an emotion can be expressed in different modes: from a direct mode, essentially lexicalized, to a more indirect mode, where emotions will only be suggested, a mode that NLP approaches generally don't take into account.   ","Another originality is that the scope is on written texts, as opposed usual work focusing on conversational (often multi-modal) data.","In this context, modes of expression are seen as a factor towards the automatic analysis of complexity in texts.   ","Experiments on French texts show acceptable results compared to the human annotators' agreement, and outperforming results compared to using a large language model with in-context learning (i.e. no fine-tuning)."],"url":"http://arxiv.org/abs/2405.14385v1","category":"cs.CL"}
{"created":"2024-05-23 10:01:39","title":"Reliable Trajectory Prediction and Uncertainty Quantification with Conditioned Diffusion Models","abstract":"This work introduces the conditioned Vehicle Motion Diffusion (cVMD) model, a novel network architecture for highway trajectory prediction using diffusion models. The proposed model ensures the drivability of the predicted trajectory by integrating non-holonomic motion constraints and physical constraints into the generative prediction module. Central to the architecture of cVMD is its capacity to perform uncertainty quantification, a feature that is crucial in safety-critical applications. By integrating the quantified uncertainty into the prediction process, the cVMD's trajectory prediction performance is improved considerably. The model's performance was evaluated using the publicly available highD dataset. Experiments show that the proposed architecture achieves competitive trajectory prediction accuracy compared to state-of-the-art models, while providing guaranteed drivable trajectories and uncertainty quantification.","sentences":["This work introduces the conditioned Vehicle Motion Diffusion (cVMD) model, a novel network architecture for highway trajectory prediction using diffusion models.","The proposed model ensures the drivability of the predicted trajectory by integrating non-holonomic motion constraints and physical constraints into the generative prediction module.","Central to the architecture of cVMD is its capacity to perform uncertainty quantification, a feature that is crucial in safety-critical applications.","By integrating the quantified uncertainty into the prediction process, the cVMD's trajectory prediction performance is improved considerably.","The model's performance was evaluated using the publicly available highD dataset.","Experiments show that the proposed architecture achieves competitive trajectory prediction accuracy compared to state-of-the-art models, while providing guaranteed drivable trajectories and uncertainty quantification."],"url":"http://arxiv.org/abs/2405.14384v1","category":"cs.LG"}
{"created":"2024-05-23 10:00:14","title":"Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering","abstract":"Large Language Models (LLMs) are widely used for knowledge-seeking yet suffer from hallucinations. The knowledge boundary (KB) of an LLM limits its factual understanding, beyond which it may begin to hallucinate. Investigating the perception of LLMs' KB is crucial for detecting hallucinations and LLMs' reliable generation. Current studies perceive LLMs' KB on questions with a concrete answer (close-ended questions) while paying limited attention to semi-open-ended questions (SoeQ) that correspond to many potential answers. Some researchers achieve it by judging whether the question is answerable or not. However, this paradigm is unsuitable for SoeQ, which are usually partially answerable, containing both answerable and ambiguous (unanswerable) answers. Ambiguous answers are essential for knowledge-seeking, but they may go beyond the KB of LLMs. In this paper, we perceive the LLMs' KB with SoeQ by discovering more ambiguous answers. First, we apply an LLM-based approach to construct SoeQ and obtain answers from a target LLM. Unfortunately, the output probabilities of mainstream black-box LLMs are inaccessible to sample for low-probability ambiguous answers. Therefore, we apply an open-sourced auxiliary model to explore ambiguous answers for the target LLM. We calculate the nearest semantic representation for existing answers to estimate their probabilities, with which we reduce the generation probability of high-probability answers to achieve a more effective generation. Finally, we compare the results from the RAG-based evaluation and LLM self-evaluation to categorize four types of ambiguous answers that are beyond the KB of the target LLM. Following our method, we construct a dataset to perceive the KB for GPT-4. We find that GPT-4 performs poorly on SoeQ and is often unaware of its KB. Besides, our auxiliary model, LLaMA-2-13B, is effective in discovering more ambiguous answers.","sentences":["Large Language Models (LLMs) are widely used for knowledge-seeking yet suffer from hallucinations.","The knowledge boundary (KB) of an LLM limits its factual understanding, beyond which it may begin to hallucinate.","Investigating the perception of LLMs' KB is crucial for detecting hallucinations and LLMs' reliable generation.","Current studies perceive LLMs' KB on questions with a concrete answer (close-ended questions) while paying limited attention to semi-open-ended questions (SoeQ) that correspond to many potential answers.","Some researchers achieve it by judging whether the question is answerable or not.","However, this paradigm is unsuitable for SoeQ, which are usually partially answerable, containing both answerable and ambiguous (unanswerable) answers.","Ambiguous answers are essential for knowledge-seeking, but they may go beyond the KB of LLMs.","In this paper, we perceive the LLMs' KB with SoeQ by discovering more ambiguous answers.","First, we apply an LLM-based approach to construct SoeQ and obtain answers from a target LLM.","Unfortunately, the output probabilities of mainstream black-box LLMs are inaccessible to sample for low-probability ambiguous answers.","Therefore, we apply an open-sourced auxiliary model to explore ambiguous answers for the target LLM.","We calculate the nearest semantic representation for existing answers to estimate their probabilities, with which we reduce the generation probability of high-probability answers to achieve a more effective generation.","Finally, we compare the results from the RAG-based evaluation and LLM self-evaluation to categorize four types of ambiguous answers that are beyond the KB of the target LLM.","Following our method, we construct a dataset to perceive the KB for GPT-4.","We find that GPT-4 performs poorly on SoeQ and is often unaware of its KB.","Besides, our auxiliary model, LLaMA-2-13B, is effective in discovering more ambiguous answers."],"url":"http://arxiv.org/abs/2405.14383v1","category":"cs.CL"}
{"created":"2024-05-23 09:54:54","title":"Can Large Language Models Create New Knowledge for Spatial Reasoning Tasks?","abstract":"The potential for Large Language Models (LLMs) to generate new information offers a potential step change for research and innovation. This is challenging to assert as it can be difficult to determine what an LLM has previously seen during training, making \"newness\" difficult to substantiate. In this paper we observe that LLMs are able to perform sophisticated reasoning on problems with a spatial dimension, that they are unlikely to have previously directly encountered. While not perfect, this points to a significant level of understanding that state-of-the-art LLMs can now achieve, supporting the proposition that LLMs are able to yield significant emergent properties. In particular, Claude 3 is found to perform well in this regard.","sentences":["The potential for Large Language Models (LLMs) to generate new information offers a potential step change for research and innovation.","This is challenging to assert as it can be difficult to determine what an LLM has previously seen during training, making \"newness\" difficult to substantiate.","In this paper we observe that LLMs are able to perform sophisticated reasoning on problems with a spatial dimension, that they are unlikely to have previously directly encountered.","While not perfect, this points to a significant level of understanding that state-of-the-art LLMs can now achieve, supporting the proposition that LLMs are able to yield significant emergent properties.","In particular, Claude 3 is found to perform well in this regard."],"url":"http://arxiv.org/abs/2405.14379v1","category":"cs.CL"}
{"created":"2024-05-23 09:52:15","title":"CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization","abstract":"Training large AI models such as deep learning recommendation systems and foundation language (or multi-modal) models costs massive GPUs and computing time. The high training cost has become only affordable to big tech companies, meanwhile also causing increasing concerns about the environmental impact. This paper presents CoMERA, a Computing- and Memory-Efficient training method via Rank-Adaptive tensor optimization. CoMERA achieves end-to-end rank-adaptive tensor-compressed training via a multi-objective optimization formulation, and improves the training to provide both a high compression ratio and excellent accuracy in the training process. Our optimized numerical computation (e.g., optimized tensorized embedding and tensor-vector contractions) and GPU implementation eliminate part of the run-time overhead in the tensorized training on GPU. This leads to, for the first time, $2-3\\times$ speedup per training epoch compared with standard training. CoMERA also outperforms the recent GaLore in terms of both memory and computing efficiency. Specifically, CoMERA is $2\\times$ faster per training epoch and $9\\times$ more memory-efficient than GaLore on a tested six-encoder transformer with single-batch training. With further HPC optimization, CoMERA may significantly reduce the training cost of large language models.","sentences":["Training large AI models such as deep learning recommendation systems and foundation language (or multi-modal) models costs massive GPUs and computing time.","The high training cost has become only affordable to big tech companies, meanwhile also causing increasing concerns about the environmental impact.","This paper presents CoMERA, a Computing- and Memory-Efficient training method via Rank-Adaptive tensor optimization.","CoMERA achieves end-to-end rank-adaptive tensor-compressed training via a multi-objective optimization formulation, and improves the training to provide both a high compression ratio and excellent accuracy in the training process.","Our optimized numerical computation (e.g., optimized tensorized embedding and tensor-vector contractions) and GPU implementation eliminate part of the run-time overhead in the tensorized training on GPU.","This leads to, for the first time, $2-3\\times$ speedup per training epoch compared with standard training.","CoMERA also outperforms the recent GaLore in terms of both memory and computing efficiency.","Specifically, CoMERA is $2\\times$ faster per training epoch and $9\\times$ more memory-efficient than GaLore on a tested six-encoder transformer with single-batch training.","With further HPC optimization, CoMERA may significantly reduce the training cost of large language models."],"url":"http://arxiv.org/abs/2405.14377v1","category":"cs.LG"}
{"created":"2024-05-23 09:50:04","title":"State-Constrained Offline Reinforcement Learning","abstract":"Traditional offline reinforcement learning methods predominantly operate in a batch-constrained setting. This confines the algorithms to a specific state-action distribution present in the dataset, reducing the effects of distributional shift but restricting the algorithm greatly. In this paper, we alleviate this limitation by introducing a novel framework named \\emph{state-constrained} offline reinforcement learning. By exclusively focusing on the dataset's state distribution, our framework significantly enhances learning potential and reduces previous limitations. The proposed setting not only broadens the learning horizon but also improves the ability to combine different trajectories from the dataset effectively, a desirable property inherent in offline reinforcement learning. Our research is underpinned by solid theoretical findings that pave the way for subsequent advancements in this domain. Additionally, we introduce StaCQ, a deep learning algorithm that is both performance-driven on the D4RL benchmark datasets and closely aligned with our theoretical propositions. StaCQ establishes a strong baseline for forthcoming explorations in state-constrained offline reinforcement learning.","sentences":["Traditional offline reinforcement learning methods predominantly operate in a batch-constrained setting.","This confines the algorithms to a specific state-action distribution present in the dataset, reducing the effects of distributional shift but restricting the algorithm greatly.","In this paper, we alleviate this limitation by introducing a novel framework named \\emph{state-constrained} offline reinforcement learning.","By exclusively focusing on the dataset's state distribution, our framework significantly enhances learning potential and reduces previous limitations.","The proposed setting not only broadens the learning horizon but also improves the ability to combine different trajectories from the dataset effectively, a desirable property inherent in offline reinforcement learning.","Our research is underpinned by solid theoretical findings that pave the way for subsequent advancements in this domain.","Additionally, we introduce StaCQ, a deep learning algorithm that is both performance-driven on the D4RL benchmark datasets and closely aligned with our theoretical propositions.","StaCQ establishes a strong baseline for forthcoming explorations in state-constrained offline reinforcement learning."],"url":"http://arxiv.org/abs/2405.14374v1","category":"stat.ML"}
{"created":"2024-05-23 09:43:52","title":"MiniCache: KV Cache Compression in Depth Dimension for Large Language Models","abstract":"A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02x, enhances inference throughput by approximately 5x, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance.","sentences":["A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching.","The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation.","However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation.","In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference.","Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs.","To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged.","Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead.","Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity.","We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput.","On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02x, enhances inference throughput by approximately 5x, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance."],"url":"http://arxiv.org/abs/2405.14366v1","category":"cs.CL"}
{"created":"2024-05-23 09:43:19","title":"JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models","abstract":"Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}.","sentences":["Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications.","To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems.","Both types of work generally lead to large costs in training or synthesis.","To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data.","To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM.","Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.","Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts.","The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM.","We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data.","Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings.","Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}."],"url":"http://arxiv.org/abs/2405.14365v1","category":"cs.CL"}
{"created":"2024-05-23 09:19:14","title":"Doubly-Dynamic ISAC Precoding for Vehicular Networks: A Constrained Deep Reinforcement Learning (CDRL) Approach","abstract":"Integrated sensing and communication (ISAC) technology is essential for enabling the vehicular networks. However, the communication channel in this scenario exhibits time-varying characteristics, and the potential targets may move rapidly, creating a doubly-dynamic phenomenon. This nature poses a challenge for real-time precoder design. While optimization-based solutions are widely researched, they are complex and heavily rely on perfect prior information, which is impractical in double dynamics. To address this challenge, we propose using constrained deep reinforcement learning (CDRL) to facilitate dynamic updates to the ISAC precoder design. Additionally, the primal dual-deep deterministic policy gradient (PD-DDPG) and Wolpertinger architecture are tailored to efficiently train the algorithm under complex constraints and variable numbers of users. The proposed scheme not only adapts to the dynamics based on observations but also leverages environmental information to enhance performance and reduce complexity. Its superiority over existing candidates has been validated through experiments.","sentences":["Integrated sensing and communication (ISAC) technology is essential for enabling the vehicular networks.","However, the communication channel in this scenario exhibits time-varying characteristics, and the potential targets may move rapidly, creating a doubly-dynamic phenomenon.","This nature poses a challenge for real-time precoder design.","While optimization-based solutions are widely researched, they are complex and heavily rely on perfect prior information, which is impractical in double dynamics.","To address this challenge, we propose using constrained deep reinforcement learning (CDRL) to facilitate dynamic updates to the ISAC precoder design.","Additionally, the primal dual-deep deterministic policy gradient (PD-DDPG) and Wolpertinger architecture are tailored to efficiently train the algorithm under complex constraints and variable numbers of users.","The proposed scheme not only adapts to the dynamics based on observations but also leverages environmental information to enhance performance and reduce complexity.","Its superiority over existing candidates has been validated through experiments."],"url":"http://arxiv.org/abs/2405.14347v1","category":"eess.SP"}
{"created":"2024-05-23 09:18:25","title":"Mixture of Public and Private Distributions in Imperfect Information Games","abstract":"In imperfect information games (e.g. Bridge, Skat, Poker), one of the fundamental considerations is to infer the missing information while at the same time avoiding the disclosure of private information. Disregarding the issue of protecting private information can lead to a highly exploitable performance. Yet, excessive attention to it leads to hesitations that are no longer consistent with our private information. In our work, we show that to improve performance, one must choose whether to use a player's private information. We extend our work by proposing a new belief distribution depending on the amount of private and public information desired. We empirically demonstrate an increase in performance and, with the aim of further improving performance, the new distribution should be used according to the position in the game. Our experiments have been done on multiple benchmarks and in multiple determinization-based algorithms (PIMC and IS-MCTS).","sentences":["In imperfect information games (e.g. Bridge, Skat, Poker), one of the fundamental considerations is to infer the missing information while at the same time avoiding the disclosure of private information.","Disregarding the issue of protecting private information can lead to a highly exploitable performance.","Yet, excessive attention to it leads to hesitations that are no longer consistent with our private information.","In our work, we show that to improve performance, one must choose whether to use a player's private information.","We extend our work by proposing a new belief distribution depending on the amount of private and public information desired.","We empirically demonstrate an increase in performance and, with the aim of further improving performance, the new distribution should be used according to the position in the game.","Our experiments have been done on multiple benchmarks and in multiple determinization-based algorithms (PIMC and IS-MCTS)."],"url":"http://arxiv.org/abs/2405.14346v1","category":"cs.AI"}
{"created":"2024-05-23 09:08:09","title":"MAMBA4D: Efficient Long-Sequence Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models","abstract":"Point cloud videos effectively capture real-world spatial geometries and temporal dynamics, which are essential for enabling intelligent agents to understand the dynamically changing 3D world we live in. Although static 3D point cloud processing has witnessed significant advancements, designing an effective 4D point cloud video backbone remains challenging, mainly due to the irregular and unordered distribution of points and temporal inconsistencies across frames. Moreover, recent state-of-the-art 4D backbones predominantly rely on transformer-based architectures, which commonly suffer from large computational costs due to their quadratic complexity, particularly when processing long video sequences. To address these challenges, we propose a novel 4D point cloud video understanding backbone based on the recently advanced State Space Models (SSMs). Specifically, our backbone begins by disentangling space and time in raw 4D sequences, and then establishing spatio-temporal correlations using our newly developed Intra-frame Spatial Mamba and Inter-frame Temporal Mamba blocks. The Intra-frame Spatial Mamba module is designed to encode locally similar or related geometric structures within a certain temporal searching stride, which can effectively capture short-term dynamics. Subsequently, these locally correlated tokens are delivered to the Inter-frame Temporal Mamba module, which globally integrates point features across the entire video with linear complexity, further establishing long-range motion dependencies. Experimental results on human action recognition and 4D semantic segmentation tasks demonstrate the superiority of our proposed method. Especially, for long video sequences, our proposed Mamba-based method has an 87.5% GPU memory reduction, 5.36 times speed-up, and much higher accuracy (up to +10.4%) compared with transformer-based counterparts on MSR-Action3D dataset.","sentences":["Point cloud videos effectively capture real-world spatial geometries and temporal dynamics, which are essential for enabling intelligent agents to understand the dynamically changing 3D world we live in.","Although static 3D point cloud processing has witnessed significant advancements, designing an effective 4D point cloud video backbone remains challenging, mainly due to the irregular and unordered distribution of points and temporal inconsistencies across frames.","Moreover, recent state-of-the-art 4D backbones predominantly rely on transformer-based architectures, which commonly suffer from large computational costs due to their quadratic complexity, particularly when processing long video sequences.","To address these challenges, we propose a novel 4D point cloud video understanding backbone based on the recently advanced State Space Models (SSMs).","Specifically, our backbone begins by disentangling space and time in raw 4D sequences, and then establishing spatio-temporal correlations using our newly developed Intra-frame Spatial Mamba and Inter-frame Temporal Mamba blocks.","The Intra-frame Spatial Mamba module is designed to encode locally similar or related geometric structures within a certain temporal searching stride, which can effectively capture short-term dynamics.","Subsequently, these locally correlated tokens are delivered to the Inter-frame Temporal Mamba module, which globally integrates point features across the entire video with linear complexity, further establishing long-range motion dependencies.","Experimental results on human action recognition and 4D semantic segmentation tasks demonstrate the superiority of our proposed method.","Especially, for long video sequences, our proposed Mamba-based method has an 87.5% GPU memory reduction, 5.36 times speed-up, and much higher accuracy (up to +10.4%) compared with transformer-based counterparts on MSR-Action3D dataset."],"url":"http://arxiv.org/abs/2405.14338v1","category":"cs.CV"}
{"created":"2024-05-23 09:07:27","title":"Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning","abstract":"This work investigates the offline formulation of the contextual bandit problem, where the goal is to leverage past interactions collected under a behavior policy to evaluate, select, and learn new, potentially better-performing, policies. Motivated by critical applications, we move beyond point estimators. Instead, we adopt the principle of pessimism where we construct upper bounds that assess a policy's worst-case performance, enabling us to confidently select and learn improved policies. Precisely, we introduce novel, fully empirical concentration bounds for a broad class of importance weighting risk estimators. These bounds are general enough to cover most existing estimators and pave the way for the development of new ones. In particular, our pursuit of the tightest bound within this class motivates a novel estimator (LS), that logarithmically smooths large importance weights. The bound for LS is provably tighter than all its competitors, and naturally results in improved policy selection and learning strategies. Extensive policy evaluation, selection, and learning experiments highlight the versatility and favorable performance of LS.","sentences":["This work investigates the offline formulation of the contextual bandit problem, where the goal is to leverage past interactions collected under a behavior policy to evaluate, select, and learn new, potentially better-performing, policies.","Motivated by critical applications, we move beyond point estimators.","Instead, we adopt the principle of pessimism where we construct upper bounds that assess a policy's worst-case performance, enabling us to confidently select and learn improved policies.","Precisely, we introduce novel, fully empirical concentration bounds for a broad class of importance weighting risk estimators.","These bounds are general enough to cover most existing estimators and pave the way for the development of new ones.","In particular, our pursuit of the tightest bound within this class motivates a novel estimator (LS), that logarithmically smooths large importance weights.","The bound for LS is provably tighter than all its competitors, and naturally results in improved policy selection and learning strategies.","Extensive policy evaluation, selection, and learning experiments highlight the versatility and favorable performance of LS."],"url":"http://arxiv.org/abs/2405.14335v1","category":"stat.ML"}
{"created":"2024-05-23 09:03:42","title":"DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data","abstract":"Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data. To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems. This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After fine-tuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%. Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any. These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs. Both the synthetic dataset and the model will be made available to facilitate further research in this promising field.","sentences":["Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability.","Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data.","To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems.","This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data.","After fine-tuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.","Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any.","These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs.","Both the synthetic dataset and the model will be made available to facilitate further research in this promising field."],"url":"http://arxiv.org/abs/2405.14333v1","category":"cs.AI"}
{"created":"2024-05-23 09:00:59","title":"LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable Computer Vision","abstract":"Prototypical parts networks combine the power of deep learning with the explainability of case-based reasoning to make accurate, interpretable decisions. They follow the this looks like that reasoning, representing each prototypical part with patches from training images. However, a single image patch comprises multiple visual features, such as color, shape, and texture, making it difficult for users to identify which feature is important to the model.   To reduce this ambiguity, we introduce the Lucid Prototypical Parts Network (LucidPPN), a novel prototypical parts network that separates color prototypes from other visual features. Our method employs two reasoning branches: one for non-color visual features, processing grayscale images, and another focusing solely on color information. This separation allows us to clarify whether the model's decisions are based on color, shape, or texture. Additionally, LucidPPN identifies prototypical parts corresponding to semantic parts of classified objects, making comparisons between data classes more intuitive, e.g., when two bird species might differ primarily in belly color.   Our experiments demonstrate that the two branches are complementary and together achieve results comparable to baseline methods. More importantly, LucidPPN generates less ambiguous prototypical parts, enhancing user understanding.","sentences":["Prototypical parts networks combine the power of deep learning with the explainability of case-based reasoning to make accurate, interpretable decisions.","They follow the this looks like that reasoning, representing each prototypical part with patches from training images.","However, a single image patch comprises multiple visual features, such as color, shape, and texture, making it difficult for users to identify which feature is important to the model.   ","To reduce this ambiguity, we introduce the Lucid Prototypical Parts Network (LucidPPN), a novel prototypical parts network that separates color prototypes from other visual features.","Our method employs two reasoning branches: one for non-color visual features, processing grayscale images, and another focusing solely on color information.","This separation allows us to clarify whether the model's decisions are based on color, shape, or texture.","Additionally, LucidPPN identifies prototypical parts corresponding to semantic parts of classified objects, making comparisons between data classes more intuitive, e.g., when two bird species might differ primarily in belly color.   ","Our experiments demonstrate that the two branches are complementary and together achieve results comparable to baseline methods.","More importantly, LucidPPN generates less ambiguous prototypical parts, enhancing user understanding."],"url":"http://arxiv.org/abs/2405.14331v1","category":"cs.CV"}
{"created":"2024-05-23 08:57:10","title":"Autoregressive Image Diffusion: Generating Image Sequence and Application in MRI","abstract":"Magnetic resonance imaging (MRI) is a widely used non-invasive imaging modality. However, a persistent challenge lies in balancing image quality with imaging speed. This trade-off is primarily constrained by k-space measurements, which traverse specific trajectories in the spatial Fourier domain (k-space). These measurements are often undersampled to shorten acquisition times, resulting in image artifacts and compromised quality. Generative models learn image distributions and can be used to reconstruct high-quality images from undersampled k-space data. In this work, we present the autoregressive image diffusion (AID) model for image sequences and use it to sample the posterior for accelerated MRI reconstruction. The algorithm incorporates both undersampled k-space and pre-existing information. Models trained with fastMRI dataset are evaluated comprehensively. The results show that the AID model can robustly generate sequentially coherent image sequences. In 3D and dynamic MRI, the AID can outperform the standard diffusion model and reduce hallucinations, due to the learned inter-image dependencies.","sentences":["Magnetic resonance imaging (MRI) is a widely used non-invasive imaging modality.","However, a persistent challenge lies in balancing image quality with imaging speed.","This trade-off is primarily constrained by k-space measurements, which traverse specific trajectories in the spatial Fourier domain (k-space).","These measurements are often undersampled to shorten acquisition times, resulting in image artifacts and compromised quality.","Generative models learn image distributions and can be used to reconstruct high-quality images from undersampled k-space data.","In this work, we present the autoregressive image diffusion (AID) model for image sequences and use it to sample the posterior for accelerated MRI reconstruction.","The algorithm incorporates both undersampled k-space and pre-existing information.","Models trained with fastMRI dataset are evaluated comprehensively.","The results show that the AID model can robustly generate sequentially coherent image sequences.","In 3D and dynamic MRI, the AID can outperform the standard diffusion model and reduce hallucinations, due to the learned inter-image dependencies."],"url":"http://arxiv.org/abs/2405.14327v1","category":"eess.IV"}
{"created":"2024-05-23 08:54:50","title":"SmartCS: Enabling the Creation of ML-Powered Computer Vision Mobile Apps for Citizen Science Applications without Coding","abstract":"It is undeniable that citizen science contributes to the advancement of various fields of study. There are now software tools that facilitate the development of citizen science apps. However, apps developed with these tools rely on individual human skills to correctly collect useful data. Machine learning (ML)-aided apps provide on-field guidance to citizen scientists on data collection tasks. However, these apps rely on server-side ML support, and therefore need a reliable internet connection. Furthermore, the development of citizen science apps with ML support requires a significant investment of time and money. For some projects, this barrier may preclude the use of citizen science effectively. We present a platform that democratizes citizen science by making it accessible to a much broader audience of both researchers and participants. The SmartCS platform allows one to create citizen science apps with ML support quickly and without coding skills. Apps developed using SmartCS have client-side ML support, making them usable in the field, even when there is no internet connection. The client-side ML helps educate users to better recognize the subjects, thereby enabling high-quality data collection. We present several citizen science apps created using SmartCS, some of which were conceived and created by high school students.","sentences":["It is undeniable that citizen science contributes to the advancement of various fields of study.","There are now software tools that facilitate the development of citizen science apps.","However, apps developed with these tools rely on individual human skills to correctly collect useful data.","Machine learning (ML)-aided apps provide on-field guidance to citizen scientists on data collection tasks.","However, these apps rely on server-side ML support, and therefore need a reliable internet connection.","Furthermore, the development of citizen science apps with ML support requires a significant investment of time and money.","For some projects, this barrier may preclude the use of citizen science effectively.","We present a platform that democratizes citizen science by making it accessible to a much broader audience of both researchers and participants.","The SmartCS platform allows one to create citizen science apps with ML support quickly and without coding skills.","Apps developed using SmartCS have client-side ML support, making them usable in the field, even when there is no internet connection.","The client-side ML helps educate users to better recognize the subjects, thereby enabling high-quality data collection.","We present several citizen science apps created using SmartCS, some of which were conceived and created by high school students."],"url":"http://arxiv.org/abs/2405.14323v1","category":"cs.CY"}
{"created":"2024-05-23 08:33:19","title":"Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration","abstract":"Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at \\url{https://read-llm.github.io/}.","sentences":["Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world.","Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination.","However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs.","In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans.","Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function.","It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task.","We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems.","Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs.","More results are given at \\url{https://read-llm.github.io/}."],"url":"http://arxiv.org/abs/2405.14314v1","category":"cs.AI"}
{"created":"2024-05-23 08:28:44","title":"AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation","abstract":"Graph Neural Networks (GNNs) have revolutionized graph-based machine learning, but their heavy computational demands pose challenges for latency-sensitive edge devices in practical industrial applications. In response, a new wave of methods, collectively known as GNN-to-MLP Knowledge Distillation, has emerged. They aim to transfer GNN-learned knowledge to a more efficient MLP student, which offers faster, resource-efficient inference while maintaining competitive performance compared to GNNs. However, these methods face significant challenges in situations with insufficient training data and incomplete test data, limiting their applicability in real-world applications. To address these challenges, we propose AdaGMLP, an AdaBoosting GNN-to-MLP Knowledge Distillation framework. It leverages an ensemble of diverse MLP students trained on different subsets of labeled nodes, addressing the issue of insufficient training data. Additionally, it incorporates a Node Alignment technique for robust predictions on test data with missing or incomplete features. Our experiments on seven benchmark datasets with different settings demonstrate that AdaGMLP outperforms existing G2M methods, making it suitable for a wide range of latency-sensitive real-world applications. We have submitted our code to the GitHub repository (https://github.com/WeigangLu/AdaGMLP-KDD24).","sentences":["Graph Neural Networks (GNNs) have revolutionized graph-based machine learning, but their heavy computational demands pose challenges for latency-sensitive edge devices in practical industrial applications.","In response, a new wave of methods, collectively known as GNN-to-MLP Knowledge Distillation, has emerged.","They aim to transfer GNN-learned knowledge to a more efficient MLP student, which offers faster, resource-efficient inference while maintaining competitive performance compared to GNNs.","However, these methods face significant challenges in situations with insufficient training data and incomplete test data, limiting their applicability in real-world applications.","To address these challenges, we propose AdaGMLP, an AdaBoosting GNN-to-MLP Knowledge Distillation framework.","It leverages an ensemble of diverse MLP students trained on different subsets of labeled nodes, addressing the issue of insufficient training data.","Additionally, it incorporates a Node Alignment technique for robust predictions on test data with missing or incomplete features.","Our experiments on seven benchmark datasets with different settings demonstrate that AdaGMLP outperforms existing G2M methods, making it suitable for a wide range of latency-sensitive real-world applications.","We have submitted our code to the GitHub repository (https://github.com/WeigangLu/AdaGMLP-KDD24)."],"url":"http://arxiv.org/abs/2405.14307v1","category":"cs.LG"}
{"created":"2024-05-23 08:21:11","title":"Does context matter in digital pathology?","abstract":"The development of Artificial Intelligence for healthcare is of great importance. Models can sometimes achieve even superior performance to human experts, however, they can reason based on spurious features. This is not acceptable to the experts as it is expected that the models catch the valid patterns in the data following domain expertise. In the work, we analyse whether Deep Learning (DL) models for vision follow the histopathologists' practice so that when diagnosing a part of a lesion, they take into account also the surrounding tissues which serve as context. It turns out that the performance of DL models significantly decreases when the amount of contextual information is limited, therefore contextual information is valuable at prediction time. Moreover, we show that the models sometimes behave in an unstable way as for some images, they change the predictions many times depending on the size of the context. It may suggest that partial contextual information can be misleading.","sentences":["The development of Artificial Intelligence for healthcare is of great importance.","Models can sometimes achieve even superior performance to human experts, however, they can reason based on spurious features.","This is not acceptable to the experts as it is expected that the models catch the valid patterns in the data following domain expertise.","In the work, we analyse whether Deep Learning (DL) models for vision follow the histopathologists' practice so that when diagnosing a part of a lesion, they take into account also the surrounding tissues which serve as context.","It turns out that the performance of DL models significantly decreases when the amount of contextual information is limited, therefore contextual information is valuable at prediction time.","Moreover, we show that the models sometimes behave in an unstable way as for some images, they change the predictions many times depending on the size of the context.","It may suggest that partial contextual information can be misleading."],"url":"http://arxiv.org/abs/2405.14301v1","category":"cs.CV"}
{"created":"2024-05-23 08:18:30","title":"Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models","abstract":"The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results. However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts and the number of experts to be activated (referred to as top-k), resulting in significant computational overhead due to the extensive model training by searching over various hyper-parameter configurations. As a remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating method that enables each token to automatically determine the number of experts to activate. (2) An adaptive process automatically adjusts the number of experts during training. Extensive numerical results across Vision, Language, and Vision-Language tasks demonstrate the effectiveness of our approach to achieve competitive performance compared to GMoE for vision and language tasks, and MoE-LLaVA for vision-language tasks, while maintaining efficiency by activating fewer parameters. Our code is available at https://github.com/LINs-lab/DynMoE.","sentences":["The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results.","However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts and the number of experts to be activated (referred to as top-k), resulting in significant computational overhead due to the extensive model training by searching over various hyper-parameter configurations.","As a remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique.","DynMoE incorporates (1) a novel gating method that enables each token to automatically determine the number of experts to activate.","(2) An adaptive process automatically adjusts the number of experts during training.","Extensive numerical results across Vision, Language, and Vision-Language tasks demonstrate the effectiveness of our approach to achieve competitive performance compared to GMoE for vision and language tasks, and MoE-LLaVA for vision-language tasks, while maintaining efficiency by activating fewer parameters.","Our code is available at https://github.com/LINs-lab/DynMoE."],"url":"http://arxiv.org/abs/2405.14297v1","category":"cs.LG"}
{"created":"2024-05-23 08:09:21","title":"Variational Bayes for Federated Continual Learning","abstract":"Federated continual learning (FCL) has received increasing attention due to its potential in handling real-world streaming data, characterized by evolving data distributions and varying client classes over time. The constraints of storage limitations and privacy concerns confine local models to exclusively access the present data within each learning cycle. Consequently, this restriction induces performance degradation in model training on previous data, termed \"catastrophic forgetting\". However, existing FCL approaches need to identify or know changes in data distribution, which is difficult in the real world. To release these limitations, this paper directs attention to a broader continuous framework. Within this framework, we introduce Federated Bayesian Neural Network (FedBNN), a versatile and efficacious framework employing a variational Bayesian neural network across all clients. Our method continually integrates knowledge from local and historical data distributions into a single model, adeptly learning from new data distributions while retaining performance on historical distributions. We rigorously evaluate FedBNN's performance against prevalent methods in federated learning and continual learning using various metrics. Experimental analyses across diverse datasets demonstrate that FedBNN achieves state-of-the-art results in mitigating forgetting.","sentences":["Federated continual learning (FCL) has received increasing attention due to its potential in handling real-world streaming data, characterized by evolving data distributions and varying client classes over time.","The constraints of storage limitations and privacy concerns confine local models to exclusively access the present data within each learning cycle.","Consequently, this restriction induces performance degradation in model training on previous data, termed \"catastrophic forgetting\".","However, existing FCL approaches need to identify or know changes in data distribution, which is difficult in the real world.","To release these limitations, this paper directs attention to a broader continuous framework.","Within this framework, we introduce Federated Bayesian Neural Network (FedBNN), a versatile and efficacious framework employing a variational Bayesian neural network across all clients.","Our method continually integrates knowledge from local and historical data distributions into a single model, adeptly learning from new data distributions while retaining performance on historical distributions.","We rigorously evaluate FedBNN's performance against prevalent methods in federated learning and continual learning using various metrics.","Experimental analyses across diverse datasets demonstrate that FedBNN achieves state-of-the-art results in mitigating forgetting."],"url":"http://arxiv.org/abs/2405.14291v1","category":"cs.LG"}
{"created":"2024-05-23 08:01:25","title":"Co-Representation Neural Hypergraph Diffusion for Edge-Dependent Node Classification","abstract":"Hypergraphs are widely employed to represent complex higher-order relationships in real-world applications. Most hypergraph learning research focuses on node- or edge-level tasks. A practically relevant but more challenging task, edge-dependent node classification (ENC), is only recently proposed. In ENC, a node can have different labels across different hyperedges, which requires the modeling of node-hyperedge pairs instead of single nodes or hyperedges. Existing solutions for this task are based on message passing and model within-edge and within-node interactions as multi-input single-output functions. This brings three limitations: (1) non-adaptive representation size, (2) node/edge agnostic messages, and (3) insufficient interactions among nodes or hyperedges. To tackle these limitations, we develop CoNHD, a new solution based on hypergraph diffusion. Specifically, we first extend hypergraph diffusion using node-hyperedge co-representations. This extension explicitly models both within-edge and within-node interactions as multi-input multi-output functions using two equivariant diffusion operators. To avoid handcrafted regularization functions, we propose a neural implementation for the co-representation hypergraph diffusion process. Extensive experiments demonstrate the effectiveness and efficiency of the proposed CoNHD model.","sentences":["Hypergraphs are widely employed to represent complex higher-order relationships in real-world applications.","Most hypergraph learning research focuses on node- or edge-level tasks.","A practically relevant but more challenging task, edge-dependent node classification (ENC), is only recently proposed.","In ENC, a node can have different labels across different hyperedges, which requires the modeling of node-hyperedge pairs instead of single nodes or hyperedges.","Existing solutions for this task are based on message passing and model within-edge and within-node interactions as multi-input single-output functions.","This brings three limitations: (1) non-adaptive representation size, (2) node/edge agnostic messages, and (3) insufficient interactions among nodes or hyperedges.","To tackle these limitations, we develop CoNHD, a new solution based on hypergraph diffusion.","Specifically, we first extend hypergraph diffusion using node-hyperedge co-representations.","This extension explicitly models both within-edge and within-node interactions as multi-input multi-output functions using two equivariant diffusion operators.","To avoid handcrafted regularization functions, we propose a neural implementation for the co-representation hypergraph diffusion process.","Extensive experiments demonstrate the effectiveness and efficiency of the proposed CoNHD model."],"url":"http://arxiv.org/abs/2405.14286v1","category":"cs.LG"}
{"created":"2024-05-23 07:51:05","title":"A fast algorithm to minimize prediction loss of the optimal solution in inverse optimization problem of MILP","abstract":"This paper tackles the problem of minimizing the prediction loss of the optimal solution (PLS) of the MILP with given data, which is one of the inverse optimization problems. While existing methods can approximately solve this problem, their implementation in the high-dimensional case to minimize the PLS is computationally expensive because they are inefficient in reducing the prediction loss of weights (PLW). We propose a fast algorithm for minimizing the PLS of MILP. To demonstrate this property, we attribute the problem of minimizing the PLS to that of minimizing the suboptimality loss (SL), which is convex. If the PLS does not vanish, we can adapt the SL to have the estimated loss (SPO loss) with a positive lower bound, which enables us to evaluate the PLW. Consequently, we prove that the proposed algorithm can effectively reduce the PLW and achieve the minimum value of PLS. Our numerical experiments demonstrated that our algorithm successfully achieved the minimum PLS. Compared to existing methods, our algorithm exhibited a smaller dimensionality effect and minimized the PLS in less than 1/7 the number of iterations. Especially in high dimensions, our algorithm significantly improved the PLS by more than two orders of magnitude compared to existing algorithms.","sentences":["This paper tackles the problem of minimizing the prediction loss of the optimal solution (PLS) of the MILP with given data, which is one of the inverse optimization problems.","While existing methods can approximately solve this problem, their implementation in the high-dimensional case to minimize the PLS is computationally expensive because they are inefficient in reducing the prediction loss of weights (PLW).","We propose a fast algorithm for minimizing the PLS of MILP.","To demonstrate this property, we attribute the problem of minimizing the PLS to that of minimizing the suboptimality loss (SL), which is convex.","If the PLS does not vanish, we can adapt the SL to have the estimated loss (SPO loss) with a positive lower bound, which enables us to evaluate the PLW.","Consequently, we prove that the proposed algorithm can effectively reduce the PLW and achieve the minimum value of PLS.","Our numerical experiments demonstrated that our algorithm successfully achieved the minimum PLS.","Compared to existing methods, our algorithm exhibited a smaller dimensionality effect and minimized the PLS in less than 1/7 the number of iterations.","Especially in high dimensions, our algorithm significantly improved the PLS by more than two orders of magnitude compared to existing algorithms."],"url":"http://arxiv.org/abs/2405.14273v1","category":"cs.LG"}
{"created":"2024-05-23 07:48:00","title":"Sparse $L^1$-Autoencoders for Scientific Data Compression","abstract":"Scientific datasets present unique challenges for machine learning-driven compression methods, including more stringent requirements on accuracy and mitigation of potential invalidating artifacts. Drawing on results from compressed sensing and rate-distortion theory, we introduce effective data compression methods by developing autoencoders using high dimensional latent spaces that are $L^1$-regularized to obtain sparse low dimensional representations. We show how these information-rich latent spaces can be used to mitigate blurring and other artifacts to obtain highly effective data compression methods for scientific data. We demonstrate our methods for short angle scattering (SAS) datasets showing they can achieve compression ratios around two orders of magnitude and in some cases better. Our compression methods show promise for use in addressing current bottlenecks in transmission, storage, and analysis in high-performance distributed computing environments. This is central to processing the large volume of SAS data being generated at shared experimental facilities around the world to support scientific investigations. Our approaches provide general ways for obtaining specialized compression methods for targeted scientific datasets.","sentences":["Scientific datasets present unique challenges for machine learning-driven compression methods, including more stringent requirements on accuracy and mitigation of potential invalidating artifacts.","Drawing on results from compressed sensing and rate-distortion theory, we introduce effective data compression methods by developing autoencoders using high dimensional latent spaces that are $L^1$-regularized to obtain sparse low dimensional representations.","We show how these information-rich latent spaces can be used to mitigate blurring and other artifacts to obtain highly effective data compression methods for scientific data.","We demonstrate our methods for short angle scattering (SAS) datasets showing they can achieve compression ratios around two orders of magnitude and in some cases better.","Our compression methods show promise for use in addressing current bottlenecks in transmission, storage, and analysis in high-performance distributed computing environments.","This is central to processing the large volume of SAS data being generated at shared experimental facilities around the world to support scientific investigations.","Our approaches provide general ways for obtaining specialized compression methods for targeted scientific datasets."],"url":"http://arxiv.org/abs/2405.14270v1","category":"cs.LG"}
{"created":"2024-05-23 07:47:06","title":"Multi-Representation Genetic Programming: A Case Study on Tree-based and Linear Representations","abstract":"Existing genetic programming (GP) methods are typically designed based on a certain representation, such as tree-based or linear representations. These representations show various pros and cons in different domains. However, due to the complicated relationships among representation and fitness landscapes of GP, it is hard to intuitively determine which GP representation is the most suitable for solving a certain problem. Evolving programs (or models) with multiple representations simultaneously can alternatively search on different fitness landscapes since representations are highly related to the search space that essentially defines the fitness landscape. Fully using the latent synergies among different GP individual representations might be helpful for GP to search for better solutions. However, existing GP literature rarely investigates the simultaneous effective use of evolving multiple representations. To fill this gap, this paper proposes a multi-representation GP algorithm based on tree-based and linear representations, which are two commonly used GP representations. In addition, we develop a new cross-representation crossover operator to harness the interplay between tree-based and linear representations. Empirical results show that navigating the learned knowledge between basic tree-based and linear representations successfully improves the effectiveness of GP with solely tree-based or linear representation in solving symbolic regression and dynamic job shop scheduling problems.","sentences":["Existing genetic programming (GP) methods are typically designed based on a certain representation, such as tree-based or linear representations.","These representations show various pros and cons in different domains.","However, due to the complicated relationships among representation and fitness landscapes of GP, it is hard to intuitively determine which GP representation is the most suitable for solving a certain problem.","Evolving programs (or models) with multiple representations simultaneously can alternatively search on different fitness landscapes since representations are highly related to the search space that essentially defines the fitness landscape.","Fully using the latent synergies among different GP individual representations might be helpful for GP to search for better solutions.","However, existing GP literature rarely investigates the simultaneous effective use of evolving multiple representations.","To fill this gap, this paper proposes a multi-representation GP algorithm based on tree-based and linear representations, which are two commonly used GP representations.","In addition, we develop a new cross-representation crossover operator to harness the interplay between tree-based and linear representations.","Empirical results show that navigating the learned knowledge between basic tree-based and linear representations successfully improves the effectiveness of GP with solely tree-based or linear representation in solving symbolic regression and dynamic job shop scheduling problems."],"url":"http://arxiv.org/abs/2405.14268v1","category":"cs.NE"}
{"created":"2024-05-23 07:45:48","title":"A Gap in Time: The Challenge of Processing Heterogeneous IoT Point Data in Buildings","abstract":"The growing need for sustainable energy solutions has driven the integration of digitalized buildings into the power grid, utilizing Internet-of-Things technology to optimize building performance and energy efficiency. However, incorporating IoT point data within deep-learning frameworks for energy management presents a complex challenge, predominantly due to the inherent data heterogeneity. This paper comprehensively analyzes the multifaceted heterogeneity present in real-world building IoT data streams. We meticulously dissect the heterogeneity across multiple dimensions, encompassing ontology, etiology, temporal irregularity, spatial diversity, and their combined effects on the IoT point data distribution. In addition, experiments using state-of-the-art forecasting models are conducted to evaluate their impacts on the performance of deep-learning models for forecasting tasks. By charting the diversity along these dimensions, we illustrate the challenges and delineate pathways for future research to leverage this heterogeneity as a resource rather than a roadblock. This exploration sets the stage for advancing the predictive abilities of deep-learning algorithms and catalyzing the evolution of intelligent energy-efficient buildings.","sentences":["The growing need for sustainable energy solutions has driven the integration of digitalized buildings into the power grid, utilizing Internet-of-Things technology to optimize building performance and energy efficiency.","However, incorporating IoT point data within deep-learning frameworks for energy management presents a complex challenge, predominantly due to the inherent data heterogeneity.","This paper comprehensively analyzes the multifaceted heterogeneity present in real-world building IoT data streams.","We meticulously dissect the heterogeneity across multiple dimensions, encompassing ontology, etiology, temporal irregularity, spatial diversity, and their combined effects on the IoT point data distribution.","In addition, experiments using state-of-the-art forecasting models are conducted to evaluate their impacts on the performance of deep-learning models for forecasting tasks.","By charting the diversity along these dimensions, we illustrate the challenges and delineate pathways for future research to leverage this heterogeneity as a resource rather than a roadblock.","This exploration sets the stage for advancing the predictive abilities of deep-learning algorithms and catalyzing the evolution of intelligent energy-efficient buildings."],"url":"http://arxiv.org/abs/2405.14267v1","category":"cs.LG"}
{"created":"2024-05-23 07:44:24","title":"Deep Reinforcement Learning for 5*5 Multiplayer Go","abstract":"In recent years, much progress has been made in computer Go and most of the results have been obtained thanks to search algorithms (Monte Carlo Tree Search) and Deep Reinforcement Learning (DRL). In this paper, we propose to use and analyze the latest algorithms that use search and DRL (AlphaZero and Descent algorithms) to automatically learn to play an extended version of the game of Go with more than two players. We show that using search and DRL we were able to improve the level of play, even though there are more than two players.","sentences":["In recent years, much progress has been made in computer Go and most of the results have been obtained thanks to search algorithms (Monte Carlo Tree Search) and Deep Reinforcement Learning (DRL).","In this paper, we propose to use and analyze the latest algorithms that use search and DRL (AlphaZero and Descent algorithms) to automatically learn to play an extended version of the game of Go with more than two players.","We show that using search and DRL we were able to improve the level of play, even though there are more than two players."],"url":"http://arxiv.org/abs/2405.14265v1","category":"cs.AI"}
{"created":"2024-05-23 07:43:24","title":"Reassessing Evaluation Functions in Algorithmic Recourse: An Empirical Study from a Human-Centered Perspective","abstract":"In this study, we critically examine the foundational premise of algorithmic recourse - a process of generating counterfactual action plans (i.e., recourses) assisting individuals to reverse adverse decisions made by AI systems. The assumption underlying algorithmic recourse is that individuals accept and act on recourses that minimize the gap between their current and desired states. This assumption, however, remains empirically unverified. To address this issue, we conducted a user study with 362 participants and assessed whether minimizing the distance function, a metric of the gap between the current and desired states, indeed prompts them to accept and act upon suggested recourses. Our findings reveal a nuanced landscape: participants' acceptance of recourses did not correlate with the recourse distance. Moreover, participants' willingness to act upon recourses peaked at the minimal recourse distance but was otherwise constant. These findings cast doubt on the prevailing assumption of algorithmic recourse research and signal the need to rethink the evaluation functions to pave the way for human-centered recourse generation.","sentences":["In this study, we critically examine the foundational premise of algorithmic recourse - a process of generating counterfactual action plans (i.e., recourses) assisting individuals to reverse adverse decisions made by AI systems.","The assumption underlying algorithmic recourse is that individuals accept and act on recourses that minimize the gap between their current and desired states.","This assumption, however, remains empirically unverified.","To address this issue, we conducted a user study with 362 participants and assessed whether minimizing the distance function, a metric of the gap between the current and desired states, indeed prompts them to accept and act upon suggested recourses.","Our findings reveal a nuanced landscape: participants' acceptance of recourses did not correlate with the recourse distance.","Moreover, participants' willingness to act upon recourses peaked at the minimal recourse distance but was otherwise constant.","These findings cast doubt on the prevailing assumption of algorithmic recourse research and signal the need to rethink the evaluation functions to pave the way for human-centered recourse generation."],"url":"http://arxiv.org/abs/2405.14264v1","category":"cs.LG"}
{"created":"2024-05-23 07:40:21","title":"Graph Sparsification via Mixture of Graphs","abstract":"Graph Neural Networks (GNNs) have demonstrated superior performance across various graph learning tasks but face significant computational challenges when applied to large-scale graphs. One effective approach to mitigate these challenges is graph sparsification, which involves removing non-essential edges to reduce computational overhead. However, previous graph sparsification methods often rely on a single global sparsity setting and uniform pruning criteria, failing to provide customized sparsification schemes for each node's complex local context. In this paper, we introduce Mixture-of-Graphs (MoG), leveraging the concept of Mixture-of-Experts (MoE), to dynamically select tailored pruning solutions for each node. Specifically, MoG incorporates multiple sparsifier experts, each characterized by unique sparsity levels and pruning criteria, and selects the appropriate experts for each node. Subsequently, MoG performs a mixture of the sparse graphs produced by different experts on the Grassmann manifold to derive an optimal sparse graph. One notable property of MoG is its entirely local nature, as it depends on the specific circumstances of each individual node. Extensive experiments on four large-scale OGB datasets and two superpixel datasets, equipped with five GNN backbones, demonstrate that MoG (I) identifies subgraphs at higher sparsity levels ($8.67\\%\\sim 50.85\\%$), with performance equal to or better than the dense graph, (II) achieves $1.47-2.62\\times$ speedup in GNN inference with negligible performance drop, and (III) boosts ``top-student'' GNN performance ($1.02\\%\\uparrow$ on RevGNN+\\textsc{ogbn-proteins} and $1.74\\%\\uparrow$ on DeeperGCN+\\textsc{ogbg-ppa}).","sentences":["Graph Neural Networks (GNNs) have demonstrated superior performance across various graph learning tasks but face significant computational challenges when applied to large-scale graphs.","One effective approach to mitigate these challenges is graph sparsification, which involves removing non-essential edges to reduce computational overhead.","However, previous graph sparsification methods often rely on a single global sparsity setting and uniform pruning criteria, failing to provide customized sparsification schemes for each node's complex local context.","In this paper, we introduce Mixture-of-Graphs (MoG), leveraging the concept of Mixture-of-Experts (MoE), to dynamically select tailored pruning solutions for each node.","Specifically, MoG incorporates multiple sparsifier experts, each characterized by unique sparsity levels and pruning criteria, and selects the appropriate experts for each node.","Subsequently, MoG performs a mixture of the sparse graphs produced by different experts on the Grassmann manifold to derive an optimal sparse graph.","One notable property of MoG is its entirely local nature, as it depends on the specific circumstances of each individual node.","Extensive experiments on four large-scale OGB datasets and two superpixel datasets, equipped with five GNN backbones, demonstrate that MoG (I) identifies subgraphs at higher sparsity levels ($8.67\\%\\sim 50.85\\%$), with performance equal to or better than the dense graph, (II) achieves $1.47-2.62\\times$ speedup in GNN inference with negligible performance drop, and (III) boosts ``top-student'' GNN performance ($1.02\\%\\uparrow$ on RevGNN+\\textsc{ogbn-proteins} and $1.74\\%\\uparrow$ on DeeperGCN+\\textsc{ogbg-ppa})."],"url":"http://arxiv.org/abs/2405.14260v1","category":"cs.LG"}
{"created":"2024-05-23 07:39:42","title":"Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Multi-modal Text Recognition","abstract":"We introduce ``Generative Fusion Decoding'' (GFD), a novel shallow fusion framework, utilized to integrate Large Language Models (LLMs) into multi-modal text recognition systems such as automatic speech recognition (ASR) and optical character recognition (OCR). We derive the formulas necessary to enable GFD to operate across mismatched token spaces of different models by mapping text token space to byte token space, enabling seamless fusion during the decoding process. The framework is plug-and-play, compatible with various auto-regressive models, and does not require re-training for feature alignment, thus overcoming limitations of previous fusion techniques. We highlight three main advantages of GFD: First, by simplifying the complexity of aligning different model sample spaces, GFD allows LLMs to correct errors in tandem with the recognition model, reducing computation latencies. Second, the in-context learning ability of LLMs is fully capitalized by GFD, increasing robustness in long-form speech recognition and instruction aware speech recognition. Third, GFD enables fusing recognition models deficient in Chinese text recognition with LLMs extensively trained on Chinese. Our evaluation demonstrates that GFD significantly improves performance in ASR and OCR tasks, with ASR reaching state-of-the-art in the NTUML2021 benchmark. GFD provides a significant step forward in model integration, offering a unified solution that could be widely applicable to leveraging existing pre-trained models through step by step fusion.","sentences":["We introduce ``Generative Fusion Decoding'' (GFD), a novel shallow fusion framework, utilized to integrate Large Language Models (LLMs) into multi-modal text recognition systems such as automatic speech recognition (ASR) and optical character recognition (OCR).","We derive the formulas necessary to enable GFD to operate across mismatched token spaces of different models by mapping text token space to byte token space, enabling seamless fusion during the decoding process.","The framework is plug-and-play, compatible with various auto-regressive models, and does not require re-training for feature alignment, thus overcoming limitations of previous fusion techniques.","We highlight three main advantages of GFD:","First, by simplifying the complexity of aligning different model sample spaces, GFD allows LLMs to correct errors in tandem with the recognition model, reducing computation latencies.","Second, the in-context learning ability of LLMs is fully capitalized by GFD, increasing robustness in long-form speech recognition and instruction aware speech recognition.","Third, GFD enables fusing recognition models deficient in Chinese text recognition with LLMs extensively trained on Chinese.","Our evaluation demonstrates that GFD significantly improves performance in ASR and OCR tasks, with ASR reaching state-of-the-art in the NTUML2021 benchmark.","GFD provides a significant step forward in model integration, offering a unified solution that could be widely applicable to leveraging existing pre-trained models through step by step fusion."],"url":"http://arxiv.org/abs/2405.14259v1","category":"cs.CL"}
{"created":"2024-05-23 07:37:16","title":"ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification","abstract":"KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for LLMs. First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced compared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and minimal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in accuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction in prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a $19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of $4096$.","sentences":["KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences.","Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance.","However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens.","In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for LLMs.","First, we construct a strong baseline for quantizing KV cache.","Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced compared to fine-grained groupwise quantization.","To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix.","Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention.","Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and minimal performance losses compared with previous KV cache compression methods.","For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in accuracy.","In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction in prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a $19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of $4096$."],"url":"http://arxiv.org/abs/2405.14256v1","category":"cs.LG"}
{"created":"2024-05-23 07:23:33","title":"Tell my why: Training preferences-based RL with human preferences and step-level explanations","abstract":"Human-in-the-loop reinforcement learning (HRL) allows the training of agents through various interfaces, even for non-expert humans. Recently, preference-based methods (PBRL), where the human has to give his preference over two trajectories, increased in popularity since they allow training in domains where more direct feedback is hard to formulate. However, the current PBRL methods have limitations and do not provide humans with an expressive interface for giving feedback. With this work, we propose a new preference-based learning method that provides humans with a more expressive interface to provide their preference over trajectories and a factual explanation (or annotation of why they have this preference). These explanations allow the human to explain what parts of the trajectory are most relevant for the preference. We allow the expression of the explanations over individual trajectory steps. We evaluate our method in various simulations using a simulated human oracle (with realistic restrictions), and our results show that our extended feedback can improve the speed of learning. Code & data: github.com/under-rewiev","sentences":["Human-in-the-loop reinforcement learning (HRL) allows the training of agents through various interfaces, even for non-expert humans.","Recently, preference-based methods (PBRL), where the human has to give his preference over two trajectories, increased in popularity since they allow training in domains where more direct feedback is hard to formulate.","However, the current PBRL methods have limitations and do not provide humans with an expressive interface for giving feedback.","With this work, we propose a new preference-based learning method that provides humans with a more expressive interface to provide their preference over trajectories and a factual explanation (or annotation of why they have this preference).","These explanations allow the human to explain what parts of the trajectory are most relevant for the preference.","We allow the expression of the explanations over individual trajectory steps.","We evaluate our method in various simulations using a simulated human oracle (with realistic restrictions), and our results show that our extended feedback can improve the speed of learning.","Code & data: github.com/under-rewiev"],"url":"http://arxiv.org/abs/2405.14244v1","category":"cs.AI"}
{"created":"2024-05-23 07:03:38","title":"Boosting Medical Image-based Cancer Detection via Text-guided Supervision from Reports","abstract":"The absence of adequately sufficient expert-level tumor annotations hinders the effectiveness of supervised learning based opportunistic cancer screening on medical imaging. Clinical reports (that are rich in descriptive textual details) can offer a \"free lunch'' supervision information and provide tumor location as a type of weak label to cope with screening tasks, thus saving human labeling workloads, if properly leveraged. However, predicting cancer only using such weak labels can be very changeling since tumors are usually presented in small anatomical regions compared to the whole 3D medical scans. Weakly semi-supervised learning (WSSL) utilizes a limited set of voxel-level tumor annotations and incorporates alongside a substantial number of medical images that have only off-the-shelf clinical reports, which may strike a good balance between minimizing expert annotation workload and optimizing screening efficacy. In this paper, we propose a novel text-guided learning method to achieve highly accurate cancer detection results. Through integrating diagnostic and tumor location text prompts into the text encoder of a vision-language model (VLM), optimization of weakly supervised learning can be effectively performed in the latent space of VLM, thereby enhancing the stability of training. Our approach can leverage clinical knowledge by large-scale pre-trained VLM to enhance generalization ability, and produce reliable pseudo tumor masks to improve cancer detection. Our extensive quantitative experimental results on a large-scale cancer dataset, including 1,651 unique patients, validate that our approach can reduce human annotation efforts by at least 70% while maintaining comparable cancer detection accuracy to competing fully supervised methods (AUC value 0.961 versus 0.966).","sentences":["The absence of adequately sufficient expert-level tumor annotations hinders the effectiveness of supervised learning based opportunistic cancer screening on medical imaging.","Clinical reports (that are rich in descriptive textual details) can offer a \"free lunch'' supervision information and provide tumor location as a type of weak label to cope with screening tasks, thus saving human labeling workloads, if properly leveraged.","However, predicting cancer only using such weak labels can be very changeling since tumors are usually presented in small anatomical regions compared to the whole 3D medical scans.","Weakly semi-supervised learning (WSSL) utilizes a limited set of voxel-level tumor annotations and incorporates alongside a substantial number of medical images that have only off-the-shelf clinical reports, which may strike a good balance between minimizing expert annotation workload and optimizing screening efficacy.","In this paper, we propose a novel text-guided learning method to achieve highly accurate cancer detection results.","Through integrating diagnostic and tumor location text prompts into the text encoder of a vision-language model (VLM), optimization of weakly supervised learning can be effectively performed in the latent space of VLM, thereby enhancing the stability of training.","Our approach can leverage clinical knowledge by large-scale pre-trained VLM to enhance generalization ability, and produce reliable pseudo tumor masks to improve cancer detection.","Our extensive quantitative experimental results on a large-scale cancer dataset, including 1,651 unique patients, validate that our approach can reduce human annotation efforts by at least 70% while maintaining comparable cancer detection accuracy to competing fully supervised methods (AUC value 0.961 versus 0.966)."],"url":"http://arxiv.org/abs/2405.14230v1","category":"cs.CV"}
{"created":"2024-05-23 06:57:04","title":"Variational Delayed Policy Optimization","abstract":"In environments with delayed observation, state augmentation by including actions within the delay window is adopted to retrieve Markovian property to enable reinforcement learning (RL). However, state-of-the-art (SOTA) RL techniques with Temporal-Difference (TD) learning frameworks often suffer from learning inefficiency, due to the significant expansion of the augmented state space with the delay. To improve learning efficiency without sacrificing performance, this work introduces a novel framework called Variational Delayed Policy Optimization (VDPO), which reformulates delayed RL as a variational inference problem. This problem is further modelled as a two-step iterative optimization problem, where the first step is TD learning in the delay-free environment with a small state space, and the second step is behaviour cloning which can be addressed much more efficiently than TD learning. We not only provide a theoretical analysis of VDPO in terms of sample complexity and performance, but also empirically demonstrate that VDPO can achieve consistent performance with SOTA methods, with a significant enhancement of sample efficiency (approximately 50\\% less amount of samples) in the MuJoCo benchmark.","sentences":["In environments with delayed observation, state augmentation by including actions within the delay window is adopted to retrieve Markovian property to enable reinforcement learning (RL).","However, state-of-the-art (SOTA) RL techniques with Temporal-Difference (TD) learning frameworks often suffer from learning inefficiency, due to the significant expansion of the augmented state space with the delay.","To improve learning efficiency without sacrificing performance, this work introduces a novel framework called Variational Delayed Policy Optimization (VDPO), which reformulates delayed RL as a variational inference problem.","This problem is further modelled as a two-step iterative optimization problem, where the first step is TD learning in the delay-free environment with a small state space, and the second step is behaviour cloning which can be addressed much more efficiently than TD learning.","We not only provide a theoretical analysis of VDPO in terms of sample complexity and performance, but also empirically demonstrate that VDPO can achieve consistent performance with SOTA methods, with a significant enhancement of sample efficiency (approximately 50\\% less amount of samples) in the MuJoCo benchmark."],"url":"http://arxiv.org/abs/2405.14226v1","category":"cs.LG"}
{"created":"2024-05-23 06:28:44","title":"Understanding the Training and Generalization of Pretrained Transformer for Sequential Decision Making","abstract":"In this paper, we consider the supervised pretrained transformer for a class of sequential decision-making problems. The class of considered problems is a subset of the general formulation of reinforcement learning in that there is no transition probability matrix, and the class of problems covers bandits, dynamic pricing, and newsvendor problems as special cases. Such a structure enables the use of optimal actions/decisions in the pretraining phase, and the usage also provides new insights for the training and generalization of the pretrained transformer. We first note that the training of the transformer model can be viewed as a performative prediction problem, and the existing methods and theories largely ignore or cannot resolve the arisen out-of-distribution issue. We propose a natural solution that includes the transformer-generated action sequences in the training procedure, and it enjoys better properties both numerically and theoretically. The availability of the optimal actions in the considered tasks also allows us to analyze the properties of the pretrained transformer as an algorithm and explains why it may lack exploration and how this can be automatically resolved. Numerically, we categorize the advantages of the pretrained transformer over the structured algorithms such as UCB and Thompson sampling into three cases: (i) it better utilizes the prior knowledge in the pretraining data; (ii) it can elegantly handle the misspecification issue suffered by the structured algorithms; (iii) for short time horizon such as $T\\le50$, it behaves more greedy and enjoys much better regret than the structured algorithms which are designed for asymptotic optimality.","sentences":["In this paper, we consider the supervised pretrained transformer for a class of sequential decision-making problems.","The class of considered problems is a subset of the general formulation of reinforcement learning in that there is no transition probability matrix, and the class of problems covers bandits, dynamic pricing, and newsvendor problems as special cases.","Such a structure enables the use of optimal actions/decisions in the pretraining phase, and the usage also provides new insights for the training and generalization of the pretrained transformer.","We first note that the training of the transformer model can be viewed as a performative prediction problem, and the existing methods and theories largely ignore or cannot resolve the arisen out-of-distribution issue.","We propose a natural solution that includes the transformer-generated action sequences in the training procedure, and it enjoys better properties both numerically and theoretically.","The availability of the optimal actions in the considered tasks also allows us to analyze the properties of the pretrained transformer as an algorithm and explains why it may lack exploration and how this can be automatically resolved.","Numerically, we categorize the advantages of the pretrained transformer over the structured algorithms such as UCB and Thompson sampling into three cases: (i) it better utilizes the prior knowledge in the pretraining data; (ii) it can elegantly handle the misspecification issue suffered by the structured algorithms; (iii) for short time horizon such as $T\\le50$, it behaves more greedy and enjoys much better regret than the structured algorithms which are designed for asymptotic optimality."],"url":"http://arxiv.org/abs/2405.14219v1","category":"cs.LG"}
{"created":"2024-05-23 06:28:08","title":"Collective modes in Fulde-Ferrell-Larkin-Ovchinnikov superconductors: The role of long-range Coulomb interaction and signatures in density response","abstract":"We theoretically investigate collective excitations in the Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) states of Pauli-limited superconducting films. When the long-range Coulomb interaction is absent, excitation spectra consist of two gapless and three gapped modes. The gapless modes are the Nambu-Goldstone modes associated with the spontaneous breaking of the ${\\rm U}(1)$ symmetry and the translational symmetry. The gapped modes include the Higgs mode and the twofold degenerate modes that cause the oscillation of the domain width and grayness of FFLO nodal planes. We find that the long-range Coulomb interaction only gaps out the gapless phase mode through the Anderson-Higgs mechanism, while the other modes remain unaffected. Furthermore, the field evolution of the dispersion of the gapless elastic mode, the Nambu-Goldstone mode associated with the translational symmetry breaking, is associated with that of the bandwidth of the mid-gap Andreev bound states. We demonstrate that the signature of the elastic mode can be detected by measuring the density-density response function.","sentences":["We theoretically investigate collective excitations in the Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) states of Pauli-limited superconducting films.","When the long-range Coulomb interaction is absent, excitation spectra consist of two gapless and three gapped modes.","The gapless modes are the Nambu-Goldstone modes associated with the spontaneous breaking of the ${\\rm U}(1)$ symmetry and the translational symmetry.","The gapped modes include the Higgs mode and the twofold degenerate modes that cause the oscillation of the domain width and grayness of FFLO nodal planes.","We find that the long-range Coulomb interaction only gaps out the gapless phase mode through the Anderson-Higgs mechanism, while the other modes remain unaffected.","Furthermore, the field evolution of the dispersion of the gapless elastic mode, the Nambu-Goldstone mode associated with the translational symmetry breaking, is associated with that of the bandwidth of the mid-gap Andreev bound states.","We demonstrate that the signature of the elastic mode can be detected by measuring the density-density response function."],"url":"http://arxiv.org/abs/2405.14218v1","category":"cond-mat.supr-con"}
{"created":"2024-05-23 06:17:26","title":"A Behavior-Aware Approach for Deep Reinforcement Learning in Non-stationary Environments without Known Change Points","abstract":"Deep reinforcement learning is used in various domains, but usually under the assumption that the environment has stationary conditions like transitions and state distributions. When this assumption is not met, performance suffers. For this reason, tracking continuous environmental changes and adapting to unpredictable conditions is challenging yet crucial because it ensures that systems remain reliable and flexible in practical scenarios. Our research introduces Behavior-Aware Detection and Adaptation (BADA), an innovative framework that merges environmental change detection with behavior adaptation. The key inspiration behind our method is that policies exhibit different global behaviors in changing environments. Specifically, environmental changes are identified by analyzing variations between behaviors using Wasserstein distances without manually set thresholds. The model adapts to the new environment through behavior regularization based on the extent of changes. The results of a series of experiments demonstrate better performance relative to several current algorithms. This research also indicates significant potential for tackling this long-standing challenge.","sentences":["Deep reinforcement learning is used in various domains, but usually under the assumption that the environment has stationary conditions like transitions and state distributions.","When this assumption is not met, performance suffers.","For this reason, tracking continuous environmental changes and adapting to unpredictable conditions is challenging yet crucial because it ensures that systems remain reliable and flexible in practical scenarios.","Our research introduces Behavior-Aware Detection and Adaptation (BADA), an innovative framework that merges environmental change detection with behavior adaptation.","The key inspiration behind our method is that policies exhibit different global behaviors in changing environments.","Specifically, environmental changes are identified by analyzing variations between behaviors using Wasserstein distances without manually set thresholds.","The model adapts to the new environment through behavior regularization based on the extent of changes.","The results of a series of experiments demonstrate better performance relative to several current algorithms.","This research also indicates significant potential for tackling this long-standing challenge."],"url":"http://arxiv.org/abs/2405.14214v1","category":"cs.LG"}
{"created":"2024-05-23 06:03:19","title":"Agent Planning with World Knowledge Model","abstract":"Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ''real'' physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. Code will be available at https://github.com/zjunlp/WKM.","sentences":["Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results.","Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ''real'' physical world.","Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning.","Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories.","Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning.","Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines.","Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world.","Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development.","Code will be available at https://github.com/zjunlp/WKM."],"url":"http://arxiv.org/abs/2405.14205v1","category":"cs.CL"}
{"created":"2024-05-23 06:02:07","title":"GLaD: Synergizing Molecular Graphs and Language Descriptors for Enhanced Power Conversion Efficiency Prediction in Organic Photovoltaic Devices","abstract":"This paper presents a novel approach for predicting Power Conversion Efficiency (PCE) of Organic Photovoltaic (OPV) devices, called GLaD: synergizing molecular Graphs and Language Descriptors for enhanced PCE prediction. Due to the lack of high-quality experimental data, we collect a dataset consisting of 500 pairs of OPV donor and acceptor molecules along with their corresponding PCE values, which we utilize as the training data for our predictive model. In this low-data regime, GLaD leverages properties learned from large language models (LLMs) pretrained on extensive scientific literature to enrich molecular structural representations, allowing for a multimodal representation of molecules. GLaD achieves precise predictions of PCE, thereby facilitating the synthesis of new OPV molecules with improved efficiency. Furthermore, GLaD showcases versatility, as it applies to a range of molecular property prediction tasks (BBBP, BACE, ClinTox, and SIDER), not limited to those concerning OPV materials. Especially, GLaD proves valuable for tasks in low-data regimes within the chemical space, as it enriches molecular representations by incorporating molecular property descriptions learned from large-scale pretraining. This capability is significant in real-world scientific endeavors like drug and material discovery, where access to comprehensive data is crucial for informed decision-making and efficient exploration of the chemical space.","sentences":["This paper presents a novel approach for predicting Power Conversion Efficiency (PCE) of Organic Photovoltaic (OPV) devices, called GLaD: synergizing molecular Graphs and Language Descriptors for enhanced PCE prediction.","Due to the lack of high-quality experimental data, we collect a dataset consisting of 500 pairs of OPV donor and acceptor molecules along with their corresponding PCE values, which we utilize as the training data for our predictive model.","In this low-data regime, GLaD leverages properties learned from large language models (LLMs) pretrained on extensive scientific literature to enrich molecular structural representations, allowing for a multimodal representation of molecules.","GLaD achieves precise predictions of PCE, thereby facilitating the synthesis of new OPV molecules with improved efficiency.","Furthermore, GLaD showcases versatility, as it applies to a range of molecular property prediction tasks (BBBP, BACE, ClinTox, and SIDER), not limited to those concerning OPV materials.","Especially, GLaD proves valuable for tasks in low-data regimes within the chemical space, as it enriches molecular representations by incorporating molecular property descriptions learned from large-scale pretraining.","This capability is significant in real-world scientific endeavors like drug and material discovery, where access to comprehensive data is crucial for informed decision-making and efficient exploration of the chemical space."],"url":"http://arxiv.org/abs/2405.14203v1","category":"cs.LG"}
{"created":"2024-05-23 05:58:10","title":"Awesome Multi-modal Object Tracking","abstract":"Multi-modal object tracking (MMOT) is an emerging field that combines data from various modalities, \\eg vision (RGB), depth, thermal infrared, event, language and audio, to estimate the state of an arbitrary object in a video sequence. It is of great significance for many applications such as autonomous driving and intelligent surveillance. In recent years, MMOT has received more and more attention. However, existing MMOT algorithms mainly focus on two modalities (\\eg RGB+depth, RGB+thermal infrared, and RGB+language). To leverage more modalities, some recent efforts have been made to learn a unified visual object tracking model for any modality. Additionally, some large-scale multi-modal tracking benchmarks have been established by simultaneously providing more than two modalities, such as vision-language-audio (\\eg WebUAV-3M) and vision-depth-language (\\eg UniMod1K). To track the latest progress in MMOT, we conduct a comprehensive investigation in this report. Specifically, we first divide existing MMOT tasks into five main categories, \\ie RGBL tracking, RGBE tracking, RGBD tracking, RGBT tracking, and miscellaneous (RGB+X), where X can be any modality, such as language, depth, and event. Then, we analyze and summarize each MMOT task, focusing on widely used datasets and mainstream tracking algorithms based on their technical paradigms (\\eg self-supervised learning, prompt learning, knowledge distillation, generative models, and state space models). Finally, we maintain a continuously updated paper list for MMOT at https://github.com/983632847/Awesome-Multimodal-Object-Tracking.","sentences":["Multi-modal object tracking (MMOT) is an emerging field that combines data from various modalities, \\eg vision (RGB), depth, thermal infrared, event, language and audio, to estimate the state of an arbitrary object in a video sequence.","It is of great significance for many applications such as autonomous driving and intelligent surveillance.","In recent years, MMOT has received more and more attention.","However, existing MMOT algorithms mainly focus on two modalities (\\eg RGB+depth, RGB+thermal infrared, and RGB+language).","To leverage more modalities, some recent efforts have been made to learn a unified visual object tracking model for any modality.","Additionally, some large-scale multi-modal tracking benchmarks have been established by simultaneously providing more than two modalities, such as vision-language-audio (\\eg WebUAV-3M) and vision-depth-language (\\eg UniMod1K).","To track the latest progress in MMOT, we conduct a comprehensive investigation in this report.","Specifically, we first divide existing MMOT tasks into five main categories, \\ie RGBL tracking, RGBE tracking, RGBD tracking, RGBT tracking, and miscellaneous (RGB+X), where X can be any modality, such as language, depth, and event.","Then, we analyze and summarize each MMOT task, focusing on widely used datasets and mainstream tracking algorithms based on their technical paradigms (\\eg self-supervised learning, prompt learning, knowledge distillation, generative models, and state space models).","Finally, we maintain a continuously updated paper list for MMOT at https://github.com/983632847/Awesome-Multimodal-Object-Tracking."],"url":"http://arxiv.org/abs/2405.14200v1","category":"cs.CV"}
{"created":"2024-05-23 05:52:38","title":"Enabling Sustainable Freight Forwarding Network via Collaborative Games","abstract":"Freight forwarding plays a crucial role in facilitating global trade and logistics. However, as the freight forwarding market is extremely fragmented, freight forwarders often face the issue of not being able to fill the available shipping capacity. This recurrent issue motivates the creation of various freight forwarding networks that aim at exchanging capacities and demands so that the resource utilization of individual freight forwarders can be maximized. In this paper, we focus on how to design such a collaborative network based on collaborative game theory, with the Shapley value representing a fair scheme for profit sharing. Noting that the exact computation of Shapley values is intractable for large-scale real-world scenarios, we incorporate the observation that collaboration among two forwarders is only possible if their service routes and demands overlap. This leads to a new class of collaborative games called the Locally Collaborative Games (LCGs), where agents can only collaborate with their neighbors. We propose an efficient approach to compute Shapley values for LCGs, and numerically demonstrate that our approach significantly outperforms the state-of-the-art approach for a wide variety of network structures.","sentences":["Freight forwarding plays a crucial role in facilitating global trade and logistics.","However, as the freight forwarding market is extremely fragmented, freight forwarders often face the issue of not being able to fill the available shipping capacity.","This recurrent issue motivates the creation of various freight forwarding networks that aim at exchanging capacities and demands so that the resource utilization of individual freight forwarders can be maximized.","In this paper, we focus on how to design such a collaborative network based on collaborative game theory, with the Shapley value representing a fair scheme for profit sharing.","Noting that the exact computation of Shapley values is intractable for large-scale real-world scenarios, we incorporate the observation that collaboration among two forwarders is only possible if their service routes and demands overlap.","This leads to a new class of collaborative games called the Locally Collaborative Games (LCGs), where agents can only collaborate with their neighbors.","We propose an efficient approach to compute Shapley values for LCGs, and numerically demonstrate that our approach significantly outperforms the state-of-the-art approach for a wide variety of network structures."],"url":"http://arxiv.org/abs/2405.14198v1","category":"cs.MA"}
{"created":"2024-05-23 05:43:38","title":"Enhanced Object Tracking by Self-Supervised Auxiliary Depth Estimation Learning","abstract":"RGB-D tracking significantly improves the accuracy of object tracking. However, its dependency on real depth inputs and the complexity involved in multi-modal fusion limit its applicability across various scenarios. The utilization of depth information in RGB-D tracking inspired us to propose a new method, named MDETrack, which trains a tracking network with an additional capability to understand the depth of scenes, through supervised or self-supervised auxiliary Monocular Depth Estimation learning. The outputs of MDETrack's unified feature extractor are fed to the side-by-side tracking head and auxiliary depth estimation head, respectively. The auxiliary module will be discarded in inference, thus keeping the same inference speed. We evaluated our models with various training strategies on multiple datasets, and the results show an improved tracking accuracy even without real depth. Through these findings we highlight the potential of depth estimation in enhancing object tracking performance.","sentences":["RGB-D tracking significantly improves the accuracy of object tracking.","However, its dependency on real depth inputs and the complexity involved in multi-modal fusion limit its applicability across various scenarios.","The utilization of depth information in RGB-D tracking inspired us to propose a new method, named MDETrack, which trains a tracking network with an additional capability to understand the depth of scenes, through supervised or self-supervised auxiliary Monocular Depth Estimation learning.","The outputs of MDETrack's unified feature extractor are fed to the side-by-side tracking head and auxiliary depth estimation head, respectively.","The auxiliary module will be discarded in inference, thus keeping the same inference speed.","We evaluated our models with various training strategies on multiple datasets, and the results show an improved tracking accuracy even without real depth.","Through these findings we highlight the potential of depth estimation in enhancing object tracking performance."],"url":"http://arxiv.org/abs/2405.14195v1","category":"cs.CV"}
{"created":"2024-05-23 05:42:38","title":"Graphlets correct for the topological information missed by random walks","abstract":"Random walks are widely used for mining networks due to the computational efficiency of computing them. For instance, graph representation learning learns a d-dimensional embedding space, so that the nodes that tend to co-occur on random walks (a proxy of being in the same network neighborhood) are close in the embedding space. Specific local network topology (i.e., structure) influences the co-occurrence of nodes on random walks, so random walks of limited length capture only partial topological information, hence diminishing the performance of downstream methods. We explicitly capture all topological neighborhood information and improve performance by introducing orbit adjacencies that quantify the adjacencies of two nodes as co-occurring on a given pair of graphlet orbits, which are symmetric positions on graphlets (small, connected, non-isomorphic, induced subgraphs of a large network). Importantly, we mathematically prove that random walks on up to k nodes capture only a subset of all the possible orbit adjacencies for up to k-node graphlets. Furthermore, we enable orbit adjacency-based analysis of networks by developing an efficient GRaphlet-orbit ADjacency COunter (GRADCO), which exhaustively computes all 28 orbit adjacency matrices for up to four-node graphlets. Note that four-node graphlets suffice, because real networks are usually small-world. In large networks on around 20,000 nodes, GRADCOcomputesthe28matricesinminutes. Onsixrealnetworksfromvarious domains, we compare the performance of node-label predictors obtained by using the network embeddings based on our orbit adjacencies to those based on random walks. We find that orbit adjacencies, which include those unseen by random walks, outperform random walk-based adjacencies, demonstrating the importance of the inclusion of the topological neighborhood information that is unseen by random walks.","sentences":["Random walks are widely used for mining networks due to the computational efficiency of computing them.","For instance, graph representation learning learns a d-dimensional embedding space, so that the nodes that tend to co-occur on random walks (a proxy of being in the same network neighborhood) are close in the embedding space.","Specific local network topology (i.e., structure) influences the co-occurrence of nodes on random walks, so random walks of limited length capture only partial topological information, hence diminishing the performance of downstream methods.","We explicitly capture all topological neighborhood information and improve performance by introducing orbit adjacencies that quantify the adjacencies of two nodes as co-occurring on a given pair of graphlet orbits, which are symmetric positions on graphlets (small, connected, non-isomorphic, induced subgraphs of a large network).","Importantly, we mathematically prove that random walks on up to k nodes capture only a subset of all the possible orbit adjacencies for up to k-node graphlets.","Furthermore, we enable orbit adjacency-based analysis of networks by developing an efficient GRaphlet-orbit ADjacency COunter (GRADCO), which exhaustively computes all 28 orbit adjacency matrices for up to four-node graphlets.","Note that four-node graphlets suffice, because real networks are usually small-world.","In large networks on around 20,000 nodes, GRADCOcomputesthe28matricesinminutes.","Onsixrealnetworksfromvarious domains, we compare the performance of node-label predictors obtained by using the network embeddings based on our orbit adjacencies to those based on random walks.","We find that orbit adjacencies, which include those unseen by random walks, outperform random walk-based adjacencies, demonstrating the importance of the inclusion of the topological neighborhood information that is unseen by random walks."],"url":"http://arxiv.org/abs/2405.14194v1","category":"cs.SI"}
{"created":"2024-05-23 05:03:49","title":"Desirable Characteristics for AI Teaching Assistants in Programming Education","abstract":"Providing timely and personalized feedback to large numbers of students is a long-standing challenge in programming courses. Relying on human teaching assistants (TAs) has been extensively studied, revealing a number of potential shortcomings. These include inequitable access for students with low confidence when needing support, as well as situations where TAs provide direct solutions without helping students to develop their own problem-solving skills. With the advent of powerful large language models (LLMs), digital teaching assistants configured for programming contexts have emerged as an appealing and scalable way to provide instant, equitable, round-the-clock support. Although digital TAs can provide a variety of help for programming tasks, from high-level problem solving advice to direct solution generation, the effectiveness of such tools depends on their ability to promote meaningful learning experiences. If students find the guardrails implemented in digital TAs too constraining, or if other expectations are not met, they may seek assistance in ways that do not help them learn. Thus, it is essential to identify the features that students believe make digital teaching assistants valuable. We deployed an LLM-powered digital assistant in an introductory programming course and collected student feedback ($n=813$) on the characteristics of the tool they perceived to be most important. Our results highlight that students value such tools for their ability to provide instant, engaging support, particularly during peak times such as before assessment deadlines. They also expressed a strong preference for features that enable them to retain autonomy in their learning journey, such as scaffolding that helps to guide them through problem-solving steps rather than simply being shown direct solutions.","sentences":["Providing timely and personalized feedback to large numbers of students is a long-standing challenge in programming courses.","Relying on human teaching assistants (TAs) has been extensively studied, revealing a number of potential shortcomings.","These include inequitable access for students with low confidence when needing support, as well as situations where TAs provide direct solutions without helping students to develop their own problem-solving skills.","With the advent of powerful large language models (LLMs), digital teaching assistants configured for programming contexts have emerged as an appealing and scalable way to provide instant, equitable, round-the-clock support.","Although digital TAs can provide a variety of help for programming tasks, from high-level problem solving advice to direct solution generation, the effectiveness of such tools depends on their ability to promote meaningful learning experiences.","If students find the guardrails implemented in digital TAs too constraining, or if other expectations are not met, they may seek assistance in ways that do not help them learn.","Thus, it is essential to identify the features that students believe make digital teaching assistants valuable.","We deployed an LLM-powered digital assistant in an introductory programming course and collected student feedback ($n=813$) on the characteristics of the tool they perceived to be most important.","Our results highlight that students value such tools for their ability to provide instant, engaging support, particularly during peak times such as before assessment deadlines.","They also expressed a strong preference for features that enable them to retain autonomy in their learning journey, such as scaffolding that helps to guide them through problem-solving steps rather than simply being shown direct solutions."],"url":"http://arxiv.org/abs/2405.14178v1","category":"cs.CY"}
{"created":"2024-05-23 05:02:00","title":"Certified Robustness against Sparse Adversarial Perturbations via Data Localization","abstract":"Recent work in adversarial robustness suggests that natural data distributions are localized, i.e., they place high probability in small volume regions of the input space, and that this property can be utilized for designing classifiers with improved robustness guarantees for $\\ell_2$-bounded perturbations. Yet, it is still unclear if this observation holds true for more general metrics. In this work, we extend this theory to $\\ell_0$-bounded adversarial perturbations, where the attacker can modify a few pixels of the image but is unrestricted in the magnitude of perturbation, and we show necessary and sufficient conditions for the existence of $\\ell_0$-robust classifiers. Theoretical certification approaches in this regime essentially employ voting over a large ensemble of classifiers. Such procedures are combinatorial and expensive or require complicated certification techniques. In contrast, a simple classifier emerges from our theory, dubbed Box-NN, which naturally incorporates the geometry of the problem and improves upon the current state-of-the-art in certified robustness against sparse attacks for the MNIST and Fashion-MNIST datasets.","sentences":["Recent work in adversarial robustness suggests that natural data distributions are localized, i.e., they place high probability in small volume regions of the input space, and that this property can be utilized for designing classifiers with improved robustness guarantees for $\\ell_2$-bounded perturbations.","Yet, it is still unclear if this observation holds true for more general metrics.","In this work, we extend this theory to $\\ell_0$-bounded adversarial perturbations, where the attacker can modify a few pixels of the image but is unrestricted in the magnitude of perturbation, and we show necessary and sufficient conditions for the existence of $\\ell_0$-robust classifiers.","Theoretical certification approaches in this regime essentially employ voting over a large ensemble of classifiers.","Such procedures are combinatorial and expensive or require complicated certification techniques.","In contrast, a simple classifier emerges from our theory, dubbed Box-NN, which naturally incorporates the geometry of the problem and improves upon the current state-of-the-art in certified robustness against sparse attacks for the MNIST and Fashion-MNIST datasets."],"url":"http://arxiv.org/abs/2405.14176v1","category":"cs.LG"}
{"created":"2024-05-23 04:58:42","title":"Human-Agent Cooperation in Games under Incomplete Information through Natural Language Communication","abstract":"Developing autonomous agents that can strategize and cooperate with humans under information asymmetry is challenging without effective communication in natural language. We introduce a shared-control game, where two players collectively control a token in alternating turns to achieve a common objective under incomplete information. We formulate a policy synthesis problem for an autonomous agent in this game with a human as the other player. To solve this problem, we propose a communication-based approach comprising a language module and a planning module. The language module translates natural language messages into and from a finite set of flags, a compact representation defined to capture player intents. The planning module leverages these flags to compute a policy using an asymmetric information-set Monte Carlo tree search with flag exchange algorithm we present. We evaluate the effectiveness of this approach in a testbed based on Gnomes at Night, a search-and-find maze board game. Results of human subject experiments show that communication narrows the information gap between players and enhances human-agent cooperation efficiency with fewer turns.","sentences":["Developing autonomous agents that can strategize and cooperate with humans under information asymmetry is challenging without effective communication in natural language.","We introduce a shared-control game, where two players collectively control a token in alternating turns to achieve a common objective under incomplete information.","We formulate a policy synthesis problem for an autonomous agent in this game with a human as the other player.","To solve this problem, we propose a communication-based approach comprising a language module and a planning module.","The language module translates natural language messages into and from a finite set of flags, a compact representation defined to capture player intents.","The planning module leverages these flags to compute a policy using an asymmetric information-set Monte Carlo tree search with flag exchange algorithm we present.","We evaluate the effectiveness of this approach in a testbed based on Gnomes at Night, a search-and-find maze board game.","Results of human subject experiments show that communication narrows the information gap between players and enhances human-agent cooperation efficiency with fewer turns."],"url":"http://arxiv.org/abs/2405.14173v1","category":"cs.AI"}
{"created":"2024-05-23 04:54:37","title":"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning","abstract":"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.","sentences":["Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge.","Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules.","However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns.","Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning.","Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers.","Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process.","Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical.","To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs.","Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules.","These rules unveil temporal patterns and facilitate interpretable reasoning.","To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events.","This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events.","Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks."],"url":"http://arxiv.org/abs/2405.14170v1","category":"cs.AI"}
{"created":"2024-05-23 04:28:50","title":"Leveraging Semantic Segmentation Masks with Embeddings for Fine-Grained Form Classification","abstract":"Efficient categorization of historical documents is crucial for fields such as genealogy, legal research, and historical scholarship, where manual classification is impractical for large collections due to its labor-intensive and error-prone nature. To address this, we propose a representational learning strategy that integrates semantic segmentation and deep learning models -- ResNets, CLIP, the Document Image Transformer (DiT), and masked auto-encoders (MAE) -- to generate embeddings that capture document features without predefined labels. To the best of our knowledge, we are the first to evaluate embeddings on fine-grained, unsupervised form classification. To improve these embeddings, we propose to first employ semantic segmentation as a preprocessing step. We contribute two novel datasets -- French 19th-century and U.S. 1950 Census records -- to demonstrate our approach. Our results show the effectiveness of these various embedding techniques in distinguishing similar document types and indicate that applying semantic segmentation can greatly improve clustering and classification results. The census datasets are available at \\href{https://github.com/tahlor/census_forms}{https://github.com/tahlor/census\\_forms}.","sentences":["Efficient categorization of historical documents is crucial for fields such as genealogy, legal research, and historical scholarship, where manual classification is impractical for large collections due to its labor-intensive and error-prone nature.","To address this, we propose a representational learning strategy that integrates semantic segmentation and deep learning models -- ResNets, CLIP, the Document Image Transformer (DiT), and masked auto-encoders (MAE) -- to generate embeddings that capture document features without predefined labels.","To the best of our knowledge, we are the first to evaluate embeddings on fine-grained, unsupervised form classification.","To improve these embeddings, we propose to first employ semantic segmentation as a preprocessing step.","We contribute two novel datasets -- French 19th-century and U.S. 1950 Census records -- to demonstrate our approach.","Our results show the effectiveness of these various embedding techniques in distinguishing similar document types and indicate that applying semantic segmentation can greatly improve clustering and classification results.","The census datasets are available at \\href{https://github.com/tahlor/census_forms}{https://github.com/tahlor/census\\_forms}."],"url":"http://arxiv.org/abs/2405.14162v1","category":"cs.CV"}
{"created":"2024-05-23 04:27:11","title":"Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models","abstract":"We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic speech recognition (ASR) systems in diverse target domains, such as noise and accents. STAR is developed for prevalent speech foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary). Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels without ground truth, thereby guiding model updates for effective unsupervised adaptation. Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation. Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data. Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large speech models and speech translation tasks. Our code aims to open source to the research communities.","sentences":["We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic speech recognition (ASR) systems in diverse target domains, such as noise and accents.","STAR is developed for prevalent speech foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary).","Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels without ground truth, thereby guiding model updates for effective unsupervised adaptation.","Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation.","Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data.","Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large speech models and speech translation tasks.","Our code aims to open source to the research communities."],"url":"http://arxiv.org/abs/2405.14161v1","category":"cs.CL"}
{"created":"2024-05-23 04:12:49","title":"Super Tiny Language Models","abstract":"The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research efforts focused on Super Tiny Language Models (STLMs), which aim to deliver high performance with significantly reduced parameter counts. We explore innovative techniques such as byte-level tokenization with a pooling mechanism, weight tying, and efficient training strategies. These methods collectively reduce the parameter count by $90\\%$ to $95\\%$ compared to traditional models while maintaining competitive performance. This series of papers will explore into various subproblems, including tokenizer-free models, self-play based training, and alternative training objectives, targeting models with 10M, 50M, and 100M parameters. Our ultimate goal is to make high-performance language models more accessible and practical for a wide range of applications.","sentences":["The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands.","This paper introduces a series of research efforts focused on Super Tiny Language Models (STLMs), which aim to deliver high performance with significantly reduced parameter counts.","We explore innovative techniques such as byte-level tokenization with a pooling mechanism, weight tying, and efficient training strategies.","These methods collectively reduce the parameter count by $90\\%$ to $95\\%$ compared to traditional models while maintaining competitive performance.","This series of papers will explore into various subproblems, including tokenizer-free models, self-play based training, and alternative training objectives, targeting models with 10M, 50M, and 100M parameters.","Our ultimate goal is to make high-performance language models more accessible and practical for a wide range of applications."],"url":"http://arxiv.org/abs/2405.14159v1","category":"cs.CL"}
{"created":"2024-05-23 03:48:26","title":"Real Time Deep Learning Weapon Detection Techniques for Mitigating Lone Wolf Attacks","abstract":"Firearm Shootings and stabbings attacks are intense and result in severe trauma and threat to public safety. Technology is needed to prevent lone-wolf attacks without human supervision. Hence designing an automatic weapon detection using deep learning, is an optimized solution to localize and detect the presence of weapon objects using Neural Networks. This research focuses on both unified and II-stage object detectors whose resultant model not only detects the presence of weapons but also classifies with respective to its weapon classes, including handgun, knife, revolver, and rifle, along with person detection. This research focuses on (You Look Only Once) family and Faster RCNN family for model validation and training. Pruning and Ensembling techniques were applied to YOLOv5 to enhance their speed and performance. models achieve the highest score of 78% with an inference speed of 8.1ms. However, Faster R-CNN models achieve the highest AP 89%.","sentences":["Firearm Shootings and stabbings attacks are intense and result in severe trauma and threat to public safety.","Technology is needed to prevent lone-wolf attacks without human supervision.","Hence designing an automatic weapon detection using deep learning, is an optimized solution to localize and detect the presence of weapon objects using Neural Networks.","This research focuses on both unified and II-stage object detectors whose resultant model not only detects the presence of weapons but also classifies with respective to its weapon classes, including handgun, knife, revolver, and rifle, along with person detection.","This research focuses on (You Look Only Once) family and Faster RCNN family for model validation and training.","Pruning and Ensembling techniques were applied to YOLOv5 to enhance their speed and performance.","models achieve the highest score of 78% with an inference speed of 8.1ms.","However, Faster R-CNN models achieve the highest AP 89%."],"url":"http://arxiv.org/abs/2405.14148v1","category":"cs.CV"}
{"created":"2024-05-23 03:46:07","title":"Minimum number of neurons in fully connected layers of a given neural network (the first approximation)","abstract":"This paper presents an algorithm for searching for the minimum number of neurons in fully connected layers of an arbitrary network solving given problem, which does not require multiple training of the network with different number of neurons. The algorithm is based at training the initial wide network using the cross-validation method over at least two folds. Then by using truncated singular value decomposition autoencoder inserted after the studied layer of trained network we search the minimum number of neurons in inference only mode of the network.   It is shown that the minimum number of neurons in a fully connected layer could be interpreted not as network hyperparameter associated with the other hyperparameters of the network, but as internal (latent) property of the solution, determined by the network architecture, the training dataset, layer position, and the quality metric used. So the minimum number of neurons can be estimated for each hidden fully connected layer independently. The proposed algorithm is the first approximation for estimating the minimum number of neurons in the layer, since, on the one hand, the algorithm does not guarantee that a neural network with the found number of neurons can be trained to the required quality, and on the other hand, it searches for the minimum number of neurons in a limited class of possible solutions.   The solution was tested on several datasets in classification and regression problems.","sentences":["This paper presents an algorithm for searching for the minimum number of neurons in fully connected layers of an arbitrary network solving given problem, which does not require multiple training of the network with different number of neurons.","The algorithm is based at training the initial wide network using the cross-validation method over at least two folds.","Then by using truncated singular value decomposition autoencoder inserted after the studied layer of trained network we search the minimum number of neurons in inference only mode of the network.   ","It is shown that the minimum number of neurons in a fully connected layer could be interpreted not as network hyperparameter associated with the other hyperparameters of the network, but as internal (latent) property of the solution, determined by the network architecture, the training dataset, layer position, and the quality metric used.","So the minimum number of neurons can be estimated for each hidden fully connected layer independently.","The proposed algorithm is the first approximation for estimating the minimum number of neurons in the layer, since, on the one hand, the algorithm does not guarantee that a neural network with the found number of neurons can be trained to the required quality, and on the other hand, it searches for the minimum number of neurons in a limited class of possible solutions.   ","The solution was tested on several datasets in classification and regression problems."],"url":"http://arxiv.org/abs/2405.14147v1","category":"cs.LG"}
{"created":"2024-05-23 03:36:31","title":"Imagery as Inquiry: Exploring A Multimodal Dataset for Conversational Recommendation","abstract":"We introduce a multimodal dataset where users express preferences through images. These images encompass a broad spectrum of visual expressions ranging from landscapes to artistic depictions. Users request recommendations for books or music that evoke similar feelings to those captured in the images, and recommendations are endorsed by the community through upvotes. This dataset supports two recommendation tasks: title generation and multiple-choice selection. Our experiments with large foundation models reveal their limitations in these tasks. Particularly, vision-language models show no significant advantage over language-only counterparts that use descriptions, which we hypothesize is due to underutilized visual capabilities. To better harness these abilities, we propose the chain-of-imagery prompting, which results in notable improvements. We release our code and datasets.","sentences":["We introduce a multimodal dataset where users express preferences through images.","These images encompass a broad spectrum of visual expressions ranging from landscapes to artistic depictions.","Users request recommendations for books or music that evoke similar feelings to those captured in the images, and recommendations are endorsed by the community through upvotes.","This dataset supports two recommendation tasks: title generation and multiple-choice selection.","Our experiments with large foundation models reveal their limitations in these tasks.","Particularly, vision-language models show no significant advantage over language-only counterparts that use descriptions, which we hypothesize is due to underutilized visual capabilities.","To better harness these abilities, we propose the chain-of-imagery prompting, which results in notable improvements.","We release our code and datasets."],"url":"http://arxiv.org/abs/2405.14142v1","category":"cs.CV"}
{"created":"2024-05-23 03:19:02","title":"Learning Geospatial Region Embedding with Heterogeneous Graph","abstract":"Learning effective geospatial embeddings is crucial for a series of geospatial applications such as city analytics and earth monitoring. However, learning comprehensive region representations presents two significant challenges: first, the deficiency of effective intra-region feature representation; and second, the difficulty of learning from intricate inter-region dependencies. In this paper, we present GeoHG, an effective heterogeneous graph structure for learning comprehensive region embeddings for various downstream tasks. Specifically, we tailor satellite image representation learning through geo-entity segmentation and point-of-interest (POI) integration for expressive intra-regional features. Furthermore, GeoHG unifies informative spatial interdependencies and socio-environmental attributes into a powerful heterogeneous graph to encourage explicit modeling of higher-order inter-regional relationships. The intra-regional features and inter-regional correlations are seamlessly integrated by a model-agnostic graph learning framework for diverse downstream tasks. Extensive experiments demonstrate the effectiveness of GeoHG in geo-prediction tasks compared to existing methods, even under extreme data scarcity (with just 5% of training data). With interpretable region representations, GeoHG exhibits strong generalization capabilities across regions. We will release code and data upon paper notification.","sentences":["Learning effective geospatial embeddings is crucial for a series of geospatial applications such as city analytics and earth monitoring.","However, learning comprehensive region representations presents two significant challenges: first, the deficiency of effective intra-region feature representation; and second, the difficulty of learning from intricate inter-region dependencies.","In this paper, we present GeoHG, an effective heterogeneous graph structure for learning comprehensive region embeddings for various downstream tasks.","Specifically, we tailor satellite image representation learning through geo-entity segmentation and point-of-interest (POI) integration for expressive intra-regional features.","Furthermore, GeoHG unifies informative spatial interdependencies and socio-environmental attributes into a powerful heterogeneous graph to encourage explicit modeling of higher-order inter-regional relationships.","The intra-regional features and inter-regional correlations are seamlessly integrated by a model-agnostic graph learning framework for diverse downstream tasks.","Extensive experiments demonstrate the effectiveness of GeoHG in geo-prediction tasks compared to existing methods, even under extreme data scarcity (with just 5% of training data).","With interpretable region representations, GeoHG exhibits strong generalization capabilities across regions.","We will release code and data upon paper notification."],"url":"http://arxiv.org/abs/2405.14135v1","category":"cs.LG"}
{"created":"2024-05-23 03:12:49","title":"Automated Loss function Search for Class-imbalanced Node Classification","abstract":"Class-imbalanced node classification tasks are prevalent in real-world scenarios. Due to the uneven distribution of nodes across different classes, learning high-quality node representations remains a challenging endeavor. The engineering of loss functions has shown promising potential in addressing this issue. It involves the meticulous design of loss functions, utilizing information about the quantities of nodes in different categories and the network's topology to learn unbiased node representations. However, the design of these loss functions heavily relies on human expert knowledge and exhibits limited adaptability to specific target tasks. In this paper, we introduce a high-performance, flexible, and generalizable automated loss function search framework to tackle this challenge. Across 15 combinations of graph neural networks and datasets, our framework achieves a significant improvement in performance compared to state-of-the-art methods. Additionally, we observe that homophily in graph-structured data significantly contributes to the transferability of the proposed framework.","sentences":["Class-imbalanced node classification tasks are prevalent in real-world scenarios.","Due to the uneven distribution of nodes across different classes, learning high-quality node representations remains a challenging endeavor.","The engineering of loss functions has shown promising potential in addressing this issue.","It involves the meticulous design of loss functions, utilizing information about the quantities of nodes in different categories and the network's topology to learn unbiased node representations.","However, the design of these loss functions heavily relies on human expert knowledge and exhibits limited adaptability to specific target tasks.","In this paper, we introduce a high-performance, flexible, and generalizable automated loss function search framework to tackle this challenge.","Across 15 combinations of graph neural networks and datasets, our framework achieves a significant improvement in performance compared to state-of-the-art methods.","Additionally, we observe that homophily in graph-structured data significantly contributes to the transferability of the proposed framework."],"url":"http://arxiv.org/abs/2405.14133v1","category":"cs.LG"}
{"created":"2024-05-23 03:11:18","title":"Text-to-Model: Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization","abstract":"Generative artificial intelligence (GenAI) has made significant progress in understanding world knowledge and generating content from human languages across various modalities, like text-to-text large language models, text-to-image stable diffusion, and text-to-video Sora. While in this paper, we investigate the capability of GenAI for text-to-model generation, to see whether GenAI can comprehend hyper-level knowledge embedded within AI itself parameters. Specifically, we study a practical scenario termed train-once-for-all personalization, aiming to generate personalized models for diverse end-users and tasks using text prompts. Inspired by the recent emergence of neural network diffusion, we present Tina, a text-conditioned neural network diffusion for train-once-for-all personalization. Tina leverages a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. Despite the astronomical number of potential personalized tasks (e.g., $1.73\\times10^{13}$), by our design, Tina demonstrates remarkable in-distribution and out-of-distribution generalization even trained on small datasets ($\\sim 1000$). We further verify whether and how \\Tina understands world knowledge by analyzing its capabilities under zero-shot/few-shot image prompts, different numbers of personalized classes, prompts of natural language descriptions, and predicting unseen entities.","sentences":["Generative artificial intelligence (GenAI) has made significant progress in understanding world knowledge and generating content from human languages across various modalities, like text-to-text large language models, text-to-image stable diffusion, and text-to-video Sora.","While in this paper, we investigate the capability of GenAI for text-to-model generation, to see whether GenAI can comprehend hyper-level knowledge embedded within AI itself parameters.","Specifically, we study a practical scenario termed train-once-for-all personalization, aiming to generate personalized models for diverse end-users and tasks using text prompts.","Inspired by the recent emergence of neural network diffusion, we present Tina, a text-conditioned neural network diffusion for train-once-for-all personalization.","Tina leverages a diffusion transformer model conditioned on task descriptions embedded using a CLIP model.","Despite the astronomical number of potential personalized tasks (e.g., $1.73\\times10^{13}$), by our design, Tina demonstrates remarkable in-distribution and out-of-distribution generalization even trained on small datasets ($\\sim 1000$).","We further verify whether and how \\Tina understands world knowledge by analyzing its capabilities under zero-shot/few-shot image prompts, different numbers of personalized classes, prompts of natural language descriptions, and predicting unseen entities."],"url":"http://arxiv.org/abs/2405.14132v1","category":"cs.LG"}
{"created":"2024-05-23 03:07:56","title":"AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability","abstract":"Multimodal Large Language Models (MLLMs) are widely regarded as crucial in the exploration of Artificial General Intelligence (AGI). The core of MLLMs lies in their capability to achieve cross-modal alignment. To attain this goal, current MLLMs typically follow a two-phase training paradigm: the pre-training phase and the instruction-tuning phase. Despite their success, there are shortcomings in the modeling of alignment capabilities within these models. Firstly, during the pre-training phase, the model usually assumes that all image-text pairs are uniformly aligned, but in fact the degree of alignment between different image-text pairs is inconsistent. Secondly, the instructions currently used for finetuning incorporate a variety of tasks, different tasks's instructions usually require different levels of alignment capabilities, but previous MLLMs overlook these differentiated alignment needs. To tackle these issues, we propose a new multimodal large language model AlignGPT. In the pre-training stage, instead of treating all image-text pairs equally, we assign different levels of alignment capabilities to different image-text pairs. Then, in the instruction-tuning phase, we adaptively combine these different levels of alignment capabilities to meet the dynamic alignment needs of different instructions. Extensive experimental results show that our model achieves competitive performance on 12 benchmarks.","sentences":["Multimodal Large Language Models (MLLMs) are widely regarded as crucial in the exploration of Artificial General Intelligence (AGI).","The core of MLLMs lies in their capability to achieve cross-modal alignment.","To attain this goal, current MLLMs typically follow a two-phase training paradigm: the pre-training phase and the instruction-tuning phase.","Despite their success, there are shortcomings in the modeling of alignment capabilities within these models.","Firstly, during the pre-training phase, the model usually assumes that all image-text pairs are uniformly aligned, but in fact the degree of alignment between different image-text pairs is inconsistent.","Secondly, the instructions currently used for finetuning incorporate a variety of tasks, different tasks's instructions usually require different levels of alignment capabilities, but previous MLLMs overlook these differentiated alignment needs.","To tackle these issues, we propose a new multimodal large language model AlignGPT.","In the pre-training stage, instead of treating all image-text pairs equally, we assign different levels of alignment capabilities to different image-text pairs.","Then, in the instruction-tuning phase, we adaptively combine these different levels of alignment capabilities to meet the dynamic alignment needs of different instructions.","Extensive experimental results show that our model achieves competitive performance on 12 benchmarks."],"url":"http://arxiv.org/abs/2405.14129v1","category":"cs.CL"}
{"created":"2024-05-23 03:01:32","title":"Transformers for Image-Goal Navigation","abstract":"Visual perception and navigation have emerged as major focus areas in the field of embodied artificial intelligence. We consider the task of image-goal navigation, where an agent is tasked to navigate to a goal specified by an image, relying only on images from an onboard camera. This task is particularly challenging since it demands robust scene understanding, goal-oriented planning and long-horizon navigation. Most existing approaches typically learn navigation policies reliant on recurrent neural networks trained via online reinforcement learning. However, training such policies requires substantial computational resources and time, and performance of these models is not reliable on long-horizon navigation. In this work, we present a generative Transformer based model that jointly models image goals, camera observations and the robot's past actions to predict future actions. We use state-of-the-art perception models and navigation policies to learn robust goal conditioned policies without the need for real-time interaction with the environment. Our model demonstrates capability in capturing and associating visual information across long time horizons, helping in effective navigation.","sentences":["Visual perception and navigation have emerged as major focus areas in the field of embodied artificial intelligence.","We consider the task of image-goal navigation, where an agent is tasked to navigate to a goal specified by an image, relying only on images from an onboard camera.","This task is particularly challenging since it demands robust scene understanding, goal-oriented planning and long-horizon navigation.","Most existing approaches typically learn navigation policies reliant on recurrent neural networks trained via online reinforcement learning.","However, training such policies requires substantial computational resources and time, and performance of these models is not reliable on long-horizon navigation.","In this work, we present a generative Transformer based model that jointly models image goals, camera observations and the robot's past actions to predict future actions.","We use state-of-the-art perception models and navigation policies to learn robust goal conditioned policies without the need for real-time interaction with the environment.","Our model demonstrates capability in capturing and associating visual information across long time horizons, helping in effective navigation."],"url":"http://arxiv.org/abs/2405.14128v1","category":"cs.RO"}
{"created":"2024-05-23 02:58:23","title":"The Disappearance of Timestep Embedding in Modern Time-Dependent Neural Networks","abstract":"Dynamical systems are often time-varying, whose modeling requires a function that evolves with respect to time. Recent studies such as the neural ordinary differential equation proposed a time-dependent neural network, which provides a neural network varying with respect to time. However, we claim that the architectural choice to build a time-dependent neural network significantly affects its time-awareness but still lacks sufficient validation in its current states. In this study, we conduct an in-depth analysis of the architecture of modern time-dependent neural networks. Here, we report a vulnerability of vanishing timestep embedding, which disables the time-awareness of a time-dependent neural network. Furthermore, we find that this vulnerability can also be observed in diffusion models because they employ a similar architecture that incorporates timestep embedding to discriminate between different timesteps during a diffusion process. Our analysis provides a detailed description of this phenomenon as well as several solutions to address the root cause. Through experiments on neural ordinary differential equations and diffusion models, we observed that ensuring alive time-awareness via proposed solutions boosted their performance, which implies that their current implementations lack sufficient time-dependency.","sentences":["Dynamical systems are often time-varying, whose modeling requires a function that evolves with respect to time.","Recent studies such as the neural ordinary differential equation proposed a time-dependent neural network, which provides a neural network varying with respect to time.","However, we claim that the architectural choice to build a time-dependent neural network significantly affects its time-awareness but still lacks sufficient validation in its current states.","In this study, we conduct an in-depth analysis of the architecture of modern time-dependent neural networks.","Here, we report a vulnerability of vanishing timestep embedding, which disables the time-awareness of a time-dependent neural network.","Furthermore, we find that this vulnerability can also be observed in diffusion models because they employ a similar architecture that incorporates timestep embedding to discriminate between different timesteps during a diffusion process.","Our analysis provides a detailed description of this phenomenon as well as several solutions to address the root cause.","Through experiments on neural ordinary differential equations and diffusion models, we observed that ensuring alive time-awareness via proposed solutions boosted their performance, which implies that their current implementations lack sufficient time-dependency."],"url":"http://arxiv.org/abs/2405.14126v1","category":"cs.LG"}
{"created":"2024-05-23 02:57:42","title":"ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based Evaluation","abstract":"Large Language Models (LLMs) can elicit unintended and even harmful content when misaligned with human values, posing severe risks to users and society. To mitigate these risks, current evaluation benchmarks predominantly employ expert-designed contextual scenarios to assess how well LLMs align with human values. However, the labor-intensive nature of these benchmarks limits their test scope, hindering their ability to generalize to the extensive variety of open-world use cases and identify rare but crucial long-tail risks. Additionally, these static tests fail to adapt to the rapid evolution of LLMs, making it hard to evaluate timely alignment issues. To address these challenges, we propose ALI-Agent, an evaluation framework that leverages the autonomous abilities of LLM-powered agents to conduct in-depth and adaptive alignment assessments. ALI-Agent operates through two principal stages: Emulation and Refinement. During the Emulation stage, ALI-Agent automates the generation of realistic test scenarios. In the Refinement stage, it iteratively refines the scenarios to probe long-tail risks. Specifically, ALI-Agent incorporates a memory module to guide test scenario generation, a tool-using module to reduce human labor in tasks such as evaluating feedback from target LLMs, and an action module to refine tests. Extensive experiments across three aspects of human values--stereotypes, morality, and legality--demonstrate that ALI-Agent, as a general evaluation framework, effectively identifies model misalignment. Systematic analysis also validates that the generated test scenarios represent meaningful use cases, as well as integrate enhanced measures to probe long-tail risks. Our code is available at https://github.com/SophieZheng998/ALI-Agent.git","sentences":["Large Language Models (LLMs) can elicit unintended and even harmful content when misaligned with human values, posing severe risks to users and society.","To mitigate these risks, current evaluation benchmarks predominantly employ expert-designed contextual scenarios to assess how well LLMs align with human values.","However, the labor-intensive nature of these benchmarks limits their test scope, hindering their ability to generalize to the extensive variety of open-world use cases and identify rare but crucial long-tail risks.","Additionally, these static tests fail to adapt to the rapid evolution of LLMs, making it hard to evaluate timely alignment issues.","To address these challenges, we propose ALI-Agent, an evaluation framework that leverages the autonomous abilities of LLM-powered agents to conduct in-depth and adaptive alignment assessments.","ALI-Agent operates through two principal stages: Emulation and Refinement.","During the Emulation stage, ALI-Agent automates the generation of realistic test scenarios.","In the Refinement stage, it iteratively refines the scenarios to probe long-tail risks.","Specifically, ALI-Agent incorporates a memory module to guide test scenario generation, a tool-using module to reduce human labor in tasks such as evaluating feedback from target LLMs, and an action module to refine tests.","Extensive experiments across three aspects of human values--stereotypes, morality, and legality--demonstrate that ALI-Agent, as a general evaluation framework, effectively identifies model misalignment.","Systematic analysis also validates that the generated test scenarios represent meaningful use cases, as well as integrate enhanced measures to probe long-tail risks.","Our code is available at https://github.com/SophieZheng998/ALI-Agent.git"],"url":"http://arxiv.org/abs/2405.14125v1","category":"cs.AI"}
{"created":"2024-05-23 02:44:12","title":"Knowledge Localization: Mission Not Accomplished? Enter Query Localization!","abstract":"Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear. The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the knowledge localization (KL) assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons. However, this assumption may be overly strong regarding knowledge storage and neglects knowledge expression mechanisms. Thus, we re-examine the KL assumption and confirm the existence of facts that do not adhere to it from both statistical and knowledge modification perspectives. Furthermore, we propose the Query Localization (QL) assumption. (1) Query-KN Mapping: The localization results are associated with the query rather than the fact. (2) Dynamic KN Selection: The attention module contributes to the selection of KNs for answering a query. Based on this, we further propose the Consistency-Aware KN modification method, which improves the performance of knowledge modification. We conduct 39 sets of experiments, along with additional visualization experiments, to rigorously validate our conclusions.","sentences":["Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear.","The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms.","This theory is based on the knowledge localization (KL) assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons.","However, this assumption may be overly strong regarding knowledge storage and neglects knowledge expression mechanisms.","Thus, we re-examine the KL assumption and confirm the existence of facts that do not adhere to it from both statistical and knowledge modification perspectives.","Furthermore, we propose the Query Localization (QL) assumption.","(1) Query-KN Mapping:","The localization results are associated with the query rather than the fact.","(2) Dynamic KN Selection: The attention module contributes to the selection of KNs for answering a query.","Based on this, we further propose the Consistency-Aware KN modification method, which improves the performance of knowledge modification.","We conduct 39 sets of experiments, along with additional visualization experiments, to rigorously validate our conclusions."],"url":"http://arxiv.org/abs/2405.14117v1","category":"cs.CL"}
{"created":"2024-05-23 02:42:32","title":"Configuring Data Augmentations to Reduce Variance Shift in Positional Embedding of Vision Transformers","abstract":"Vision transformers (ViTs) have demonstrated remarkable performance in a variety of vision tasks. Despite their promising capabilities, training a ViT requires a large amount of diverse data. Several studies empirically found that using rich data augmentations, such as Mixup, Cutmix, and random erasing, is critical to the successful training of ViTs. Now, the use of rich data augmentations has become a standard practice in the current state. However, we report a vulnerability to this practice: Certain data augmentations such as Mixup cause a variance shift in the positional embedding of ViT, which has been a hidden factor that degrades the performance of ViT during the test phase. We claim that achieving a stable effect from positional embedding requires a specific condition on the image, which is often broken for the current data augmentation methods. We provide a detailed analysis of this problem as well as the correct configuration for these data augmentations to remove the side effects of variance shift. Experiments showed that adopting our guidelines improves the performance of ViTs compared with the current configuration of data augmentations.","sentences":["Vision transformers (ViTs) have demonstrated remarkable performance in a variety of vision tasks.","Despite their promising capabilities, training a ViT requires a large amount of diverse data.","Several studies empirically found that using rich data augmentations, such as Mixup, Cutmix, and random erasing, is critical to the successful training of ViTs.","Now, the use of rich data augmentations has become a standard practice in the current state.","However, we report a vulnerability to this practice: Certain data augmentations such as Mixup cause a variance shift in the positional embedding of ViT, which has been a hidden factor that degrades the performance of ViT during the test phase.","We claim that achieving a stable effect from positional embedding requires a specific condition on the image, which is often broken for the current data augmentation methods.","We provide a detailed analysis of this problem as well as the correct configuration for these data augmentations to remove the side effects of variance shift.","Experiments showed that adopting our guidelines improves the performance of ViTs compared with the current configuration of data augmentations."],"url":"http://arxiv.org/abs/2405.14115v1","category":"cs.CV"}
{"created":"2024-05-23 02:41:36","title":"Offline Reinforcement Learning from Datasets with Structured Non-Stationarity","abstract":"Current Reinforcement Learning (RL) is often limited by the large amount of data needed to learn a successful policy. Offline RL aims to solve this issue by using transitions collected by a different behavior policy. We address a novel Offline RL problem setting in which, while collecting the dataset, the transition and reward functions gradually change between episodes but stay constant within each episode. We propose a method based on Contrastive Predictive Coding that identifies this non-stationarity in the offline dataset, accounts for it when training a policy, and predicts it during evaluation. We analyze our proposed method and show that it performs well in simple continuous control tasks and challenging, high-dimensional locomotion tasks. We show that our method often achieves the oracle performance and performs better than baselines.","sentences":["Current Reinforcement Learning (RL) is often limited by the large amount of data needed to learn a successful policy.","Offline RL aims to solve this issue by using transitions collected by a different behavior policy.","We address a novel Offline RL problem setting in which, while collecting the dataset, the transition and reward functions gradually change between episodes but stay constant within each episode.","We propose a method based on Contrastive Predictive Coding that identifies this non-stationarity in the offline dataset, accounts for it when training a policy, and predicts it during evaluation.","We analyze our proposed method and show that it performs well in simple continuous control tasks and challenging, high-dimensional locomotion tasks.","We show that our method often achieves the oracle performance and performs better than baselines."],"url":"http://arxiv.org/abs/2405.14114v1","category":"cs.LG"}
{"created":"2024-05-23 02:27:39","title":"Deep Learning for Protein-Ligand Docking: Are We There Yet?","abstract":"The effects of ligand binding on protein structures and their in vivo functions carry numerous implications for modern biomedical research and biotechnology development efforts such as drug discovery. Although several deep learning (DL) methods and benchmarks designed for protein-ligand docking have recently been introduced, to date no prior works have systematically studied the behavior of docking methods within the practical context of (1) predicted (apo) protein structures, (2) multiple ligands concurrently binding to a given target protein, and (3) having no prior knowledge of binding pockets. To enable a deeper understanding of docking methods' real-world utility, we introduce PoseBench, the first comprehensive benchmark for practical protein-ligand docking. PoseBench enables researchers to rigorously and systematically evaluate DL docking methods for apo-to-holo protein-ligand docking and protein-ligand structure generation using both single and multi-ligand benchmark datasets, the latter of which we introduce for the first time to the DL community. Empirically, using PoseBench, we find that all recent DL docking methods but one fail to generalize to multi-ligand protein targets and also that template-based docking algorithms perform equally well or better for multi-ligand docking as recent single-ligand DL docking methods, suggesting areas of improvement for future work. Code, data, tutorials, and benchmark results are available at https://github.com/BioinfoMachineLearning/PoseBench.","sentences":["The effects of ligand binding on protein structures and their in vivo functions carry numerous implications for modern biomedical research and biotechnology development efforts such as drug discovery.","Although several deep learning (DL) methods and benchmarks designed for protein-ligand docking have recently been introduced, to date no prior works have systematically studied the behavior of docking methods within the practical context of (1) predicted (apo) protein structures, (2) multiple ligands concurrently binding to a given target protein, and (3) having no prior knowledge of binding pockets.","To enable a deeper understanding of docking methods' real-world utility, we introduce PoseBench, the first comprehensive benchmark for practical protein-ligand docking.","PoseBench enables researchers to rigorously and systematically evaluate DL docking methods for apo-to-holo protein-ligand docking and protein-ligand structure generation using both single and multi-ligand benchmark datasets, the latter of which we introduce for the first time to the DL community.","Empirically, using PoseBench, we find that all recent DL docking methods but one fail to generalize to multi-ligand protein targets and also that template-based docking algorithms perform equally well or better for multi-ligand docking as recent single-ligand DL docking methods, suggesting areas of improvement for future work.","Code, data, tutorials, and benchmark results are available at https://github.com/BioinfoMachineLearning/PoseBench."],"url":"http://arxiv.org/abs/2405.14108v1","category":"cs.LG"}
{"created":"2024-05-23 02:14:17","title":"Distributed Speculative Inference of Large Language Models","abstract":"Accelerating the inference of large language models (LLMs) is an important challenge in artificial intelligence. This paper introduces distributed speculative inference (DSI), a novel distributed inference algorithm that is provably faster than speculative inference (SI) [leviathan2023fast, chen2023accelerating, miao2023specinfer] and traditional autoregressive inference (non-SI). Like other SI algorithms, DSI works on frozen LLMs, requiring no training or architectural modifications, and it preserves the target distribution.   Prior studies on SI have demonstrated empirical speedups (compared to non-SI) but require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs often do not have matching drafters that are sufficiently fast and accurate. We show a gap: SI gets slower than non-SI when using slower or less accurate drafters. We close this gap by proving that DSI is faster than both SI and non-SI given any drafters. By orchestrating multiple instances of the target and drafters, DSI is not only faster than SI but also supports LLMs that cannot be accelerated with SI.   Our simulations show speedups of off-the-shelf LLMs in realistic settings: DSI is 1.29-1.92x faster than SI.","sentences":["Accelerating the inference of large language models (LLMs) is an important challenge in artificial intelligence.","This paper introduces distributed speculative inference (DSI), a novel distributed inference algorithm that is provably faster than speculative inference (SI) [leviathan2023fast, chen2023accelerating, miao2023specinfer] and traditional autoregressive inference (non-SI).","Like other SI algorithms, DSI works on frozen LLMs, requiring no training or architectural modifications, and it preserves the target distribution.   ","Prior studies on SI have demonstrated empirical speedups (compared to non-SI) but require a fast and accurate drafter LLM.","In practice, off-the-shelf LLMs often do not have matching drafters that are sufficiently fast and accurate.","We show a gap: SI gets slower than non-SI when using slower or less accurate drafters.","We close this gap by proving that DSI is faster than both SI and non-SI given any drafters.","By orchestrating multiple instances of the target and drafters, DSI is not only faster than SI but also supports LLMs that cannot be accelerated with SI.   ","Our simulations show speedups of off-the-shelf LLMs in realistic settings: DSI is 1.29-1.92x faster than SI."],"url":"http://arxiv.org/abs/2405.14105v1","category":"cs.DC"}
{"created":"2024-05-23 01:48:32","title":"Attending to Topological Spaces: The Cellular Transformer","abstract":"Topological Deep Learning seeks to enhance the predictive performance of neural network models by harnessing topological structures in input data. Topological neural networks operate on spaces such as cell complexes and hypergraphs, that can be seen as generalizations of graphs. In this work, we introduce the Cellular Transformer (CT), a novel architecture that generalizes graph-based transformers to cell complexes. First, we propose a new formulation of the usual self- and cross-attention mechanisms, tailored to leverage incidence relations in cell complexes, e.g., edge-face and node-edge relations. Additionally, we propose a set of topological positional encodings specifically designed for cell complexes. By transforming three graph datasets into cell complex datasets, our experiments reveal that CT not only achieves state-of-the-art performance, but it does so without the need for more complex enhancements such as virtual nodes, in-domain structural encodings, or graph rewiring.","sentences":["Topological Deep Learning seeks to enhance the predictive performance of neural network models by harnessing topological structures in input data.","Topological neural networks operate on spaces such as cell complexes and hypergraphs, that can be seen as generalizations of graphs.","In this work, we introduce the Cellular Transformer (CT), a novel architecture that generalizes graph-based transformers to cell complexes.","First, we propose a new formulation of the usual self- and cross-attention mechanisms, tailored to leverage incidence relations in cell complexes, e.g., edge-face and node-edge relations.","Additionally, we propose a set of topological positional encodings specifically designed for cell complexes.","By transforming three graph datasets into cell complex datasets, our experiments reveal that CT not only achieves state-of-the-art performance, but it does so without the need for more complex enhancements such as virtual nodes, in-domain structural encodings, or graph rewiring."],"url":"http://arxiv.org/abs/2405.14094v1","category":"cs.LG"}
{"created":"2024-05-23 01:32:25","title":"High-dimensional Learning with Noisy Labels","abstract":"This paper provides theoretical insights into high-dimensional binary classification with class-conditional noisy labels. Specifically, we study the behavior of a linear classifier with a label noisiness aware loss function, when both the dimension of data $p$ and the sample size $n$ are large and comparable. Relying on random matrix theory by supposing a Gaussian mixture data model, the performance of the linear classifier when $p,n\\to \\infty$ is shown to converge towards a limit, involving scalar statistics of the data. Importantly, our findings show that the low-dimensional intuitions to handle label noise do not hold in high-dimension, in the sense that the optimal classifier in low-dimension dramatically fails in high-dimension. Based on our derivations, we design an optimized method that is shown to be provably more efficient in handling noisy labels in high dimensions. Our theoretical conclusions are further confirmed by experiments on real datasets, where we show that our optimized approach outperforms the considered baselines.","sentences":["This paper provides theoretical insights into high-dimensional binary classification with class-conditional noisy labels.","Specifically, we study the behavior of a linear classifier with a label noisiness aware loss function, when both the dimension of data $p$ and the sample size $n$ are large and comparable.","Relying on random matrix theory by supposing a Gaussian mixture data model, the performance of the linear classifier when $p,n\\to \\infty$ is shown to converge towards a limit, involving scalar statistics of the data.","Importantly, our findings show that the low-dimensional intuitions to handle label noise do not hold in high-dimension, in the sense that the optimal classifier in low-dimension dramatically fails in high-dimension.","Based on our derivations, we design an optimized method that is shown to be provably more efficient in handling noisy labels in high dimensions.","Our theoretical conclusions are further confirmed by experiments on real datasets, where we show that our optimized approach outperforms the considered baselines."],"url":"http://arxiv.org/abs/2405.14088v1","category":"cs.LG"}
{"created":"2024-05-23 01:06:05","title":"Exclusively Penalized Q-learning for Offline Reinforcement Learning","abstract":"Constraint-based offline reinforcement learning (RL) involves policy constraints or imposing penalties on the value function to mitigate overestimation errors caused by distributional shift. This paper focuses on a limitation in existing offline RL methods with penalized value function, indicating the potential for underestimation bias due to unnecessary bias introduced in the value function. To address this concern, we propose Exclusively Penalized Q-learning (EPQ), which reduces estimation bias in the value function by selectively penalizing states that are prone to inducing estimation errors. Numerical results show that our method significantly reduces underestimation bias and improves performance in various offline control tasks compared to other offline RL methods","sentences":["Constraint-based offline reinforcement learning (RL) involves policy constraints or imposing penalties on the value function to mitigate overestimation errors caused by distributional shift.","This paper focuses on a limitation in existing offline RL methods with penalized value function, indicating the potential for underestimation bias due to unnecessary bias introduced in the value function.","To address this concern, we propose Exclusively Penalized Q-learning (EPQ), which reduces estimation bias in the value function by selectively penalizing states that are prone to inducing estimation errors.","Numerical results show that our method significantly reduces underestimation bias and improves performance in various offline control tasks compared to other offline RL methods"],"url":"http://arxiv.org/abs/2405.14082v1","category":"cs.LG"}
{"created":"2024-05-23 00:52:38","title":"A finite time analysis of distributed Q-learning","abstract":"Multi-agent reinforcement learning (MARL) has witnessed a remarkable surge in interest, fueled by the empirical success achieved in applications of single-agent reinforcement learning (RL). In this study, we consider a distributed Q-learning scenario, wherein a number of agents cooperatively solve a sequential decision making problem without access to the central reward function which is an average of the local rewards. In particular, we study finite-time analysis of a distributed Q-learning algorithm, and provide a new sample complexity result of $\\tilde{\\mathcal{O}}\\left( \\min\\left\\{\\frac{1}{\\epsilon^2}\\frac{t_{\\text{mix}}}{(1-\\gamma)^6 d_{\\min}^4 } ,\\frac{1}{\\epsilon}\\frac{\\sqrt{|\\gS||\\gA|}}{(1-\\sigma_2(\\boldsymbol{W}))(1-\\gamma)^4 d_{\\min}^3} \\right\\}\\right)$ under tabular lookup","sentences":["Multi-agent reinforcement learning (MARL) has witnessed a remarkable surge in interest, fueled by the empirical success achieved in applications of single-agent reinforcement learning (RL).","In this study, we consider a distributed Q-learning scenario, wherein a number of agents cooperatively solve a sequential decision making problem without access to the central reward function which is an average of the local rewards.","In particular, we study finite-time analysis of a distributed Q-learning algorithm, and provide a new sample complexity result of $\\tilde{\\mathcal{O}}\\left( \\min\\left\\{\\frac{1}{\\epsilon^2}\\frac{t_{\\text{mix}}}{(1-\\gamma)^6 d_{\\min}^4 } ,\\frac{1}{\\epsilon}\\frac{\\sqrt{|\\gS||\\gA|}}{(1-\\sigma_2(\\boldsymbol{W}))(1-\\gamma)^4 d_{\\min}^3} \\right\\}\\right)$ under tabular lookup"],"url":"http://arxiv.org/abs/2405.14078v1","category":"cs.AI"}
{"created":"2024-05-23 00:46:53","title":"Learning to Transform Dynamically for Better Adversarial Transferability","abstract":"Adversarial examples, crafted by adding perturbations imperceptible to humans, can deceive neural networks. Recent studies identify the adversarial transferability across various models, \\textit{i.e.}, the cross-model attack ability of adversarial samples. To enhance such adversarial transferability, existing input transformation-based methods diversify input data with transformation augmentation. However, their effectiveness is limited by the finite number of available transformations. In our study, we introduce a novel approach named Learning to Transform (L2T). L2T increases the diversity of transformed images by selecting the optimal combination of operations from a pool of candidates, consequently improving adversarial transferability. We conceptualize the selection of optimal transformation combinations as a trajectory optimization problem and employ a reinforcement learning strategy to effectively solve the problem. Comprehensive experiments on the ImageNet dataset, as well as practical tests with Google Vision and GPT-4V, reveal that L2T surpasses current methodologies in enhancing adversarial transferability, thereby confirming its effectiveness and practical significance. The code is available at https://github.com/RongyiZhu/L2T.","sentences":["Adversarial examples, crafted by adding perturbations imperceptible to humans, can deceive neural networks.","Recent studies identify the adversarial transferability across various models, \\textit{i.e.}, the cross-model attack ability of adversarial samples.","To enhance such adversarial transferability, existing input transformation-based methods diversify input data with transformation augmentation.","However, their effectiveness is limited by the finite number of available transformations.","In our study, we introduce a novel approach named Learning to Transform (L2T).","L2T increases the diversity of transformed images by selecting the optimal combination of operations from a pool of candidates, consequently improving adversarial transferability.","We conceptualize the selection of optimal transformation combinations as a trajectory optimization problem and employ a reinforcement learning strategy to effectively solve the problem.","Comprehensive experiments on the ImageNet dataset, as well as practical tests with Google Vision and GPT-4V, reveal that L2T surpasses current methodologies in enhancing adversarial transferability, thereby confirming its effectiveness and practical significance.","The code is available at https://github.com/RongyiZhu/L2T."],"url":"http://arxiv.org/abs/2405.14077v1","category":"cs.CV"}
{"created":"2024-05-23 00:40:43","title":"$T^2$ of Thoughts: Temperature Tree Elicits Reasoning in Large Language Models","abstract":"Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, especially in complex decision-making scenarios, but their static problem-solving strategies often limit their adaptability to dynamic environments. We explore the enhancement of reasoning capabilities in LLMs through Temperature Tree ($T^2$) prompting via Particle Swarm Optimization, termed as $T^2$ of Thoughts ($T^2oT$). The primary focus is on enhancing decision-making processes by dynamically adjusting search parameters, especially temperature, to improve accuracy without increasing computational demands. We empirically validate that our hybrid $T^2oT$ approach yields enhancements in, single-solution accuracy, multi-solution generation and text generation quality. Our findings suggest that while dynamic search depth adjustments based on temperature can yield mixed results, a fixed search depth, when coupled with adaptive capabilities of $T^2oT$, provides a more reliable and versatile problem-solving strategy. This work highlights the potential for future explorations in optimizing algorithmic interactions with foundational language models, particularly illustrated by our development for the Game of 24 and Creative Writing tasks.","sentences":["Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, especially in complex decision-making scenarios, but their static problem-solving strategies often limit their adaptability to dynamic environments.","We explore the enhancement of reasoning capabilities in LLMs through Temperature Tree ($T^2$) prompting via Particle Swarm Optimization, termed as $T^2$ of Thoughts ($T^2oT$).","The primary focus is on enhancing decision-making processes by dynamically adjusting search parameters, especially temperature, to improve accuracy without increasing computational demands.","We empirically validate that our hybrid $T^2oT$ approach yields enhancements in, single-solution accuracy, multi-solution generation and text generation quality.","Our findings suggest that while dynamic search depth adjustments based on temperature can yield mixed results, a fixed search depth, when coupled with adaptive capabilities of $T^2oT$, provides a more reliable and versatile problem-solving strategy.","This work highlights the potential for future explorations in optimizing algorithmic interactions with foundational language models, particularly illustrated by our development for the Game of 24 and Creative Writing tasks."],"url":"http://arxiv.org/abs/2405.14075v1","category":"cs.CL"}
{"created":"2024-05-22 23:53:46","title":"ABI Approach: Automatic Bias Identification in Decision-Making Under Risk based in an Ontology of Behavioral Economics","abstract":"Organizational decision-making is crucial for success, yet cognitive biases can significantly affect risk preferences, leading to suboptimal outcomes. Risk seeking preferences for losses, driven by biases such as loss aversion, pose challenges and can result in severe negative consequences, including financial losses. This research introduces the ABI approach, a novel solution designed to support organizational decision-makers by automatically identifying and explaining risk seeking preferences during decision-making. This research makes a novel contribution by automating the identification and explanation of risk seeking preferences using Cumulative Prospect theory (CPT) from Behavioral Economics. The ABI approach transforms theoretical insights into actionable, real-time guidance, making them accessible to a broader range of organizations and decision-makers without requiring specialized personnel. By contextualizing CPT concepts into business language, the approach facilitates widespread adoption and enhances decision-making processes with deep behavioral insights. Our systematic literature review identified significant gaps in existing methods, especially the lack of automated solutions with a concrete mechanism for automatically identifying risk seeking preferences, and the absence of formal knowledge representation, such as ontologies, for identifying and explaining the risk preferences. The ABI Approach addresses these gaps, offering a significant contribution to decision-making research and practice. Furthermore, it enables automatic collection of historical decision data with risk preferences, providing valuable insights for enhancing strategic management and long-term organizational performance. An experiment provided preliminary evidence on its effectiveness in helping decision-makers recognize their risk seeking preferences during decision-making in the loss domain.","sentences":["Organizational decision-making is crucial for success, yet cognitive biases can significantly affect risk preferences, leading to suboptimal outcomes.","Risk seeking preferences for losses, driven by biases such as loss aversion, pose challenges and can result in severe negative consequences, including financial losses.","This research introduces the ABI approach, a novel solution designed to support organizational decision-makers by automatically identifying and explaining risk seeking preferences during decision-making.","This research makes a novel contribution by automating the identification and explanation of risk seeking preferences using Cumulative Prospect theory (CPT) from Behavioral Economics.","The ABI approach transforms theoretical insights into actionable, real-time guidance, making them accessible to a broader range of organizations and decision-makers without requiring specialized personnel.","By contextualizing CPT concepts into business language, the approach facilitates widespread adoption and enhances decision-making processes with deep behavioral insights.","Our systematic literature review identified significant gaps in existing methods, especially the lack of automated solutions with a concrete mechanism for automatically identifying risk seeking preferences, and the absence of formal knowledge representation, such as ontologies, for identifying and explaining the risk preferences.","The ABI Approach addresses these gaps, offering a significant contribution to decision-making research and practice.","Furthermore, it enables automatic collection of historical decision data with risk preferences, providing valuable insights for enhancing strategic management and long-term organizational performance.","An experiment provided preliminary evidence on its effectiveness in helping decision-makers recognize their risk seeking preferences during decision-making in the loss domain."],"url":"http://arxiv.org/abs/2405.14067v1","category":"cs.HC"}
{"created":"2024-05-22 23:21:15","title":"ChatScene: Knowledge-Enabled Safety-Critical Scenario Generation for Autonomous Vehicles","abstract":"We present ChatScene, a Large Language Model (LLM)-based agent that leverages the capabilities of LLMs to generate safety-critical scenarios for autonomous vehicles. Given unstructured language instructions, the agent first generates textually described traffic scenarios using LLMs. These scenario descriptions are subsequently broken down into several sub-descriptions for specified details such as behaviors and locations of vehicles. The agent then distinctively transforms the textually described sub-scenarios into domain-specific languages, which then generate actual code for prediction and control in simulators, facilitating the creation of diverse and complex scenarios within the CARLA simulation environment. A key part of our agent is a comprehensive knowledge retrieval component, which efficiently translates specific textual descriptions into corresponding domain-specific code snippets by training a knowledge database containing the scenario description and code pairs. Extensive experimental results underscore the efficacy of ChatScene in improving the safety of autonomous vehicles. For instance, the scenarios generated by ChatScene show a 15% increase in collision rates compared to state-of-the-art baselines when tested against different reinforcement learning-based ego vehicles. Furthermore, we show that by using our generated safety-critical scenarios to fine-tune different RL-based autonomous driving models, they can achieve a 9% reduction in collision rates, surpassing current SOTA methods. ChatScene effectively bridges the gap between textual descriptions of traffic scenarios and practical CARLA simulations, providing a unified way to conveniently generate safety-critical scenarios for safety testing and improvement for AVs.","sentences":["We present ChatScene, a Large Language Model (LLM)-based agent that leverages the capabilities of LLMs to generate safety-critical scenarios for autonomous vehicles.","Given unstructured language instructions, the agent first generates textually described traffic scenarios using LLMs.","These scenario descriptions are subsequently broken down into several sub-descriptions for specified details such as behaviors and locations of vehicles.","The agent then distinctively transforms the textually described sub-scenarios into domain-specific languages, which then generate actual code for prediction and control in simulators, facilitating the creation of diverse and complex scenarios within the CARLA simulation environment.","A key part of our agent is a comprehensive knowledge retrieval component, which efficiently translates specific textual descriptions into corresponding domain-specific code snippets by training a knowledge database containing the scenario description and code pairs.","Extensive experimental results underscore the efficacy of ChatScene in improving the safety of autonomous vehicles.","For instance, the scenarios generated by ChatScene show a 15% increase in collision rates compared to state-of-the-art baselines when tested against different reinforcement learning-based ego vehicles.","Furthermore, we show that by using our generated safety-critical scenarios to fine-tune different RL-based autonomous driving models, they can achieve a 9% reduction in collision rates, surpassing current SOTA methods.","ChatScene effectively bridges the gap between textual descriptions of traffic scenarios and practical CARLA simulations, providing a unified way to conveniently generate safety-critical scenarios for safety testing and improvement for AVs."],"url":"http://arxiv.org/abs/2405.14062v1","category":"cs.AI"}
{"created":"2024-05-22 23:18:58","title":"Meanings and Feelings of Large Language Models: Observability of Latent States in Generative AI","abstract":"We tackle the question of whether Large Language Models (LLMs), viewed as dynamical systems with state evolving in the embedding space of symbolic tokens, are observable. That is, whether there exist multiple 'mental' state trajectories that yield the same sequence of generated tokens, or sequences that belong to the same Nerode equivalence class ('meaning'). If not observable, mental state trajectories ('experiences') evoked by an input ('perception') or by feedback from the model's own state ('thoughts') could remain self-contained and evolve unbeknown to the user while being potentially accessible to the model provider. Such \"self-contained experiences evoked by perception or thought\" are akin to what the American Psychological Association (APA) defines as 'feelings'. Beyond the lexical curiosity, we show that current LLMs implemented by autoregressive Transformers cannot have 'feelings' according to this definition: The set of state trajectories indistinguishable from the tokenized output is a singleton. But if there are 'system prompts' not visible to the user, then the set of indistinguishable trajectories becomes non-trivial, and there can be multiple state trajectories that yield the same verbalized output. We prove these claims analytically, and show examples of modifications to standard LLMs that engender such 'feelings.' Our analysis sheds light on possible designs that would enable a model to perform non-trivial computation that is not visible to the user, as well as on controls that the provider of services using the model could take to prevent unintended behavior.","sentences":["We tackle the question of whether Large Language Models (LLMs), viewed as dynamical systems with state evolving in the embedding space of symbolic tokens, are observable.","That is, whether there exist multiple 'mental' state trajectories that yield the same sequence of generated tokens, or sequences that belong to the same Nerode equivalence class ('meaning').","If not observable, mental state trajectories ('experiences') evoked by an input ('perception') or by feedback from the model's own state ('thoughts') could remain self-contained and evolve unbeknown to the user while being potentially accessible to the model provider.","Such \"self-contained experiences evoked by perception or thought\" are akin to what the American Psychological Association (APA) defines as 'feelings'.","Beyond the lexical curiosity, we show that current LLMs implemented by autoregressive Transformers cannot have 'feelings' according to this definition: The set of state trajectories indistinguishable from the tokenized output is a singleton.","But if there are 'system prompts' not visible to the user, then the set of indistinguishable trajectories becomes non-trivial, and there can be multiple state trajectories that yield the same verbalized output.","We prove these claims analytically, and show examples of modifications to standard LLMs that engender such 'feelings.'","Our analysis sheds light on possible designs that would enable a model to perform non-trivial computation that is not visible to the user, as well as on controls that the provider of services using the model could take to prevent unintended behavior."],"url":"http://arxiv.org/abs/2405.14061v1","category":"cs.AI"}
{"created":"2024-05-22 23:06:34","title":"Formally Verifying Deep Reinforcement Learning Controllers with Lyapunov Barrier Certificates","abstract":"Deep reinforcement learning (DRL) is a powerful machine learning paradigm for generating agents that control autonomous systems. However, the \"black box\" nature of DRL agents limits their deployment in real-world safety-critical applications. A promising approach for providing strong guarantees on an agent's behavior is to use Neural Lyapunov Barrier (NLB) certificates, which are learned functions over the system whose properties indirectly imply that an agent behaves as desired. However, NLB-based certificates are typically difficult to learn and even more difficult to verify, especially for complex systems. In this work, we present a novel method for training and verifying NLB-based certificates for discrete-time systems. Specifically, we introduce a technique for certificate composition, which simplifies the verification of highly-complex systems by strategically designing a sequence of certificates. When jointly verified with neural network verification engines, these certificates provide a formal guarantee that a DRL agent both achieves its goals and avoids unsafe behavior. Furthermore, we introduce a technique for certificate filtering, which significantly simplifies the process of producing formally verified certificates. We demonstrate the merits of our approach with a case study on providing safety and liveness guarantees for a DRL-controlled spacecraft.","sentences":["Deep reinforcement learning (DRL) is a powerful machine learning paradigm for generating agents that control autonomous systems.","However, the \"black box\" nature of DRL agents limits their deployment in real-world safety-critical applications.","A promising approach for providing strong guarantees on an agent's behavior is to use Neural Lyapunov Barrier (NLB) certificates, which are learned functions over the system whose properties indirectly imply that an agent behaves as desired.","However, NLB-based certificates are typically difficult to learn and even more difficult to verify, especially for complex systems.","In this work, we present a novel method for training and verifying NLB-based certificates for discrete-time systems.","Specifically, we introduce a technique for certificate composition, which simplifies the verification of highly-complex systems by strategically designing a sequence of certificates.","When jointly verified with neural network verification engines, these certificates provide a formal guarantee that a DRL agent both achieves its goals and avoids unsafe behavior.","Furthermore, we introduce a technique for certificate filtering, which significantly simplifies the process of producing formally verified certificates.","We demonstrate the merits of our approach with a case study on providing safety and liveness guarantees for a DRL-controlled spacecraft."],"url":"http://arxiv.org/abs/2405.14058v1","category":"cs.AI"}
{"created":"2024-05-22 23:02:42","title":"Your Large Language Models Are Leaving Fingerprints","abstract":"It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features. We show how to visualize such fingerprints, describe how they can be used to detect machine-generated text and find that they are even robust across textual domains. We find that fingerprints are often persistent across models in the same model family (e.g. llama-13b vs. llama-65b) and that models fine-tuned for chat are easier to detect than standard language models, indicating that LLM fingerprints may be directly induced by the training data.","sentences":["It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data.","To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features.","We show how to visualize such fingerprints, describe how they can be used to detect machine-generated text and find that they are even robust across textual domains.","We find that fingerprints are often persistent across models in the same model family (e.g. llama-13b vs. llama-65b) and that models fine-tuned for chat are easier to detect than standard language models, indicating that LLM fingerprints may be directly induced by the training data."],"url":"http://arxiv.org/abs/2405.14057v1","category":"cs.CL"}
{"created":"2024-05-22 22:57:04","title":"How Many Bytes Can You Take Out Of Brain-To-Text Decoding?","abstract":"Brain-computer interfaces have promising medical and scientific applications for aiding speech and studying the brain. In this work, we propose an information-based evaluation metric for brain-to-text decoders. Using this metric, we examine two methods to augment existing state-of-the-art continuous text decoders. We show that these methods, in concert, can improve brain decoding performance by upwards of 40% when compared to a baseline model. We further examine the informatic properties of brain-to-text decoders and show empirically that they have Zipfian power law dynamics. Finally, we provide an estimate for the idealized performance of an fMRI-based text decoder. We compare this idealized model to our current model, and use our information-based metric to quantify the main sources of decoding error. We conclude that a practical brain-to-text decoder is likely possible given further algorithmic improvements.","sentences":["Brain-computer interfaces have promising medical and scientific applications for aiding speech and studying the brain.","In this work, we propose an information-based evaluation metric for brain-to-text decoders.","Using this metric, we examine two methods to augment existing state-of-the-art continuous text decoders.","We show that these methods, in concert, can improve brain decoding performance by upwards of 40% when compared to a baseline model.","We further examine the informatic properties of brain-to-text decoders and show empirically that they have Zipfian power law dynamics.","Finally, we provide an estimate for the idealized performance of an fMRI-based text decoder.","We compare this idealized model to our current model, and use our information-based metric to quantify the main sources of decoding error.","We conclude that a practical brain-to-text decoder is likely possible given further algorithmic improvements."],"url":"http://arxiv.org/abs/2405.14055v1","category":"cs.CL"}
{"created":"2024-05-22 22:41:56","title":"A Concentration Inequality for Maximum Mean Discrepancy (MMD)-based Statistics and Its Application in Generative Models","abstract":"Maximum Mean Discrepancy (MMD) is a probability metric that has found numerous applications in machine learning. In this work, we focus on its application in generative models, including the minimum MMD estimator, Generative Moment Matching Network (GMMN), and Generative Adversarial Network (GAN). In these cases, MMD is part of an objective function in a minimization or min-max optimization problem. Even if its empirical performance is competitive, the consistency and convergence rate analysis of the corresponding MMD-based estimators has yet to be carried out.   We propose a uniform concentration inequality for a class of Maximum Mean Discrepancy (MMD)-based estimators, that is, a maximum deviation bound of empirical MMD values over a collection of generated distributions and adversarially learned kernels. Here, our inequality serves as an efficient tool in the theoretical analysis for MMD-based generative models. As elaborating examples, we applied our main result to provide the generalization error bounds for the MMD-based estimators in the context of the minimum MMD estimator and MMD GAN.","sentences":["Maximum Mean Discrepancy (MMD) is a probability metric that has found numerous applications in machine learning.","In this work, we focus on its application in generative models, including the minimum MMD estimator, Generative Moment Matching Network (GMMN), and Generative Adversarial Network (GAN).","In these cases, MMD is part of an objective function in a minimization or min-max optimization problem.","Even if its empirical performance is competitive, the consistency and convergence rate analysis of the corresponding MMD-based estimators has yet to be carried out.   ","We propose a uniform concentration inequality for a class of Maximum Mean Discrepancy (MMD)-based estimators, that is, a maximum deviation bound of empirical MMD values over a collection of generated distributions and adversarially learned kernels.","Here, our inequality serves as an efficient tool in the theoretical analysis for MMD-based generative models.","As elaborating examples, we applied our main result to provide the generalization error bounds for the MMD-based estimators in the context of the minimum MMD estimator and MMD GAN."],"url":"http://arxiv.org/abs/2405.14051v1","category":"cs.LG"}
{"created":"2024-05-22 22:41:30","title":"Atomistic-informed phase field modeling of magnesium twin growth by disconnections","abstract":"The nucleation and propagation of disconnections play an essential role during twin growth. Atomistic methods can reveal such small structural features on twin facets and model their motion, yet are limited by the simulation length and time scales. Alternatively, mesoscale modeling approaches (such as the phase field method) address these constraints of atomistic simulations and can maintain atomic-level accuracy when integrated with atomic-level information. In this work, a phase field model is used to simulate the disconnection-mediated twinning, informed by molecular dynamics (MD) simulations. This work considers the specific case of the growth of $\\{10\\bar{1}2\\}$ twin in magnesium. MD simulations are first conducted to obtain the orientation-dependent interface mobility and motion threshold, and to simulate twin embryo growth and collect facet velocities, which can be used for calibrating the continuum model. The phase field disconnections model, based on the principle of minimum dissipation potential, provides the theoretical framework. This model incorporates a nonconvex grain boundary energy, elasticity and shear coupling, and simulates disconnections as a natural emergence under the elastic driving force. The phase field model is further optimized by including the anisotropic interface mobility and motion threshold suggested by MD simulations. Results agree with MD simulations of twin embryo growth in the aspects of final twin thickness, twin shape, and twin size, as well as the kinetic behavior of twin boundaries and twin tips. The simulated twin microstructure is also consistent with experimental observations, demonstrating the fidelity of the model.","sentences":["The nucleation and propagation of disconnections play an essential role during twin growth.","Atomistic methods can reveal such small structural features on twin facets and model their motion, yet are limited by the simulation length and time scales.","Alternatively, mesoscale modeling approaches (such as the phase field method) address these constraints of atomistic simulations and can maintain atomic-level accuracy when integrated with atomic-level information.","In this work, a phase field model is used to simulate the disconnection-mediated twinning, informed by molecular dynamics (MD) simulations.","This work considers the specific case of the growth of $\\{10\\bar{1}2\\}$ twin in magnesium.","MD simulations are first conducted to obtain the orientation-dependent interface mobility and motion threshold, and to simulate twin embryo growth and collect facet velocities, which can be used for calibrating the continuum model.","The phase field disconnections model, based on the principle of minimum dissipation potential, provides the theoretical framework.","This model incorporates a nonconvex grain boundary energy, elasticity and shear coupling, and simulates disconnections as a natural emergence under the elastic driving force.","The phase field model is further optimized by including the anisotropic interface mobility and motion threshold suggested by MD simulations.","Results agree with MD simulations of twin embryo growth in the aspects of final twin thickness, twin shape, and twin size, as well as the kinetic behavior of twin boundaries and twin tips.","The simulated twin microstructure is also consistent with experimental observations, demonstrating the fidelity of the model."],"url":"http://arxiv.org/abs/2405.14050v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-22 22:22:25","title":"Trajectory Volatility for Out-of-Distribution Detection in Mathematical Reasoning","abstract":"Real-world data deviating from the independent and identically distributed (i.i.d.) assumption of in-distribution training data poses security threats to deep networks, thus advancing out-of-distribution (OOD) detection algorithms. Detection methods in generative language models (GLMs) mainly focus on uncertainty estimation and embedding distance measurement, with the latter proven to be most effective in traditional linguistic tasks like summarization and translation. However, another complex generative scenario mathematical reasoning poses significant challenges to embedding-based methods due to its high-density feature of output spaces, but this feature causes larger discrepancies in the embedding shift trajectory between different samples in latent spaces. Hence, we propose a trajectory-based method TV score, which uses trajectory volatility for OOD detection in mathematical reasoning. Experiments show that our method outperforms all traditional algorithms on GLMs under mathematical reasoning scenarios and can be extended to more applications with high-density features in output spaces, such as multiple-choice questions.","sentences":["Real-world data deviating from the independent and identically distributed (i.i.d.) assumption of in-distribution training data poses security threats to deep networks, thus advancing out-of-distribution (OOD) detection algorithms.","Detection methods in generative language models (GLMs) mainly focus on uncertainty estimation and embedding distance measurement, with the latter proven to be most effective in traditional linguistic tasks like summarization and translation.","However, another complex generative scenario mathematical reasoning poses significant challenges to embedding-based methods due to its high-density feature of output spaces, but this feature causes larger discrepancies in the embedding shift trajectory between different samples in latent spaces.","Hence, we propose a trajectory-based method TV score, which uses trajectory volatility for OOD detection in mathematical reasoning.","Experiments show that our method outperforms all traditional algorithms on GLMs under mathematical reasoning scenarios and can be extended to more applications with high-density features in output spaces, such as multiple-choice questions."],"url":"http://arxiv.org/abs/2405.14039v1","category":"cs.CL"}
{"created":"2024-05-22 22:10:40","title":"Remote Keylogging Attacks in Multi-user VR Applications","abstract":"As Virtual Reality (VR) applications grow in popularity, they have bridged distances and brought users closer together. However, with this growth, there have been increasing concerns about security and privacy, especially related to the motion data used to create immersive experiences. In this study, we highlight a significant security threat in multi-user VR applications, which are applications that allow multiple users to interact with each other in the same virtual space. Specifically, we propose a remote attack that utilizes the avatar rendering information collected from an adversary's game clients to extract user-typed secrets like credit card information, passwords, or private conversations. We do this by (1) extracting motion data from network packets, and (2) mapping motion data to keystroke entries. We conducted a user study to verify the attack's effectiveness, in which our attack successfully inferred 97.62% of the keystrokes. Besides, we performed an additional experiment to underline that our attack is practical, confirming its effectiveness even when (1) there are multiple users in a room, and (2) the attacker cannot see the victims. Moreover, we replicated our proposed attack on four applications to demonstrate the generalizability of the attack. These results underscore the severity of the vulnerability and its potential impact on millions of VR social platform users.","sentences":["As Virtual Reality (VR) applications grow in popularity, they have bridged distances and brought users closer together.","However, with this growth, there have been increasing concerns about security and privacy, especially related to the motion data used to create immersive experiences.","In this study, we highlight a significant security threat in multi-user VR applications, which are applications that allow multiple users to interact with each other in the same virtual space.","Specifically, we propose a remote attack that utilizes the avatar rendering information collected from an adversary's game clients to extract user-typed secrets like credit card information, passwords, or private conversations.","We do this by (1) extracting motion data from network packets, and (2) mapping motion data to keystroke entries.","We conducted a user study to verify the attack's effectiveness, in which our attack successfully inferred 97.62% of the keystrokes.","Besides, we performed an additional experiment to underline that our attack is practical, confirming its effectiveness even when (1) there are multiple users in a room, and (2) the attacker cannot see the victims.","Moreover, we replicated our proposed attack on four applications to demonstrate the generalizability of the attack.","These results underscore the severity of the vulnerability and its potential impact on millions of VR social platform users."],"url":"http://arxiv.org/abs/2405.14036v1","category":"cs.CR"}
{"created":"2024-05-22 22:09:32","title":"Generative AI Search Engines as Arbiters of Public Knowledge: An Audit of Bias and Authority","abstract":"This paper reports on an audit study of generative AI systems (ChatGPT, Bing Chat, and Perplexity) which investigates how these new search engines construct responses and establish authority for topics of public importance. We collected system responses using a set of 48 authentic queries for 4 topics over a 7-day period and analyzed the data using sentiment analysis, inductive coding and source classification. Results provide an overview of the nature of system responses across these systems and provide evidence of sentiment bias based on the queries and topics, and commercial and geographic bias in sources. The quality of sources used to support claims is uneven, relying heavily on News and Media, Business and Digital Media websites. Implications for system users emphasize the need to critically examine Generative AI system outputs when making decisions related to public interest and personal well-being.","sentences":["This paper reports on an audit study of generative AI systems (ChatGPT, Bing Chat, and Perplexity) which investigates how these new search engines construct responses and establish authority for topics of public importance.","We collected system responses using a set of 48 authentic queries for 4 topics over a 7-day period and analyzed the data using sentiment analysis, inductive coding and source classification.","Results provide an overview of the nature of system responses across these systems and provide evidence of sentiment bias based on the queries and topics, and commercial and geographic bias in sources.","The quality of sources used to support claims is uneven, relying heavily on News and Media, Business and Digital Media websites.","Implications for system users emphasize the need to critically examine Generative AI system outputs when making decisions related to public interest and personal well-being."],"url":"http://arxiv.org/abs/2405.14034v1","category":"cs.IR"}
{"created":"2024-05-22 22:04:54","title":"Energy-efficient predictive control for connected, automated driving under localization uncertainty","abstract":"This paper presents a data-driven Model Predictive Control (MPC) for energy-efficient urban road driving for connected, automated vehicles. The proposed MPC aims to minimize total energy consumption by controlling the vehicle's longitudinal motion on roads with traffic lights and preceding vehicles. Its terminal cost function and terminal constraints are learned from data, which consists of the closed-loop state and input trajectories. The terminal cost function represents the remaining energy-to-spend starting from a given terminal state. The terminal constraints are designed to ensure that the controlled vehicle timely crosses the upcoming traffic light, adheres to traffic laws, and accounts for the preceding vehicles. We validate the effectiveness of our method through both simulations and real-world vehicle experiments, demonstrating $\\textbf{19\\%}$ improvement in average energy consumption compared to conventional approaches that involve solving a long-horizon optimal control problem for speed planning and employing a separate controller for speed tracking.","sentences":["This paper presents a data-driven Model Predictive Control (MPC) for energy-efficient urban road driving for connected, automated vehicles.","The proposed MPC aims to minimize total energy consumption by controlling the vehicle's longitudinal motion on roads with traffic lights and preceding vehicles.","Its terminal cost function and terminal constraints are learned from data, which consists of the closed-loop state and input trajectories.","The terminal cost function represents the remaining energy-to-spend starting from a given terminal state.","The terminal constraints are designed to ensure that the controlled vehicle timely crosses the upcoming traffic light, adheres to traffic laws, and accounts for the preceding vehicles.","We validate the effectiveness of our method through both simulations and real-world vehicle experiments, demonstrating $\\textbf{19\\%}$ improvement in average energy consumption compared to conventional approaches that involve solving a long-horizon optimal control problem for speed planning and employing a separate controller for speed tracking."],"url":"http://arxiv.org/abs/2405.14031v1","category":"eess.SY"}
{"created":"2024-05-22 21:59:46","title":"Two Heads are Better Than One: Neural Networks Quantization with 2D Hilbert Curve-based Output Representation","abstract":"Quantization is widely used to increase deep neural networks' (DNN) memory, computation, and power efficiency. Various techniques, such as post-training quantization and quantization-aware training, have been proposed to improve quantization quality. We introduce a novel approach for DNN quantization that uses a redundant representation of DNN's output. We represent the target quantity as a point on a 2D parametric curve. The DNN model is modified to predict 2D points that are mapped back to the target quantity at a post-processing stage. We demonstrate that this mapping can reduce quantization error. For the low-order parametric Hilbert curve, Depth-From-Stereo task, and two models represented by U-Net architecture and vision transformer, we achieved a quantization error reduction by about 5 times for the INT8 model at both CPU and DSP delegates. This gain comes with a minimal inference time increase (less than 7%). Our approach can be applied to other tasks, including segmentation, object detection, and key-points prediction.","sentences":["Quantization is widely used to increase deep neural networks' (DNN) memory, computation, and power efficiency.","Various techniques, such as post-training quantization and quantization-aware training, have been proposed to improve quantization quality.","We introduce a novel approach for DNN quantization that uses a redundant representation of DNN's output.","We represent the target quantity as a point on a 2D parametric curve.","The DNN model is modified to predict 2D points that are mapped back to the target quantity at a post-processing stage.","We demonstrate that this mapping can reduce quantization error.","For the low-order parametric Hilbert curve, Depth-From-Stereo task, and two models represented by U-Net architecture and vision transformer, we achieved a quantization error reduction by about 5 times for the INT8 model at both CPU and DSP delegates.","This gain comes with a minimal inference time increase (less than 7%).","Our approach can be applied to other tasks, including segmentation, object detection, and key-points prediction."],"url":"http://arxiv.org/abs/2405.14024v1","category":"cs.CV"}
{"created":"2024-05-22 21:54:05","title":"Unlearning Information Bottleneck: Machine Unlearning of Systematic Patterns and Biases","abstract":"Effective adaptation to distribution shifts in training data is pivotal for sustaining robustness in neural networks, especially when removing specific biases or outdated information, a process known as machine unlearning. Traditional approaches typically assume that data variations are random, which makes it difficult to adjust the model parameters accurately to remove patterns and characteristics from unlearned data. In this work, we present Unlearning Information Bottleneck (UIB), a novel information-theoretic framework designed to enhance the process of machine unlearning that effectively leverages the influence of systematic patterns and biases for parameter adjustment. By proposing a variational upper bound, we recalibrate the model parameters through a dynamic prior that integrates changes in data distribution with an affordable computational cost, allowing efficient and accurate removal of outdated or unwanted data patterns and biases. Our experiments across various datasets, models, and unlearning methods demonstrate that our approach effectively removes systematic patterns and biases while maintaining the performance of models post-unlearning.","sentences":["Effective adaptation to distribution shifts in training data is pivotal for sustaining robustness in neural networks, especially when removing specific biases or outdated information, a process known as machine unlearning.","Traditional approaches typically assume that data variations are random, which makes it difficult to adjust the model parameters accurately to remove patterns and characteristics from unlearned data.","In this work, we present Unlearning Information Bottleneck (UIB), a novel information-theoretic framework designed to enhance the process of machine unlearning that effectively leverages the influence of systematic patterns and biases for parameter adjustment.","By proposing a variational upper bound, we recalibrate the model parameters through a dynamic prior that integrates changes in data distribution with an affordable computational cost, allowing efficient and accurate removal of outdated or unwanted data patterns and biases.","Our experiments across various datasets, models, and unlearning methods demonstrate that our approach effectively removes systematic patterns and biases while maintaining the performance of models post-unlearning."],"url":"http://arxiv.org/abs/2405.14020v1","category":"cs.LG"}
{"created":"2024-05-22 21:49:28","title":"Towards a Unified Framework for Evaluating Explanations","abstract":"The challenge of creating interpretable models has been taken up by two main research communities: ML researchers primarily focused on lower-level explainability methods that suit the needs of engineers, and HCI researchers who have more heavily emphasized user-centered approaches often based on participatory design methods. This paper reviews how these communities have evaluated interpretability, identifying overlaps and semantic misalignments. We propose moving towards a unified framework of evaluation criteria and lay the groundwork for such a framework by articulating the relationships between existing criteria. We argue that explanations serve as mediators between models and stakeholders, whether for intrinsically interpretable models or opaque black-box models analyzed via post-hoc techniques. We further argue that useful explanations require both faithfulness and intelligibility. Explanation plausibility is a prerequisite for intelligibility, while stability is a prerequisite for explanation faithfulness. We illustrate these criteria, as well as specific evaluation methods, using examples from an ongoing study of an interpretable neural network for predicting a particular learner behavior.","sentences":["The challenge of creating interpretable models has been taken up by two main research communities: ML researchers primarily focused on lower-level explainability methods that suit the needs of engineers, and HCI researchers who have more heavily emphasized user-centered approaches often based on participatory design methods.","This paper reviews how these communities have evaluated interpretability, identifying overlaps and semantic misalignments.","We propose moving towards a unified framework of evaluation criteria and lay the groundwork for such a framework by articulating the relationships between existing criteria.","We argue that explanations serve as mediators between models and stakeholders, whether for intrinsically interpretable models or opaque black-box models analyzed via post-hoc techniques.","We further argue that useful explanations require both faithfulness and intelligibility.","Explanation plausibility is a prerequisite for intelligibility, while stability is a prerequisite for explanation faithfulness.","We illustrate these criteria, as well as specific evaluation methods, using examples from an ongoing study of an interpretable neural network for predicting a particular learner behavior."],"url":"http://arxiv.org/abs/2405.14016v1","category":"cs.LG"}
{"created":"2024-05-22 21:48:17","title":"RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar","abstract":"3D occupancy-based perception pipeline has significantly advanced autonomous driving by capturing detailed scene descriptions and demonstrating strong generalizability across various object categories and shapes. Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars. To improve perception robustness, we leverage the recent advances in automotive radars and introduce a novel approach that utilizes 4D imaging radar sensors for 3D occupancy prediction. Our method, RadarOcc, circumvents the limitations of sparse radar point clouds by directly processing the 4D radar tensor, thus preserving essential scene details. RadarOcc innovatively addresses the challenges associated with the voluminous and noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware spatial sparsification, and range-wise self-attention mechanisms. To minimize the interpolation errors associated with direct coordinate transformations, we also devise a spherical-based feature encoding followed by spherical-to-Cartesian feature aggregation. We benchmark various baseline methods based on distinct modalities on the public K-Radar dataset. The results demonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy prediction and promising results even when compared with LiDAR- or camera-based methods. Additionally, we present qualitative evidence of the superior performance of 4D radar in adverse weather conditions and explore the impact of key pipeline components through ablation studies.","sentences":["3D occupancy-based perception pipeline has significantly advanced autonomous driving by capturing detailed scene descriptions and demonstrating strong generalizability across various object categories and shapes.","Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction.","These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars.","To improve perception robustness, we leverage the recent advances in automotive radars and introduce a novel approach that utilizes 4D imaging radar sensors for 3D occupancy prediction.","Our method, RadarOcc, circumvents the limitations of sparse radar point clouds by directly processing the 4D radar tensor, thus preserving essential scene details.","RadarOcc innovatively addresses the challenges associated with the voluminous and noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware spatial sparsification, and range-wise self-attention mechanisms.","To minimize the interpolation errors associated with direct coordinate transformations, we also devise a spherical-based feature encoding followed by spherical-to-Cartesian feature aggregation.","We benchmark various baseline methods based on distinct modalities on the public K-Radar dataset.","The results demonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy prediction and promising results even when compared with LiDAR- or camera-based methods.","Additionally, we present qualitative evidence of the superior performance of 4D radar in adverse weather conditions and explore the impact of key pipeline components through ablation studies."],"url":"http://arxiv.org/abs/2405.14014v1","category":"cs.CV"}
{"created":"2024-05-22 21:40:34","title":"Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large Language Models","abstract":"In applications such as personal assistants, large language models (LLMs) must consider the user's personal information and preferences. However, LLMs lack the inherent ability to learn from user interactions. This paper explores capturing personal information from user prompts using ontology and knowledge-graph approaches. We use a subset of the KNOW ontology, which models personal information, to train the language model on these concepts. We then evaluate the success of knowledge capture using a specially constructed dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTODSKC","sentences":["In applications such as personal assistants, large language models (LLMs) must consider the user's personal information and preferences.","However, LLMs lack the inherent ability to learn from user interactions.","This paper explores capturing personal information from user prompts using ontology and knowledge-graph approaches.","We use a subset of the KNOW ontology, which models personal information, to train the language model on these concepts.","We then evaluate the success of knowledge capture using a specially constructed dataset.","Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTODSKC"],"url":"http://arxiv.org/abs/2405.14012v1","category":"cs.AI"}
{"created":"2024-05-22 21:38:38","title":"Nonlinear Response Theory of Molecular Machines","abstract":"Chemical affinities are responsible for driving active matter systems out of equilibrium. At the nano-scale, molecular machines interact with the surrounding environment and are subjected to external forces. The mechano-chemical coupling which arises naturally in these systems reveals a complex interplay between chemical and mechanical degrees of freedom with strong impact on their active mechanism. By considering various models far from equilibrium, we show that the tuning of applied forces give rise to a nonlinear response that causes a non-monotonic behaviour in the machines' activity. Our findings have implications in understanding, designing, and triggering such processes by controlled application of external fields, including the collective dynamics of larger non-equilibrium systems where the total dissipation and performance might be affected by internal and inter-particle interactions.","sentences":["Chemical affinities are responsible for driving active matter systems out of equilibrium.","At the nano-scale, molecular machines interact with the surrounding environment and are subjected to external forces.","The mechano-chemical coupling which arises naturally in these systems reveals a complex interplay between chemical and mechanical degrees of freedom with strong impact on their active mechanism.","By considering various models far from equilibrium, we show that the tuning of applied forces give rise to a nonlinear response that causes a non-monotonic behaviour in the machines' activity.","Our findings have implications in understanding, designing, and triggering such processes by controlled application of external fields, including the collective dynamics of larger non-equilibrium systems where the total dissipation and performance might be affected by internal and inter-particle interactions."],"url":"http://arxiv.org/abs/2405.14011v1","category":"cond-mat.soft"}
{"created":"2024-05-22 21:22:51","title":"Evaluating Large Language Models with Human Feedback: Establishing a Swedish Benchmark","abstract":"In the rapidly evolving field of artificial intelligence, large language models (LLMs) have demonstrated significant capabilities across numerous applications. However, the performance of these models in languages with fewer resources, such as Swedish, remains under-explored. This study introduces a comprehensive human benchmark to assess the efficacy of prominent LLMs in understanding and generating Swedish language texts using forced choice ranking. We employ a modified version of the ChatbotArena benchmark, incorporating human feedback to evaluate eleven different models, including GPT-4, GPT-3.5, various Claude and Llama models, and bespoke models like Dolphin-2.9-llama3b-8b-flashback and BeagleCatMunin. These models were chosen based on their performance on LMSYS chatbot arena and the Scandeval benchmarks. We release the chatbotarena.se benchmark as a tool to improve our understanding of language model performance in Swedish with the hopes that it will be widely used. We aim to create a leaderboard once sufficient data has been collected and analysed.","sentences":["In the rapidly evolving field of artificial intelligence, large language models (LLMs) have demonstrated significant capabilities across numerous applications.","However, the performance of these models in languages with fewer resources, such as Swedish, remains under-explored.","This study introduces a comprehensive human benchmark to assess the efficacy of prominent LLMs in understanding and generating Swedish language texts using forced choice ranking.","We employ a modified version of the ChatbotArena benchmark, incorporating human feedback to evaluate eleven different models, including GPT-4, GPT-3.5, various Claude and Llama models, and bespoke models like Dolphin-2.9-llama3b-8b-flashback and BeagleCatMunin.","These models were chosen based on their performance on LMSYS chatbot arena and the Scandeval benchmarks.","We release the chatbotarena.se benchmark as a tool to improve our understanding of language model performance in Swedish with the hopes that it will be widely used.","We aim to create a leaderboard once sufficient data has been collected and analysed."],"url":"http://arxiv.org/abs/2405.14006v1","category":"cs.CL"}
{"created":"2024-05-22 21:19:35","title":"Towards A Comprehensive Assessment of AI's Environmental Impact","abstract":"Artificial Intelligence, machine learning (AI/ML) has allowed exploring solutions for a variety of environmental and climate questions ranging from natural disasters, greenhouse gas emission, monitoring biodiversity, agriculture, to weather and climate modeling, enabling progress towards climate change mitigation. However, the intersection of AI/ML and environment is not always positive. The recent surge of interest in ML, made possible by processing very large volumes of data, fueled by access to massive compute power, has sparked a trend towards large-scale adoption of AI/ML. This interest places tremendous pressure on natural resources, that are often overlooked and under-reported. There is a need for a framework that monitors the environmental impact and degradation from AI/ML throughout its lifecycle for informing policymakers, stakeholders to adequately implement standards and policies and track the policy outcome over time. For these policies to be effective, AI's environmental impact needs to be monitored in a spatially-disaggregated, timely manner across the globe at the key activity sites. This study proposes a methodology to track environmental variables relating to the multifaceted impact of AI around datacenters using openly available energy data and globally acquired satellite observations. We present a case study around Northern Virginia, United States that hosts a growing number of datacenters and observe changes in multiple satellite-based environmental metrics. We then discuss the steps to expand this methodology for comprehensive assessment of AI's environmental impact across the planet. We also identify data gaps and formulate recommendations for improving the understanding and monitoring AI-induced changes to the environment and climate.","sentences":["Artificial Intelligence, machine learning (AI/ML) has allowed exploring solutions for a variety of environmental and climate questions ranging from natural disasters, greenhouse gas emission, monitoring biodiversity, agriculture, to weather and climate modeling, enabling progress towards climate change mitigation.","However, the intersection of AI/ML and environment is not always positive.","The recent surge of interest in ML, made possible by processing very large volumes of data, fueled by access to massive compute power, has sparked a trend towards large-scale adoption of AI/ML.","This interest places tremendous pressure on natural resources, that are often overlooked and under-reported.","There is a need for a framework that monitors the environmental impact and degradation from AI/ML throughout its lifecycle for informing policymakers, stakeholders to adequately implement standards and policies and track the policy outcome over time.","For these policies to be effective, AI's environmental impact needs to be monitored in a spatially-disaggregated, timely manner across the globe at the key activity sites.","This study proposes a methodology to track environmental variables relating to the multifaceted impact of AI around datacenters using openly available energy data and globally acquired satellite observations.","We present a case study around Northern Virginia, United States that hosts a growing number of datacenters and observe changes in multiple satellite-based environmental metrics.","We then discuss the steps to expand this methodology for comprehensive assessment of AI's environmental impact across the planet.","We also identify data gaps and formulate recommendations for improving the understanding and monitoring AI-induced changes to the environment and climate."],"url":"http://arxiv.org/abs/2405.14004v1","category":"cs.CY"}
{"created":"2024-05-23 17:59:43","title":"Borromean states in a one-dimensional three-body system","abstract":"We show the existence of Borromean bound states in a one-dimensional quantum three-body system composed of two identical bosons and a distinguishable particle. It is assumed that there is no interaction between the two bosons, while the mass-imbalanced two-body subsystems can be tuned to be either bound or unbound. Within the framework of the Faddeev equations, the three-body spectrum and the corresponding wave functions are computed numerically. In addition, we identify the parameter-space region for the two-body interaction, where the Borromean states occur, evaluate their geometric properties, and investigate their dependence on the mass ratio.","sentences":["We show the existence of Borromean bound states in a one-dimensional quantum three-body system composed of two identical bosons and a distinguishable particle.","It is assumed that there is no interaction between the two bosons, while the mass-imbalanced two-body subsystems can be tuned to be either bound or unbound.","Within the framework of the Faddeev equations, the three-body spectrum and the corresponding wave functions are computed numerically.","In addition, we identify the parameter-space region for the two-body interaction, where the Borromean states occur, evaluate their geometric properties, and investigate their dependence on the mass ratio."],"url":"http://arxiv.org/abs/2405.14865v1","category":"quant-ph"}
{"created":"2024-05-23 17:58:00","title":"The Channel Capacity of a Relativistic String","abstract":"I explore the limitations on the capacity of a relativistic channel to transmit power and information that arise because of the finiteness of the transverse speed of light. As a model system, I consider a rope constructed from a fundamental string, for which relativistic invariance is built in. By wiggling one end of the string, both power and information may be transmitted to the other end. I argue that even though an unbounded amount of power and information may be traveling down the string, there is a bound on how much may be transmitted. Further, I conjecture that the two kinds of channel capacity -- power and information -- interfere with each other, so that the only way to transmit the maximum amount of power is to send no information, and vice versa.","sentences":["I explore the limitations on the capacity of a relativistic channel to transmit power and information that arise because of the finiteness of the transverse speed of light.","As a model system, I consider a rope constructed from a fundamental string, for which relativistic invariance is built in.","By wiggling one end of the string, both power and information may be transmitted to the other end.","I argue that even though an unbounded amount of power and information may be traveling down the string, there is a bound on how much may be transmitted.","Further, I conjecture that the two kinds of channel capacity -- power and information -- interfere with each other, so that the only way to transmit the maximum amount of power is to send no information, and vice versa."],"url":"http://arxiv.org/abs/2405.14856v1","category":"hep-th"}
{"created":"2024-05-23 17:56:47","title":"Floer-theoretic filtration on Painlev\u00e9 Hitchin systems","abstract":"We classify equivariant $\\mathbb{C}^*$-actions on moduli spaces of Higgs bundles corresponding to the Painlev\\'e equations. We deduce the Floer-theoretic filtrations on the cohomology of these spaces, introduced by Ritter and the second author in arXiv:2304.13026. Filtration is then compared with the $``P=W\"$ and the filtration obtained by multiplicities of the irreducible components of the nilpotent cone, for all the 2-dimensional Higgs moduli.","sentences":["We classify equivariant $\\mathbb{C}^*$-actions on moduli spaces of Higgs bundles corresponding to the Painlev\\'e equations.","We deduce the Floer-theoretic filtrations on the cohomology of these spaces, introduced by Ritter and the second author in arXiv:2304.13026.","Filtration is then compared with the $``P=W\"$ and the filtration obtained by multiplicities of the irreducible components of the nilpotent cone, for all the 2-dimensional Higgs moduli."],"url":"http://arxiv.org/abs/2405.14850v1","category":"math.AG"}
{"created":"2024-05-23 17:56:38","title":"Local Causal Discovery for Structural Evidence of Direct Discrimination","abstract":"Fairness is a critical objective in policy design and algorithmic decision-making. Identifying the causal pathways of unfairness requires knowledge of the underlying structural causal model, which may be incomplete or unavailable. This limits the practicality of causal fairness analysis in complex or low-knowledge domains. To mitigate this practicality gap, we advocate for developing efficient causal discovery methods for fairness applications. To this end, we introduce local discovery for direct discrimination (LD3): a polynomial-time algorithm that recovers structural evidence of direct discrimination. LD3 performs a linear number of conditional independence tests with respect to variable set size. Moreover, we propose a graphical criterion for identifying the weighted controlled direct effect (CDE), a qualitative measure of direct discrimination. We prove that this criterion is satisfied by the knowledge returned by LD3, increasing the accessibility of the weighted CDE as a causal fairness measure. Taking liver transplant allocation as a case study, we highlight the potential impact of LD3 for modeling fairness in complex decision systems. Results on real-world data demonstrate more plausible causal relations than baselines, which took 197x to 5870x longer to execute.","sentences":["Fairness is a critical objective in policy design and algorithmic decision-making.","Identifying the causal pathways of unfairness requires knowledge of the underlying structural causal model, which may be incomplete or unavailable.","This limits the practicality of causal fairness analysis in complex or low-knowledge domains.","To mitigate this practicality gap, we advocate for developing efficient causal discovery methods for fairness applications.","To this end, we introduce local discovery for direct discrimination (LD3): a polynomial-time algorithm that recovers structural evidence of direct discrimination.","LD3 performs a linear number of conditional independence tests with respect to variable set size.","Moreover, we propose a graphical criterion for identifying the weighted controlled direct effect (CDE), a qualitative measure of direct discrimination.","We prove that this criterion is satisfied by the knowledge returned by LD3, increasing the accessibility of the weighted CDE as a causal fairness measure.","Taking liver transplant allocation as a case study, we highlight the potential impact of LD3 for modeling fairness in complex decision systems.","Results on real-world data demonstrate more plausible causal relations than baselines, which took 197x to 5870x longer to execute."],"url":"http://arxiv.org/abs/2405.14848v1","category":"stat.ML"}
{"created":"2024-05-23 17:56:17","title":"Semiclassical analysis on principal bundles","abstract":"Let $G$ be a compact Lie group. We introduce a semiclassical framework, called Borel-Weil calculus, to investigate $G$-equivariant (pseudo)differential operators acting on $G$-principal bundles over closed manifolds. In this calculus, the semiclassical parameters correspond to the highest roots in the Weyl chamber of the group $G$ that parametrize irreducible representations, and operators are pseudodifferential in the base variable, with values in Toeplitz operators on the flag manifold associated to the group. This monograph unfolds two main applications of our calculus.   Firstly, in the realm of dynamical systems, we obtain explicit sufficient conditions for rapid mixing of volume-preserving partially hyperbolic flows obtained as extensions of an Anosov flow to a $G$-principal bundle (for an arbitrary $G$). In particular, when $G = \\mathrm{U}(1)$, we prove that the flow on the extension is rapid mixing whenever the Anosov flow is not jointly integrable, and the circle bundle is not torsion. When $G$ is semisimple, we prove that ergodicity of the extension is equivalent to rapid mixing. Secondly, we study the spectral theory of sub-elliptic Laplacians obtained as horizontal Laplacians of a $G$-equivariant connection on a principal bundle. When $G$ is semisimple, we prove that the horizontal Laplacian is globally hypoelliptic as soon as the connection has a dense holonomy group in $G$. Notably, this result encompasses all flat bundles with a dense monodromy group in $G$. We also prove a quantum ergodicity result for flat principal bundles with dense holonomy group if the base space has Anosov geodesic flow (e.g. in negative sectional curvature).   We believe that this monograph will serve as a cornerstone for future investigations applying the Borel-Weil calculus across different fields.","sentences":["Let $G$ be a compact Lie group.","We introduce a semiclassical framework, called Borel-Weil calculus, to investigate $G$-equivariant (pseudo)differential operators acting on $G$-principal bundles over closed manifolds.","In this calculus, the semiclassical parameters correspond to the highest roots in the Weyl chamber of the group $G$ that parametrize irreducible representations, and operators are pseudodifferential in the base variable, with values in Toeplitz operators on the flag manifold associated to the group.","This monograph unfolds two main applications of our calculus.   ","Firstly, in the realm of dynamical systems, we obtain explicit sufficient conditions for rapid mixing of volume-preserving partially hyperbolic flows obtained as extensions of an Anosov flow to a $G$-principal bundle (for an arbitrary $G$).","In particular, when $G = \\mathrm{U}(1)$, we prove that the flow on the extension is rapid mixing whenever the Anosov flow is not jointly integrable, and the circle bundle is not torsion.","When $G$ is semisimple, we prove that ergodicity of the extension is equivalent to rapid mixing.","Secondly, we study the spectral theory of sub-elliptic Laplacians obtained as horizontal Laplacians of a $G$-equivariant connection on a principal bundle.","When $G$ is semisimple, we prove that the horizontal Laplacian is globally hypoelliptic as soon as the connection has a dense holonomy group in $G$. Notably, this result encompasses all flat bundles with a dense monodromy group in $G$. We also prove a quantum ergodicity result for flat principal bundles with dense holonomy group if the base space has Anosov geodesic flow (e.g. in negative sectional curvature).   ","We believe that this monograph will serve as a cornerstone for future investigations applying the Borel-Weil calculus across different fields."],"url":"http://arxiv.org/abs/2405.14846v1","category":"math.AP"}
{"created":"2024-05-23 17:55:11","title":"Learning to Detect and Segment Mobile Objects from Unlabeled Videos","abstract":"Embodied agents must detect and localize objects of interest, e.g. traffic participants for self-driving cars. Supervision in the form of bounding boxes for this task is extremely expensive. As such, prior work has looked at unsupervised object segmentation, but in the absence of annotated boxes, it is unclear how pixels must be grouped into objects and which objects are of interest. This results in over- / under-segmentation and irrelevant objects. Inspired both by the human visual system and by practical applications, we posit that the key missing cue is motion: objects of interest are typically mobile objects. We propose MOD-UV, a Mobile Object Detector learned from Unlabeled Videos only. We begin with pseudo-labels derived from motion segmentation, but introduce a novel training paradigm to progressively discover small objects and static-but-mobile objects that are missed by motion segmentation. As a result, though only learned from unlabeled videos, MOD-UV can detect and segment mobile objects from a single static image. Empirically, we achieve state-of-the-art performance in unsupervised mobile object detection on Waymo Open, nuScenes, and KITTI Dataset without using any external data or supervised models. Code is publicly available at https://github.com/YihongSun/MOD-UV.","sentences":["Embodied agents must detect and localize objects of interest, e.g. traffic participants for self-driving cars.","Supervision in the form of bounding boxes for this task is extremely expensive.","As such, prior work has looked at unsupervised object segmentation, but in the absence of annotated boxes, it is unclear how pixels must be grouped into objects and which objects are of interest.","This results in over- / under-segmentation and irrelevant objects.","Inspired both by the human visual system and by practical applications, we posit that the key missing cue is motion: objects of interest are typically mobile objects.","We propose MOD-UV, a Mobile Object Detector learned from Unlabeled Videos only.","We begin with pseudo-labels derived from motion segmentation, but introduce a novel training paradigm to progressively discover small objects and static-but-mobile objects that are missed by motion segmentation.","As a result, though only learned from unlabeled videos, MOD-UV can detect and segment mobile objects from a single static image.","Empirically, we achieve state-of-the-art performance in unsupervised mobile object detection on Waymo Open, nuScenes, and KITTI Dataset without using any external data or supervised models.","Code is publicly available at https://github.com/YihongSun/MOD-UV."],"url":"http://arxiv.org/abs/2405.14841v1","category":"cs.CV"}
{"created":"2024-05-23 17:46:49","title":"Deep learning lattice gauge theories","abstract":"Monte Carlo methods have led to profound insights into the strong-coupling behaviour of lattice gauge theories and produced remarkable results such as first-principles computations of hadron masses. Despite tremendous progress over the last four decades, fundamental challenges such as the sign problem and the inability to simulate real-time dynamics remain. Neural network quantum states have emerged as an alternative method that seeks to overcome these challenges. In this work, we use gauge-invariant neural network quantum states to accurately compute the ground state of $\\mathbb{Z}_N$ lattice gauge theories in $2+1$ dimensions. Using transfer learning, we study the distinct topological phases and the confinement phase transition of these theories. For $\\mathbb{Z}_2$, we identify a continuous transition and compute critical exponents, finding excellent agreement with existing numerics for the expected Ising universality class. In the $\\mathbb{Z}_3$ case, we observe a weakly first-order transition and identify the critical coupling. Our findings suggest that neural network quantum states are a promising method for precise studies of lattice gauge theory.","sentences":["Monte Carlo methods have led to profound insights into the strong-coupling behaviour of lattice gauge theories and produced remarkable results such as first-principles computations of hadron masses.","Despite tremendous progress over the last four decades, fundamental challenges such as the sign problem and the inability to simulate real-time dynamics remain.","Neural network quantum states have emerged as an alternative method that seeks to overcome these challenges.","In this work, we use gauge-invariant neural network quantum states to accurately compute the ground state of $\\mathbb{Z}_N$ lattice gauge theories in $2+1$ dimensions.","Using transfer learning, we study the distinct topological phases and the confinement phase transition of these theories.","For $\\mathbb{Z}_2$, we identify a continuous transition and compute critical exponents, finding excellent agreement with existing numerics for the expected Ising universality class.","In the $\\mathbb{Z}_3$ case, we observe a weakly first-order transition and identify the critical coupling.","Our findings suggest that neural network quantum states are a promising method for precise studies of lattice gauge theory."],"url":"http://arxiv.org/abs/2405.14830v1","category":"hep-lat"}
{"created":"2024-05-23 17:41:51","title":"Lindbladian way for the relaxation time approximation, application to Kibble-Zurek processes due to environment temperature quench, and to Lindbladian perturbation theory","abstract":"In the present paper, a global Lindbladian ansatz is constructed which leads to thermalization at temperature $T$ to the Gibs state of the investigated system. This ansatz connects every two eigenstates of the Hamiltonian and leads to a simple master equation known in the literature as the relaxation time approximation (RTA). The main message of this paper is that RTA, being a Lindbladian approach itself, can be used as Lindbladian securing thermalization when modeling physical processes, and can be consequently combined with other types of Lindbladians which would drive the system of the equilibrium state. I demonstrate it with two applications. The first application is the slow cooling (or heating) of quantum systems by varying the environment temperature to a critical point. With this RTA-Lindblad ansatz, one can directly relate to the equilibrium behavior of the system, and if an order parameter has the exponent $\\Psi$, the remaining value at the phase transition will decrease with $1/\\tau^{\\Psi}$, where $\\tau$ is the overall time of the slow process. In the second application, I investigate the change in the expectation value of a conserved quantity (an operator commuting with the Hamiltonian) due to an extra Lindbladian term which would drive the system out from equilibrium, while the thermalizing RTA-Lindbladian term is also present. I give a closed perturbative expression in the first order for the expectation value in the new steady state using only expectation values calculated in the original thermal equilibrium.","sentences":["In the present paper, a global Lindbladian ansatz is constructed which leads to thermalization at temperature $T$ to the Gibs state of the investigated system.","This ansatz connects every two eigenstates of the Hamiltonian and leads to a simple master equation known in the literature as the relaxation time approximation (RTA).","The main message of this paper is that RTA, being a Lindbladian approach itself, can be used as Lindbladian securing thermalization when modeling physical processes, and can be consequently combined with other types of Lindbladians which would drive the system of the equilibrium state.","I demonstrate it with two applications.","The first application is the slow cooling (or heating) of quantum systems by varying the environment temperature to a critical point.","With this RTA-Lindblad ansatz, one can directly relate to the equilibrium behavior of the system, and if an order parameter has the exponent $\\Psi$, the remaining value at the phase transition will decrease with $1/\\tau^{\\Psi}$, where $\\tau$ is the overall time of the slow process.","In the second application, I investigate the change in the expectation value of a conserved quantity (an operator commuting with the Hamiltonian) due to an extra Lindbladian term which would drive the system out from equilibrium, while the thermalizing RTA-Lindbladian term is also present.","I give a closed perturbative expression in the first order for the expectation value in the new steady state using only expectation values calculated in the original thermal equilibrium."],"url":"http://arxiv.org/abs/2405.14825v1","category":"quant-ph"}
{"created":"2024-05-23 17:38:13","title":"Evaluating Vulnerability of Chiplet-Based Systems to Contactless Probing Techniques","abstract":"Driven by a need for ever increasing chip performance and inclusion of innovative features, a growing number of semiconductor companies are opting for all-inclusive System-on-Chip (SoC) architectures. Although Moore's Law has been able to keep up with the demand for more complex logic, manufacturing large dies still poses a challenge. Increasingly the solution adopted to minimize the impact of silicon defects on manufacturing yield has been to split a design into multiple smaller dies called chiplets which are then brought together on a silicon interposer. Advanced 2.5D and 3D packaging techniques that enable this kind of integration also promise increased power efficiency and opportunities for heterogeneous integration.   However, despite their advantages, chiplets are not without issues. Apart from manufacturing challenges that come with new packaging techniques, disaggregating a design into multiple logically and physically separate dies introduces new threats, including the possibility of tampering with and probing exposed data lines. In this paper we evaluate the exposure of chiplets to probing by applying laser contactless probing techniques to a chiplet-based AMD/Xilinx VU9P FPGA. First, we identify and map interposer wire drivers and show that probing them is easier compared to probing internal nodes. Lastly, we demonstrate that delay-based sensors, which can be used to protect against physical probes, are insufficient to protect against laser probing as the delay change due to laser probing is only 0.792ps even at 100\\% laser power.","sentences":["Driven by a need for ever increasing chip performance and inclusion of innovative features, a growing number of semiconductor companies are opting for all-inclusive System-on-Chip (SoC) architectures.","Although Moore's Law has been able to keep up with the demand for more complex logic, manufacturing large dies still poses a challenge.","Increasingly the solution adopted to minimize the impact of silicon defects on manufacturing yield has been to split a design into multiple smaller dies called chiplets which are then brought together on a silicon interposer.","Advanced 2.5D and 3D packaging techniques that enable this kind of integration also promise increased power efficiency and opportunities for heterogeneous integration.   ","However, despite their advantages, chiplets are not without issues.","Apart from manufacturing challenges that come with new packaging techniques, disaggregating a design into multiple logically and physically separate dies introduces new threats, including the possibility of tampering with and probing exposed data lines.","In this paper we evaluate the exposure of chiplets to probing by applying laser contactless probing techniques to a chiplet-based AMD/Xilinx VU9P FPGA.","First, we identify and map interposer wire drivers and show that probing them is easier compared to probing internal nodes.","Lastly, we demonstrate that delay-based sensors, which can be used to protect against physical probes, are insufficient to protect against laser probing as the delay change due to laser probing is only 0.792ps even at 100\\% laser power."],"url":"http://arxiv.org/abs/2405.14821v1","category":"cs.CR"}
{"created":"2024-05-23 17:37:41","title":"Identification of hot gas around low-mass protostars","abstract":"The low carbon content of Earth and primitive meteorites compared to the Sun and interstellar grains suggests that carbon-rich grains were destroyed in the inner few astronomical units of the young solar system. A promising mechanism to selectively destroy carbonaceous grains is thermal sublimation within the soot line at $\\gtrsim$ 300 K. To address whether such hot conditions are common amongst low-mass protostars, we observe CH$_3$CN transitions at 1, 2 and 3 mm with the NOrthern Extended Millimeter Array (NOEMA) toward seven low-mass and one intermediate-mass protostar ($L_{\\rm{bol}} \\sim2-300 L_\\odot$), as CH$_3$CN is an excellent temperature tracer. We find $>$ 300 K gas toward all sources, indicating that hot gas may be prevalent. Moreover, the excitation temperature for CH$_3$OH obtained with the same observations is always lower ($\\sim$135-250 K), suggesting that CH$_3$CN and CH$_3$OH have a different spatial distribution. A comparison of the column densities at 1 and 3 mm shows a stronger increase at 3 mm for CH$_3$CN than for CH$_3$OH. Since the dust opacity is lower at longer wavelengths, this indicates that CH$_3$CN is enhanced in the hot gas compared to CH$_3$OH. If this CH$_3$CN enhancement is the result of carbon-grain sublimation, these results suggests that Earth's initial formation conditions may not be rare.","sentences":["The low carbon content of Earth and primitive meteorites compared to the Sun and interstellar grains suggests that carbon-rich grains were destroyed in the inner few astronomical units of the young solar system.","A promising mechanism to selectively destroy carbonaceous grains is thermal sublimation within the soot line at $\\gtrsim$ 300 K. To address whether such hot conditions are common amongst low-mass protostars, we observe CH$_3$CN transitions at 1, 2 and 3 mm with the NOrthern Extended Millimeter Array (NOEMA) toward seven low-mass and one intermediate-mass protostar ($L_{\\rm{bol}} \\sim2-300 L_\\odot$), as CH$_3$CN is an excellent temperature tracer.","We find $>$ 300 K gas toward all sources, indicating that hot gas may be prevalent.","Moreover, the excitation temperature for CH$_3$OH obtained with the same observations is always lower ($\\sim$135-250 K), suggesting that CH$_3$CN and CH$_3$OH have a different spatial distribution.","A comparison of the column densities at 1 and 3 mm shows a stronger increase at 3 mm for CH$_3$CN than for CH$_3$OH.","Since the dust opacity is lower at longer wavelengths, this indicates that CH$_3$CN is enhanced in the hot gas compared to CH$_3$OH.","If this CH$_3$CN enhancement is the result of carbon-grain sublimation, these results suggests that Earth's initial formation conditions may not be rare."],"url":"http://arxiv.org/abs/2405.14820v1","category":"astro-ph.GA"}
{"created":"2024-05-23 17:29:38","title":"A noncollinear density functional theory ansatz for the phononic and thermodynamic properties of $\u03b1$-Pu","abstract":"Plutonium's phase diagram is host to complex structures and interactions that make the description of its ground state properties elusive. Using all-electron density functional theory, we study the thermodynamic properties of $\\alpha$-Pu. To do this, we build on recent work in the literature by introducing a novel noncollinear magnetic ansatz for $\\alpha$-Pu's ground state. The noncollinear ansatz accurately recovers the experimental phonon density of states, heat capacity, and thermal expansion. These new results on $\\alpha$-Pu along with recent results on $\\delta$-Pu demonstrate the efficacy of noncollinear ansatzes for the description of plutonium.","sentences":["Plutonium's phase diagram is host to complex structures and interactions that make the description of its ground state properties elusive.","Using all-electron density functional theory, we study the thermodynamic properties of $\\alpha$-Pu.","To do this, we build on recent work in the literature by introducing a novel noncollinear magnetic ansatz for $\\alpha$-Pu's ground state.","The noncollinear ansatz accurately recovers the experimental phonon density of states, heat capacity, and thermal expansion.","These new results on $\\alpha$-Pu along with recent results on $\\delta$-Pu demonstrate the efficacy of noncollinear ansatzes for the description of plutonium."],"url":"http://arxiv.org/abs/2405.14816v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 16:55:29","title":"The TESS-Keck Survey XX: 15 New TESS Planets and a Uniform RV Analysis of all Survey Targets","abstract":"The Transiting Exoplanet Survey Satellite (TESS) has discovered hundreds of new worlds, with TESS planet candidates now outnumbering the total number of confirmed planets from $\\textit{Kepler}$. Owing to differences in survey design, TESS continues to provide planets that are better suited for subsequent follow-up studies, including mass measurement through radial velocity (RV) observations, compared to Kepler targets. In this work, we present the TESS-Keck Survey's (TKS) Mass Catalog: a uniform analysis of all TKS RV survey data which has resulted in mass constraints for 126 planets and candidate signals. This includes 58 mass measurements that have reached $\\geq5\\sigma$ precision. We confirm or validate 32 new planets from the TESS mission either by significant mass measurement (15) or statistical validation (17), and we find no evidence of likely false positives among our entire sample. This work also serves as a data release for all previously unpublished TKS survey data, including 9,204 RV measurements and associated activity indicators over our three year survey. We took the opportunity to assess the performance of our survey, and found that we achieved many of our goals including measuring the mass of 38 small ($<4R_{\\oplus}$) planets, nearly achieving the TESS mission's basic science requirement. In addition, we evaluated the performance of the Automated Planet Finder (APF) as survey support and observed meaningful constraints on system parameters due to its more uniform phase coverage. Finally, we compared our measured masses to those predicted by commonly used mass-radius relations and investigated evidence of systematic bias.","sentences":["The Transiting Exoplanet Survey Satellite (TESS) has discovered hundreds of new worlds, with TESS planet candidates now outnumbering the total number of confirmed planets from $\\textit{Kepler}$. Owing to differences in survey design, TESS continues to provide planets that are better suited for subsequent follow-up studies, including mass measurement through radial velocity (RV) observations, compared to Kepler targets.","In this work, we present the TESS-Keck Survey's (TKS) Mass Catalog: a uniform analysis of all TKS RV survey data which has resulted in mass constraints for 126 planets and candidate signals.","This includes 58 mass measurements that have reached $\\geq5\\sigma$ precision.","We confirm or validate 32 new planets from the TESS mission either by significant mass measurement (15) or statistical validation (17), and we find no evidence of likely false positives among our entire sample.","This work also serves as a data release for all previously unpublished TKS survey data, including 9,204 RV measurements and associated activity indicators over our three year survey.","We took the opportunity to assess the performance of our survey, and found that we achieved many of our goals including measuring the mass of 38 small ($<4R_{\\oplus}$) planets, nearly achieving the TESS mission's basic science requirement.","In addition, we evaluated the performance of the Automated Planet Finder (APF) as survey support and observed meaningful constraints on system parameters due to its more uniform phase coverage.","Finally, we compared our measured masses to those predicted by commonly used mass-radius relations and investigated evidence of systematic bias."],"url":"http://arxiv.org/abs/2405.14786v1","category":"astro-ph.EP"}
{"created":"2024-05-23 16:44:29","title":"Kinetics of orbital ordering in cooperative Jahn-Teller models: Machine-learning enabled large-scale simulations","abstract":"We present a scalable machine learning (ML) force-field model for the adiabatic dynamics of cooperative Jahn-Teller (JT) systems. Large scale dynamical simulations of the JT model also shed light on the orbital ordering dynamics in colossal magnetoresistance manganites. The JT effect in these materials describes the distortion of local oxygen octahedra driven by a coupling to the orbital degrees of freedom of $e_g$ electrons. An effective electron-mediated interaction between the local JT modes leads to a structural transition and the emergence of long-range orbital order at low temperatures. Assuming the principle of locality, a deep-learning neural-network model is developed to accurately and efficiently predict the electron-induced forces that drive the dynamical evolution of JT phonons. A group-theoretical method is utilized to develop a descriptor that incorporates the combined orbital and lattice symmetry into the ML model. Large-scale Langevin dynamics simulations, enabled by the ML force-field models, are performed to investigate the coarsening dynamics of the composite JT distortion and orbital order after a thermal quench. The late-stage coarsening of orbital domains exhibits pronounced freezing behaviors which are likely related to the unusual morphology of the domain structures. Our work highlights a promising avenue for multi-scale dynamical modeling of correlated electron systems.","sentences":["We present a scalable machine learning (ML) force-field model for the adiabatic dynamics of cooperative Jahn-Teller (JT) systems.","Large scale dynamical simulations of the JT model also shed light on the orbital ordering dynamics in colossal magnetoresistance manganites.","The JT effect in these materials describes the distortion of local oxygen octahedra driven by a coupling to the orbital degrees of freedom of $e_g$ electrons.","An effective electron-mediated interaction between the local JT modes leads to a structural transition and the emergence of long-range orbital order at low temperatures.","Assuming the principle of locality, a deep-learning neural-network model is developed to accurately and efficiently predict the electron-induced forces that drive the dynamical evolution of JT phonons.","A group-theoretical method is utilized to develop a descriptor that incorporates the combined orbital and lattice symmetry into the ML model.","Large-scale Langevin dynamics simulations, enabled by the ML force-field models, are performed to investigate the coarsening dynamics of the composite JT distortion and orbital order after a thermal quench.","The late-stage coarsening of orbital domains exhibits pronounced freezing behaviors which are likely related to the unusual morphology of the domain structures.","Our work highlights a promising avenue for multi-scale dynamical modeling of correlated electron systems."],"url":"http://arxiv.org/abs/2405.14776v1","category":"cond-mat.str-el"}
{"created":"2024-05-23 16:38:57","title":"Dunkl symmetric coherent pairs of measures. An application to Fourier Dunkl-Sobolev expansions","abstract":"Let $\\mathcal{T}_{\\mu}$ be the Dunkl operator. A pair of symmetric measures $(u, v)$ supported on a symmetric subset of the real line is said to be a symmetric Dunkl-coherent pair if the corresponding sequences of monic orthogonal polynomials $\\{P_n\\}_{n\\geq 0}$ and $\\{R_n\\}_{n\\geq 0}$ (resp.) satisfy $$   R_{n}(x)=\\frac{\\mathcal{T}_{\\mu}P_{n+1} (x)}{\\mu_{n+1}}-\\sigma_{n-1}\\frac{\\mathcal{T}_{\\mu} P_{n-1}(x)}{\\mu_{n-1}}, n\\geq 2,$$   where $\\{\\sigma_n\\}_{n\\geq1}$ is a sequence of non-zero complex numbers and $\\mu_{2n}=2n, \\mu_{2n-1}= 2n-1+ 2\\mu, n\\geq 1.$   In this contribution we focus the attention on the sequence $\\{S_n^{(\\lambda,\\mu)}\\}_{n\\geq 0}$ of monic orthogonal polynomials with respect to the Dunkl-Sobolev inner product $$   <p,q>_{s,\\mu}=<u,pq>+\\lambda<v,\\mathcal{T}_{\\mu}p\\mathcal{T}_{\\mu}q>, \\lambda >0, \\ \\ p, \\ q \\ \\in \\mathcal{P}.$$   An algorithm is stated to compute the coefficients of the Fourier--Sobolev type expansions with respect to $<. , .>$ for suitable smooth functions $f$ such that $f \\in\\mathcal{W}_2^1(R, u, v, \\mu)=\\{ f; \\ \\|f\\|_{u}^{2} + \\lambda \\| \\mathcal{T}_{\\mu }f\\|_{v}^{2} <\\infty\\}$. Finally, two illustrative numerical examples are presented.","sentences":["Let $\\mathcal{T}_{\\mu}$ be the Dunkl operator.","A pair of symmetric measures $(u, v)$ supported on a symmetric subset of the real line is said to be a symmetric Dunkl-coherent pair if the corresponding sequences of monic orthogonal polynomials $\\{P_n\\}_{n\\geq 0}$ and $\\{R_n\\}_{n\\geq 0}$ (resp.)","satisfy $$   R_{n}(x)=\\frac{\\mathcal{T}_{\\mu}P_{n+1} (x)}{\\mu_{n+1}}-\\sigma_{n-1}\\frac{\\mathcal{T}_{\\mu} P_{n-1}(x)}{\\mu_{n-1}}, n\\geq 2,$$   where $\\{\\sigma_n\\}_{n\\geq1}$ is a sequence of non-zero complex numbers and $\\mu_{2n}=2n, \\mu_{2n-1}=","2n-1+ 2\\mu, n\\geq 1.$   In this contribution we focus the attention on the sequence $\\{S_n^{(\\lambda,\\mu)}\\}_{n\\geq 0}$ of monic orthogonal polynomials with respect to the Dunkl-Sobolev inner product $$   <p,q>_{s,\\mu}=<u,pq>+\\lambda<v,\\mathcal{T}_{\\mu}p\\mathcal{T}_{\\mu}q>, \\lambda >0, \\ \\ p, \\ q \\ \\in \\mathcal{P}.$$   An algorithm is stated to compute the coefficients of the Fourier--Sobolev type expansions with respect to $<.",", .>$ for suitable smooth functions $f$ such that $f \\in\\mathcal{W}_2^1(R, u, v, \\mu)=\\{ f; \\ \\|f\\|_{u}^{2} + \\lambda \\| \\mathcal{T}_{\\mu }f\\|_{v}^{2} <\\infty\\}$.","Finally, two illustrative numerical examples are presented."],"url":"http://arxiv.org/abs/2405.14771v1","category":"math.CA"}
{"created":"2024-05-23 16:33:13","title":"A Quantum Speed-Up for Approximating the Top Eigenvectors of a Matrix","abstract":"Finding a good approximation of the top eigenvector of a given $d\\times d$ matrix $A$ is a basic and important computational problem, with many applications. We give two different quantum algorithms that, given query access to the entries of a Hermitian matrix $A$ and assuming a constant eigenvalue gap, output a classical description of a good approximation of the top eigenvector: one algorithm with time complexity $\\mathcal{\\tilde{O}}(d^{1.75})$ and one with time complexity $d^{1.5+o(1)}$ (the first algorithm has a slightly better dependence on the $\\ell_2$-error of the approximating vector than the second, and uses different techniques of independent interest). Both of our quantum algorithms provide a polynomial speed-up over the best-possible classical algorithm, which needs $\\Omega(d^2)$ queries to entries of $A$, and hence $\\Omega(d^2)$ time. We extend this to a quantum algorithm that outputs a classical description of the subspace spanned by the top-$q$ eigenvectors in time $qd^{1.5+o(1)}$. We also prove a nearly-optimal lower bound of $\\tilde{\\Omega}(d^{1.5})$ on the quantum query complexity of approximating the top eigenvector.   Our quantum algorithms run a version of the classical power method that is robust to certain benign kinds of errors, where we implement each matrix-vector multiplication with small and well-behaved error on a quantum computer, in different ways for the two algorithms. Our first algorithm estimates the matrix-vector product one entry at a time, using a new ``Gaussian phase estimation'' procedure. Our second algorithm uses block-encoding techniques to compute the matrix-vector product as a quantum state, from which we obtain a classical description by a new time-efficient unbiased pure-state tomography procedure.","sentences":["Finding a good approximation of the top eigenvector of a given $d\\times d$ matrix $A$ is a basic and important computational problem, with many applications.","We give two different quantum algorithms that, given query access to the entries of a Hermitian matrix $A$ and assuming a constant eigenvalue gap, output a classical description of a good approximation of the top eigenvector: one algorithm with time complexity $\\mathcal{\\tilde{O}}(d^{1.75})$ and one with time complexity $d^{1.5+o(1)}$ (the first algorithm has a slightly better dependence on the $\\ell_2$-error of the approximating vector than the second, and uses different techniques of independent interest).","Both of our quantum algorithms provide a polynomial speed-up over the best-possible classical algorithm, which needs $\\Omega(d^2)$ queries to entries of $A$, and hence $\\Omega(d^2)$ time.","We extend this to a quantum algorithm that outputs a classical description of the subspace spanned by the top-$q$ eigenvectors in time $qd^{1.5+o(1)}$. We also prove a nearly-optimal lower bound of $\\tilde{\\Omega}(d^{1.5})$ on the quantum query complexity of approximating the top eigenvector.   ","Our quantum algorithms run a version of the classical power method that is robust to certain benign kinds of errors, where we implement each matrix-vector multiplication with small and well-behaved error on a quantum computer, in different ways for the two algorithms.","Our first algorithm estimates the matrix-vector product one entry at a time, using a new ``Gaussian phase estimation'' procedure.","Our second algorithm uses block-encoding techniques to compute the matrix-vector product as a quantum state, from which we obtain a classical description by a new time-efficient unbiased pure-state tomography procedure."],"url":"http://arxiv.org/abs/2405.14765v1","category":"quant-ph"}
{"created":"2024-05-23 16:32:06","title":"Structure preserving finite element schemes for the Navier-Stokes-Cahn-Hilliard system with degenerate mobility","abstract":"In this work we present two new numerical schemes to approximate the Navier-Stokes-Cahn-Hilliard system with degenerate mobility using finite differences in time and finite elements in space. The proposed schemes are conservative, energy-stable and preserve the maximum principle approximately (the amount of the phase variable being outside of the interval [0,1] goes to zero in terms of a truncation parameter). Additionally, we present several numerical results to illustrate the accuracy and the well behavior of the proposed schemes, as well as a comparison with the behavior of the Navier-Stokes-Cahn-Hilliard model with constant mobility.","sentences":["In this work we present two new numerical schemes to approximate the Navier-Stokes-Cahn-Hilliard system with degenerate mobility using finite differences in time and finite elements in space.","The proposed schemes are conservative, energy-stable and preserve the maximum principle approximately (the amount of the phase variable being outside of the interval [0,1] goes to zero in terms of a truncation parameter).","Additionally, we present several numerical results to illustrate the accuracy and the well behavior of the proposed schemes, as well as a comparison with the behavior of the Navier-Stokes-Cahn-Hilliard model with constant mobility."],"url":"http://arxiv.org/abs/2405.14763v1","category":"math.NA"}
{"created":"2024-05-23 16:29:30","title":"Fault Tolerant ML: Efficient Meta-Aggregation and Synchronous Training","abstract":"In this paper, we investigate the challenging framework of Byzantine-robust training in distributed machine learning (ML) systems, focusing on enhancing both efficiency and practicality. As distributed ML systems become integral for complex ML tasks, ensuring resilience against Byzantine failures-where workers may contribute incorrect updates due to malice or error-gains paramount importance. Our first contribution is the introduction of the Centered Trimmed Meta Aggregator (CTMA), an efficient meta-aggregator that upgrades baseline aggregators to optimal performance levels, while requiring low computational demands. Additionally, we propose harnessing a recently developed gradient estimation technique based on a double-momentum strategy within the Byzantine context. Our paper highlights its theoretical and practical advantages for Byzantine-robust training, especially in simplifying the tuning process and reducing the reliance on numerous hyperparameters. The effectiveness of this technique is supported by theoretical insights within the stochastic convex optimization (SCO) framework.","sentences":["In this paper, we investigate the challenging framework of Byzantine-robust training in distributed machine learning (ML) systems, focusing on enhancing both efficiency and practicality.","As distributed ML systems become integral for complex ML tasks, ensuring resilience against Byzantine failures-where workers may contribute incorrect updates due to malice or error-gains paramount importance.","Our first contribution is the introduction of the Centered Trimmed Meta Aggregator (CTMA), an efficient meta-aggregator that upgrades baseline aggregators to optimal performance levels, while requiring low computational demands.","Additionally, we propose harnessing a recently developed gradient estimation technique based on a double-momentum strategy within the Byzantine context.","Our paper highlights its theoretical and practical advantages for Byzantine-robust training, especially in simplifying the tuning process and reducing the reliance on numerous hyperparameters.","The effectiveness of this technique is supported by theoretical insights within the stochastic convex optimization (SCO) framework."],"url":"http://arxiv.org/abs/2405.14759v1","category":"cs.LG"}
{"created":"2024-05-23 16:17:44","title":"AGILE: A Novel Framework of LLM Agents","abstract":"We introduce a novel framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent's abilities include not only conversation but also reflection, utilization of tools, and consultation with experts. We formulate the construction of such an LLM agent as a reinforcement learning problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA and MedMCQA show that AGILE agents based on 13B and 7B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance.","sentences":["We introduce a novel framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts.","The agent's abilities include not only conversation but also reflection, utilization of tools, and consultation with experts.","We formulate the construction of such an LLM agent as a reinforcement learning problem, in which the LLM serves as the policy model.","We fine-tune the LLM using labeled data of actions and the PPO algorithm.","We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping.","Our extensive experiments on ProductQA and MedMCQA show that AGILE agents based on 13B and 7B LLMs trained with PPO can outperform GPT-4 agents.","Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance."],"url":"http://arxiv.org/abs/2405.14751v1","category":"cs.LG"}
{"created":"2024-05-23 16:04:35","title":"Entropy maximization in the two-dimensional Euler equations","abstract":"We consider variational problem related to entropy maximization in the two-dimensional Euler equations, in order to investigate the long-time dynamics of solutions with bounded vorticity. Using variations on the classical min-max principle and borrowing ideas from optimal transportation and quantitative rearrangement inequalities, we prove results on the structure of entropy maximizers arising in the investigation of the long-time behavior of vortex patches. We further show that the same techniques apply in the study of stability of the canonical Gibbs measure associated to a system of point vortices.","sentences":["We consider variational problem related to entropy maximization in the two-dimensional Euler equations, in order to investigate the long-time dynamics of solutions with bounded vorticity.","Using variations on the classical min-max principle and borrowing ideas from optimal transportation and quantitative rearrangement inequalities, we prove results on the structure of entropy maximizers arising in the investigation of the long-time behavior of vortex patches.","We further show that the same techniques apply in the study of stability of the canonical Gibbs measure associated to a system of point vortices."],"url":"http://arxiv.org/abs/2405.14738v1","category":"math.AP"}
{"created":"2024-05-23 16:00:18","title":"The Data Acquisition System of the LZ Dark Matter Detector: FADR","abstract":"The Data Acquisition System (DAQ) for the LUX-ZEPLIN (LZ) dark matter detector is described. The signals from 745 PMTs, distributed across three subsystems, are sampled with 100-MHz 32-channel digitizers (DDC-32s). A basic waveform analysis is carried out on the on-board Field Programmable Gate Arrays (FPGAs) to extract information about the observed scintillation and electroluminescence signals. This information is used to determine if the digitized waveforms should be preserved for offline analysis.   The system is designed around the Kintex-7 FPGA. In addition to digitizing the PMT signals and providing basic event selection in real time, the flexibility provided by the use of FPGAs allows us to monitor the performance of the detector and the DAQ in parallel to normal data acquisition.   The hardware and software/firmware of this FPGA-based Architecture for Data acquisition and Realtime monitoring (FADR) are discussed and performance measurements are described.","sentences":["The Data Acquisition System (DAQ) for the LUX-ZEPLIN (LZ) dark matter detector is described.","The signals from 745 PMTs, distributed across three subsystems, are sampled with 100-MHz 32-channel digitizers (DDC-32s).","A basic waveform analysis is carried out on the on-board Field Programmable Gate Arrays (FPGAs) to extract information about the observed scintillation and electroluminescence signals.","This information is used to determine if the digitized waveforms should be preserved for offline analysis.   ","The system is designed around the Kintex-7 FPGA.","In addition to digitizing the PMT signals and providing basic event selection in real time, the flexibility provided by the use of FPGAs allows us to monitor the performance of the detector and the DAQ in parallel to normal data acquisition.   ","The hardware and software/firmware of this FPGA-based Architecture for Data acquisition and Realtime monitoring (FADR) are discussed and performance measurements are described."],"url":"http://arxiv.org/abs/2405.14732v1","category":"physics.ins-det"}
{"created":"2024-05-23 15:57:11","title":"Embedding Compression for Efficient Re-Identification","abstract":"Real world re-identfication (ReID) algorithms aim to map new observations of an object to previously recorded instances. These systems are often constrained by quantity and size of the stored embeddings. To combat this scaling problem, we attempt to shrink the size of these vectors by using a variety of compression techniques. In this paper, we benchmark quantization-aware-training along with three different dimension reduction methods: iterative structured pruning, slicing the embeddings at initialize, and using low rank embeddings. We find that ReID embeddings can be compressed by up to 96x with minimal drop in performance. This implies that modern re-identification paradigms do not fully leverage the high dimensional latent space, opening up further research to increase the capabilities of these systems.","sentences":["Real world re-identfication (ReID) algorithms aim to map new observations of an object to previously recorded instances.","These systems are often constrained by quantity and size of the stored embeddings.","To combat this scaling problem, we attempt to shrink the size of these vectors by using a variety of compression techniques.","In this paper, we benchmark quantization-aware-training along with three different dimension reduction methods: iterative structured pruning, slicing the embeddings at initialize, and using low rank embeddings.","We find that ReID embeddings can be compressed by up to 96x with minimal drop in performance.","This implies that modern re-identification paradigms do not fully leverage the high dimensional latent space, opening up further research to increase the capabilities of these systems."],"url":"http://arxiv.org/abs/2405.14730v1","category":"cs.CV"}
{"created":"2024-05-23 15:56:02","title":"Dielectric Properties of Water: A Molecular Dynamics Study on the Effects of Molecule Count and Cutoff Radius","abstract":"Currently, the study of systems confined within various materials, such as graphene and graphene oxide, for diverse applications such as water desalination, metal separation from water, battery cells, and high-efficiency capacitors, is very common. Among these systems, water is a prominent subject of investigation. Understanding the impact of the number of molecules and the optimal cutoff radius is essential in order to determine the parameters. This study investigates the dielectric properties of H2O in relation to both the number of molecules and the cutoff radius within the simulation cell. We employ the flexible water model FBA/$\\epsilon$, known for its improved accuracy in reproducing water properties across various thermodynamic states. We range from 60 to 1372 molecules and use cutoff radius from 6 nm to 1.2 nm, encompassing a wide range of calculations to assess their impact on the simulation results, including critical properties like the dielectric constant, density, and phase change enthalpy.","sentences":["Currently, the study of systems confined within various materials, such as graphene and graphene oxide, for diverse applications such as water desalination, metal separation from water, battery cells, and high-efficiency capacitors, is very common.","Among these systems, water is a prominent subject of investigation.","Understanding the impact of the number of molecules and the optimal cutoff radius is essential in order to determine the parameters.","This study investigates the dielectric properties of H2O in relation to both the number of molecules and the cutoff radius within the simulation cell.","We employ the flexible water model FBA/$\\epsilon$, known for its improved accuracy in reproducing water properties across various thermodynamic states.","We range from 60 to 1372 molecules and use cutoff radius from 6 nm to 1.2 nm, encompassing a wide range of calculations to assess their impact on the simulation results, including critical properties like the dielectric constant, density, and phase change enthalpy."],"url":"http://arxiv.org/abs/2405.14729v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-23 15:53:55","title":"Learning-Based Intermittent CSI Estimation with Adaptive Intervals in Integrated Sensing and Communication Systems","abstract":"Due to the distinct objectives and multipath utilization mechanisms between the communication module and radar module, the system design of integrated sensing and communication (ISAC) necessitates two types of channel state information (CSI), i.e., communication CSI representing the whole channel gain and phase shifts, and radar CSI exclusively focused on target mobility and position information. However, current ISAC systems apply an identical mechanism to estimate both types of CSI at the same predetermined estimation interval, leading to significant overhead and compromised performances. Therefore, this paper proposes an intermittent communication and radar CSI estimation scheme with adaptive intervals for individual users/targets, where both types of CSI can be predicted using channel temporal correlations for cost reduction or re-estimated via training signal transmission for improved estimation accuracy. Specifically, we jointly optimize the binary CSI re-estimation/prediction decisions and transmit beamforming matrices for individual users/targets to maximize communication transmission rates and minimize radar tracking errors and costs in a multiple-input single-output (MISO) ISAC system. Unfortunately, this problem has causality issues because it requires comparing system performances under re-estimated CSI and predicted CSI during the optimization. Additionally, the binary decision makes the joint design a mixed integer nonlinear programming (MINLP) problem, resulting in high complexity when using conventional optimization algorithms. Therefore, we propose a deep reinforcement online learning (DROL) framework that first implements an online deep neural network (DNN) to learn the binary CSI updating decisions from the experiences. Given the learned decisions, we propose an efficient algorithm to solve the remaining beamforming design problem efficiently.","sentences":["Due to the distinct objectives and multipath utilization mechanisms between the communication module and radar module, the system design of integrated sensing and communication (ISAC) necessitates two types of channel state information (CSI), i.e., communication CSI representing the whole channel gain and phase shifts, and radar CSI exclusively focused on target mobility and position information.","However, current ISAC systems apply an identical mechanism to estimate both types of CSI at the same predetermined estimation interval, leading to significant overhead and compromised performances.","Therefore, this paper proposes an intermittent communication and radar CSI estimation scheme with adaptive intervals for individual users/targets, where both types of CSI can be predicted using channel temporal correlations for cost reduction or re-estimated via training signal transmission for improved estimation accuracy.","Specifically, we jointly optimize the binary CSI re-estimation/prediction decisions and transmit beamforming matrices for individual users/targets to maximize communication transmission rates and minimize radar tracking errors and costs in a multiple-input single-output (MISO) ISAC system.","Unfortunately, this problem has causality issues because it requires comparing system performances under re-estimated CSI and predicted CSI during the optimization.","Additionally, the binary decision makes the joint design a mixed integer nonlinear programming (MINLP) problem, resulting in high complexity when using conventional optimization algorithms.","Therefore, we propose a deep reinforcement online learning (DROL) framework that first implements an online deep neural network (DNN) to learn the binary CSI updating decisions from the experiences.","Given the learned decisions, we propose an efficient algorithm to solve the remaining beamforming design problem efficiently."],"url":"http://arxiv.org/abs/2405.14724v1","category":"eess.SP"}
{"created":"2024-05-23 15:49:48","title":"A periodic Kingman model for the balance between mutation and selection","abstract":"We consider a periodic extension of the classical Kingman non-linear model (Kingman, 1978) for the balance between selection and mutation in a large population. In the original model, the fitness distribution of the population is modeled by a probability measure on the unit interval evolving through a simple dynamical system in discrete time: selection acts through size-biasing, and the mutation probability and distribution are kept fixed through time. A natural extension of Kingman model is given by a periodic mutation environment; in this setting, we prove the convergence of the fitness distribution along subsequences and find an explicit criterion in terms of the Perron eigenvalue of an appropriately chosen matrix to decide whether an atom emerges at the largest fitness, a phenomenon usually called condensation.","sentences":["We consider a periodic extension of the classical Kingman non-linear model (Kingman, 1978) for the balance between selection and mutation in a large population.","In the original model, the fitness distribution of the population is modeled by a probability measure on the unit interval evolving through a simple dynamical system in discrete time: selection acts through size-biasing, and the mutation probability and distribution are kept fixed through time.","A natural extension of Kingman model is given by a periodic mutation environment; in this setting, we prove the convergence of the fitness distribution along subsequences and find an explicit criterion in terms of the Perron eigenvalue of an appropriately chosen matrix to decide whether an atom emerges at the largest fitness, a phenomenon usually called condensation."],"url":"http://arxiv.org/abs/2405.14721v1","category":"math.PR"}
{"created":"2024-05-23 15:48:24","title":"The impact of temporal hydrogen regulation on hydrogen exporters and their domestic energy transition","abstract":"As global demand for green hydrogen rises, potential hydrogen exporters move into the spotlight. However, the large-scale installation of on-grid hydrogen electrolysis for export can have profound impacts on domestic energy prices and energy-related emissions. Our investigation explores the interplay of hydrogen exports, domestic energy transition and temporal hydrogen regulation, employing a sector-coupled energy model in Morocco. We find substantial co-benets of domestic climate change mitigation and hydrogen exports, whereby exports can reduce domestic electricity prices while mitigation reduces hydrogen export prices. However, increasing hydrogen exports quickly in a system that is still dominated by fossil fuels can substantially raise domestic electricity prices, if green hydrogen production is not regulated. Surprisingly, temporal matching of hydrogen production lowers domestic electricity cost by up to 31% while the effect on exporters is minimal. This policy instrument can steer the welfare (re-)distribution between hydrogen exporting firms, hydrogen importers, and domestic electricity consumers and hereby increases acceptance among actors.","sentences":["As global demand for green hydrogen rises, potential hydrogen exporters move into the spotlight.","However, the large-scale installation of on-grid hydrogen electrolysis for export can have profound impacts on domestic energy prices and energy-related emissions.","Our investigation explores the interplay of hydrogen exports, domestic energy transition and temporal hydrogen regulation, employing a sector-coupled energy model in Morocco.","We find substantial co-benets of domestic climate change mitigation and hydrogen exports, whereby exports can reduce domestic electricity prices while mitigation reduces hydrogen export prices.","However, increasing hydrogen exports quickly in a system that is still dominated by fossil fuels can substantially raise domestic electricity prices, if green hydrogen production is not regulated.","Surprisingly, temporal matching of hydrogen production lowers domestic electricity cost by up to 31% while the effect on exporters is minimal.","This policy instrument can steer the welfare (re-)distribution between hydrogen exporting firms, hydrogen importers, and domestic electricity consumers and hereby increases acceptance among actors."],"url":"http://arxiv.org/abs/2405.14717v1","category":"physics.soc-ph"}
{"created":"2024-05-23 15:46:34","title":"Defining error accumulation in ML atmospheric simulators","abstract":"Machine learning (ML) has recently shown significant promise in modelling atmospheric systems, such as the weather. Many of these ML models are autoregressive, and error accumulation in their forecasts is a key problem. However, there is no clear definition of what `error accumulation' actually entails. In this paper, we propose a definition and an associated metric to measure it. Our definition distinguishes between errors which are due to model deficiencies, which we may hope to fix, and those due to the intrinsic properties of atmospheric systems (chaos, unobserved variables), which are not fixable. We illustrate the usefulness of this definition by proposing a simple regularization loss penalty inspired by it. This approach shows performance improvements (according to RMSE and spread/skill) in a selection of atmospheric systems, including the real-world weather prediction task.","sentences":["Machine learning (ML) has recently shown significant promise in modelling atmospheric systems, such as the weather.","Many of these ML models are autoregressive, and error accumulation in their forecasts is a key problem.","However, there is no clear definition of what `error accumulation' actually entails.","In this paper, we propose a definition and an associated metric to measure it.","Our definition distinguishes between errors which are due to model deficiencies, which we may hope to fix, and those due to the intrinsic properties of atmospheric systems (chaos, unobserved variables), which are not fixable.","We illustrate the usefulness of this definition by proposing a simple regularization loss penalty inspired by it.","This approach shows performance improvements (according to RMSE and spread/skill) in a selection of atmospheric systems, including the real-world weather prediction task."],"url":"http://arxiv.org/abs/2405.14714v1","category":"cs.LG"}
{"created":"2024-05-23 15:31:46","title":"Quantum amplitude estimation from classical signal processing","abstract":"We demonstrate that the problem of amplitude estimation, a core subroutine used in many quantum algorithms, can be mapped directly to a problem in signal processing called direction of arrival (DOA) estimation. The DOA task is to determine the direction of arrival of an incoming wave with the fewest possible measurements. The connection between amplitude estimation and DOA allows us to make use of the vast amount of signal processing algorithms to post-process the measurements of the Grover iterator at predefined depths. Using an off-the-shelf DOA algorithm called ESPRIT together with a compressed-sensing based sampling approach, we create a phase-estimation free, parallel quantum amplitude estimation (QAE) algorithm with a total query complexity of $\\sim 4.9/\\varepsilon$ and a parallel query complexity of $\\sim 0.40/\\varepsilon$ at 95% confidence. This performance is a factor of $1.1\\times$ and $14\\times$ improvement over Rall and Fuller [Quantum 7, 937 (2023)], for worst-case complexity, which to our knowledge is the best published result for amplitude estimation. The approach presented here provides a simple, robust, parallel method to performing QAE, with many possible avenues for improvement borrowing ideas from the wealth of literature in classical signal processing.","sentences":["We demonstrate that the problem of amplitude estimation, a core subroutine used in many quantum algorithms, can be mapped directly to a problem in signal processing called direction of arrival (DOA) estimation.","The DOA task is to determine the direction of arrival of an incoming wave with the fewest possible measurements.","The connection between amplitude estimation and DOA allows us to make use of the vast amount of signal processing algorithms to post-process the measurements of the Grover iterator at predefined depths.","Using an off-the-shelf DOA algorithm called ESPRIT together with a compressed-sensing based sampling approach, we create a phase-estimation free, parallel quantum amplitude estimation (QAE) algorithm with a total query complexity of $\\sim 4.9/\\varepsilon$ and a parallel query complexity of $\\sim 0.40/\\varepsilon$ at 95% confidence.","This performance is a factor of $1.1\\times$ and $14\\times$ improvement over Rall and Fuller","[Quantum 7, 937 (2023)], for worst-case complexity, which to our knowledge is the best published result for amplitude estimation.","The approach presented here provides a simple, robust, parallel method to performing QAE, with many possible avenues for improvement borrowing ideas from the wealth of literature in classical signal processing."],"url":"http://arxiv.org/abs/2405.14697v1","category":"quant-ph"}
{"created":"2024-05-23 15:29:18","title":"Interpolation and synthesis of sparse samples in exoplanet atmospheric modeling","abstract":"This paper highlights methods from geostatistics that are relevant to the interpretation, intercomparison, and synthesis of atmospheric model data, with a specific application to exoplanet atmospheric modeling. Climate models are increasingly used to study theoretical and observational properties of exoplanets, which include a hierarchy of models ranging from fast and idealized models to those that are slower but more comprehensive. Exploring large parameter spaces with computationally-expensive models can be accomplished with sparse sampling techniques, but analyzing such sparse samples can pose challenges for conventional interpolation functions. Ordinary kriging is a statistical method for describing the spatial distribution of a data set in terms of the variogram function, which can be used to interpolate sparse samples across any number of dimensions. Variograms themselves may also be useful diagnostic tools for describing the spatial distribution of model data in exoplanet atmospheric model intercomparison projects. Universal kriging is another method that can synthesize data calculated by models of different complexity, which can be used to combine sparse samples of data from slow models with larger samples of data from fast models. Ordinary and universal kriging can also provide a way to synthesize model predictions with sparse samples of exoplanet observations and may have other applications in exoplanet science.","sentences":["This paper highlights methods from geostatistics that are relevant to the interpretation, intercomparison, and synthesis of atmospheric model data, with a specific application to exoplanet atmospheric modeling.","Climate models are increasingly used to study theoretical and observational properties of exoplanets, which include a hierarchy of models ranging from fast and idealized models to those that are slower but more comprehensive.","Exploring large parameter spaces with computationally-expensive models can be accomplished with sparse sampling techniques, but analyzing such sparse samples can pose challenges for conventional interpolation functions.","Ordinary kriging is a statistical method for describing the spatial distribution of a data set in terms of the variogram function, which can be used to interpolate sparse samples across any number of dimensions.","Variograms themselves may also be useful diagnostic tools for describing the spatial distribution of model data in exoplanet atmospheric model intercomparison projects.","Universal kriging is another method that can synthesize data calculated by models of different complexity, which can be used to combine sparse samples of data from slow models with larger samples of data from fast models.","Ordinary and universal kriging can also provide a way to synthesize model predictions with sparse samples of exoplanet observations and may have other applications in exoplanet science."],"url":"http://arxiv.org/abs/2405.14693v1","category":"astro-ph.EP"}
{"created":"2024-05-23 15:25:30","title":"Efficient Robot Learning for Perception and Mapping","abstract":"Holistic scene understanding poses a fundamental contribution to the autonomous operation of a robotic agent in its environment. Key ingredients include a well-defined representation of the surroundings to capture its spatial structure as well as assigning semantic meaning while delineating individual objects. Classic components from the toolbox of roboticists to address these tasks are simultaneous localization and mapping (SLAM) and panoptic segmentation. Although recent methods demonstrate impressive advances, mostly due to employing deep learning, they commonly utilize in-domain training on large datasets. Since following such a paradigm substantially limits their real-world application, my research investigates how to minimize human effort in deploying perception-based robotic systems to previously unseen environments. In particular, I focus on leveraging continual learning and reducing human annotations for efficient learning. An overview of my work can be found at https://vniclas.github.io.","sentences":["Holistic scene understanding poses a fundamental contribution to the autonomous operation of a robotic agent in its environment.","Key ingredients include a well-defined representation of the surroundings to capture its spatial structure as well as assigning semantic meaning while delineating individual objects.","Classic components from the toolbox of roboticists to address these tasks are simultaneous localization and mapping (SLAM) and panoptic segmentation.","Although recent methods demonstrate impressive advances, mostly due to employing deep learning, they commonly utilize in-domain training on large datasets.","Since following such a paradigm substantially limits their real-world application, my research investigates how to minimize human effort in deploying perception-based robotic systems to previously unseen environments.","In particular, I focus on leveraging continual learning and reducing human annotations for efficient learning.","An overview of my work can be found at https://vniclas.github.io."],"url":"http://arxiv.org/abs/2405.14688v1","category":"cs.RO"}
{"created":"2024-05-23 15:22:30","title":"Controlling dephasing of coupled qubits via shared-bath coherence","abstract":"The interaction of a quantum system with its environment limits qubit coherence times and restricts its utility in quantum information processing applications. In this Letter, we show that the decoherence of a coupled qubit system can be minimized, or even eliminated by exploiting the quantum coherence of the bath itself. We investigate the dephasing in a system of two spatially separated, electronically decoupled qubits, with direct or mediated coupling, interacting with a shared bath. For illustration we treat F\\\"orster or cavity-mediated coupling between semiconductor quantum dots interacting with acoustic phonons. Using the rigorous method of Trotter's decomposition with cumulant expansion, we demonstrate a reduction in the dephasing rates at specific distances. This control is a coherent effect of the shared bath and is absent for independent baths. It can be understood in terms of phonon-assisted transitions between the entangled qubit states of the coupled system.","sentences":["The interaction of a quantum system with its environment limits qubit coherence times and restricts its utility in quantum information processing applications.","In this Letter, we show that the decoherence of a coupled qubit system can be minimized, or even eliminated by exploiting the quantum coherence of the bath itself.","We investigate the dephasing in a system of two spatially separated, electronically decoupled qubits, with direct or mediated coupling, interacting with a shared bath.","For illustration we treat F\\\"orster or cavity-mediated coupling between semiconductor quantum dots interacting with acoustic phonons.","Using the rigorous method of Trotter's decomposition with cumulant expansion, we demonstrate a reduction in the dephasing rates at specific distances.","This control is a coherent effect of the shared bath and is absent for independent baths.","It can be understood in terms of phonon-assisted transitions between the entangled qubit states of the coupled system."],"url":"http://arxiv.org/abs/2405.14685v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 15:19:34","title":"Development of a Gaussian Approximation Potential to Study Structure and Thermodynamics of Nickel Nanoclusters","abstract":"Machine Learning (ML) potentials such as Gaussian Approximation Potential (GAP) have demonstrated impressive capabilities in mapping structure to properties across diverse systems. Here, we introduce a GAP model for low-dimensional Ni nanoclusters and demonstrate its flexibility and effectiveness in capturing the energetics, structural diversity and thermodynamic properties of Ni nanoclusters across a broad size range. Through a systematic approach encompassing model development, validation, and application, we evaluate the model's efficacy in representing energetics and configurational features in low-dimensional regimes, while also examining its extrapolative nature to vastly different spatiotemporal regimes. Our analysis and discussion shed light on the data quality required to effectively train such models. Trajectories from large scale MD simulations using the GAP model analyzed with data-driven models like Graph Neural Networks (GNN) reveal intriguing insights into the size-dependent phase behavior and thermo-mechanical stability characteristics of porous Ni nanoparticles. Overall, our work underscores the potential of ML models which coupled with data-driven approaches serve as versatile tools for studying low-dimensional systems and complex material dynamics.","sentences":["Machine Learning (ML) potentials such as Gaussian Approximation Potential (GAP) have demonstrated impressive capabilities in mapping structure to properties across diverse systems.","Here, we introduce a GAP model for low-dimensional Ni nanoclusters and demonstrate its flexibility and effectiveness in capturing the energetics, structural diversity and thermodynamic properties of Ni nanoclusters across a broad size range.","Through a systematic approach encompassing model development, validation, and application, we evaluate the model's efficacy in representing energetics and configurational features in low-dimensional regimes, while also examining its extrapolative nature to vastly different spatiotemporal regimes.","Our analysis and discussion shed light on the data quality required to effectively train such models.","Trajectories from large scale MD simulations using the GAP model analyzed with data-driven models like Graph Neural Networks (GNN) reveal intriguing insights into the size-dependent phase behavior and thermo-mechanical stability characteristics of porous Ni nanoparticles.","Overall, our work underscores the potential of ML models which coupled with data-driven approaches serve as versatile tools for studying low-dimensional systems and complex material dynamics."],"url":"http://arxiv.org/abs/2405.14683v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 15:09:35","title":"The Non-collinear Path to Topological Superconductivity","abstract":"Combining spin textures in ultra-thin films with conventional superconductors has emerged as a powerful and versatile platform for designing topologically non-trivial superconducting phases as well as spin-triplet Cooper pairs. As a consequence, two-dimensional magnet-superconductor hybrids (2D MSHs) are promising candidate systems to realize devices for topology-based quantum technologies and superconducting spintronics. So far, studies have focused mostly on systems hosting collinear ferromagnets or antiferromagnets. However, topologically non-trivial phases have been predicted to emerge in MSH systems with non-collinear spin textures as well. In this article, we present the experimental discovery of topological superconductivity in the MSH system Fe/Ta(110) where a magnetic spiral is realized in the Fe monolayer on the surface of the s-wave superconductor Ta. By combining low-temperature spin-polarized scanning tunneling microscopy measurements with theoretical modeling, we are able to conclude that the system is in a topological nodal-point superconducting phase with low-energy edge modes. Due to the non-collinear spin texture in our MSH system, these edge modes exhibit a magnetization direction-dependent dispersion. Furthermore, we identify direct signatures of Rashba spin-orbit coupling in the experimentally measured differential tunneling conductance. The present work realizes a non-collinear spin texture-based path to topological superconductivity.","sentences":["Combining spin textures in ultra-thin films with conventional superconductors has emerged as a powerful and versatile platform for designing topologically non-trivial superconducting phases as well as spin-triplet Cooper pairs.","As a consequence, two-dimensional magnet-superconductor hybrids (2D MSHs) are promising candidate systems to realize devices for topology-based quantum technologies and superconducting spintronics.","So far, studies have focused mostly on systems hosting collinear ferromagnets or antiferromagnets.","However, topologically non-trivial phases have been predicted to emerge in MSH systems with non-collinear spin textures as well.","In this article, we present the experimental discovery of topological superconductivity in the MSH system Fe/Ta(110) where a magnetic spiral is realized in the Fe monolayer on the surface of the s-wave superconductor Ta.","By combining low-temperature spin-polarized scanning tunneling microscopy measurements with theoretical modeling, we are able to conclude that the system is in a topological nodal-point superconducting phase with low-energy edge modes.","Due to the non-collinear spin texture in our MSH system, these edge modes exhibit a magnetization direction-dependent dispersion.","Furthermore, we identify direct signatures of Rashba spin-orbit coupling in the experimentally measured differential tunneling conductance.","The present work realizes a non-collinear spin texture-based path to topological superconductivity."],"url":"http://arxiv.org/abs/2405.14673v1","category":"cond-mat.supr-con"}
{"created":"2024-05-23 15:07:21","title":"Overcoming the Challenges of Batch Normalization in Federated Learning","abstract":"Batch normalization has proven to be a very beneficial mechanism to accelerate the training and improve the accuracy of deep neural networks in centralized environments. Yet, the scheme faces significant challenges in federated learning, especially under high data heterogeneity. Essentially, the main challenges arise from external covariate shifts and inconsistent statistics across clients. We introduce in this paper Federated BatchNorm (FBN), a novel scheme that restores the benefits of batch normalization in federated learning. Essentially, FBN ensures that the batch normalization during training is consistent with what would be achieved in a centralized execution, hence preserving the distribution of the data, and providing running statistics that accurately approximate the global statistics. FBN thereby reduces the external covariate shift and matches the evaluation performance of the centralized setting. We also show that, with a slight increase in complexity, we can robustify FBN to mitigate erroneous statistics and potentially adversarial attacks.","sentences":["Batch normalization has proven to be a very beneficial mechanism to accelerate the training and improve the accuracy of deep neural networks in centralized environments.","Yet, the scheme faces significant challenges in federated learning, especially under high data heterogeneity.","Essentially, the main challenges arise from external covariate shifts and inconsistent statistics across clients.","We introduce in this paper Federated BatchNorm (FBN), a novel scheme that restores the benefits of batch normalization in federated learning.","Essentially, FBN ensures that the batch normalization during training is consistent with what would be achieved in a centralized execution, hence preserving the distribution of the data, and providing running statistics that accurately approximate the global statistics.","FBN thereby reduces the external covariate shift and matches the evaluation performance of the centralized setting.","We also show that, with a slight increase in complexity, we can robustify FBN to mitigate erroneous statistics and potentially adversarial attacks."],"url":"http://arxiv.org/abs/2405.14670v1","category":"cs.LG"}
{"created":"2024-05-23 15:04:23","title":"Load Estimation in a Two-Priority mMTC Random Access Channel","abstract":"The use of cellular networks for massive machine-type communications (mMTC) is an appealing solution due to the wide availability of cellular infrastructure. Estimating the number of devices (network load) is vital for efficient allocation of the available resources, especially for managing the random access channel (RACH) of the network. This paper considers a two-priority RACH and proposes two network load estimators: a maximum likelihood (ML) estimator and a reduced complexity (RCML) variant. The estimators are based on a novel model of the random access behavior of the devices coupled with a flexible analytical framework to calculate the involved probabilities. Monte Carlo simulations demonstrate the accuracy of the proposed estimators for different network configurations.","sentences":["The use of cellular networks for massive machine-type communications (mMTC) is an appealing solution due to the wide availability of cellular infrastructure.","Estimating the number of devices (network load) is vital for efficient allocation of the available resources, especially for managing the random access channel (RACH) of the network.","This paper considers a two-priority RACH and proposes two network load estimators: a maximum likelihood (ML) estimator and a reduced complexity (RCML) variant.","The estimators are based on a novel model of the random access behavior of the devices coupled with a flexible analytical framework to calculate the involved probabilities.","Monte Carlo simulations demonstrate the accuracy of the proposed estimators for different network configurations."],"url":"http://arxiv.org/abs/2405.14667v1","category":"eess.SP"}
{"created":"2024-05-23 14:52:35","title":"Total Radiated Power Measurements of a mmWave Phased Array in a Reverberation Chamber","abstract":"This paper explores the use of reverberation chambers for TRP measurements of beamformed radiation by phased arrays at mmWave frequencies. First, the received power was verified by the one-sample K-S GoF test to follow the exponential probability distribution. Different numbers of samples and stirrers' positions were considered. Second, we showed that the effective number of independent samples is different depending on the number of samples and stirrers' positions. Third, the beamforming TRP estimates are presented for all beams, analyzing the statistical significance of the observed differences with a selection of samplings.","sentences":["This paper explores the use of reverberation chambers for TRP measurements of beamformed radiation by phased arrays at mmWave frequencies.","First, the received power was verified by the one-sample K-S GoF test to follow the exponential probability distribution.","Different numbers of samples and stirrers' positions were considered.","Second, we showed that the effective number of independent samples is different depending on the number of samples and stirrers' positions.","Third, the beamforming TRP estimates are presented for all beams, analyzing the statistical significance of the observed differences with a selection of samplings."],"url":"http://arxiv.org/abs/2405.14651v1","category":"eess.SY"}
{"created":"2024-05-23 14:50:59","title":"PhiNets: Brain-inspired Non-contrastive Learning Based on Temporal Prediction Hypothesis","abstract":"SimSiam is a prominent self-supervised learning method that achieves impressive results in various vision tasks under static environments. However, it has two critical issues: high sensitivity to hyperparameters, especially weight decay, and unsatisfactory performance in online and continual learning, where neuroscientists believe that powerful memory functions are necessary, as in brains. In this paper, we propose PhiNet, inspired by a hippocampal model based on the temporal prediction hypothesis. Unlike SimSiam, which aligns two augmented views of the original image, PhiNet integrates an additional predictor block that estimates the original image representation to imitate the CA1 region in the hippocampus. Moreover, we model the neocortex inspired by the Complementary Learning Systems theory with a momentum encoder block as a slow learner, which works as long-term memory. We demonstrate through analysing the learning dynamics that PhiNet benefits from the additional predictor to prevent the complete collapse of learned representations, a notorious challenge in non-contrastive learning. This dynamics analysis may partially corroborate why this hippocampal model is biologically plausible. Experimental results demonstrate that PhiNet is more robust to weight decay and performs better than SimSiam in memory-intensive tasks like online and continual learning.","sentences":["SimSiam is a prominent self-supervised learning method that achieves impressive results in various vision tasks under static environments.","However, it has two critical issues: high sensitivity to hyperparameters, especially weight decay, and unsatisfactory performance in online and continual learning, where neuroscientists believe that powerful memory functions are necessary, as in brains.","In this paper, we propose PhiNet, inspired by a hippocampal model based on the temporal prediction hypothesis.","Unlike SimSiam, which aligns two augmented views of the original image, PhiNet integrates an additional predictor block that estimates the original image representation to imitate the CA1 region in the hippocampus.","Moreover, we model the neocortex inspired by the Complementary Learning Systems theory with a momentum encoder block as a slow learner, which works as long-term memory.","We demonstrate through analysing the learning dynamics that PhiNet benefits from the additional predictor to prevent the complete collapse of learned representations, a notorious challenge in non-contrastive learning.","This dynamics analysis may partially corroborate why this hippocampal model is biologically plausible.","Experimental results demonstrate that PhiNet is more robust to weight decay and performs better than SimSiam in memory-intensive tasks like online and continual learning."],"url":"http://arxiv.org/abs/2405.14650v1","category":"cs.LG"}
{"created":"2024-05-23 14:47:07","title":"Lagrangian Neural Networks for Reversible Dissipative Evolution","abstract":"There is a growing attention given to utilizing Lagrangian and Hamiltonian mechanics with network training in order to incorporate physics into the network. Most commonly, conservative systems are modeled, in which there are no frictional losses, so the system may be run forward and backward in time without requiring regularization. This work addresses systems in which the reverse direction is ill-posed because of the dissipation that occurs in forward evolution. The novelty is the use of Morse-Feshbach Lagrangian, which models dissipative dynamics by doubling the number of dimensions of the system in order to create a mirror latent representation that would counterbalance the dissipation of the observable system, making it a conservative system, albeit embedded in a larger space. We start with their formal approach by redefining a new Dissipative Lagrangian, such that the unknown matrices in the Euler-Lagrange's equations arise as partial derivatives of the Lagrangian with respect to only the observables. We then train a network from simulated training data for dissipative systems such as Fickian diffusion that arise in materials sciences. It is shown by experiments that the systems can be evolved in both forward and reverse directions without regularization beyond that provided by the Morse-Feshbach Lagrangian. Experiments of dissipative systems, such as Fickian diffusion, demonstrate the degree to which dynamics can be reversed.","sentences":["There is a growing attention given to utilizing Lagrangian and Hamiltonian mechanics with network training in order to incorporate physics into the network.","Most commonly, conservative systems are modeled, in which there are no frictional losses, so the system may be run forward and backward in time without requiring regularization.","This work addresses systems in which the reverse direction is ill-posed because of the dissipation that occurs in forward evolution.","The novelty is the use of Morse-Feshbach Lagrangian, which models dissipative dynamics by doubling the number of dimensions of the system in order to create a mirror latent representation that would counterbalance the dissipation of the observable system, making it a conservative system, albeit embedded in a larger space.","We start with their formal approach by redefining a new Dissipative Lagrangian, such that the unknown matrices in the Euler-Lagrange's equations arise as partial derivatives of the Lagrangian with respect to only the observables.","We then train a network from simulated training data for dissipative systems such as Fickian diffusion that arise in materials sciences.","It is shown by experiments that the systems can be evolved in both forward and reverse directions without regularization beyond that provided by the Morse-Feshbach Lagrangian.","Experiments of dissipative systems, such as Fickian diffusion, demonstrate the degree to which dynamics can be reversed."],"url":"http://arxiv.org/abs/2405.14645v1","category":"cs.LG"}
{"created":"2024-05-23 14:46:10","title":"Adoption of a token-based authentication model for the CMS Submission Infrastructure","abstract":"The CMS Submission Infrastructure (SI) is the main computing resource provisioning system for CMS workloads. A number of HTCondor pools are employed to manage this infrastructure, which aggregates geographically distributed resources from the WLCG and other providers. Historically, the model of authentication among the diverse components of this infrastructure has relied on the Grid Security Infrastructure (GSI), based on identities and X509 certificates. In contrast, commonly used modern authentication standards are based on capabilities and tokens. The WLCG has identified this trend and aims at a transparent replacement of GSI for all its workload management, data transfer and storage access operations, to be completed during the current LHC Run 3. As part of this effort, and within the context of CMS computing, the Submission Infrastructure group is in the process of phasing out the GSI part of its authentication layers, in favor of IDTokens and Scitokens. The use of tokens is already well integrated into the HTCondor Software Suite, which has allowed us to fully migrate the authentication between internal components of SI. Additionally, recent versions of the HTCondor-CE support tokens as well, enabling CMS resource requests to Grid sites employing this CE technology to be granted by means of token exchange. After a rollout campaign to sites, successfully completed by the third quarter of 2022, the totality of HTCondor CEs in use by CMS are already receiving Scitoken-based pilot jobs. On the ARC CE side, a parallel campaign was launched to foster the adoption of the REST interface at CMS sites (required to enable token-based job submission via HTCondor-G), which is nearing completion as well. In this contribution, the newly adopted authentication model will be described. We will then report on the migration status and final steps towards complete GSI phase out in the CMS SI.","sentences":["The CMS Submission Infrastructure (SI) is the main computing resource provisioning system for CMS workloads.","A number of HTCondor pools are employed to manage this infrastructure, which aggregates geographically distributed resources from the WLCG and other providers.","Historically, the model of authentication among the diverse components of this infrastructure has relied on the Grid Security Infrastructure (GSI), based on identities and X509 certificates.","In contrast, commonly used modern authentication standards are based on capabilities and tokens.","The WLCG has identified this trend and aims at a transparent replacement of GSI for all its workload management, data transfer and storage access operations, to be completed during the current LHC Run 3.","As part of this effort, and within the context of CMS computing, the Submission Infrastructure group is in the process of phasing out the GSI part of its authentication layers, in favor of IDTokens and Scitokens.","The use of tokens is already well integrated into the HTCondor Software Suite, which has allowed us to fully migrate the authentication between internal components of SI.","Additionally, recent versions of the HTCondor-CE support tokens as well, enabling CMS resource requests to Grid sites employing this CE technology to be granted by means of token exchange.","After a rollout campaign to sites, successfully completed by the third quarter of 2022, the totality of HTCondor CEs in use by CMS are already receiving Scitoken-based pilot jobs.","On the ARC CE side, a parallel campaign was launched to foster the adoption of the REST interface at CMS sites (required to enable token-based job submission via HTCondor-G), which is nearing completion as well.","In this contribution, the newly adopted authentication model will be described.","We will then report on the migration status and final steps towards complete GSI phase out in the CMS SI."],"url":"http://arxiv.org/abs/2405.14644v1","category":"cs.DC"}
{"created":"2024-05-23 14:45:16","title":"Circuit realization of topological physics","abstract":"Recently, topolectrical circuits (TECs) boom in studying the topological states of matter. The resemblance between circuit Laplacians and tight-binding models in condensed matter physics allows for the exploration of exotic topological phases on the circuit platform. In this review, we begin by presenting the basic equations for the circuit elements and units, along with the fundamentals and experimental methods for TECs. Subsequently, we retrospect the main literature in this field, encompassing the circuit realization of (higher-order) topological insulators and semimetals. Due to the abundant electrical elements and flexible connections, many unconventional topological states like the non-Hermitian, nonlinear, non-Abelian, non-periodic, non-Euclidean, and higher-dimensional topological states that are challenging to observe in conventional condensed matter physics, have been observed in circuits and summarized in this review. Furthermore, we show the capability of electrical circuits for exploring the physical phenomena in other systems, such as photonic and magnetic ones. Importantly, we highlight TEC systems are convenient for manufacture and miniaturization because of their compatibility with the traditional integrated circuits. Finally, we prospect the future directions in this exciting field, and connect the emerging TECs with the development of topology physics, (meta)material designs, and device applications.","sentences":["Recently, topolectrical circuits (TECs) boom in studying the topological states of matter.","The resemblance between circuit Laplacians and tight-binding models in condensed matter physics allows for the exploration of exotic topological phases on the circuit platform.","In this review, we begin by presenting the basic equations for the circuit elements and units, along with the fundamentals and experimental methods for TECs.","Subsequently, we retrospect the main literature in this field, encompassing the circuit realization of (higher-order) topological insulators and semimetals.","Due to the abundant electrical elements and flexible connections, many unconventional topological states like the non-Hermitian, nonlinear, non-Abelian, non-periodic, non-Euclidean, and higher-dimensional topological states that are challenging to observe in conventional condensed matter physics, have been observed in circuits and summarized in this review.","Furthermore, we show the capability of electrical circuits for exploring the physical phenomena in other systems, such as photonic and magnetic ones.","Importantly, we highlight TEC systems are convenient for manufacture and miniaturization because of their compatibility with the traditional integrated circuits.","Finally, we prospect the future directions in this exciting field, and connect the emerging TECs with the development of topology physics, (meta)material designs, and device applications."],"url":"http://arxiv.org/abs/2405.14643v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 14:44:49","title":"GPU Implementations for Midsize Integer Addition and Multiplication","abstract":"This paper explores practical aspects of using a high-level functional language for GPU-based arithmetic on ``midsize'' integers. By this we mean integers of up to about a quarter million bits, which is sufficient for most practical purposes. The goal is to understand whether it is possible to support efficient nested-parallel programs with a small, flexible code base. We report on GPU implementations for addition and multiplication of integers that fit in one CUDA block, thus leveraging temporal reuse from scratchpad memories. Our key contribution resides in the simplicity of the proposed solutions: We recognize that addition is a straightforward application of scan, which is known to allow efficient GPU implementation. For quadratic multiplication we employ a simple work-partitioning strategy that offers good temporal locality. For FFT multiplication, we efficiently map the computation in the domain of integral fields by finding ``good'' primes that enable almost-full utilization of machine words. In comparison, related work uses complex tiling strategies -- which feel too big a hammer for the job -- or uses the computational domain of reals, which may degrade the magnitude of the base in which the computation is carried. We evaluate the performance in comparison to the state-of-the-art CGBN library, authored by NvidiaLab, and report that our CUDA prototype outperforms CGBN for integer sizes higher than 32K bits, while offering comparable performance for smaller sizes. Moreover, we are, to our knowledge, the first to report that FFT multiplication outperforms the classical one on the larger sizes that still fit in a CUDA block. Finally, we examine Futhark's strengths and weaknesses for efficiently supporting such computations and find out that a compiler pass aimed at efficient sequentialization of excess parallelism would significantly improve performance.","sentences":["This paper explores practical aspects of using a high-level functional language for GPU-based arithmetic on ``midsize'' integers.","By this we mean integers of up to about a quarter million bits, which is sufficient for most practical purposes.","The goal is to understand whether it is possible to support efficient nested-parallel programs with a small, flexible code base.","We report on GPU implementations for addition and multiplication of integers that fit in one CUDA block, thus leveraging temporal reuse from scratchpad memories.","Our key contribution resides in the simplicity of the proposed solutions: We recognize that addition is a straightforward application of scan, which is known to allow efficient GPU implementation.","For quadratic multiplication we employ a simple work-partitioning strategy that offers good temporal locality.","For FFT multiplication, we efficiently map the computation in the domain of integral fields by finding ``good'' primes that enable almost-full utilization of machine words.","In comparison, related work uses complex tiling strategies -- which feel too big a hammer for the job -- or uses the computational domain of reals, which may degrade the magnitude of the base in which the computation is carried.","We evaluate the performance in comparison to the state-of-the-art CGBN library, authored by NvidiaLab, and report that our CUDA prototype outperforms CGBN for integer sizes higher than 32K bits, while offering comparable performance for smaller sizes.","Moreover, we are, to our knowledge, the first to report that FFT multiplication outperforms the classical one on the larger sizes that still fit in a CUDA block.","Finally, we examine Futhark's strengths and weaknesses for efficiently supporting such computations and find out that a compiler pass aimed at efficient sequentialization of excess parallelism would significantly improve performance."],"url":"http://arxiv.org/abs/2405.14642v1","category":"cs.DC"}
{"created":"2024-05-23 14:41:22","title":"PerLLM: Personalized Inference Scheduling with Edge-Cloud Collaboration for Diverse LLM Services","abstract":"With the rapid growth in the number of large language model (LLM) users, it is difficult for bandwidth-constrained cloud servers to simultaneously process massive LLM services in real-time. Recently, edge-cloud infrastructures have been used to improve the processing efficiency of large-scale LLM services. However, the diversity of task requirements and the dynamics of resources pose great challenges to inference scheduling, leading to the wastage of many resources. In this paper, we present PerLLM, a personalized inference scheduling framework with edge-cloud collaboration designed for diverse LLM services. For the complexity of multiple constraints and the decision-making process of edge-cloud collaboration, we integrate the upper confidence bound algorithm based on the constraint satisfaction mechanism in PerLLM. For diverse LLM services, PerLLM can optimize service scheduling and resource allocation solutions within the edge-cloud infrastructure to meet processing time requirements while minimizing energy costs. Experimental results from different model deployments show that PerLLM can effectively meet the processing time requirements of personalized services. Compared to other methods, PerLLM achieves 2.2x, 2.1x, and 1.6x throughput and reduces the energy cost by more than 50%.","sentences":["With the rapid growth in the number of large language model (LLM) users, it is difficult for bandwidth-constrained cloud servers to simultaneously process massive LLM services in real-time.","Recently, edge-cloud infrastructures have been used to improve the processing efficiency of large-scale LLM services.","However, the diversity of task requirements and the dynamics of resources pose great challenges to inference scheduling, leading to the wastage of many resources.","In this paper, we present PerLLM, a personalized inference scheduling framework with edge-cloud collaboration designed for diverse LLM services.","For the complexity of multiple constraints and the decision-making process of edge-cloud collaboration, we integrate the upper confidence bound algorithm based on the constraint satisfaction mechanism in PerLLM.","For diverse LLM services, PerLLM can optimize service scheduling and resource allocation solutions within the edge-cloud infrastructure to meet processing time requirements while minimizing energy costs.","Experimental results from different model deployments show that PerLLM can effectively meet the processing time requirements of personalized services.","Compared to other methods, PerLLM achieves 2.2x, 2.1x, and 1.6x throughput and reduces the energy cost by more than 50%."],"url":"http://arxiv.org/abs/2405.14636v1","category":"cs.DC"}
{"created":"2024-05-23 14:39:52","title":"Flatten Anything: Unsupervised Neural Surface Parameterization","abstract":"Surface parameterization plays an essential role in numerous computer graphics and geometry processing applications. Traditional parameterization approaches are designed for high-quality meshes laboriously created by specialized 3D modelers, thus unable to meet the processing demand for the current explosion of ordinary 3D data. Moreover, their working mechanisms are typically restricted to certain simple topologies, thus relying on cumbersome manual efforts (e.g., surface cutting, part segmentation) for pre-processing. In this paper, we introduce the Flatten Anything Model (FAM), an unsupervised neural architecture to achieve global free-boundary surface parameterization via learning point-wise mappings between 3D points on the target geometric surface and adaptively-deformed UV coordinates within the 2D parameter domain. To mimic the actual physical procedures, we ingeniously construct geometrically-interpretable sub-networks with specific functionalities of surface cutting, UV deforming, unwrapping, and wrapping, which are assembled into a bi-directional cycle mapping framework. Compared with previous methods, our FAM directly operates on discrete surface points without utilizing connectivity information, thus significantly reducing the strict requirements for mesh quality and even applicable to unstructured point cloud data. More importantly, our FAM is fully-automated without the need for pre-cutting and can deal with highly-complex topologies, since its learning process adaptively finds reasonable cutting seams and UV boundaries. Extensive experiments demonstrate the universality, superiority, and inspiring potential of our proposed neural surface parameterization paradigm. The code will be publicly available.","sentences":["Surface parameterization plays an essential role in numerous computer graphics and geometry processing applications.","Traditional parameterization approaches are designed for high-quality meshes laboriously created by specialized 3D modelers, thus unable to meet the processing demand for the current explosion of ordinary 3D data.","Moreover, their working mechanisms are typically restricted to certain simple topologies, thus relying on cumbersome manual efforts (e.g., surface cutting, part segmentation) for pre-processing.","In this paper, we introduce the Flatten Anything Model (FAM), an unsupervised neural architecture to achieve global free-boundary surface parameterization via learning point-wise mappings between 3D points on the target geometric surface and adaptively-deformed UV coordinates within the 2D parameter domain.","To mimic the actual physical procedures, we ingeniously construct geometrically-interpretable sub-networks with specific functionalities of surface cutting, UV deforming, unwrapping, and wrapping, which are assembled into a bi-directional cycle mapping framework.","Compared with previous methods, our FAM directly operates on discrete surface points without utilizing connectivity information, thus significantly reducing the strict requirements for mesh quality and even applicable to unstructured point cloud data.","More importantly, our FAM is fully-automated without the need for pre-cutting and can deal with highly-complex topologies, since its learning process adaptively finds reasonable cutting seams and UV boundaries.","Extensive experiments demonstrate the universality, superiority, and inspiring potential of our proposed neural surface parameterization paradigm.","The code will be publicly available."],"url":"http://arxiv.org/abs/2405.14633v1","category":"cs.CV"}
{"created":"2024-05-23 14:36:59","title":"HPC resources for CMS offline computing: An integration and scalability challenge for the Submission Infrastructure","abstract":"The computing resource needs of LHC experiments are expected to continue growing significantly during the Run 3 and into the HL-LHC era. The landscape of available resources will also evolve, as High Performance Computing (HPC) and Cloud resources will provide a comparable, or even dominant, fraction of the total compute capacity. The future years present a challenge for the experiments' resource provisioning models, both in terms of scalability and increasing complexity. The CMS Submission Infrastructure (SI) provisions computing resources for CMS workflows. This infrastructure is built on a set of federated HTCondor pools, currently aggregating 400k CPU cores distributed worldwide and supporting the simultaneous execution of over 200k computing tasks. Incorporating HPC resources into CMS computing represents firstly an integration challenge, as HPC centers are much more diverse compared to Grid sites. Secondly, evolving the present SI, dimensioned to harness the current CMS computing capacity, to reach the resource scales required for the HLLHC phase, while maintaining global flexibility and efficiency, will represent an additional challenge for the SI. To preventively address future potential scalability limits, the SI team regularly runs tests to explore the maximum reach of our infrastructure. In this note, the integration of HPC resources into CMS offline computing is summarized, the potential concerns for the SI derived from the increased scale of operations are described, and the most recent results of scalability test on the CMS SI are reported.","sentences":["The computing resource needs of LHC experiments are expected to continue growing significantly during the Run 3 and into the HL-LHC era.","The landscape of available resources will also evolve, as High Performance Computing (HPC) and Cloud resources will provide a comparable, or even dominant, fraction of the total compute capacity.","The future years present a challenge for the experiments' resource provisioning models, both in terms of scalability and increasing complexity.","The CMS Submission Infrastructure (SI) provisions computing resources for CMS workflows.","This infrastructure is built on a set of federated HTCondor pools, currently aggregating 400k CPU cores distributed worldwide and supporting the simultaneous execution of over 200k computing tasks.","Incorporating HPC resources into CMS computing represents firstly an integration challenge, as HPC centers are much more diverse compared to Grid sites.","Secondly, evolving the present SI, dimensioned to harness the current CMS computing capacity, to reach the resource scales required for the HLLHC phase, while maintaining global flexibility and efficiency, will represent an additional challenge for the SI.","To preventively address future potential scalability limits, the SI team regularly runs tests to explore the maximum reach of our infrastructure.","In this note, the integration of HPC resources into CMS offline computing is summarized, the potential concerns for the SI derived from the increased scale of operations are described, and the most recent results of scalability test on the CMS SI are reported."],"url":"http://arxiv.org/abs/2405.14631v1","category":"cs.DC"}
{"created":"2024-05-23 14:32:04","title":"Quantum Simulation of Spin-Boson Models with Structured Bath","abstract":"The spin-boson model, involving a spin interacting with a bath of quantum harmonic oscillators, is a widely used representation of open quantum systems. Trapped ions present a natural platform for the quantum simulation of such models; however, previous experiments have been limited to simulating a scenario where a spin is coherently coupled to bosonic modes, which neglects capturing the dissipation of the bath. In our work, we perform quantum simulations of spin-boson models with structured baths using the motional states of trapped ions. We demonstrate the capability for adjusting the bath's temperature and continuous spectral density by adding randomness to the control parameters. Subsequently, we simulate the dynamics of various spin-boson models with spectral densities composed of up to three Lorentzian peaks. The experimental outcomes closely align with theoretical predictions, suggesting the advantage of adding the trapped-ion system's experimental noise for simulating open quantum systems.","sentences":["The spin-boson model, involving a spin interacting with a bath of quantum harmonic oscillators, is a widely used representation of open quantum systems.","Trapped ions present a natural platform for the quantum simulation of such models; however, previous experiments have been limited to simulating a scenario where a spin is coherently coupled to bosonic modes, which neglects capturing the dissipation of the bath.","In our work, we perform quantum simulations of spin-boson models with structured baths using the motional states of trapped ions.","We demonstrate the capability for adjusting the bath's temperature and continuous spectral density by adding randomness to the control parameters.","Subsequently, we simulate the dynamics of various spin-boson models with spectral densities composed of up to three Lorentzian peaks.","The experimental outcomes closely align with theoretical predictions, suggesting the advantage of adding the trapped-ion system's experimental noise for simulating open quantum systems."],"url":"http://arxiv.org/abs/2405.14624v1","category":"quant-ph"}
{"created":"2024-05-23 14:29:15","title":"Closed-form Symbolic Solutions: A New Perspective on Solving Partial Differential Equations","abstract":"Solving partial differential equations (PDEs) in Euclidean space with closed-form symbolic solutions has long been a dream for mathematicians. Inspired by deep learning, Physics-Informed Neural Networks (PINNs) have shown great promise in numerically solving PDEs. However, since PINNs essentially approximate solutions within the continuous function space, their numerical solutions fall short in both precision and interpretability compared to symbolic solutions. This paper proposes a novel framework: a closed-form \\textbf{Sym}bolic framework for \\textbf{PDE}s (SymPDE), exploring the use of deep reinforcement learning to directly obtain symbolic solutions for PDEs. SymPDE alleviates the challenges PINNs face in fitting high-frequency and steeply changing functions. To our knowledge, no prior work has implemented this approach. Experiments on solving the Poisson's equation and heat equation in time-independent and spatiotemporal dynamical systems respectively demonstrate that SymPDE can provide accurate closed-form symbolic solutions for various types of PDEs.","sentences":["Solving partial differential equations (PDEs) in Euclidean space with closed-form symbolic solutions has long been a dream for mathematicians.","Inspired by deep learning, Physics-Informed Neural Networks (PINNs) have shown great promise in numerically solving PDEs.","However, since PINNs essentially approximate solutions within the continuous function space, their numerical solutions fall short in both precision and interpretability compared to symbolic solutions.","This paper proposes a novel framework: a closed-form \\textbf{Sym}bolic framework for \\textbf{PDE}s (SymPDE), exploring the use of deep reinforcement learning to directly obtain symbolic solutions for PDEs.","SymPDE alleviates the challenges PINNs face in fitting high-frequency and steeply changing functions.","To our knowledge, no prior work has implemented this approach.","Experiments on solving the Poisson's equation and heat equation in time-independent and spatiotemporal dynamical systems respectively demonstrate that SymPDE can provide accurate closed-form symbolic solutions for various types of PDEs."],"url":"http://arxiv.org/abs/2405.14620v1","category":"cs.LG"}
{"created":"2024-05-23 14:27:50","title":"Phase Separations of Strongly Coupled Fine Particles and Fine Particle Mixtures in Plasmas","abstract":"Phase separations in strongly coupled fine particles in plasmas are discussed and two-component mixtures are simulated by molecular dynamics with the background plasma being treated as continuum. The system size of laboratory experiments is assumed and separations into phases with a common electron density of the background plasma are analyzed. Since the charge on fine particles increases approximately in proportion to the size, we expect the larger component with stronger coupling condensates from the mixture. Results expressed in terms of strengths of Coulomb coupling and screening of the larger component seem to be mostly similar to the one-component case, at least in cases where the ratio of fine particle sizes is 2 and the mixing ratio is in the range from 0.25 to 0.75.","sentences":["Phase separations in strongly coupled fine particles in plasmas are discussed and two-component mixtures are simulated by molecular dynamics with the background plasma being treated as continuum.","The system size of laboratory experiments is assumed and separations into phases with a common electron density of the background plasma are analyzed.","Since the charge on fine particles increases approximately in proportion to the size, we expect the larger component with stronger coupling condensates from the mixture.","Results expressed in terms of strengths of Coulomb coupling and screening of the larger component seem to be mostly similar to the one-component case, at least in cases where the ratio of fine particle sizes is 2 and the mixing ratio is in the range from 0.25 to 0.75."],"url":"http://arxiv.org/abs/2405.14618v1","category":"physics.plasm-ph"}
{"created":"2024-05-23 14:22:38","title":"Dimensions of Riesz products and pluriharmonic measures","abstract":"We estimate the energy and Hausdorff dimensions of the Riesz products on the unit sphere of $\\mathbb{C}^n$, $n\\ge 2$. Also, we obtain similar results for the pluriharmonic measures on the torus.","sentences":["We estimate the energy and Hausdorff dimensions of the Riesz products on the unit sphere of $\\mathbb{C}^n$, $n\\ge 2$. Also, we obtain similar results for the pluriharmonic measures on the torus."],"url":"http://arxiv.org/abs/2405.14609v1","category":"math.CV"}
{"created":"2024-05-23 14:17:33","title":"Spectral analysis of block preconditioners for double saddle-point linear systems with application to PDE-constrained optimization","abstract":"In this paper, we describe and analyze the spectral properties of a symmetric positive definite inexact block preconditioner for a class of symmetric, double saddle-point linear systems.   We develop a spectral analysis of the preconditioned matrix, showing that its eigenvalues can be described in terms of the roots of a cubic polynomial with real coefficients.   We illustrate the efficiency of the proposed preconditioners, and verify the theoretical bounds, in solving large-scale PDE-constrained optimization problems.","sentences":["In this paper, we describe and analyze the spectral properties of a symmetric positive definite inexact block preconditioner for a class of symmetric, double saddle-point linear systems.   ","We develop a spectral analysis of the preconditioned matrix, showing that its eigenvalues can be described in terms of the roots of a cubic polynomial with real coefficients.   ","We illustrate the efficiency of the proposed preconditioners, and verify the theoretical bounds, in solving large-scale PDE-constrained optimization problems."],"url":"http://arxiv.org/abs/2405.14605v1","category":"math.NA"}
{"created":"2024-05-23 14:17:05","title":"The role of excitation vector fields and all-polarisation state control of cavity magnonics","abstract":"Recently the field of cavity magnonics, a field focused on controlling the interaction between magnons and confined microwave photons within microwave resonators, has drawn significant attention as it offers a platform for enabling advancements in quantum- and spin-based technologies. Here, we introduce excitation vector fields, whose polarisation and profile can be easily tuned in a two-port cavity setup, thus acting as an effective experimental knob to explore the coupled dynamics of cavity magnon-polaritons. Moreover, we develop theoretical models that accurately predict and reproduce the experimental results for any polarisation state and field profile within the cavity resonator. This versatile experimental platform offers a new avenue for controlling spin-photon interactions and as such also delivering a mechanism to readily control the exchange of information between hybrid systems.","sentences":["Recently the field of cavity magnonics, a field focused on controlling the interaction between magnons and confined microwave photons within microwave resonators, has drawn significant attention as it offers a platform for enabling advancements in quantum- and spin-based technologies.","Here, we introduce excitation vector fields, whose polarisation and profile can be easily tuned in a two-port cavity setup, thus acting as an effective experimental knob to explore the coupled dynamics of cavity magnon-polaritons.","Moreover, we develop theoretical models that accurately predict and reproduce the experimental results for any polarisation state and field profile within the cavity resonator.","This versatile experimental platform offers a new avenue for controlling spin-photon interactions and as such also delivering a mechanism to readily control the exchange of information between hybrid systems."],"url":"http://arxiv.org/abs/2405.14603v1","category":"quant-ph"}
{"created":"2024-05-23 14:08:27","title":"On the role of the fast oscillations in the secular dynamics of the lunar coplanar perturbation on Galileo satellites","abstract":"Motivated by the practical interest in the third-body perturbation as a natural cleaning mechanism for high-altitude Earth orbits, we investigate the dynamics stemming from the secular Hamiltonian associated with the lunar perturbation, assuming that the Moon lies on the ecliptic plane. The secular Hamiltonian defined in that way is characterized by two timescales. We compare the location and stability of the fixed points associated with the secular Hamiltonian averaged with respect to the fast variable with the corresponding periodic orbits of the full system. Focusing on the orbit of the Galileo satellites, it turns out that the two dynamics cannot be confused, as the relative difference depends on the ratio between the semi-major axis of Galileo and the one of the Moon, that is not negligible.   The result is relevant to construct rigorously the Arnold diffusion mechanism that can drive a natural growth in eccentricity that allows a satellite initially on a circular orbit in Medium Earth Orbit to reenter into the Earth's atmosphere.","sentences":["Motivated by the practical interest in the third-body perturbation as a natural cleaning mechanism for high-altitude Earth orbits, we investigate the dynamics stemming from the secular Hamiltonian associated with the lunar perturbation, assuming that the Moon lies on the ecliptic plane.","The secular Hamiltonian defined in that way is characterized by two timescales.","We compare the location and stability of the fixed points associated with the secular Hamiltonian averaged with respect to the fast variable with the corresponding periodic orbits of the full system.","Focusing on the orbit of the Galileo satellites, it turns out that the two dynamics cannot be confused, as the relative difference depends on the ratio between the semi-major axis of Galileo and the one of the Moon, that is not negligible.   ","The result is relevant to construct rigorously the Arnold diffusion mechanism that can drive a natural growth in eccentricity that allows a satellite initially on a circular orbit in Medium Earth Orbit to reenter into the Earth's atmosphere."],"url":"http://arxiv.org/abs/2405.14593v1","category":"astro-ph.EP"}
{"created":"2024-05-23 13:58:50","title":"Effects of Topological Boundary Conditions on Bell Nonlocality","abstract":"Bell nonlocality is the resource that enables device-independent quantum information processing tasks. It is revealed through the violation of so-called Bell inequalities, indicating that the observed correlations cannot be reproduced by any local hidden variable model. While well explored in few-body settings, the question of which Bell inequalities are best suited for a given task remains quite open in the many-body scenario. One natural approach is to assign Bell inequalities to physical Hamiltonians, mapping their interaction graph to two-body, nearest-neighbor terms. Here, we investigate the effect of boundary conditions in a two-dimensional square lattice, which can induce different topologies in lattice systems. We find a relation between the induced topology and the Bell inequality's effectiveness in revealing nonlocal correlations. By using a combination of tropical algebra and tensor networks, we quantify their detection capacity for nonlocality. Our work can act as a guide to certify Bell nonlocality in many-qubit devices by choosing a suitable Hamiltonian and measuring its ground state energy; a task that many quantum experiments are purposely built for.","sentences":["Bell nonlocality is the resource that enables device-independent quantum information processing tasks.","It is revealed through the violation of so-called Bell inequalities, indicating that the observed correlations cannot be reproduced by any local hidden variable model.","While well explored in few-body settings, the question of which Bell inequalities are best suited for a given task remains quite open in the many-body scenario.","One natural approach is to assign Bell inequalities to physical Hamiltonians, mapping their interaction graph to two-body, nearest-neighbor terms.","Here, we investigate the effect of boundary conditions in a two-dimensional square lattice, which can induce different topologies in lattice systems.","We find a relation between the induced topology and the Bell inequality's effectiveness in revealing nonlocal correlations.","By using a combination of tropical algebra and tensor networks, we quantify their detection capacity for nonlocality.","Our work can act as a guide to certify Bell nonlocality in many-qubit devices by choosing a suitable Hamiltonian and measuring its ground state energy; a task that many quantum experiments are purposely built for."],"url":"http://arxiv.org/abs/2405.14587v1","category":"quant-ph"}
{"created":"2024-05-23 13:58:32","title":"Low-frequency signature of magnetization nutation in nanomagnets","abstract":"In this work, we show that surface anisotropy in nanomagnets induces a nutational motion of their magnetization at various frequencies, the lowest of which can be described by the macrospin model whose dynamics is governed by an effective energy potential. We derive analytical expressions for the precession and nutation frequencies and amplitudes as functions of the size of the nanomagnet and its atomistic parameters, such as the exchange coupling and the onsite anisotropy. Our analytical model predicts a reduction of the precession frequency with increased surface anisotropy. We also simulate the dynamics of the corresponding atomistic many-spin system and compare the results with the effective model. We thereby show that the first nutation mode induced by the finite size and surface anisotropy occurs at a frequency that is four times larger than the precession frequency, thus lending itself to a relatively easy detection by standard experiments of magnetic resonance.","sentences":["In this work, we show that surface anisotropy in nanomagnets induces a nutational motion of their magnetization at various frequencies, the lowest of which can be described by the macrospin model whose dynamics is governed by an effective energy potential.","We derive analytical expressions for the precession and nutation frequencies and amplitudes as functions of the size of the nanomagnet and its atomistic parameters, such as the exchange coupling and the onsite anisotropy.","Our analytical model predicts a reduction of the precession frequency with increased surface anisotropy.","We also simulate the dynamics of the corresponding atomistic many-spin system and compare the results with the effective model.","We thereby show that the first nutation mode induced by the finite size and surface anisotropy occurs at a frequency that is four times larger than the precession frequency, thus lending itself to a relatively easy detection by standard experiments of magnetic resonance."],"url":"http://arxiv.org/abs/2405.14586v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 13:54:12","title":"Anosov vector fields and Fried sections","abstract":"The purpose of this paper is to prove that if $Y$ is a compact manifold, if $Z$ is an Anosov vector field on $Y$, and if $F$ is a flat vector bundle, then there is a corresponding canonical nonzero section $\\tau_{\\nu}\\left(i_{Z}\\right)$ of the determinant line $\\nu=\\mathrm{det} H\\left(Y,F\\right)$. In families, this section is $C^{1}$. When $F$ is flat on the total space of the corresponding fibration, our section is flat with respect to the Gauss-Manin connection on $\\nu$.","sentences":["The purpose of this paper is to prove that if $Y$ is a compact manifold, if $Z$ is an Anosov vector field on $Y$, and if $F$ is a flat vector bundle, then there is a corresponding canonical nonzero section $\\tau_{\\nu}\\left(i_{Z}\\right)$ of the determinant line $\\nu=\\mathrm{det} H\\left(Y,F\\right)$.","In families, this section is $C^{1}$. When $F$ is flat on the total space of the corresponding fibration, our section is flat with respect to the Gauss-Manin connection on $\\nu$."],"url":"http://arxiv.org/abs/2405.14583v1","category":"math.DG"}
{"created":"2024-05-23 13:47:31","title":"Multicontinuum Homogenization for Coupled Flow and Transport Equations","abstract":"In this paper, we present the derivation of a multicontinuum model for the coupled flow and transport equations by applying multicontinuum homogenization. We perform the multicontinuum expansion for both flow and transport solutions and formulate novel coupled constraint cell problems to capture the multiscale property, where oversampled regions are utilized to avoid boundary effects. Assuming the smoothness of macroscopic variables, we obtain a multicontinuum system composed of macroscopic elliptic equations and convection-diffusion-reaction equations with homogenized effective properties. Finally, we present numerical results for various coefficient fields and boundary conditions to validate our proposed algorithm.","sentences":["In this paper, we present the derivation of a multicontinuum model for the coupled flow and transport equations by applying multicontinuum homogenization.","We perform the multicontinuum expansion for both flow and transport solutions and formulate novel coupled constraint cell problems to capture the multiscale property, where oversampled regions are utilized to avoid boundary effects.","Assuming the smoothness of macroscopic variables, we obtain a multicontinuum system composed of macroscopic elliptic equations and convection-diffusion-reaction equations with homogenized effective properties.","Finally, we present numerical results for various coefficient fields and boundary conditions to validate our proposed algorithm."],"url":"http://arxiv.org/abs/2405.14572v1","category":"math.NA"}
{"created":"2024-05-23 13:40:54","title":"Quantum Mixtures of Ultracold Atomic Gases","abstract":"The combination of different kinds of ultracold gases constitutes a novel powerful experimental framework for the investigation of a variety of physical problems. We illustrate the differences among possible quantum mixtures, be they homonuclear spin mixtures or heteronuclear ones, and show how they can be exploited to investigate a plethora of topics from the few-body to the many-body regimes. In particular, we discuss quantum mixtures of ultracold gases under three different perspectives: systems made of a few atoms of different kinds, single impurities immersed in a host quantum gas, and quantum mixtures of two interacting gases. Given the broad spectrum of possible topics and experimental regimes, in this review we restrict the discussion on single harmonic or flat traps, predominantly in a three-dimensional configuration. A selection of results on recent experiments and possible interesting future directions are given.","sentences":["The combination of different kinds of ultracold gases constitutes a novel powerful experimental framework for the investigation of a variety of physical problems.","We illustrate the differences among possible quantum mixtures, be they homonuclear spin mixtures or heteronuclear ones, and show how they can be exploited to investigate a plethora of topics from the few-body to the many-body regimes.","In particular, we discuss quantum mixtures of ultracold gases under three different perspectives: systems made of a few atoms of different kinds, single impurities immersed in a host quantum gas, and quantum mixtures of two interacting gases.","Given the broad spectrum of possible topics and experimental regimes, in this review we restrict the discussion on single harmonic or flat traps, predominantly in a three-dimensional configuration.","A selection of results on recent experiments and possible interesting future directions are given."],"url":"http://arxiv.org/abs/2405.14562v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-23 13:37:26","title":"FUSE: Fast Unified Simulation and Estimation for PDEs","abstract":"The joint prediction of continuous fields and statistical estimation of the underlying discrete parameters is a common problem for many physical systems, governed by PDEs. Hitherto, it has been separately addressed by employing operator learning surrogates for field prediction while using simulation-based inference (and its variants) for statistical parameter determination. Here, we argue that solving both problems within the same framework can lead to consistent gains in accuracy and robustness. To this end, We propose a novel and flexible formulation of the operator learning problem that allows jointly predicting continuous quantities and inferring distributions of discrete parameters, and thus amortizing the cost of both the inverse and the surrogate models to a joint pre-training step. We present the capabilities of the proposed methodology for predicting continuous and discrete biomarkers in full-body haemodynamics simulations under different levels of missing information. We also consider a test case for atmospheric large-eddy simulation of a two-dimensional dry cold bubble, where we infer both continuous time-series and information about the systems conditions. We present comparisons against different baselines to showcase significantly increased accuracy in both the inverse and the surrogate tasks.","sentences":["The joint prediction of continuous fields and statistical estimation of the underlying discrete parameters is a common problem for many physical systems, governed by PDEs.","Hitherto, it has been separately addressed by employing operator learning surrogates for field prediction while using simulation-based inference (and its variants) for statistical parameter determination.","Here, we argue that solving both problems within the same framework can lead to consistent gains in accuracy and robustness.","To this end, We propose a novel and flexible formulation of the operator learning problem that allows jointly predicting continuous quantities and inferring distributions of discrete parameters, and thus amortizing the cost of both the inverse and the surrogate models to a joint pre-training step.","We present the capabilities of the proposed methodology for predicting continuous and discrete biomarkers in full-body haemodynamics simulations under different levels of missing information.","We also consider a test case for atmospheric large-eddy simulation of a two-dimensional dry cold bubble, where we infer both continuous time-series and information about the systems conditions.","We present comparisons against different baselines to showcase significantly increased accuracy in both the inverse and the surrogate tasks."],"url":"http://arxiv.org/abs/2405.14558v1","category":"cs.LG"}
{"created":"2024-05-23 13:31:04","title":"Lattice Boltzmann methods for soft flowing matter","abstract":"Over the last decade, the Lattice Boltzmann method has found major scope for the simulation of a large spectrum of problems in soft matter, from multiphase and multi-component microfluidic flows, to foams, emulsions, colloidal flows, to name but a few. Crucial to many such applications is the role of supramolecular interactions which occur whenever mesoscale structures, such as bubbles or droplets, come in close contact, say of the order of tens of nanometers. Regardless of their specific physico-chemical origin, such near-contact interactions are vital to preserve the coherence of the mesoscale structures against coalescence phenomena promoted by capillarity and surface tension, hence the need of including them in Lattice Boltzmann schemes. Strictly speaking, this entails a complex multiscale problem, covering about six spatial decades, from centimeters down to tens of nanometers, and almost twice as many in time. Such a multiscale problem can hardly be taken by a single computational method, hence the need for coarse-grained models for the near-contact interactions. In this review, we shall discuss such coarse-grained models and illustrate their application to a variety of soft flowing matter problems, such as soft flowing crystals, strongly confined dense emulsions, flowing hierarchical emulsions, soft granular flows, as well as the transmigration of active droplets across constrictions. Finally, we conclude with a few considerations on future developments in the direction of quantum-nanofluidics, machine learning, and quantum computing for soft flows applications.","sentences":["Over the last decade, the Lattice Boltzmann method has found major scope for the simulation of a large spectrum of problems in soft matter, from multiphase and multi-component microfluidic flows, to foams, emulsions, colloidal flows, to name but a few.","Crucial to many such applications is the role of supramolecular interactions which occur whenever mesoscale structures, such as bubbles or droplets, come in close contact, say of the order of tens of nanometers.","Regardless of their specific physico-chemical origin, such near-contact interactions are vital to preserve the coherence of the mesoscale structures against coalescence phenomena promoted by capillarity and surface tension, hence the need of including them in Lattice Boltzmann schemes.","Strictly speaking, this entails a complex multiscale problem, covering about six spatial decades, from centimeters down to tens of nanometers, and almost twice as many in time.","Such a multiscale problem can hardly be taken by a single computational method, hence the need for coarse-grained models for the near-contact interactions.","In this review, we shall discuss such coarse-grained models and illustrate their application to a variety of soft flowing matter problems, such as soft flowing crystals, strongly confined dense emulsions, flowing hierarchical emulsions, soft granular flows, as well as the transmigration of active droplets across constrictions.","Finally, we conclude with a few considerations on future developments in the direction of quantum-nanofluidics, machine learning, and quantum computing for soft flows applications."],"url":"http://arxiv.org/abs/2405.14551v1","category":"cond-mat.soft"}
{"created":"2024-05-23 13:25:39","title":"Global Behavior of Learning Dynamics in Zero-Sum Games with Memory Asymmetry","abstract":"This study examines the global behavior of dynamics in learning in games between two players, X and Y. We consider the simplest situation for memory asymmetry between two players: X memorizes the other Y's previous action and uses reactive strategies, while Y has no memory. Although this memory complicates the learning dynamics, we discover two novel quantities that characterize the global behavior of such complex dynamics. One is an extended Kullback-Leibler divergence from the Nash equilibrium, a well-known conserved quantity from previous studies. The other is a family of Lyapunov functions of X's reactive strategy. These two quantities capture the global behavior in which X's strategy becomes more exploitative, and the exploited Y's strategy converges to the Nash equilibrium. Indeed, we theoretically prove that Y's strategy globally converges to the Nash equilibrium in the simplest game equipped with an equilibrium in the interior of strategy spaces. Furthermore, our experiments also suggest that this global convergence is universal for more advanced zero-sum games than the simplest game. This study provides a novel characterization of the global behavior of learning in games through a couple of indicators.","sentences":["This study examines the global behavior of dynamics in learning in games between two players, X and Y. We consider the simplest situation for memory asymmetry between two players: X memorizes the other Y's previous action and uses reactive strategies, while Y has no memory.","Although this memory complicates the learning dynamics, we discover two novel quantities that characterize the global behavior of such complex dynamics.","One is an extended Kullback-Leibler divergence from the Nash equilibrium, a well-known conserved quantity from previous studies.","The other is a family of Lyapunov functions of X's reactive strategy.","These two quantities capture the global behavior in which X's strategy becomes more exploitative, and the exploited Y's strategy converges to the Nash equilibrium.","Indeed, we theoretically prove that Y's strategy globally converges to the Nash equilibrium in the simplest game equipped with an equilibrium in the interior of strategy spaces.","Furthermore, our experiments also suggest that this global convergence is universal for more advanced zero-sum games than the simplest game.","This study provides a novel characterization of the global behavior of learning in games through a couple of indicators."],"url":"http://arxiv.org/abs/2405.14546v1","category":"cs.GT"}
{"created":"2024-05-23 13:23:49","title":"Emergence of metastability in frustrated oscillatory networks: the key role of hierarchical modularity","abstract":"Oscillatory complex networks in the metastable regime have been used to study the emergence of integrated and segregated activity in the brain, which are hypothesised to be fundamental for cognition. Yet, the parameters and the underlying mechanisms necessary to achieve the metastable regime are hard to identify, often relying on maximising the correlation with empirical functional connectivity dynamics. Here, we propose and show that the brain's hierarchically modular mesoscale structure alone can give rise to robust metastable dynamics and (metastable) chimera states in the presence of phase frustration. We construct unweighted $3$-layer hierarchical networks of identical Kuramoto-Sakaguchi oscillators, parameterized by the average degree of the network and a structural parameter determining the ratio of connections between and within blocks in the upper two layers. Together, these parameters affect the characteristic timescales of the system. Away from the critical synchronization point, we detect the emergence of metastable states in the lowest hierarchical layer coexisting with chimera and metastable states in the upper layers. Using the Laplacian renormalization group flow approach, we uncover two distinct pathways towards achieving the metastable regimes detected in these distinct layers. In the upper layers, we show how the symmetry-breaking states depend on the slow eigenmodes of the system. In the lowest layer instead, metastable dynamics can be achieved as the separation of timescales between layers reaches a critical threshold. Our results show an explicit relationship between metastability, chimera states, and the eigenmodes of the system, bridging the gap between harmonic based studies of empirical data and oscillatory models.","sentences":["Oscillatory complex networks in the metastable regime have been used to study the emergence of integrated and segregated activity in the brain, which are hypothesised to be fundamental for cognition.","Yet, the parameters and the underlying mechanisms necessary to achieve the metastable regime are hard to identify, often relying on maximising the correlation with empirical functional connectivity dynamics.","Here, we propose and show that the brain's hierarchically modular mesoscale structure alone can give rise to robust metastable dynamics and (metastable) chimera states in the presence of phase frustration.","We construct unweighted $3$-layer hierarchical networks of identical Kuramoto-Sakaguchi oscillators, parameterized by the average degree of the network and a structural parameter determining the ratio of connections between and within blocks in the upper two layers.","Together, these parameters affect the characteristic timescales of the system.","Away from the critical synchronization point, we detect the emergence of metastable states in the lowest hierarchical layer coexisting with chimera and metastable states in the upper layers.","Using the Laplacian renormalization group flow approach, we uncover two distinct pathways towards achieving the metastable regimes detected in these distinct layers.","In the upper layers, we show how the symmetry-breaking states depend on the slow eigenmodes of the system.","In the lowest layer instead, metastable dynamics can be achieved as the separation of timescales between layers reaches a critical threshold.","Our results show an explicit relationship between metastability, chimera states, and the eigenmodes of the system, bridging the gap between harmonic based studies of empirical data and oscillatory models."],"url":"http://arxiv.org/abs/2405.14542v1","category":"physics.bio-ph"}
{"created":"2024-05-23 13:22:52","title":"VINS-Multi: A Robust Asynchronous Multi-camera-IMU State Estimator","abstract":"State estimation is a critical foundational module in robotics applications, where robustness and performance are paramount. Although in recent years, many works have been focusing on improving one of the most widely adopted state estimation methods, visual inertial odometry (VIO), by incorporating multiple cameras, these efforts predominantly address synchronous camera systems. Asynchronous cameras, which offer simpler hardware configurations and enhanced resilience, have been largely overlooked. To fill this gap, this paper presents VINS-Multi, a novel multi-camera-IMU state estimator for asynchronous cameras. The estimator comprises parallel front ends, a front end coordinator, and a back end optimization module capable of handling asynchronous input frames. It utilizes the frames effectively through a dynamic feature number allocation and a frame priority coordination strategy. The proposed estimator is integrated into a customized quadrotor platform and tested in multiple realistic and challenging scenarios to validate its practicality. Additionally, comprehensive benchmark results are provided to showcase the robustness and superior performance of the proposed estimator.","sentences":["State estimation is a critical foundational module in robotics applications, where robustness and performance are paramount.","Although in recent years, many works have been focusing on improving one of the most widely adopted state estimation methods, visual inertial odometry (VIO), by incorporating multiple cameras, these efforts predominantly address synchronous camera systems.","Asynchronous cameras, which offer simpler hardware configurations and enhanced resilience, have been largely overlooked.","To fill this gap, this paper presents VINS-Multi, a novel multi-camera-IMU state estimator for asynchronous cameras.","The estimator comprises parallel front ends, a front end coordinator, and a back end optimization module capable of handling asynchronous input frames.","It utilizes the frames effectively through a dynamic feature number allocation and a frame priority coordination strategy.","The proposed estimator is integrated into a customized quadrotor platform and tested in multiple realistic and challenging scenarios to validate its practicality.","Additionally, comprehensive benchmark results are provided to showcase the robustness and superior performance of the proposed estimator."],"url":"http://arxiv.org/abs/2405.14539v1","category":"cs.RO"}
{"created":"2024-05-23 13:18:46","title":"Low-resolution descriptions of model neural activity reveal hidden features and underlying system properties","abstract":"The analysis of complex systems such as neural networks is made particularly difficult by the overwhelming number of their interacting components. In the absence of prior knowledge, identifying a small but informative subset of network nodes on which the analysis should focus is a rather difficult task. In this work, we address this problem in the context of a Hopfield model, for which we seek representations given in terms of a subgroup of its neurons with the aim of identifying those that can reveal the largest amount of information about the system. Using a methodology based on concepts from information theory, we reveal that such optimised low-resolution representations are not only informative per se, but rather they also serve as probes of the neural network properties; these results show a tight and potentially fruitful relation between the level of detail at which the network is inspected and the type and amount of information that can be extracted from it.","sentences":["The analysis of complex systems such as neural networks is made particularly difficult by the overwhelming number of their interacting components.","In the absence of prior knowledge, identifying a small but informative subset of network nodes on which the analysis should focus is a rather difficult task.","In this work, we address this problem in the context of a Hopfield model, for which we seek representations given in terms of a subgroup of its neurons with the aim of identifying those that can reveal the largest amount of information about the system.","Using a methodology based on concepts from information theory, we reveal that such optimised low-resolution representations are not only informative per se, but rather they also serve as probes of the neural network properties; these results show a tight and potentially fruitful relation between the level of detail at which the network is inspected and the type and amount of information that can be extracted from it."],"url":"http://arxiv.org/abs/2405.14531v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-23 13:14:08","title":"Towards Privacy-Aware and Personalised Assistive Robots: A User-Centred Approach","abstract":"The global increase in the elderly population necessitates innovative long-term care solutions to improve the quality of life for vulnerable individuals while reducing caregiver burdens. Assistive robots, leveraging advancements in Machine Learning, offer promising personalised support. However, their integration into daily life raises significant privacy concerns. Widely used frameworks like the Robot Operating System (ROS) historically lack inherent privacy mechanisms, complicating data-driven approaches in robotics. This research pioneers user-centric, privacy-aware technologies such as Federated Learning (FL) to advance assistive robotics. FL enables collaborative learning without sharing sensitive data, addressing privacy and scalability issues. This work includes developing solutions for smart wheelchair assistance, enhancing user independence and well-being. By tackling challenges related to non-stationary data and heterogeneous environments, the research aims to improve personalisation and user experience. Ultimately, it seeks to lead the responsible integration of assistive robots into society, enhancing the quality of life for elderly and care-dependent individuals.","sentences":["The global increase in the elderly population necessitates innovative long-term care solutions to improve the quality of life for vulnerable individuals while reducing caregiver burdens.","Assistive robots, leveraging advancements in Machine Learning, offer promising personalised support.","However, their integration into daily life raises significant privacy concerns.","Widely used frameworks like the Robot Operating System (ROS) historically lack inherent privacy mechanisms, complicating data-driven approaches in robotics.","This research pioneers user-centric, privacy-aware technologies such as Federated Learning (FL) to advance assistive robotics.","FL enables collaborative learning without sharing sensitive data, addressing privacy and scalability issues.","This work includes developing solutions for smart wheelchair assistance, enhancing user independence and well-being.","By tackling challenges related to non-stationary data and heterogeneous environments, the research aims to improve personalisation and user experience.","Ultimately, it seeks to lead the responsible integration of assistive robots into society, enhancing the quality of life for elderly and care-dependent individuals."],"url":"http://arxiv.org/abs/2405.14528v1","category":"cs.RO"}
{"created":"2024-05-23 12:47:45","title":"Investigating strangeness enhancement with multiplicity in pp collisions using angular correlations","abstract":"A study of strange hadron production associated with hard scattering processes and with the underlying event is conducted to investigate the origin of the enhanced production of strange hadrons in small collision systems characterised by large charged-particle multiplicities. For this purpose, the production of the single-strange meson ${\\rm K^0_S}$ and the double-strange baryon $\\Xi^{\\pm}$ is measured, in each event, in the azimuthal direction of the highest-$p_{\\rm T}$ particle (``trigger\" particle), related to hard scattering processes, and in the direction transverse to it in azimuth, associated with the underlying event, in pp collisions at $\\sqrt{s}=5.02$ TeV and $\\sqrt{s}=13$ TeV using the ALICE detector at the LHC. The per-trigger yields of ${\\rm K^0_S}$ and $\\Xi^{\\pm}$ are dominated by the transverse-to-leading production (i.e., in the direction transverse to the trigger particle), whose contribution relative to the toward-leading production is observed to increase with the event charged-particle multiplicity. The transverse-to-leading and the toward-leading $\\Xi^{\\pm}$/${\\rm K^0_S}$ yield ratios increase with the multiplicity of charged particles, suggesting that strangeness enhancement with multiplicity is associated with both hard scattering processes and the underlying event. The relative production of $\\Xi^{\\pm}$ with respect to ${\\rm K^0_S}$ is higher in transverse-to-leading processes over the whole multiplicity interval covered by the measurement. The ${\\rm K}^{0}_{\\rm{S}}$ and $\\Xi^{\\pm}$ per-trigger yields and yield ratios are compared with predictions of three different phenomenological models, namely PYTHIA 8.2 with the Monash tune, PYTHIA 8.2 with ropes and EPOS LHC. The comparison shows that none of them can quantitatively describe either the transverse-to-leading or the toward-leading yields of ${\\rm K}^{0}_{\\rm{S}}$ and $\\Xi^{\\pm}$.","sentences":["A study of strange hadron production associated with hard scattering processes and with the underlying event is conducted to investigate the origin of the enhanced production of strange hadrons in small collision systems characterised by large charged-particle multiplicities.","For this purpose, the production of the single-strange meson ${\\rm K^0_S}$ and the double-strange baryon $\\Xi^{\\pm}$ is measured, in each event, in the azimuthal direction of the highest-$p_{\\rm T}$ particle (``trigger\" particle), related to hard scattering processes, and in the direction transverse to it in azimuth, associated with the underlying event, in pp collisions at $\\sqrt{s}=5.02$ TeV and $\\sqrt{s}=13$ TeV using the ALICE detector at the LHC.","The per-trigger yields of ${\\rm K^0_S}$ and $\\Xi^{\\pm}$ are dominated by the transverse-to-leading production (i.e., in the direction transverse to the trigger particle), whose contribution relative to the toward-leading production is observed to increase with the event charged-particle multiplicity.","The transverse-to-leading and the toward-leading $\\Xi^{\\pm}$/${\\rm K^0_S}$ yield ratios increase with the multiplicity of charged particles, suggesting that strangeness enhancement with multiplicity is associated with both hard scattering processes and the underlying event.","The relative production of $\\Xi^{\\pm}$ with respect to ${\\rm K^0_S}$ is higher in transverse-to-leading processes over the whole multiplicity interval covered by the measurement.","The ${\\rm K}^{0}_{\\rm{S}}$ and $\\Xi^{\\pm}$ per-trigger yields and yield ratios are compared with predictions of three different phenomenological models, namely PYTHIA 8.2 with the Monash tune, PYTHIA 8.2 with ropes and EPOS LHC.","The comparison shows that none of them can quantitatively describe either the transverse-to-leading or the toward-leading yields of ${\\rm K}^{0}_{\\rm{S}}$ and $\\Xi^{\\pm}$."],"url":"http://arxiv.org/abs/2405.14511v1","category":"hep-ex"}
{"created":"2024-05-23 12:47:19","title":"Prediction of cancer dynamics under treatment using Bayesian neural networks: A simulated study","abstract":"Predicting cancer dynamics under treatment is challenging due to high inter-patient heterogeneity, lack of predictive biomarkers, and sparse and noisy longitudinal data. Mathematical models can summarize cancer dynamics by a few interpretable parameters per patient. Machine learning methods can then be trained to predict the model parameters from baseline covariates, but do not account for uncertainty in the parameter estimates. Instead, hierarchical Bayesian modeling can model the relationship between baseline covariates to longitudinal measurements via mechanistic parameters while accounting for uncertainty in every part of the model.   The mapping from baseline covariates to model parameters can be modeled in several ways. A linear mapping simplifies inference but fails to capture nonlinear covariate effects and scale poorly for interaction modeling when the number of covariates is large. In contrast, Bayesian neural networks can potentially discover interactions between covariates automatically, but at a substantial cost in computational complexity.   In this work, we develop a hierarchical Bayesian model of subpopulation dynamics that uses baseline covariate information to predict cancer dynamics under treatment, inspired by cancer dynamics in multiple myeloma (MM), where serum M protein is a well-known proxy of tumor burden. As a working example, we apply the model to a simulated dataset and compare its ability to predict M protein trajectories to a model with linear covariate effects. Our results show that the Bayesian neural network covariate effect model predicts cancer dynamics more accurately than a linear covariate effect model when covariate interactions are present. The framework can also be applied to other types of cancer or other time series prediction problems that can be described with a parametric model.","sentences":["Predicting cancer dynamics under treatment is challenging due to high inter-patient heterogeneity, lack of predictive biomarkers, and sparse and noisy longitudinal data.","Mathematical models can summarize cancer dynamics by a few interpretable parameters per patient.","Machine learning methods can then be trained to predict the model parameters from baseline covariates, but do not account for uncertainty in the parameter estimates.","Instead, hierarchical Bayesian modeling can model the relationship between baseline covariates to longitudinal measurements via mechanistic parameters while accounting for uncertainty in every part of the model.   ","The mapping from baseline covariates to model parameters can be modeled in several ways.","A linear mapping simplifies inference but fails to capture nonlinear covariate effects and scale poorly for interaction modeling when the number of covariates is large.","In contrast, Bayesian neural networks can potentially discover interactions between covariates automatically, but at a substantial cost in computational complexity.   ","In this work, we develop a hierarchical Bayesian model of subpopulation dynamics that uses baseline covariate information to predict cancer dynamics under treatment, inspired by cancer dynamics in multiple myeloma (MM), where serum M protein is a well-known proxy of tumor burden.","As a working example, we apply the model to a simulated dataset and compare its ability to predict M protein trajectories to a model with linear covariate effects.","Our results show that the Bayesian neural network covariate effect model predicts cancer dynamics more accurately than a linear covariate effect model when covariate interactions are present.","The framework can also be applied to other types of cancer or other time series prediction problems that can be described with a parametric model."],"url":"http://arxiv.org/abs/2405.14508v1","category":"q-bio.QM"}
{"created":"2024-05-23 12:32:13","title":"On the compexity of p-adic continued fractions of rational number","abstract":"In this paper, we study the complexity of p-adic continued fractions of a rational number, which is the p-adic analogue of the theorem of Lame. We calculate the length of Browkin expansion, and the length of Schneider expansion. Also, some numerical examples have been given.","sentences":["In this paper, we study the complexity of p-adic continued fractions of a rational number, which is the p-adic analogue of the theorem of Lame.","We calculate the length of Browkin expansion, and the length of Schneider expansion.","Also, some numerical examples have been given."],"url":"http://arxiv.org/abs/2405.14500v1","category":"math.NT"}
{"created":"2024-05-23 12:30:24","title":"From Stellar Nurseries to Old Stellar Populations: A Multi-wavelength Case of NGC 1055","abstract":"Given the complex nature of galaxies' interstellar medium (ISM), multi-wavelength data are required to probe the interplay among gas, dust, and stellar populations. Spiral galaxies are ideal laboratories for such a goal as they are rich in gas and dust. Using carbon monoxide (CO) along with GALEX far-ultraviolet (FUV) and Spitzer near-infrared (NIR) data we probe the correlations amongst the properties of stellar populations, gas, and dust over the disc of the spiral galaxy NGC~1055 at multiple angular resolutions, i.e. 2, 4, and 17 arcsec corresponding to a linear size of 144 pc, 288 pc, and 1.2 kpc, respectively. Our results indicate an asymmetry in the physical conditions along the galaxy's disc, i.e. the gas is slightly more extended and brighter, and molecular gas mass is higher on the disc's eastern side than the western side. All physical properties (i.e. molecular gas mass, CO line ratios, stellar mass, NIR emission) decrease from the centre going outwards in the disc with some exceptions (i.e. the extinction, FUV radiation, and the [3.6]-[4.5] colour). Our analysis indicates that the colour gets bluer (metallicity increases) halfway through the disc, then redder (metallicity decreases) going outwards further in the disc.","sentences":["Given the complex nature of galaxies' interstellar medium (ISM), multi-wavelength data are required to probe the interplay among gas, dust, and stellar populations.","Spiral galaxies are ideal laboratories for such a goal as they are rich in gas and dust.","Using carbon monoxide (CO) along with GALEX far-ultraviolet (FUV) and Spitzer near-infrared (NIR) data we probe the correlations amongst the properties of stellar populations, gas, and dust over the disc of the spiral galaxy NGC~1055 at multiple angular resolutions, i.e. 2, 4, and 17 arcsec corresponding to a linear size of 144 pc, 288 pc, and 1.2 kpc, respectively.","Our results indicate an asymmetry in the physical conditions along the galaxy's disc, i.e. the gas is slightly more extended and brighter, and molecular gas mass is higher on the disc's eastern side than the western side.","All physical properties (i.e. molecular gas mass, CO line ratios, stellar mass, NIR emission) decrease from the centre going outwards in the disc with some exceptions (i.e. the extinction, FUV radiation, and the [3.6]-[4.5] colour).","Our analysis indicates that the colour gets bluer (metallicity increases) halfway through the disc, then redder (metallicity decreases) going outwards further in the disc."],"url":"http://arxiv.org/abs/2405.14498v1","category":"astro-ph.GA"}
{"created":"2024-05-23 12:24:46","title":"Investigating strangeness enhancement in jet and medium via $\u03c6$(1020) production in p$-$Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV","abstract":"This work aims to differentiate strangeness produced from hard processes (jet-like) and softer processes (underlying event) by measuring the angular correlation between a high-momentum trigger hadron (h) acting as a jet-proxy and a produced strange hadron ($\\phi(1020)$ meson). Measuring h$-\\phi$ correlations at midrapidity in p$-$Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV as a function of event multiplicity provides insight into the microscopic origin of strangeness enhancement in small collision systems. The jet-like and the underlying-event-like strangeness production are investigated as a function of event multiplicity. They are also compared between a lower and higher momentum region. The evolution of the per-trigger yields within the near-side (aligned with the trigger hadron) and away-side (in the opposite direction of the trigger hadron) jet is studied separately, allowing for the characterization of two distinct jet-like production regimes. Furthermore, the h$-\\phi$ correlations within the underlying event give access to a production regime dominated by soft production processes, which can be compared directly to the in-jet production. Comparisons between h$-\\phi$ and dihadron correlations show that the observed strangeness enhancement is largely driven by the underlying event, where the $\\phi/\\mathrm{h}$ ratio is significantly larger than within the jet regions. As multiplicity increases, the fraction of the total $\\phi(1020)$ yield coming from jets decreases compared to the underlying event production, leading to high-multiplicity events being dominated by the increased strangeness production from the underlying event.","sentences":["This work aims to differentiate strangeness produced from hard processes (jet-like) and softer processes (underlying event) by measuring the angular correlation between a high-momentum trigger hadron (h) acting as a jet-proxy and a produced strange hadron ($\\phi(1020)$ meson).","Measuring h$-\\phi$ correlations at midrapidity in p$-$Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV as a function of event multiplicity provides insight into the microscopic origin of strangeness enhancement in small collision systems.","The jet-like and the underlying-event-like strangeness production are investigated as a function of event multiplicity.","They are also compared between a lower and higher momentum region.","The evolution of the per-trigger yields within the near-side (aligned with the trigger hadron) and away-side (in the opposite direction of the trigger hadron) jet is studied separately, allowing for the characterization of two distinct jet-like production regimes.","Furthermore, the h$-\\phi$ correlations within the underlying event give access to a production regime dominated by soft production processes, which can be compared directly to the in-jet production.","Comparisons between h$-\\phi$ and dihadron correlations show that the observed strangeness enhancement is largely driven by the underlying event, where the $\\phi/\\mathrm{h}$ ratio is significantly larger than within the jet regions.","As multiplicity increases, the fraction of the total $\\phi(1020)$ yield coming from jets decreases compared to the underlying event production, leading to high-multiplicity events being dominated by the increased strangeness production from the underlying event."],"url":"http://arxiv.org/abs/2405.14491v1","category":"nucl-ex"}
{"created":"2024-05-23 12:14:29","title":"Quantifying Multivariate Graph Dependencies: Theory and Estimation for Multiplex Graphs","abstract":"Multiplex graphs, characterised by their layered structure, exhibit informative interdependencies within layers that are crucial for understanding complex network dynamics. Quantifying the interaction and shared information among these layers is challenging due to the non-Euclidean structure of graphs. Our paper introduces a comprehensive theory of multivariate information measures for multiplex graphs. We introduce graphon mutual information for pairs of graphs and expand this to graphon interaction information for three or more graphs, including their conditional variants. We then define graphon total correlation and graphon dual total correlation, along with their conditional forms, and introduce graphon $O-$information. We discuss and quantify the concepts of synergy and redundancy in graphs for the first time, introduce consistent nonparametric estimators for these multivariate graphon information--theoretic measures, and provide their convergence rates. We also conduct a simulation study to illustrate our theoretical findings and demonstrate the relationship between the introduced measures, multiplex graph structure, and higher--order interdependecies. Real-world applications further show the utility of our estimators in revealing shared information and dependence structures in real-world multiplex graphs. This work not only answers fundamental questions about information sharing across multiple graphs but also sets the stage for advanced pattern analysis in complex networks.","sentences":["Multiplex graphs, characterised by their layered structure, exhibit informative interdependencies within layers that are crucial for understanding complex network dynamics.","Quantifying the interaction and shared information among these layers is challenging due to the non-Euclidean structure of graphs.","Our paper introduces a comprehensive theory of multivariate information measures for multiplex graphs.","We introduce graphon mutual information for pairs of graphs and expand this to graphon interaction information for three or more graphs, including their conditional variants.","We then define graphon total correlation and graphon dual total correlation, along with their conditional forms, and introduce graphon $O-$information.","We discuss and quantify the concepts of synergy and redundancy in graphs for the first time, introduce consistent nonparametric estimators for these multivariate graphon information--theoretic measures, and provide their convergence rates.","We also conduct a simulation study to illustrate our theoretical findings and demonstrate the relationship between the introduced measures, multiplex graph structure, and higher--order interdependecies.","Real-world applications further show the utility of our estimators in revealing shared information and dependence structures in real-world multiplex graphs.","This work not only answers fundamental questions about information sharing across multiple graphs but also sets the stage for advanced pattern analysis in complex networks."],"url":"http://arxiv.org/abs/2405.14482v1","category":"math.ST"}
{"created":"2024-05-23 12:13:55","title":"A logic of judgmental existence and its relation to proof irrelevance","abstract":"We introduce a simple natural deduction system for reasoning with judgments of the form \"there exists a proof of $\\varphi$\" to explore the notion of judgmental existence following Martin-L\\\"{o}f's methodology of distinguishing between judgments and propositions. In this system, the existential judgment can be internalized into a modal notion of propositional existence that is closely related to truncation modality, a key tool for obtaining proof irrelevance, and lax modality. We provide a computational interpretation in the style of the Curry-Howard isomorphism for the existence modality and show that the corresponding system has some desirable properties such as strong normalization or subject reduction.","sentences":["We introduce a simple natural deduction system for reasoning with judgments of the form \"there exists a proof of $\\varphi$\" to explore the notion of judgmental existence following Martin-L\\\"{o}f's methodology of distinguishing between judgments and propositions.","In this system, the existential judgment can be internalized into a modal notion of propositional existence that is closely related to truncation modality, a key tool for obtaining proof irrelevance, and lax modality.","We provide a computational interpretation in the style of the Curry-Howard isomorphism for the existence modality and show that the corresponding system has some desirable properties such as strong normalization or subject reduction."],"url":"http://arxiv.org/abs/2405.14481v1","category":"cs.LO"}
{"created":"2024-05-23 12:12:11","title":"Scalable Visual State Space Model with Fractal Scanning","abstract":"Foundational models have significantly advanced in natural language processing (NLP) and computer vision (CV), with the Transformer architecture becoming a standard backbone. However, the Transformer's quadratic complexity poses challenges for handling longer sequences and higher resolution images. To address this challenge, State Space Models (SSMs) like Mamba have emerged as efficient alternatives, initially matching Transformer performance in NLP tasks and later surpassing Vision Transformers (ViTs) in various CV tasks. To improve the performance of SSMs, one crucial aspect is effective serialization of image patches. Existing methods, relying on linear scanning curves, often fail to capture complex spatial relationships and produce repetitive patterns, leading to biases. To address these limitations, we propose using fractal scanning curves for patch serialization. Fractal curves maintain high spatial proximity and adapt to different image resolutions, avoiding redundancy and enhancing SSMs' ability to model complex patterns accurately. We validate our method in image classification, detection, and segmentation tasks, and the superior performance validates its effectiveness.","sentences":["Foundational models have significantly advanced in natural language processing (NLP) and computer vision (CV), with the Transformer architecture becoming a standard backbone.","However, the Transformer's quadratic complexity poses challenges for handling longer sequences and higher resolution images.","To address this challenge, State Space Models (SSMs) like Mamba have emerged as efficient alternatives, initially matching Transformer performance in NLP tasks and later surpassing Vision Transformers (ViTs) in various CV tasks.","To improve the performance of SSMs, one crucial aspect is effective serialization of image patches.","Existing methods, relying on linear scanning curves, often fail to capture complex spatial relationships and produce repetitive patterns, leading to biases.","To address these limitations, we propose using fractal scanning curves for patch serialization.","Fractal curves maintain high spatial proximity and adapt to different image resolutions, avoiding redundancy and enhancing SSMs' ability to model complex patterns accurately.","We validate our method in image classification, detection, and segmentation tasks, and the superior performance validates its effectiveness."],"url":"http://arxiv.org/abs/2405.14480v1","category":"cs.CV"}
{"created":"2024-05-23 11:54:11","title":"Ultra-cold atoms quantum tunneling through single and double optical barriers","abstract":"We realize textbook experiments on Bose-Einstein condensate tunnelling through thin repulsive potential barriers. In particular, we demonstrate atom tunnelling though a single optical barrier in the quantum scattering regime where the De Broglie wavelength of the atoms is larger than the barrier width. Such a beam splitter can be used for atom interferometry and we study the case of two barriers creating an atomic Fabry-P{\\'e}rot cavity. Technically, the velocity of the atoms is reduced thanks to the use of a 39K Bose-Einstein condensate with no interactions. The potential barriers are created optically and their width is tunable thanks to the use of a digital micro-mirror device. In addition, our scattering experiments enable in-situ characterization of the optical aberrations of the barrier optical system.","sentences":["We realize textbook experiments on Bose-Einstein condensate tunnelling through thin repulsive potential barriers.","In particular, we demonstrate atom tunnelling though a single optical barrier in the quantum scattering regime where the De Broglie wavelength of the atoms is larger than the barrier width.","Such a beam splitter can be used for atom interferometry and we study the case of two barriers creating an atomic Fabry-P{\\'e}rot cavity.","Technically, the velocity of the atoms is reduced thanks to the use of a 39K Bose-Einstein condensate with no interactions.","The potential barriers are created optically and their width is tunable thanks to the use of a digital micro-mirror device.","In addition, our scattering experiments enable in-situ characterization of the optical aberrations of the barrier optical system."],"url":"http://arxiv.org/abs/2405.14466v1","category":"physics.atom-ph"}
{"created":"2024-05-23 11:52:56","title":"On resonant energy sets for Hamiltonian systems with reflections","abstract":"We study two uncoupled oscillators, horizontal and vertical, residing in rectilinear polygons (with only vertical and horizontal sides) and impacting elastically from their boundary. The main purpose of the article is to analyze the occurrence of resonance in such systems, depending on the shape of the analytical potentials that determine the oscillators. We define resonant energy levels; roughly speaking, these are levels for which the resonance phenomenon occurs more often than rarely. We focus on unimodal analytic potentials with the minimum at zero. The most important result of the work describes the size of the set of resonance levels in the form of the following trichotomy: it is mostly empty or is one-element or is large, i.e. non-empty and open. We also indicate which classes of potentials each of the three possibilities can occur in. From this point of view, the last case (strongly resonant) is the most interesting. Then, the potentials belong to a special class of potentials, denoted by $\\mathcal{SP}$, which seems unknown in the literature. The presented results appear to be new, even in the simplest case, when the uncoupled oscillators are not trapped in any set.","sentences":["We study two uncoupled oscillators, horizontal and vertical, residing in rectilinear polygons (with only vertical and horizontal sides) and impacting elastically from their boundary.","The main purpose of the article is to analyze the occurrence of resonance in such systems, depending on the shape of the analytical potentials that determine the oscillators.","We define resonant energy levels; roughly speaking, these are levels for which the resonance phenomenon occurs more often than rarely.","We focus on unimodal analytic potentials with the minimum at zero.","The most important result of the work describes the size of the set of resonance levels in the form of the following trichotomy: it is mostly empty or is one-element or is large, i.e. non-empty and open.","We also indicate which classes of potentials each of the three possibilities can occur in.","From this point of view, the last case (strongly resonant) is the most interesting.","Then, the potentials belong to a special class of potentials, denoted by $\\mathcal{SP}$, which seems unknown in the literature.","The presented results appear to be new, even in the simplest case, when the uncoupled oscillators are not trapped in any set."],"url":"http://arxiv.org/abs/2405.14464v1","category":"math.DS"}
{"created":"2024-05-23 11:49:59","title":"A Power Tower Control: A New Sliding Mode Control","abstract":"A control based power tower function at order 2 is proposed in this paper. This leads to a new sliding mode control, which allows employing backstepping technique that combines both guaranteed and finite time convergence. The proposed control is applied to a double integrator subject to perturbation $d$. Both guaranteed and finite convergence are ensured by the controller when $d$ is considered constant and bounded, without knowing its upper bound. For the case, when $d$ is variable and bounded with its upper bound known, only a finite time convergence is obtained. Simulation results are given to show the well founded of the proposed novel control.","sentences":["A control based power tower function at order 2 is proposed in this paper.","This leads to a new sliding mode control, which allows employing backstepping technique that combines both guaranteed and finite time convergence.","The proposed control is applied to a double integrator subject to perturbation $d$. Both guaranteed and finite convergence are ensured by the controller when $d$ is considered constant and bounded, without knowing its upper bound.","For the case, when $d$ is variable and bounded with its upper bound known, only a finite time convergence is obtained.","Simulation results are given to show the well founded of the proposed novel control."],"url":"http://arxiv.org/abs/2405.14461v1","category":"math.DS"}
{"created":"2024-05-23 11:46:57","title":"Detecting slow magnetization relaxation via magnetotransport measurements based on the current-reversal method","abstract":"Slow magnetization relaxation processes are an important time-dependent property of many magnetic materials. We show that magnetotransport measurements based on a well-established current-reversal method can be utilized to implement a simple and robust screening scheme for such relaxation processes. We demonstrate our approach considering the anomalous Hall effect in a Pt/Co/AlOx trilayer model system, and then explore relaxation in {\\tau} -MnAl films. Compared to magnetotransport experiments based on ac lock-in techniques, we find that the dc current-reversal method is particulary sensitive to relaxation processes with relaxation time scales on the order of seconds, comparable to the current-reversal measurement time scales.","sentences":["Slow magnetization relaxation processes are an important time-dependent property of many magnetic materials.","We show that magnetotransport measurements based on a well-established current-reversal method can be utilized to implement a simple and robust screening scheme for such relaxation processes.","We demonstrate our approach considering the anomalous Hall effect in a Pt/Co/AlOx trilayer model system, and then explore relaxation in {\\tau} -MnAl films.","Compared to magnetotransport experiments based on ac lock-in techniques, we find that the dc current-reversal method is particulary sensitive to relaxation processes with relaxation time scales on the order of seconds, comparable to the current-reversal measurement time scales."],"url":"http://arxiv.org/abs/2405.14460v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 11:35:23","title":"Domain-specific augmentations with resolution agnostic self-attention mechanism improves choroid segmentation in optical coherence tomography images","abstract":"The choroid is a key vascular layer of the eye, supplying oxygen to the retinal photoreceptors. Non-invasive enhanced depth imaging optical coherence tomography (EDI-OCT) has recently improved access and visualisation of the choroid, making it an exciting frontier for discovering novel vascular biomarkers in ophthalmology and wider systemic health. However, current methods to measure the choroid often require use of multiple, independent semi-automatic and deep learning-based algorithms which are not made open-source. Previously, Choroidalyzer -- an open-source, fully automatic deep learning method trained on 5,600 OCT B-scans from 385 eyes -- was developed to fully segment and quantify the choroid in EDI-OCT images, thus addressing these issues. Using the same dataset, we propose a Robust, Resolution-agnostic and Efficient Attention-based network for CHoroid segmentation (REACH). REACHNet leverages multi-resolution training with domain-specific data augmentation to promote generalisation, and uses a lightweight architecture with resolution-agnostic self-attention which is not only faster than Choroidalyzer's previous network (4 images/s vs. 2.75 images/s on a standard laptop CPU), but has greater performance for segmenting the choroid region, vessels and fovea (Dice coefficient for region 0.9769 vs. 0.9749, vessels 0.8612 vs. 0.8192 and fovea 0.8243 vs. 0.3783) due to its improved hyperparameter configuration and model training pipeline. REACHNet can be used with Choroidalyzer as a drop-in replacement for the original model and will be made available upon publication.","sentences":["The choroid is a key vascular layer of the eye, supplying oxygen to the retinal photoreceptors.","Non-invasive enhanced depth imaging optical coherence tomography (EDI-OCT) has recently improved access and visualisation of the choroid, making it an exciting frontier for discovering novel vascular biomarkers in ophthalmology and wider systemic health.","However, current methods to measure the choroid often require use of multiple, independent semi-automatic and deep learning-based algorithms which are not made open-source.","Previously, Choroidalyzer -- an open-source, fully automatic deep learning method trained on 5,600 OCT B-scans from 385 eyes -- was developed to fully segment and quantify the choroid in EDI-OCT images, thus addressing these issues.","Using the same dataset, we propose a Robust, Resolution-agnostic and Efficient Attention-based network for CHoroid segmentation (REACH).","REACHNet leverages multi-resolution training with domain-specific data augmentation to promote generalisation, and uses a lightweight architecture with resolution-agnostic self-attention which is not only faster than Choroidalyzer's previous network (4 images/s vs. 2.75 images/s on a standard laptop CPU), but has greater performance for segmenting the choroid region, vessels and fovea (Dice coefficient for region 0.9769 vs. 0.9749, vessels 0.8612 vs. 0.8192 and fovea 0.8243 vs. 0.3783) due to its improved hyperparameter configuration and model training pipeline.","REACHNet can be used with Choroidalyzer as a drop-in replacement for the original model and will be made available upon publication."],"url":"http://arxiv.org/abs/2405.14453v1","category":"eess.IV"}
{"created":"2024-05-23 11:31:52","title":"On fractional parabolic systems of vector order","abstract":"The paper considers the Cauchy problem for the system of partial differential equations of fractional order $D_t^{\\mathcal{B}} {U}(t,x) + \\mathbb{A}(D) {U} (t,x)=H(t,x) $. Here $U$ and $H$ are vector-functions, the $m\\times m$ matrix of differential operators $\\mathbb{A}(D)$ is triangular (elements above or below the diagonal are zero). Operators located on the diagonal are elliptic. The main distinctive feature of this system is that the vector-order $\\mathcal{B}$ has different components $\\beta_j\\in (0,1]$, and $\\beta_j$ are not necessarily rational. Sufficient conditions (in some cases they are necessary) on the initial function and the right-hand side of the equation are found to ensure the existence of a classical solution. Note that the existence of a classical solution to systems of fractional differential equations was studied by various authors, but in all these works the fractional order had the same components for each equation: $\\beta_j=\\beta$, $j=1,...,m$.","sentences":["The paper considers the Cauchy problem for the system of partial differential equations of fractional order $D_t^{\\mathcal{B}} {U}(t,x) + \\mathbb{A}(D) {U} (t,x)=H(t,x) $.","Here $U$ and $H$ are vector-functions, the $m\\times m$ matrix of differential operators $\\mathbb{A}(D)$ is triangular (elements above or below the diagonal are zero).","Operators located on the diagonal are elliptic.","The main distinctive feature of this system is that the vector-order $\\mathcal{B}$ has different components $\\beta_j\\in (0,1]$, and $\\beta_j$ are not necessarily rational.","Sufficient conditions (in some cases they are necessary) on the initial function and the right-hand side of the equation are found to ensure the existence of a classical solution.","Note that the existence of a classical solution to systems of fractional differential equations was studied by various authors, but in all these works the fractional order had the same components for each equation: $\\beta_j=\\beta$, $j=1,...,m$."],"url":"http://arxiv.org/abs/2405.14451v1","category":"math.AP"}
{"created":"2024-05-23 11:18:12","title":"Leveraging Natural Load Dynamics with Variable Gear-ratio Actuators","abstract":"This paper presents a robotic system where the gear-ratio of an actuator is dynamically changed to either leverage or attenuate the natural load dynamics. Based on this principle, lightweight robotic systems can be made fast and strong; exploiting the natural load dynamics for moving at higher speeds (small reduction ratio), while also able to bear a large load through the attenuation of the load dynamics (large reduction ratio). A model-based control algorithm to automatically select the optimal gear-ratios that minimize the total actuator torques for an arbitrary dynamic state and expected uncertainty level is proposed. Also, a novel 3-DoF robot arm using custom actuators with two discrete gear-ratios is presented. The advantages of gear-shifting dynamically are demonstrated through experiments and simulations. Results show that actively changing the gear-ratio using the proposed control algorithms can lead to an order-of-magnitude reduction of necessary actuator torque and power, and also increase robustness to disturbances.","sentences":["This paper presents a robotic system where the gear-ratio of an actuator is dynamically changed to either leverage or attenuate the natural load dynamics.","Based on this principle, lightweight robotic systems can be made fast and strong; exploiting the natural load dynamics for moving at higher speeds (small reduction ratio), while also able to bear a large load through the attenuation of the load dynamics (large reduction ratio).","A model-based control algorithm to automatically select the optimal gear-ratios that minimize the total actuator torques for an arbitrary dynamic state and expected uncertainty level is proposed.","Also, a novel 3-DoF robot arm using custom actuators with two discrete gear-ratios is presented.","The advantages of gear-shifting dynamically are demonstrated through experiments and simulations.","Results show that actively changing the gear-ratio using the proposed control algorithms can lead to an order-of-magnitude reduction of necessary actuator torque and power, and also increase robustness to disturbances."],"url":"http://arxiv.org/abs/2405.14441v1","category":"cs.RO"}
{"created":"2024-05-23 11:11:01","title":"Coherence-enhanced single-qubit thermometry out of equilibrium","abstract":"The metrological limits of thermometry operated in nonequilibrium dynamical regimes are analyzed. We consider a finite-dimensional quantum system, employed as a quantum thermometer, in contact with a thermal bath inducing Markovian thermalization dynamics. The quantum thermometer is initialized in a generic quantum state, possibly including quantum coherence w.r.t. the Hamiltonian basis. We prove that the sensitivity of the thermometer, quantified by the quantum Fisher information, is enhanced by the quantum coherence in its initial state. We analytically show this in the specific case of qubit thermometers for which the maximization of the quantum Fisher information occurs at a finite time during the transient of the thermalization dynamics. Such a finite-time sensitivity enhancement can be better than the sensitivity that is achieved asymptotically.","sentences":["The metrological limits of thermometry operated in nonequilibrium dynamical regimes are analyzed.","We consider a finite-dimensional quantum system, employed as a quantum thermometer, in contact with a thermal bath inducing Markovian thermalization dynamics.","The quantum thermometer is initialized in a generic quantum state, possibly including quantum coherence w.r.t.","the Hamiltonian basis.","We prove that the sensitivity of the thermometer, quantified by the quantum Fisher information, is enhanced by the quantum coherence in its initial state.","We analytically show this in the specific case of qubit thermometers for which the maximization of the quantum Fisher information occurs at a finite time during the transient of the thermalization dynamics.","Such a finite-time sensitivity enhancement can be better than the sensitivity that is achieved asymptotically."],"url":"http://arxiv.org/abs/2405.14439v1","category":"quant-ph"}
{"created":"2024-05-23 11:03:09","title":"High-Level Event Mining: Overview and Future Work","abstract":"Process mining traditionally relies on input consisting of low-level events that capture individual activities, such as filling out a form or processing a product. However, many of the complex problems inherent in processes, such as bottlenecks and compliance issues, extend beyond the scope of individual events and process instances. Consider congestion, for instance, it can involve and impact numerous cases, much like how a traffic jam affects many cars simultaneously. High-level event mining seeks to address such phenomena using the regular event data available. This report offers an extensive and comprehensive overview at existing work and challenges encountered when lifting the perspective from individual events and cases to system-level events.","sentences":["Process mining traditionally relies on input consisting of low-level events that capture individual activities, such as filling out a form or processing a product.","However, many of the complex problems inherent in processes, such as bottlenecks and compliance issues, extend beyond the scope of individual events and process instances.","Consider congestion, for instance, it can involve and impact numerous cases, much like how a traffic jam affects many cars simultaneously.","High-level event mining seeks to address such phenomena using the regular event data available.","This report offers an extensive and comprehensive overview at existing work and challenges encountered when lifting the perspective from individual events and cases to system-level events."],"url":"http://arxiv.org/abs/2405.14435v1","category":"cs.DB"}
{"created":"2024-05-23 11:02:54","title":"X-ray and UV radiation in the planet-forming T-Tauri system PDS 70. Signs of accretion and coronal activity","abstract":"Planet formation takes place in protoplanetary discs around young T-Tauri stars. PDS 70 is one of the first confirmed examples of a system where the planets are currently forming in gaps in the disc, and can be directly imaged. One of the main early influences on planet formation is the lifetime of the protoplanetary disk, which is limited by the intense stellar X-ray and UV radiation. Stellar coronal activity and accretion of material onto the star are both potential sources of XUV radiation. Previous \\textit{Swift} observations detected UV emission, which were consistent with a low rate of accretion. We present follow up observations with the XMM-Newton observatory, which observed PDS 70 simultaneously in X-ray and UV in order to determine intensity of XUV radiation in the system, and identify if the source is coronal, accretion, or both. We detect a strong source in both X-ray and UV, with an average X-ray 0.2-12 keV luminosity of $1.37\\times10^{30}\\ \\mathrm{erg\\ s}^{-1}$, and a possible flare which increased the luminosity to $2.8\\times10^{30}\\ \\mathrm{erg\\ s}^{-1}$. The UV flux density is in excess of what would be expected from chromospheric emission, and supports the interpretation that PDS 70 has continuing weak accretion less than $\\sim10^{-10}\\ \\mathrm{M_{\\odot}\\ yr^{-1}}$. The implications of the detected X-ray and UV radiation are that the disc is likely to be in the final stages of dispersal, and will be completely evaporated in the next million years, bringing an end to the primary planet formation process.","sentences":["Planet formation takes place in protoplanetary discs around young T-Tauri stars.","PDS 70 is one of the first confirmed examples of a system where the planets are currently forming in gaps in the disc, and can be directly imaged.","One of the main early influences on planet formation is the lifetime of the protoplanetary disk, which is limited by the intense stellar X-ray and UV radiation.","Stellar coronal activity and accretion of material onto the star are both potential sources of XUV radiation.","Previous \\textit{Swift} observations detected UV emission, which were consistent with a low rate of accretion.","We present follow up observations with the XMM-Newton observatory, which observed PDS 70 simultaneously in X-ray and UV in order to determine intensity of XUV radiation in the system, and identify if the source is coronal, accretion, or both.","We detect a strong source in both X-ray and UV, with an average X-ray 0.2-12 keV luminosity of $1.37\\times10^{30}\\ \\mathrm{erg\\ s}^{-1}$, and a possible flare which increased the luminosity to $2.8\\times10^{30}\\ \\mathrm{erg\\ s}^{-1}$.","The UV flux density is in excess of what would be expected from chromospheric emission, and supports the interpretation that PDS 70 has continuing weak accretion less than $\\sim10^{-10}\\ \\mathrm{M_{\\odot}\\ yr^{-1}}$.","The implications of the detected X-ray and UV radiation are that the disc is likely to be in the final stages of dispersal, and will be completely evaporated in the next million years, bringing an end to the primary planet formation process."],"url":"http://arxiv.org/abs/2405.14434v1","category":"astro-ph.SR"}
{"created":"2024-05-23 11:02:33","title":"Discrete Hankel Prolate Spheroidal Wave Functions: Spectral Analysis and Application","abstract":"Since the early 1960s, the fields of signal processing, data transmission, channel equalisation, filter design and others have been technologically developed and modernised as a result of the research carried out by D. Slepian and his co-authors H. J Landau and H. O Pollack on the time and band-limited wave system known as discrete and continuous spheroidal waves systems. Our aim in this paper is to introduce new discrete wave sequences called discrete Hankel Prolate spheroidal sequences {\\bf DHPSS} and their counterparts in the frequency domain called discrete Hankel Prolate spheroidal wave functions {\\bf DHPSWF} as radial parts of different solutions of a discrete multidimensional energy maximization problem similar to the one given by D. Slepian and which will generalize his classical pioneering work. In the meantime, we will ensure that our new family is the eigenfunctions set of a finite rank integral operator defined on $L^2(0,\\omega),\\,0<\\omega<1,$ with an associated kernel given by $\\sum_{k=1}^N\\phi^{\\alpha}_{n}(r)\\phi^{\\alpha}_{n}(r'),$ where $\\phi^{\\alpha}_{n}(r)=\\frac{\\sqrt{2r}J_{\\alpha}(s_n^{(\\alpha)}r)}{|J_{\\alpha+1}(s^{(\\alpha)}_n)|},0\\leq r\\leq 1.$ Here $J_\\alpha$ is the Bessel function of the first kind and $(s_n^{(\\alpha)})_n$ are the associated positive zeros. In addition, we will extend the various classical results proposed concerning the decay rate and spectral distribution associated with the classical case, then we will finish our work by an application on the Ingham's universal constant which we will specify with an upper bound estimate.","sentences":["Since the early 1960s, the fields of signal processing, data transmission, channel equalisation, filter design and others have been technologically developed and modernised as a result of the research carried out by D. Slepian and his co-authors H. J Landau and H. O Pollack on the time and band-limited wave system known as discrete and continuous spheroidal waves systems.","Our aim in this paper is to introduce new discrete wave sequences called discrete Hankel Prolate spheroidal sequences {\\bf DHPSS} and their counterparts in the frequency domain called discrete Hankel Prolate spheroidal wave functions {\\bf DHPSWF} as radial parts of different solutions of a discrete multidimensional energy maximization problem similar to the one given by D. Slepian and which will generalize his classical pioneering work.","In the meantime, we will ensure that our new family is the eigenfunctions set of a finite rank integral operator defined on $L^2(0,\\omega),\\,0<\\omega<1,$ with an associated kernel given by $\\sum_{k=1}^N\\phi^{\\alpha}_{n}(r)\\phi^{\\alpha}_{n}(r'),$ where $\\phi^{\\alpha}_{n}(r)=\\frac{\\sqrt{2r}J_{\\alpha}(s_n^{(\\alpha)}r)}{|J_{\\alpha+1}(s^{(\\alpha)}_n)|},0\\leq r\\leq 1.$ Here $J_\\alpha$ is the Bessel function of the first kind and $(s_n^{(\\alpha)})_n$ are the associated positive zeros.","In addition, we will extend the various classical results proposed concerning the decay rate and spectral distribution associated with the classical case, then we will finish our work by an application on the Ingham's universal constant which we will specify with an upper bound estimate."],"url":"http://arxiv.org/abs/2405.14433v1","category":"math.FA"}
{"created":"2024-05-23 10:57:56","title":"Invariance of Gaussian RKHSs under Koopman operators of stochastic differential equations with constant matrix coefficients","abstract":"We consider the Koopman operator semigroup $(K^t)_{t\\ge 0}$ associated with stochastic differential equations of the form $dX_t = AX_t\\,dt + B\\,dW_t$ with constant matrices $A$ and $B$ and Brownian motion $W_t$. We prove that the reproducing kernel Hilbert space $\\bH_C$ generated by a Gaussian kernel with a positive definite covariance matrix $C$ is invariant under each Koopman operator $K^t$ if the matrices $A$, $B$, and $C$ satisfy the following Lyapunov-like matrix inequality: $AC^2 + C^2A^\\top\\le 2BB^\\top$. In this course, we prove a characterization concerning the inclusion $\\bH_{C_1}\\subset\\bH_{C_2}$ of Gaussian RKHSs for two positive definite matrices $C_1$ and $C_2$. The question of whether the sufficient Lyapunov-condition is also necessary is left as an open problem.","sentences":["We consider the Koopman operator semigroup $(K^t)_{t\\ge 0}$ associated with stochastic differential equations of the form $dX_t = AX_t\\,dt","+","B\\,dW_t$ with constant matrices $A$ and $B$ and Brownian motion $W_t$. We prove that the reproducing kernel Hilbert space $\\bH_C$ generated by a Gaussian kernel with a positive definite covariance matrix $C$ is invariant under each Koopman operator $K^t$ if the matrices $A$, $B$, and $C$ satisfy the following Lyapunov-like matrix inequality: $AC^2 + C^2A^\\top\\le 2BB^\\top$. In this course, we prove a characterization concerning the inclusion $\\bH_{C_1}\\subset\\bH_{C_2}$ of Gaussian RKHSs for two positive definite matrices $C_1$ and $C_2$. The question of whether the sufficient Lyapunov-condition is also necessary is left as an open problem."],"url":"http://arxiv.org/abs/2405.14429v1","category":"math.PR"}
{"created":"2024-05-23 10:53:35","title":"Advanced Safety Filter for Smooth Transient Operation of a Battery Energy Storage System","abstract":"In this paper, we implement an advanced safety filter to smoothly limit the current of an inverter-based Battery Energy Storage System. The task involves finding suitable Control Barrier Function and Control Lyapunov Function via Sum-of-Squares optimization to certify the system's safety during grid transients. In contrast to the conventional safety filter, the advanced safety filter not only provides a safety certificate but also achieves finite-time convergence to a nominal region. Within this region, the action of the nominal control, i.e. the Enhanced Direct Power Control, remains unaltered by the safety filter. The advanced safety filter is implemented using a Quadratically Constrained Quadratic Program, providing the capability to also encode quadratic input constraints. Finally, we showcase the effectiveness of the implementation through simulations involving a load step at the Point of Common Coupling, and we compare the outcomes with those obtained using a standard vector current controller.","sentences":["In this paper, we implement an advanced safety filter to smoothly limit the current of an inverter-based Battery Energy Storage System.","The task involves finding suitable Control Barrier Function and Control Lyapunov Function via Sum-of-Squares optimization to certify the system's safety during grid transients.","In contrast to the conventional safety filter, the advanced safety filter not only provides a safety certificate but also achieves finite-time convergence to a nominal region.","Within this region, the action of the nominal control, i.e. the Enhanced Direct Power Control, remains unaltered by the safety filter.","The advanced safety filter is implemented using a Quadratically Constrained Quadratic Program, providing the capability to also encode quadratic input constraints.","Finally, we showcase the effectiveness of the implementation through simulations involving a load step at the Point of Common Coupling, and we compare the outcomes with those obtained using a standard vector current controller."],"url":"http://arxiv.org/abs/2405.14427v1","category":"eess.SY"}
{"created":"2024-05-23 10:45:50","title":"A note on composition operators on the bi-disc","abstract":"In this note we characterise the bounded composition operators on the anisotropic Dirichlet-type spaces $\\mathfrak{D}_{\\vec{a}}(\\mathbb{D}^2)$ induced by holomorphic self maps of the bi-disc $\\mathbb{D}^2$ of the form $\\Phi(z_1,z_2)=(\\phi_1(z_1),\\phi_2(z_2))$ via the respective generalised Nevanlinna counting functions of both symbols. We also characterise the bounded composition operators from $A^2(\\mathbb{D}^2)$ to $\\mathfrak{D}(\\mathbb{D}^2)$ for general self maps of the bidisc via a Carleson-type condition.","sentences":["In this note we characterise the bounded composition operators on the anisotropic Dirichlet-type spaces $\\mathfrak{D}_{\\vec{a}}(\\mathbb{D}^2)$ induced by holomorphic self maps of the bi-disc $\\mathbb{D}^2$ of the form $\\Phi(z_1,z_2)=(\\phi_1(z_1),\\phi_2(z_2))$ via the respective generalised Nevanlinna counting functions of both symbols.","We also characterise the bounded composition operators from $A^2(\\mathbb{D}^2)$ to $\\mathfrak{D}(\\mathbb{D}^2)$ for general self maps of the bidisc via a Carleson-type condition."],"url":"http://arxiv.org/abs/2405.14423v1","category":"math.CV"}
{"created":"2024-05-23 10:39:32","title":"Continuous-time Equilibrium Returns in Markets with Price Impact and Transaction Costs","abstract":"We consider an Ito-financial market at which the risky assets' returns are derived endogenously through a market-clearing condition amongst heterogeneous risk-averse investors with quadratic preferences and random endowments. Investors act strategically by taking into account the impact that their orders have on the assets' drift. A frictionless market and an one with quadratic transaction costs are analysed and compared. In the former, we derive the unique Nash equilibrium at which investors' demand processes reveal different hedging needs than their true ones, resulting in a deviation of the Nash equilibrium from its competitive counterpart. Under price impact and transaction costs, we characterize the Nash equilibrium as the (unique) solution of a system of FBSDEs and derive its closed-form expression. We furthermore show that under common risk aversion and absence of noise traders, transaction costs do not change the equilibrium returns. On the contrary, when noise traders are present, the effect of transaction costs on equilibrium returns is amplified due to price impact.","sentences":["We consider an Ito-financial market at which the risky assets' returns are derived endogenously through a market-clearing condition amongst heterogeneous risk-averse investors with quadratic preferences and random endowments.","Investors act strategically by taking into account the impact that their orders have on the assets' drift.","A frictionless market and an one with quadratic transaction costs are analysed and compared.","In the former, we derive the unique Nash equilibrium at which investors' demand processes reveal different hedging needs than their true ones, resulting in a deviation of the Nash equilibrium from its competitive counterpart.","Under price impact and transaction costs, we characterize the Nash equilibrium as the (unique) solution of a system of FBSDEs and derive its closed-form expression.","We furthermore show that under common risk aversion and absence of noise traders, transaction costs do not change the equilibrium returns.","On the contrary, when noise traders are present, the effect of transaction costs on equilibrium returns is amplified due to price impact."],"url":"http://arxiv.org/abs/2405.14418v1","category":"q-fin.MF"}
{"created":"2024-05-23 10:37:34","title":"VelCrys: Interactive web-based application to compute acoustic wave velocity in crystals and its magnetic corrections","abstract":"We present VelCrys, a web-based interactive tool, that allows to perform further post-processing of the elastic tensor in order to compute and plot the group velocity of the acoustic waves for any crystal symmetry. We also implemented the calculation of effective magnetic corrections to the elastic tensor and corresponding fractional change in group velocity under a magnetic field. We apply it to dry sandstone, cubic CoPt and hcp Co to show some of the program features. In the analysis of magnetic corrections, we find complex landscapes of fractional change in group velocity as a function of ray direction, as well as a field dependence consistent with Simon effect.","sentences":["We present VelCrys, a web-based interactive tool, that allows to perform further post-processing of the elastic tensor in order to compute and plot the group velocity of the acoustic waves for any crystal symmetry.","We also implemented the calculation of effective magnetic corrections to the elastic tensor and corresponding fractional change in group velocity under a magnetic field.","We apply it to dry sandstone, cubic CoPt and hcp Co to show some of the program features.","In the analysis of magnetic corrections, we find complex landscapes of fractional change in group velocity as a function of ray direction, as well as a field dependence consistent with Simon effect."],"url":"http://arxiv.org/abs/2405.14416v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 10:34:21","title":"GeoFaaS: An Edge-to-Cloud FaaS Platform","abstract":"The massive growth of mobile and IoT devices demands geographically distributed computing systems for optimal performance, privacy, and scalability. However, existing edge-to-cloud serverless platforms lack location awareness, resulting in inefficient network usage and increased latency.   In this paper, we propose GeoFaaS, a novel edge-to-cloud Function-as-a-Service (FaaS) platform that leverages real-time client location information for transparent request execution on the nearest available FaaS node. If needed, GeoFaaS transparently offloads requests to the cloud when edge resources are overloaded, thus, ensuring consistent execution without user intervention. GeoFaaS has a modular and decentralized architecture: building on the single-node FaaS system tinyFaaS, GeoFaaS works as a stand-alone edge-to-cloud FaaS platform but can also integrate and act as a routing layer for existing FaaS services, e.g., in the cloud. To evaluate our approach, we implemented an open-source proof-of-concept prototype and studied performance and fault-tolerance behavior in experiments.","sentences":["The massive growth of mobile and IoT devices demands geographically distributed computing systems for optimal performance, privacy, and scalability.","However, existing edge-to-cloud serverless platforms lack location awareness, resulting in inefficient network usage and increased latency.   ","In this paper, we propose GeoFaaS, a novel edge-to-cloud Function-as-a-Service (FaaS) platform that leverages real-time client location information for transparent request execution on the nearest available FaaS node.","If needed, GeoFaaS transparently offloads requests to the cloud when edge resources are overloaded, thus, ensuring consistent execution without user intervention.","GeoFaaS has a modular and decentralized architecture: building on the single-node FaaS system tinyFaaS, GeoFaaS works as a stand-alone edge-to-cloud FaaS platform but can also integrate and act as a routing layer for existing FaaS services, e.g., in the cloud.","To evaluate our approach, we implemented an open-source proof-of-concept prototype and studied performance and fault-tolerance behavior in experiments."],"url":"http://arxiv.org/abs/2405.14413v1","category":"cs.DC"}
{"created":"2024-05-23 10:32:29","title":"Time-dependent Hamiltonians and Geometry of Operators Generated by Them","abstract":"We obtain the complexity geometry associated with the Hamiltonian of a quantum mechanical system, specifically in cases where the Hamiltonian is explicitly time-dependent. Using Nielsen's geometric formulation of circuit complexity, we calculate the bi-invariant cost associated with these time-dependent Hamiltonians by suitably regularising their norms and obtain analytical expressions of the costs for several well-known time-dependent quantum mechanical systems. Specifically, we show that an equivalence exists between the total costs of obtaining an operator through time evolution generated by a unit mass harmonic oscillator whose frequency depends on time, and a harmonic oscillator whose both mass and frequency are functions of time. These results are illustrated with several examples, including a specific smooth quench protocol where the comparison of time variation of the cost with other information theoretic quantities, such as the Shannon entropy, is discussed.","sentences":["We obtain the complexity geometry associated with the Hamiltonian of a quantum mechanical system, specifically in cases where the Hamiltonian is explicitly time-dependent.","Using Nielsen's geometric formulation of circuit complexity, we calculate the bi-invariant cost associated with these time-dependent Hamiltonians by suitably regularising their norms and obtain analytical expressions of the costs for several well-known time-dependent quantum mechanical systems.","Specifically, we show that an equivalence exists between the total costs of obtaining an operator through time evolution generated by a unit mass harmonic oscillator whose frequency depends on time, and a harmonic oscillator whose both mass and frequency are functions of time.","These results are illustrated with several examples, including a specific smooth quench protocol where the comparison of time variation of the cost with other information theoretic quantities, such as the Shannon entropy, is discussed."],"url":"http://arxiv.org/abs/2405.14410v1","category":"quant-ph"}
{"created":"2024-05-23 10:30:48","title":"Investigating the Common Authorship of Signatures by Off-Line Automatic Signature Verification Without the Use of Reference Signatures","abstract":"In automatic signature verification, questioned specimens are usually compared with reference signatures. In writer-dependent schemes, a number of reference signatures are required to build up the individual signer model while a writer-independent system requires a set of reference signatures from several signers to develop the model of the system. This paper addresses the problem of automatic signature verification when no reference signatures are available. The scenario we explore consists of a set of signatures, which could be signed by the same author or by multiple signers. As such, we discuss three methods which estimate automatically the common authorship of a set of off-line signatures. The first method develops a score similarity matrix, worked out with the assistance of duplicated signatures; the second uses a feature-distance matrix for each pair of signatures; and the last method introduces pre-classification based on the complexity of each signature. Publicly available signatures were used in the experiments, which gave encouraging results. As a baseline for the performance obtained by our approaches, we carried out a visual Turing Test where forensic and non-forensic human volunteers, carrying out the same task, performed less well than the automatic schemes.","sentences":["In automatic signature verification, questioned specimens are usually compared with reference signatures.","In writer-dependent schemes, a number of reference signatures are required to build up the individual signer model while a writer-independent system requires a set of reference signatures from several signers to develop the model of the system.","This paper addresses the problem of automatic signature verification when no reference signatures are available.","The scenario we explore consists of a set of signatures, which could be signed by the same author or by multiple signers.","As such, we discuss three methods which estimate automatically the common authorship of a set of off-line signatures.","The first method develops a score similarity matrix, worked out with the assistance of duplicated signatures; the second uses a feature-distance matrix for each pair of signatures; and the last method introduces pre-classification based on the complexity of each signature.","Publicly available signatures were used in the experiments, which gave encouraging results.","As a baseline for the performance obtained by our approaches, we carried out a visual Turing Test where forensic and non-forensic human volunteers, carrying out the same task, performed less well than the automatic schemes."],"url":"http://arxiv.org/abs/2405.14409v1","category":"cs.CV"}
{"created":"2024-05-23 10:30:00","title":"Adaptive tempering schedules with approximative intermediate measures for filtering problems","abstract":"Data assimilation algorithms integrate prior information from numerical model simulations with observed data. Ensemble-based filters, regarded as state-of-the-art, are widely employed for large-scale estimation tasks in disciplines such as geoscience and meteorology. Despite their inability to produce the true posterior distribution for nonlinear systems, their robustness and capacity for state tracking are noteworthy. In contrast, Particle filters yield the correct distribution in the ensemble limit but require substantially larger ensemble sizes than ensemble-based filters to maintain stability in higher-dimensional spaces. It is essential to transcend traditional Gaussian assumptions to achieve realistic quantification of uncertainties. One approach involves the hybridisation of filters, facilitated by tempering, to harness the complementary strengths of different filters. A new adaptive tempering method is proposed to tune the underlying schedule, aiming to systematically surpass the performance previously achieved. Although promising numerical results for certain filter combinations in toy examples exist in the literature, the tuning of hyperparameters presents a considerable challenge. A deeper understanding of these interactions is crucial for practical applications.","sentences":["Data assimilation algorithms integrate prior information from numerical model simulations with observed data.","Ensemble-based filters, regarded as state-of-the-art, are widely employed for large-scale estimation tasks in disciplines such as geoscience and meteorology.","Despite their inability to produce the true posterior distribution for nonlinear systems, their robustness and capacity for state tracking are noteworthy.","In contrast, Particle filters yield the correct distribution in the ensemble limit but require substantially larger ensemble sizes than ensemble-based filters to maintain stability in higher-dimensional spaces.","It is essential to transcend traditional Gaussian assumptions to achieve realistic quantification of uncertainties.","One approach involves the hybridisation of filters, facilitated by tempering, to harness the complementary strengths of different filters.","A new adaptive tempering method is proposed to tune the underlying schedule, aiming to systematically surpass the performance previously achieved.","Although promising numerical results for certain filter combinations in toy examples exist in the literature, the tuning of hyperparameters presents a considerable challenge.","A deeper understanding of these interactions is crucial for practical applications."],"url":"http://arxiv.org/abs/2405.14408v1","category":"math.NA"}
{"created":"2024-05-23 10:26:16","title":"A Unification Between Deep-Learning Vision, Compartmental Dynamical Thermodynamics, and Robotic Manipulation for a Circular Economy","abstract":"The shift from a linear to a circular economy has the potential to simultaneously reduce uncertainties of material supplies and waste generation. To date, the development of robotic and, more generally, autonomous systems have been rarely integrated into circular economy implementation strategies. In this review, we merge deep-learning vision, compartmental dynamical thermodynamics, and robotic manipulation into a theoretically-coherent physics-based research framework to lay the foundations of circular flow designs of materials, and hence, to speed-up the transition from linearity to circularity. Then, we discuss opportunities for robotics in circular economy.","sentences":["The shift from a linear to a circular economy has the potential to simultaneously reduce uncertainties of material supplies and waste generation.","To date, the development of robotic and, more generally, autonomous systems have been rarely integrated into circular economy implementation strategies.","In this review, we merge deep-learning vision, compartmental dynamical thermodynamics, and robotic manipulation into a theoretically-coherent physics-based research framework to lay the foundations of circular flow designs of materials, and hence, to speed-up the transition from linearity to circularity.","Then, we discuss opportunities for robotics in circular economy."],"url":"http://arxiv.org/abs/2405.14406v1","category":"cs.RO"}
{"created":"2024-05-23 10:21:28","title":"Hadronic molecules with four charm or beauty quarks","abstract":"We apply the extended local hidden gauge formalism to study the meson-meson interactions with the quark constituents $cc\\bar c\\bar c$, $cc\\bar c\\bar b/\\bar c\\bar c cb$, $cc\\bar b\\bar b/\\bar c\\bar c bb$, $bb\\bar c\\bar b/\\bar b\\bar b cb$, and $bb\\bar b\\bar b$, where the exchanged mesons can only be the fully-heavy vector mesons $J/\\psi$, $B_c^*$, and $\\Upsilon$. We solve the coupled-channel Bethe-Salpeter equation to derive two poles in the $bb\\bar c\\bar b$ system and two poles in the $cc\\bar c\\bar b$ system (with four charge-conjugated poles in the $\\bar b\\bar b cb$ and $\\bar c\\bar c cb$ systems): one pole existing in the $bb\\bar c\\bar b$ system corresponds to the sub-threshold bound state when setting the cutoff momentum $\\Lambda > 850$ MeV, while it causes the threshold effect with $\\Lambda < 850$ MeV; the other pole existing in this system corresponds to the sub-threshold bound state with $\\Lambda > 1100$ MeV; the two poles existing in the $cc\\bar c\\bar b$ system correspond to the sub-threshold bound states only when setting $\\Lambda > 1550$ MeV and $\\Lambda > 2650$ MeV, making them difficult to be identified as deeply-bound hadronic molecules. We propose to investigate the two poles of the $bb\\bar c\\bar b$ system in the $\\mu^+\\mu^-B_c^-$ channel at LHC.","sentences":["We apply the extended local hidden gauge formalism to study the meson-meson interactions with the quark constituents $cc\\bar c\\bar c$, $cc\\bar c\\bar b/\\bar c\\bar c","cb$, $cc\\bar b\\bar b/\\bar c\\bar c","bb$, $bb\\bar c\\bar b/\\bar","b\\bar b cb$, and $bb\\bar b\\bar b$, where the exchanged mesons can only be the fully-heavy vector mesons $J/\\psi$, $B_c^*$, and $\\Upsilon$. We solve the coupled-channel Bethe-Salpeter equation to derive two poles in the $bb\\bar c\\bar b$ system and two poles in the $cc\\bar c\\bar b$ system (with four charge-conjugated poles in the $\\bar b\\bar b cb$ and $\\bar c\\bar c cb$ systems): one pole existing in the $bb\\bar c\\bar b$ system corresponds to the sub-threshold bound state when setting the cutoff momentum $\\Lambda > 850$ MeV, while it causes the threshold effect with $\\Lambda < 850$ MeV; the other pole existing in this system corresponds to the sub-threshold bound state with $\\Lambda > 1100$ MeV; the two poles existing in the $cc\\bar c\\bar b$ system correspond to the sub-threshold bound states only when setting $\\Lambda > 1550$ MeV and $\\Lambda > 2650$ MeV, making them difficult to be identified as deeply-bound hadronic molecules.","We propose to investigate the two poles of the $bb\\bar c\\bar b$ system in the $\\mu^+\\mu^-B_c^-$ channel at LHC."],"url":"http://arxiv.org/abs/2405.14404v1","category":"hep-ph"}
{"created":"2024-05-23 10:14:33","title":"BORA: A Personalized Data Display for Large-scale Experiments","abstract":"Given the rapid improvement of the detectors at high-energy physics experiments, the need for real-time data monitoring systems has become imperative. The significance of these systems lies in their ability to display experiment status, steer software and hardware instrumentation, and provide alarms, thus enabling researchers to manage their experiments better. However, researchers typically build most data monitoring systems as standalone in-house solutions that cannot be reused for other experiments or future upgrades. We present BORA (personalized collaBORAtive data display), a lightweight browser-based monitoring system that supports diverse protocols and is built specifically for customizable visualization of complex data, which we standardize via video streaming. We show how absolute positioning layout and visual overlay background can address the diverse data display design requirements. Using the client-server architecture, we enable support for diverse communication protocols, with the server component responsible for parsing the incoming data. We integrate the Jupyter Notebook as part of our ecosystem to address the limitations of the web-based framework, providing a foundation to leverage scripting capabilities and integrate popular AI frameworks. Since video streaming is a core component of our framework, we evaluate viable approaches to streaming protocols like HLS, WebRTC, and MPEG-Websocket. The study explores the implications for our use case, highlighting its potential to transform data visualization and decision-making processes.","sentences":["Given the rapid improvement of the detectors at high-energy physics experiments, the need for real-time data monitoring systems has become imperative.","The significance of these systems lies in their ability to display experiment status, steer software and hardware instrumentation, and provide alarms, thus enabling researchers to manage their experiments better.","However, researchers typically build most data monitoring systems as standalone in-house solutions that cannot be reused for other experiments or future upgrades.","We present BORA (personalized collaBORAtive data display), a lightweight browser-based monitoring system that supports diverse protocols and is built specifically for customizable visualization of complex data, which we standardize via video streaming.","We show how absolute positioning layout and visual overlay background can address the diverse data display design requirements.","Using the client-server architecture, we enable support for diverse communication protocols, with the server component responsible for parsing the incoming data.","We integrate the Jupyter Notebook as part of our ecosystem to address the limitations of the web-based framework, providing a foundation to leverage scripting capabilities and integrate popular AI frameworks.","Since video streaming is a core component of our framework, we evaluate viable approaches to streaming protocols like HLS, WebRTC, and MPEG-Websocket.","The study explores the implications for our use case, highlighting its potential to transform data visualization and decision-making processes."],"url":"http://arxiv.org/abs/2405.14397v1","category":"cs.HC"}
{"created":"2024-05-23 10:13:59","title":"Corrupted sensing quantum state tomography","abstract":"The reliable characterization of quantum states as well as any potential noise in various quantum systems is crucial for advancing quantum technologies. In this work we propose the concept of corrupted sensing quantum state tomography which enables the simultaneous reconstruction of quantum states and structured noise with the aid of simple Pauli measurements only. Without additional prior information, we investigate the reliability and robustness of the framework. The power of our algorithm is demonstrated by assuming Gaussian and Poisson sparse noise for low-rank state tomography. In particular, our approach is able to achieve a high quality of the recovery with incomplete sets of measurements and is also suitable for performance improvement of large quantum systems. It is envisaged that the techniques can become a practical tool to greatly reduce the cost and computational effort for quantum tomography in noisy quantum systems.","sentences":["The reliable characterization of quantum states as well as any potential noise in various quantum systems is crucial for advancing quantum technologies.","In this work we propose the concept of corrupted sensing quantum state tomography which enables the simultaneous reconstruction of quantum states and structured noise with the aid of simple Pauli measurements only.","Without additional prior information, we investigate the reliability and robustness of the framework.","The power of our algorithm is demonstrated by assuming Gaussian and Poisson sparse noise for low-rank state tomography.","In particular, our approach is able to achieve a high quality of the recovery with incomplete sets of measurements and is also suitable for performance improvement of large quantum systems.","It is envisaged that the techniques can become a practical tool to greatly reduce the cost and computational effort for quantum tomography in noisy quantum systems."],"url":"http://arxiv.org/abs/2405.14396v1","category":"quant-ph"}
{"created":"2024-05-23 10:04:36","title":"Evaluation of the Programming Skills of Large Language Models","abstract":"The advent of Large Language Models (LLM) has revolutionized the efficiency and speed with which tasks are completed, marking a significant leap in productivity through technological innovation. As these chatbots tackle increasingly complex tasks, the challenge of assessing the quality of their outputs has become paramount. This paper critically examines the output quality of two leading LLMs, OpenAI's ChatGPT and Google's Gemini AI, by comparing the quality of programming code generated in both their free versions. Through the lens of a real-world example coupled with a systematic dataset, we investigate the code quality produced by these LLMs. Given their notable proficiency in code generation, this aspect of chatbot capability presents a particularly compelling area for analysis. Furthermore, the complexity of programming code often escalates to levels where its verification becomes a formidable task, underscoring the importance of our study. This research aims to shed light on the efficacy and reliability of LLMs in generating high-quality programming code, an endeavor that has significant implications for the field of software development and beyond.","sentences":["The advent of Large Language Models (LLM) has revolutionized the efficiency and speed with which tasks are completed, marking a significant leap in productivity through technological innovation.","As these chatbots tackle increasingly complex tasks, the challenge of assessing the quality of their outputs has become paramount.","This paper critically examines the output quality of two leading LLMs, OpenAI's ChatGPT and Google's Gemini AI, by comparing the quality of programming code generated in both their free versions.","Through the lens of a real-world example coupled with a systematic dataset, we investigate the code quality produced by these LLMs.","Given their notable proficiency in code generation, this aspect of chatbot capability presents a particularly compelling area for analysis.","Furthermore, the complexity of programming code often escalates to levels where its verification becomes a formidable task, underscoring the importance of our study.","This research aims to shed light on the efficacy and reliability of LLMs in generating high-quality programming code, an endeavor that has significant implications for the field of software development and beyond."],"url":"http://arxiv.org/abs/2405.14388v1","category":"cs.SE"}
{"created":"2024-05-23 10:04:23","title":"Capsule Network Projectors are Equivariant and Invariant Learners","abstract":"Learning invariant representations has been the longstanding approach to self-supervised learning. However, recently progress has been made in preserving equivariant properties in representations, yet do so with highly prescribed architectures. In this work, we propose an invariant-equivariant self-supervised architecture that employs Capsule Networks (CapsNets) which have been shown to capture equivariance with respect to novel viewpoints. We demonstrate that the use of CapsNets in equivariant self-supervised architectures achieves improved downstream performance on equivariant tasks with higher efficiency and fewer network parameters. To accommodate the architectural changes of CapsNets, we introduce a new objective function based on entropy minimisation. This approach, which we name CapsIE (Capsule Invariant Equivariant Network), achieves state-of-the-art performance across all invariant and equivariant downstream tasks on the 3DIEBench dataset, while outperforming supervised baselines. Our results demonstrate the ability of CapsNets to learn complex and generalised representations for large-scale, multi-task datasets compared to previous CapsNet benchmarks. Code is available at https://github.com/AberdeenML/CapsIE.","sentences":["Learning invariant representations has been the longstanding approach to self-supervised learning.","However, recently progress has been made in preserving equivariant properties in representations, yet do so with highly prescribed architectures.","In this work, we propose an invariant-equivariant self-supervised architecture that employs Capsule Networks (CapsNets) which have been shown to capture equivariance with respect to novel viewpoints.","We demonstrate that the use of CapsNets in equivariant self-supervised architectures achieves improved downstream performance on equivariant tasks with higher efficiency and fewer network parameters.","To accommodate the architectural changes of CapsNets, we introduce a new objective function based on entropy minimisation.","This approach, which we name CapsIE (Capsule Invariant Equivariant Network), achieves state-of-the-art performance across all invariant and equivariant downstream tasks on the 3DIEBench dataset, while outperforming supervised baselines.","Our results demonstrate the ability of CapsNets to learn complex and generalised representations for large-scale, multi-task datasets compared to previous CapsNet benchmarks.","Code is available at https://github.com/AberdeenML/CapsIE."],"url":"http://arxiv.org/abs/2405.14386v1","category":"cs.CV"}
{"created":"2024-05-23 09:59:04","title":"Multi-purpose robot for rehabilitation of small diameter water pipes","abstract":"Rehabilitating cast iron pipes through lining offers several advantages, including increased durability, reduced water leaks, and minimal disruption.This approach presents a cost effective and environmentally friendly solution by sealing cracks and joints, extending the pipeline's lifespan, and reducing water wastage, all while avoiding the need for trench excavation. However, due to the relining process, branch connections are sealed and need to be reestablished. To address the issue of rehabilitating small-diameter water pipes, we have designed a modular robot capable of traversing and working within 200 meter long, 100 mm diameter cast iron pipes. This robot is equipped with perception functions to detect, locate, and characterize the branch connections in cast iron pipes and relocate them after lining, as well as machining functions. A first prototype of this system has been developed and validated on an 8 meter long section, in a laboratory environment.","sentences":["Rehabilitating cast iron pipes through lining offers several advantages, including increased durability, reduced water leaks, and minimal disruption.","This approach presents a cost effective and environmentally friendly solution by sealing cracks and joints, extending the pipeline's lifespan, and reducing water wastage, all while avoiding the need for trench excavation.","However, due to the relining process, branch connections are sealed and need to be reestablished.","To address the issue of rehabilitating small-diameter water pipes, we have designed a modular robot capable of traversing and working within 200 meter long, 100 mm diameter cast iron pipes.","This robot is equipped with perception functions to detect, locate, and characterize the branch connections in cast iron pipes and relocate them after lining, as well as machining functions.","A first prototype of this system has been developed and validated on an 8 meter long section, in a laboratory environment."],"url":"http://arxiv.org/abs/2405.14382v1","category":"cs.RO"}
{"created":"2024-05-23 09:53:36","title":"Spin-$S\\,$ Kitaev-Heisenberg model on the honeycomb lattice: A high-order treatment via the many-body coupled cluster method","abstract":"We study the spin-$S$ Kitaev-Heisenberg model on the honeycomb lattice for $S\\!=\\!1/2$, $1$ and $3/2$, by using the coupled cluster method (CCM) of microscopic quantum many-body theory. This system is one of the earliest extensions of the Kitaev model and is believed to contain two extended spin liquid phases for any value of the spin quantum number $S$. We show that the CCM delivers accurate estimates for the phase boundaries of these spin liquid phases, as well as other transition points in the phase diagram. Moreover, we find evidence of two unexpected narrow phases for $S\\!=\\!1/2$, one sandwiched between the zigzag and ferromagnetic phases and the other between the N\\'eel and the stripy phases. The results establish the CCM as a versatile numerical technique that can capture the strong quantum-mechanical fluctuations that are inherently present in generalized Kitaev models with competing bond-dependent anisotropies.","sentences":["We study the spin-$S$ Kitaev-Heisenberg model on the honeycomb lattice for $S\\!=\\!1/2$, $1$ and $3/2$, by using the coupled cluster method (CCM) of microscopic quantum many-body theory.","This system is one of the earliest extensions of the Kitaev model and is believed to contain two extended spin liquid phases for any value of the spin quantum number $S$. We show that the CCM delivers accurate estimates for the phase boundaries of these spin liquid phases, as well as other transition points in the phase diagram.","Moreover, we find evidence of two unexpected narrow phases for $S\\!=\\!1/2$, one sandwiched between the zigzag and ferromagnetic phases and the other between the N\\'eel and the stripy phases.","The results establish the CCM as a versatile numerical technique that can capture the strong quantum-mechanical fluctuations that are inherently present in generalized Kitaev models with competing bond-dependent anisotropies."],"url":"http://arxiv.org/abs/2405.14378v1","category":"cond-mat.str-el"}
{"created":"2024-05-23 09:50:59","title":"Quantum Chaos in Random Ising Networks","abstract":"We report a systematic investigation of universal quantum chaotic signatures in the transverse field Ising model on an Erd\\H{o}s-R\\'enyi network. This is achieved by studying local spectral measures such as the level spacing and the level velocity statistics. A spectral form factor analysis is also performed as a global measure, probing energy level correlations at arbitrary spectral distances. Our findings show that these measures capture the breakdown of chaotic behavior upon varying the connectivity and strength of the transverse field in various regimes. We demonstrate that the level spacing statistics and the spectral form factor signal this breakdown for sparsely and densely connected networks. The velocity statistics capture the surviving chaotic signatures in the sparse limit. However, these integrable-like regimes extend over a vanishingly small segment in the full range of connectivity.","sentences":["We report a systematic investigation of universal quantum chaotic signatures in the transverse field Ising model on an Erd\\H{o}s-R\\'enyi network.","This is achieved by studying local spectral measures such as the level spacing and the level velocity statistics.","A spectral form factor analysis is also performed as a global measure, probing energy level correlations at arbitrary spectral distances.","Our findings show that these measures capture the breakdown of chaotic behavior upon varying the connectivity and strength of the transverse field in various regimes.","We demonstrate that the level spacing statistics and the spectral form factor signal this breakdown for sparsely and densely connected networks.","The velocity statistics capture the surviving chaotic signatures in the sparse limit.","However, these integrable-like regimes extend over a vanishingly small segment in the full range of connectivity."],"url":"http://arxiv.org/abs/2405.14376v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-23 09:44:43","title":"Ferri-ionic Coupling in CuInP$_2$S$_6$ Nanoflakes: Polarization States and Controllable Negative Capacitance","abstract":"We consider nanoflakes of van der Waals ferrielectric CuInP$_2$S$_6$ covered by an ionic surface charge and reveal the appearance of polar states with relatively high polarization ~5 microC/cm$^2$ and stored free charge ~10 microC/cm$%2$, which can mimic \"mid-gap\" states related with a surface field-induced transfer of Cu and/or In ions in the van der Waals gap. The change in the ionic screening degree and mismatch strains induce a broad range of the transitions between paraelectric phase, antiferroelectric, ferrielectric, and ferri-ionic states in CuInP$_2$S$_6$ nanoflakes. The states' stability and/or metastability is determined by the minimum of the system free energy consisting of electrostatic energy, elastic energy, and a Landau-type four-well potential of the ferrielectric dipole polarization. The possibility to govern the transitions by strain and ionic screening can be useful for controlling the tunneling barrier in thin film devices based on CuInP$_2$S$_6$ nanoflakes. Also, we predict that the CuInP$_2$S$_6$ nanoflakes reveal features of the controllable negative capacitance effect, which make them attractive for advanced electronic devices, such as nano-capacitors and gate oxide nanomaterials with reduced heat dissipation.","sentences":["We consider nanoflakes of van der Waals ferrielectric CuInP$_2$S$_6$ covered by an ionic surface charge and reveal the appearance of polar states with relatively high polarization ~5 microC/cm$^2$ and stored free charge ~10 microC/cm$%2$, which can mimic \"mid-gap\" states related with a surface field-induced transfer of Cu and/or","In ions in the van der Waals gap.","The change in the ionic screening degree and mismatch strains induce a broad range of the transitions between paraelectric phase, antiferroelectric, ferrielectric, and ferri-ionic states in CuInP$_2$S$_6$ nanoflakes.","The states' stability and/or metastability is determined by the minimum of the system free energy consisting of electrostatic energy, elastic energy, and a Landau-type four-well potential of the ferrielectric dipole polarization.","The possibility to govern the transitions by strain and ionic screening can be useful for controlling the tunneling barrier in thin film devices based on CuInP$_2$S$_6$ nanoflakes.","Also, we predict that the CuInP$_2$S$_6$ nanoflakes reveal features of the controllable negative capacitance effect, which make them attractive for advanced electronic devices, such as nano-capacitors and gate oxide nanomaterials with reduced heat dissipation."],"url":"http://arxiv.org/abs/2405.14368v1","category":"physics.app-ph"}
{"created":"2024-05-23 09:44:36","title":"Bell Nonlocality from Wigner Negativity in Qudit Systems","abstract":"Nonlocality is an essential concept that distinguishes quantum from classical models and has been extensively studied in systems of qubits. For higher-dimensional systems, certain results for their two-level counterpart, like Bell violations with stabilizer states and Clifford operators, do not generalize. On the other hand, similar to continuous variable systems, Wigner negativity is necessary for nonlocality in qudit systems. We propose a family of Bell inequalities that inquire correlations related to the Wigner negativity of stabilizer states under the adjoint action of a generalization of the qubit $\\pi/8$ gate, which, in the bipartite case, is an abstraction of the CHSH inequality. The classical bound is simple to compute, and a specified stabilizer state maximally violates the inequality among all qudit states based on the Wigner negativity and an inequality between the 1-norm and the maximum norm. The Bell operator not only serves as a measure for the singlet fraction but also quantifies the volume of Wigner negativity. Furthermore, we give deterministic Bell violations, as well as violations with a constant number of measurements, for the Bell state relying on operators innate to higher-dimensional systems than the qudit at hand.","sentences":["Nonlocality is an essential concept that distinguishes quantum from classical models and has been extensively studied in systems of qubits.","For higher-dimensional systems, certain results for their two-level counterpart, like Bell violations with stabilizer states and Clifford operators, do not generalize.","On the other hand, similar to continuous variable systems, Wigner negativity is necessary for nonlocality in qudit systems.","We propose a family of Bell inequalities that inquire correlations related to the Wigner negativity of stabilizer states under the adjoint action of a generalization of the qubit $\\pi/8$ gate, which, in the bipartite case, is an abstraction of the CHSH inequality.","The classical bound is simple to compute, and a specified stabilizer state maximally violates the inequality among all qudit states based on the Wigner negativity and an inequality between the 1-norm and the maximum norm.","The Bell operator not only serves as a measure for the singlet fraction but also quantifies the volume of Wigner negativity.","Furthermore, we give deterministic Bell violations, as well as violations with a constant number of measurements, for the Bell state relying on operators innate to higher-dimensional systems than the qudit at hand."],"url":"http://arxiv.org/abs/2405.14367v1","category":"quant-ph"}
{"created":"2024-05-23 09:39:55","title":"Ricci--Bourguignon Almost Solitons with Special Potential on Sasaki-like Almost Contact Complex Riemannian Manifolds","abstract":"Almost contact complex Riemannian manifolds, also known as almost contact B-metric manifolds, are equipped with a pair of pseudo-Riemannian metrics that are mutually associated with each other using the tensor structure. Here we consider a special class of these manifolds, those of the Sasaki-like type. They have an interesting geometric interpretation: the complex cone of such a manifold is a holomorphic complex Riemannian manifold (also called a K\\\"ahler-Norden manifold). The basic metric on the considered manifold is specialized here as a soliton, i.e. has an additional curvature property such that the metric is a self-similar solution of an intrinsic geometric flow. Almost solitons are more general objects than solitons because they use functions rather than constants as coefficients in the defining condition. A $\\beta$-Ricci-Bourguignon-like almost soliton ($\\beta$ is a real constant) is defined using the pair of metrics. The introduced soliton is a generalization of some well-known (almost) solitons (such as those of Ricci, Schouten, and Einstein), which in principle arise from a single metric rather than a pair of metrics. The soliton potential is chosen to be pointwise collinear to the Reeb vector field, or the Lie derivative of any B-metric along the potential to be the same metric multiplied by a function. The resulting manifolds equipped with the introduced almost solitons are characterized geometrically. Appropriate examples for two types of almost solitons are constructed and the properties obtained in the theoretical part are confirmed.","sentences":["Almost contact complex Riemannian manifolds, also known as almost contact B-metric manifolds, are equipped with a pair of pseudo-Riemannian metrics that are mutually associated with each other using the tensor structure.","Here we consider a special class of these manifolds, those of the Sasaki-like type.","They have an interesting geometric interpretation: the complex cone of such a manifold is a holomorphic complex Riemannian manifold (also called a K\\\"ahler-Norden manifold).","The basic metric on the considered manifold is specialized here as a soliton, i.e. has an additional curvature property such that the metric is a self-similar solution of an intrinsic geometric flow.","Almost solitons are more general objects than solitons because they use functions rather than constants as coefficients in the defining condition.","A $\\beta$-Ricci-Bourguignon-like almost soliton ($\\beta$ is a real constant) is defined using the pair of metrics.","The introduced soliton is a generalization of some well-known (almost) solitons (such as those of Ricci, Schouten, and Einstein), which in principle arise from a single metric rather than a pair of metrics.","The soliton potential is chosen to be pointwise collinear to the Reeb vector field, or the Lie derivative of any B-metric along the potential to be the same metric multiplied by a function.","The resulting manifolds equipped with the introduced almost solitons are characterized geometrically.","Appropriate examples for two types of almost solitons are constructed and the properties obtained in the theoretical part are confirmed."],"url":"http://arxiv.org/abs/2405.14364v1","category":"math.DG"}
{"created":"2024-05-23 09:39:23","title":"Optimal Whole Body Trajectory Planning for Mobile Manipulators in Planetary Exploration and Construction","abstract":"Space robotics poses unique challenges arising from the limitation of energy and computational resources, and the complexity of the environment and employed platforms. At the control center, offline motion planning is fundamental in the computation of optimized trajectories accounting for the system's constraints. Smooth movements, collision and forbidden areas avoidance, target visibility and energy consumption are all important factors to consider to be able to generate feasible and optimal plans. When mobile manipulators (terrestrial, aerial) are employed, the base and the arm movements are often separately planned, ultimately resulting in sub-optimal solutions. We propose an Optimal Whole Body Planner (OptiWB) based on Discrete Dynamic Programming (DDP) and optimal interpolation. Kinematic redundancy is exploited for collision and forbidden areas avoidance, and to improve target illumination and visibility from onboard cameras. The planner, implemented in ROS (Robot Operating System), interfaces 3DROCS, a mission planner used in several programs of the European Space Agency (ESA) to support planetary exploration surface missions and part of the ExoMars Rover's planning software. The proposed approach is exercised on a simplified version of the Analog-1 Interact rover by ESA, a 7-DOFs robotic arm mounted on a four wheels non-holonomic platform.","sentences":["Space robotics poses unique challenges arising from the limitation of energy and computational resources, and the complexity of the environment and employed platforms.","At the control center, offline motion planning is fundamental in the computation of optimized trajectories accounting for the system's constraints.","Smooth movements, collision and forbidden areas avoidance, target visibility and energy consumption are all important factors to consider to be able to generate feasible and optimal plans.","When mobile manipulators (terrestrial, aerial) are employed, the base and the arm movements are often separately planned, ultimately resulting in sub-optimal solutions.","We propose an Optimal Whole Body Planner (OptiWB) based on Discrete Dynamic Programming (DDP) and optimal interpolation.","Kinematic redundancy is exploited for collision and forbidden areas avoidance, and to improve target illumination and visibility from onboard cameras.","The planner, implemented in ROS (Robot Operating System), interfaces 3DROCS, a mission planner used in several programs of the European Space Agency (ESA) to support planetary exploration surface missions and part of the ExoMars Rover's planning software.","The proposed approach is exercised on a simplified version of the Analog-1 Interact rover by ESA, a 7-DOFs robotic arm mounted on a four wheels non-holonomic platform."],"url":"http://arxiv.org/abs/2405.14363v1","category":"cs.RO"}
{"created":"2024-05-23 09:36:24","title":"On discount functions for economic model predictive control without terminal conditions","abstract":"In this paper, we investigate discounted economic model predictive control (E-MPC) schemes without terminal conditions in scenarios where the optimal operating behavior is a periodic orbit. For such a setting, it is known that a linearly discounted stage cost guarantees asymptotic stability of any arbitrarily small neighborhood of the optimal orbit if the prediction horizon is sufficiently long. However, in some examples very long prediction horizons are needed to achieve the desired performance. In this work, we extend these results by providing the same qualitative stability guarantees for a large class of discount functions. Numerical examples illustrate the influence of the discount function and show that with suitable discounting we can achieve significantly better performance than the linearly discounted E-MPC, even for short prediction horizons.","sentences":["In this paper, we investigate discounted economic model predictive control (E-MPC) schemes without terminal conditions in scenarios where the optimal operating behavior is a periodic orbit.","For such a setting, it is known that a linearly discounted stage cost guarantees asymptotic stability of any arbitrarily small neighborhood of the optimal orbit if the prediction horizon is sufficiently long.","However, in some examples very long prediction horizons are needed to achieve the desired performance.","In this work, we extend these results by providing the same qualitative stability guarantees for a large class of discount functions.","Numerical examples illustrate the influence of the discount function and show that with suitable discounting we can achieve significantly better performance than the linearly discounted E-MPC, even for short prediction horizons."],"url":"http://arxiv.org/abs/2405.14361v1","category":"math.OC"}
{"created":"2024-05-23 09:35:10","title":"Influence of the competition in the spatial dynamics of a population of Aedes mosquitoes","abstract":"In this article, we investigate a competitive reaction-diffusion system modelling the interaction between several species of mosquitoes. In particular, it has been observed that in tropical regions, Aedes aegypti mosquitoes are well established in urban area whereas Aedes albopictus mosquitoes spread widely in forest region. The aim of this paper is to propose a simple mathematical system modeling this segregation phenomenon. Moreprecisely, after modeling the dynamics by a competitive reaction-diffusion system with spatial heterogeneity, we prove that when there is a strong competition between these two species of mosquitoes, solutions to this system converge in long time to segregated stationary solutions. Then we study the influence of this strong competition on the success of a population replacement strategy using Wolbachia bacteria. Our theoretical results are also illustrated by some numerical simulations.","sentences":["In this article, we investigate a competitive reaction-diffusion system modelling the interaction between several species of mosquitoes.","In particular, it has been observed that in tropical regions, Aedes aegypti mosquitoes are well established in urban area whereas Aedes albopictus mosquitoes spread widely in forest region.","The aim of this paper is to propose a simple mathematical system modeling this segregation phenomenon.","Moreprecisely, after modeling the dynamics by a competitive reaction-diffusion system with spatial heterogeneity, we prove that when there is a strong competition between these two species of mosquitoes, solutions to this system converge in long time to segregated stationary solutions.","Then we study the influence of this strong competition on the success of a population replacement strategy using Wolbachia bacteria.","Our theoretical results are also illustrated by some numerical simulations."],"url":"http://arxiv.org/abs/2405.14360v1","category":"math.AP"}
{"created":"2024-05-23 09:33:57","title":"AI-Olympics: Exploring the Generalization of Agents through Open Competitions","abstract":"Between 2021 and 2023, AI-Olympics, a series of online AI competitions was hosted by the online evaluation platform Jidi in collaboration with the IJCAI committee. In these competitions, an agent is required to accomplish diverse sports tasks in a two-dimensional continuous world, while competing against an opponent. This paper provides a brief overview of the competition series and highlights notable findings. We aim to contribute insights to the field of multi-agent decision-making and explore the generalization of agents through engineering efforts.","sentences":["Between 2021 and 2023, AI-Olympics, a series of online AI competitions was hosted by the online evaluation platform Jidi in collaboration with the IJCAI committee.","In these competitions, an agent is required to accomplish diverse sports tasks in a two-dimensional continuous world, while competing against an opponent.","This paper provides a brief overview of the competition series and highlights notable findings.","We aim to contribute insights to the field of multi-agent decision-making and explore the generalization of agents through engineering efforts."],"url":"http://arxiv.org/abs/2405.14358v1","category":"cs.MA"}
{"created":"2024-05-23 09:29:00","title":"Retrieval-Augmented Mining of Temporal Logic Specifications from Data","abstract":"The integration of cyber-physical systems (CPS) into everyday life raises the critical necessity of ensuring their safety and reliability. An important step in this direction is requirement mining, i.e. inferring formally specified system properties from observed behaviors, in order to discover knowledge about the system. Signal Temporal Logic (STL) offers a concise yet expressive language for specifying requirements, particularly suited for CPS, where behaviors are typically represented as time series data. This work addresses the task of learning STL requirements from observed behaviors in a data-driven manner, focusing on binary classification, i.e. on inferring properties of the system which are able to discriminate between regular and anomalous behaviour, and that can be used both as classifiers and as monitors of the compliance of the CPS to desirable specifications. We present a novel framework that combines Bayesian Optimization (BO) and Information Retrieval (IR) techniques to simultaneously learn both the structure and the parameters of STL formulae, without restrictions on the STL grammar. Specifically, we propose a framework that leverages a dense vector database containing semantic-preserving continuous representations of millions of formulae, queried for facilitating the mining of requirements inside a BO loop. We demonstrate the effectiveness of our approach in several signal classification applications, showing its ability to extract interpretable insights from system executions and advance the state-of-the-art in requirement mining for CPS.","sentences":["The integration of cyber-physical systems (CPS) into everyday life raises the critical necessity of ensuring their safety and reliability.","An important step in this direction is requirement mining, i.e. inferring formally specified system properties from observed behaviors, in order to discover knowledge about the system.","Signal Temporal Logic (STL) offers a concise yet expressive language for specifying requirements, particularly suited for CPS, where behaviors are typically represented as time series data.","This work addresses the task of learning STL requirements from observed behaviors in a data-driven manner, focusing on binary classification, i.e. on inferring properties of the system which are able to discriminate between regular and anomalous behaviour, and that can be used both as classifiers and as monitors of the compliance of the CPS to desirable specifications.","We present a novel framework that combines Bayesian Optimization (BO) and Information Retrieval (IR) techniques to simultaneously learn both the structure and the parameters of STL formulae, without restrictions on the STL grammar.","Specifically, we propose a framework that leverages a dense vector database containing semantic-preserving continuous representations of millions of formulae, queried for facilitating the mining of requirements inside a BO loop.","We demonstrate the effectiveness of our approach in several signal classification applications, showing its ability to extract interpretable insights from system executions and advance the state-of-the-art in requirement mining for CPS."],"url":"http://arxiv.org/abs/2405.14355v1","category":"cs.LG"}
{"created":"2024-05-23 09:27:46","title":"Bayesian optimisation with improved information sharing for the variational quantum eigensolver","abstract":"This work presents a detailed empirical analysis of Bayesian optimisation with information sharing (BOIS) for the variational quantum eigensolver (VQE). The method is applied to computing the potential energy surfaces (PES) of the hydrogen and water molecules. We performed noise-free simulations and investigated the algorithms' performance under the influence of noise for the hydrogen molecule, using both emulated and real quantum hardware (IBMQ System One). Based on the noise free simulations we compared different existing information sharing schemes and proposed a new one, which trades parallelisability of the algorithm for a significant reduction in the amount of quantum computing resources required until convergence. In particular, our numerical experiments show an improvement by a factor of 1.5 as compared to the previously reported sharing schemes in H2, and at least by a factor of 5 as compared to no sharing in H2O. Other algorithmic aspects of the Bayesian optimisation, namely, the acquisition weight decrease rate and kernel, are shown to have an influence on the quantum computation (QC) demand of the same order of magnitude.","sentences":["This work presents a detailed empirical analysis of Bayesian optimisation with information sharing (BOIS) for the variational quantum eigensolver (VQE).","The method is applied to computing the potential energy surfaces (PES) of the hydrogen and water molecules.","We performed noise-free simulations and investigated the algorithms' performance under the influence of noise for the hydrogen molecule, using both emulated and real quantum hardware (IBMQ System One).","Based on the noise free simulations we compared different existing information sharing schemes and proposed a new one, which trades parallelisability of the algorithm for a significant reduction in the amount of quantum computing resources required until convergence.","In particular, our numerical experiments show an improvement by a factor of 1.5 as compared to the previously reported sharing schemes in H2, and at least by a factor of 5 as compared to no sharing in H2O. Other algorithmic aspects of the Bayesian optimisation, namely, the acquisition weight decrease rate and kernel, are shown to have an influence on the quantum computation (QC) demand of the same order of magnitude."],"url":"http://arxiv.org/abs/2405.14353v1","category":"quant-ph"}
{"created":"2024-05-23 09:24:33","title":"Explaining Graph Neural Networks via Structure-aware Interaction Index","abstract":"The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.","sentences":["The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation.","However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance.","This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes.","Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion.","We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes.","Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively.","Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.14352v1","category":"cs.LG"}
{"created":"2024-05-23 09:13:36","title":"Efficient Visual State Space Model for Image Deblurring","abstract":"Convolutional neural networks (CNNs) and Vision Transformers (ViTs) have achieved excellent performance in image restoration. ViTs typically yield superior results in image restoration compared to CNNs due to their ability to capture long-range dependencies and input-dependent characteristics. However, the computational complexity of Transformer-based models grows quadratically with the image resolution, limiting their practical appeal in high-resolution image restoration tasks. In this paper, we propose a simple yet effective visual state space model (EVSSM) for image deblurring, leveraging the benefits of state space models (SSMs) to visual data. In contrast to existing methods that employ several fixed-direction scanning for feature extraction, which significantly increases the computational cost, we develop an efficient visual scan block that applies various geometric transformations before each SSM-based module, capturing useful non-local information and maintaining high efficiency. Extensive experimental results show that the proposed EVSSM performs favorably against state-of-the-art image deblurring methods on benchmark datasets and real-captured images.","sentences":["Convolutional neural networks (CNNs) and Vision Transformers (ViTs) have achieved excellent performance in image restoration.","ViTs typically yield superior results in image restoration compared to CNNs due to their ability to capture long-range dependencies and input-dependent characteristics.","However, the computational complexity of Transformer-based models grows quadratically with the image resolution, limiting their practical appeal in high-resolution image restoration tasks.","In this paper, we propose a simple yet effective visual state space model (EVSSM) for image deblurring, leveraging the benefits of state space models (SSMs) to visual data.","In contrast to existing methods that employ several fixed-direction scanning for feature extraction, which significantly increases the computational cost, we develop an efficient visual scan block that applies various geometric transformations before each SSM-based module, capturing useful non-local information and maintaining high efficiency.","Extensive experimental results show that the proposed EVSSM performs favorably against state-of-the-art image deblurring methods on benchmark datasets and real-captured images."],"url":"http://arxiv.org/abs/2405.14343v1","category":"cs.CV"}
{"created":"2024-05-23 09:11:47","title":"How do Observable Users Decompose D3 Code? An Exploratory Study","abstract":"Users often struggle to program visualizations using complex toolkits like D3. Before we can design effective code assistants to support them, we must first understand how D3 users reason about their code. In this work, we explore users' understanding of D3 using an important gauge of code comprehension in CS education: code decomposition. We qualitatively analyze 560 D3 programs published on Observable and identify three distinct strategies to decomposing D3 programs: segmenting code into layers of functionality, keeping everything all in one cell, or creating reusable visualization functions. We also observe how users inherit decomposition methods from copied examples and reorganize copied code to suit their needs. We corroborate our findings for decomposition preferences through interviews with D3 and Observable users. Based on our findings, we suggest strategies for generating more intuitive D3 code recommendations using decomposition preferences and highlight new research opportunities for visualization code assistants. All supplemental materials are available at https://osf.io/sudb8/?view_only=302fc5c8d397412aac35c6e094ae7dd6.","sentences":["Users often struggle to program visualizations using complex toolkits like D3.","Before we can design effective code assistants to support them, we must first understand how D3 users reason about their code.","In this work, we explore users' understanding of D3 using an important gauge of code comprehension in CS education: code decomposition.","We qualitatively analyze 560 D3 programs published on Observable and identify three distinct strategies to decomposing D3 programs: segmenting code into layers of functionality, keeping everything all in one cell, or creating reusable visualization functions.","We also observe how users inherit decomposition methods from copied examples and reorganize copied code to suit their needs.","We corroborate our findings for decomposition preferences through interviews with D3 and Observable users.","Based on our findings, we suggest strategies for generating more intuitive D3 code recommendations using decomposition preferences and highlight new research opportunities for visualization code assistants.","All supplemental materials are available at https://osf.io/sudb8/?view_only=302fc5c8d397412aac35c6e094ae7dd6."],"url":"http://arxiv.org/abs/2405.14341v1","category":"cs.HC"}
{"created":"2024-05-23 09:11:33","title":"Two new motivic complexes for non-smooth schemes","abstract":"These are expanded notes from a talk at the RIMS Workshop, Algebraic Number Theory and Related Topics, December 13th, 2023. We discussed Elmanto-Morrow's motivic complex, the procdh sheafification of the classical motivic complex, and their comparison. The procdh topology and the comparison is joint work with Shuji Saito. The comparison was obtained through joint discussion with Morrow, and its proof relies heavily on the main results of [EM23].","sentences":["These are expanded notes from a talk at the RIMS Workshop, Algebraic Number Theory and Related Topics, December 13th, 2023.","We discussed Elmanto-Morrow's motivic complex, the procdh sheafification of the classical motivic complex, and their comparison.","The procdh topology and the comparison is joint work with Shuji Saito.","The comparison was obtained through joint discussion with Morrow, and its proof relies heavily on the main results of [EM23]."],"url":"http://arxiv.org/abs/2405.14340v1","category":"math.AG"}
{"created":"2024-05-23 09:11:21","title":"Green Multi-Objective Scheduling -- A memetic NSGA-III for flexible production with real-time energy cost and emissions","abstract":"The use of renewable energies strengthens decarbonization strategies. To integrate volatile renewable sources, energy systems require grid expansion, storage capabilities, or flexible consumption. This study focuses on industries adjusting production to real-time energy markets, offering flexible consumption to the grid. Flexible production considers not only traditional goals like minimizing production time but also minimizing energy costs and emissions, thereby enhancing the sustainability of businesses. However, existing research focuses on single goals, neglects the combination of makespan, energy costs and emissions, or assumes constant or periodic tariffs instead of a dynamic energy market. We present a novel memetic NSGA-III to minimize makespan, energy cost, and emissions, integrating real energy market data, and allowing manufacturers to adapt consumption to current grid conditions. Evaluating it with benchmark instances from literature and real energy market data, we explore the trade-offs between objectives, showcasing potential savings in energy costs and emissions on estimated Pareto fronts.","sentences":["The use of renewable energies strengthens decarbonization strategies.","To integrate volatile renewable sources, energy systems require grid expansion, storage capabilities, or flexible consumption.","This study focuses on industries adjusting production to real-time energy markets, offering flexible consumption to the grid.","Flexible production considers not only traditional goals like minimizing production time but also minimizing energy costs and emissions, thereby enhancing the sustainability of businesses.","However, existing research focuses on single goals, neglects the combination of makespan, energy costs and emissions, or assumes constant or periodic tariffs instead of a dynamic energy market.","We present a novel memetic NSGA-III to minimize makespan, energy cost, and emissions, integrating real energy market data, and allowing manufacturers to adapt consumption to current grid conditions.","Evaluating it with benchmark instances from literature and real energy market data, we explore the trade-offs between objectives, showcasing potential savings in energy costs and emissions on estimated Pareto fronts."],"url":"http://arxiv.org/abs/2405.14339v1","category":"cs.NE"}
{"created":"2024-05-23 09:07:46","title":"Coherence-mixedness trade-offs","abstract":"Quantum coherence constitutes a foundational characteristic of quantum mechanics and is integral to emerging quantum resource theories. However, quantum coherence is severely restricted by environmental noise in general quantum processing, indicated by the loss of information of a quantum system. Such processing can be described by the trade-offs between the coherence and the mixedness. Based on the $l_2$ norm coherence, conditional von Neumann entropy and Wigner-Yanase skew information, we derive basis-independent constraints on the attainable quantum coherence imposed by the mixedness of a quantum state, which generalize the prior basis-dependent relations, provide fundamental insights into the latent coherence resources present within arbitrary quantum systems that undergo decoherence and quantify the inherent limits on extractable coherence imposed by environmental noise.","sentences":["Quantum coherence constitutes a foundational characteristic of quantum mechanics and is integral to emerging quantum resource theories.","However, quantum coherence is severely restricted by environmental noise in general quantum processing, indicated by the loss of information of a quantum system.","Such processing can be described by the trade-offs between the coherence and the mixedness.","Based on the $l_2$ norm coherence, conditional von Neumann entropy and Wigner-Yanase skew information, we derive basis-independent constraints on the attainable quantum coherence imposed by the mixedness of a quantum state, which generalize the prior basis-dependent relations, provide fundamental insights into the latent coherence resources present within arbitrary quantum systems that undergo decoherence and quantify the inherent limits on extractable coherence imposed by environmental noise."],"url":"http://arxiv.org/abs/2405.14337v1","category":"quant-ph"}
{"created":"2024-05-23 09:07:21","title":"Hierarchical Salient Patch Identification for Interpretable Fundus Disease Localization","abstract":"With the widespread application of deep learning technology in medical image analysis, how to effectively explain model decisions and improve diagnosis accuracy has become an urgent problem that needs to be solved. Attribution methods have become a key tool to help doctors better understand the diagnostic basis of models, and they are used to explain and localize diseases in medical images. However, previous methods suffer from inaccurate and incomplete localization problems for fundus diseases with complex and diverse structures. In order to solve the above problems, we propose a weakly supervised interpretable fundus disease localization method hierarchical salient patch identification (HSPI), which can achieve interpretable disease localization using only image-level labels and neural network classifiers. First, we proposed salient patch identification (SPI), which divides the image into several patches and optimizes consistency loss to identify which patch in the input image is most important for decision-making to locate the disease. Secondly, we propose a hierarchical identification strategy to force SPI to analyze the importance of different areas to neural network classifiers decision-making to comprehensively locate disease areas. Then, we introduced conditional peak focusing to ensure that the mask vector can accurately locate the decision area. Finally, we also propose patch selection based on multi-size intersection to filter out incorrectly or additionally identified non-disease regions. We conduct disease localization experiments on medical image datasets and achieve the best performance on multiple evaluation metrics compared with previous interpretable attribution methods. We performed additional ablation studies to verify the effectiveness of each method.","sentences":["With the widespread application of deep learning technology in medical image analysis, how to effectively explain model decisions and improve diagnosis accuracy has become an urgent problem that needs to be solved.","Attribution methods have become a key tool to help doctors better understand the diagnostic basis of models, and they are used to explain and localize diseases in medical images.","However, previous methods suffer from inaccurate and incomplete localization problems for fundus diseases with complex and diverse structures.","In order to solve the above problems, we propose a weakly supervised interpretable fundus disease localization method hierarchical salient patch identification (HSPI), which can achieve interpretable disease localization using only image-level labels and neural network classifiers.","First, we proposed salient patch identification (SPI), which divides the image into several patches and optimizes consistency loss to identify which patch in the input image is most important for decision-making to locate the disease.","Secondly, we propose a hierarchical identification strategy to force SPI to analyze the importance of different areas to neural network classifiers decision-making to comprehensively locate disease areas.","Then, we introduced conditional peak focusing to ensure that the mask vector can accurately locate the decision area.","Finally, we also propose patch selection based on multi-size intersection to filter out incorrectly or additionally identified non-disease regions.","We conduct disease localization experiments on medical image datasets and achieve the best performance on multiple evaluation metrics compared with previous interpretable attribution methods.","We performed additional ablation studies to verify the effectiveness of each method."],"url":"http://arxiv.org/abs/2405.14334v1","category":"cs.CV"}
{"created":"2024-05-23 08:59:41","title":"Simultaneous measurement of refraction and absorption with an integrated near-infrared Mach-Zehnder interferometer","abstract":"Most integrated evanescent-field photonic sensors measure changes in either the real part or the imaginary part of the complex refractive index of the sample, i.e., refraction or absorption. Here we propose and experimentally demonstrate a near-infrared sensor based on a silicon nitride Mach-Zehnder interferometer which provides a direct measurement of the complex refractive index. Our architecture exhibits a high sensitivity, achieving limits of detection below $2\\cdot 10^{-6}\\,\\mathrm{RIU}$ for both the real and imaginary parts of the refractive index. We furthermore show that our sensor can be employed as an integrated dispersion spectrometer.","sentences":["Most integrated evanescent-field photonic sensors measure changes in either the real part or the imaginary part of the complex refractive index of the sample, i.e., refraction or absorption.","Here we propose and experimentally demonstrate a near-infrared sensor based on a silicon nitride Mach-Zehnder interferometer which provides a direct measurement of the complex refractive index.","Our architecture exhibits a high sensitivity, achieving limits of detection below $2\\cdot 10^{-6}\\,\\mathrm{RIU}$ for both the real and imaginary parts of the refractive index.","We furthermore show that our sensor can be employed as an integrated dispersion spectrometer."],"url":"http://arxiv.org/abs/2405.14328v1","category":"physics.optics"}
{"created":"2024-05-23 08:55:20","title":"Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection","abstract":"Recent studies highlighted a practical setting of unsupervised anomaly detection (UAD) that builds a unified model for multi-class images, serving as an alternative to the conventional one-class-one-model setup. Despite various advancements addressing this challenging task, the detection performance under the multi-class setting still lags far behind state-of-the-art class-separated models. Our research aims to bridge this substantial performance gap. In this paper, we introduce a minimalistic reconstruction-based anomaly detection framework, namely Dinomaly, which leverages pure Transformer architectures without relying on complex designs, additional modules, or specialized tricks. Given this powerful framework consisted of only Attentions and MLPs, we found four simple components that are essential to multi-class anomaly detection: (1) Foundation Transformers that extracts universal and discriminative features, (2) Noisy Bottleneck where pre-existing Dropouts do all the noise injection tricks, (3) Linear Attention that naturally cannot focus, and (4) Loose Reconstruction that does not force layer-to-layer and point-by-point reconstruction. Extensive experiments are conducted across three popular anomaly detection benchmarks including MVTec-AD, VisA, and the recently released Real-IAD. Our proposed Dinomaly achieves impressive image AUROC of 99.6%, 98.7%, and 89.3% on the three datasets respectively, which is not only superior to state-of-the-art multi-class UAD methods, but also surpasses the most advanced class-separated UAD records.","sentences":["Recent studies highlighted a practical setting of unsupervised anomaly detection (UAD) that builds a unified model for multi-class images, serving as an alternative to the conventional one-class-one-model setup.","Despite various advancements addressing this challenging task, the detection performance under the multi-class setting still lags far behind state-of-the-art class-separated models.","Our research aims to bridge this substantial performance gap.","In this paper, we introduce a minimalistic reconstruction-based anomaly detection framework, namely Dinomaly, which leverages pure Transformer architectures without relying on complex designs, additional modules, or specialized tricks.","Given this powerful framework consisted of only Attentions and MLPs, we found four simple components that are essential to multi-class anomaly detection: (1) Foundation Transformers that extracts universal and discriminative features, (2) Noisy Bottleneck where pre-existing Dropouts do all the noise injection tricks, (3) Linear Attention that naturally cannot focus, and (4) Loose Reconstruction that does not force layer-to-layer and point-by-point reconstruction.","Extensive experiments are conducted across three popular anomaly detection benchmarks including MVTec-AD, VisA, and the recently released Real-IAD.","Our proposed Dinomaly achieves impressive image AUROC of 99.6%, 98.7%, and 89.3% on the three datasets respectively, which is not only superior to state-of-the-art multi-class UAD methods, but also surpasses the most advanced class-separated UAD records."],"url":"http://arxiv.org/abs/2405.14325v1","category":"cs.CV"}
{"created":"2024-05-23 08:52:55","title":"Active Magnetic Matter: Propelling Ferrimagnetic Domain Walls by Dynamical Frustration","abstract":"Active matter encompasses many-particle systems with self-propelling units, such as flocks of birds or schools of fish. Here, we show how self-propelling domain walls can be realised in a solid-state system when a ferrimagnet is weakly driven out of thermal equilibrium by an oscillating field. This activates the Goldstone mode, inducing a rotation of the antiferromagnetic xy-order in a clockwise or anticlockwise direction, determined by the sign of the ferromagnetic component. Two opposite directions of rotation meet at a ferromagnetic domain wall, resulting in 'dynamical frustration', with three main consequences. (i) Domain walls move actively in a direction chosen by spontaneous symmetry breaking. Their speed is proportional to the square root of the driving power across large parameter regimes. (ii) In one dimension (1D), after a quench into the ferrimagnetic phase, this motion and strong hydrodynamic interactions lead to a linear growth of the magnetic correlation length over time, much faster than in equilibrium. (iii) The dynamical frustration makes the system highly resilient to noise. The correlation length of the weakly driven 1D system can be orders of magnitude larger than in the corresponding equilibrium system with the same noise level.","sentences":["Active matter encompasses many-particle systems with self-propelling units, such as flocks of birds or schools of fish.","Here, we show how self-propelling domain walls can be realised in a solid-state system when a ferrimagnet is weakly driven out of thermal equilibrium by an oscillating field.","This activates the Goldstone mode, inducing a rotation of the antiferromagnetic xy-order in a clockwise or anticlockwise direction, determined by the sign of the ferromagnetic component.","Two opposite directions of rotation meet at a ferromagnetic domain wall, resulting in 'dynamical frustration', with three main consequences.","(i) Domain walls move actively in a direction chosen by spontaneous symmetry breaking.","Their speed is proportional to the square root of the driving power across large parameter regimes.","(ii) In one dimension (1D), after a quench into the ferrimagnetic phase, this motion and strong hydrodynamic interactions lead to a linear growth of the magnetic correlation length over time, much faster than in equilibrium.","(iii)","The dynamical frustration makes the system highly resilient to noise.","The correlation length of the weakly driven 1D system can be orders of magnitude larger than in the corresponding equilibrium system with the same noise level."],"url":"http://arxiv.org/abs/2405.14320v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-23 08:49:55","title":"Variational Signal Separation for Automotive Radar Interference Mitigation","abstract":"Algorithms for joint mutual interference mitigation and object parameter estimation are a key enabler for automotive applications of frequency-modulated continuous wave (FMCW) radar. The underlying signal model poses a challenge for signal separation, since both the coherent radar echo and the non-coherent interference influenced by individual multipath propagation channels must be considered. In particular, under certain assumptions, the model is described as a superposition of multipath channels weighted by parametric chirp envelopes in the case of interference. In this paper, we introduce a method inspired by sparse Bayesian learning (SBL) to detect and estimate radar object parameters while also estimating and successively canceling the interference signal. An augmented probabilistic model is employed that uses hierarchical Gamma-Gaussian prior model for each multipath channel separately. Based on this model an iterative inference algorithm is derived using the variational expectation-maximization (EM) methodology. The algorithm is statistically evaluated in terms of object parameter estimation accuracy and robustness, indicating that it is fundamentally capable of achieving the Cramer-Rao lower bound (CRLB) with respect to the accuracy of object estimates and it closely follows the radar performance achieved when no interference is present.","sentences":["Algorithms for joint mutual interference mitigation and object parameter estimation are a key enabler for automotive applications of frequency-modulated continuous wave (FMCW) radar.","The underlying signal model poses a challenge for signal separation, since both the coherent radar echo and the non-coherent interference influenced by individual multipath propagation channels must be considered.","In particular, under certain assumptions, the model is described as a superposition of multipath channels weighted by parametric chirp envelopes in the case of interference.","In this paper, we introduce a method inspired by sparse Bayesian learning (SBL) to detect and estimate radar object parameters while also estimating and successively canceling the interference signal.","An augmented probabilistic model is employed that uses hierarchical Gamma-Gaussian prior model for each multipath channel separately.","Based on this model an iterative inference algorithm is derived using the variational expectation-maximization (EM) methodology.","The algorithm is statistically evaluated in terms of object parameter estimation accuracy and robustness, indicating that it is fundamentally capable of achieving the Cramer-Rao lower bound (CRLB) with respect to the accuracy of object estimates and it closely follows the radar performance achieved when no interference is present."],"url":"http://arxiv.org/abs/2405.14319v1","category":"eess.SP"}
{"created":"2024-05-23 08:32:58","title":"Improving Gloss-free Sign Language Translation by Reducing Representation Density","abstract":"Gloss-free sign language translation (SLT) aims to develop well-performing SLT systems with no requirement for the costly gloss annotations, but currently still lags behind gloss-based approaches significantly. In this paper, we identify a representation density problem that could be a bottleneck in restricting the performance of gloss-free SLT. Specifically, the representation density problem describes that the visual representations of semantically distinct sign gestures tend to be closely packed together in feature space, which makes gloss-free methods struggle with distinguishing different sign gestures and suffer from a sharp performance drop. To address the representation density problem, we introduce a simple but effective contrastive learning strategy, namely SignCL, which encourages gloss-free models to learn more discriminative feature representation in a self-supervised manner. Our experiments demonstrate that the proposed SignCL can significantly reduce the representation density and improve performance across various translation frameworks. Specifically, SignCL achieves a significant improvement in BLEU score for the Sign Language Transformer and GFSLT-VLP on the CSL-Daily dataset by 39% and 46%, respectively, without any increase of model parameters. Compared to Sign2GPT, a state-of-the-art method based on large-scale pre-trained vision and language models, SignCL achieves better performance with only 35% of its parameters. Implementation and Checkpoints are available at https://github.com/JinhuiYE/SignCL.","sentences":["Gloss-free sign language translation (SLT) aims to develop well-performing SLT systems with no requirement for the costly gloss annotations, but currently still lags behind gloss-based approaches significantly.","In this paper, we identify a representation density problem that could be a bottleneck in restricting the performance of gloss-free SLT.","Specifically, the representation density problem describes that the visual representations of semantically distinct sign gestures tend to be closely packed together in feature space, which makes gloss-free methods struggle with distinguishing different sign gestures and suffer from a sharp performance drop.","To address the representation density problem, we introduce a simple but effective contrastive learning strategy, namely SignCL, which encourages gloss-free models to learn more discriminative feature representation in a self-supervised manner.","Our experiments demonstrate that the proposed SignCL can significantly reduce the representation density and improve performance across various translation frameworks.","Specifically, SignCL achieves a significant improvement in BLEU score for the Sign Language Transformer and GFSLT-VLP on the CSL-Daily dataset by 39% and 46%, respectively, without any increase of model parameters.","Compared to Sign2GPT, a state-of-the-art method based on large-scale pre-trained vision and language models, SignCL achieves better performance with only 35% of its parameters.","Implementation and Checkpoints are available at https://github.com/JinhuiYE/SignCL."],"url":"http://arxiv.org/abs/2405.14312v1","category":"cs.CV"}
{"created":"2024-05-23 08:26:49","title":"On the convergence of the polarization tensor in space-time of three dimensions","abstract":"In this paper, we consider the convergence properties of the polarization tensor of graphene obtained in the framework of thermal quantum field theory in three-dimensional space-time. During the last years, this problem attracted much attention in connection with calculation of the Casimir force in graphene systems and investigation of the electrical conductivity and reflectance of graphene sheets. There are contradictory statements in the literature, especially on whether this tensor has an ultraviolet divergence in three dimensions. Here, we analyze this problem using the well known method of dimensional regularization. It is shown that the thermal correction to the polarization tensor is finite at any $D$, whereas its zero-temperature part behaves differently for $D=3$ and 4. For $D=3$, it is obtained by the analytic continuation with no subtracting infinitely large terms. As to the space-time of $D=4$, the finite result for the polarization tensor at zero temperature is found after subtracting the pole term. Our results are in agreement with previous calculations of the polarization tensor at both zero and nonzero temperature. This opens possibility for a wider application of the quantum field theoretical approach in investigations of graphene and other two-dimensional novel materials.","sentences":["In this paper, we consider the convergence properties of the polarization tensor of graphene obtained in the framework of thermal quantum field theory in three-dimensional space-time.","During the last years, this problem attracted much attention in connection with calculation of the Casimir force in graphene systems and investigation of the electrical conductivity and reflectance of graphene sheets.","There are contradictory statements in the literature, especially on whether this tensor has an ultraviolet divergence in three dimensions.","Here, we analyze this problem using the well known method of dimensional regularization.","It is shown that the thermal correction to the polarization tensor is finite at any $D$, whereas its zero-temperature part behaves differently for $D=3$ and 4.","For $D=3$, it is obtained by the analytic continuation with no subtracting infinitely large terms.","As to the space-time of $D=4$, the finite result for the polarization tensor at zero temperature is found after subtracting the pole term.","Our results are in agreement with previous calculations of the polarization tensor at both zero and nonzero temperature.","This opens possibility for a wider application of the quantum field theoretical approach in investigations of graphene and other two-dimensional novel materials."],"url":"http://arxiv.org/abs/2405.14306v1","category":"hep-th"}
{"created":"2024-05-23 08:07:41","title":"Frequency-Domain Sound Field from the Perspective of Band-Limited Functions","abstract":"In this paper, the frequency-domain sound field is regarded as an element of some band-limited function space, and a representation of the field as a linear combination of the reproducing kernel in that space is proposed. This model has the strongest representational capacity of all function systems when we know only the sound pressure information at arbitrary positions. The proposed model can be considered a generalization of the existing three-dimensional sound field model using the reproducing kernel of the solution space of the Helmholtz equation to the spatial dimension. One of the advantages of capturing the frequency-domain sound field in this way is the simplicity achieved for the estimation formula of the wavenumber spectrum. Two numerical simulations were conducted to validate the proposed methods.","sentences":["In this paper, the frequency-domain sound field is regarded as an element of some band-limited function space, and a representation of the field as a linear combination of the reproducing kernel in that space is proposed.","This model has the strongest representational capacity of all function systems when we know only the sound pressure information at arbitrary positions.","The proposed model can be considered a generalization of the existing three-dimensional sound field model using the reproducing kernel of the solution space of the Helmholtz equation to the spatial dimension.","One of the advantages of capturing the frequency-domain sound field in this way is the simplicity achieved for the estimation formula of the wavenumber spectrum.","Two numerical simulations were conducted to validate the proposed methods."],"url":"http://arxiv.org/abs/2405.14290v1","category":"cs.SD"}
{"created":"2024-05-23 08:05:54","title":"Generating-functional analysis of random Lotka-Volterra systems: A step-by-step guide","abstract":"This paper provides what is hopefully a self-contained set of notes describing the detailed steps of a generating-functional analysis of systems of generalised Lotka-Volterra equations with random interaction coefficients. Nothing in these notes is original, instead the generating-functional method (also known as the Martin-Siggia-Rose-DeDominic-Janssen formalism) and the resulting dynamic mean field theories have been used for the study of disordered systems and spin glasses for decades. But it is hard to find unifying sources which would allow a beginner to learn step-by-step how these methods can be used. My aim is to provide such a source. Most of the calculations are specific to generalised Lotka-Volterra systems, but much can be transferred to disordered systems in more general.","sentences":["This paper provides what is hopefully a self-contained set of notes describing the detailed steps of a generating-functional analysis of systems of generalised Lotka-Volterra equations with random interaction coefficients.","Nothing in these notes is original, instead the generating-functional method (also known as the Martin-Siggia-Rose-DeDominic-Janssen formalism) and the resulting dynamic mean field theories have been used for the study of disordered systems and spin glasses for decades.","But it is hard to find unifying sources which would allow a beginner to learn step-by-step how these methods can be used.","My aim is to provide such a source.","Most of the calculations are specific to generalised Lotka-Volterra systems, but much can be transferred to disordered systems in more general."],"url":"http://arxiv.org/abs/2405.14289v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-23 17:59:58","title":"Federated Online Adaptation for Deep Stereo","abstract":"We introduce a novel approach for adapting deep stereo networks in a collaborative manner. By building over principles of federated learning, we develop a distributed framework allowing for demanding the optimization process to a number of clients deployed in different environments. This makes it possible, for a deep stereo network running on resourced-constrained devices, to capitalize on the adaptation process carried out by other instances of the same architecture, and thus improve its accuracy in challenging environments even when it cannot carry out adaptation on its own. Experimental results show how federated adaptation performs equivalently to on-device adaptation, and even better when dealing with challenging environments.","sentences":["We introduce a novel approach for adapting deep stereo networks in a collaborative manner.","By building over principles of federated learning, we develop a distributed framework allowing for demanding the optimization process to a number of clients deployed in different environments.","This makes it possible, for a deep stereo network running on resourced-constrained devices, to capitalize on the adaptation process carried out by other instances of the same architecture, and thus improve its accuracy in challenging environments even when it cannot carry out adaptation on its own.","Experimental results show how federated adaptation performs equivalently to on-device adaptation, and even better when dealing with challenging environments."],"url":"http://arxiv.org/abs/2405.14873v1","category":"cs.CV"}
{"created":"2024-05-23 17:58:43","title":"Mamba-R: Vision Mamba ALSO Needs Registers","abstract":"Similar to Vision Transformers, this paper identifies artifacts also present within the feature maps of Vision Mamba. These artifacts, corresponding to high-norm tokens emerging in low-information background areas of images, appear much more severe in Vision Mamba -- they exist prevalently even with the tiny-sized model and activate extensively across background regions. To mitigate this issue, we follow the prior solution of introducing register tokens into Vision Mamba. To better cope with Mamba blocks' uni-directional inference paradigm, two key modifications are introduced: 1) evenly inserting registers throughout the input token sequence, and 2) recycling registers for final decision predictions. We term this new architecture Mamba-R. Qualitative observations suggest, compared to vanilla Vision Mamba, Mamba-R's feature maps appear cleaner and more focused on semantically meaningful regions. Quantitatively, Mamba-R attains stronger performance and scales better. For example, on the ImageNet benchmark, our base-size Mamba-R attains 82.9% accuracy, significantly outperforming Vim-B's 81.8%; furthermore, we provide the first successful scaling to the large model size (i.e., with 341M parameters), attaining a competitive accuracy of 83.2% (84.5% if finetuned with 384x384 inputs). Additional validation on the downstream semantic segmentation task also supports Mamba-R's efficacy.","sentences":["Similar to Vision Transformers, this paper identifies artifacts also present within the feature maps of Vision Mamba.","These artifacts, corresponding to high-norm tokens emerging in low-information background areas of images, appear much more severe in Vision Mamba -- they exist prevalently even with the tiny-sized model and activate extensively across background regions.","To mitigate this issue, we follow the prior solution of introducing register tokens into Vision Mamba.","To better cope with Mamba blocks' uni-directional inference paradigm, two key modifications are introduced: 1) evenly inserting registers throughout the input token sequence, and 2) recycling registers for final decision predictions.","We term this new architecture Mamba-R. Qualitative observations suggest, compared to vanilla Vision Mamba, Mamba-R's feature maps appear cleaner and more focused on semantically meaningful regions.","Quantitatively, Mamba-R attains stronger performance and scales better.","For example, on the ImageNet benchmark, our base-size Mamba-R attains 82.9% accuracy, significantly outperforming Vim-B's 81.8%; furthermore, we provide the first successful scaling to the large model size (i.e., with 341M parameters), attaining a competitive accuracy of 83.2% (84.5% if finetuned with 384x384 inputs).","Additional validation on the downstream semantic segmentation task also supports Mamba-R's efficacy."],"url":"http://arxiv.org/abs/2405.14858v1","category":"cs.CV"}
{"created":"2024-05-23 17:55:09","title":"Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon Divergence Between Initial and Target Distribution","abstract":"Differentiable annealed importance sampling (DAIS), proposed by Geffner & Domke (2021) and Zhang et al. (2021), allows optimizing, among others, over the initial distribution of AIS. In this paper, we show that, in the limit of many transitions, DAIS minimizes the symmetrized KL divergence (Jensen-Shannon divergence) between the initial and target distribution. Thus, DAIS can be seen as a form of variational inference (VI) in that its initial distribution is a parametric fit to an intractable target distribution. We empirically evaluate the usefulness of the initial distribution as a variational distribution on synthetic and real-world data, observing that it often provides more accurate uncertainty estimates than standard VI (optimizing the reverse KL divergence), importance weighted VI, and Markovian score climbing (optimizing the forward KL divergence).","sentences":["Differentiable annealed importance sampling (DAIS), proposed by Geffner & Domke (2021) and Zhang et al. (2021), allows optimizing, among others, over the initial distribution of AIS.","In this paper, we show that, in the limit of many transitions, DAIS minimizes the symmetrized KL divergence (Jensen-Shannon divergence) between the initial and target distribution.","Thus, DAIS can be seen as a form of variational inference (VI) in that its initial distribution is a parametric fit to an intractable target distribution.","We empirically evaluate the usefulness of the initial distribution as a variational distribution on synthetic and real-world data, observing that it often provides more accurate uncertainty estimates than standard VI (optimizing the reverse KL divergence), importance weighted VI, and Markovian score climbing (optimizing the forward KL divergence)."],"url":"http://arxiv.org/abs/2405.14840v1","category":"stat.ML"}
{"created":"2024-05-23 17:50:38","title":"First Order Logic of Sparse Graphs with Given Degree Sequences","abstract":"We consider limit probabilities of first order properties in random graphs with a given degree sequence. Under mild conditions on the degree sequence, we show that the closure set of limit probabilities is a finite union of closed intervals. Moreover, we characterize the degree sequences for which this closure set is the interval $[0,1]$, a property that is intimately related with the probability that the random graph is acyclic. As a side result, we compile a full description of the cycle distribution of random graphs and study their fragment (disjoint union of unicyclic components) in the subcritical regime. Finally, we amend the proof of the existence of limit probabilities for first order properties in random graphs with a given degree sequence; this result was already claimed by Lynch~[IEEE LICS 2003] but his proof contained some inaccuracies.","sentences":["We consider limit probabilities of first order properties in random graphs with a given degree sequence.","Under mild conditions on the degree sequence, we show that the closure set of limit probabilities is a finite union of closed intervals.","Moreover, we characterize the degree sequences for which this closure set is the interval $[0,1]$, a property that is intimately related with the probability that the random graph is acyclic.","As a side result, we compile a full description of the cycle distribution of random graphs and study their fragment (disjoint union of unicyclic components) in the subcritical regime.","Finally, we amend the proof of the existence of limit probabilities for first order properties in random graphs with a given degree sequence; this result was already claimed by Lynch~[IEEE LICS 2003] but his proof contained some inaccuracies."],"url":"http://arxiv.org/abs/2405.14836v1","category":"math.CO"}
{"created":"2024-05-23 17:43:15","title":"New limits on neutrino decay from high-energy astrophysical neutrinos","abstract":"Since neutrinos have mass differences, they could decay into one another. But their lifetimes are likely long, even when shortened by new physics, so decay likely impacts neutrinos only during long trips. This makes high-energy astrophysical neutrinos, traveling for up to billions of light-years, sensitive probes of decay. However, their sensitivity must be tempered by reality. We derive from them thorough bounds on the neutrino lifetimes accounting for critical astrophysical unknowns and the nuances of neutrino detection. Using the diffuse neutrino flux, we disfavor lifetimes $\\tau \\lesssim 20$-450 s $(m/{\\rm eV})$, based on present IceCube data, and forecast factor-of-10 improvements by upcoming detectors. Using, for the first time, neutrinos from the active galaxy NGC 1068, extant unknowns preclude placing lifetime bounds today, but upcoming detectors could disfavor $\\tau \\sim 100$-5000 s $(m/{\\rm eV})$.","sentences":["Since neutrinos have mass differences, they could decay into one another.","But their lifetimes are likely long, even when shortened by new physics, so decay likely impacts neutrinos only during long trips.","This makes high-energy astrophysical neutrinos, traveling for up to billions of light-years, sensitive probes of decay.","However, their sensitivity must be tempered by reality.","We derive from them thorough bounds on the neutrino lifetimes accounting for critical astrophysical unknowns and the nuances of neutrino detection.","Using the diffuse neutrino flux, we disfavor lifetimes $\\tau \\lesssim 20$-450 s $(m/{\\rm eV})$, based on present IceCube data, and forecast factor-of-10 improvements by upcoming detectors.","Using, for the first time, neutrinos from the active galaxy NGC 1068, extant unknowns preclude placing lifetime bounds today, but upcoming detectors could disfavor $\\tau \\sim 100$-5000 s $(m/{\\rm eV})$."],"url":"http://arxiv.org/abs/2405.14826v1","category":"astro-ph.HE"}
{"created":"2024-05-23 17:41:15","title":"Camera Relocalization in Shadow-free Neural Radiance Fields","abstract":"Camera relocalization is a crucial problem in computer vision and robotics. Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images. Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process. In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization. We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process. To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process. Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions. Code and data will be made publicly available.","sentences":["Camera relocalization is a crucial problem in computer vision and robotics.","Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images.","Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process.","In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization.","We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process.","To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process.","Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions.","Code and data will be made publicly available."],"url":"http://arxiv.org/abs/2405.14824v1","category":"cs.CV"}
{"created":"2024-05-23 17:22:02","title":"As an AI Language Model, \"Yes I Would Recommend Calling the Police'': Norm Inconsistency in LLM Decision-Making","abstract":"We investigate the phenomenon of norm inconsistency: where LLMs apply different norms in similar situations. Specifically, we focus on the high-risk application of deciding whether to call the police in Amazon Ring home surveillance videos. We evaluate the decisions of three state-of-the-art LLMs -- GPT-4, Gemini 1.0, and Claude 3 Sonnet -- in relation to the activities portrayed in the videos, the subjects' skin-tone and gender, and the characteristics of the neighborhoods where the videos were recorded. Our analysis reveals significant norm inconsistencies: (1) a discordance between the recommendation to call the police and the actual presence of criminal activity, and (2) biases influenced by the racial demographics of the neighborhoods. These results highlight the arbitrariness of model decisions in the surveillance context and the limitations of current bias detection and mitigation strategies in normative decision-making.","sentences":["We investigate the phenomenon of norm inconsistency: where LLMs apply different norms in similar situations.","Specifically, we focus on the high-risk application of deciding whether to call the police in Amazon Ring home surveillance videos.","We evaluate the decisions of three state-of-the-art LLMs -- GPT-4, Gemini 1.0, and Claude 3 Sonnet -- in relation to the activities portrayed in the videos, the subjects' skin-tone and gender, and the characteristics of the neighborhoods where the videos were recorded.","Our analysis reveals significant norm inconsistencies: (1) a discordance between the recommendation to call the police and the actual presence of criminal activity, and (2) biases influenced by the racial demographics of the neighborhoods.","These results highlight the arbitrariness of model decisions in the surveillance context and the limitations of current bias detection and mitigation strategies in normative decision-making."],"url":"http://arxiv.org/abs/2405.14812v1","category":"cs.CY"}
{"created":"2024-05-23 17:17:57","title":"Quantitative kinetic rules for plastic strain-induced $\u03b1$-$\u03c9$ phase transformation in Zr under high pressure","abstract":"Plastic strain-induced phase transformations (PTs) and chemical reactions under high pressure are broadly spread in modern technologies, friction and wear, geophysics, and astrogeology. However, because of very heterogeneous fields of plastic strain $\\mathbf{E}^{p}$ and stress $\\mathbf{\\sigma}$ tensors and volume fraction $c$ of phases in a sample compressed in a diamond anvil cell (DAC) and impossibility of measurements of $\\mathbf{\\sigma}$ and $\\mathbf{E}^{p}$, there are no strict kinetic equations for them. Here, we develop combined experimental-computational approaches to determine all fields in strongly plastically predeformed Zr and kinetic equation for $\\alpha$-$\\omega$ PT consistent with experimental data for the entire sample. Kinetic equation depends on accumulated plastic strain (instead of time) and pressure and is independent of plastic strain and deviatoric stress tensors, i.e., it can be applied for various above processes. Our results initiate kinetic studies of strain-induced PTs and provide efforts toward more comprehensive understanding of material behavior in extreme conditions.","sentences":["Plastic strain-induced phase transformations (PTs) and chemical reactions under high pressure are broadly spread in modern technologies, friction and wear, geophysics, and astrogeology.","However, because of very heterogeneous fields of plastic strain $\\mathbf{E}^{p}$ and stress $\\mathbf{\\sigma}$ tensors and volume fraction $c$ of phases in a sample compressed in a diamond anvil cell (DAC) and impossibility of measurements of $\\mathbf{\\sigma}$ and $\\mathbf{E}^{p}$, there are no strict kinetic equations for them.","Here, we develop combined experimental-computational approaches to determine all fields in strongly plastically predeformed Zr and kinetic equation for $\\alpha$-$\\omega$ PT consistent with experimental data for the entire sample.","Kinetic equation depends on accumulated plastic strain (instead of time) and pressure and is independent of plastic strain and deviatoric stress tensors, i.e., it can be applied for various above processes.","Our results initiate kinetic studies of strain-induced PTs and provide efforts toward more comprehensive understanding of material behavior in extreme conditions."],"url":"http://arxiv.org/abs/2405.14807v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 16:52:16","title":"Unconventional superconductivity in twisted bilayer WSe2","abstract":"Moir\\'e materials have enabled the realization of flat electron bands and quantum phases that are driven by strong correlations associated with flat bands. Superconductivity has been observed, but solely, in graphene moir\\'e materials. The absence of robust superconductivity in moir\\'e materials beyond graphene, such as semiconductor moir\\'e materials, has remained a mystery and challenged our current understanding of superconductivity in flat bands. Here, we report the observation of robust superconductivity in 3.65-degree twisted bilayer WSe2 which hosts a honeycomb moir\\'e lattice. Superconductivity emerges at half-band filling and under small sublattice potential differences, where the moir\\'e band is a flat Chern band. The optimal superconducting transition temperature is about 220 mK and constitutes 2% of the effective Fermi temperature; the latter is comparable to the value in high-temperature cuprate superconductors and suggests strong pairing. The superconductor borders on two distinct metals below and above half-band filling; it undergoes a continuous transition to a correlated insulator by tuning the sublattice potential difference. The observed superconductivity on the verge of Coulomb-induced charge localization suggests roots in strong electron correlations.","sentences":["Moir\\'e materials have enabled the realization of flat electron bands and quantum phases that are driven by strong correlations associated with flat bands.","Superconductivity has been observed, but solely, in graphene moir\\'e materials.","The absence of robust superconductivity in moir\\'e materials beyond graphene, such as semiconductor moir\\'e materials, has remained a mystery and challenged our current understanding of superconductivity in flat bands.","Here, we report the observation of robust superconductivity in 3.65-degree twisted bilayer WSe2 which hosts a honeycomb moir\\'e lattice.","Superconductivity emerges at half-band filling and under small sublattice potential differences, where the moir\\'e band is a flat Chern band.","The optimal superconducting transition temperature is about 220 mK and constitutes 2% of the effective Fermi temperature; the latter is comparable to the value in high-temperature cuprate superconductors and suggests strong pairing.","The superconductor borders on two distinct metals below and above half-band filling; it undergoes a continuous transition to a correlated insulator by tuning the sublattice potential difference.","The observed superconductivity on the verge of Coulomb-induced charge localization suggests roots in strong electron correlations."],"url":"http://arxiv.org/abs/2405.14784v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 16:42:34","title":"Fully charmed tetraquarks from LHC to FCC: Natural stability from fragmentation","abstract":"We investigate the inclusive production of fully charmed tetraquarks, $T_{4c}(0^{++})$ or $T_{4c}(2^{++})$ radial excitations, in high-energy proton collisions. We build our study upon the collinear fragmentation of a single parton in a variable-flavor number scheme, suited to describe the tetraquark formation mechanism from moderate to large transverse-momentum regimes. To this extent, we derive a novel set of DGLAP-evolving collinear fragmentation functions, named TQ4Q1.0 determinations. They encode initial-scale inputs corresponding to both gluon and heavy-quark fragmentation channels, defined within the context of quark-potential and spin-physics inspired models, respectively. We work within the NLL/NLO$^+$ hybrid factorization and make use of the JETHAD numeric interface along with the symJETHAD symbolic calculation plugin. With these tools, we provide predictions for high-energy observables sensitive to $T_{4c}$ plus jet emissions at center-of-mass energies ranging from 14 TeV at the LHC to the 100 TeV nominal energy of the FCC.","sentences":["We investigate the inclusive production of fully charmed tetraquarks, $T_{4c}(0^{++})$ or $T_{4c}(2^{++})$ radial excitations, in high-energy proton collisions.","We build our study upon the collinear fragmentation of a single parton in a variable-flavor number scheme, suited to describe the tetraquark formation mechanism from moderate to large transverse-momentum regimes.","To this extent, we derive a novel set of DGLAP-evolving collinear fragmentation functions, named TQ4Q1.0 determinations.","They encode initial-scale inputs corresponding to both gluon and heavy-quark fragmentation channels, defined within the context of quark-potential and spin-physics inspired models, respectively.","We work within the NLL/NLO$^+$ hybrid factorization and make use of the JETHAD numeric interface along with the symJETHAD symbolic calculation plugin.","With these tools, we provide predictions for high-energy observables sensitive to $T_{4c}$ plus jet emissions at center-of-mass energies ranging from 14 TeV at the LHC to the 100 TeV nominal energy of the FCC."],"url":"http://arxiv.org/abs/2405.14773v1","category":"hep-ph"}
{"created":"2024-05-23 16:35:20","title":"FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models","abstract":"As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making.   In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM. Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly. Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis. We open-source FinRobot at \\url{https://github.com/AI4Finance-Foundation/FinRobot}.","sentences":["As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community.","These challenges impede the AI community's ability to enhance financial tasks effectively.","Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making.   ","In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM.","Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly.","Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis.","We open-source FinRobot at \\url{https://github.com/AI4Finance-Foundation/FinRobot}."],"url":"http://arxiv.org/abs/2405.14767v1","category":"q-fin.ST"}
{"created":"2024-05-23 16:14:33","title":"Scalable embedding of parity constraints in quantum annealing hardware","abstract":"One of the main bottlenecks in solving combinatorial optimization problems with quantum annealers is the qubit connectivity in the hardware. A possible solution for larger connectivty is minor embedding. This techniques makes the geometrical properties of the combinatorial optimization problem, encoded as a Hamiltonian, match the properties of the quantum annealing hardware. The embedding itself is a hard computational problem and therefore heuristic algorithms are required. In this work, we present fixed, modular and scalable embeddings that can be used to embed any combinatorial optimization problem described as an Ising Hamiltonian. These embeddings are the result of an extension of the well-known parity mapping, which has been used in the past to map higher-order Ising Hamiltonians to quadratic Hamiltonians, which are suitable for existing quantum hardware. We show how our new embeddings can be mapped to existing quantum annealers and that the embedded Hamiltonian physical properties match the original Hamiltonian properties.","sentences":["One of the main bottlenecks in solving combinatorial optimization problems with quantum annealers is the qubit connectivity in the hardware.","A possible solution for larger connectivty is minor embedding.","This techniques makes the geometrical properties of the combinatorial optimization problem, encoded as a Hamiltonian, match the properties of the quantum annealing hardware.","The embedding itself is a hard computational problem and therefore heuristic algorithms are required.","In this work, we present fixed, modular and scalable embeddings that can be used to embed any combinatorial optimization problem described as an Ising Hamiltonian.","These embeddings are the result of an extension of the well-known parity mapping, which has been used in the past to map higher-order Ising Hamiltonians to quadratic Hamiltonians, which are suitable for existing quantum hardware.","We show how our new embeddings can be mapped to existing quantum annealers and that the embedded Hamiltonian physical properties match the original Hamiltonian properties."],"url":"http://arxiv.org/abs/2405.14746v1","category":"quant-ph"}
{"created":"2024-05-23 15:54:03","title":"A Systematic and Formal Study of the Impact of Local Differential Privacy on Fairness: Preliminary Results","abstract":"Machine learning (ML) algorithms rely primarily on the availability of training data, and, depending on the domain, these data may include sensitive information about the data providers, thus leading to significant privacy issues. Differential privacy (DP) is the predominant solution for privacy-preserving ML, and the local model of DP is the preferred choice when the server or the data collector are not trusted. Recent experimental studies have shown that local DP can impact ML prediction for different subgroups of individuals, thus affecting fair decision-making. However, the results are conflicting in the sense that some studies show a positive impact of privacy on fairness while others show a negative one. In this work, we conduct a systematic and formal study of the effect of local DP on fairness. Specifically, we perform a quantitative study of how the fairness of the decisions made by the ML model changes under local DP for different levels of privacy and data distributions. In particular, we provide bounds in terms of the joint distributions and the privacy level, delimiting the extent to which local DP can impact the fairness of the model. We characterize the cases in which privacy reduces discrimination and those with the opposite effect. We validate our theoretical findings on synthetic and real-world datasets. Our results are preliminary in the sense that, for now, we study only the case of one sensitive attribute, and only statistical disparity, conditional statistical disparity, and equal opportunity difference.","sentences":["Machine learning (ML) algorithms rely primarily on the availability of training data, and, depending on the domain, these data may include sensitive information about the data providers, thus leading to significant privacy issues.","Differential privacy (DP) is the predominant solution for privacy-preserving ML, and the local model of DP is the preferred choice when the server or the data collector are not trusted.","Recent experimental studies have shown that local DP can impact ML prediction for different subgroups of individuals, thus affecting fair decision-making.","However, the results are conflicting in the sense that some studies show a positive impact of privacy on fairness while others show a negative one.","In this work, we conduct a systematic and formal study of the effect of local DP on fairness.","Specifically, we perform a quantitative study of how the fairness of the decisions made by the ML model changes under local DP for different levels of privacy and data distributions.","In particular, we provide bounds in terms of the joint distributions and the privacy level, delimiting the extent to which local DP can impact the fairness of the model.","We characterize the cases in which privacy reduces discrimination and those with the opposite effect.","We validate our theoretical findings on synthetic and real-world datasets.","Our results are preliminary in the sense that, for now, we study only the case of one sensitive attribute, and only statistical disparity, conditional statistical disparity, and equal opportunity difference."],"url":"http://arxiv.org/abs/2405.14725v1","category":"cs.LG"}
{"created":"2024-05-23 15:07:32","title":"Multiplicity fluctuations and rapidity correlations in ultracentral proton-nucleus collisions","abstract":"A collision between a proton and a heavy nucleus at ultrarelativistic energy creates particles whose rapidity distribution is asymmetric, with more particles emitted in the direction of the nucleus than in the direction of the proton. This asymmetry becomes more pronounced as the centrality estimator, defined from the energy deposited in a calorimeter, increases. We argue that for high-multiplicity collisions, the variation of the impact parameter plays a negligible role, and that the fluctuations of the multiplicity and of the centrality estimator are dominated by quantum fluctuations, whose probability distribution can be well approximated by a correlated gamma distribution. We show that this simple model reproduces existing data, and we make quantitative predictions for collisions in the $0-0.1\\%$ and $0-0.01\\%$ centrality windows. We argue that by repeating the same analysis with a different centrality estimator, one can obtain direct information about the rapidity decorrelation in particle production.","sentences":["A collision between a proton and a heavy nucleus at ultrarelativistic energy creates particles whose rapidity distribution is asymmetric, with more particles emitted in the direction of the nucleus than in the direction of the proton.","This asymmetry becomes more pronounced as the centrality estimator, defined from the energy deposited in a calorimeter, increases.","We argue that for high-multiplicity collisions, the variation of the impact parameter plays a negligible role, and that the fluctuations of the multiplicity and of the centrality estimator are dominated by quantum fluctuations, whose probability distribution can be well approximated by a correlated gamma distribution.","We show that this simple model reproduces existing data, and we make quantitative predictions for collisions in the $0-0.1\\%$ and $0-0.01\\%$ centrality windows.","We argue that by repeating the same analysis with a different centrality estimator, one can obtain direct information about the rapidity decorrelation in particle production."],"url":"http://arxiv.org/abs/2405.14671v1","category":"nucl-th"}
{"created":"2024-05-23 14:55:18","title":"Heteroscedastic Preferential Bayesian Optimization with Informative Noise Distributions","abstract":"Preferential Bayesian optimization (PBO) is a sample-efficient framework for learning human preferences between candidate designs. PBO classically relies on homoscedastic noise models to represent human aleatoric uncertainty. Yet, such noise fails to accurately capture the varying levels of human aleatoric uncertainty, particularly when the user possesses partial knowledge among different pairs of candidates. For instance, a chemist with solid expertise in glucose-related molecules may easily compare two compounds from that family while struggling to compare alcohol-related molecules. Currently, PBO overlooks this uncertainty during the search for a new candidate through the maximization of the acquisition function, consequently underestimating the risk associated with human uncertainty. To address this issue, we propose a heteroscedastic noise model to capture human aleatoric uncertainty. This model adaptively assigns noise levels based on the distance of a specific input to a predefined set of reliable inputs known as anchors provided by the human. Anchors encapsulate partial knowledge and offer insight into the comparative difficulty of evaluating different candidate pairs. Such a model can be seamlessly integrated into the acquisition function, thus leading to candidate design pairs that elegantly trade informativeness and ease of comparison for the human expert. We perform an extensive empirical evaluation of the proposed approach, demonstrating a consistent improvement over homoscedastic PBO.","sentences":["Preferential Bayesian optimization (PBO) is a sample-efficient framework for learning human preferences between candidate designs.","PBO classically relies on homoscedastic noise models to represent human aleatoric uncertainty.","Yet, such noise fails to accurately capture the varying levels of human aleatoric uncertainty, particularly when the user possesses partial knowledge among different pairs of candidates.","For instance, a chemist with solid expertise in glucose-related molecules may easily compare two compounds from that family while struggling to compare alcohol-related molecules.","Currently, PBO overlooks this uncertainty during the search for a new candidate through the maximization of the acquisition function, consequently underestimating the risk associated with human uncertainty.","To address this issue, we propose a heteroscedastic noise model to capture human aleatoric uncertainty.","This model adaptively assigns noise levels based on the distance of a specific input to a predefined set of reliable inputs known as anchors provided by the human.","Anchors encapsulate partial knowledge and offer insight into the comparative difficulty of evaluating different candidate pairs.","Such a model can be seamlessly integrated into the acquisition function, thus leading to candidate design pairs that elegantly trade informativeness and ease of comparison for the human expert.","We perform an extensive empirical evaluation of the proposed approach, demonstrating a consistent improvement over homoscedastic PBO."],"url":"http://arxiv.org/abs/2405.14657v1","category":"cs.LG"}
{"created":"2024-05-23 14:52:53","title":"Statistical inference for high-dimensional convoluted rank regression","abstract":"High-dimensional penalized rank regression is a powerful tool for modeling high-dimensional data due to its robustness and estimation efficiency. However, the non-smoothness of the rank loss brings great challenges to the computation. To solve this critical issue, high-dimensional convoluted rank regression is recently proposed, and penalized convoluted rank regression estimators are introduced. However, these developed estimators cannot be directly used to make inference. In this paper, we investigate the inference problem of high-dimensional convoluted rank regression. We first establish estimation error bounds of penalized convoluted rank regression estimators under weaker conditions on the predictors. Based on the penalized convoluted rank regression estimators, we further introduce a debiased estimator. We then provide Bahadur representation for our proposed estimator. We further develop simultaneous inference procedures. A novel bootstrap procedure is proposed and its theoretical validity is also established. Finally, simulation and real data analysis are conducted to illustrate the merits of our proposed methods.","sentences":["High-dimensional penalized rank regression is a powerful tool for modeling high-dimensional data due to its robustness and estimation efficiency.","However, the non-smoothness of the rank loss brings great challenges to the computation.","To solve this critical issue, high-dimensional convoluted rank regression is recently proposed, and penalized convoluted rank regression estimators are introduced.","However, these developed estimators cannot be directly used to make inference.","In this paper, we investigate the inference problem of high-dimensional convoluted rank regression.","We first establish estimation error bounds of penalized convoluted rank regression estimators under weaker conditions on the predictors.","Based on the penalized convoluted rank regression estimators, we further introduce a debiased estimator.","We then provide Bahadur representation for our proposed estimator.","We further develop simultaneous inference procedures.","A novel bootstrap procedure is proposed and its theoretical validity is also established.","Finally, simulation and real data analysis are conducted to illustrate the merits of our proposed methods."],"url":"http://arxiv.org/abs/2405.14652v1","category":"stat.ME"}
{"created":"2024-05-23 14:40:17","title":"Defective Parking Functions and Young Tableaux","abstract":"Recall that a defective $(m,n)$-parking function with defect $d$ is a parking function with $m$ cars attempting to park on a street with $n$ parking spots in which exactly $d$ cars fail to park. We establish a way to compute the defect of a defective $(m,n)$-parking function and show that the defect of a parking function is invariant under the action of $\\mathfrak{S}_m$ the symmetric group on $[m]=\\{1,2,\\ldots,m\\}$. We also show that the set of nondecreasing defective $(m,n)$-parking functions with defect $d$ are in bijection with the set of standard Young tableaux of shape $(n + d, m - d)$. This implies that the number of $\\mathfrak{S}_m$-orbits of defective $(m,n)$-parking functions with defect $d$ is given by $\\frac{n-m+2d+1}{n+d+1}\\binom{m+n}{n+d}$. We also give a multinomial formula for the size of an $\\mathfrak{S}_m$-orbit of a nondecreasing $(m,n)$-parking function with defect $d$. We conclude by using these results to give a new formula for the number of defective parking functions.","sentences":["Recall that a defective $(m,n)$-parking function with defect $d$ is a parking function with $m$ cars attempting to park on a street with $n$ parking spots in which exactly $d$ cars fail to park.","We establish a way to compute the defect of a defective $(m,n)$-parking function and show that the defect of a parking function is invariant under the action of $\\mathfrak{S}_m$ the symmetric group on $[m]=\\{1,2,\\ldots,m\\}$. We also show that the set of nondecreasing defective $(m,n)$-parking functions with defect $d$ are in bijection with the set of standard Young tableaux of shape $(n + d, m - d)$.","This implies that the number of $\\mathfrak{S}_m$-orbits of defective $(m,n)$-parking functions with defect $d$ is given by $\\frac{n-m+2d+1}{n+d+1}\\binom{m+n}{n+d}$. We also give a multinomial formula for the size of an $\\mathfrak{S}_m$-orbit of a nondecreasing $(m,n)$-parking function with defect $d$. We conclude by using these results to give a new formula for the number of defective parking functions."],"url":"http://arxiv.org/abs/2405.14635v1","category":"math.CO"}
{"created":"2024-05-23 14:34:54","title":"Online robust estimation and bootstrap inference for function-on-scalar regression","abstract":"We propose a novel and robust online function-on-scalar regression technique via geometric median to learn associations between functional responses and scalar covariates based on massive or streaming datasets. The online estimation procedure, developed using the average stochastic gradient descent algorithm, offers an efficient and cost-effective method for analyzing sequentially augmented datasets, eliminating the need to store large volumes of data in memory. We establish the almost sure consistency, $L_p$ convergence, and asymptotic normality of the online estimator. To enable efficient and fast inference of the parameters of interest, including the derivation of confidence intervals, we also develop an innovative two-step online bootstrap procedure to approximate the limiting error distribution of the robust online estimator. Numerical studies under a variety of scenarios demonstrate the effectiveness and efficiency of the proposed online learning method. A real application analyzing PM$_{2.5}$ air-quality data is also included to exemplify the proposed online approach.","sentences":["We propose a novel and robust online function-on-scalar regression technique via geometric median to learn associations between functional responses and scalar covariates based on massive or streaming datasets.","The online estimation procedure, developed using the average stochastic gradient descent algorithm, offers an efficient and cost-effective method for analyzing sequentially augmented datasets, eliminating the need to store large volumes of data in memory.","We establish the almost sure consistency, $L_p$ convergence, and asymptotic normality of the online estimator.","To enable efficient and fast inference of the parameters of interest, including the derivation of confidence intervals, we also develop an innovative two-step online bootstrap procedure to approximate the limiting error distribution of the robust online estimator.","Numerical studies under a variety of scenarios demonstrate the effectiveness and efficiency of the proposed online learning method.","A real application analyzing PM$_{2.5}$ air-quality data is also included to exemplify the proposed online approach."],"url":"http://arxiv.org/abs/2405.14628v1","category":"stat.ME"}
{"created":"2024-05-23 14:17:01","title":"Controllable Continual Test-Time Adaptation","abstract":"Continual Test-Time Adaptation (CTTA) is an emerging and challenging task where a model trained in a source domain must adapt to continuously changing conditions during testing, without access to the original source data. CTTA is prone to error accumulation due to uncontrollable domain shifts, leading to blurred decision boundaries between categories. Existing CTTA methods primarily focus on suppressing domain shifts, which proves inadequate during the unsupervised test phase. In contrast, we introduce a novel approach that guides rather than suppresses these shifts. Specifically, we propose $\\textbf{C}$ontrollable $\\textbf{Co}$ntinual $\\textbf{T}$est-$\\textbf{T}$ime $\\textbf{A}$daptation (C-CoTTA), which explicitly prevents any single category from encroaching on others, thereby mitigating the mutual influence between categories caused by uncontrollable shifts. Moreover, our method reduces the sensitivity of model to domain transformations, thereby minimizing the magnitude of category shifts. Extensive quantitative experiments demonstrate the effectiveness of our method, while qualitative analyses, such as t-SNE plots, confirm the theoretical validity of our approach.","sentences":["Continual Test-Time Adaptation (CTTA) is an emerging and challenging task where a model trained in a source domain must adapt to continuously changing conditions during testing, without access to the original source data.","CTTA is prone to error accumulation due to uncontrollable domain shifts, leading to blurred decision boundaries between categories.","Existing CTTA methods primarily focus on suppressing domain shifts, which proves inadequate during the unsupervised test phase.","In contrast, we introduce a novel approach that guides rather than suppresses these shifts.","Specifically, we propose $\\textbf{C}$ontrollable $\\textbf{Co}$ntinual $\\textbf{T}$est-$\\textbf{T}$ime $\\textbf{A}$daptation (C-CoTTA), which explicitly prevents any single category from encroaching on others, thereby mitigating the mutual influence between categories caused by uncontrollable shifts.","Moreover, our method reduces the sensitivity of model to domain transformations, thereby minimizing the magnitude of category shifts.","Extensive quantitative experiments demonstrate the effectiveness of our method, while qualitative analyses, such as t-SNE plots, confirm the theoretical validity of our approach."],"url":"http://arxiv.org/abs/2405.14602v1","category":"cs.LG"}
{"created":"2024-05-23 14:11:26","title":"Linear Mode Connectivity in Differentiable Tree Ensembles","abstract":"Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for validating the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.","sentences":["Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space.","For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for validating the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging.","While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question.","In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice.","We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees.","Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition.","Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC."],"url":"http://arxiv.org/abs/2405.14596v1","category":"cs.LG"}
{"created":"2024-05-23 13:57:31","title":"Testing the CKM unitarity at high energy via the $W^+W^-$ production at the LHC and future colliders","abstract":"We propose a novel test to assess the unitarity of the Cabibbo-Kobayashi-Maskawa matrix, $V_{\\scriptscriptstyle{\\rm CKM}}$, at present and future collider experiments. Our strategy makes use of the $W^+W^-$ production cross section to directly probe the $V_{\\scriptscriptstyle{\\rm CKM}}^{\\dagger} V_{\\scriptscriptstyle{\\rm CKM}}$ product, which regulates the high-energy behavior of the observable. The violation of unitarity is signalled by an anomalous behavior of the cross section that grows quadratically with the $W^+W^-$ invariant mass with respect to the Standard Model prediction. By using the recent ATLAS measurements of the $W^+W^-$ cross section we are able to constrain the maximal unitarity violation allowed by current data, producing a bound complementary to the results of flavor physics experiments. Forecasts for the high luminosity phase of the LHC and for the future 100 TeV hadron collider are also discussed.","sentences":["We propose a novel test to assess the unitarity of the Cabibbo-Kobayashi-Maskawa matrix, $V_{\\scriptscriptstyle{\\rm CKM}}$, at present and future collider experiments.","Our strategy makes use of the $W^+W^-$ production cross section to directly probe the $V_{\\scriptscriptstyle{\\rm CKM}}^{\\dagger} V_{\\scriptscriptstyle{\\rm CKM}}$ product, which regulates the high-energy behavior of the observable.","The violation of unitarity is signalled by an anomalous behavior of the cross section that grows quadratically with the $W^+W^-$ invariant mass with respect to the Standard Model prediction.","By using the recent ATLAS measurements of the $W^+W^-$ cross section we are able to constrain the maximal unitarity violation allowed by current data, producing a bound complementary to the results of flavor physics experiments.","Forecasts for the high luminosity phase of the LHC and for the future 100 TeV hadron collider are also discussed."],"url":"http://arxiv.org/abs/2405.14585v1","category":"hep-ph"}
{"created":"2024-05-23 13:53:05","title":"Assessment of the Role and Origin of S* in Orange Carotenoid Protein Photoconversion","abstract":"The orange carotenoid protein (OCP) is the water-soluble mediator of non-photochemical quenching in cyanobacteria, a crucial photoprotective mechanism in response to excess illumination. OCP converts from a globular, inactive state (OCPo) to an extended, active conformation (OCPr) under high-light conditions, resulting in a concomitant redshift in the absorption of the bound carotenoid. Here, OCP was trapped in either the active or inactive state by fixing each protein conformation in trehalose-sucrose glass. Glass-encapsulated OCPo did not convert under intense illumination and OCPr did not convert in darkness, allowing the optical properties of each conformation to be determined at room temperature. We measured pump wavelength-dependent transient absorption of OCPo in glass films and found that initial OCP photoproducts are still formed, despite the glass preventing completion of the photocycle. By comparison to the pump wavelength dependence of the OCPo to OCPr photoconversion yield in buffer, we show that the long-lived carotenoid singlet-like feature (S*) is associated with ground-state heterogeneity within OCPo, rather than triggering OCP photoconversion.","sentences":["The orange carotenoid protein (OCP) is the water-soluble mediator of non-photochemical quenching in cyanobacteria, a crucial photoprotective mechanism in response to excess illumination.","OCP converts from a globular, inactive state (OCPo) to an extended, active conformation (OCPr) under high-light conditions, resulting in a concomitant redshift in the absorption of the bound carotenoid.","Here, OCP was trapped in either the active or inactive state by fixing each protein conformation in trehalose-sucrose glass.","Glass-encapsulated OCPo did not convert under intense illumination and OCPr did not convert in darkness, allowing the optical properties of each conformation to be determined at room temperature.","We measured pump wavelength-dependent transient absorption of OCPo in glass films and found that initial OCP photoproducts are still formed, despite the glass preventing completion of the photocycle.","By comparison to the pump wavelength dependence of the OCPo to OCPr photoconversion yield in buffer, we show that the long-lived carotenoid singlet-like feature (S*) is associated with ground-state heterogeneity within OCPo, rather than triggering OCP photoconversion."],"url":"http://arxiv.org/abs/2405.14579v1","category":"physics.chem-ph"}
{"created":"2024-05-23 13:42:55","title":"$L^1$-Contraction Property of Entropy Solutions for Scalar Conservation Laws with Minimal Regularity Assumptions on the Flux","abstract":"This paper is concerned with entropy solutions of scalar conservation laws of the form $\\partial_{t}u+\\diver f=0$ in $\\mathbb{R}^d\\times(0,\\infty)$. The flux $f=f(x,u)$ depends explicitly on the spatial variable $x$. Using an extension of Kruzkov's method, we establish the $L^1$-contraction property of entropy solutions under minimal regularity assumptions on the flux.","sentences":["This paper is concerned with entropy solutions of scalar conservation laws of the form $\\partial_{t}u+\\diver f=0$ in $\\mathbb{R}^d\\times(0,\\infty)$. The flux $f=f(x,u)$ depends explicitly on the spatial variable $x$.","Using an extension of Kruzkov's method, we establish the $L^1$-contraction property of entropy solutions under minimal regularity assumptions on the flux."],"url":"http://arxiv.org/abs/2405.14565v1","category":"math.AP"}
{"created":"2024-05-23 13:29:47","title":"Non-Euclidean conformal devices with continuously varying refractive index profiles based on bi-spheres","abstract":"Either conformal transformation optics or geodesic mapping provides a design method to bend light rays in two-dimensional space with a nonuniform refractive index profile. In this paper, we combine both methods above to design a conformal invisible cloak based on bi-spheres with a refractive index profile varying from 0 to 10.7, smaller than 24.6 for the previous case of a single sphere. Moreover, we obtain an omnidirectional retro-reflector and a specular reflector by making position adjustments to mirrors, and achieve similar invisible effect by tuning sizes of the bi-spheres. Our work expands the toolkits for designing conformal devices with continuously-varying index profile.","sentences":["Either conformal transformation optics or geodesic mapping provides a design method to bend light rays in two-dimensional space with a nonuniform refractive index profile.","In this paper, we combine both methods above to design a conformal invisible cloak based on bi-spheres with a refractive index profile varying from 0 to 10.7, smaller than 24.6 for the previous case of a single sphere.","Moreover, we obtain an omnidirectional retro-reflector and a specular reflector by making position adjustments to mirrors, and achieve similar invisible effect by tuning sizes of the bi-spheres.","Our work expands the toolkits for designing conformal devices with continuously-varying index profile."],"url":"http://arxiv.org/abs/2405.14550v1","category":"physics.optics"}
{"created":"2024-05-23 13:24:38","title":"Nuclear Norm Regularization for Deep Learning","abstract":"Penalizing the nuclear norm of a function's Jacobian encourages it to locally behave like a low-rank linear map. Such functions vary locally along only a handful of directions, making the Jacobian nuclear norm a natural regularizer for machine learning problems. However, this regularizer is intractable for high-dimensional problems, as it requires computing a large Jacobian matrix and taking its singular value decomposition. We show how to efficiently penalize the Jacobian nuclear norm using techniques tailor-made for deep learning. We prove that for functions parametrized as compositions $f = g \\circ h$, one may equivalently penalize the average squared Frobenius norm of $Jg$ and $Jh$. We then propose a denoising-style approximation that avoids the Jacobian computations altogether. Our method is simple, efficient, and accurate, enabling Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems. We complement our theory with an empirical study of our regularizer's performance and investigate applications to denoising and representation learning.","sentences":["Penalizing the nuclear norm of a function's Jacobian encourages it to locally behave like a low-rank linear map.","Such functions vary locally along only a handful of directions, making the Jacobian nuclear norm a natural regularizer for machine learning problems.","However, this regularizer is intractable for high-dimensional problems, as it requires computing a large Jacobian matrix and taking its singular value decomposition.","We show how to efficiently penalize the Jacobian nuclear norm using techniques tailor-made for deep learning.","We prove that for functions parametrized as compositions $f = g \\circ h$, one may equivalently penalize the average squared Frobenius norm of $Jg$ and $Jh$. We then propose a denoising-style approximation that avoids the Jacobian computations altogether.","Our method is simple, efficient, and accurate, enabling Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems.","We complement our theory with an empirical study of our regularizer's performance and investigate applications to denoising and representation learning."],"url":"http://arxiv.org/abs/2405.14544v1","category":"cs.LG"}
{"created":"2024-05-23 13:15:13","title":"AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2","abstract":"Recent advances in multimodal foundation models have set new standards in few-shot anomaly detection. This paper explores whether high-quality visual features alone are sufficient to rival existing state-of-the-art vision-language models. We affirm this by adapting DINOv2 for one-shot and few-shot anomaly detection, with a focus on industrial applications. We show that this approach does not only rival existing techniques but can even outmatch them in many settings. Our proposed vision-only approach, AnomalyDINO, is based on patch similarities and enables both image-level anomaly prediction and pixel-level anomaly segmentation. The approach is methodologically simple and training-free and, thus, does not require any additional data for fine-tuning or meta-learning. Despite its simplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot anomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an AUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding few-shot performance, makes AnomalyDINO a strong candidate for fast deployment, for example, in industrial contexts.","sentences":["Recent advances in multimodal foundation models have set new standards in few-shot anomaly detection.","This paper explores whether high-quality visual features alone are sufficient to rival existing state-of-the-art vision-language models.","We affirm this by adapting DINOv2 for one-shot and few-shot anomaly detection, with a focus on industrial applications.","We show that this approach does not only rival existing techniques but can even outmatch them in many settings.","Our proposed vision-only approach, AnomalyDINO, is based on patch similarities and enables both image-level anomaly prediction and pixel-level anomaly segmentation.","The approach is methodologically simple and training-free and, thus, does not require any additional data for fine-tuning or meta-learning.","Despite its simplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot anomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an AUROC of 93.1% to 96.6%).","The reduced overhead, coupled with its outstanding few-shot performance, makes AnomalyDINO a strong candidate for fast deployment, for example, in industrial contexts."],"url":"http://arxiv.org/abs/2405.14529v1","category":"cs.CV"}
{"created":"2024-05-23 13:07:33","title":"QoE-Aware and Secure UAV-Aided Rate-Splitting Multiple Access Based Communications","abstract":"In this work, we address the issue of quality of experience (QoE) in unmanned aerial vehicle (UAV) aided multiuser rate-splitting multiple access (RSMA) networks under secrecy constraints. The problem is formulated as maximization of sum mean opinion scores (MOSs) of the users. The problem is decomposed into two subproblems, beamforming and rate allocation and UAV trajectory subproblem. For, beamforming and rate allocation subproblem, we use the epigraph method, property of polynomials, and the norm-bounded error of channels, we linearize the objective function. Then, applying second-order conic (SOC) and first Taylor expansion, we convexify the remaining nonconvex constraints. For the highly nonconvex UAV trajectory, we unroll the constraints and we apply first Taylor expansion on the unrolled constraints. The simulation results demonstrate the efficiency of the proposed framework.","sentences":["In this work, we address the issue of quality of experience (QoE) in unmanned aerial vehicle (UAV) aided multiuser rate-splitting multiple access (RSMA) networks under secrecy constraints.","The problem is formulated as maximization of sum mean opinion scores (MOSs) of the users.","The problem is decomposed into two subproblems, beamforming and rate allocation and UAV trajectory subproblem.","For, beamforming and rate allocation subproblem, we use the epigraph method, property of polynomials, and the norm-bounded error of channels, we linearize the objective function.","Then, applying second-order conic (SOC) and first Taylor expansion, we convexify the remaining nonconvex constraints.","For the highly nonconvex UAV trajectory, we unroll the constraints and we apply first Taylor expansion on the unrolled constraints.","The simulation results demonstrate the efficiency of the proposed framework."],"url":"http://arxiv.org/abs/2405.14524v1","category":"cs.NI"}
{"created":"2024-05-23 12:35:08","title":"DEX: Scalable Range Indexing on Disaggregated Memory [Extended Version]","abstract":"Memory disaggregation can potentially allow memory-optimized range indexes such as B+-trees to scale beyond one machine while attaining high hardware utilization and low cost. Designing scalable indexes on disaggregated memory, however, is challenging due to rudimentary caching, unprincipled offloading and excessive inconsistency among servers.   This paper proposes DEX, a new scalable B+-tree for memory disaggregation. DEX includes a set of techniques to reduce remote accesses, including logical partitioning, lightweight caching and cost-aware offloading. Our evaluation shows that DEX can outperform the state-of-the-art by 1.7--56.3X, and the advantage remains under various setups, such as cache size and skewness.","sentences":["Memory disaggregation can potentially allow memory-optimized range indexes such as B+-trees to scale beyond one machine while attaining high hardware utilization and low cost.","Designing scalable indexes on disaggregated memory, however, is challenging due to rudimentary caching, unprincipled offloading and excessive inconsistency among servers.   ","This paper proposes DEX, a new scalable B+-tree for memory disaggregation.","DEX includes a set of techniques to reduce remote accesses, including logical partitioning, lightweight caching and cost-aware offloading.","Our evaluation shows that DEX can outperform the state-of-the-art by 1.7--56.3X, and the advantage remains under various setups, such as cache size and skewness."],"url":"http://arxiv.org/abs/2405.14502v1","category":"cs.DB"}
{"created":"2024-05-23 11:38:23","title":"Cumulant-based approximation for fast and efficient prediction for species distribution","abstract":"Species distribution modeling plays an important role in estimating the habitat suitability of species using environmental variables. For this purpose, Maxent and the Poisson point process are popular and powerful methods extensively employed across various ecological and biological sciences. However, the computational speed becomes prohibitively slow when using huge background datasets, which is often the case with fine-resolution data or global-scale estimations. To address this problem, we propose a computationally efficient species distribution model using a cumulant-based approximation (CBA) applied to the loss function of $\\gamma$-divergence. Additionally, we introduce a sequential estimating algorithm with an $L_1$ penalty to select important environmental variables closely associated with species distribution. The regularized geometric-mean method, derived from the CBA, demonstrates high computational efficiency and estimation accuracy. Moreover, by applying CBA to Maxent, we establish that Maxent and Fisher linear discriminant analysis are equivalent under a normality assumption. This equivalence leads to an highly efficient computational method for estimating species distribution. The effectiveness of our proposed methods is illustrated through simulation studies and by analyzing data on 226 species from the National Centre for Ecological Analysis and Synthesis and 709 Japanese vascular plant species. The computational efficiency of the proposed methods is significantly improved compared to Maxent, while maintaining comparable estimation accuracy. A R package {\\tt CBA} is also prepared to provide all programming codes used in simulation studies and real data analysis.","sentences":["Species distribution modeling plays an important role in estimating the habitat suitability of species using environmental variables.","For this purpose, Maxent and the Poisson point process are popular and powerful methods extensively employed across various ecological and biological sciences.","However, the computational speed becomes prohibitively slow when using huge background datasets, which is often the case with fine-resolution data or global-scale estimations.","To address this problem, we propose a computationally efficient species distribution model using a cumulant-based approximation (CBA) applied to the loss function of $\\gamma$-divergence.","Additionally, we introduce a sequential estimating algorithm with an $L_1$ penalty to select important environmental variables closely associated with species distribution.","The regularized geometric-mean method, derived from the CBA, demonstrates high computational efficiency and estimation accuracy.","Moreover, by applying CBA to Maxent, we establish that Maxent and Fisher linear discriminant analysis are equivalent under a normality assumption.","This equivalence leads to an highly efficient computational method for estimating species distribution.","The effectiveness of our proposed methods is illustrated through simulation studies and by analyzing data on 226 species from the National Centre for Ecological Analysis and Synthesis and 709 Japanese vascular plant species.","The computational efficiency of the proposed methods is significantly improved compared to Maxent, while maintaining comparable estimation accuracy.","A R package {\\tt CBA} is also prepared to provide all programming codes used in simulation studies and real data analysis."],"url":"http://arxiv.org/abs/2405.14456v1","category":"stat.ME"}
{"created":"2024-05-23 11:23:57","title":"DuEDL: Dual-Branch Evidential Deep Learning for Scribble-Supervised Medical Image Segmentation","abstract":"Despite the recent progress in medical image segmentation with scribble-based annotations, the segmentation results of most models are still not ro-bust and generalizable enough in open environments. Evidential deep learn-ing (EDL) has recently been proposed as a promising solution to model predictive uncertainty and improve the reliability of medical image segmen-tation. However directly applying EDL to scribble-supervised medical im-age segmentation faces a tradeoff between accuracy and reliability. To ad-dress the challenge, we propose a novel framework called Dual-Branch Evi-dential Deep Learning (DuEDL). Firstly, the decoder of the segmentation network is changed to two different branches, and the evidence of the two branches is fused to generate high-quality pseudo-labels. Then the frame-work applies partial evidence loss and two-branch consistent loss for joint training of the model to adapt to the scribble supervision learning. The pro-posed method was tested on two cardiac datasets: ACDC and MSCMRseg. The results show that our method significantly enhances the reliability and generalization ability of the model without sacrificing accuracy, outper-forming state-of-the-art baselines. The code is available at https://github.com/Gardnery/DuEDL.","sentences":["Despite the recent progress in medical image segmentation with scribble-based annotations, the segmentation results of most models are still not ro-bust and generalizable enough in open environments.","Evidential deep learn-ing (EDL) has recently been proposed as a promising solution to model predictive uncertainty and improve the reliability of medical image segmen-tation.","However directly applying EDL to scribble-supervised medical im-age segmentation faces a tradeoff between accuracy and reliability.","To ad-dress the challenge, we propose a novel framework called Dual-Branch Evi-dential Deep Learning (DuEDL).","Firstly, the decoder of the segmentation network is changed to two different branches, and the evidence of the two branches is fused to generate high-quality pseudo-labels.","Then the frame-work applies partial evidence loss and two-branch consistent loss for joint training of the model to adapt to the scribble supervision learning.","The pro-posed method was tested on two cardiac datasets: ACDC and MSCMRseg.","The results show that our method significantly enhances the reliability and generalization ability of the model without sacrificing accuracy, outper-forming state-of-the-art baselines.","The code is available at https://github.com/Gardnery/DuEDL."],"url":"http://arxiv.org/abs/2405.14444v1","category":"cs.CV"}
{"created":"2024-05-23 11:14:35","title":"Bayesian Adaptive Calibration and Optimal Design","abstract":"The process of calibrating computer models of natural phenomena is essential for applications in the physical sciences, where plenty of domain knowledge can be embedded into simulations and then calibrated against real observations. Current machine learning approaches, however, mostly rely on rerunning simulations over a fixed set of designs available in the observed data, potentially neglecting informative correlations across the design space and requiring a large amount of simulations. Instead, we consider the calibration process from the perspective of Bayesian adaptive experimental design and propose a data-efficient algorithm to run maximally informative simulations within a batch-sequential process. At each round, the algorithm jointly estimates the parameters of the posterior distribution and optimal designs by maximising a variational lower bound of the expected information gain. The simulator is modelled as a sample from a Gaussian process, which allows us to correlate simulations and observed data with the unknown calibration parameters. We show the benefits of our method when compared to related approaches across synthetic and real-data problems.","sentences":["The process of calibrating computer models of natural phenomena is essential for applications in the physical sciences, where plenty of domain knowledge can be embedded into simulations and then calibrated against real observations.","Current machine learning approaches, however, mostly rely on rerunning simulations over a fixed set of designs available in the observed data, potentially neglecting informative correlations across the design space and requiring a large amount of simulations.","Instead, we consider the calibration process from the perspective of Bayesian adaptive experimental design and propose a data-efficient algorithm to run maximally informative simulations within a batch-sequential process.","At each round, the algorithm jointly estimates the parameters of the posterior distribution and optimal designs by maximising a variational lower bound of the expected information gain.","The simulator is modelled as a sample from a Gaussian process, which allows us to correlate simulations and observed data with the unknown calibration parameters.","We show the benefits of our method when compared to related approaches across synthetic and real-data problems."],"url":"http://arxiv.org/abs/2405.14440v1","category":"cs.LG"}
{"created":"2024-05-23 11:10:32","title":"LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention Networks","abstract":"Numerous crucial tasks in real-world decision-making rely on machine learning algorithms with calibrated uncertainty estimates. However, modern methods often yield overconfident and uncalibrated predictions. Various approaches involve training an ensemble of separate models to quantify the uncertainty related to the model itself, known as epistemic uncertainty. In an explicit implementation, the ensemble approach has high computational cost and high memory requirements. This particular challenge is evident in state-of-the-art neural networks such as transformers, where even a single network is already demanding in terms of compute and memory. Consequently, efforts are made to emulate the ensemble model without actually instantiating separate ensemble members, referred to as implicit ensembling. We introduce LoRA-Ensemble, a parameter-efficient deep ensemble method for self-attention networks, which is based on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM fine-tuning, we extend LoRA to an implicit ensembling approach. By employing a single pre-trained self-attention network with weights shared across all members, we train member-specific low-rank matrices for the attention projections. Our method exhibits superior calibration compared to explicit ensembles and achieves similar or better accuracy across various prediction tasks and datasets.","sentences":["Numerous crucial tasks in real-world decision-making rely on machine learning algorithms with calibrated uncertainty estimates.","However, modern methods often yield overconfident and uncalibrated predictions.","Various approaches involve training an ensemble of separate models to quantify the uncertainty related to the model itself, known as epistemic uncertainty.","In an explicit implementation, the ensemble approach has high computational cost and high memory requirements.","This particular challenge is evident in state-of-the-art neural networks such as transformers, where even a single network is already demanding in terms of compute and memory.","Consequently, efforts are made to emulate the ensemble model without actually instantiating separate ensemble members, referred to as implicit ensembling.","We introduce LoRA-Ensemble, a parameter-efficient deep ensemble method for self-attention networks, which is based on Low-Rank Adaptation (LoRA).","Initially developed for efficient LLM fine-tuning, we extend LoRA to an implicit ensembling approach.","By employing a single pre-trained self-attention network with weights shared across all members, we train member-specific low-rank matrices for the attention projections.","Our method exhibits superior calibration compared to explicit ensembles and achieves similar or better accuracy across various prediction tasks and datasets."],"url":"http://arxiv.org/abs/2405.14438v1","category":"cs.LG"}
{"created":"2024-05-23 11:00:31","title":"Boosting Robustness by Clipping Gradients in Distributed Learning","abstract":"Robust distributed learning consists in achieving good learning performance despite the presence of misbehaving workers. State-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods, relying on robust aggregation, have been proven to be optimal: Their learning error matches the lower bound established under the standard heterogeneity model of $(G, B)$-gradient dissimilarity. The learning guarantee of SOTA Robust-DGD cannot be further improved when model initialization is done arbitrarily. However, we show that it is possible to circumvent the lower bound, and improve the learning performance, when the workers' gradients at model initialization are assumed to be bounded. We prove this by proposing pre-aggregation clipping of workers' gradients, using a novel scheme called adaptive robust clipping (ARC). Incorporating ARC in Robust-DGD provably improves the learning, under the aforementioned assumption on model initialization. The factor of improvement is prominent when the tolerable fraction of misbehaving workers approaches the breakdown point. ARC induces this improvement by constricting the search space, while preserving the robustness property of the original aggregation scheme at the same time. We validate this theoretical finding through exhaustive experiments on benchmark image classification tasks.","sentences":["Robust distributed learning consists in achieving good learning performance despite the presence of misbehaving workers.","State-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods, relying on robust aggregation, have been proven to be optimal: Their learning error matches the lower bound established under the standard heterogeneity model of $(G, B)$-gradient dissimilarity.","The learning guarantee of SOTA Robust-DGD cannot be further improved when model initialization is done arbitrarily.","However, we show that it is possible to circumvent the lower bound, and improve the learning performance, when the workers' gradients at model initialization are assumed to be bounded.","We prove this by proposing pre-aggregation clipping of workers' gradients, using a novel scheme called adaptive robust clipping (ARC).","Incorporating ARC in Robust-DGD provably improves the learning, under the aforementioned assumption on model initialization.","The factor of improvement is prominent when the tolerable fraction of misbehaving workers approaches the breakdown point.","ARC induces this improvement by constricting the search space, while preserving the robustness property of the original aggregation scheme at the same time.","We validate this theoretical finding through exhaustive experiments on benchmark image classification tasks."],"url":"http://arxiv.org/abs/2405.14432v1","category":"cs.LG"}
{"created":"2024-05-23 10:35:29","title":"Progress in implementing the kinematical constraint into the small-x JIMWLK evolution equation","abstract":"The most complete high-energy evolution of Wilson line operators is described by the set of equations called Balitsky-JIMWLK evolution equations. It is known from the studies of the linear - the BFKL - evolution equation that the leading corrections come from the kinematically enhanced double collinear logarithms. A method for resumming such logarithmic corrections to all orders for the Balitsky-Kovchegov equation is known under the name of kinematical constraint. In this work, we discuss the progress in implementing these corrections into the Langevin formulation of the JIMWLK equation. In particular, we introduce a set of correlation functions which are nonlocal in the rapidity variable. They appear in the construction of the kinematical constraint, however, their behavior with rapidity has not been investigated numerically so far. We derive their large-$N$ evolution equations, solve them numerically, and comment on their implications for the implementation of the full kinematical constraint.","sentences":["The most complete high-energy evolution of Wilson line operators is described by the set of equations called Balitsky-JIMWLK evolution equations.","It is known from the studies of the linear - the BFKL - evolution equation that the leading corrections come from the kinematically enhanced double collinear logarithms.","A method for resumming such logarithmic corrections to all orders for the Balitsky-Kovchegov equation is known under the name of kinematical constraint.","In this work, we discuss the progress in implementing these corrections into the Langevin formulation of the JIMWLK equation.","In particular, we introduce a set of correlation functions which are nonlocal in the rapidity variable.","They appear in the construction of the kinematical constraint, however, their behavior with rapidity has not been investigated numerically so far.","We derive their large-$N$ evolution equations, solve them numerically, and comment on their implications for the implementation of the full kinematical constraint."],"url":"http://arxiv.org/abs/2405.14415v1","category":"hep-ph"}
{"created":"2024-05-23 10:21:05","title":"Exact Gauss-Newton Optimization for Training Deep Neural Networks","abstract":"We present EGN, a stochastic second-order optimization algorithm that combines the generalized Gauss-Newton (GN) Hessian approximation with low-rank linear algebra to compute the descent direction. Leveraging the Duncan-Guttman matrix identity, the parameter update is obtained by factorizing a matrix which has the size of the mini-batch. This is particularly advantageous for large-scale machine learning problems where the dimension of the neural network parameter vector is several orders of magnitude larger than the batch size. Additionally, we show how improvements such as line search, adaptive regularization, and momentum can be seamlessly added to EGN to further accelerate the algorithm. Moreover, under mild assumptions, we prove that our algorithm converges to an $\\epsilon$-stationary point at a linear rate. Finally, our numerical experiments demonstrate that EGN consistently exceeds, or at most matches the generalization performance of well-tuned SGD, Adam, and SGN optimizers across various supervised and reinforcement learning tasks.","sentences":["We present EGN, a stochastic second-order optimization algorithm that combines the generalized Gauss-Newton (GN) Hessian approximation with low-rank linear algebra to compute the descent direction.","Leveraging the Duncan-Guttman matrix identity, the parameter update is obtained by factorizing a matrix which has the size of the mini-batch.","This is particularly advantageous for large-scale machine learning problems where the dimension of the neural network parameter vector is several orders of magnitude larger than the batch size.","Additionally, we show how improvements such as line search, adaptive regularization, and momentum can be seamlessly added to EGN to further accelerate the algorithm.","Moreover, under mild assumptions, we prove that our algorithm converges to an $\\epsilon$-stationary point at a linear rate.","Finally, our numerical experiments demonstrate that EGN consistently exceeds, or at most matches the generalization performance of well-tuned SGD, Adam, and SGN optimizers across various supervised and reinforcement learning tasks."],"url":"http://arxiv.org/abs/2405.14402v1","category":"cs.LG"}
{"created":"2024-05-23 10:08:19","title":"Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows","abstract":"Continuous normalizing flows (CNFs) learn the probability path between a reference and a target density by modeling the vector field generating said path using neural networks. Recently, Lipman et al. (2022) introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM). In this paper, we re-purpose this method for probabilistic inference by incorporating Markovian sampling methods in evaluating the FM objective and using the learned probability path to improve Monte Carlo sampling. We propose a sequential method, which uses samples from a Markov chain to fix the probability path defining the FM objective. We augment this scheme with an adaptive tempering mechanism that allows the discovery of multiple modes in the target. Under mild assumptions, we establish convergence to a local optimum of the FM objective, discuss improvements in the convergence rate, and illustrate our methods on synthetic and real-world examples.","sentences":["Continuous normalizing flows (CNFs) learn the probability path between a reference and a target density by modeling the vector field generating said path using neural networks.","Recently, Lipman et al. (2022) introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM).","In this paper, we re-purpose this method for probabilistic inference by incorporating Markovian sampling methods in evaluating the FM objective and using the learned probability path to improve Monte Carlo sampling.","We propose a sequential method, which uses samples from a Markov chain to fix the probability path defining the FM objective.","We augment this scheme with an adaptive tempering mechanism that allows the discovery of multiple modes in the target.","Under mild assumptions, we establish convergence to a local optimum of the FM objective, discuss improvements in the convergence rate, and illustrate our methods on synthetic and real-world examples."],"url":"http://arxiv.org/abs/2405.14392v1","category":"stat.ME"}
{"created":"2024-05-23 10:05:29","title":"Speculating About Multi-user Conversational Interfaces and LLMs: What If Chatting Wasn't So Lonely?","abstract":"The advent of LLMs means that CUIs are cool again, but what isn't so cool is that we're doomed to use them alone. The one user, one account, one device paradigm has dominated the design of CUIs and is not going away as new conversational technologies emerge. In this provocation we explore some of the technical, legal, and design difficulties that seem to make multi-user CUIs so difficult to implement. Drawing inspiration from the ways that people manage messy group discussions, such as parliamentary and consensus-based paradigms, we show how LLM-based CUIs might be well suited to bridging the gap. With any luck, this might even result in everyone having to sit through fewer poorly run meetings and agonising group discussions - truly a laudable goal!","sentences":["The advent of LLMs means that CUIs are cool again, but what isn't so cool is that we're doomed to use them alone.","The one user, one account, one device paradigm has dominated the design of CUIs and is not going away as new conversational technologies emerge.","In this provocation we explore some of the technical, legal, and design difficulties that seem to make multi-user CUIs so difficult to implement.","Drawing inspiration from the ways that people manage messy group discussions, such as parliamentary and consensus-based paradigms, we show how LLM-based CUIs might be well suited to bridging the gap.","With any luck, this might even result in everyone having to sit through fewer poorly run meetings and agonising group discussions - truly a laudable goal!"],"url":"http://arxiv.org/abs/2405.14390v1","category":"cs.HC"}
{"created":"2024-05-23 10:04:35","title":"Uniform growth in small cancellation groups","abstract":"An open question asks whether every group acting acylindrically on a hyperbolic space has uniform exponential growth. We prove that the class of groups of uniform uniform exponential growth acting acylindrically on a hyperbolic space is closed under taking certain geometric small cancellation quotients. There are two consequences: firstly, there is a finitely generated acylindrically hyperbolic group that has uniform exponential growth but has arbitrarily large torsion balls. Secondly, the uniform uniform exponential growth rate of a classical $C''(\\lambda)$-small cancellation group, for sufficiently small $\\lambda$, is bounded from below by a universal positive constant. We give a similar result for uniform entropy-cardinality estimates. This yields an explicit upper bound on the isomorphism class of marked $\\delta$-hyperbolic $C''(\\lambda)$-small cancellation groups of uniformly bounded entropy in terms of $\\delta$ and the entropy bound.","sentences":["An open question asks whether every group acting acylindrically on a hyperbolic space has uniform exponential growth.","We prove that the class of groups of uniform uniform exponential growth acting acylindrically on a hyperbolic space is closed under taking certain geometric small cancellation quotients.","There are two consequences: firstly, there is a finitely generated acylindrically hyperbolic group that has uniform exponential growth but has arbitrarily large torsion balls.","Secondly, the uniform uniform exponential growth rate of a classical $C''(\\lambda)$-small cancellation group, for sufficiently small $\\lambda$, is bounded from below by a universal positive constant.","We give a similar result for uniform entropy-cardinality estimates.","This yields an explicit upper bound on the isomorphism class of marked $\\delta$-hyperbolic $C''(\\lambda)$-small cancellation groups of uniformly bounded entropy in terms of $\\delta$ and the entropy bound."],"url":"http://arxiv.org/abs/2405.14387v1","category":"math.GR"}
{"created":"2024-05-23 09:55:15","title":"Determining $\u03b1_s(m_Z)$ from Thrust with Power Corrections","abstract":"We update and extend a previous N$^3$LL$^\\prime$+${\\cal O}(\\alpha_s^3)$ strong coupling determination from thrust data. In particular, we carry out a fit with data fully restricted to the dijet region seeking to minimize the potential impact of power corrections that go beyond dijet configurations. In addition, we parametrize deviations from the dijet power correction in order to add an additional source of uncertainty in the result for $\\alpha_s(m_Z)$. We also show that the inclusion of resummation is important to achieve stability with respect to varying the fit region.","sentences":["We update and extend a previous N$^3$LL$^\\prime$+${\\cal O}(\\alpha_s^3)$ strong coupling determination from thrust data.","In particular, we carry out a fit with data fully restricted to the dijet region seeking to minimize the potential impact of power corrections that go beyond dijet configurations.","In addition, we parametrize deviations from the dijet power correction in order to add an additional source of uncertainty in the result for $\\alpha_s(m_Z)$. We also show that the inclusion of resummation is important to achieve stability with respect to varying the fit region."],"url":"http://arxiv.org/abs/2405.14380v1","category":"hep-ph"}
{"created":"2024-05-23 09:49:46","title":"Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift: an unadjusted Barker algorithm","abstract":"We propose a new simple and explicit numerical scheme for time-homogeneous stochastic differential equations. The scheme is based on sampling increments at each time step from a skew-symmetric probability distribution, with the level of skewness determined by the drift and volatility of the underlying process. We show that as the step-size decreases the scheme converges weakly to the diffusion of interest. We then consider the problem of simulating from the limiting distribution of an ergodic diffusion process using the numerical scheme with a fixed step-size. We establish conditions under which the numerical scheme converges to equilibrium at a geometric rate, and quantify the bias between the equilibrium distributions of the scheme and of the true diffusion process. Notably, our results do not require a global Lipschitz assumption on the drift, in contrast to those required for the Euler--Maruyama scheme for long-time simulation at fixed step-sizes. Our weak convergence result relies on an extension of the theory of Milstein \\& Tretyakov to stochastic differential equations with non-Lipschitz drift, which could also be of independent interest. We support our theoretical results with numerical simulations.","sentences":["We propose a new simple and explicit numerical scheme for time-homogeneous stochastic differential equations.","The scheme is based on sampling increments at each time step from a skew-symmetric probability distribution, with the level of skewness determined by the drift and volatility of the underlying process.","We show that as the step-size decreases the scheme converges weakly to the diffusion of interest.","We then consider the problem of simulating from the limiting distribution of an ergodic diffusion process using the numerical scheme with a fixed step-size.","We establish conditions under which the numerical scheme converges to equilibrium at a geometric rate, and quantify the bias between the equilibrium distributions of the scheme and of the true diffusion process.","Notably, our results do not require a global Lipschitz assumption on the drift, in contrast to those required for the Euler--Maruyama scheme for long-time simulation at fixed step-sizes.","Our weak convergence result relies on an extension of the theory of Milstein \\& Tretyakov to stochastic differential equations with non-Lipschitz drift, which could also be of independent interest.","We support our theoretical results with numerical simulations."],"url":"http://arxiv.org/abs/2405.14373v1","category":"math.PR"}
{"created":"2024-05-23 09:48:48","title":"Learning Constrained Markov Decision Processes With Non-stationary Rewards and Constraints","abstract":"In constrained Markov decision processes (CMDPs) with adversarial rewards and constraints, a well-known impossibility result prevents any algorithm from attaining both sublinear regret and sublinear constraint violation, when competing against a best-in-hindsight policy that satisfies constraints on average. In this paper, we show that this negative result can be eased in CMDPs with non-stationary rewards and constraints, by providing algorithms whose performances smoothly degrade as non-stationarity increases. Specifically, we propose algorithms attaining $\\tilde{\\mathcal{O}} (\\sqrt{T} + C)$ regret and positive constraint violation under bandit feedback, where $C$ is a corruption value measuring the environment non-stationarity. This can be $\\Theta(T)$ in the worst case, coherently with the impossibility result for adversarial CMDPs. First, we design an algorithm with the desired guarantees when $C$ is known. Then, in the case $C$ is unknown, we show how to obtain the same results by embedding such an algorithm in a general meta-procedure. This is of independent interest, as it can be applied to any non-stationary constrained online learning setting.","sentences":["In constrained Markov decision processes (CMDPs) with adversarial rewards and constraints, a well-known impossibility result prevents any algorithm from attaining both sublinear regret and sublinear constraint violation, when competing against a best-in-hindsight policy that satisfies constraints on average.","In this paper, we show that this negative result can be eased in CMDPs with non-stationary rewards and constraints, by providing algorithms whose performances smoothly degrade as non-stationarity increases.","Specifically, we propose algorithms attaining $\\tilde{\\mathcal{O}} (\\sqrt{T} + C)$ regret and positive constraint violation under bandit feedback, where $C$ is a corruption value measuring the environment non-stationarity.","This can be $\\Theta(T)$ in the worst case, coherently with the impossibility result for adversarial CMDPs.","First, we design an algorithm with the desired guarantees when $C$ is known.","Then, in the case $C$ is unknown, we show how to obtain the same results by embedding such an algorithm in a general meta-procedure.","This is of independent interest, as it can be applied to any non-stationary constrained online learning setting."],"url":"http://arxiv.org/abs/2405.14372v1","category":"cs.LG"}
{"created":"2024-05-23 09:46:22","title":"EdgeShard: Efficient LLM Inference via Collaborative Edge Computing","abstract":"Large language models (LLMs) have shown great potential in natural language processing and content generation. However, current LLMs heavily rely on cloud computing, leading to prolonged latency, high bandwidth cost, and privacy concerns. Edge computing is promising to address such concerns by deploying LLMs on edge devices, closer to data sources. Some works try to leverage model quantization to reduce the model size to fit the resource-constraint edge devices, but they lead to accuracy loss. Other works use cloud-edge collaboration, suffering from unstable network connections. In this work, we leverage collaborative edge computing to facilitate the collaboration among edge devices and cloud servers for jointly performing efficient LLM inference. We propose a general framework to partition the LLM model into shards and deploy on distributed devices. To achieve efficient LLM inference, we formulate an adaptive joint device selection and model partition problem and design an efficient dynamic programming algorithm to optimize the inference latency and throughput, respectively. Experiments of Llama2 serial models on a heterogeneous physical prototype demonstrate that EdgeShard achieves up to 50% latency reduction and 2x throughput improvement over baseline methods.","sentences":["Large language models (LLMs) have shown great potential in natural language processing and content generation.","However, current LLMs heavily rely on cloud computing, leading to prolonged latency, high bandwidth cost, and privacy concerns.","Edge computing is promising to address such concerns by deploying LLMs on edge devices, closer to data sources.","Some works try to leverage model quantization to reduce the model size to fit the resource-constraint edge devices, but they lead to accuracy loss.","Other works use cloud-edge collaboration, suffering from unstable network connections.","In this work, we leverage collaborative edge computing to facilitate the collaboration among edge devices and cloud servers for jointly performing efficient LLM inference.","We propose a general framework to partition the LLM model into shards and deploy on distributed devices.","To achieve efficient LLM inference, we formulate an adaptive joint device selection and model partition problem and design an efficient dynamic programming algorithm to optimize the inference latency and throughput, respectively.","Experiments of Llama2 serial models on a heterogeneous physical prototype demonstrate that EdgeShard achieves up to 50% latency reduction and 2x throughput improvement over baseline methods."],"url":"http://arxiv.org/abs/2405.14371v1","category":"cs.DC"}
{"created":"2024-05-23 09:15:46","title":"Expert exploranation for communicating scientific methods -- A case study in conflict research","abstract":"Science communication aims at making key research insights accessible to the broad public. If explanatory and exploratory visualization techniques are combined to do so, the approach is also referred to as exploranation. In this context, the audience is usually not required to have domain expertise. However, we show that exploranation can not only support the communication between researchers and a broad audience, but also between researchers directly. With the goal of communicating an existing method for conducting causal inference on spatio-temporal conflict event data, we investigated how to perform exploranation for experts, i.e., expert exploranation. Based on application scenarios of the inference method, we developed three versions of an interactive visual story to explain the method to conflict researchers. We abstracted the corresponding design process and evaluated the stories both with experts who were unfamiliar with the explained method and experts who were already familiar with it. The positive and extensive feedback from the evaluation shows that expert exploranation is a promising direction for visual storytelling, as it can help to improve scientific outreach, methodological understanding, and accessibility for researchers new to a field.","sentences":["Science communication aims at making key research insights accessible to the broad public.","If explanatory and exploratory visualization techniques are combined to do so, the approach is also referred to as exploranation.","In this context, the audience is usually not required to have domain expertise.","However, we show that exploranation can not only support the communication between researchers and a broad audience, but also between researchers directly.","With the goal of communicating an existing method for conducting causal inference on spatio-temporal conflict event data, we investigated how to perform exploranation for experts, i.e., expert exploranation.","Based on application scenarios of the inference method, we developed three versions of an interactive visual story to explain the method to conflict researchers.","We abstracted the corresponding design process and evaluated the stories both with experts who were unfamiliar with the explained method and experts who were already familiar with it.","The positive and extensive feedback from the evaluation shows that expert exploranation is a promising direction for visual storytelling, as it can help to improve scientific outreach, methodological understanding, and accessibility for researchers new to a field."],"url":"http://arxiv.org/abs/2405.14345v1","category":"cs.CY"}
{"created":"2024-05-23 08:56:29","title":"Optimal diffusion of chiral active particles with strategic reorientations","abstract":"We investigate the competing effects of simultaneous presence of chirality and generalised tumbles in the dynamics of an active Brownian particle. Chiral active particles perform circular motions that give rise to slow transport at late times. By interrupting these circular trajectories at the right time or by performing a tumble at the correct angle, we show that particles can enhance their diffusion. After deriving exact expressions for the orientational propagator and correlations, we use this to calculate the first two moments of displacement. For the effective diffusion coefficient, we study various optimal tumbling strategies. We show that under optimisation of the tumbling rate, the case of symmetrically distributed tumbles always give rise to enhanced diffusion, with an effective diffusion coefficient taking a universal value. Next, two cases are considered in detail, namely directional reversal and tumbles at an arbitrary but fixed angle. We discuss how asymmetric tumbles can enhance diffusion beyond that of symmetric tumbles.","sentences":["We investigate the competing effects of simultaneous presence of chirality and generalised tumbles in the dynamics of an active Brownian particle.","Chiral active particles perform circular motions that give rise to slow transport at late times.","By interrupting these circular trajectories at the right time or by performing a tumble at the correct angle, we show that particles can enhance their diffusion.","After deriving exact expressions for the orientational propagator and correlations, we use this to calculate the first two moments of displacement.","For the effective diffusion coefficient, we study various optimal tumbling strategies.","We show that under optimisation of the tumbling rate, the case of symmetrically distributed tumbles always give rise to enhanced diffusion, with an effective diffusion coefficient taking a universal value.","Next, two cases are considered in detail, namely directional reversal and tumbles at an arbitrary but fixed angle.","We discuss how asymmetric tumbles can enhance diffusion beyond that of symmetric tumbles."],"url":"http://arxiv.org/abs/2405.14326v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-23 08:40:57","title":"qtOCT: quantitative transmission optical coherence tomography","abstract":"Transmission optical coherence tomography (OCT) enables analysis of biological specimens in vitro through detection of forward scattered light. Up to now, transmission OCT was considered as a technique that cannot directly retrieve quantitative phase and is thus a qualitative method. In this paper, we present qtOCT, a novel quantitative transmission optical coherence tomography method. Unlike existing approaches, qtOCT allows for a direct, easy, fast and rigorous retrieval of 2D integrated phase information from transmission full-field swept-source OCT measurements. Our method is based on coherence gating and allows user-defined temporal measurement range selection, making it potentially suitable for analyzing multiple-scattering samples. We demonstrate high consistency between qtOCT and digital holographic microscopy phase images. This approach enhances transmission OCT capabilities, positioning it as a viable alternative to quantitative phase imaging techniques.","sentences":["Transmission optical coherence tomography (OCT) enables analysis of biological specimens in vitro through detection of forward scattered light.","Up to now, transmission OCT was considered as a technique that cannot directly retrieve quantitative phase and is thus a qualitative method.","In this paper, we present qtOCT, a novel quantitative transmission optical coherence tomography method.","Unlike existing approaches, qtOCT allows for a direct, easy, fast and rigorous retrieval of 2D integrated phase information from transmission full-field swept-source OCT measurements.","Our method is based on coherence gating and allows user-defined temporal measurement range selection, making it potentially suitable for analyzing multiple-scattering samples.","We demonstrate high consistency between qtOCT and digital holographic microscopy phase images.","This approach enhances transmission OCT capabilities, positioning it as a viable alternative to quantitative phase imaging techniques."],"url":"http://arxiv.org/abs/2405.14315v1","category":"physics.optics"}
{"created":"2024-05-23 08:23:22","title":"Similarity-Navigated Conformal Prediction for Graph Neural Networks","abstract":"Graph Neural Networks have achieved remarkable accuracy in semi-supervised node classification tasks. However, these results lack reliable uncertainty estimates. Conformal prediction methods provide a theoretical guarantee for node classification tasks, ensuring that the conformal prediction set contains the ground-truth label with a desired probability (e.g., 95%). In this paper, we empirically show that for each node, aggregating the non-conformity scores of nodes with the same label can improve the efficiency of conformal prediction sets. This observation motivates us to propose a novel algorithm named Similarity-Navigated Adaptive Prediction Sets (SNAPS), which aggregates the non-conformity scores based on feature similarity and structural neighborhood. The key idea behind SNAPS is that nodes with high feature similarity or direct connections tend to have the same label. By incorporating adaptive similar nodes information, SNAPS can generate compact prediction sets and increase the singleton hit ratio (correct prediction sets of size one). Moreover, we theoretically provide a finite-sample coverage guarantee of SNAPS. Extensive experiments demonstrate the superiority of SNAPS, improving the efficiency of prediction sets and singleton hit ratio while maintaining valid coverage.","sentences":["Graph Neural Networks have achieved remarkable accuracy in semi-supervised node classification tasks.","However, these results lack reliable uncertainty estimates.","Conformal prediction methods provide a theoretical guarantee for node classification tasks, ensuring that the conformal prediction set contains the ground-truth label with a desired probability (e.g., 95%).","In this paper, we empirically show that for each node, aggregating the non-conformity scores of nodes with the same label can improve the efficiency of conformal prediction sets.","This observation motivates us to propose a novel algorithm named Similarity-Navigated Adaptive Prediction Sets (SNAPS), which aggregates the non-conformity scores based on feature similarity and structural neighborhood.","The key idea behind SNAPS is that nodes with high feature similarity or direct connections tend to have the same label.","By incorporating adaptive similar nodes information, SNAPS can generate compact prediction sets and increase the singleton hit ratio (correct prediction sets of size one).","Moreover, we theoretically provide a finite-sample coverage guarantee of SNAPS.","Extensive experiments demonstrate the superiority of SNAPS, improving the efficiency of prediction sets and singleton hit ratio while maintaining valid coverage."],"url":"http://arxiv.org/abs/2405.14303v1","category":"cs.LG"}
{"created":"2024-05-23 08:15:49","title":"Focus Anywhere for Fine-grained Multi-page Document Understanding","abstract":"Modern LVLMs still struggle to achieve fine-grained document understanding, such as OCR/translation/caption for regions of interest to the user, tasks that require the context of the entire page, or even multiple pages. Accordingly, this paper proposes Fox, an effective pipeline, hybrid data, and tuning strategy, that catalyzes LVLMs to focus anywhere on single/multi-page documents. We introduce a novel task to boost the document understanding by making LVLMs focus attention on the document-level region, such as redefining full-page OCR as foreground focus. We employ multiple vision vocabularies to extract visual hybrid knowledge for interleaved document pages (e.g., a page containing a photo). Meanwhile, we render cross-vocabulary vision data as the catalyzer to achieve a full reaction of multiple visual vocabularies and in-document figure understanding. Further, without modifying the weights of multiple vision vocabularies, the above catalyzed fine-grained understanding capabilities can be efficiently tuned to multi-page documents, enabling the model to focus anywhere in both format-free and page-free manners. Besides, we build a benchmark including 9 fine-grained sub-tasks (e.g., region-level OCR/summary, color-guided OCR) to promote document analysis in the community. The experimental results verify the superiority of our model.","sentences":["Modern LVLMs still struggle to achieve fine-grained document understanding, such as OCR/translation/caption for regions of interest to the user, tasks that require the context of the entire page, or even multiple pages.","Accordingly, this paper proposes Fox, an effective pipeline, hybrid data, and tuning strategy, that catalyzes LVLMs to focus anywhere on single/multi-page documents.","We introduce a novel task to boost the document understanding by making LVLMs focus attention on the document-level region, such as redefining full-page OCR as foreground focus.","We employ multiple vision vocabularies to extract visual hybrid knowledge for interleaved document pages (e.g., a page containing a photo).","Meanwhile, we render cross-vocabulary vision data as the catalyzer to achieve a full reaction of multiple visual vocabularies and in-document figure understanding.","Further, without modifying the weights of multiple vision vocabularies, the above catalyzed fine-grained understanding capabilities can be efficiently tuned to multi-page documents, enabling the model to focus anywhere in both format-free and page-free manners.","Besides, we build a benchmark including 9 fine-grained sub-tasks (e.g., region-level OCR/summary, color-guided OCR) to promote document analysis in the community.","The experimental results verify the superiority of our model."],"url":"http://arxiv.org/abs/2405.14295v1","category":"cs.CV"}
{"created":"2024-05-23 08:04:37","title":"Recursive Green's functions optimized for atomistic modelling of large superlattice-based devices","abstract":"The Green's function method is recognized to be a very powerful tool for modelling quantum transport in nanoscale electronic devices. As atomistic calculations are generally expensive, numerical methods and related algorithms have been developed accordingly to optimize their computation cost. In particular, recursive techniques have been efficiently applied within the Green's function calculation approach. Recently, with the discovery of Moir\\'e materials, several attractive superlattices have been explored using these recursive Green's function techniques. However, numerical difficulty issues were reported as most of these superlattices have relatively large supercells, and consequently a huge number of atoms to be considered. In this article, improvements to solve these issues are proposed in order to keep optimizing the recursive Green's function calculations. These improvements make the electronic structure calculations feasible and efficient in modelling large superlattice-based devices. As an illustrative example, twisted bilayer graphene superlattices are computed and presented to demonstrate the efficiency of the method.","sentences":["The Green's function method is recognized to be a very powerful tool for modelling quantum transport in nanoscale electronic devices.","As atomistic calculations are generally expensive, numerical methods and related algorithms have been developed accordingly to optimize their computation cost.","In particular, recursive techniques have been efficiently applied within the Green's function calculation approach.","Recently, with the discovery of Moir\\'e materials, several attractive superlattices have been explored using these recursive Green's function techniques.","However, numerical difficulty issues were reported as most of these superlattices have relatively large supercells, and consequently a huge number of atoms to be considered.","In this article, improvements to solve these issues are proposed in order to keep optimizing the recursive Green's function calculations.","These improvements make the electronic structure calculations feasible and efficient in modelling large superlattice-based devices.","As an illustrative example, twisted bilayer graphene superlattices are computed and presented to demonstrate the efficiency of the method."],"url":"http://arxiv.org/abs/2405.14288v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 08:00:45","title":"Computing the Bias of Constant-step Stochastic Approximation with Markovian Noise","abstract":"We study stochastic approximation algorithms with Markovian noise and constant step-size $\\alpha$. We develop a method based on infinitesimal generator comparisons to study the bias of the algorithm, which is the expected difference between $\\theta_n$ -- the value at iteration $n$ -- and $\\theta^*$ -- the unique equilibrium of the corresponding ODE. We show that, under some smoothness conditions, this bias is of order $O(\\alpha)$. Furthermore, we show that the time-averaged bias is equal to $\\alpha V + O(\\alpha^2)$, where $V$ is a constant characterized by a Lyapunov equation, showing that $\\esp{\\bar{\\theta}_n} \\approx \\theta^*+V\\alpha + O(\\alpha^2)$, where $\\bar{\\theta}_n=(1/n)\\sum_{k=1}^n\\theta_k$ is the Polyak-Ruppert average. We also show that $\\bar{\\theta}_n$ converges with high probability around $\\theta^*+\\alpha V$. We illustrate how to combine this with Richardson-Romberg extrapolation to derive an iterative scheme with a bias of order $O(\\alpha^2)$.","sentences":["We study stochastic approximation algorithms with Markovian noise and constant step-size $\\alpha$. We develop a method based on infinitesimal generator comparisons to study the bias of the algorithm, which is the expected difference between $\\theta_n$ -- the value at iteration $n$ -- and $\\theta^*$ -- the unique equilibrium of the corresponding ODE.","We show that, under some smoothness conditions, this bias is of order $O(\\alpha)$.","Furthermore, we show that the time-averaged bias is equal to $\\alpha V + O(\\alpha^2)$, where $V$ is a constant characterized by a Lyapunov equation, showing that $\\esp{\\bar{\\theta}_n} \\approx \\theta^*+V\\alpha + O(\\alpha^2)$, where $\\bar{\\theta}_n=(1/n)\\sum_{k=1}^n\\theta_k$ is the Polyak-Ruppert average.","We also show that $\\bar{\\theta}_n$ converges with high probability around $\\theta^*+\\alpha V$. We illustrate how to combine this with Richardson-Romberg extrapolation to derive an iterative scheme with a bias of order","$O(\\alpha^2)$."],"url":"http://arxiv.org/abs/2405.14285v1","category":"stat.ML"}
{"created":"2024-05-23 07:54:14","title":"Optimized Cost Per Click in Online Advertising: A Theoretical Analysis","abstract":"In recent years, Optimized Cost Per Click (OCPC) and Optimized Cost Per Mille (OCPM) have emerged as the most widely adopted pricing models in the online advertising industry. However, the existing literature has yet to identify the specific conditions under which these models outperform traditional pricing models like Cost Per Click (CPC) and Cost Per Action (CPA). To fill the gap, this paper builds an economic model that compares OCPC with CPC and CPA theoretically, which incorporates out-site scenarios and outside options as two key factors. Our analysis reveals that OCPC can effectively replace CPA by tackling the problem of advertisers strategically manipulating conversion reporting in out-site scenarios where conversions occur outside the advertising platform. Furthermore, OCPC exhibits the potential to surpass CPC in platform payoffs by providing higher advertiser payoffs and consequently attracting more advertisers. However, if advertisers have less competitive outside options and consistently stay in the focal platform, the platform may achieve higher payoffs using CPC. Our findings deliver valuable insights for online advertising platforms in selecting optimal pricing models, and provide recommendations for further enhancing their payoffs. To the best of our knowledge, this is the first study to analyze OCPC from an economic perspective. Moreover, our analysis can be applied to the OCPM model as well.","sentences":["In recent years, Optimized Cost Per Click (OCPC) and Optimized Cost Per Mille (OCPM) have emerged as the most widely adopted pricing models in the online advertising industry.","However, the existing literature has yet to identify the specific conditions under which these models outperform traditional pricing models like Cost Per Click (CPC) and Cost Per Action (CPA).","To fill the gap, this paper builds an economic model that compares OCPC with CPC and CPA theoretically, which incorporates out-site scenarios and outside options as two key factors.","Our analysis reveals that OCPC can effectively replace CPA by tackling the problem of advertisers strategically manipulating conversion reporting in out-site scenarios where conversions occur outside the advertising platform.","Furthermore, OCPC exhibits the potential to surpass CPC in platform payoffs by providing higher advertiser payoffs and consequently attracting more advertisers.","However, if advertisers have less competitive outside options and consistently stay in the focal platform, the platform may achieve higher payoffs using CPC.","Our findings deliver valuable insights for online advertising platforms in selecting optimal pricing models, and provide recommendations for further enhancing their payoffs.","To the best of our knowledge, this is the first study to analyze OCPC from an economic perspective.","Moreover, our analysis can be applied to the OCPM model as well."],"url":"http://arxiv.org/abs/2405.14279v1","category":"cs.GT"}
{"created":"2024-05-23 07:53:01","title":"D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup","abstract":"Over the past years, we have observed an abundance of approaches for modeling dynamic 3D scenes using Gaussian Splatting (GS). Such solutions use GS to represent the scene's structure and the neural network to model dynamics. Such approaches allow fast rendering and extracting each element of such a dynamic scene. However, modifying such objects over time is challenging. SC-GS (Sparse Controlled Gaussian Splatting) enhanced with Deformed Control Points partially solves this issue. However, this approach necessitates selecting elements that need to be kept fixed, as well as centroids that should be adjusted throughout editing. Moreover, this task poses additional difficulties regarding the re-productivity of such editing. To address this, we propose Dynamic Multi-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired representation of dynamic GS. Additionally, we propose a strategy of linking parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh. Consequently, we can separately construct new trajectories for the 3D objects composing the scene. Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics.","sentences":["Over the past years, we have observed an abundance of approaches for modeling dynamic 3D scenes using Gaussian Splatting (GS).","Such solutions use GS to represent the scene's structure and the neural network to model dynamics.","Such approaches allow fast rendering and extracting each element of such a dynamic scene.","However, modifying such objects over time is challenging.","SC-GS (Sparse Controlled Gaussian Splatting) enhanced with Deformed Control Points partially solves this issue.","However, this approach necessitates selecting elements that need to be kept fixed, as well as centroids that should be adjusted throughout editing.","Moreover, this task poses additional difficulties regarding the re-productivity of such editing.","To address this, we propose Dynamic Multi-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired representation of dynamic GS.","Additionally, we propose a strategy of linking parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh.","Consequently, we can separately construct new trajectories for the 3D objects composing the scene.","Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics."],"url":"http://arxiv.org/abs/2405.14276v1","category":"cs.CV"}
{"created":"2024-05-23 07:48:19","title":"Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models","abstract":"Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D representations with synchronized images and point clouds, often faces a self-conflict dilemma. This issue arises as contrastive losses unintentionally dissociate features of unmatched points and pixels that share semantic labels, compromising the integrity of learned representations. To overcome this, we harness Visual Foundation Models (VFMs), which have revolutionized the acquisition of pixel-level semantics, to enhance 3D representation learning. Specifically, we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation. Additionally, we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs. Furthermore, we adapt sampling probabilities of points to address imbalances in spatial distribution and category frequency, promoting comprehensive and balanced learning. Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks. The source code is available at \\href{https://github.com/Eaphan/OLIVINE.}{\\color{black}https://github.com/Eaphan/OLIVINE}.","sentences":["Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D representations with synchronized images and point clouds, often faces a self-conflict dilemma.","This issue arises as contrastive losses unintentionally dissociate features of unmatched points and pixels that share semantic labels, compromising the integrity of learned representations.","To overcome this, we harness Visual Foundation Models (VFMs), which have revolutionized the acquisition of pixel-level semantics, to enhance 3D representation learning.","Specifically, we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation.","Additionally, we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs.","Furthermore, we adapt sampling probabilities of points to address imbalances in spatial distribution and category frequency, promoting comprehensive and balanced learning.","Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks.","The source code is available at \\href{https://github.com/Eaphan/OLIVINE.}{\\color{black}https://github.com/Eaphan/OLIVINE}."],"url":"http://arxiv.org/abs/2405.14271v1","category":"cs.CV"}
{"created":"2024-05-23 07:34:55","title":"Stochastic Proximal Point Methods for Monotone Inclusions under Expected Similarity","abstract":"Monotone inclusions have a wide range of applications, including minimization, saddle-point, and equilibria problems. We introduce new stochastic algorithms, with or without variance reduction, to estimate a root of the expectation of possibly set-valued monotone operators, using at every iteration one call to the resolvent of a randomly sampled operator. We also introduce a notion of similarity between the operators, which holds even for discontinuous operators. We leverage it to derive linear convergence results in the strongly monotone setting.","sentences":["Monotone inclusions have a wide range of applications, including minimization, saddle-point, and equilibria problems.","We introduce new stochastic algorithms, with or without variance reduction, to estimate a root of the expectation of possibly set-valued monotone operators, using at every iteration one call to the resolvent of a randomly sampled operator.","We also introduce a notion of similarity between the operators, which holds even for discontinuous operators.","We leverage it to derive linear convergence results in the strongly monotone setting."],"url":"http://arxiv.org/abs/2405.14255v1","category":"math.OC"}
{"created":"2024-05-23 07:31:25","title":"Path-Reporting Distance Oracles with Linear Size","abstract":"Given an undirected weighted graph, an (approximate) distance oracle is a data structure that can (approximately) answer distance queries. A {\\em Path-Reporting Distance Oracle}, or {\\em PRDO}, is a distance oracle that must also return a path between the queried vertices. Given a graph on $n$ vertices and an integer parameter $k\\ge 1$, Thorup and Zwick \\cite{TZ01} showed a PRDO with stretch $2k-1$, size $O(k\\cdot n^{1+1/k})$ and query time $O(k)$ (for the query time of PRDOs, we omit the time needed to report the path itself). Subsequent works \\cite{MN06,C14,C15} improved the size to $O(n^{1+1/k})$ and the query time to $O(1)$. However, these improvements produce distance oracles which are not path-reporting. Several other works \\cite{ENW16,EP15} focused on small size PRDO for general graphs, but all known results on distance oracles with linear size suffer from polynomial stretch, polynomial query time, or not being path-reporting.   In this paper we devise the first linear size PRDO with poly-logarithmic stretch and low query time $O(\\log\\log n)$. More generally, for any integer $k\\ge 1$, we obtain a PRDO with stretch at most $O(k^{4.82})$, size $O(n^{1+1/k})$, and query time $O(\\log k)$. In addition, we can make the size of our PRDO as small as $n+o(n)$, at the cost of increasing the query time to poly-logarithmic. For unweighted graphs, we improve the stretch to $O(k^2)$.   We also consider {\\em pairwise PRDO}, which is a PRDO that is only required to answer queries from a given set of pairs ${\\cal P}$. An exact PRDO of size $O(n+|{\\cal P}|^2)$ and constant query time was provided in \\cite{EP15}. In this work we dramatically improve the size, at the cost of slightly increasing the stretch. Specifically, given any $\\epsilon>0$, we devise a pairwise PRDO with stretch $1+\\epsilon$, constant query time, and near optimal size $n^{o(1)}\\cdot (n+|{\\cal P}|)$.","sentences":["Given an undirected weighted graph, an (approximate) distance oracle is a data structure that can (approximately) answer distance queries.","A {\\em Path-Reporting Distance Oracle}, or {\\em PRDO}, is a distance oracle that must also return a path between the queried vertices.","Given a graph on $n$ vertices and an integer parameter $k\\ge 1$, Thorup and Zwick \\cite{TZ01} showed a PRDO with stretch $2k-1$, size $O(k\\cdot n^{1","+1/k})$ and query time $O(k)$ (for the query time of PRDOs, we omit the time needed to report the path itself).","Subsequent works \\cite{MN06,C14,C15} improved the size to $O(n^{1+1/k})$ and the query time to $O(1)$. However, these improvements produce distance oracles which are not path-reporting.","Several other works \\cite{ENW16,EP15} focused on small size PRDO for general graphs, but all known results on distance oracles with linear size suffer from polynomial stretch, polynomial query time, or not being path-reporting.   ","In this paper we devise the first linear size PRDO with poly-logarithmic stretch and low query time $O(\\log\\log n)$. More generally, for any integer $k\\ge 1$, we obtain a PRDO with stretch at most $O(k^{4.82})$, size $O(n^{1+1/k})$, and query time $O(\\log k)$.","In addition, we can make the size of our PRDO as small as $n+o(n)$, at the cost of increasing the query time to poly-logarithmic.","For unweighted graphs, we improve the stretch to $O(k^2)$.   We also consider {\\em pairwise PRDO}, which is a PRDO that is only required to answer queries from a given set of pairs ${\\cal P}$.","An exact PRDO of size $O(n+|{\\cal P}|^2)$ and constant query time was provided in \\cite{EP15}.","In this work we dramatically improve the size, at the cost of slightly increasing the stretch.","Specifically, given any $\\epsilon>0$, we devise a pairwise PRDO with stretch $1+\\epsilon$, constant query time, and near optimal size $n^{o(1)}\\cdot (n+|{\\cal P}|)$."],"url":"http://arxiv.org/abs/2405.14254v1","category":"cs.DS"}
{"created":"2024-05-23 07:31:10","title":"Time-FFM: Towards LM-Empowered Federated Foundation Model for Time Series Forecasting","abstract":"Unlike natural language processing and computer vision, the development of Foundation Models (FMs) for time series forecasting is blocked due to data scarcity. While recent efforts are focused on building such FMs by unlocking the potential of language models (LMs) for time series analysis, dedicated parameters for various downstream forecasting tasks need training, which hinders the common knowledge sharing across domains. Moreover, data owners may hesitate to share the access to local data due to privacy concerns and copyright protection, which makes it impossible to simply construct a FM on cross-domain training instances. To address these issues, we propose Time-FFM, a Federated Foundation Model for Time series forecasting by leveraging pretrained LMs. Specifically, we begin by transforming time series into the modality of text tokens. To bootstrap LMs for time series reasoning, we propose a prompt adaption module to determine domain-customized prompts dynamically instead of artificially. Given the data heterogeneity across domains, we design a personalized federated training strategy by learning global encoders and local prediction heads. Our comprehensive experiments indicate that Time-FFM outperforms state-of-the-arts and promises effective few-shot and zero-shot forecaster.","sentences":["Unlike natural language processing and computer vision, the development of Foundation Models (FMs) for time series forecasting is blocked due to data scarcity.","While recent efforts are focused on building such FMs by unlocking the potential of language models (LMs) for time series analysis, dedicated parameters for various downstream forecasting tasks need training, which hinders the common knowledge sharing across domains.","Moreover, data owners may hesitate to share the access to local data due to privacy concerns and copyright protection, which makes it impossible to simply construct a FM on cross-domain training instances.","To address these issues, we propose Time-FFM, a Federated Foundation Model for Time series forecasting by leveraging pretrained LMs.","Specifically, we begin by transforming time series into the modality of text tokens.","To bootstrap LMs for time series reasoning, we propose a prompt adaption module to determine domain-customized prompts dynamically instead of artificially.","Given the data heterogeneity across domains, we design a personalized federated training strategy by learning global encoders and local prediction heads.","Our comprehensive experiments indicate that Time-FFM outperforms state-of-the-arts and promises effective few-shot and zero-shot forecaster."],"url":"http://arxiv.org/abs/2405.14252v1","category":"cs.LG"}
{"created":"2024-05-23 07:28:26","title":"Identifying Breakdowns in Conversational Recommender Systems using User Simulation","abstract":"We present a methodology to systematically test conversational recommender systems with regards to conversational breakdowns. It involves examining conversations generated between the system and simulated users for a set of pre-defined breakdown types, extracting responsible conversational paths, and characterizing them in terms of the underlying dialogue intents. User simulation offers the advantages of simplicity, cost-effectiveness, and time efficiency for obtaining conversations where potential breakdowns can be identified. The proposed methodology can be used as diagnostic tool as well as a development tool to improve conversational recommendation systems. We apply our methodology in a case study with an existing conversational recommender system and user simulator, demonstrating that with just a few iterations, we can make the system more robust to conversational breakdowns.","sentences":["We present a methodology to systematically test conversational recommender systems with regards to conversational breakdowns.","It involves examining conversations generated between the system and simulated users for a set of pre-defined breakdown types, extracting responsible conversational paths, and characterizing them in terms of the underlying dialogue intents.","User simulation offers the advantages of simplicity, cost-effectiveness, and time efficiency for obtaining conversations where potential breakdowns can be identified.","The proposed methodology can be used as diagnostic tool as well as a development tool to improve conversational recommendation systems.","We apply our methodology in a case study with an existing conversational recommender system and user simulator, demonstrating that with just a few iterations, we can make the system more robust to conversational breakdowns."],"url":"http://arxiv.org/abs/2405.14249v1","category":"cs.IR"}
{"created":"2024-05-23 07:21:01","title":"NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation","abstract":"Point Cloud Interpolation confronts challenges from point sparsity, complex spatiotemporal dynamics, and the difficulty of deriving complete 3D point clouds from sparse temporal information. This paper presents NeuroGauss4D-PCI, which excels at modeling complex non-rigid deformations across varied dynamic scenes. The method begins with an iterative Gaussian cloud soft clustering module, offering structured temporal point cloud representations. The proposed temporal radial basis function Gaussian residual utilizes Gaussian parameter interpolation over time, enabling smooth parameter transitions and capturing temporal residuals of Gaussian distributions. Additionally, a 4D Gaussian deformation field tracks the evolution of these parameters, creating continuous spatiotemporal deformation fields. A 4D neural field transforms low-dimensional spatiotemporal coordinates ($x,y,z,t$) into a high-dimensional latent space. Finally, we adaptively and efficiently fuse the latent features from neural fields and the geometric features from Gaussian deformation fields. NeuroGauss4D-PCI outperforms existing methods in point cloud frame interpolation, delivering leading performance on both object-level (DHB) and large-scale autonomous driving datasets (NL-Drive), with scalability to auto-labeling and point cloud densification tasks. The source code is released at https://github.com/jiangchaokang/NeuroGauss4D-PCI.","sentences":["Point Cloud Interpolation confronts challenges from point sparsity, complex spatiotemporal dynamics, and the difficulty of deriving complete 3D point clouds from sparse temporal information.","This paper presents NeuroGauss4D-PCI, which excels at modeling complex non-rigid deformations across varied dynamic scenes.","The method begins with an iterative Gaussian cloud soft clustering module, offering structured temporal point cloud representations.","The proposed temporal radial basis function Gaussian residual utilizes Gaussian parameter interpolation over time, enabling smooth parameter transitions and capturing temporal residuals of Gaussian distributions.","Additionally, a 4D Gaussian deformation field tracks the evolution of these parameters, creating continuous spatiotemporal deformation fields.","A 4D neural field transforms low-dimensional spatiotemporal coordinates ($x,y,z,t$) into a high-dimensional latent space.","Finally, we adaptively and efficiently fuse the latent features from neural fields and the geometric features from Gaussian deformation fields.","NeuroGauss4D-PCI outperforms existing methods in point cloud frame interpolation, delivering leading performance on both object-level (DHB) and large-scale autonomous driving datasets (NL-Drive), with scalability to auto-labeling and point cloud densification tasks.","The source code is released at https://github.com/jiangchaokang/NeuroGauss4D-PCI."],"url":"http://arxiv.org/abs/2405.14241v1","category":"cs.CV"}
{"created":"2024-05-23 07:14:40","title":"Condensed-space methods for nonlinear programming on GPUs","abstract":"This paper explores two condensed-space interior-point methods to efficiently solve large-scale nonlinear programs on graphics processing units (GPUs). The interior-point method solves a sequence of symmetric indefinite linear systems, or Karush-Kuhn-Tucker (KKT) systems, which become increasingly ill-conditioned as we approach the solution. Solving a KKT system with traditional sparse factorization methods involve numerical pivoting, making parallelization difficult. A solution is to condense the KKT system into a symmetric positive-definite matrix and solve it with a Cholesky factorization, stable without pivoting. Although condensed KKT systems are more prone to ill-conditioning than the original ones, they exhibit structured ill-conditioning that mitigates the loss of accuracy. This paper compares the benefits of two recent condensed-space interior-point methods, HyKKT and LiftedKKT. We implement the two methods on GPUs using MadNLP.jl, an optimization solver interfaced with the NVIDIA sparse linear solver cuDSS and with the GPU-accelerated modeler ExaModels.jl. Our experiments on the PGLIB and the COPS benchmarks reveal that GPUs can attain up to a tenfold speed increase compared to CPUs when solving large-scale instances.","sentences":["This paper explores two condensed-space interior-point methods to efficiently solve large-scale nonlinear programs on graphics processing units (GPUs).","The interior-point method solves a sequence of symmetric indefinite linear systems, or Karush-Kuhn-Tucker (KKT) systems, which become increasingly ill-conditioned as we approach the solution.","Solving a KKT system with traditional sparse factorization methods involve numerical pivoting, making parallelization difficult.","A solution is to condense the KKT system into a symmetric positive-definite matrix and solve it with a Cholesky factorization, stable without pivoting.","Although condensed KKT systems are more prone to ill-conditioning than the original ones, they exhibit structured ill-conditioning that mitigates the loss of accuracy.","This paper compares the benefits of two recent condensed-space interior-point methods, HyKKT and LiftedKKT.","We implement the two methods on GPUs using MadNLP.jl, an optimization solver interfaced with the NVIDIA sparse linear solver cuDSS and with the GPU-accelerated modeler ExaModels.jl.","Our experiments on the PGLIB and the COPS benchmarks reveal that GPUs can attain up to a tenfold speed increase compared to CPUs when solving large-scale instances."],"url":"http://arxiv.org/abs/2405.14236v1","category":"math.OC"}
{"created":"2024-05-23 07:04:41","title":"FloodDamageCast: Building Flood Damage Nowcasting with Machine Learning and Data Augmentation","abstract":"Near-real time estimation of damage to buildings and infrastructure, referred to as damage nowcasting in this study, is crucial for empowering emergency responders to make informed decisions regarding evacuation orders and infrastructure repair priorities during disaster response and recovery. Here, we introduce FloodDamageCast, a machine learning framework tailored for property flood damage nowcasting. The framework leverages heterogeneous data to predict residential flood damage at a resolution of 500 meters by 500 meters within Harris County, Texas, during the 2017 Hurricane Harvey. To deal with data imbalance, FloodDamageCast incorporates a generative adversarial networks-based data augmentation coupled with an efficient machine learning model. The results demonstrate the model's ability to identify high-damage spatial areas that would be overlooked by baseline models. Insights gleaned from flood damage nowcasting can assist emergency responders to more efficiently identify repair needs, allocate resources, and streamline on-the-ground inspections, thereby saving both time and effort.","sentences":["Near-real time estimation of damage to buildings and infrastructure, referred to as damage nowcasting in this study, is crucial for empowering emergency responders to make informed decisions regarding evacuation orders and infrastructure repair priorities during disaster response and recovery.","Here, we introduce FloodDamageCast, a machine learning framework tailored for property flood damage nowcasting.","The framework leverages heterogeneous data to predict residential flood damage at a resolution of 500 meters by 500 meters within Harris County, Texas, during the 2017 Hurricane Harvey.","To deal with data imbalance, FloodDamageCast incorporates a generative adversarial networks-based data augmentation coupled with an efficient machine learning model.","The results demonstrate the model's ability to identify high-damage spatial areas that would be overlooked by baseline models.","Insights gleaned from flood damage nowcasting can assist emergency responders to more efficiently identify repair needs, allocate resources, and streamline on-the-ground inspections, thereby saving both time and effort."],"url":"http://arxiv.org/abs/2405.14232v1","category":"cs.LG"}
{"created":"2024-05-23 06:53:18","title":"DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis","abstract":"Diffusion models have achieved great success in image generation, with the backbone evolving from U-Net to Vision Transformers. However, the computational cost of Transformers is quadratic to the number of tokens, leading to significant challenges when dealing with high-resolution images. In this work, we propose Diffusion Mamba (DiM), which combines the efficiency of Mamba, a sequence model based on State Space Models (SSM), with the expressive power of diffusion models for efficient high-resolution image synthesis. To address the challenge that Mamba cannot generalize to 2D signals, we make several architecture designs including multi-directional scans, learnable padding tokens at the end of each row and column, and lightweight local feature enhancement. Our DiM architecture achieves inference-time efficiency for high-resolution images. In addition, to further improve training efficiency for high-resolution image generation with DiM, we investigate ``weak-to-strong'' training strategy that pretrains DiM on low-resolution images ($256\\times 256$) and then finetune it on high-resolution images ($512 \\times 512$). We further explore training-free upsampling strategies to enable the model to generate higher-resolution images (e.g., $1024\\times 1024$ and $1536\\times 1536$) without further fine-tuning. Experiments demonstrate the effectiveness and efficiency of our DiM.","sentences":["Diffusion models have achieved great success in image generation, with the backbone evolving from U-Net to Vision Transformers.","However, the computational cost of Transformers is quadratic to the number of tokens, leading to significant challenges when dealing with high-resolution images.","In this work, we propose Diffusion Mamba (DiM), which combines the efficiency of Mamba, a sequence model based on State Space Models (SSM), with the expressive power of diffusion models for efficient high-resolution image synthesis.","To address the challenge that Mamba cannot generalize to 2D signals, we make several architecture designs including multi-directional scans, learnable padding tokens at the end of each row and column, and lightweight local feature enhancement.","Our DiM architecture achieves inference-time efficiency for high-resolution images.","In addition, to further improve training efficiency for high-resolution image generation with DiM, we investigate ``weak-to-strong'' training strategy that pretrains DiM on low-resolution images ($256\\times 256$) and then finetune it on high-resolution images ($512 \\times 512$).","We further explore training-free upsampling strategies to enable the model to generate higher-resolution images (e.g., $1024\\times 1024$ and $1536\\times 1536$) without further fine-tuning.","Experiments demonstrate the effectiveness and efficiency of our DiM."],"url":"http://arxiv.org/abs/2405.14224v1","category":"cs.CV"}
{"created":"2024-05-23 06:46:26","title":"Metric distortion Under Probabilistic Voting","abstract":"Metric distortion in social choice provides a framework for assessing how well voting rules minimize social cost in scenarios where voters and candidates exist in a shared metric space, with voters submitting rankings and the rule outputting a single winner. We expand this framework to include probabilistic voting. Our extension encompasses a broad range of probability functions, including widely studied models like Plackett-Luce (PL) and Bradley-Terry, and a novel \"pairwise quantal voting\" model inspired by quantal response theory.   We demonstrate that distortion results under probabilistic voting better correspond with conventional intuitions regarding popular voting rules such as Plurality, Copeland, and Random Dictator (RD) than those under deterministic voting. For example, in the PL model with candidate strength inversely proportional to the square of their metric distance, we show that Copeland's distortion is at most 2, whereas that of RD is $\\Omega(\\sqrt{m})$ in large elections, where $m$ is the number of candidates. This contrasts sharply with the classical model, where RD beats Copeland with a distortion of 3 versus 5 [1].","sentences":["Metric distortion in social choice provides a framework for assessing how well voting rules minimize social cost in scenarios where voters and candidates exist in a shared metric space, with voters submitting rankings and the rule outputting a single winner.","We expand this framework to include probabilistic voting.","Our extension encompasses a broad range of probability functions, including widely studied models like Plackett-Luce (PL) and Bradley-Terry, and a novel \"pairwise quantal voting\" model inspired by quantal response theory.   ","We demonstrate that distortion results under probabilistic voting better correspond with conventional intuitions regarding popular voting rules such as Plurality, Copeland, and Random Dictator (RD) than those under deterministic voting.","For example, in the PL model with candidate strength inversely proportional to the square of their metric distance, we show that Copeland's distortion is at most 2, whereas that of RD is $\\Omega(\\sqrt{m})$ in large elections, where $m$ is the number of candidates.","This contrasts sharply with the classical model, where RD beats Copeland with a distortion of 3 versus 5","[1]."],"url":"http://arxiv.org/abs/2405.14223v1","category":"cs.GT"}
{"created":"2024-05-23 06:32:42","title":"RAQ-VAE: Rate-Adaptive Vector-Quantized Variational Autoencoder","abstract":"Vector Quantized Variational AutoEncoder (VQ-VAE) is an established technique in machine learning for learning discrete representations across various modalities. However, its scalability and applicability are limited by the need to retrain the model to adjust the codebook for different data or model scales. We introduce the Rate-Adaptive VQ-VAE (RAQ-VAE) framework, which addresses this challenge with two novel codebook representation methods: a model-based approach using a clustering-based technique on an existing well-trained VQ-VAE model, and a data-driven approach utilizing a sequence-to-sequence (Seq2Seq) model for variable-rate codebook generation. Our experiments demonstrate that RAQ-VAE achieves effective reconstruction performance across multiple rates, often outperforming conventional fixed-rate VQ-VAE models. This work enhances the adaptability and performance of VQ-VAEs, with broad applications in data reconstruction, generation, and computer vision tasks.","sentences":["Vector Quantized Variational AutoEncoder (VQ-VAE) is an established technique in machine learning for learning discrete representations across various modalities.","However, its scalability and applicability are limited by the need to retrain the model to adjust the codebook for different data or model scales.","We introduce the Rate-Adaptive VQ-VAE (RAQ-VAE) framework, which addresses this challenge with two novel codebook representation methods: a model-based approach using a clustering-based technique on an existing well-trained VQ-VAE model, and a data-driven approach utilizing a sequence-to-sequence (Seq2Seq) model for variable-rate codebook generation.","Our experiments demonstrate that RAQ-VAE achieves effective reconstruction performance across multiple rates, often outperforming conventional fixed-rate VQ-VAE models.","This work enhances the adaptability and performance of VQ-VAEs, with broad applications in data reconstruction, generation, and computer vision tasks."],"url":"http://arxiv.org/abs/2405.14222v1","category":"cs.LG"}
{"created":"2024-05-23 06:06:27","title":"An Empirical Comparison of Methods to Produce Business Statistics Using Non-Probability Data","abstract":"There is a growing trend among statistical agencies to explore non-probability data sources for producing more timely and detailed statistics, while reducing costs and respondent burden. Coverage and measurement error are two issues that may be present in such data. The imperfections may be corrected using available information relating to the population of interest, such as a census or a reference probability sample.   In this paper, we compare a wide range of existing methods for producing population estimates using a non-probability dataset through a simulation study based on a realistic business population. The study was conducted to examine the performance of the methods under different missingness and data quality assumptions. The results confirm the ability of the methods examined to address selection bias. When no measurement error is present in the non-probability dataset, a screening dual-frame approach for the probability sample tends to yield lower sample size and mean squared error results. The presence of measurement error and/or nonignorable missingness increases mean squared errors for estimators that depend heavily on the non-probability data. In this case, the best approach tends to be to fall back to a model-assisted estimator based on the probability sample.","sentences":["There is a growing trend among statistical agencies to explore non-probability data sources for producing more timely and detailed statistics, while reducing costs and respondent burden.","Coverage and measurement error are two issues that may be present in such data.","The imperfections may be corrected using available information relating to the population of interest, such as a census or a reference probability sample.   ","In this paper, we compare a wide range of existing methods for producing population estimates using a non-probability dataset through a simulation study based on a realistic business population.","The study was conducted to examine the performance of the methods under different missingness and data quality assumptions.","The results confirm the ability of the methods examined to address selection bias.","When no measurement error is present in the non-probability dataset, a screening dual-frame approach for the probability sample tends to yield lower sample size and mean squared error results.","The presence of measurement error and/or nonignorable missingness increases mean squared errors for estimators that depend heavily on the non-probability data.","In this case, the best approach tends to be to fall back to a model-assisted estimator based on the probability sample."],"url":"http://arxiv.org/abs/2405.14208v1","category":"stat.ME"}
{"created":"2024-05-23 06:02:32","title":"Multi-instrument analysis of L-band amplitude scintillation observed over the Eastern Arabian Peninsula","abstract":"This study investigates the spatial and temporal characteristics of L1 amplitude scintillation-causing ionospheric irregularities over the Eastern Arabian Peninsula during the ascending phase of solar cycle 25 (years 2020--2023). The temporal occurrences of weak and strong scintillation were separated by sunset, with weak scintillation observed predominantly pre-sunset during the winter solstice and strong scintillation observed mainly post-sunset during the autumnal equinox. Strong scintillation was much more pronounced in 2023 compared to the other three years, indicating a strong influence of solar activity. Spatially, weak-scintillation-causing irregularities exhibited a wide distribution in azimuth and elevation, while strong-scintillation-causing irregularities were concentrated southwards. The combined analysis of S4 and rate of total electron content index (ROTI) suggested that small-scale ionospheric irregularities were present in both pre- and post-sunset periods, while large-scale irregularities were only seen during the post-sunset period. Furthermore, the presence of southward traveling ionospheric disturbances (TIDs) during the 2023 autumnal equinox was confirmed with the total electron content anomaly ($\\Delta\\text{TEC}$), while the Ionospheric Bubble Index (IBI) provided by the Swarm mission was unable to confirm the presence of equatorial plasma bubbles during the same period. Observations from the FORMOSAT-7/COSMIC-2 mission indicated that strong-scintillation-causing irregularities were more prevalent under the F2-layer peak, while the weak-scintillation-causing irregularities were mostly observed at the E-layer, F2-layer, and above the F2-layer.","sentences":["This study investigates the spatial and temporal characteristics of L1 amplitude scintillation-causing ionospheric irregularities over the Eastern Arabian Peninsula during the ascending phase of solar cycle 25 (years 2020--2023).","The temporal occurrences of weak and strong scintillation were separated by sunset, with weak scintillation observed predominantly pre-sunset during the winter solstice and strong scintillation observed mainly post-sunset during the autumnal equinox.","Strong scintillation was much more pronounced in 2023 compared to the other three years, indicating a strong influence of solar activity.","Spatially, weak-scintillation-causing irregularities exhibited a wide distribution in azimuth and elevation, while strong-scintillation-causing irregularities were concentrated southwards.","The combined analysis of S4 and rate of total electron content index (ROTI) suggested that small-scale ionospheric irregularities were present in both pre- and post-sunset periods, while large-scale irregularities were only seen during the post-sunset period.","Furthermore, the presence of southward traveling ionospheric disturbances (TIDs) during the 2023 autumnal equinox was confirmed with the total electron content anomaly ($\\Delta\\text{TEC}$), while the Ionospheric Bubble Index (IBI) provided by the Swarm mission was unable to confirm the presence of equatorial plasma bubbles during the same period.","Observations from the FORMOSAT-7/COSMIC-2 mission indicated that strong-scintillation-causing irregularities were more prevalent under the F2-layer peak, while the weak-scintillation-causing irregularities were mostly observed at the E-layer, F2-layer, and above the F2-layer."],"url":"http://arxiv.org/abs/2405.14204v1","category":"physics.ao-ph"}
{"created":"2024-05-23 05:34:31","title":"S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models","abstract":"Large Language Models have gained considerable attention for their revolutionary capabilities. However, there is also growing concern on their safety implications, making a comprehensive safety evaluation for LLMs urgently needed before model deployment. In this work, we propose S-Eval, a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark. At the core of S-Eval is a novel LLM-based automatic test prompt generation and selection framework, which trains an expert testing LLM Mt combined with a range of test selection strategies to automatically construct a high-quality test suite for the safety evaluation. The key to the automation of this process is a novel expert safety-critique LLM Mc able to quantify the riskiness score of a LLM's response, and additionally produce risk tags and explanations. Besides, the generation process is also guided by a carefully designed risk taxonomy with four different levels, covering comprehensive and multi-dimensional safety risks of concern. Based on these, we systematically construct a new and large-scale safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts, including 20,000 base risk prompts (10,000 in Chinese and 10,000 in English) and 200, 000 corresponding attack prompts derived from 10 popular adversarial instruction attacks against LLMs. Moreover, considering the rapid evolution of LLMs and accompanied safety threats, S-Eval can be flexibly configured and adapted to include new risks, attacks and models. S-Eval is extensively evaluated on 20 popular and representative LLMs. The results confirm that S-Eval can better reflect and inform the safety risks of LLMs compared to existing benchmarks. We also explore the impacts of parameter scales, language environments, and decoding parameters on the evaluation, providing a systematic methodology for evaluating the safety of LLMs.","sentences":["Large Language Models have gained considerable attention for their revolutionary capabilities.","However, there is also growing concern on their safety implications, making a comprehensive safety evaluation for LLMs urgently needed before model deployment.","In this work, we propose S-Eval, a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark.","At the core of S-Eval is a novel LLM-based automatic test prompt generation and selection framework, which trains an expert testing LLM Mt combined with a range of test selection strategies to automatically construct a high-quality test suite for the safety evaluation.","The key to the automation of this process is a novel expert safety-critique LLM Mc able to quantify the riskiness score of a LLM's response, and additionally produce risk tags and explanations.","Besides, the generation process is also guided by a carefully designed risk taxonomy with four different levels, covering comprehensive and multi-dimensional safety risks of concern.","Based on these, we systematically construct a new and large-scale safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts, including 20,000 base risk prompts (10,000 in Chinese and 10,000 in English) and 200, 000 corresponding attack prompts derived from 10 popular adversarial instruction attacks against LLMs.","Moreover, considering the rapid evolution of LLMs and accompanied safety threats, S-Eval can be flexibly configured and adapted to include new risks, attacks and models.","S-Eval is extensively evaluated on 20 popular and representative LLMs.","The results confirm that S-Eval can better reflect and inform the safety risks of LLMs compared to existing benchmarks.","We also explore the impacts of parameter scales, language environments, and decoding parameters on the evaluation, providing a systematic methodology for evaluating the safety of LLMs."],"url":"http://arxiv.org/abs/2405.14191v1","category":"cs.CR"}
{"created":"2024-05-23 05:29:53","title":"Bifunctional Noble Metal-free Ternary Chalcogenide Electrocatalysts for Overall Water Splitting","abstract":"Hydrogen has been identified as a clean, zero carbon, sustainable, and promising energy source for the future, and electrochemical water splitting for hydrogen production is an emission-free, efficient energy conversion technology. A major limitation of this approach is the unavailability of efficient, abundant, inexpensive catalysts, which prompts the need for new catalytic materials. Here, we report the synthesis and electrocatalytic properties of a novel transition metal-based ternary chalcogenide family, LaMS$_3$ (M = Mn, Fe, Co, Ni). Powder X-ray diffraction confirms the phase purity of these materials, while composition analysis using energy dispersive spectroscopy (EDS) confirms the presence of the stoichiometric ratio of elements in these compounds. X-ray photoelectron spectroscopy (XPS) and X-ray absorption spectroscopy (XAS) were used to study the chemical states on the surface and in bulk, respectively. These materials exhibit bifunctional catalytic activity towards the two half-reactions of the water-splitting process, with LaNiS$_3$ being the most active material for both hydrogen evolution reaction (HER) and oxygen evolution reaction (OER). The LaMS$_3$ compounds show long-term stability with negligible change in the overpotential at a constant current density of 10 mA cm$^{-2}$ over 18 hours of measurements. As compared to the corresponding ternary oxides, the LaMS$_3$ materials exhibit higher activity and significantly lower Tafel slopes. The ability to catalyze both half-reactions of water electrolysis makes these materials promising candidates for bifunctional catalysts and presents a new avenue to search for high-efficiency electrocatalysts for water splitting.","sentences":["Hydrogen has been identified as a clean, zero carbon, sustainable, and promising energy source for the future, and electrochemical water splitting for hydrogen production is an emission-free, efficient energy conversion technology.","A major limitation of this approach is the unavailability of efficient, abundant, inexpensive catalysts, which prompts the need for new catalytic materials.","Here, we report the synthesis and electrocatalytic properties of a novel transition metal-based ternary chalcogenide family, LaMS$_3$ (M = Mn, Fe, Co, Ni).","Powder X-ray diffraction confirms the phase purity of these materials, while composition analysis using energy dispersive spectroscopy (EDS) confirms the presence of the stoichiometric ratio of elements in these compounds.","X-ray photoelectron spectroscopy (XPS) and X-ray absorption spectroscopy (XAS) were used to study the chemical states on the surface and in bulk, respectively.","These materials exhibit bifunctional catalytic activity towards the two half-reactions of the water-splitting process, with LaNiS$_3$ being the most active material for both hydrogen evolution reaction (HER) and oxygen evolution reaction (OER).","The LaMS$_3$ compounds show long-term stability with negligible change in the overpotential at a constant current density of 10 mA cm$^{-2}$ over 18 hours of measurements.","As compared to the corresponding ternary oxides, the LaMS$_3$ materials exhibit higher activity and significantly lower Tafel slopes.","The ability to catalyze both half-reactions of water electrolysis makes these materials promising candidates for bifunctional catalysts and presents a new avenue to search for high-efficiency electrocatalysts for water splitting."],"url":"http://arxiv.org/abs/2405.14187v1","category":"physics.chem-ph"}
{"created":"2024-05-23 05:27:51","title":"Deterministic Policies for Constrained Reinforcement Learning in Polynomial-Time","abstract":"We present a novel algorithm that efficiently computes near-optimal deterministic policies for constrained reinforcement learning (CRL) problems. Our approach combines three key ideas: (1) value-demand augmentation, (2) action-space approximate dynamic programming, and (3) time-space rounding. Under mild reward assumptions, our algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for a diverse class of cost criteria. This class requires that the cost of a policy can be computed recursively over both time and (state) space, which includes classical expectation, almost sure, and anytime constraints. Our work not only provides provably efficient algorithms to address real-world challenges in decision-making but also offers a unifying theory for the efficient computation of constrained deterministic policies.","sentences":["We present a novel algorithm that efficiently computes near-optimal deterministic policies for constrained reinforcement learning (CRL) problems.","Our approach combines three key ideas: (1) value-demand augmentation, (2) action-space approximate dynamic programming, and (3) time-space rounding.","Under mild reward assumptions, our algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for a diverse class of cost criteria.","This class requires that the cost of a policy can be computed recursively over both time and (state) space, which includes classical expectation, almost sure, and anytime constraints.","Our work not only provides provably efficient algorithms to address real-world challenges in decision-making but also offers a unifying theory for the efficient computation of constrained deterministic policies."],"url":"http://arxiv.org/abs/2405.14183v1","category":"cs.LG"}
{"created":"2024-05-23 05:00:24","title":"Subdivision of KLRW Algebras in Affine Type A","abstract":"In this paper, we consider the subdivision map between two KLRW algebras of type $A^{(1)}_e$ and $A^{(1)}_{e+1}$. We show that the image of an idempotent indexed by a partition under this map is still an idempotent indexed by a partition, and give the form of this new partition. Moreover, we give an equality of some graded decomposition numbers.","sentences":["In this paper, we consider the subdivision map between two KLRW algebras of type $A^{(1)}_e$ and $A^{(1)}_{e+1}$. We show that the image of an idempotent indexed by a partition under this map is still an idempotent indexed by a partition, and give the form of this new partition.","Moreover, we give an equality of some graded decomposition numbers."],"url":"http://arxiv.org/abs/2405.14175v1","category":"math.RT"}
{"created":"2024-05-23 04:57:55","title":"Automated Optimal Layout Generator for Animal Shelters: A framework based on Genetic Algorithm, TOPSIS and Graph Theory","abstract":"Overpopulation in animal shelters contributes to increased disease spread and higher expenses on animal healthcare, leading to fewer adoptions and more shelter deaths. Additionally, one of the greatest challenges that shelters face is the noise level in the dog kennel area, which is physically and physiologically hazardous for both animals and staff. This paper proposes a multi-criteria optimization framework to automatically design cage layouts that maximize shelter capacity, minimize tension in the dog kennel area by reducing the number of cages facing each other, and ensure accessibility for staff and visitors. The proposed framework uses a Genetic Algorithm (GA) to systematically generate and improve layouts. A novel graph theory-based algorithm is introduced to process solutions and calculate fitness values. Additionally, the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) is used to rank and sort the layouts in each iteration. The graph-based algorithm calculates variables such as cage accessibility and shortest paths to access points. Furthermore, a heuristic algorithm is developed to calculate layout scores based on the number of cages facing each other. This framework provides animal shelter management with a flexible decision-support system that allows for different strategies by assigning various weights to the TOPSIS criteria. Results from cats' and dogs' kennel areas show that the proposed framework can suggest optimal layouts that respect different priorities within acceptable runtimes.","sentences":["Overpopulation in animal shelters contributes to increased disease spread and higher expenses on animal healthcare, leading to fewer adoptions and more shelter deaths.","Additionally, one of the greatest challenges that shelters face is the noise level in the dog kennel area, which is physically and physiologically hazardous for both animals and staff.","This paper proposes a multi-criteria optimization framework to automatically design cage layouts that maximize shelter capacity, minimize tension in the dog kennel area by reducing the number of cages facing each other, and ensure accessibility for staff and visitors.","The proposed framework uses a Genetic Algorithm (GA) to systematically generate and improve layouts.","A novel graph theory-based algorithm is introduced to process solutions and calculate fitness values.","Additionally, the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) is used to rank and sort the layouts in each iteration.","The graph-based algorithm calculates variables such as cage accessibility and shortest paths to access points.","Furthermore, a heuristic algorithm is developed to calculate layout scores based on the number of cages facing each other.","This framework provides animal shelter management with a flexible decision-support system that allows for different strategies by assigning various weights to the TOPSIS criteria.","Results from cats' and dogs' kennel areas show that the proposed framework can suggest optimal layouts that respect different priorities within acceptable runtimes."],"url":"http://arxiv.org/abs/2405.14172v1","category":"cs.NE"}
{"created":"2024-05-23 04:57:41","title":"Multi-view Remote Sensing Image Segmentation With SAM priors","abstract":"Multi-view segmentation in Remote Sensing (RS) seeks to segment images from diverse perspectives within a scene. Recent methods leverage 3D information extracted from an Implicit Neural Field (INF), bolstering result consistency across multiple views while using limited accounts of labels (even within 3-5 labels) to streamline labor. Nonetheless, achieving superior performance within the constraints of limited-view labels remains challenging due to inadequate scene-wide supervision and insufficient semantic features within the INF. To address these. we propose to inject the prior of the visual foundation model-Segment Anything(SAM), to the INF to obtain better results under the limited number of training data. Specifically, we contrast SAM features between testing and training views to derive pseudo labels for each testing view, augmenting scene-wide labeling information. Subsequently, we introduce SAM features via a transformer into the INF of the scene, supplementing the semantic information. The experimental results demonstrate that our method outperforms the mainstream method, confirming the efficacy of SAM as a supplement to the INF for this task.","sentences":["Multi-view segmentation in Remote Sensing (RS) seeks to segment images from diverse perspectives within a scene.","Recent methods leverage 3D information extracted from an Implicit Neural Field (INF), bolstering result consistency across multiple views while using limited accounts of labels (even within 3-5 labels) to streamline labor.","Nonetheless, achieving superior performance within the constraints of limited-view labels remains challenging due to inadequate scene-wide supervision and insufficient semantic features within the INF.","To address these.","we propose to inject the prior of the visual foundation model-Segment Anything(SAM), to the INF to obtain better results under the limited number of training data.","Specifically, we contrast SAM features between testing and training views to derive pseudo labels for each testing view, augmenting scene-wide labeling information.","Subsequently, we introduce SAM features via a transformer into the INF of the scene, supplementing the semantic information.","The experimental results demonstrate that our method outperforms the mainstream method, confirming the efficacy of SAM as a supplement to the INF for this task."],"url":"http://arxiv.org/abs/2405.14171v1","category":"cs.CV"}
{"created":"2024-05-23 04:52:02","title":"Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography","abstract":"Vision-Large-Language-Models (Vision-LLMs) are increasingly being integrated into autonomous driving (AD) systems due to their advanced visual-language reasoning capabilities, targeting the perception, prediction, planning, and control mechanisms. However, Vision-LLMs have demonstrated susceptibilities against various types of adversarial attacks, which would compromise their reliability and safety. To further explore the risk in AD systems and the transferability of practical threats, we propose to leverage typographic attacks against AD systems relying on the decision-making capabilities of Vision-LLMs. Different from the few existing works developing general datasets of typographic attacks, this paper focuses on realistic traffic scenarios where these attacks can be deployed, on their potential effects on the decision-making autonomy, and on the practical ways in which these attacks can be physically presented. To achieve the above goals, we first propose a dataset-agnostic framework for automatically generating false answers that can mislead Vision-LLMs' reasoning. Then, we present a linguistic augmentation scheme that facilitates attacks at image-level and region-level reasoning, and we extend it with attack patterns against multiple reasoning tasks simultaneously. Based on these, we conduct a study on how these attacks can be realized in physical traffic scenarios. Through our empirical study, we evaluate the effectiveness, transferability, and realizability of typographic attacks in traffic scenes. Our findings demonstrate particular harmfulness of the typographic attacks against existing Vision-LLMs (e.g., LLaVA, Qwen-VL, VILA, and Imp), thereby raising community awareness of vulnerabilities when incorporating such models into AD systems. We will release our source code upon acceptance.","sentences":["Vision-Large-Language-Models (Vision-LLMs) are increasingly being integrated into autonomous driving (AD) systems due to their advanced visual-language reasoning capabilities, targeting the perception, prediction, planning, and control mechanisms.","However, Vision-LLMs have demonstrated susceptibilities against various types of adversarial attacks, which would compromise their reliability and safety.","To further explore the risk in AD systems and the transferability of practical threats, we propose to leverage typographic attacks against AD systems relying on the decision-making capabilities of Vision-LLMs.","Different from the few existing works developing general datasets of typographic attacks, this paper focuses on realistic traffic scenarios where these attacks can be deployed, on their potential effects on the decision-making autonomy, and on the practical ways in which these attacks can be physically presented.","To achieve the above goals, we first propose a dataset-agnostic framework for automatically generating false answers that can mislead Vision-LLMs' reasoning.","Then, we present a linguistic augmentation scheme that facilitates attacks at image-level and region-level reasoning, and we extend it with attack patterns against multiple reasoning tasks simultaneously.","Based on these, we conduct a study on how these attacks can be realized in physical traffic scenarios.","Through our empirical study, we evaluate the effectiveness, transferability, and realizability of typographic attacks in traffic scenes.","Our findings demonstrate particular harmfulness of the typographic attacks against existing Vision-LLMs (e.g., LLaVA, Qwen-VL, VILA, and Imp), thereby raising community awareness of vulnerabilities when incorporating such models into AD systems.","We will release our source code upon acceptance."],"url":"http://arxiv.org/abs/2405.14169v1","category":"cs.CV"}
{"created":"2024-05-23 04:39:59","title":"Optimal Bayesian predictive probability for delayed response in single-arm clinical trials with binary efficacy outcome","abstract":"In oncology, phase II or multiple expansion cohort trials are crucial for clinical development plans. This is because they aid in identifying potent agents with sufficient activity to continue development and confirm the proof of concept. Typically, these clinical trials are single-arm trials, with the primary endpoint being short-term treatment efficacy. Despite the development of several well-designed methodologies, there may be a practical impediment in that the endpoints may be observed within a sufficient time such that adaptive go/no-go decisions can be made in a timely manner at each interim monitoring. Specifically, Response Evaluation Criteria in Solid Tumors guideline defines a confirmed response and necessitates it in non-randomized trials, where the response is the primary endpoint. However, obtaining the confirmed outcome from all participants entered at interim monitoring may be time-consuming as non-responders should be followed up until the disease progresses. Thus, this study proposed an approach to accelerate the decision-making process that incorporated the outcome without confirmation by discounting its contribution to the decision-making framework using the generalized Bayes' theorem. Further, the behavior of the proposed approach was evaluated through a simple simulation study. The results demonstrated that the proposed approach made appropriate interim go/no-go decisions.","sentences":["In oncology, phase II or multiple expansion cohort trials are crucial for clinical development plans.","This is because they aid in identifying potent agents with sufficient activity to continue development and confirm the proof of concept.","Typically, these clinical trials are single-arm trials, with the primary endpoint being short-term treatment efficacy.","Despite the development of several well-designed methodologies, there may be a practical impediment in that the endpoints may be observed within a sufficient time such that adaptive go/no-go decisions can be made in a timely manner at each interim monitoring.","Specifically, Response Evaluation Criteria in Solid Tumors guideline defines a confirmed response and necessitates it in non-randomized trials, where the response is the primary endpoint.","However, obtaining the confirmed outcome from all participants entered at interim monitoring may be time-consuming as non-responders should be followed up until the disease progresses.","Thus, this study proposed an approach to accelerate the decision-making process that incorporated the outcome without confirmation by discounting its contribution to the decision-making framework using the generalized Bayes' theorem.","Further, the behavior of the proposed approach was evaluated through a simple simulation study.","The results demonstrated that the proposed approach made appropriate interim go/no-go decisions."],"url":"http://arxiv.org/abs/2405.14166v1","category":"stat.ME"}
{"created":"2024-05-23 03:49:11","title":"A Direct Importance Sampling-based Framework for Rare Event Uncertainty Quantification in Non-Gaussian Spaces","abstract":"This work introduces a novel framework for precisely and efficiently estimating rare event probabilities in complex, high-dimensional non-Gaussian spaces, building on our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) approach. An unnormalized sampling target is first constructed and sampled, relaxing the optimal importance sampling distribution and appropriately designed for non-Gaussian spaces. Post-sampling, its normalizing constant is estimated using a stable inverse importance sampling procedure, employing an importance sampling density based on the already available samples. The sought probability is then computed based on the estimates evaluated in these two stages. The proposed estimator is theoretically analyzed, proving its unbiasedness and deriving its analytical coefficient of variation. To sample the constructed target, we resort to our developed Quasi-Newton mass preconditioned Hamiltonian MCMC (QNp-HMCMC) and we prove that it converges to the correct stationary target distribution. To avoid the challenging task of tuning the trajectory length in complex spaces, QNp-HMCMC is effectively utilized in this work with a single-step integration. We thus show the equivalence of QNp-HMCMC with single-step implementation to a unique and efficient preconditioned Metropolis-adjusted Langevin algorithm (MALA). An optimization approach is also leveraged to initiate QNp-HMCMC effectively, and the implementation of the developed framework in bounded spaces is eventually discussed. A series of diverse problems involving high dimensionality (several hundred inputs), strong nonlinearity, and non-Gaussianity is presented, showcasing the capabilities and efficiency of the suggested framework and demonstrating its advantages compared to relevant state-of-the-art sampling methods.","sentences":["This work introduces a novel framework for precisely and efficiently estimating rare event probabilities in complex, high-dimensional non-Gaussian spaces, building on our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) approach.","An unnormalized sampling target is first constructed and sampled, relaxing the optimal importance sampling distribution and appropriately designed for non-Gaussian spaces.","Post-sampling, its normalizing constant is estimated using a stable inverse importance sampling procedure, employing an importance sampling density based on the already available samples.","The sought probability is then computed based on the estimates evaluated in these two stages.","The proposed estimator is theoretically analyzed, proving its unbiasedness and deriving its analytical coefficient of variation.","To sample the constructed target, we resort to our developed Quasi-Newton mass preconditioned Hamiltonian MCMC (QNp-HMCMC) and we prove that it converges to the correct stationary target distribution.","To avoid the challenging task of tuning the trajectory length in complex spaces, QNp-HMCMC is effectively utilized in this work with a single-step integration.","We thus show the equivalence of QNp-HMCMC with single-step implementation to a unique and efficient preconditioned Metropolis-adjusted Langevin algorithm (MALA).","An optimization approach is also leveraged to initiate QNp-HMCMC effectively, and the implementation of the developed framework in bounded spaces is eventually discussed.","A series of diverse problems involving high dimensionality (several hundred inputs), strong nonlinearity, and non-Gaussianity is presented, showcasing the capabilities and efficiency of the suggested framework and demonstrating its advantages compared to relevant state-of-the-art sampling methods."],"url":"http://arxiv.org/abs/2405.14149v1","category":"stat.ME"}
{"created":"2024-05-23 03:38:00","title":"A Single Motor Nano Aerial Vehicle with Novel Peer-to-Peer Communication and Sensing Mechanism","abstract":"Communication and position sensing are among the most important capabilities for swarm robots to interact with their peers and perform tasks collaboratively. However, the hardware required to facilitate communication and position sensing is often too complicated, expensive, and bulky to be carried on swarm robots. Here we present Maneuverable Piccolissimo 3 (MP3), a minimalist, single motor drone capable of executing inter-robot communication via infrared light and triangulation-based sensing of relative bearing, distance, and elevation using message arrival time. Thanks to its novel design, MP3 can communicate with peers and localize itself using simple components, keeping its size and mass small and making it inherently safe for human interaction. Here we present the hardware and software design of MP3 and demonstrate its capability to localize itself, fly stably and maneuver in the environment using peer-to-peer communication and sensing.","sentences":["Communication and position sensing are among the most important capabilities for swarm robots to interact with their peers and perform tasks collaboratively.","However, the hardware required to facilitate communication and position sensing is often too complicated, expensive, and bulky to be carried on swarm robots.","Here we present Maneuverable Piccolissimo 3 (MP3), a minimalist, single motor drone capable of executing inter-robot communication via infrared light and triangulation-based sensing of relative bearing, distance, and elevation using message arrival time.","Thanks to its novel design, MP3 can communicate with peers and localize itself using simple components, keeping its size and mass small and making it inherently safe for human interaction.","Here we present the hardware and software design of MP3 and demonstrate its capability to localize itself, fly stably and maneuver in the environment using peer-to-peer communication and sensing."],"url":"http://arxiv.org/abs/2405.14144v1","category":"cs.RO"}
{"created":"2024-05-23 03:37:34","title":"A Geometric Perspective on the Closed Convex Hull of Some Spectral Sets","abstract":"We propose a geometric approach to characterize the closed convex hull of a spectral set $\\mathcal{S}$ under certain structural assumptions, where $\\mathcal{S}$ which is defined as the pre-image of a set $\\mathcal{C}\\subseteq\\mathbb{R}^n$ under the ``spectral map'' that includes the eigenvalue and singular-value maps as special cases. Our approach is conceptually and technically simple, and yields geometric characterizations of the closed convex hull of $\\mathcal{S}$ in a unified manner that works for all the spectral maps. From our results, we can easily recover the results in Kim et al. (2022) when the spectral map is the eigenvalue or singular-value map, and $\\mathcal{C}$ is permutation- and/or sign-invariant. Lastly, we discuss the polynomial computability of the membership and separation oracles associated with the (lifted) closed convex hull of $\\mathcal{S}$.","sentences":["We propose a geometric approach to characterize the closed convex hull of a spectral set $\\mathcal{S}$ under certain structural assumptions, where $\\mathcal{S}$ which is defined as the pre-image of a set $\\mathcal{C}\\subseteq\\mathbb{R}^n$ under the ``spectral map'' that includes the eigenvalue and singular-value maps as special cases.","Our approach is conceptually and technically simple, and yields geometric characterizations of the closed convex hull of $\\mathcal{S}$ in a unified manner that works for all the spectral maps.","From our results, we can easily recover the results in Kim et al.","(2022) when the spectral map is the eigenvalue or singular-value map, and $\\mathcal{C}$ is permutation- and/or sign-invariant.","Lastly, we discuss the polynomial computability of the membership and separation oracles associated with the (lifted) closed convex hull of $\\mathcal{S}$."],"url":"http://arxiv.org/abs/2405.14143v1","category":"math.OC"}
{"created":"2024-05-23 03:11:07","title":"Statistical Advantages of Perturbing Cosine Router in Sparse Mixture of Experts","abstract":"The cosine router in sparse Mixture of Experts (MoE) has recently emerged as an attractive alternative to the conventional linear router. Indeed, the cosine router demonstrates favorable performance in image and language tasks and exhibits better ability to mitigate the representation collapse issue, which often leads to parameter redundancy and limited representation potentials. Despite its empirical success, a comprehensive analysis of the cosine router in sparse MoE has been lacking. Considering the least square estimation of the cosine routing sparse MoE, we demonstrate that due to the intrinsic interaction of the model parameters in the cosine router via some partial differential equations, regardless of the structures of the experts, the estimation rates of experts and model parameters can be as slow as $\\mathcal{O}(1/\\log^{\\tau}(n))$ where $\\tau > 0$ is some constant and $n$ is the sample size. Surprisingly, these pessimistic non-polynomial convergence rates can be circumvented by the widely used technique in practice to stabilize the cosine router -- simply adding noises to the $\\mathbb{L}_{2}$ norms in the cosine router, which we refer to as \\textit{perturbed cosine router}. Under the strongly identifiable settings of the expert functions, we prove that the estimation rates for both the experts and model parameters under the perturbed cosine routing sparse MoE are significantly improved to polynomial rates. Finally, we conduct extensive simulation studies in both synthetic and real data settings to empirically validate our theoretical results.","sentences":["The cosine router in sparse Mixture of Experts (MoE) has recently emerged as an attractive alternative to the conventional linear router.","Indeed, the cosine router demonstrates favorable performance in image and language tasks and exhibits better ability to mitigate the representation collapse issue, which often leads to parameter redundancy and limited representation potentials.","Despite its empirical success, a comprehensive analysis of the cosine router in sparse MoE has been lacking.","Considering the least square estimation of the cosine routing sparse MoE, we demonstrate that due to the intrinsic interaction of the model parameters in the cosine router via some partial differential equations, regardless of the structures of the experts, the estimation rates of experts and model parameters can be as slow as $\\mathcal{O}(1/\\log^{\\tau}(n))$ where $\\tau > 0$ is some constant and $n$ is the sample size.","Surprisingly, these pessimistic non-polynomial convergence rates can be circumvented by the widely used technique in practice to stabilize the cosine router -- simply adding noises to the $\\mathbb{L}_{2}$ norms in the cosine router, which we refer to as \\textit{perturbed cosine router}.","Under the strongly identifiable settings of the expert functions, we prove that the estimation rates for both the experts and model parameters under the perturbed cosine routing sparse MoE are significantly improved to polynomial rates.","Finally, we conduct extensive simulation studies in both synthetic and real data settings to empirically validate our theoretical results."],"url":"http://arxiv.org/abs/2405.14131v1","category":"stat.ML"}
{"created":"2024-05-23 03:10:28","title":"High-probability complexity guarantees for nonconvex minimax problems","abstract":"Stochastic smooth nonconvex minimax problems are prevalent in machine learning, e.g., GAN training, fair classification, and distributionally robust learning. Stochastic gradient descent ascent (GDA)-type methods are popular in practice due to their simplicity and single-loop nature. However, there is a significant gap between the theory and practice regarding high-probability complexity guarantees for these methods on stochastic nonconvex minimax problems. Existing high-probability bounds for GDA-type single-loop methods only apply to convex/concave minimax problems and to particular non-monotone variational inequality problems under some restrictive assumptions. In this work, we address this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems corresponding to a smooth function that satisfies the PL-condition in the dual variable. Specifically, we show that when the stochastic gradients are light-tailed, the smoothed alternating GDA method can compute an $\\varepsilon$-stationary point within $\\mathcal{O}(\\frac{\\ell \\kappa^2 \\delta^2}{\\varepsilon^4} + \\frac{\\kappa}{\\varepsilon^2}(\\ell+\\delta^2\\log({1}/{\\bar{q}})))$ stochastic gradient calls with probability at least $1-\\bar{q}$ for any $\\bar{q}\\in(0,1)$, where $\\mu$ is the PL constant, $\\ell$ is the Lipschitz constant of the gradient, $\\kappa=\\ell/\\mu$ is the condition number, and $\\delta^2$ denotes a bound on the variance of stochastic gradients. We also present numerical results on a nonconvex/PL problem with synthetic data and on distributionally robust optimization problems with real data, illustrating our theoretical findings.","sentences":["Stochastic smooth nonconvex minimax problems are prevalent in machine learning, e.g., GAN training, fair classification, and distributionally robust learning.","Stochastic gradient descent ascent (GDA)-type methods are popular in practice due to their simplicity and single-loop nature.","However, there is a significant gap between the theory and practice regarding high-probability complexity guarantees for these methods on stochastic nonconvex minimax problems.","Existing high-probability bounds for GDA-type single-loop methods only apply to convex/concave minimax problems and to particular non-monotone variational inequality problems under some restrictive assumptions.","In this work, we address this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems corresponding to a smooth function that satisfies the PL-condition in the dual variable.","Specifically, we show that when the stochastic gradients are light-tailed, the smoothed alternating GDA method can compute an $\\varepsilon$-stationary point within $\\mathcal{O}(\\frac{\\ell \\kappa^2 \\delta^2}{\\varepsilon^4} + \\frac{\\kappa}{\\varepsilon^2}(\\ell+\\delta^2\\log({1}/{\\bar{q}})))$ stochastic gradient calls with probability at least $1-\\bar{q}$ for any $\\bar{q}\\in(0,1)$, where $\\mu$ is the PL constant, $\\ell$ is the Lipschitz constant of the gradient, $\\kappa=\\ell/\\mu$ is the condition number, and $\\delta^2$ denotes a bound on the variance of stochastic gradients.","We also present numerical results on a nonconvex/PL problem with synthetic data and on distributionally robust optimization problems with real data, illustrating our theoretical findings."],"url":"http://arxiv.org/abs/2405.14130v1","category":"math.OC"}
{"created":"2024-05-23 02:49:05","title":"Modeling Other Players with Bayesian Beliefs for Games with Incomplete Information","abstract":"Bayesian games model interactive decision-making where players have incomplete information -- e.g., regarding payoffs and private data on players' strategies and preferences -- and must actively reason and update their belief models (with regard to such information) using observation and interaction history. Existing work on counterfactual regret minimization have shown great success for games with complete or imperfect information, but not for Bayesian games. To this end, we introduced a new CFR algorithm: Bayesian-CFR and analyze its regret bound with respect to Bayesian Nash Equilibria in Bayesian games. First, we present a method for updating the posterior distribution of beliefs about the game and other players' types. The method uses a kernel-density estimate and is shown to converge to the true distribution. Second, we define Bayesian regret and present a Bayesian-CFR minimization algorithm for computing the Bayesian Nash equilibrium. Finally, we extend this new approach to other existing algorithms, such as Bayesian-CFR+ and Deep Bayesian CFR. Experimental results show that our proposed solutions significantly outperform existing methods in classical Texas Hold'em games.","sentences":["Bayesian games model interactive decision-making where players have incomplete information -- e.g., regarding payoffs and private data on players' strategies and preferences -- and must actively reason and update their belief models (with regard to such information) using observation and interaction history.","Existing work on counterfactual regret minimization have shown great success for games with complete or imperfect information, but not for Bayesian games.","To this end, we introduced a new CFR algorithm: Bayesian-CFR and analyze its regret bound with respect to Bayesian Nash Equilibria in Bayesian games.","First, we present a method for updating the posterior distribution of beliefs about the game and other players' types.","The method uses a kernel-density estimate and is shown to converge to the true distribution.","Second, we define Bayesian regret and present a Bayesian-CFR minimization algorithm for computing the Bayesian Nash equilibrium.","Finally, we extend this new approach to other existing algorithms, such as Bayesian-CFR+ and Deep Bayesian CFR.","Experimental results show that our proposed solutions significantly outperform existing methods in classical Texas Hold'em games."],"url":"http://arxiv.org/abs/2405.14122v1","category":"cs.GT"}
{"created":"2024-05-23 02:44:19","title":"Microwave Quantum Illumination with Optical Memory and Single-Mode Phase-Conjugate Receiver","abstract":"Microwave quantum illumination with entangled pairs of microwave signal and optical idler modes, can achieve the sub-optimal performance with joint measurement of the signal and idler modes. Here, we first propose a testbed of microwave quantum illumination with an optical memory which is simulated with a delay line in the idler mode. It provides how much an input two-mode squeezing is necessary to compensate the loss of the optical memory, while maintaining quantum advantage over coherent state. When the memory is lossy, the input two-mode squeezing has to be higher through high cooperativity in the optical mode. Under the testbed, we propose a single-mode phase conjugate receiver that consists of a low-reflectivity beam splitter, an electro-optomechanical phase conjugator, and a photon number resolving detector. The performance of the newly proposed receiver approaches the maximum quantum advantage for local measurement. Furthermore, the quantum advantage is obtained even with an on-off detection while being robust against the loss of the memory.","sentences":["Microwave quantum illumination with entangled pairs of microwave signal and optical idler modes, can achieve the sub-optimal performance with joint measurement of the signal and idler modes.","Here, we first propose a testbed of microwave quantum illumination with an optical memory which is simulated with a delay line in the idler mode.","It provides how much an input two-mode squeezing is necessary to compensate the loss of the optical memory, while maintaining quantum advantage over coherent state.","When the memory is lossy, the input two-mode squeezing has to be higher through high cooperativity in the optical mode.","Under the testbed, we propose a single-mode phase conjugate receiver that consists of a low-reflectivity beam splitter, an electro-optomechanical phase conjugator, and a photon number resolving detector.","The performance of the newly proposed receiver approaches the maximum quantum advantage for local measurement.","Furthermore, the quantum advantage is obtained even with an on-off detection while being robust against the loss of the memory."],"url":"http://arxiv.org/abs/2405.14118v1","category":"quant-ph"}
{"created":"2024-05-23 02:43:31","title":"Learning Multimodal Confidence for Intention Recognition in Human-Robot Interaction","abstract":"The rapid development of collaborative robotics has provided a new possibility of helping the elderly who has difficulties in daily life, allowing robots to operate according to specific intentions. However, efficient human-robot cooperation requires natural, accurate and reliable intention recognition in shared environments. The current paramount challenge for this is reducing the uncertainty of multimodal fused intention to be recognized and reasoning adaptively a more reliable result despite current interactive condition. In this work we propose a novel learning-based multimodal fusion framework Batch Multimodal Confidence Learning for Opinion Pool (BMCLOP). Our approach combines Bayesian multimodal fusion method and batch confidence learning algorithm to improve accuracy, uncertainty reduction and success rate given the interactive condition. In particular, the generic and practical multimodal intention recognition framework can be easily extended further. Our desired assistive scenarios consider three modalities gestures, speech and gaze, all of which produce categorical distributions over all the finite intentions. The proposed method is validated with a six-DoF robot through extensive experiments and exhibits high performance compared to baselines.","sentences":["The rapid development of collaborative robotics has provided a new possibility of helping the elderly who has difficulties in daily life, allowing robots to operate according to specific intentions.","However, efficient human-robot cooperation requires natural, accurate and reliable intention recognition in shared environments.","The current paramount challenge for this is reducing the uncertainty of multimodal fused intention to be recognized and reasoning adaptively a more reliable result despite current interactive condition.","In this work we propose a novel learning-based multimodal fusion framework Batch Multimodal Confidence Learning for Opinion Pool (BMCLOP).","Our approach combines Bayesian multimodal fusion method and batch confidence learning algorithm to improve accuracy, uncertainty reduction and success rate given the interactive condition.","In particular, the generic and practical multimodal intention recognition framework can be easily extended further.","Our desired assistive scenarios consider three modalities gestures, speech and gaze, all of which produce categorical distributions over all the finite intentions.","The proposed method is validated with a six-DoF robot through extensive experiments and exhibits high performance compared to baselines."],"url":"http://arxiv.org/abs/2405.14116v1","category":"cs.RO"}
{"created":"2024-05-23 02:31:55","title":"Improving Generalization of Deep Neural Networks by Optimum Shifting","abstract":"Recent studies showed that the generalization of neural networks is correlated with the sharpness of the loss landscape, and flat minima suggests a better generalization ability than sharp minima. In this paper, we propose a novel method called \\emph{optimum shifting}, which changes the parameters of a neural network from a sharp minimum to a flatter one while maintaining the same training loss value. Our method is based on the observation that when the input and output of a neural network are fixed, the matrix multiplications within the network can be treated as systems of under-determined linear equations, enabling adjustment of parameters in the solution space, which can be simply accomplished by solving a constrained optimization problem. Furthermore, we introduce a practical stochastic optimum shifting technique utilizing the Neural Collapse theory to reduce computational costs and provide more degrees of freedom for optimum shifting. Extensive experiments (including classification and detection) with various deep neural network architectures on benchmark datasets demonstrate the effectiveness of our method.","sentences":["Recent studies showed that the generalization of neural networks is correlated with the sharpness of the loss landscape, and flat minima suggests a better generalization ability than sharp minima.","In this paper, we propose a novel method called \\emph{optimum shifting}, which changes the parameters of a neural network from a sharp minimum to a flatter one while maintaining the same training loss value.","Our method is based on the observation that when the input and output of a neural network are fixed, the matrix multiplications within the network can be treated as systems of under-determined linear equations, enabling adjustment of parameters in the solution space, which can be simply accomplished by solving a constrained optimization problem.","Furthermore, we introduce a practical stochastic optimum shifting technique utilizing the Neural Collapse theory to reduce computational costs and provide more degrees of freedom for optimum shifting.","Extensive experiments (including classification and detection) with various deep neural network architectures on benchmark datasets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.14111v1","category":"cs.LG"}
{"created":"2024-05-23 01:58:32","title":"A continuous perspective on the inertial corrected primal-dual proximal splitting","abstract":"We give a continuous perspective on the Inertial Corrected Primal-Dual Proximal Splitting (IC-PDPS) proposed by Valkonen ({\\it SIAM J. Optim.}, 30(2): 1391--1420, 2020) for solving saddle-point problems. The algorithm possesses nonergodic convergence rate and admits a tight preconditioned proximal point formulation which involves both inertia and additional correction. Based on new understandings on the relation between the discrete step size and rescaling effect, we rebuild IC-PDPS as a semi-implicit Euler scheme with respect to its iterative sequences and integrated parameters. This leads to two novel second-order ordinary differential equation (ODE) models that are equivalent under proper time transformation, and also provides an alternative interpretation from the continuous point of view. Besides, we present the convergence analysis of the Lagrangian gap along the continuous trajectory by using proper Lyapunov functions.","sentences":["We give a continuous perspective on the Inertial Corrected Primal-Dual Proximal Splitting (IC-PDPS) proposed by Valkonen ({\\it SIAM J. Optim.}, 30(2): 1391--1420, 2020) for solving saddle-point problems.","The algorithm possesses nonergodic convergence rate and admits a tight preconditioned proximal point formulation which involves both inertia and additional correction.","Based on new understandings on the relation between the discrete step size and rescaling effect, we rebuild IC-PDPS as a semi-implicit Euler scheme with respect to its iterative sequences and integrated parameters.","This leads to two novel second-order ordinary differential equation (ODE) models that are equivalent under proper time transformation, and also provides an alternative interpretation from the continuous point of view.","Besides, we present the convergence analysis of the Lagrangian gap along the continuous trajectory by using proper Lyapunov functions."],"url":"http://arxiv.org/abs/2405.14098v1","category":"math.OC"}
{"created":"2024-05-23 01:34:21","title":"Actively Learning Combinatorial Optimization Using a Membership Oracle","abstract":"We consider solving a combinatorial optimization problem with an unknown linear constraint using a membership oracle that, given a solution, determines whether it is feasible or infeasible with absolute certainty. The goal of the decision maker is to find the best possible solution subject to a budget on the number of oracle calls. Inspired by active learning based on Support Vector Machines (SVMs), we adapt a classical framework in order to solve the problem by learning and exploiting a surrogate linear constraint. The resulting new framework includes training a linear separator on the labeled points and selecting new points to be labeled, which is achieved by applying a sampling strategy and solving a 0-1 integer linear program. Following the active learning literature, one can consider using SVM as a linear classifier and the information-based sampling strategy known as Simple margin. We improve on both sides: we propose an alternative sampling strategy based on mixed-integer quadratic programming and a linear separation method inspired by an algorithm for convex optimization in the oracle model. We conduct experiments on the pure knapsack problem and on a college study plan problem from the literature to show how different linear separation methods and sampling strategies influence the quality of the results in terms of objective value.","sentences":["We consider solving a combinatorial optimization problem with an unknown linear constraint using a membership oracle that, given a solution, determines whether it is feasible or infeasible with absolute certainty.","The goal of the decision maker is to find the best possible solution subject to a budget on the number of oracle calls.","Inspired by active learning based on Support Vector Machines (SVMs), we adapt a classical framework in order to solve the problem by learning and exploiting a surrogate linear constraint.","The resulting new framework includes training a linear separator on the labeled points and selecting new points to be labeled, which is achieved by applying a sampling strategy and solving a 0-1 integer linear program.","Following the active learning literature, one can consider using SVM as a linear classifier and the information-based sampling strategy known as Simple margin.","We improve on both sides: we propose an alternative sampling strategy based on mixed-integer quadratic programming and a linear separation method inspired by an algorithm for convex optimization in the oracle model.","We conduct experiments on the pure knapsack problem and on a college study plan problem from the literature to show how different linear separation methods and sampling strategies influence the quality of the results in terms of objective value."],"url":"http://arxiv.org/abs/2405.14090v1","category":"cs.LG"}
{"created":"2024-05-23 01:34:12","title":"Improved Canonicalization for Model Agnostic Equivariance","abstract":"This work introduces a novel approach to achieving architecture-agnostic equivariance in deep learning, particularly addressing the limitations of traditional equivariant architectures and the inefficiencies of the existing architecture-agnostic methods. Building equivariant models using traditional methods requires designing equivariant versions of existing models and training them from scratch, a process that is both impractical and resource-intensive. Canonicalization has emerged as a promising alternative for inducing equivariance without altering model architecture, but it suffers from the need for highly expressive and expensive equivariant networks to learn canonical orientations accurately. We propose a new method that employs any non-equivariant network for canonicalization. Our method uses contrastive learning to efficiently learn a unique canonical orientation and offers more flexibility for the choice of canonicalization network. We empirically demonstrate that this approach outperforms existing methods in achieving equivariance for large pretrained models and significantly speeds up the canonicalization process, making it up to 2 times faster.","sentences":["This work introduces a novel approach to achieving architecture-agnostic equivariance in deep learning, particularly addressing the limitations of traditional equivariant architectures and the inefficiencies of the existing architecture-agnostic methods.","Building equivariant models using traditional methods requires designing equivariant versions of existing models and training them from scratch, a process that is both impractical and resource-intensive.","Canonicalization has emerged as a promising alternative for inducing equivariance without altering model architecture, but it suffers from the need for highly expressive and expensive equivariant networks to learn canonical orientations accurately.","We propose a new method that employs any non-equivariant network for canonicalization.","Our method uses contrastive learning to efficiently learn a unique canonical orientation and offers more flexibility for the choice of canonicalization network.","We empirically demonstrate that this approach outperforms existing methods in achieving equivariance for large pretrained models and significantly speeds up the canonicalization process, making it up to 2 times faster."],"url":"http://arxiv.org/abs/2405.14089v1","category":"cs.LG"}
{"created":"2024-05-23 01:05:19","title":"Laboratory-scale Perpendicular Collisionless Shock Generation and Ion Acceleration in Magnetized Head-on Colliding Plasmas","abstract":"Magnetized collisionless shocks drive particle acceleration broadly in space and astrophysics. We perform the first large-scale particle-in-cell simulations with realistic laboratory parameters (density, temperature, and velocity) to investigate the magnetized shock in head-on colliding plasmas with an applied magnetic field of tens of Tesla. It is shown that a perpendicular collisionless shock is formed with about fourfold density jump when two pre-magnetized flows collide. This shock is also characterized by rapid increase of neutron yield, triggered by the beam-beam nuclear reactions between injected deuterons and ones reflected by the shock. Distinct from the shocks arising from the interaction of injected flows with a magnetized background, the self-generated magnetic field in this colliding plasmas experiences a significant amplification due to the increasing diamagnetic current, approximately 30 times of upstream magnetic field. Moreover, we find that ions, regardless of whether they pass through or are reflected by the shock, can gain energy by the shock surfing acceleration, generating a power-law energy spectrum. In addition, we also demonstrate that the shock mediated only by filamentation instability cannot be generated under the prevailing unmagnetized experimental parameters. These results provide a direct connection of astrophysical field amplification to the magnetized shock formation and nonthermal ion generation.","sentences":["Magnetized collisionless shocks drive particle acceleration broadly in space and astrophysics.","We perform the first large-scale particle-in-cell simulations with realistic laboratory parameters (density, temperature, and velocity) to investigate the magnetized shock in head-on colliding plasmas with an applied magnetic field of tens of Tesla.","It is shown that a perpendicular collisionless shock is formed with about fourfold density jump when two pre-magnetized flows collide.","This shock is also characterized by rapid increase of neutron yield, triggered by the beam-beam nuclear reactions between injected deuterons and ones reflected by the shock.","Distinct from the shocks arising from the interaction of injected flows with a magnetized background, the self-generated magnetic field in this colliding plasmas experiences a significant amplification due to the increasing diamagnetic current, approximately 30 times of upstream magnetic field.","Moreover, we find that ions, regardless of whether they pass through or are reflected by the shock, can gain energy by the shock surfing acceleration, generating a power-law energy spectrum.","In addition, we also demonstrate that the shock mediated only by filamentation instability cannot be generated under the prevailing unmagnetized experimental parameters.","These results provide a direct connection of astrophysical field amplification to the magnetized shock formation and nonthermal ion generation."],"url":"http://arxiv.org/abs/2405.14081v1","category":"physics.plasm-ph"}
{"created":"2024-05-23 00:59:25","title":"Fano line shape metamorphosis in resonant two-photon ionization","abstract":"Two-photon atomic ionization driven by time-locked XUV and IR pulses allows to study dynamics of Fano resonances in time and energy domains. Different time evolution of the two interfering pathways leading to a Fano resonance can be exploited to turn the Fano profile of the two-photon XUV/IR ionization into a symmetric Gaussian once the directly ejected photoelectron leaves the parent ion and cannot any longer absorb an IR photon. This line shape transformation allows for the direct determination of the resonant lifetime from the spectroscopic measurements without need for an extremely fine energy resolution. Ubiquitous nature of Fano resonances makes this determination a universal tool in diverse quantum systems ranging from nuclei to nano-fabricated solids.","sentences":["Two-photon atomic ionization driven by time-locked XUV and IR pulses allows to study dynamics of Fano resonances in time and energy domains.","Different time evolution of the two interfering pathways leading to a Fano resonance can be exploited to turn the Fano profile of the two-photon XUV/IR ionization into a symmetric Gaussian once the directly ejected photoelectron leaves the parent ion and cannot any longer absorb an IR photon.","This line shape transformation allows for the direct determination of the resonant lifetime from the spectroscopic measurements without need for an extremely fine energy resolution.","Ubiquitous nature of Fano resonances makes this determination a universal tool in diverse quantum systems ranging from nuclei to nano-fabricated solids."],"url":"http://arxiv.org/abs/2405.14080v1","category":"physics.atom-ph"}
{"created":"2024-05-23 16:04:50","title":"A Duty-Cycle-Efficient Synchronization Protocol for Slotted-Aloha in LoRaWAN","abstract":"In the current context of massive IoT, the Pure-Aloha scheme used in LoRaWAN is reaching its limit, and Slotted-Aloha is being considered as an alternative, as it offers twice Pure-Aloha's packet success rate. It however requires synchronization across the nodes. In this paper, we propose a new slot structure adapted to devices with low quality clock, and a duty-cycle efficient synchronization protocol for LoRaWAN class A devices with the lowest overhead to date. We discuss the conditions of its integration into LoRaWAN. The experimental results confirm that it succeeds in tracking each device's synchronization state, identifying the exact moment they desynchronize and resynchronizing them. The proposed protocol is also proven to be more duty-cycle efficient than existing fixed-rate synchronization solutions.","sentences":["In the current context of massive IoT, the Pure-Aloha scheme used in LoRaWAN is reaching its limit, and Slotted-Aloha is being considered as an alternative, as it offers twice Pure-Aloha's packet success rate.","It however requires synchronization across the nodes.","In this paper, we propose a new slot structure adapted to devices with low quality clock, and a duty-cycle efficient synchronization protocol for LoRaWAN class A devices with the lowest overhead to date.","We discuss the conditions of its integration into LoRaWAN.","The experimental results confirm that it succeeds in tracking each device's synchronization state, identifying the exact moment they desynchronize and resynchronizing them.","The proposed protocol is also proven to be more duty-cycle efficient than existing fixed-rate synchronization solutions."],"url":"http://arxiv.org/abs/2405.14740v1","category":"cs.NI"}
{"created":"2024-05-23 15:34:53","title":"Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference","abstract":"Parameter-efficient fine-tuning (PEFT) has emerged as a popular approach for adapting pre-trained Vision Transformer (ViT) models to downstream applications. While current PEFT methods achieve parameter efficiency, they overlook GPU memory and time efficiency during both fine-tuning and inference, due to the repeated computation of redundant tokens in the ViT architecture. This falls short of practical requirements for downstream task adaptation. In this paper, we propose \\textbf{Sparse-Tuning}, a novel tuning paradigm that substantially enhances both fine-tuning and inference efficiency for pre-trained ViT models. Sparse-Tuning efficiently fine-tunes the pre-trained ViT by sparsely preserving the informative tokens and merging redundant ones, enabling the ViT to focus on the foreground while reducing computational costs on background regions in the images. To accurately distinguish informative tokens from uninformative ones, we introduce a tailored Dense Adapter, which establishes dense connections across different encoder layers in the ViT, thereby enhancing the representational capacity and quality of token sparsification. Empirical results on VTAB-1K, three complete image datasets, and two complete video datasets demonstrate that Sparse-Tuning reduces the GFLOPs to \\textbf{62\\%-70\\%} of the original ViT-B while achieving state-of-the-art performance. Source code is available at \\url{https://github.com/liuting20/Sparse-Tuning}.","sentences":["Parameter-efficient fine-tuning (PEFT) has emerged as a popular approach for adapting pre-trained Vision Transformer (ViT) models to downstream applications.","While current PEFT methods achieve parameter efficiency, they overlook GPU memory and time efficiency during both fine-tuning and inference, due to the repeated computation of redundant tokens in the ViT architecture.","This falls short of practical requirements for downstream task adaptation.","In this paper, we propose \\textbf{Sparse-Tuning}, a novel tuning paradigm that substantially enhances both fine-tuning and inference efficiency for pre-trained ViT models.","Sparse-Tuning efficiently fine-tunes the pre-trained ViT by sparsely preserving the informative tokens and merging redundant ones, enabling the ViT to focus on the foreground while reducing computational costs on background regions in the images.","To accurately distinguish informative tokens from uninformative ones, we introduce a tailored Dense Adapter, which establishes dense connections across different encoder layers in the ViT, thereby enhancing the representational capacity and quality of token sparsification.","Empirical results on VTAB-1K, three complete image datasets, and two complete video datasets demonstrate that Sparse-Tuning reduces the GFLOPs to \\textbf{62\\%-70\\%} of the original ViT-B while achieving state-of-the-art performance.","Source code is available at \\url{https://github.com/liuting20/Sparse-Tuning}."],"url":"http://arxiv.org/abs/2405.14700v1","category":"cs.CV"}
{"created":"2024-05-23 15:11:26","title":"Utilizing indicator functions with computational data to confirm nature of overlap in normal turbulent stresses: logarithmic or quarter-power","abstract":"Indicator functions of the streamwise normal-stress profiles (NSP), based on careful differentiation of some of the best direct numerical simulations (DNS) data from channel and pipe flows, over the range $550<Re_\\tau<16,000$, are examined to establish the existence and range in wall distances of either a logarithmic-trend segment or a $1/4$-power region. For the nine out of fifteen cases of DNS data we examined where $Re_\\tau<2,000$, the NSP did not contain either of the proposed trends. As $Re_\\tau$ exceeds around $2,000$ a $1/4$-power, reflecting the ``bounded-dissipation'' predictions of Chen \\& Sreenivasan and data analysis of Monkewitz , develops near $y^+=1,000$ and expands with Reynolds numbers extending to $1,000<y^+<10,000$ for $Re_\\tau$ around $15,000$. This range of $1/4$-power NSP corresponds to a range of outer-scaled $Y$ between around $0.3$ and $0.7$. The computational database examined did not include the zero-pressure-gradient boundary layer experiments at higher Reynolds numbers where the logarithmic trend in the NSP has been previously reported around $y^+$ of $1,000$ by Marusic et al. according to a ``wall-scaled eddy model''.","sentences":["Indicator functions of the streamwise normal-stress profiles (NSP), based on careful differentiation of some of the best direct numerical simulations (DNS) data from channel and pipe flows, over the range $550<Re_\\tau<16,000$, are examined to establish the existence and range in wall distances of either a logarithmic-trend segment or a $1/4$-power region.","For the nine out of fifteen cases of DNS data we examined where $Re_\\tau<2,000$, the NSP did not contain either of the proposed trends.","As $Re_\\tau$ exceeds around $2,000$ a $1/4$-power, reflecting the ``bounded-dissipation'' predictions of Chen \\& Sreenivasan and data analysis of Monkewitz , develops near $y^+=1,000$ and expands with Reynolds numbers extending to $1,000<y^+<10,000$ for $Re_\\tau$ around $15,000$. This range of $1/4$-power NSP corresponds to a range of outer-scaled $Y$ between around $0.3$ and $0.7$. The computational database examined did not include the zero-pressure-gradient boundary layer experiments at higher Reynolds numbers where the logarithmic trend in the NSP has been previously reported around $y^+$ of $1,000$ by Marusic et al.","according to a ``wall-scaled eddy model''."],"url":"http://arxiv.org/abs/2405.14675v1","category":"physics.flu-dyn"}
{"created":"2024-05-23 14:48:23","title":"The integration of heterogeneous resources in the CMS Submission Infrastructure for the LHC Run 3 and beyond","abstract":"While the computing landscape supporting LHC experiments is currently dominated by x86 processors at WLCG sites, this configuration will evolve in the coming years. LHC collaborations will be increasingly employing HPC and Cloud facilities to process the vast amounts of data expected during the LHC Run 3 and the future HL-LHC phase. These facilities often feature diverse compute resources, including alternative CPU architectures like ARM and IBM Power, as well as a variety of GPU specifications. Using these heterogeneous resources efficiently is thus essential for the LHC collaborations reaching their future scientific goals. The Submission Infrastructure (SI) is a central element in CMS Computing, enabling resource acquisition and exploitation by CMS data processing, simulation and analysis tasks. The SI must therefore be adapted to ensure access and optimal utilization of this heterogeneous compute capacity. Some steps in this evolution have been already taken, as CMS is currently using opportunistically a small pool of GPU slots provided mainly at the CMS WLCG sites. Additionally, Power9 processors have been validated for CMS production at the Marconi-100 cluster at CINECA. This note will describe the updated capabilities of the SI to continue ensuring the efficient allocation and use of computing resources by CMS, despite their increasing diversity. The next steps towards a full integration and support of heterogeneous resources according to CMS needs will also be reported.","sentences":["While the computing landscape supporting LHC experiments is currently dominated by x86 processors at WLCG sites, this configuration will evolve in the coming years.","LHC collaborations will be increasingly employing HPC and Cloud facilities to process the vast amounts of data expected during the LHC Run 3 and the future HL-LHC phase.","These facilities often feature diverse compute resources, including alternative CPU architectures like ARM and IBM Power, as well as a variety of GPU specifications.","Using these heterogeneous resources efficiently is thus essential for the LHC collaborations reaching their future scientific goals.","The Submission Infrastructure (SI) is a central element in CMS Computing, enabling resource acquisition and exploitation by CMS data processing, simulation and analysis tasks.","The SI must therefore be adapted to ensure access and optimal utilization of this heterogeneous compute capacity.","Some steps in this evolution have been already taken, as CMS is currently using opportunistically a small pool of GPU slots provided mainly at the CMS WLCG sites.","Additionally, Power9 processors have been validated for CMS production at the Marconi-100 cluster at CINECA.","This note will describe the updated capabilities of the SI to continue ensuring the efficient allocation and use of computing resources by CMS, despite their increasing diversity.","The next steps towards a full integration and support of heterogeneous resources according to CMS needs will also be reported."],"url":"http://arxiv.org/abs/2405.14647v1","category":"cs.DC"}
{"created":"2024-05-23 13:19:54","title":"Hyperlogarithms in the theory of turbulence of infinite dimension","abstract":"Parametric integration with hyperlogarithms so far has been successfully used in problems of high energy physics (HEP) and critical statics. In this work, for the first time, it is applied to a problem of critical dynamics, namely, a stochastic model of developed turbulence in high-dimensional spaces, which has a propagator that is non-standard with respect to the HEP: $(-i \\omega + \\nu k^2)^{-1}$. Adaptation of the hyperlogarithm method is carried out by choosing a proper renormalization scheme and considering an effective dimension of the space. Analytical calculation of the renormalization group functions is performed up to the fourth order of the perturbation theory, $\\varepsilon$-expansion of the critical exponent $\\omega$ responsible for the infrared stability of the fixed point is obtained.","sentences":["Parametric integration with hyperlogarithms so far has been successfully used in problems of high energy physics (HEP) and critical statics.","In this work, for the first time, it is applied to a problem of critical dynamics, namely, a stochastic model of developed turbulence in high-dimensional spaces, which has a propagator that is non-standard with respect to the HEP: $(-i \\omega + \\nu k^2)^{-1}$. Adaptation of the hyperlogarithm method is carried out by choosing a proper renormalization scheme and considering an effective dimension of the space.","Analytical calculation of the renormalization group functions is performed up to the fourth order of the perturbation theory, $\\varepsilon$-expansion of the critical exponent $\\omega$ responsible for the infrared stability of the fixed point is obtained."],"url":"http://arxiv.org/abs/2405.14533v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-23 11:46:03","title":"Semi-Discrete Optimal Transport: Nearly Minimax Estimation With Stochastic Gradient Descent and Adaptive Entropic Regularization","abstract":"Optimal Transport (OT) based distances are powerful tools for machine learning to compare probability measures and manipulate them using OT maps. In this field, a setting of interest is semi-discrete OT, where the source measure $\\mu$ is continuous, while the target $\\nu$ is discrete. Recent works have shown that the minimax rate for the OT map is $\\mathcal{O}(t^{-1/2})$ when using $t$ i.i.d. subsamples from each measure (two-sample setting). An open question is whether a better convergence rate can be achieved when the full information of the discrete measure $\\nu$ is known (one-sample setting). In this work, we answer positively to this question by (i) proving an $\\mathcal{O}(t^{-1})$ lower bound rate for the OT map, using the similarity between Laguerre cells estimation and density support estimation, and (ii) proposing a Stochastic Gradient Descent (SGD) algorithm with adaptive entropic regularization and averaging acceleration. To nearly achieve the desired fast rate, characteristic of non-regular parametric problems, we design an entropic regularization scheme decreasing with the number of samples. Another key step in our algorithm consists of using a projection step that permits to leverage the local strong convexity of the regularized OT problem. Our convergence analysis integrates online convex optimization and stochastic gradient techniques, complemented by the specificities of the OT semi-dual. Moreover, while being as computationally and memory efficient as vanilla SGD, our algorithm achieves the unusual fast rates of our theory in numerical experiments.","sentences":["Optimal Transport (OT) based distances are powerful tools for machine learning to compare probability measures and manipulate them using OT maps.","In this field, a setting of interest is semi-discrete OT, where the source measure $\\mu$ is continuous, while the target $\\nu$ is discrete.","Recent works have shown that the minimax rate for the OT map is $\\mathcal{O}(t^{-1/2})$ when using $t$ i.i.d.","subsamples from each measure (two-sample setting).","An open question is whether a better convergence rate can be achieved when the full information of the discrete measure $\\nu$ is known (one-sample setting).","In this work, we answer positively to this question by (i) proving an $\\mathcal{O}(t^{-1})$ lower bound rate for the OT map, using the similarity between Laguerre cells estimation and density support estimation, and (ii) proposing a Stochastic Gradient Descent (SGD) algorithm with adaptive entropic regularization and averaging acceleration.","To nearly achieve the desired fast rate, characteristic of non-regular parametric problems, we design an entropic regularization scheme decreasing with the number of samples.","Another key step in our algorithm consists of using a projection step that permits to leverage the local strong convexity of the regularized OT problem.","Our convergence analysis integrates online convex optimization and stochastic gradient techniques, complemented by the specificities of the OT semi-dual.","Moreover, while being as computationally and memory efficient as vanilla SGD, our algorithm achieves the unusual fast rates of our theory in numerical experiments."],"url":"http://arxiv.org/abs/2405.14459v1","category":"stat.ML"}
{"created":"2024-05-23 11:08:35","title":"Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models","abstract":"Recently, using large pretrained Transformer models for transfer learning tasks has evolved to the point where they have become one of the flagship trends in the Natural Language Processing (NLP) community, giving rise to various outlooks such as prompt-based, adapters or combinations with unsupervised approaches, among many others. This work proposes a 3 Phase technique to adjust a base model for a classification task. First, we adapt the model's signal to the data distribution by performing further training with a Denoising Autoencoder (DAE). Second, we adjust the representation space of the output to the corresponding classes by clustering through a Contrastive Learning (CL) method. In addition, we introduce a new data augmentation approach for Supervised Contrastive Learning to correct the unbalanced datasets. Third, we apply fine-tuning to delimit the predefined categories. These different phases provide relevant and complementary knowledge to the model to learn the final task. We supply extensive experimental results on several datasets to demonstrate these claims. Moreover, we include an ablation study and compare the proposed method against other ways of combining these techniques.","sentences":["Recently, using large pretrained Transformer models for transfer learning tasks has evolved to the point where they have become one of the flagship trends in the Natural Language Processing (NLP) community, giving rise to various outlooks such as prompt-based, adapters or combinations with unsupervised approaches, among many others.","This work proposes a 3 Phase technique to adjust a base model for a classification task.","First, we adapt the model's signal to the data distribution by performing further training with a Denoising Autoencoder (DAE).","Second, we adjust the representation space of the output to the corresponding classes by clustering through a Contrastive Learning (CL) method.","In addition, we introduce a new data augmentation approach for Supervised Contrastive Learning to correct the unbalanced datasets.","Third, we apply fine-tuning to delimit the predefined categories.","These different phases provide relevant and complementary knowledge to the model to learn the final task.","We supply extensive experimental results on several datasets to demonstrate these claims.","Moreover, we include an ablation study and compare the proposed method against other ways of combining these techniques."],"url":"http://arxiv.org/abs/2405.14437v1","category":"cs.CL"}
{"created":"2024-05-23 10:26:18","title":"Gradient Transformation: Towards Efficient and Model-Agnostic Unlearning for Dynamic Graph Neural Networks","abstract":"Graph unlearning has emerged as an essential tool for safeguarding user privacy and mitigating the negative impacts of undesirable data. Meanwhile, the advent of dynamic graph neural networks (DGNNs) marks a significant advancement due to their superior capability in learning from dynamic graphs, which encapsulate spatial-temporal variations in diverse real-world applications (e.g., traffic forecasting). With the increasing prevalence of DGNNs, it becomes imperative to investigate the implementation of dynamic graph unlearning. However, current graph unlearning methodologies are designed for GNNs operating on static graphs and exhibit limitations including their serving in a pre-processing manner and impractical resource demands. Furthermore, the adaptation of these methods to DGNNs presents non-trivial challenges, owing to the distinctive nature of dynamic graphs. To this end, we propose an effective, efficient, model-agnostic, and post-processing method to implement DGNN unlearning. Specifically, we first define the unlearning requests and formulate dynamic graph unlearning in the context of continuous-time dynamic graphs. After conducting a role analysis on the unlearning data, the remaining data, and the target DGNN model, we propose a method called Gradient Transformation and a loss function to map the unlearning request to the desired parameter update. Evaluations on six real-world datasets and state-of-the-art DGNN backbones demonstrate its effectiveness (e.g., limited performance drop even obvious improvement) and efficiency (e.g., at most 7.23$\\times$ speed-up) outperformance, and potential advantages in handling future unlearning requests (e.g., at most 32.59$\\times$ speed-up).","sentences":["Graph unlearning has emerged as an essential tool for safeguarding user privacy and mitigating the negative impacts of undesirable data.","Meanwhile, the advent of dynamic graph neural networks (DGNNs) marks a significant advancement due to their superior capability in learning from dynamic graphs, which encapsulate spatial-temporal variations in diverse real-world applications (e.g., traffic forecasting).","With the increasing prevalence of DGNNs, it becomes imperative to investigate the implementation of dynamic graph unlearning.","However, current graph unlearning methodologies are designed for GNNs operating on static graphs and exhibit limitations including their serving in a pre-processing manner and impractical resource demands.","Furthermore, the adaptation of these methods to DGNNs presents non-trivial challenges, owing to the distinctive nature of dynamic graphs.","To this end, we propose an effective, efficient, model-agnostic, and post-processing method to implement DGNN unlearning.","Specifically, we first define the unlearning requests and formulate dynamic graph unlearning in the context of continuous-time dynamic graphs.","After conducting a role analysis on the unlearning data, the remaining data, and the target DGNN model, we propose a method called Gradient Transformation and a loss function to map the unlearning request to the desired parameter update.","Evaluations on six real-world datasets and state-of-the-art DGNN backbones demonstrate its effectiveness (e.g., limited performance drop even obvious improvement) and efficiency (e.g., at most 7.23$\\times$ speed-up) outperformance, and potential advantages in handling future unlearning requests (e.g., at most 32.59$\\times$ speed-up)."],"url":"http://arxiv.org/abs/2405.14407v1","category":"cs.LG"}
{"created":"2024-05-23 10:21:57","title":"Qubit-efficient Variational Quantum Algorithms for Image Segmentation","abstract":"Quantum computing is expected to transform a range of computational tasks beyond the reach of classical algorithms. In this work, we examine the application of variational quantum algorithms (VQAs) for unsupervised image segmentation to partition images into separate semantic regions. Specifically, we formulate the task as a graph cut optimization problem and employ two established qubit-efficient VQAs, which we refer to as Parametric Gate Encoding (PGE) and Ancilla Basis Encoding (ABE), to find the optimal segmentation mask. In addition, we propose Adaptive Cost Encoding (ACE), a new approach that leverages the same circuit architecture as ABE but adopts a problem-dependent cost function. We benchmark PGE, ABE and ACE on synthetically generated images, focusing on quality and trainability. ACE shows consistently faster convergence in training the parameterized quantum circuits in comparison to PGE and ABE. Furthermore, we provide a theoretical analysis of the scalability of these approaches against the Quantum Approximate Optimization Algorithm (QAOA), showing a significant cutback in the quantum resources, especially in the number of qubits that logarithmically depends on the number of pixels. The results validate the strengths of ACE, while concurrently highlighting its inherent limitations and challenges. This paves way for further research in quantum-enhanced computer vision.","sentences":["Quantum computing is expected to transform a range of computational tasks beyond the reach of classical algorithms.","In this work, we examine the application of variational quantum algorithms (VQAs) for unsupervised image segmentation to partition images into separate semantic regions.","Specifically, we formulate the task as a graph cut optimization problem and employ two established qubit-efficient VQAs, which we refer to as Parametric Gate Encoding (PGE) and Ancilla Basis Encoding (ABE), to find the optimal segmentation mask.","In addition, we propose Adaptive Cost Encoding (ACE), a new approach that leverages the same circuit architecture as ABE but adopts a problem-dependent cost function.","We benchmark PGE, ABE and ACE on synthetically generated images, focusing on quality and trainability.","ACE shows consistently faster convergence in training the parameterized quantum circuits in comparison to PGE and ABE.","Furthermore, we provide a theoretical analysis of the scalability of these approaches against the Quantum Approximate Optimization Algorithm (QAOA), showing a significant cutback in the quantum resources, especially in the number of qubits that logarithmically depends on the number of pixels.","The results validate the strengths of ACE, while concurrently highlighting its inherent limitations and challenges.","This paves way for further research in quantum-enhanced computer vision."],"url":"http://arxiv.org/abs/2405.14405v1","category":"cs.CV"}
{"created":"2024-05-23 09:14:34","title":"Local precursors to anomalous dissipation in Navier-Stokes turbulence: Burgers vortex-type models and simulation analysis","abstract":"Anomalous dissipation is a dissipation mechanism of kinetic energy which is established by a sufficiently spatially rough velocity field. It implies that the rescaled mean kinetic energy dissipation rate becomes constant with respect to Reynolds number ${\\rm Re}$, the dimensionless parameter that characterizes the strength of turbulence, given that ${\\rm Re}\\gg 1$. The present study aims at bridging this statistical behavior of high-Reynolds-number turbulence to specific structural building blocks of fluid turbulence -- local vortex stretching configurations which take in the simplest case the form of Burgers' classical vortex stretching model from 1948. We discuss the anomalous dissipation in the framework of Duchon and Robert for this analytical solution of the Navier-Stokes equations, apply the same analysis subsequently to a generalized model of randomly oriented Burgers vortices by Kambe and Hatakeyama, and analyse finally direct numerical simulation data of three-dimensional homogeneous, isotropic box turbulence in this respect. We identify local high-vorticity events in fully developed Navier-Stokes turbulence that approximate the analytical models of strong vortex stretching well. They also correspond to precursors of enhanced anomalous dissipation.","sentences":["Anomalous dissipation is a dissipation mechanism of kinetic energy which is established by a sufficiently spatially rough velocity field.","It implies that the rescaled mean kinetic energy dissipation rate becomes constant with respect to Reynolds number ${\\rm Re}$, the dimensionless parameter that characterizes the strength of turbulence, given that ${\\rm Re}\\gg 1$.","The present study aims at bridging this statistical behavior of high-Reynolds-number turbulence to specific structural building blocks of fluid turbulence -- local vortex stretching configurations which take in the simplest case the form of Burgers' classical vortex stretching model from 1948.","We discuss the anomalous dissipation in the framework of Duchon and Robert for this analytical solution of the Navier-Stokes equations, apply the same analysis subsequently to a generalized model of randomly oriented Burgers vortices by Kambe and Hatakeyama, and analyse finally direct numerical simulation data of three-dimensional homogeneous, isotropic box turbulence in this respect.","We identify local high-vorticity events in fully developed Navier-Stokes turbulence that approximate the analytical models of strong vortex stretching well.","They also correspond to precursors of enhanced anomalous dissipation."],"url":"http://arxiv.org/abs/2405.14344v1","category":"physics.flu-dyn"}
{"created":"2024-05-23 08:43:09","title":"Adaptive Rentention & Correction for Continual Learning","abstract":"Continual learning, also known as lifelong learning or incremental learning, refers to the process by which a model learns from a stream of incoming data over time. A common problem in continual learning is the classification layer's bias towards the most recent task. Traditionally, methods have relied on incorporating data from past tasks during training to mitigate this issue. However, the recent shift in continual learning to memory-free environments has rendered these approaches infeasible. In this study, we propose a solution focused on the testing phase. We first introduce a simple Out-of-Task Detection method, OTD, designed to accurately identify samples from past tasks during testing. Leveraging OTD, we then propose: (1) an Adaptive Retention mechanism for dynamically tuning the classifier layer on past task data; (2) an Adaptive Correction mechanism for revising predictions when the model classifies data from previous tasks into classes from the current task. We name our approach Adaptive Retention & Correction (ARC). While designed for memory-free environments, ARC also proves effective in memory-based settings. Extensive experiments show that our proposed method can be plugged in to virtually any existing continual learning approach without requiring any modifications to its training procedure. Specifically, when integrated with state-of-the-art approaches, ARC achieves an average performance increase of 2.7% and 2.6% on the CIFAR-100 and Imagenet-R datasets, respectively.","sentences":["Continual learning, also known as lifelong learning or incremental learning, refers to the process by which a model learns from a stream of incoming data over time.","A common problem in continual learning is the classification layer's bias towards the most recent task.","Traditionally, methods have relied on incorporating data from past tasks during training to mitigate this issue.","However, the recent shift in continual learning to memory-free environments has rendered these approaches infeasible.","In this study, we propose a solution focused on the testing phase.","We first introduce a simple Out-of-Task Detection method, OTD, designed to accurately identify samples from past tasks during testing.","Leveraging OTD, we then propose: (1) an Adaptive Retention mechanism for dynamically tuning the classifier layer on past task data; (2) an Adaptive Correction mechanism for revising predictions when the model classifies data from previous tasks into classes from the current task.","We name our approach Adaptive Retention & Correction (ARC).","While designed for memory-free environments, ARC also proves effective in memory-based settings.","Extensive experiments show that our proposed method can be plugged in to virtually any existing continual learning approach without requiring any modifications to its training procedure.","Specifically, when integrated with state-of-the-art approaches, ARC achieves an average performance increase of 2.7% and 2.6% on the CIFAR-100 and Imagenet-R datasets, respectively."],"url":"http://arxiv.org/abs/2405.14318v1","category":"cs.CV"}
{"created":"2024-05-23 07:59:26","title":"Diffusion-based Quantum Error Mitigation using Stochastic Differential Equation","abstract":"Unlike closed systems, where the total energy and information are conserved within the system, open systems interact with the external environment which often leads to complex behaviors not seen in closed systems. The random fluctuations that arise due to the interaction with the external environment cause noise affecting the states of the quantum system, resulting in system errors. To effectively concern quantum error in open quantum systems, this paper introduces a novel approach to mitigate errors using diffusion models. This approach can be realized by noise occurrence formulation during the state evolution as forward-backward stochastic differential equations (FBSDE) and adapting the score-based generative model (SGM) to denoise errors in quantum states.","sentences":["Unlike closed systems, where the total energy and information are conserved within the system, open systems interact with the external environment which often leads to complex behaviors not seen in closed systems.","The random fluctuations that arise due to the interaction with the external environment cause noise affecting the states of the quantum system, resulting in system errors.","To effectively concern quantum error in open quantum systems, this paper introduces a novel approach to mitigate errors using diffusion models.","This approach can be realized by noise occurrence formulation during the state evolution as forward-backward stochastic differential equations (FBSDE) and adapting the score-based generative model (SGM) to denoise errors in quantum states."],"url":"http://arxiv.org/abs/2405.14283v1","category":"quant-ph"}
{"created":"2024-05-23 07:53:10","title":"SCMix: Stochastic Compound Mixing for Open Compound Domain Adaptation in Semantic Segmentation","abstract":"Open compound domain adaptation (OCDA) aims to transfer knowledge from a labeled source domain to a mix of unlabeled homogeneous compound target domains while generalizing to open unseen domains. Existing OCDA methods solve the intra-domain gaps by a divide-and-conquer strategy, which divides the problem into several individual and parallel domain adaptation (DA) tasks. Such approaches often contain multiple sub-networks or stages, which may constrain the model's performance. In this work, starting from the general DA theory, we establish the generalization bound for the setting of OCDA. Built upon this, we argue that conventional OCDA approaches may substantially underestimate the inherent variance inside the compound target domains for model generalization. We subsequently present Stochastic Compound Mixing (SCMix), an augmentation strategy with the primary objective of mitigating the divergence between source and mixed target distributions. We provide theoretical analysis to substantiate the superiority of SCMix and prove that the previous methods are sub-groups of our methods. Extensive experiments show that our method attains a lower empirical risk on OCDA semantic segmentation tasks, thus supporting our theories. Combining the transformer architecture, SCMix achieves a notable performance boost compared to the SoTA results.","sentences":["Open compound domain adaptation (OCDA) aims to transfer knowledge from a labeled source domain to a mix of unlabeled homogeneous compound target domains while generalizing to open unseen domains.","Existing OCDA methods solve the intra-domain gaps by a divide-and-conquer strategy, which divides the problem into several individual and parallel domain adaptation (DA) tasks.","Such approaches often contain multiple sub-networks or stages, which may constrain the model's performance.","In this work, starting from the general DA theory, we establish the generalization bound for the setting of OCDA.","Built upon this, we argue that conventional OCDA approaches may substantially underestimate the inherent variance inside the compound target domains for model generalization.","We subsequently present Stochastic Compound Mixing (SCMix), an augmentation strategy with the primary objective of mitigating the divergence between source and mixed target distributions.","We provide theoretical analysis to substantiate the superiority of SCMix and prove that the previous methods are sub-groups of our methods.","Extensive experiments show that our method attains a lower empirical risk on OCDA semantic segmentation tasks, thus supporting our theories.","Combining the transformer architecture, SCMix achieves a notable performance boost compared to the SoTA results."],"url":"http://arxiv.org/abs/2405.14278v1","category":"cs.CV"}
{"created":"2024-05-23 07:37:33","title":"Deep Learning Methods for Adjusting Global MFD Speed Estimations to Local Link Configurations","abstract":"In large-scale traffic optimization, models based on Macroscopic Fundamental Diagram (MFD) are recognized for their efficiency in broad analyses. However, they fail to reflect variations in the individual traffic status of each road link, leading to a gap in detailed traffic optimization and analysis. To address the limitation, this study introduces a Local Correction Factor (LCF) that a function integrates MFD-derived network mean speed with network configurations to accurately estimate the individual speed of the link. We use a novel deep learning framework combining Graph Attention Networks (GATs) with Gated Recurrent Units (GRUs) to capture both spatial configurations and temporal dynamics of the network. Coupled with a strategic network partitioning method, our model enhances the precision of link-level traffic speed estimations while preserving the computational benefits of aggregate models. In the experiment, we evaluate the proposed LCF through various urban traffic scenarios, including different demand levels, origin-destination distributions, and road configurations. The results show the robust adaptability and effectiveness of the proposed model. Furthermore, we validate the practicality of our model by calculating the travel time of each randomly generated path, with the average error relative to MFD-based results being reduced to approximately 76%.","sentences":["In large-scale traffic optimization, models based on Macroscopic Fundamental Diagram (MFD) are recognized for their efficiency in broad analyses.","However, they fail to reflect variations in the individual traffic status of each road link, leading to a gap in detailed traffic optimization and analysis.","To address the limitation, this study introduces a Local Correction Factor (LCF) that a function integrates MFD-derived network mean speed with network configurations to accurately estimate the individual speed of the link.","We use a novel deep learning framework combining Graph Attention Networks (GATs) with Gated Recurrent Units (GRUs) to capture both spatial configurations and temporal dynamics of the network.","Coupled with a strategic network partitioning method, our model enhances the precision of link-level traffic speed estimations while preserving the computational benefits of aggregate models.","In the experiment, we evaluate the proposed LCF through various urban traffic scenarios, including different demand levels, origin-destination distributions, and road configurations.","The results show the robust adaptability and effectiveness of the proposed model.","Furthermore, we validate the practicality of our model by calculating the travel time of each randomly generated path, with the average error relative to MFD-based results being reduced to approximately 76%."],"url":"http://arxiv.org/abs/2405.14257v1","category":"cs.LG"}
{"created":"2024-05-23 05:52:42","title":"Adaptive Teaching in Heterogeneous Agents: Balancing Surprise in Sparse Reward Scenarios","abstract":"Learning from Demonstration (LfD) can be an efficient way to train systems with analogous agents by enabling ``Student'' agents to learn from the demonstrations of the most experienced ``Teacher'' agent, instead of training their policy in parallel. However, when there are discrepancies in agent capabilities, such as divergent actuator power or joint angle constraints, naively replicating demonstrations that are out of bounds for the Student's capability can limit efficient learning. We present a Teacher-Student learning framework specifically tailored to address the challenge of heterogeneity between the Teacher and Student agents. Our framework is based on the concept of ``surprise'', inspired by its application in exploration incentivization in sparse-reward environments. Surprise is repurposed to enable the Teacher to detect and adapt to differences between itself and the Student. By focusing on maximizing its surprise in response to the environment while concurrently minimizing the Student's surprise in response to the demonstrations, the Teacher agent can effectively tailor its demonstrations to the Student's specific capabilities and constraints. We validate our method by demonstrating improvements in the Student's learning in control tasks within sparse-reward environments.","sentences":["Learning from Demonstration (LfD) can be an efficient way to train systems with analogous agents by enabling ``Student'' agents to learn from the demonstrations of the most experienced ``Teacher'' agent, instead of training their policy in parallel.","However, when there are discrepancies in agent capabilities, such as divergent actuator power or joint angle constraints, naively replicating demonstrations that are out of bounds for the Student's capability can limit efficient learning.","We present a Teacher-Student learning framework specifically tailored to address the challenge of heterogeneity between the Teacher and Student agents.","Our framework is based on the concept of ``surprise'', inspired by its application in exploration incentivization in sparse-reward environments.","Surprise is repurposed to enable the Teacher to detect and adapt to differences between itself and the Student.","By focusing on maximizing its surprise in response to the environment while concurrently minimizing the Student's surprise in response to the demonstrations, the Teacher agent can effectively tailor its demonstrations to the Student's specific capabilities and constraints.","We validate our method by demonstrating improvements in the Student's learning in control tasks within sparse-reward environments."],"url":"http://arxiv.org/abs/2405.14199v1","category":"cs.RO"}
{"created":"2024-05-23 05:35:57","title":"IB-AdCSCNet:Adaptive Convolutional Sparse Coding Network Driven by Information Bottleneck","abstract":"In the realm of neural network models, the perpetual challenge remains in retaining task-relevant information while effectively discarding redundant data during propagation. In this paper, we introduce IB-AdCSCNet, a deep learning model grounded in information bottleneck theory. IB-AdCSCNet seamlessly integrates the information bottleneck trade-off strategy into deep networks by dynamically adjusting the trade-off hyperparameter $\\lambda$ through gradient descent, updating it within the FISTA(Fast Iterative Shrinkage-Thresholding Algorithm ) framework. By optimizing the compressive excitation loss function induced by the information bottleneck principle, IB-AdCSCNet achieves an optimal balance between compression and fitting at a global level, approximating the globally optimal representation feature. This information bottleneck trade-off strategy driven by downstream tasks not only helps to learn effective features of the data, but also improves the generalization of the model. This study's contribution lies in presenting a model with consistent performance and offering a fresh perspective on merging deep learning with sparse representation theory, grounded in the information bottleneck concept. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate that IB-AdCSCNet not only matches the performance of deep residual convolutional networks but also outperforms them when handling corrupted data. Through the inference of the IB trade-off, the model's robustness is notably enhanced.","sentences":["In the realm of neural network models, the perpetual challenge remains in retaining task-relevant information while effectively discarding redundant data during propagation.","In this paper, we introduce IB-AdCSCNet, a deep learning model grounded in information bottleneck theory.","IB-AdCSCNet seamlessly integrates the information bottleneck trade-off strategy into deep networks by dynamically adjusting the trade-off hyperparameter $\\lambda$ through gradient descent, updating it within the FISTA(Fast Iterative Shrinkage-Thresholding Algorithm ) framework.","By optimizing the compressive excitation loss function induced by the information bottleneck principle, IB-AdCSCNet achieves an optimal balance between compression and fitting at a global level, approximating the globally optimal representation feature.","This information bottleneck trade-off strategy driven by downstream tasks not only helps to learn effective features of the data, but also improves the generalization of the model.","This study's contribution lies in presenting a model with consistent performance and offering a fresh perspective on merging deep learning with sparse representation theory, grounded in the information bottleneck concept.","Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate that IB-AdCSCNet not only matches the performance of deep residual convolutional networks but also outperforms them when handling corrupted data.","Through the inference of the IB trade-off, the model's robustness is notably enhanced."],"url":"http://arxiv.org/abs/2405.14192v1","category":"cs.CV"}
{"created":"2024-05-23 04:03:39","title":"Skip-SCAR: A Modular Approach to ObjectGoal Navigation with Sparsity and Adaptive Skips","abstract":"In ObjectGoal navigation (ObjectNav), agents must locate specific objects within unseen environments, requiring effective observation, prediction, and navigation capabilities. This study found that traditional methods looking only for prediction accuracy often compromise on computational efficiency. To address this, we introduce \"Skip-SCAR,\" a modular framework that enhances efficiency by leveraging sparsity and adaptive skips. The SparseConv-Augmented ResNet (SCAR) at the core of our approach uses sparse and dense feature processing in parallel, optimizing both the computation and memory footprint. Our adaptive skip technique further reduces computational demands by selectively bypassing unnecessary semantic segmentation steps based on environmental constancy. Tested on the HM3D ObjectNav datasets, Skip-SCAR not only minimizes resource use but also sets new performance benchmarks, demonstrating a robust method for improving efficiency and accuracy in robotic navigation tasks.","sentences":["In ObjectGoal navigation (ObjectNav), agents must locate specific objects within unseen environments, requiring effective observation, prediction, and navigation capabilities.","This study found that traditional methods looking only for prediction accuracy often compromise on computational efficiency.","To address this, we introduce \"Skip-SCAR,\" a modular framework that enhances efficiency by leveraging sparsity and adaptive skips.","The SparseConv-Augmented ResNet (SCAR) at the core of our approach uses sparse and dense feature processing in parallel, optimizing both the computation and memory footprint.","Our adaptive skip technique further reduces computational demands by selectively bypassing unnecessary semantic segmentation steps based on environmental constancy.","Tested on the HM3D ObjectNav datasets, Skip-SCAR not only minimizes resource use but also sets new performance benchmarks, demonstrating a robust method for improving efficiency and accuracy in robotic navigation tasks."],"url":"http://arxiv.org/abs/2405.14154v1","category":"cs.RO"}
{"created":"2024-05-23 04:03:36","title":"A Neighbor-Searching Discrepancy-based Drift Detection Scheme for Learning Evolving Data","abstract":"Uncertain changes in data streams present challenges for machine learning models to dynamically adapt and uphold performance in real-time. Particularly, classification boundary change, also known as real concept drift, is the major cause of classification performance deterioration. However, accurately detecting real concept drift remains challenging because the theoretical foundations of existing drift detection methods - two-sample distribution tests and monitoring classification error rate, both suffer from inherent limitations such as the inability to distinguish virtual drift (changes not affecting the classification boundary, will introduce unnecessary model maintenance), limited statistical power, or high computational cost. Furthermore, no existing detection method can provide information on the trend of the drift, which could be invaluable for model maintenance. This work presents a novel real concept drift detection method based on Neighbor-Searching Discrepancy, a new statistic that measures the classification boundary difference between two samples. The proposed method is able to detect real concept drift with high accuracy while ignoring virtual drift. It can also indicate the direction of the classification boundary change by identifying the invasion or retreat of a certain class, which is also an indicator of separability change between classes. A comprehensive evaluation of 11 experiments is conducted, including empirical verification of the proposed theory using artificial datasets, and experimental comparisons with commonly used drift handling methods on real-world datasets. The results show that the proposed theory is robust against a range of distributions and dimensions, and the drift detection method outperforms state-of-the-art alternative methods.","sentences":["Uncertain changes in data streams present challenges for machine learning models to dynamically adapt and uphold performance in real-time.","Particularly, classification boundary change, also known as real concept drift, is the major cause of classification performance deterioration.","However, accurately detecting real concept drift remains challenging because the theoretical foundations of existing drift detection methods - two-sample distribution tests and monitoring classification error rate, both suffer from inherent limitations such as the inability to distinguish virtual drift (changes not affecting the classification boundary, will introduce unnecessary model maintenance), limited statistical power, or high computational cost.","Furthermore, no existing detection method can provide information on the trend of the drift, which could be invaluable for model maintenance.","This work presents a novel real concept drift detection method based on Neighbor-Searching Discrepancy, a new statistic that measures the classification boundary difference between two samples.","The proposed method is able to detect real concept drift with high accuracy while ignoring virtual drift.","It can also indicate the direction of the classification boundary change by identifying the invasion or retreat of a certain class, which is also an indicator of separability change between classes.","A comprehensive evaluation of 11 experiments is conducted, including empirical verification of the proposed theory using artificial datasets, and experimental comparisons with commonly used drift handling methods on real-world datasets.","The results show that the proposed theory is robust against a range of distributions and dimensions, and the drift detection method outperforms state-of-the-art alternative methods."],"url":"http://arxiv.org/abs/2405.14153v1","category":"cs.LG"}
{"created":"2024-05-23 03:54:25","title":"jp-evalb: Robust Alignment-based PARSEVAL Measures","abstract":"We introduce an evaluation system designed to compute PARSEVAL measures, offering a viable alternative to \\texttt{evalb} commonly used for constituency parsing evaluation. The widely used \\texttt{evalb} script has traditionally been employed for evaluating the accuracy of constituency parsing results, albeit with the requirement for consistent tokenization and sentence boundaries. In contrast, our approach, named \\texttt{jp-evalb}, is founded on an alignment method. This method aligns sentences and words when discrepancies arise. It aims to overcome several known issues associated with \\texttt{evalb} by utilizing the `jointly preprocessed (JP)' alignment-based method. We introduce a more flexible and adaptive framework, ultimately contributing to a more accurate assessment of constituency parsing performance.","sentences":["We introduce an evaluation system designed to compute PARSEVAL measures, offering a viable alternative to \\texttt{evalb} commonly used for constituency parsing evaluation.","The widely used \\texttt{evalb} script has traditionally been employed for evaluating the accuracy of constituency parsing results, albeit with the requirement for consistent tokenization and sentence boundaries.","In contrast, our approach, named \\texttt{jp-evalb}, is founded on an alignment method.","This method aligns sentences and words when discrepancies arise.","It aims to overcome several known issues associated with \\texttt{evalb} by utilizing the `jointly preprocessed (JP)' alignment-based method.","We introduce a more flexible and adaptive framework, ultimately contributing to a more accurate assessment of constituency parsing performance."],"url":"http://arxiv.org/abs/2405.14150v1","category":"cs.CL"}
{"created":"2024-05-23 00:35:23","title":"PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning","abstract":"Designing generalizable agents capable of adapting to diverse embodiments has achieved significant attention in Reinforcement Learning (RL), which is critical for deploying RL agents in various real-world applications. Previous Cross-Embodiment RL approaches have focused on transferring knowledge across embodiments within specific tasks. These methods often result in knowledge tightly coupled with those tasks and fail to adequately capture the distinct characteristics of different embodiments. To address this limitation, we introduce the notion of Cross-Embodiment Unsupervised RL (CEURL), which leverages unsupervised learning to enable agents to acquire embodiment-aware and task-agnostic knowledge through online interactions within reward-free environments. We formulate CEURL as a novel Controlled Embodiment Markov Decision Process (CE-MDP) and systematically analyze CEURL's pre-training objectives under CE-MDP. Based on these analyses, we develop a novel algorithm Pre-trained Embodiment-Aware Control (PEAC) for handling CEURL, incorporating an intrinsic reward function specifically designed for cross-embodiment pre-training. PEAC not only provides an intuitive optimization strategy for cross-embodiment pre-training but also can integrate flexibly with existing unsupervised RL methods, facilitating cross-embodiment exploration and skill discovery. Extensive experiments in both simulated (e.g., DMC and Robosuite) and real-world environments (e.g., legged locomotion) demonstrate that PEAC significantly improves adaptation performance and cross-embodiment generalization, demonstrating its effectiveness in overcoming the unique challenges of CEURL.","sentences":["Designing generalizable agents capable of adapting to diverse embodiments has achieved significant attention in Reinforcement Learning (RL), which is critical for deploying RL agents in various real-world applications.","Previous Cross-Embodiment RL approaches have focused on transferring knowledge across embodiments within specific tasks.","These methods often result in knowledge tightly coupled with those tasks and fail to adequately capture the distinct characteristics of different embodiments.","To address this limitation, we introduce the notion of Cross-Embodiment Unsupervised RL (CEURL), which leverages unsupervised learning to enable agents to acquire embodiment-aware and task-agnostic knowledge through online interactions within reward-free environments.","We formulate CEURL as a novel Controlled Embodiment Markov Decision Process (CE-MDP) and systematically analyze CEURL's pre-training objectives under CE-MDP.","Based on these analyses, we develop a novel algorithm Pre-trained Embodiment-Aware Control (PEAC) for handling CEURL, incorporating an intrinsic reward function specifically designed for cross-embodiment pre-training.","PEAC not only provides an intuitive optimization strategy for cross-embodiment pre-training but also can integrate flexibly with existing unsupervised RL methods, facilitating cross-embodiment exploration and skill discovery.","Extensive experiments in both simulated (e.g., DMC and Robosuite) and real-world environments (e.g., legged locomotion) demonstrate that PEAC significantly improves adaptation performance and cross-embodiment generalization, demonstrating its effectiveness in overcoming the unique challenges of CEURL."],"url":"http://arxiv.org/abs/2405.14073v1","category":"cs.LG"}
{"created":"2024-05-22 23:09:57","title":"Probabilistic Inference in the Era of Tensor Networks and Differential Programming","abstract":"Probabilistic inference is a fundamental task in modern machine learning. Recent advances in tensor network (TN) contraction algorithms have enabled the development of better exact inference methods. However, many common inference tasks in probabilistic graphical models (PGMs) still lack corresponding TN-based adaptations. In this work, we advance the connection between PGMs and TNs by formulating and implementing tensor-based solutions for the following inference tasks: (i) computing the partition function, (ii) computing the marginal probability of sets of variables in the model, (iii) determining the most likely assignment to a set of variables, and (iv) the same as (iii) but after having marginalized a different set of variables. We also present a generalized method for generating samples from a learned probability distribution. Our work is motivated by recent technical advances in the fields of quantum circuit simulation, quantum many-body physics, and statistical physics. Through an experimental evaluation, we demonstrate that the integration of these quantum technologies with a series of algorithms introduced in this study significantly improves the effectiveness of existing methods for solving probabilistic inference tasks.","sentences":["Probabilistic inference is a fundamental task in modern machine learning.","Recent advances in tensor network (TN) contraction algorithms have enabled the development of better exact inference methods.","However, many common inference tasks in probabilistic graphical models (PGMs) still lack corresponding TN-based adaptations.","In this work, we advance the connection between PGMs and TNs by formulating and implementing tensor-based solutions for the following inference tasks: (i) computing the partition function, (ii) computing the marginal probability of sets of variables in the model, (iii) determining the most likely assignment to a set of variables, and (iv) the same as (iii) but after having marginalized a different set of variables.","We also present a generalized method for generating samples from a learned probability distribution.","Our work is motivated by recent technical advances in the fields of quantum circuit simulation, quantum many-body physics, and statistical physics.","Through an experimental evaluation, we demonstrate that the integration of these quantum technologies with a series of algorithms introduced in this study significantly improves the effectiveness of existing methods for solving probabilistic inference tasks."],"url":"http://arxiv.org/abs/2405.14060v1","category":"cs.LG"}
{"created":"2024-05-22 22:49:54","title":"On the Role of Non-Terrestrial Networks for Boosting Terrestrial Network Performance in Dynamic Traffic Scenarios","abstract":"Due to an ever-expansive network deployment, numerous questions are being raised regarding the energy consumption of the mobile network. Recently, Non-Terrestrial Networks (NTNs) have proven to be a useful, and complementary solution to Terrestrial Networks (TN) to provide ubiquitous coverage. In this paper, we consider an integrated TN-NTN, and study how to maximize its resource usage in a dynamic traffic scenario. We introduce BLASTER, a framework designed to control User Equipment (UE) association, Base Station (BS) transmit power and activation, and bandwidth allocation between the terrestrial and non-terrestrial tiers. Our proposal is able to adapt to fluctuating daily traffic, focusing on reducing power consumption throughout the network during low traffic and distributing the load otherwise. Simulation results show an average daily decrease of total power consumption by 45% compared to a network model following 3GPP recommendation, as well as an average throughput increase of roughly 250%. Our paper underlines the central and dynamic role that the NTN plays in improving key areas of concern for network flexibility.","sentences":["Due to an ever-expansive network deployment, numerous questions are being raised regarding the energy consumption of the mobile network.","Recently, Non-Terrestrial Networks (NTNs) have proven to be a useful, and complementary solution to Terrestrial Networks (TN) to provide ubiquitous coverage.","In this paper, we consider an integrated TN-NTN, and study how to maximize its resource usage in a dynamic traffic scenario.","We introduce BLASTER, a framework designed to control User Equipment (UE) association, Base Station (BS) transmit power and activation, and bandwidth allocation between the terrestrial and non-terrestrial tiers.","Our proposal is able to adapt to fluctuating daily traffic, focusing on reducing power consumption throughout the network during low traffic and distributing the load otherwise.","Simulation results show an average daily decrease of total power consumption by 45% compared to a network model following 3GPP recommendation, as well as an average throughput increase of roughly 250%.","Our paper underlines the central and dynamic role that the NTN plays in improving key areas of concern for network flexibility."],"url":"http://arxiv.org/abs/2405.14053v1","category":"cs.NI"}
{"created":"2024-05-22 22:39:29","title":"Particle physics DL-simulation with control over generated data properties","abstract":"The research of innovative methods aimed at reducing costs and shortening the time needed for simulation, going beyond conventional approaches based on Monte Carlo methods, has been sparked by the development of collision simulations at the Large Hadron Collider at CERN. Deep learning generative methods including VAE, GANs and diffusion models have been used for this purpose. Although they are much faster and simpler than standard approaches, they do not always keep high fidelity of the simulated data. This work aims to mitigate this issue, by providing an alternative solution to currently employed algorithms by introducing the mechanism of control over the generated data properties. To achieve this, we extend the recently introduced CorrVAE, which enables user-defined parameter manipulation of the generated output. We adapt the model to the problem of particle physics simulation. The proposed solution achieved promising results, demonstrating control over the parameters of the generated output and constituting an alternative for simulating the ZDC calorimeter in the ALICE experiment at CERN.","sentences":["The research of innovative methods aimed at reducing costs and shortening the time needed for simulation, going beyond conventional approaches based on Monte Carlo methods, has been sparked by the development of collision simulations at the Large Hadron Collider at CERN.","Deep learning generative methods including VAE, GANs and diffusion models have been used for this purpose.","Although they are much faster and simpler than standard approaches, they do not always keep high fidelity of the simulated data.","This work aims to mitigate this issue, by providing an alternative solution to currently employed algorithms by introducing the mechanism of control over the generated data properties.","To achieve this, we extend the recently introduced CorrVAE, which enables user-defined parameter manipulation of the generated output.","We adapt the model to the problem of particle physics simulation.","The proposed solution achieved promising results, demonstrating control over the parameters of the generated output and constituting an alternative for simulating the ZDC calorimeter in the ALICE experiment at CERN."],"url":"http://arxiv.org/abs/2405.14049v1","category":"cs.LG"}
{"created":"2024-05-22 22:38:53","title":"fsemipar: an R package for SoF semiparametric regression","abstract":"Functional data analysis has become a tool of interest in applied areas such as economics, medicine, and chemistry. Among the techniques developed in recent literature, functional semiparametric regression stands out for its balance between flexible modelling and output interpretation. Despite the large variety of research papers dealing with scalar-on-function (SoF) semiparametric models, there is a notable gap in software tools for their implementation. This article introduces the R package \\texttt{fsemipar}, tailored for these models. \\texttt{fsemipar} not only estimates functional single-index models using kernel smoothing techniques but also estimates and selects relevant scalar variables in semi-functional models with multivariate linear components. A standout feature is its ability to identify impact points of a curve on the response, even in models with multiple functional covariates, and to integrate both continuous and pointwise effects of functional predictors within a single model. In addition, it allows the use of location-adaptive estimators based on the $k$-nearest-neighbours approach for all the semiparametric models included. Its flexible interface empowers users to customise a wide range of input parameters and includes the standard S3 methods for prediction, statistical analysis, and estimate visualization (\\texttt{predict}, \\texttt{summary}, \\texttt{print}, and \\texttt{plot}), enhancing clear result interpretation. Throughout the article, we illustrate the functionalities and the practicality of \\texttt{fsemipar} using two chemometric datasets.","sentences":["Functional data analysis has become a tool of interest in applied areas such as economics, medicine, and chemistry.","Among the techniques developed in recent literature, functional semiparametric regression stands out for its balance between flexible modelling and output interpretation.","Despite the large variety of research papers dealing with scalar-on-function (SoF) semiparametric models, there is a notable gap in software tools for their implementation.","This article introduces the R package \\texttt{fsemipar}, tailored for these models.","\\texttt{fsemipar} not only estimates functional single-index models using kernel smoothing techniques but also estimates and selects relevant scalar variables in semi-functional models with multivariate linear components.","A standout feature is its ability to identify impact points of a curve on the response, even in models with multiple functional covariates, and to integrate both continuous and pointwise effects of functional predictors within a single model.","In addition, it allows the use of location-adaptive estimators based on the $k$-nearest-neighbours approach for all the semiparametric models included.","Its flexible interface empowers users to customise a wide range of input parameters and includes the standard S3 methods for prediction, statistical analysis, and estimate visualization (\\texttt{predict}, \\texttt{summary}, \\texttt{print}, and \\texttt{plot}), enhancing clear result interpretation.","Throughout the article, we illustrate the functionalities and the practicality of \\texttt{fsemipar} using two chemometric datasets."],"url":"http://arxiv.org/abs/2405.14048v1","category":"stat.ME"}
{"created":"2024-05-22 21:55:58","title":"I2I-Mamba: Multi-modal medical image synthesis via selective state space modeling","abstract":"In recent years, deep learning models comprising transformer components have pushed the performance envelope in medical image synthesis tasks. Contrary to convolutional neural networks (CNNs) that use static, local filters, transformers use self-attention mechanisms to permit adaptive, non-local filtering to sensitively capture long-range context. However, this sensitivity comes at the expense of substantial model complexity, which can compromise learning efficacy particularly on relatively modest-sized imaging datasets. Here, we propose a novel adversarial model for multi-modal medical image synthesis, I2I-Mamba, that leverages selective state space modeling (SSM) to efficiently capture long-range context while maintaining local precision. To do this, I2I-Mamba injects channel-mixed Mamba (cmMamba) blocks in the bottleneck of a convolutional backbone. In cmMamba blocks, SSM layers are used to learn context across the spatial dimension and channel-mixing layers are used to learn context across the channel dimension of feature maps. Comprehensive demonstrations are reported for imputing missing images in multi-contrast MRI and MRI-CT protocols. Our results indicate that I2I-Mamba offers superior performance against state-of-the-art CNN- and transformer-based methods in synthesizing target-modality images.","sentences":["In recent years, deep learning models comprising transformer components have pushed the performance envelope in medical image synthesis tasks.","Contrary to convolutional neural networks (CNNs) that use static, local filters, transformers use self-attention mechanisms to permit adaptive, non-local filtering to sensitively capture long-range context.","However, this sensitivity comes at the expense of substantial model complexity, which can compromise learning efficacy particularly on relatively modest-sized imaging datasets.","Here, we propose a novel adversarial model for multi-modal medical image synthesis, I2I-Mamba, that leverages selective state space modeling (SSM) to efficiently capture long-range context while maintaining local precision.","To do this, I2I-Mamba injects channel-mixed Mamba (cmMamba) blocks in the bottleneck of a convolutional backbone.","In cmMamba blocks, SSM layers are used to learn context across the spatial dimension and channel-mixing layers are used to learn context across the channel dimension of feature maps.","Comprehensive demonstrations are reported for imputing missing images in multi-contrast MRI and MRI-CT protocols.","Our results indicate that I2I-Mamba offers superior performance against state-of-the-art CNN- and transformer-based methods in synthesizing target-modality images."],"url":"http://arxiv.org/abs/2405.14022v1","category":"eess.IV"}
{"created":"2024-05-22 21:35:56","title":"SlipStream: Adapting Pipelines for Distributed Training of Large DNNs Amid Failures","abstract":"Training large Deep Neural Network (DNN) models requires thousands of GPUs for days or weeks at a time. At these scales, failures are frequent and can have a big impact on training throughput. Restoring performance using spare GPU servers becomes increasingly expensive as models grow. SlipStream is a system for efficient DNN training in the presence of failures, without using spare servers. It exploits the functional redundancy inherent in distributed training systems -- servers hold the same model parameters across data-parallel groups -- as well as the bubbles in the pipeline schedule within each data-parallel group. SlipStream dynamically re-routes the work of a failed server to its data-parallel peers, ensuring continuous training despite multiple failures. However, re-routing work leads to imbalances across pipeline stages that degrades training throughput. SlipStream introduces two optimizations that allow re-routed work to execute within bubbles of the original pipeline schedule. First, it decouples the backward pass computation into two phases. Second, it staggers the execution of the optimizer step across pipeline stages. Combined, these optimizations enable schedules that minimize or even eliminate training throughput degradation during failures. We describe a prototype for SlipStream and show that it achieves high training throughput under multiple failures, outperforming recent proposals for fault-tolerant training such as Oobleck and Bamboo by up to 1.46x and 1.64x, respectively.","sentences":["Training large Deep Neural Network (DNN) models requires thousands of GPUs for days or weeks at a time.","At these scales, failures are frequent and can have a big impact on training throughput.","Restoring performance using spare GPU servers becomes increasingly expensive as models grow.","SlipStream is a system for efficient DNN training in the presence of failures, without using spare servers.","It exploits the functional redundancy inherent in distributed training systems -- servers hold the same model parameters across data-parallel groups -- as well as the bubbles in the pipeline schedule within each data-parallel group.","SlipStream dynamically re-routes the work of a failed server to its data-parallel peers, ensuring continuous training despite multiple failures.","However, re-routing work leads to imbalances across pipeline stages that degrades training throughput.","SlipStream introduces two optimizations that allow re-routed work to execute within bubbles of the original pipeline schedule.","First, it decouples the backward pass computation into two phases.","Second, it staggers the execution of the optimizer step across pipeline stages.","Combined, these optimizations enable schedules that minimize or even eliminate training throughput degradation during failures.","We describe a prototype for SlipStream and show that it achieves high training throughput under multiple failures, outperforming recent proposals for fault-tolerant training such as Oobleck and Bamboo by up to 1.46x and 1.64x, respectively."],"url":"http://arxiv.org/abs/2405.14009v1","category":"cs.DC"}
{"created":"2024-05-22 21:17:54","title":"Animal Behavior Analysis Methods Using Deep Learning: A Survey","abstract":"Animal behavior serves as a reliable indicator of the adaptation of organisms to their environment and their overall well-being. Through rigorous observation of animal actions and interactions, researchers and observers can glean valuable insights into diverse facets of their lives, encompassing health, social dynamics, ecological relationships, and neuroethological dimensions. Although state-of-the-art deep learning models have demonstrated remarkable accuracy in classifying various forms of animal data, their adoption in animal behavior studies remains limited. This survey article endeavors to comprehensively explore deep learning architectures and strategies applied to the identification of animal behavior, spanning auditory, visual, and audiovisual methodologies. Furthermore, the manuscript scrutinizes extant animal behavior datasets, offering a detailed examination of the principal challenges confronting this research domain. The article culminates in a comprehensive discussion of key research directions within deep learning that hold potential for advancing the field of animal behavior studies.","sentences":["Animal behavior serves as a reliable indicator of the adaptation of organisms to their environment and their overall well-being.","Through rigorous observation of animal actions and interactions, researchers and observers can glean valuable insights into diverse facets of their lives, encompassing health, social dynamics, ecological relationships, and neuroethological dimensions.","Although state-of-the-art deep learning models have demonstrated remarkable accuracy in classifying various forms of animal data, their adoption in animal behavior studies remains limited.","This survey article endeavors to comprehensively explore deep learning architectures and strategies applied to the identification of animal behavior, spanning auditory, visual, and audiovisual methodologies.","Furthermore, the manuscript scrutinizes extant animal behavior datasets, offering a detailed examination of the principal challenges confronting this research domain.","The article culminates in a comprehensive discussion of key research directions within deep learning that hold potential for advancing the field of animal behavior studies."],"url":"http://arxiv.org/abs/2405.14002v1","category":"cs.LG"}
{"created":"2024-05-22 21:13:23","title":"Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective","abstract":"Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces. Here we uncover a connection between operator learning architectures and conditioned neural fields from computer vision, providing a unified perspective for examining differences between popular operator learning models. We find that many commonly used operator learning models can be viewed as neural fields with conditioning mechanisms restricted to point-wise and/or global information. Motivated by this, we propose the Continuous Vision Transformer (CViT), a novel neural operator architecture that employs a vision transformer encoder and uses cross-attention to modulate a base field constructed with a trainable grid-based positional encoding of query coordinates. Despite its simplicity, CViT achieves state-of-the-art results across challenging benchmarks in climate modeling and fluid dynamics. Our contributions can be viewed as a first step towards adapting advanced computer vision architectures for building more flexible and accurate machine learning models in physical sciences.","sentences":["Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces.","Here we uncover a connection between operator learning architectures and conditioned neural fields from computer vision, providing a unified perspective for examining differences between popular operator learning models.","We find that many commonly used operator learning models can be viewed as neural fields with conditioning mechanisms restricted to point-wise and/or global information.","Motivated by this, we propose the Continuous Vision Transformer (CViT), a novel neural operator architecture that employs a vision transformer encoder and uses cross-attention to modulate a base field constructed with a trainable grid-based positional encoding of query coordinates.","Despite its simplicity, CViT achieves state-of-the-art results across challenging benchmarks in climate modeling and fluid dynamics.","Our contributions can be viewed as a first step towards adapting advanced computer vision architectures for building more flexible and accurate machine learning models in physical sciences."],"url":"http://arxiv.org/abs/2405.13998v1","category":"cs.LG"}
{"created":"2024-05-22 20:20:43","title":"EchoSpike Predictive Plasticity: An Online Local Learning Rule for Spiking Neural Networks","abstract":"The drive to develop artificial neural networks that efficiently utilize resources has generated significant interest in bio-inspired Spiking Neural Networks (SNNs). These networks are particularly attractive due to their potential in applications requiring low power and memory. This potential is further enhanced by the ability to perform online local learning, enabling them to adapt to dynamic environments. This requires the model to be adaptive in a self-supervised manner. While self-supervised learning has seen great success in many deep learning domains, its application for online local learning in multi-layer SNNs remains underexplored. In this paper, we introduce the \"EchoSpike Predictive Plasticity\" (ESPP) learning rule, a pioneering online local learning rule designed to leverage hierarchical temporal dynamics in SNNs through predictive and contrastive coding. We validate the effectiveness of this approach using benchmark datasets, demonstrating that it performs on par with current state-of-the-art supervised learning rules. The temporal and spatial locality of ESPP makes it particularly well-suited for low-cost neuromorphic processors, representing a significant advancement in developing biologically plausible self-supervised learning models for neuromorphic computing at the edge.","sentences":["The drive to develop artificial neural networks that efficiently utilize resources has generated significant interest in bio-inspired Spiking Neural Networks (SNNs).","These networks are particularly attractive due to their potential in applications requiring low power and memory.","This potential is further enhanced by the ability to perform online local learning, enabling them to adapt to dynamic environments.","This requires the model to be adaptive in a self-supervised manner.","While self-supervised learning has seen great success in many deep learning domains, its application for online local learning in multi-layer SNNs remains underexplored.","In this paper, we introduce the \"EchoSpike Predictive Plasticity\" (ESPP) learning rule, a pioneering online local learning rule designed to leverage hierarchical temporal dynamics in SNNs through predictive and contrastive coding.","We validate the effectiveness of this approach using benchmark datasets, demonstrating that it performs on par with current state-of-the-art supervised learning rules.","The temporal and spatial locality of ESPP makes it particularly well-suited for low-cost neuromorphic processors, representing a significant advancement in developing biologically plausible self-supervised learning models for neuromorphic computing at the edge."],"url":"http://arxiv.org/abs/2405.13976v1","category":"cs.NE"}
{"created":"2024-05-22 19:55:33","title":"Learning To Play Atari Games Using Dueling Q-Learning and Hebbian Plasticity","abstract":"In this work, an advanced deep reinforcement learning architecture is used to train neural network agents playing atari games. Given only the raw game pixels, action space, and reward information, the system can train agents to play any Atari game. At first, this system uses advanced techniques like deep Q-networks and dueling Q-networks to train efficient agents, the same techniques used by DeepMind to train agents that beat human players in Atari games. As an extension, plastic neural networks are used as agents, and their feasibility is analyzed in this scenario. The plasticity implementation was based on backpropagation and the Hebbian update rule. Plastic neural networks have excellent features like lifelong learning after the initial training, which makes them highly suitable in adaptive learning environments. As a new analysis of plasticity in this context, this work might provide valuable insights and direction for future works.","sentences":["In this work, an advanced deep reinforcement learning architecture is used to train neural network agents playing atari games.","Given only the raw game pixels, action space, and reward information, the system can train agents to play any Atari game.","At first, this system uses advanced techniques like deep Q-networks and dueling Q-networks to train efficient agents, the same techniques used by DeepMind to train agents that beat human players in Atari games.","As an extension, plastic neural networks are used as agents, and their feasibility is analyzed in this scenario.","The plasticity implementation was based on backpropagation and the Hebbian update rule.","Plastic neural networks have excellent features like lifelong learning after the initial training, which makes them highly suitable in adaptive learning environments.","As a new analysis of plasticity in this context, this work might provide valuable insights and direction for future works."],"url":"http://arxiv.org/abs/2405.13960v1","category":"cs.AI"}
{"created":"2024-05-22 19:36:55","title":"Spectral Adapter: Fine-Tuning in Spectral Space","abstract":"Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pretrained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and orthogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tuning performance as well as benefits multi-adapter fusion. The code will be open-sourced for reproducibility.","sentences":["Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pretrained deep neural networks have captured widespread interest.","In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure.","We investigate two spectral adaptation mechanisms, namely additive tuning and orthogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space.","We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget.","We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tuning performance as well as benefits multi-adapter fusion.","The code will be open-sourced for reproducibility."],"url":"http://arxiv.org/abs/2405.13952v1","category":"cs.LG"}
{"created":"2024-05-22 19:30:24","title":"PitVQA: Image-grounded Text Embedding LLM for Visual Question Answering in Pituitary Surgery","abstract":"Visual Question Answering (VQA) within the surgical domain, utilizing Large Language Models (LLMs), offers a distinct opportunity to improve intra-operative decision-making and facilitate intuitive surgeon-AI interaction. However, the development of LLMs for surgical VQA is hindered by the scarcity of diverse and extensive datasets with complex reasoning tasks. Moreover, contextual fusion of the image and text modalities remains an open research challenge due to the inherent differences between these two types of information and the complexity involved in aligning them. This paper introduces PitVQA, a novel dataset specifically designed for VQA in endonasal pituitary surgery and PitVQA-Net, an adaptation of the GPT2 with a novel image-grounded text embedding for surgical VQA. PitVQA comprises 25 procedural videos and a rich collection of question-answer pairs spanning crucial surgical aspects such as phase and step recognition, context understanding, tool detection and localization, and tool-tissue interactions. PitVQA-Net consists of a novel image-grounded text embedding that projects image and text features into a shared embedding space and GPT2 Backbone with an excitation block classification head to generate contextually relevant answers within the complex domain of endonasal pituitary surgery. Our image-grounded text embedding leverages joint embedding, cross-attention and contextual representation to understand the contextual relationship between questions and surgical images. We demonstrate the effectiveness of PitVQA-Net on both the PitVQA and the publicly available EndoVis18-VQA dataset, achieving improvements in balanced accuracy of 8% and 9% over the most recent baselines, respectively. Our code and dataset is available at https://github.com/mobarakol/PitVQA.","sentences":["Visual Question Answering (VQA) within the surgical domain, utilizing Large Language Models (LLMs), offers a distinct opportunity to improve intra-operative decision-making and facilitate intuitive surgeon-AI interaction.","However, the development of LLMs for surgical VQA is hindered by the scarcity of diverse and extensive datasets with complex reasoning tasks.","Moreover, contextual fusion of the image and text modalities remains an open research challenge due to the inherent differences between these two types of information and the complexity involved in aligning them.","This paper introduces PitVQA, a novel dataset specifically designed for VQA in endonasal pituitary surgery and PitVQA-Net, an adaptation of the GPT2 with a novel image-grounded text embedding for surgical VQA.","PitVQA comprises 25 procedural videos and a rich collection of question-answer pairs spanning crucial surgical aspects such as phase and step recognition, context understanding, tool detection and localization, and tool-tissue interactions.","PitVQA-Net consists of a novel image-grounded text embedding that projects image and text features into a shared embedding space and GPT2 Backbone with an excitation block classification head to generate contextually relevant answers within the complex domain of endonasal pituitary surgery.","Our image-grounded text embedding leverages joint embedding, cross-attention and contextual representation to understand the contextual relationship between questions and surgical images.","We demonstrate the effectiveness of PitVQA-Net on both the PitVQA and the publicly available EndoVis18-VQA dataset, achieving improvements in balanced accuracy of 8% and 9% over the most recent baselines, respectively.","Our code and dataset is available at https://github.com/mobarakol/PitVQA."],"url":"http://arxiv.org/abs/2405.13949v1","category":"cs.CV"}
{"created":"2024-05-22 19:27:17","title":"Embodied Design for Enhanced Flipper-Based Locomotion in Complex Terrains","abstract":"Robots are becoming increasingly essential for traversing complex environments such as disaster areas, extraterrestrial terrains, and marine environments. Yet, their potential is often limited by mobility and adaptability constraints. In nature, various animals have evolved finely tuned designs and anatomical features that enable efficient locomotion in diverse environments. Sea turtles, for instance, possess specialized flippers that facilitate both long-distance underwater travel and adept maneuvers across a range of coastal terrains. Building on the principles of embodied intelligence and drawing inspiration from sea turtle hatchings, this paper examines the critical interplay between a robot's physical form and its environmental interactions, focusing on how morphological traits and locomotive behaviors affect terrestrial navigation. We present a bio-inspired robotic system and study the impacts of flipper/body morphology and gait patterns on its terrestrial mobility across diverse terrains ranging from sand to rocks. Evaluating key performance metrics such as speed and cost of transport, our experimental results highlight adaptive designs as crucial for multi-terrain robotic mobility to achieve not only speed and efficiency but also the versatility needed to tackle the varied and complex terrains encountered in real-world applications.","sentences":["Robots are becoming increasingly essential for traversing complex environments such as disaster areas, extraterrestrial terrains, and marine environments.","Yet, their potential is often limited by mobility and adaptability constraints.","In nature, various animals have evolved finely tuned designs and anatomical features that enable efficient locomotion in diverse environments.","Sea turtles, for instance, possess specialized flippers that facilitate both long-distance underwater travel and adept maneuvers across a range of coastal terrains.","Building on the principles of embodied intelligence and drawing inspiration from sea turtle hatchings, this paper examines the critical interplay between a robot's physical form and its environmental interactions, focusing on how morphological traits and locomotive behaviors affect terrestrial navigation.","We present a bio-inspired robotic system and study the impacts of flipper/body morphology and gait patterns on its terrestrial mobility across diverse terrains ranging from sand to rocks.","Evaluating key performance metrics such as speed and cost of transport, our experimental results highlight adaptive designs as crucial for multi-terrain robotic mobility to achieve not only speed and efficiency but also the versatility needed to tackle the varied and complex terrains encountered in real-world applications."],"url":"http://arxiv.org/abs/2405.13948v1","category":"cs.RO"}
{"created":"2024-05-22 19:16:53","title":"Distributed and Decentralized Control and Task Allocation for Flexible Swarms","abstract":"This paper introduces a novel bio-mimetic approach for distributed control of robotic swarms, inspired by the collective behaviors of swarms in nature such as schools of fish and flocks of birds. The agents are assumed to have limited sensory perception, lack memory, be Identical, anonymous, and operate without interagent explicit communication. Despite these limitations, we demonstrate that collaborative exploration and task allocation can be executed by applying simple local rules of interactions between the agents. A comprehensive model comprised of agent, formation, and swarm layers is proposed in this paper, where each layer performs a specific function in shaping the swarm's collective behavior, thereby contributing to the emergence of the anticipated behaviors. We consider four principles combined in the design of the distributed control process: Cohesiveness, Flexibility, Attraction-Repulsion, and Peristaltic Motion. We design the control algorithms as reactive behaviour that enables the swarm to maintain connectivity, adapt to dynamic environments, spread out and cover a region with a size determined by the number of agents, and respond to various local task requirements. We explore some simple broadcast control-based steering methods, that result in inducing \"anonymous ad-hoc leaders\" among the agents, capable of guiding the swarm towards yet unexplored regions with further tasks. Our analysis is complemented by simulations, validating the efficacy of our algorithms. The experiments with various scenarios showcase the swarm`s capability to self-organize and perform tasks effectively under the proposed framework. The possible implementations include domains that necessitate emergent coordination and control in multi-agent systems, without the need for advanced individual abilities or direct communication.","sentences":["This paper introduces a novel bio-mimetic approach for distributed control of robotic swarms, inspired by the collective behaviors of swarms in nature such as schools of fish and flocks of birds.","The agents are assumed to have limited sensory perception, lack memory, be Identical, anonymous, and operate without interagent explicit communication.","Despite these limitations, we demonstrate that collaborative exploration and task allocation can be executed by applying simple local rules of interactions between the agents.","A comprehensive model comprised of agent, formation, and swarm layers is proposed in this paper, where each layer performs a specific function in shaping the swarm's collective behavior, thereby contributing to the emergence of the anticipated behaviors.","We consider four principles combined in the design of the distributed control process: Cohesiveness, Flexibility, Attraction-Repulsion, and Peristaltic Motion.","We design the control algorithms as reactive behaviour that enables the swarm to maintain connectivity, adapt to dynamic environments, spread out and cover a region with a size determined by the number of agents, and respond to various local task requirements.","We explore some simple broadcast control-based steering methods, that result in inducing \"anonymous ad-hoc leaders\" among the agents, capable of guiding the swarm towards yet unexplored regions with further tasks.","Our analysis is complemented by simulations, validating the efficacy of our algorithms.","The experiments with various scenarios showcase the swarm`s capability to self-organize and perform tasks effectively under the proposed framework.","The possible implementations include domains that necessitate emergent coordination and control in multi-agent systems, without the need for advanced individual abilities or direct communication."],"url":"http://arxiv.org/abs/2405.13941v1","category":"cs.MA"}
{"created":"2024-05-22 19:06:39","title":"Text-Free Multi-domain Graph Pre-training:Toward Graph Foundation Models","abstract":"Given the ubiquity of graph data, it is intriguing to ask: Is it possible to train a graph foundation model on a broad range of graph data across diverse domains? A major hurdle toward this goal lies in the fact that graphs from different domains often exhibit profoundly divergent characteristics. Although there have been some initial efforts in integrating multi-domain graphs for pre-training, they primarily rely on textual descriptions to align the graphs, limiting their application to text-attributed graphs. Moreover, different source domains may conflict or interfere with each other, and their relevance to the target domain can vary significantly. To address these issues, we propose MDGPT, a text free Multi-Domain Graph Pre-Training and adaptation framework designed to exploit multi-domain knowledge for graph learning. First, we propose a set of domain tokens to to align features across source domains for synergistic pre-training. Second, we propose a dual prompts, consisting of a unifying prompt and a mixing prompt, to further adapt the target domain with unified multi-domain knowledge and a tailored mixture of domain-specific knowledge. Finally, we conduct extensive experiments involving six public datasets to evaluate and analyze MDGPT, which outperforms prior art by up to 37.9%.","sentences":["Given the ubiquity of graph data, it is intriguing to ask: Is it possible to train a graph foundation model on a broad range of graph data across diverse domains?","A major hurdle toward this goal lies in the fact that graphs from different domains often exhibit profoundly divergent characteristics.","Although there have been some initial efforts in integrating multi-domain graphs for pre-training, they primarily rely on textual descriptions to align the graphs, limiting their application to text-attributed graphs.","Moreover, different source domains may conflict or interfere with each other, and their relevance to the target domain can vary significantly.","To address these issues, we propose MDGPT, a text free Multi-Domain Graph Pre-Training and adaptation framework designed to exploit multi-domain knowledge for graph learning.","First, we propose a set of domain tokens to to align features across source domains for synergistic pre-training.","Second, we propose a dual prompts, consisting of a unifying prompt and a mixing prompt, to further adapt the target domain with unified multi-domain knowledge and a tailored mixture of domain-specific knowledge.","Finally, we conduct extensive experiments involving six public datasets to evaluate and analyze MDGPT, which outperforms prior art by up to 37.9%."],"url":"http://arxiv.org/abs/2405.13934v1","category":"cs.LG"}
{"created":"2024-05-22 18:58:58","title":"Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian","abstract":"There has been a surge in the development of various Large Language Models (LLMs). However, text generation for languages other than English often faces significant challenges, including poor generation quality and the reduced computational performance due to the disproportionate representation of tokens in model's vocabulary. In this work, we address these issues and introduce Vikhr, a new state-of-the-art open-source instruction-tuned LLM designed specifically for the Russian language. Unlike previous efforts for Russian that utilize computationally inexpensive LoRA adapters on top of English-oriented models, Vikhr features an adapted tokenizer vocabulary and undergoes the continued pre-training and instruction tuning of all weights. This approach not only enhances the model's performance but also significantly improves its computational and contextual efficiency. The remarkable performance of Vikhr across various Russian-language benchmarks can also be attributed to our efforts in expanding instruction datasets and corpora for continued pre-training. Vikhr not only sets the new state of the art among open-source LLMs for Russian, but even outperforms some proprietary closed-source models on certain benchmarks. The model weights, instruction sets, and code are publicly available","sentences":["There has been a surge in the development of various Large Language Models (LLMs).","However, text generation for languages other than English often faces significant challenges, including poor generation quality and the reduced computational performance due to the disproportionate representation of tokens in model's vocabulary.","In this work, we address these issues and introduce Vikhr, a new state-of-the-art open-source instruction-tuned LLM designed specifically for the Russian language.","Unlike previous efforts for Russian that utilize computationally inexpensive LoRA adapters on top of English-oriented models, Vikhr features an adapted tokenizer vocabulary and undergoes the continued pre-training and instruction tuning of all weights.","This approach not only enhances the model's performance but also significantly improves its computational and contextual efficiency.","The remarkable performance of Vikhr across various Russian-language benchmarks can also be attributed to our efforts in expanding instruction datasets and corpora for continued pre-training.","Vikhr not only sets the new state of the art among open-source LLMs for Russian, but even outperforms some proprietary closed-source models on certain benchmarks.","The model weights, instruction sets, and code are publicly available"],"url":"http://arxiv.org/abs/2405.13929v1","category":"cs.CL"}
{"created":"2024-05-22 18:53:25","title":"Why Not Transform Chat Large Language Models to Non-English?","abstract":"The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.","sentences":["The scarcity of non-English data limits the development of non-English large language models (LLMs).","Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method.","Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4.","Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety.","However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data?","(2) How can we prevent the original knowledge from catastrophic forgetting during transformation?","We target these issues by introducing a simple framework called TransLLM.","For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step.","We further enhance the performance of sub-tasks with publicly available data.","For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters.","In the experiments, we transform the LLaMA-2-chat-7B to the Thai language.","Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench.","Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4."],"url":"http://arxiv.org/abs/2405.13923v1","category":"cs.CL"}
{"created":"2024-05-22 18:41:11","title":"HeteGraph-Mamba: Heterogeneous Graph Learning via Selective State Space Model","abstract":"We propose a heterogeneous graph mamba network (HGMN) as the first exploration in leveraging the selective state space models (SSSMs) for heterogeneous graph learning. Compared with the literature, our HGMN overcomes two major challenges: (i) capturing long-range dependencies among heterogeneous nodes and (ii) adapting SSSMs to heterogeneous graph data. Our key contribution is a general graph architecture that can solve heterogeneous nodes in real-world scenarios, followed an efficient flow. Methodologically, we introduce a two-level efficient tokenization approach that first captures long-range dependencies within identical node types, and subsequently across all node types. Empirically, we conduct comparisons between our framework and 19 state-of-the-art methods on the heterogeneous benchmarks. The extensive comparisons demonstrate that our framework outperforms other methods in both the accuracy and efficiency dimensions.","sentences":["We propose a heterogeneous graph mamba network (HGMN) as the first exploration in leveraging the selective state space models (SSSMs) for heterogeneous graph learning.","Compared with the literature, our HGMN overcomes two major challenges: (i) capturing long-range dependencies among heterogeneous nodes and (ii) adapting SSSMs to heterogeneous graph data.","Our key contribution is a general graph architecture that can solve heterogeneous nodes in real-world scenarios, followed an efficient flow.","Methodologically, we introduce a two-level efficient tokenization approach that first captures long-range dependencies within identical node types, and subsequently across all node types.","Empirically, we conduct comparisons between our framework and 19 state-of-the-art methods on the heterogeneous benchmarks.","The extensive comparisons demonstrate that our framework outperforms other methods in both the accuracy and efficiency dimensions."],"url":"http://arxiv.org/abs/2405.13915v1","category":"cs.LG"}
{"created":"2024-05-22 18:39:55","title":"A geometrical description of non-Hermitian dynamics: speed limits in finite rank density operators","abstract":"Non-Hermitian dynamics in quantum systems preserves the rank of the state density operator. We use this insight to develop its geometrical description. In particular, we identify mutually orthogonal coherent and incoherent directions and give their physical interpretation. This understanding allows us to optimize the success rate for implementing non-Hermitian driving along prescribed trajectories. We show its significance for shortcuts to adiabaticity. We introduce the geometrical interpretation of a speed limit for non-Hermitian Hamiltonians and analyze its tightness. We derive the explicit expression that saturates such a speed limit and illustrate our results on a minimal example of a dissipative qubit.","sentences":["Non-Hermitian dynamics in quantum systems preserves the rank of the state density operator.","We use this insight to develop its geometrical description.","In particular, we identify mutually orthogonal coherent and incoherent directions and give their physical interpretation.","This understanding allows us to optimize the success rate for implementing non-Hermitian driving along prescribed trajectories.","We show its significance for shortcuts to adiabaticity.","We introduce the geometrical interpretation of a speed limit for non-Hermitian Hamiltonians and analyze its tightness.","We derive the explicit expression that saturates such a speed limit and illustrate our results on a minimal example of a dissipative qubit."],"url":"http://arxiv.org/abs/2405.13913v1","category":"quant-ph"}
{"created":"2024-05-22 18:29:02","title":"Spin-Peierls transition in the frustrated spinels ZnCr2O4 and MgCr2O4","abstract":"The chromium spinels MgCr2O4 and ZnCr2O4 are prime examples of the highly frustrated pyrochlore lattice antiferromagnet. Experiment has carefully established that both materials, upon cooling, distort to lower symmetry and order magnetically. We study the nature of this process by a combination of density-functional-theory based energy mapping and classical Monte Carlo simulations. We first computationally establish precise Heisenberg Hamiltonian parameters for the high temperature cubic and the low temperature tetragonal and orthorhombic structures of both spinels. We then investigate the respective ordering temperatures of high symmetry and low symmetry structures. We carefully compare our results with experimental facts and find that our simulations are remarkably consistent with a type of spin-Peierls mechanism, adapted to three dimensions, where the structural distortion is mediated by a magnetic energy gain due to a lower degree of frustration.","sentences":["The chromium spinels MgCr2O4 and ZnCr2O4 are prime examples of the highly frustrated pyrochlore lattice antiferromagnet.","Experiment has carefully established that both materials, upon cooling, distort to lower symmetry and order magnetically.","We study the nature of this process by a combination of density-functional-theory based energy mapping and classical Monte Carlo simulations.","We first computationally establish precise Heisenberg Hamiltonian parameters for the high temperature cubic and the low temperature tetragonal and orthorhombic structures of both spinels.","We then investigate the respective ordering temperatures of high symmetry and low symmetry structures.","We carefully compare our results with experimental facts and find that our simulations are remarkably consistent with a type of spin-Peierls mechanism, adapted to three dimensions, where the structural distortion is mediated by a magnetic energy gain due to a lower degree of frustration."],"url":"http://arxiv.org/abs/2405.13908v1","category":"cond-mat.str-el"}
{"created":"2024-05-22 18:13:38","title":"Rehearsal-free Federated Domain-incremental Learning","abstract":"We introduce a rehearsal-free federated domain incremental learning framework, RefFiL, based on a global prompt-sharing paradigm to alleviate catastrophic forgetting challenges in federated domain-incremental learning, where unseen domains are continually learned. Typical methods for mitigating forgetting, such as the use of additional datasets and the retention of private data from earlier tasks, are not viable in federated learning (FL) due to devices' limited resources. Our method, RefFiL, addresses this by learning domain-invariant knowledge and incorporating various domain-specific prompts from the domains represented by different FL participants. A key feature of RefFiL is the generation of local fine-grained prompts by our domain adaptive prompt generator, which effectively learns from local domain knowledge while maintaining distinctive boundaries on a global scale. We also introduce a domain-specific prompt contrastive learning loss that differentiates between locally generated prompts and those from other domains, enhancing RefFiL's precision and effectiveness. Compared to existing methods, RefFiL significantly alleviates catastrophic forgetting without requiring extra memory space, making it ideal for privacy-sensitive and resource-constrained devices.","sentences":["We introduce a rehearsal-free federated domain incremental learning framework, RefFiL, based on a global prompt-sharing paradigm to alleviate catastrophic forgetting challenges in federated domain-incremental learning, where unseen domains are continually learned.","Typical methods for mitigating forgetting, such as the use of additional datasets and the retention of private data from earlier tasks, are not viable in federated learning (FL) due to devices' limited resources.","Our method, RefFiL, addresses this by learning domain-invariant knowledge and incorporating various domain-specific prompts from the domains represented by different FL participants.","A key feature of RefFiL is the generation of local fine-grained prompts by our domain adaptive prompt generator, which effectively learns from local domain knowledge while maintaining distinctive boundaries on a global scale.","We also introduce a domain-specific prompt contrastive learning loss that differentiates between locally generated prompts and those from other domains, enhancing RefFiL's precision and effectiveness.","Compared to existing methods, RefFiL significantly alleviates catastrophic forgetting without requiring extra memory space, making it ideal for privacy-sensitive and resource-constrained devices."],"url":"http://arxiv.org/abs/2405.13900v1","category":"cs.LG"}
{"created":"2024-05-22 18:08:26","title":"A General Framework for Jersey Number Recognition in Sports Video","abstract":"Jersey number recognition is an important task in sports video analysis, partly due to its importance for long-term player tracking. It can be viewed as a variant of scene text recognition. However, there is a lack of published attempts to apply scene text recognition models on jersey number data. Here we introduce a novel public jersey number recognition dataset for hockey and study how scene text recognition methods can be adapted to this problem. We address issues of occlusions and assess the degree to which training on one sport (hockey) can be generalized to another (soccer). For the latter, we also consider how jersey number recognition at the single-image level can be aggregated across frames to yield tracklet-level jersey number labels. We demonstrate high performance on image- and tracklet-level tasks, achieving 91.4% accuracy for hockey images and 87.4% for soccer tracklets. Code, models, and data are available at https://github.com/mkoshkina/jersey-number-pipeline.","sentences":["Jersey number recognition is an important task in sports video analysis, partly due to its importance for long-term player tracking.","It can be viewed as a variant of scene text recognition.","However, there is a lack of published attempts to apply scene text recognition models on jersey number data.","Here we introduce a novel public jersey number recognition dataset for hockey and study how scene text recognition methods can be adapted to this problem.","We address issues of occlusions and assess the degree to which training on one sport (hockey) can be generalized to another (soccer).","For the latter, we also consider how jersey number recognition at the single-image level can be aggregated across frames to yield tracklet-level jersey number labels.","We demonstrate high performance on image- and tracklet-level tasks, achieving 91.4% accuracy for hockey images and 87.4% for soccer tracklets.","Code, models, and data are available at https://github.com/mkoshkina/jersey-number-pipeline."],"url":"http://arxiv.org/abs/2405.13896v1","category":"cs.CV"}
{"created":"2024-05-22 17:46:08","title":"ReVideo: Remake a Video with Motion and Content Control","abstract":"Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.","sentences":["Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge.","Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing.","In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion.","Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience.","ReVideo addresses a new task involving the coupling and training imbalance between content and motion control.","To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine.","Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations.","Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories.","Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness."],"url":"http://arxiv.org/abs/2405.13865v1","category":"cs.CV"}
{"created":"2024-05-22 17:38:16","title":"Transformers Learn Temporal Difference Methods for In-Context Reinforcement Learning","abstract":"In-context learning refers to the learning ability of a model during inference time without adapting its parameters. The input (i.e., prompt) to the model (e.g., transformers) consists of both a context (i.e., instance-label pairs) and a query instance. The model is then able to output a label for the query instance according to the context during inference. A possible explanation for in-context learning is that the forward pass of (linear) transformers implements iterations of gradient descent on the instance-label pairs in the context. In this paper, we prove by construction that transformers can also implement temporal difference (TD) learning in the forward pass, a phenomenon we refer to as in-context TD. We demonstrate the emergence of in-context TD after training the transformer with a multi-task TD algorithm, accompanied by theoretical analysis. Furthermore, we prove that transformers are expressive enough to implement many other policy evaluation algorithms in the forward pass, including residual gradient, TD with eligibility trace, and average-reward TD.","sentences":["In-context learning refers to the learning ability of a model during inference time without adapting its parameters.","The input (i.e., prompt) to the model (e.g., transformers) consists of both a context (i.e., instance-label pairs) and a query instance.","The model is then able to output a label for the query instance according to the context during inference.","A possible explanation for in-context learning is that the forward pass of (linear) transformers implements iterations of gradient descent on the instance-label pairs in the context.","In this paper, we prove by construction that transformers can also implement temporal difference (TD) learning in the forward pass, a phenomenon we refer to as in-context TD.","We demonstrate the emergence of in-context TD after training the transformer with a multi-task TD algorithm, accompanied by theoretical analysis.","Furthermore, we prove that transformers are expressive enough to implement many other policy evaluation algorithms in the forward pass, including residual gradient, TD with eligibility trace, and average-reward TD."],"url":"http://arxiv.org/abs/2405.13861v1","category":"cs.LG"}
{"created":"2024-05-22 17:30:02","title":"Consistent expansion of the Langevin propagator with application to entropy production","abstract":"Stochastic thermodynamics is a developing theory for systems out of thermal equilibrium. It allows to formulate a wealth of nontrivial relations among thermodynamic quantities such as heat dissipation, excess work, and entropy production in generic nonequilibrium stochastic processes. A key quantity for the derivation of these relations is the propagator - the probability to observe a transition from one point in phase space to another after a given time. Here, applying stochastic Taylor expansions, we devise a formal expansion procedure for the propagator of overdamped Langevin dynamics. The three leading orders are obtained explicitly. The technique resolves the shortcomings of the current mathematical machinery for the calculation of the propagator. For the evaluation of the first two displacement cumulants, the leading order Gaussian propagator is sufficient. However, some functionals of the propagator - such as the entropy production - which we refer to as \"first derivatives of the trajectory\", need to be evaluated to a previously-unrecognized higher order. The method presented here can be extended to arbitrarily higher orders in order to accurately compute any other functional of the propagator.","sentences":["Stochastic thermodynamics is a developing theory for systems out of thermal equilibrium.","It allows to formulate a wealth of nontrivial relations among thermodynamic quantities such as heat dissipation, excess work, and entropy production in generic nonequilibrium stochastic processes.","A key quantity for the derivation of these relations is the propagator - the probability to observe a transition from one point in phase space to another after a given time.","Here, applying stochastic Taylor expansions, we devise a formal expansion procedure for the propagator of overdamped Langevin dynamics.","The three leading orders are obtained explicitly.","The technique resolves the shortcomings of the current mathematical machinery for the calculation of the propagator.","For the evaluation of the first two displacement cumulants, the leading order Gaussian propagator is sufficient.","However, some functionals of the propagator - such as the entropy production - which we refer to as \"first derivatives of the trajectory\", need to be evaluated to a previously-unrecognized higher order.","The method presented here can be extended to arbitrarily higher orders in order to accurately compute any other functional of the propagator."],"url":"http://arxiv.org/abs/2405.13855v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-22 17:14:19","title":"AI-Protected Blockchain-based IoT environments: Harnessing the Future of Network Security and Privacy","abstract":"Integrating blockchain technology with the Internet of Things offers transformative possibilities for enhancing network security and privacy in the contemporary digital landscape, where interconnected devices and expansive networks are ubiquitous. This paper explores the pivotal role of artificial intelligence in bolstering blockchain-enabled IoT systems, potentially marking a significant leap forward in safeguarding data integrity and confidentiality across networks. Blockchain technology provides a decentralized and immutable ledger, ideal for the secure management of device identities and transactions in IoT networks. When coupled with AI, these systems gain the ability to not only automate and optimize security protocols but also adaptively respond to new and evolving cyber threats. This dual capability enhances the resilience of networks against cyber-attacks, a critical consideration as IoT devices increasingly permeate critical infrastructures. The synergy between AI and blockchain in IoT is profound. AI algorithms can analyze vast amounts of data from IoT devices to detect patterns and anomalies that may signify security breaches. Concurrently, blockchain can ensure that data records are tamper-proof, enhancing the reliability of AI-driven security measures. Moreover, this research evaluates the implications of AI-enhanced blockchain systems on privacy protection within IoT networks. IoT devices often collect sensitive personal data, making privacy a paramount concern. AI can facilitate the development of new protocols that ensure data privacy and user anonymity without compromising the functionality of IoT systems. Through comprehensive analysis and case studies, this paper aims to provide an in-depth understanding of how AI-enhanced blockchain technology can revolutionize network security and privacy in IoT environments.","sentences":["Integrating blockchain technology with the Internet of Things offers transformative possibilities for enhancing network security and privacy in the contemporary digital landscape, where interconnected devices and expansive networks are ubiquitous.","This paper explores the pivotal role of artificial intelligence in bolstering blockchain-enabled IoT systems, potentially marking a significant leap forward in safeguarding data integrity and confidentiality across networks.","Blockchain technology provides a decentralized and immutable ledger, ideal for the secure management of device identities and transactions in IoT networks.","When coupled with AI, these systems gain the ability to not only automate and optimize security protocols but also adaptively respond to new and evolving cyber threats.","This dual capability enhances the resilience of networks against cyber-attacks, a critical consideration as IoT devices increasingly permeate critical infrastructures.","The synergy between AI and blockchain in IoT is profound.","AI algorithms can analyze vast amounts of data from IoT devices to detect patterns and anomalies that may signify security breaches.","Concurrently, blockchain can ensure that data records are tamper-proof, enhancing the reliability of AI-driven security measures.","Moreover, this research evaluates the implications of AI-enhanced blockchain systems on privacy protection within IoT networks.","IoT devices often collect sensitive personal data, making privacy a paramount concern.","AI can facilitate the development of new protocols that ensure data privacy and user anonymity without compromising the functionality of IoT systems.","Through comprehensive analysis and case studies, this paper aims to provide an in-depth understanding of how AI-enhanced blockchain technology can revolutionize network security and privacy in IoT environments."],"url":"http://arxiv.org/abs/2405.13847v1","category":"cs.CR"}
{"created":"2024-05-22 17:11:19","title":"Robot Explanation Identity","abstract":"To bring robots into human everyday life, their capacity for social interaction must increase. One way for robots to acquire social skills is by assigning them the concept of identity. This research focuses on the concept of \\textit{Explanation Identity} within the broader context of robots' roles in society, particularly their ability to interact socially and explain decisions. Explanation Identity refers to the combination of characteristics and approaches robots use to justify their actions to humans. Drawing from different technical and social disciplines, we introduce Explanation Identity as a multidisciplinary concept and discuss its importance in Human-Robot Interaction. Our theoretical framework highlights the necessity for robots to adapt their explanations to the user's context, demonstrating empathy and ethical integrity. This research emphasizes the dynamic nature of robot identity and guides the integration of explanation capabilities in social robots, aiming to improve user engagement and acceptance.","sentences":["To bring robots into human everyday life, their capacity for social interaction must increase.","One way for robots to acquire social skills is by assigning them the concept of identity.","This research focuses on the concept of \\textit{Explanation Identity} within the broader context of robots' roles in society, particularly their ability to interact socially and explain decisions.","Explanation Identity refers to the combination of characteristics and approaches robots use to justify their actions to humans.","Drawing from different technical and social disciplines, we introduce Explanation Identity as a multidisciplinary concept and discuss its importance in Human-Robot Interaction.","Our theoretical framework highlights the necessity for robots to adapt their explanations to the user's context, demonstrating empathy and ethical integrity.","This research emphasizes the dynamic nature of robot identity and guides the integration of explanation capabilities in social robots, aiming to improve user engagement and acceptance."],"url":"http://arxiv.org/abs/2405.13841v1","category":"cs.RO"}
{"created":"2024-05-22 16:52:29","title":"Normalizing Basis Functions: Approximate Stationary Models for Large Spatial Data","abstract":"In geostatistics, traditional spatial models often rely on the Gaussian Process (GP) to fit stationary covariances to data. It is well known that this approach becomes computationally infeasible when dealing with large data volumes, necessitating the use of approximate methods. A powerful class of methods approximate the GP as a sum of basis functions with random coefficients. Although this technique offers computational efficiency, it does not inherently guarantee a stationary covariance. To mitigate this issue, the basis functions can be \"normalized\" to maintain a constant marginal variance, avoiding unwanted artifacts and edge effects. This allows for the fitting of nearly stationary models to large, potentially non-stationary datasets, providing a rigorous base to extend to more complex problems. Unfortunately, the process of normalizing these basis functions is computationally demanding. To address this, we introduce two fast and accurate algorithms to the normalization step, allowing for efficient prediction on fine grids. The practical value of these algorithms is showcased in the context of a spatial analysis on a large dataset, where significant computational speedups are achieved. While implementation and testing are done specifically within the LatticeKrig framework, these algorithms can be adapted to other basis function methods operating on regular grids.","sentences":["In geostatistics, traditional spatial models often rely on the Gaussian Process (GP) to fit stationary covariances to data.","It is well known that this approach becomes computationally infeasible when dealing with large data volumes, necessitating the use of approximate methods.","A powerful class of methods approximate the GP as a sum of basis functions with random coefficients.","Although this technique offers computational efficiency, it does not inherently guarantee a stationary covariance.","To mitigate this issue, the basis functions can be \"normalized\" to maintain a constant marginal variance, avoiding unwanted artifacts and edge effects.","This allows for the fitting of nearly stationary models to large, potentially non-stationary datasets, providing a rigorous base to extend to more complex problems.","Unfortunately, the process of normalizing these basis functions is computationally demanding.","To address this, we introduce two fast and accurate algorithms to the normalization step, allowing for efficient prediction on fine grids.","The practical value of these algorithms is showcased in the context of a spatial analysis on a large dataset, where significant computational speedups are achieved.","While implementation and testing are done specifically within the LatticeKrig framework, these algorithms can be adapted to other basis function methods operating on regular grids."],"url":"http://arxiv.org/abs/2405.13821v1","category":"stat.CO"}
{"created":"2024-05-22 16:41:23","title":"Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations","abstract":"The rapid expansion of Location-Based Social Networks (LBSNs) has highlighted the importance of effective next Point-of-Interest (POI) recommendations, which leverage historical check-in data to predict users' next POIs to visit. Traditional centralized deep neural networks (DNNs) offer impressive POI recommendation performance but face challenges due to privacy concerns and limited timeliness. In response, on-device POI recommendations have been introduced, utilizing federated learning (FL) and decentralized approaches to ensure privacy and recommendation timeliness. However, these methods often suffer from computational strain on devices and struggle to adapt to new users and regions. This paper introduces a novel collaborative learning framework, Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations (DCPR), leveraging the diffusion model known for its success across various domains. DCPR operates with a cloud-edge-device architecture to offer region-specific and highly personalized POI recommendations while reducing on-device computational burdens. DCPR minimizes on-device computational demands through a unique blend of global and local learning processes. Our evaluation with two real-world datasets demonstrates DCPR's superior performance in recommendation accuracy, efficiency, and adaptability to new users and regions, marking a significant step forward in on-device POI recommendation technology.","sentences":["The rapid expansion of Location-Based Social Networks (LBSNs) has highlighted the importance of effective next Point-of-Interest (POI) recommendations, which leverage historical check-in data to predict users' next POIs to visit.","Traditional centralized deep neural networks (DNNs) offer impressive POI recommendation performance but face challenges due to privacy concerns and limited timeliness.","In response, on-device POI recommendations have been introduced, utilizing federated learning (FL) and decentralized approaches to ensure privacy and recommendation timeliness.","However, these methods often suffer from computational strain on devices and struggle to adapt to new users and regions.","This paper introduces a novel collaborative learning framework, Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations (DCPR), leveraging the diffusion model known for its success across various domains.","DCPR operates with a cloud-edge-device architecture to offer region-specific and highly personalized POI recommendations while reducing on-device computational burdens.","DCPR minimizes on-device computational demands through a unique blend of global and local learning processes.","Our evaluation with two real-world datasets demonstrates DCPR's superior performance in recommendation accuracy, efficiency, and adaptability to new users and regions, marking a significant step forward in on-device POI recommendation technology."],"url":"http://arxiv.org/abs/2405.13811v1","category":"cs.IR"}
{"created":"2024-05-22 16:37:22","title":"Hybrid Quantum-Classical Normalizing Flow","abstract":"With the rapid development of quantum computing technology, we have entered the era of noisy intermediate-scale quantum (NISQ) computers. Therefore, designing quantum algorithms that adapt to the hardware conditions of current NISQ devices and can preliminarily solve some practical problems has become the focus of researchers. In this paper, we focus on quantum generative models in the field of quantum machine learning, and propose a hybrid quantum-classical normalizing flow (HQCNF) model based on parameterized quantum circuits. Based on the ideas of classical normalizing flow models and the characteristics of parameterized quantum circuits, we cleverly design the form of the ansatz and the hybrid method of quantum and classical computing, and derive the form of the loss function in the case that quantum computing is involved. We test our model on the image generation problem. Experimental results show that our model is capable of generating images of good quality. Compared with other quantum generative models, such as quantum generative adversarial networks (QGAN), our model achieves lower (better) Fr\\'echet inception distance (FID) score, and compared with classical generative models, we can complete the image generation task with significantly fewer parameters. These results prove the advantage of our proposed model.","sentences":["With the rapid development of quantum computing technology, we have entered the era of noisy intermediate-scale quantum (NISQ) computers.","Therefore, designing quantum algorithms that adapt to the hardware conditions of current NISQ devices and can preliminarily solve some practical problems has become the focus of researchers.","In this paper, we focus on quantum generative models in the field of quantum machine learning, and propose a hybrid quantum-classical normalizing flow (HQCNF) model based on parameterized quantum circuits.","Based on the ideas of classical normalizing flow models and the characteristics of parameterized quantum circuits, we cleverly design the form of the ansatz and the hybrid method of quantum and classical computing, and derive the form of the loss function in the case that quantum computing is involved.","We test our model on the image generation problem.","Experimental results show that our model is capable of generating images of good quality.","Compared with other quantum generative models, such as quantum generative adversarial networks (QGAN), our model achieves lower (better) Fr\\'echet inception distance (FID) score, and compared with classical generative models, we can complete the image generation task with significantly fewer parameters.","These results prove the advantage of our proposed model."],"url":"http://arxiv.org/abs/2405.13808v1","category":"quant-ph"}
{"created":"2024-05-22 16:36:29","title":"MPI Progress For All","abstract":"The progression of communication in the Message Passing Interface (MPI) is not well defined, yet it is critical for application performance, particularly in achieving effective computation and communication overlap. The opaque nature of MPI progress poses significant challenges in advancing MPI within modern high-performance computing (HPC) practices. Firstly, the lack of clarity hinders the development of explicit guidelines for enhancing computation and communication overlap in applications. Secondly, it prevents MPI from seamlessly integrating with contemporary programming paradigms, such as task-based runtimes and event-driven programming. Thirdly, it limits the extension of MPI functionalities from the user space. In this paper, we examine the role of MPI progress by analyzing the implementation details of MPI messaging. We then generalize the asynchronous communication pattern and identify key factors influencing application performance. Based on this analysis, we propose a set of MPI extensions designed to enable users to explicitly construct and manage an efficient progress engine. We provide example codes to demonstrate the use of these proposed APIs in achieving improved performance, adapting MPI to task-based or event-driven programming styles, and constructing collective algorithms that rival the performance of native implementations. Our approach is compared to previous efforts in the field, highlighting its reduced complexity and increased effectiveness.","sentences":["The progression of communication in the Message Passing Interface (MPI) is not well defined, yet it is critical for application performance, particularly in achieving effective computation and communication overlap.","The opaque nature of MPI progress poses significant challenges in advancing MPI within modern high-performance computing (HPC) practices.","Firstly, the lack of clarity hinders the development of explicit guidelines for enhancing computation and communication overlap in applications.","Secondly, it prevents MPI from seamlessly integrating with contemporary programming paradigms, such as task-based runtimes and event-driven programming.","Thirdly, it limits the extension of MPI functionalities from the user space.","In this paper, we examine the role of MPI progress by analyzing the implementation details of MPI messaging.","We then generalize the asynchronous communication pattern and identify key factors influencing application performance.","Based on this analysis, we propose a set of MPI extensions designed to enable users to explicitly construct and manage an efficient progress engine.","We provide example codes to demonstrate the use of these proposed APIs in achieving improved performance, adapting MPI to task-based or event-driven programming styles, and constructing collective algorithms that rival the performance of native implementations.","Our approach is compared to previous efforts in the field, highlighting its reduced complexity and increased effectiveness."],"url":"http://arxiv.org/abs/2405.13807v1","category":"cs.DC"}
{"created":"2024-05-22 16:21:02","title":"Generalizing Weather Forecast to Fine-grained Temporal Scales via Physics-AI Hybrid Modeling","abstract":"Data-driven artificial intelligence (AI) models have made significant advancements in weather forecasting, particularly in medium-range and nowcasting. However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension. Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales. This paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which Generalizes weather forecasts to Finer-grained Temporal scales beyond training dataset. Specifically, we employ a carefully designed PDE kernel to simulate physical evolution on a small time scale (e.g., 300 seconds) and use a parallel neural networks with a learnable router for bias correction. Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times. The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively. Extensive experiments show that WeatherGFT trained on an hourly dataset, achieves state-of-the-art performance across multiple lead times and exhibits the capability to generalize 30-minute forecasts.","sentences":["Data-driven artificial intelligence (AI) models have made significant advancements in weather forecasting, particularly in medium-range and nowcasting.","However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension.","Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales.","This paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which Generalizes weather forecasts to Finer-grained Temporal scales beyond training dataset.","Specifically, we employ a carefully designed PDE kernel to simulate physical evolution on a small time scale (e.g., 300 seconds) and use a parallel neural networks with a learnable router for bias correction.","Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times.","The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively.","Extensive experiments show that WeatherGFT trained on an hourly dataset, achieves state-of-the-art performance across multiple lead times and exhibits the capability to generalize 30-minute forecasts."],"url":"http://arxiv.org/abs/2405.13796v1","category":"cs.LG"}
{"created":"2024-05-22 16:15:17","title":"xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token","abstract":"This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems","sentences":["This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation.","xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality.","By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate.","In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen.","This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation.","Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration.","xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53.","Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems"],"url":"http://arxiv.org/abs/2405.13792v1","category":"cs.CL"}
{"created":"2024-05-22 16:02:36","title":"On the integrality gap of the Complete Metric Steiner Tree Problem via a novel formulation","abstract":"In this work, we compute the lower bound of the integrality gap of the Metric Steiner Tree Problem (MSTP) on a graph for some small values of number of nodes and terminals. After debating about some limitations of the most used formulation for the Steiner Tree Problem, namely the Bidirected Cut Formulation, we introduce a novel formulation, that we named Complete Metric formulation, tailored for the metric case. We prove some interesting properties of this formulation and characterize some types of vertices. Finally, we define a linear program (LP) by adapting a method already used in the context of the Travelling Salesman Problem. This LP takes as input a vertex of the polytope of the CM relaxation and provides an MSTP instance such that (a) the optimal solution is precisely that vertex and (b) among all of the instances having that vertex as its optimal solution, the selected instance is the one having the highest integrality gap. We propose two heuristics for generating vertices to provide inputs for our procedure. In conclusion, we raise several conjectures and open questions.","sentences":["In this work, we compute the lower bound of the integrality gap of the Metric Steiner Tree Problem (MSTP) on a graph for some small values of number of nodes and terminals.","After debating about some limitations of the most used formulation for the Steiner Tree Problem, namely the Bidirected Cut Formulation, we introduce a novel formulation, that we named Complete Metric formulation, tailored for the metric case.","We prove some interesting properties of this formulation and characterize some types of vertices.","Finally, we define a linear program (LP) by adapting a method already used in the context of the Travelling Salesman Problem.","This LP takes as input a vertex of the polytope of the CM relaxation and provides an MSTP instance such that (a) the optimal solution is precisely that vertex and (b) among all of the instances having that vertex as its optimal solution, the selected instance is the one having the highest integrality gap.","We propose two heuristics for generating vertices to provide inputs for our procedure.","In conclusion, we raise several conjectures and open questions."],"url":"http://arxiv.org/abs/2405.13773v1","category":"math.OC"}
{"created":"2024-05-22 15:51:30","title":"The Power of Extrapolation in Federated Learning","abstract":"We propose and study several server-extrapolation strategies for enhancing the theoretical and empirical convergence properties of the popular federated learning optimizer FedProx [Li et al., 2020]. While it has long been known that some form of extrapolation can help in the practice of FL, only a handful of works provide any theoretical guarantees. The phenomenon seems elusive, and our current theoretical understanding remains severely incomplete. In our work, we focus on smooth convex or strongly convex problems in the interpolation regime. In particular, we propose Extrapolated FedProx (FedExProx), and study three extrapolation strategies: a constant strategy (depending on various smoothness parameters and the number of participating devices), and two smoothness-adaptive strategies; one based on the notion of gradient diversity (FedExProx-GraDS), and the other one based on the stochastic Polyak stepsize (FedExProx-StoPS). Our theory is corroborated with carefully constructed numerical experiments.","sentences":["We propose and study several server-extrapolation strategies for enhancing the theoretical and empirical convergence properties of the popular federated learning optimizer FedProx","[Li et al., 2020].","While it has long been known that some form of extrapolation can help in the practice of FL, only a handful of works provide any theoretical guarantees.","The phenomenon seems elusive, and our current theoretical understanding remains severely incomplete.","In our work, we focus on smooth convex or strongly convex problems in the interpolation regime.","In particular, we propose Extrapolated FedProx (FedExProx), and study three extrapolation strategies: a constant strategy (depending on various smoothness parameters and the number of participating devices), and two smoothness-adaptive strategies; one based on the notion of gradient diversity (FedExProx-GraDS), and the other one based on the stochastic Polyak stepsize (FedExProx-StoPS).","Our theory is corroborated with carefully constructed numerical experiments."],"url":"http://arxiv.org/abs/2405.13766v1","category":"math.OC"}
{"created":"2024-05-22 15:32:38","title":"CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models","abstract":"The success of current Large-Language Models (LLMs) hinges on extensive training data that is collected and stored centrally, called Centralized Learning (CL). However, such a collection manner poses a privacy threat, and one potential solution is Federated Learning (FL), which transfers gradients, not raw data, among clients. Unlike traditional networks, FL for LLMs incurs significant communication costs due to their tremendous parameters. This study introduces an innovative approach to compress gradients to improve communication efficiency during LLM FL, formulating the new FL pipeline named CG-FedLLM. This approach integrates an encoder on the client side to acquire the compressed gradient features and a decoder on the server side to reconstruct the gradients. We also developed a novel training strategy that comprises Temporal-ensemble Gradient-Aware Pre-training (TGAP) to identify characteristic gradients of the target model and Federated AutoEncoder-Involved Fine-tuning (FAF) to compress gradients adaptively. Extensive experiments confirm that our approach reduces communication costs and improves performance (e.g., average 3 points increment compared with traditional CL- and FL-based fine-tuning with LlaMA on a well-recognized benchmark, C-Eval). This improvement is because our encoder-decoder, trained via TGAP and FAF, can filter gradients while selectively preserving critical features. Furthermore, we present a series of experimental analyses focusing on the signal-to-noise ratio, compression rate, and robustness within this privacy-centric framework, providing insight into developing more efficient and secure LLMs.","sentences":["The success of current Large-Language Models (LLMs) hinges on extensive training data that is collected and stored centrally, called Centralized Learning (CL).","However, such a collection manner poses a privacy threat, and one potential solution is Federated Learning (FL), which transfers gradients, not raw data, among clients.","Unlike traditional networks, FL for LLMs incurs significant communication costs due to their tremendous parameters.","This study introduces an innovative approach to compress gradients to improve communication efficiency during LLM FL, formulating the new FL pipeline named CG-FedLLM.","This approach integrates an encoder on the client side to acquire the compressed gradient features and a decoder on the server side to reconstruct the gradients.","We also developed a novel training strategy that comprises Temporal-ensemble Gradient-Aware Pre-training (TGAP) to identify characteristic gradients of the target model and Federated AutoEncoder-Involved Fine-tuning (FAF) to compress gradients adaptively.","Extensive experiments confirm that our approach reduces communication costs and improves performance (e.g., average 3 points increment compared with traditional CL- and FL-based fine-tuning with LlaMA on a well-recognized benchmark, C-Eval).","This improvement is because our encoder-decoder, trained via TGAP and FAF, can filter gradients while selectively preserving critical features.","Furthermore, we present a series of experimental analyses focusing on the signal-to-noise ratio, compression rate, and robustness within this privacy-centric framework, providing insight into developing more efficient and secure LLMs."],"url":"http://arxiv.org/abs/2405.13746v1","category":"cs.LG"}
{"created":"2024-05-22 15:20:27","title":"Score-based Generative Models with Adaptive Momentum","abstract":"Score-based generative models have demonstrated significant practical success in data-generating tasks. The models establish a diffusion process that perturbs the ground truth data to Gaussian noise and then learn the reverse process to transform noise into data. However, existing denoising methods such as Langevin dynamic and numerical stochastic differential equation solvers enjoy randomness but generate data slowly with a large number of score function evaluations, and the ordinary differential equation solvers enjoy faster sampling speed but no randomness may influence the sample quality. To this end, motivated by the Stochastic Gradient Descent (SGD) optimization methods and the high connection between the model sampling process with the SGD, we propose adaptive momentum sampling to accelerate the transforming process without introducing additional hyperparameters. Theoretically, we proved our method promises convergence under given conditions. In addition, we empirically show that our sampler can produce more faithful images/graphs in small sampling steps with 2 to 5 times speed up and obtain competitive scores compared to the baselines on image and graph generation tasks.","sentences":["Score-based generative models have demonstrated significant practical success in data-generating tasks.","The models establish a diffusion process that perturbs the ground truth data to Gaussian noise and then learn the reverse process to transform noise into data.","However, existing denoising methods such as Langevin dynamic and numerical stochastic differential equation solvers enjoy randomness but generate data slowly with a large number of score function evaluations, and the ordinary differential equation solvers enjoy faster sampling speed but no randomness may influence the sample quality.","To this end, motivated by the Stochastic Gradient Descent (SGD) optimization methods and the high connection between the model sampling process with the SGD, we propose adaptive momentum sampling to accelerate the transforming process without introducing additional hyperparameters.","Theoretically, we proved our method promises convergence under given conditions.","In addition, we empirically show that our sampler can produce more faithful images/graphs in small sampling steps with 2 to 5 times speed up and obtain competitive scores compared to the baselines on image and graph generation tasks."],"url":"http://arxiv.org/abs/2405.13726v1","category":"cs.LG"}
{"created":"2024-05-22 15:08:44","title":"Bose-Einstein condensation of an optical thermodynamic system into a solitonic state","abstract":"Recent years have seen a resurgence of interest in multimode fibers due to their intriguing physics and applications, with spatial beam self-cleaning (BSC) having received special attention. In BSC light condenses into the fundamental fiber mode at elevated intensities. Despite extensive efforts utilizing optical thermodynamics to explain such counterintuitive beam reshaping process, several challenges still remain in fully understanding underlying physics. Here we provide compelling experimental evidence that BSC in a dissipative dual-core fiber can be understood in full analogy to Bose-Einstein condensation (BEC) in dilute gases. Being ruled by the identical Gross-Pitaevskii Equation, both systems feature a Townes soliton solution, for which we find further evidence by modal decomposition of our experimental data. Specifically, we observe that efficient BSC only sets in after an initial thermalization phase, causing converge towards a Townes beam profile once a threshold intensity has been surpassed. This process is akin to a transition from classical to quantum-mechanical thermodynamics in BEC. Furthermore, our analysis also identifies dissipative processes as a crucial, yet previously unidentified component for efficient BSC in multimode fiber. This discovery paves the way for unprecedented applications of multimode-fiber based systems in ultrafast lasers, communications, and fiber-based delivery of high-power laser beams.","sentences":["Recent years have seen a resurgence of interest in multimode fibers due to their intriguing physics and applications, with spatial beam self-cleaning (BSC) having received special attention.","In BSC light condenses into the fundamental fiber mode at elevated intensities.","Despite extensive efforts utilizing optical thermodynamics to explain such counterintuitive beam reshaping process, several challenges still remain in fully understanding underlying physics.","Here we provide compelling experimental evidence that BSC in a dissipative dual-core fiber can be understood in full analogy to Bose-Einstein condensation (BEC) in dilute gases.","Being ruled by the identical Gross-Pitaevskii Equation, both systems feature a Townes soliton solution, for which we find further evidence by modal decomposition of our experimental data.","Specifically, we observe that efficient BSC only sets in after an initial thermalization phase, causing converge towards a Townes beam profile once a threshold intensity has been surpassed.","This process is akin to a transition from classical to quantum-mechanical thermodynamics in BEC.","Furthermore, our analysis also identifies dissipative processes as a crucial, yet previously unidentified component for efficient BSC in multimode fiber.","This discovery paves the way for unprecedented applications of multimode-fiber based systems in ultrafast lasers, communications, and fiber-based delivery of high-power laser beams."],"url":"http://arxiv.org/abs/2405.13716v1","category":"physics.optics"}
{"created":"2024-05-22 14:43:29","title":"Uncertainty-aware Evaluation of Auxiliary Anomalies with the Expected Anomaly Posterior","abstract":"Anomaly detection is the task of identifying examples that do not behave as expected. Because anomalies are rare and unexpected events, collecting real anomalous examples is often challenging in several applications. In addition, learning an anomaly detector with limited (or no) anomalies often yields poor prediction performance. One option is to employ auxiliary synthetic anomalies to improve the model training. However, synthetic anomalies may be of poor quality: anomalies that are unrealistic or indistinguishable from normal samples may deteriorate the detector's performance. Unfortunately, no existing methods quantify the quality of auxiliary anomalies. We fill in this gap and propose the expected anomaly posterior (EAP), an uncertainty-based score function that measures the quality of auxiliary anomalies by quantifying the total uncertainty of an anomaly detector. Experimentally on 40 benchmark datasets of images and tabular data, we show that EAP outperforms 12 adapted data quality estimators in the majority of cases.","sentences":["Anomaly detection is the task of identifying examples that do not behave as expected.","Because anomalies are rare and unexpected events, collecting real anomalous examples is often challenging in several applications.","In addition, learning an anomaly detector with limited (or no) anomalies often yields poor prediction performance.","One option is to employ auxiliary synthetic anomalies to improve the model training.","However, synthetic anomalies may be of poor quality: anomalies that are unrealistic or indistinguishable from normal samples may deteriorate the detector's performance.","Unfortunately, no existing methods quantify the quality of auxiliary anomalies.","We fill in this gap and propose the expected anomaly posterior (EAP), an uncertainty-based score function that measures the quality of auxiliary anomalies by quantifying the total uncertainty of an anomaly detector.","Experimentally on 40 benchmark datasets of images and tabular data, we show that EAP outperforms 12 adapted data quality estimators in the majority of cases."],"url":"http://arxiv.org/abs/2405.13699v1","category":"cs.LG"}
{"created":"2024-05-22 13:12:57","title":"Building BESSER: an open-source low-code platform","abstract":"Low-code platforms (latest reincarnation of the long tradition of model-driven engineering approaches) have the potential of saving us countless hours of repetitive boilerplate coding tasks. However, as software systems grow in complexity, low-code platforms need to adapt as well. Notably, nowadays this implies adapting to the modeling and generation of smart software. At the same time, if we want to broaden the userbase of this type of tools, we should also be able to provide more open source alternatives that help potential users avoid vendor lock-ins and give them the freedom to explore low-code development approaches (even adapting the tool to better fit their needs). To fulfil these needs, we are building BESSER, an open source low-code platform for developing (smart) software. BESSER offers various forms (i.e., notations) for system and domain specification (e.g. UML for technical users and chatbots for business users) together with a number of generators. Both types of components can be extended and are open to contributions from the community.","sentences":["Low-code platforms (latest reincarnation of the long tradition of model-driven engineering approaches) have the potential of saving us countless hours of repetitive boilerplate coding tasks.","However, as software systems grow in complexity, low-code platforms need to adapt as well.","Notably, nowadays this implies adapting to the modeling and generation of smart software.","At the same time, if we want to broaden the userbase of this type of tools, we should also be able to provide more open source alternatives that help potential users avoid vendor lock-ins and give them the freedom to explore low-code development approaches (even adapting the tool to better fit their needs).","To fulfil these needs, we are building BESSER, an open source low-code platform for developing (smart) software.","BESSER offers various forms (i.e., notations) for system and domain specification (e.g. UML for technical users and chatbots for business users) together with a number of generators.","Both types of components can be extended and are open to contributions from the community."],"url":"http://arxiv.org/abs/2405.13620v1","category":"cs.SE"}
{"created":"2024-05-22 13:06:13","title":"Integrating supervised and reinforcement learning for predictive control with an unmodulated pyramid wavefront sensor for adaptive optics","abstract":"We propose a novel control approach that combines offline supervised learning to address the challenges posed by non-linear phase reconstruction using unmodulated pyramid wavefront sensors (P-WFS) and online reinforcement learning for predictive control. The control approach uses a high-order P-WFS to drive a tip-tilt stage and a high-dimensional mirror concurrently. Simulation results demonstrate that our method outperforms traditional control techniques, showing significant improvements in performance under challenging conditions such as faint stars and poor seeing, and exhibits robustness against variations in atmospheric conditions.","sentences":["We propose a novel control approach that combines offline supervised learning to address the challenges posed by non-linear phase reconstruction using unmodulated pyramid wavefront sensors (P-WFS) and online reinforcement learning for predictive control.","The control approach uses a high-order P-WFS to drive a tip-tilt stage and a high-dimensional mirror concurrently.","Simulation results demonstrate that our method outperforms traditional control techniques, showing significant improvements in performance under challenging conditions such as faint stars and poor seeing, and exhibits robustness against variations in atmospheric conditions."],"url":"http://arxiv.org/abs/2405.13610v1","category":"astro-ph.IM"}
{"created":"2024-05-22 12:36:32","title":"Running in circles: is practical application feasible for data fission and data thinning in post-clustering differential analysis?","abstract":"The standard pipeline to analyse single-cell RNA sequencing (scRNA-seq) often involves two steps : clustering and Differential Expression Analysis (DEA) to annotate cell populations based on gene expression. However, using clustering results for data-driven hypothesis formulation compromises statistical properties, especially Type I error control. Data fission was introduced to split the information contained in each observation into two independent parts that can be used for clustering and testing. However, data fission was originally designed for non-mixture distributions, and adapting it for mixtures requires knowledge of the unknown clustering structure to estimate component-specific scale parameters. As components are typically unavailable in practice, scale parameter estimators often exhibit bias. We explicitly quantify how this bias affects subsequent post-clustering differential analysis Type I error rate despite employing data fission. In response, we propose a novel approach that involves modeling each observation as a realization of its distribution, with scale parameters estimated non-parametrically. Simulations study showcase the efficacy of our method when component are clearly separated. However, the level of separability required to reach good performance presents complexities in its application to real scRNA-seq data.","sentences":["The standard pipeline to analyse single-cell RNA sequencing (scRNA-seq) often involves two steps : clustering and Differential Expression Analysis (DEA) to annotate cell populations based on gene expression.","However, using clustering results for data-driven hypothesis formulation compromises statistical properties, especially Type I error control.","Data fission was introduced to split the information contained in each observation into two independent parts that can be used for clustering and testing.","However, data fission was originally designed for non-mixture distributions, and adapting it for mixtures requires knowledge of the unknown clustering structure to estimate component-specific scale parameters.","As components are typically unavailable in practice, scale parameter estimators often exhibit bias.","We explicitly quantify how this bias affects subsequent post-clustering differential analysis Type I error rate despite employing data fission.","In response, we propose a novel approach that involves modeling each observation as a realization of its distribution, with scale parameters estimated non-parametrically.","Simulations study showcase the efficacy of our method when component are clearly separated.","However, the level of separability required to reach good performance presents complexities in its application to real scRNA-seq data."],"url":"http://arxiv.org/abs/2405.13591v1","category":"stat.ME"}
{"created":"2024-05-22 12:15:52","title":"ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation","abstract":"Ensuring large language models (LLM) behave consistently with human goals, values, and intentions is crucial for their safety but yet computationally expensive. To reduce the computational cost of alignment training of LLMs, especially for those with a huge number of parameters, and to reutilize learned value alignment, we propose ConTrans, a novel framework that enables weak-to-strong alignment transfer via concept transplantation. From the perspective of representation engineering, ConTrans refines concept vectors in value alignment from a source LLM (usually a weak yet aligned LLM). The refined concept vectors are then reformulated to adapt to the target LLM (usually a strong yet unaligned base LLM) via affine transformation. In the third step, ConTrans transplants the reformulated concept vectors into the residual stream of the target LLM. Experiments demonstrate the successful transplantation of a wide range of aligned concepts from 7B models to 13B and 70B models across multiple LLMs and LLM families. Remarkably, ConTrans even surpasses instruction-tuned models in terms of truthfulness. Experiment results validate the effectiveness of both inter-LLM-family and intra-LLM-family concept transplantation. Our work successfully demonstrates an alternative way to achieve weak-to-strong alignment generalization and control.","sentences":["Ensuring large language models (LLM) behave consistently with human goals, values, and intentions is crucial for their safety but yet computationally expensive.","To reduce the computational cost of alignment training of LLMs, especially for those with a huge number of parameters, and to reutilize learned value alignment, we propose ConTrans, a novel framework that enables weak-to-strong alignment transfer via concept transplantation.","From the perspective of representation engineering, ConTrans refines concept vectors in value alignment from a source LLM (usually a weak yet aligned LLM).","The refined concept vectors are then reformulated to adapt to the target LLM (usually a strong yet unaligned base LLM) via affine transformation.","In the third step, ConTrans transplants the reformulated concept vectors into the residual stream of the target LLM.","Experiments demonstrate the successful transplantation of a wide range of aligned concepts from 7B models to 13B and 70B models across multiple LLMs and LLM families.","Remarkably, ConTrans even surpasses instruction-tuned models in terms of truthfulness.","Experiment results validate the effectiveness of both inter-LLM-family and intra-LLM-family concept transplantation.","Our work successfully demonstrates an alternative way to achieve weak-to-strong alignment generalization and control."],"url":"http://arxiv.org/abs/2405.13578v1","category":"cs.CL"}
{"created":"2024-05-22 12:11:12","title":"Reinforcement Learning for Adaptive MCMC","abstract":"An informal observation, made by several authors, is that the adaptive design of a Markov transition kernel has the flavour of a reinforcement learning task. Yet, to-date it has remained unclear how to actually exploit modern reinforcement learning technologies for adaptive MCMC. The aim of this paper is to set out a general framework, called Reinforcement Learning Metropolis--Hastings, that is theoretically supported and empirically validated. Our principal focus is on learning fast-mixing Metropolis--Hastings transition kernels, which we cast as deterministic policies and optimise via a policy gradient. Control of the learning rate provably ensures conditions for ergodicity are satisfied. The methodology is used to construct a gradient-free sampler that out-performs a popular gradient-free adaptive Metropolis--Hastings algorithm on $\\approx 90 \\%$ of tasks in the PosteriorDB benchmark.","sentences":["An informal observation, made by several authors, is that the adaptive design of a Markov transition kernel has the flavour of a reinforcement learning task.","Yet, to-date it has remained unclear how to actually exploit modern reinforcement learning technologies for adaptive MCMC.","The aim of this paper is to set out a general framework, called Reinforcement Learning Metropolis--Hastings, that is theoretically supported and empirically validated.","Our principal focus is on learning fast-mixing Metropolis--Hastings transition kernels, which we cast as deterministic policies and optimise via a policy gradient.","Control of the learning rate provably ensures conditions for ergodicity are satisfied.","The methodology is used to construct a gradient-free sampler that out-performs a popular gradient-free adaptive Metropolis--Hastings algorithm on $\\approx 90 \\%$ of tasks in the PosteriorDB benchmark."],"url":"http://arxiv.org/abs/2405.13574v1","category":"stat.CO"}
{"created":"2024-05-22 12:07:34","title":"Synchronization through frequency shuffling","abstract":"A wide variety of engineered and natural systems are modelled as networks of coupled nonlinear oscillators. In nature, the intrinsic frequencies of these oscillators are not constant in time. Here, we probe the effect of such a temporal heterogeneity on coupled oscillator networks, through the lens of the Kuramoto model. To do this, we shuffle repeatedly the intrinsic frequencies among the oscillators at either random or regular time intervals. What emerges is the remarkable effect that frequent shuffling induces earlier onset (i.e., at a lower coupling) of synchrony among the oscillator phases. Our study provides a novel strategy to induce and control synchrony under resource constraints. We demonstrate our results analytically and in experiments with a network of Wien Bridge oscillators with internal frequencies being shuffled in time.","sentences":["A wide variety of engineered and natural systems are modelled as networks of coupled nonlinear oscillators.","In nature, the intrinsic frequencies of these oscillators are not constant in time.","Here, we probe the effect of such a temporal heterogeneity on coupled oscillator networks, through the lens of the Kuramoto model.","To do this, we shuffle repeatedly the intrinsic frequencies among the oscillators at either random or regular time intervals.","What emerges is the remarkable effect that frequent shuffling induces earlier onset (i.e., at a lower coupling) of synchrony among the oscillator phases.","Our study provides a novel strategy to induce and control synchrony under resource constraints.","We demonstrate our results analytically and in experiments with a network of Wien Bridge oscillators with internal frequencies being shuffled in time."],"url":"http://arxiv.org/abs/2405.13569v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-22 11:57:04","title":"Hybrid Event-triggered Control of Nonlinear System with Full State Constraints and Disturbance","abstract":"This article focuses on the problem of adaptive tracking control for a specific type of nonlinear system that is subject to full-state constraints via a hybrid event-triggered control (HETC) strategy. With the auxiliary system, we proposed a 'log' function to deal with the full-state constraint. Additionally, a disturbance observer (DO) is constructed to handle the unmeasurable external disturbance. Then, by employing radial basis function neural networks (RBFNNs) and a first-order differentiator, an opportune backstepping design procedure is given to avoid the problem of \"explosion of complexity\". The HETC strategy, including the fixed and relative threshold, is presented to provide more flexibility in balancing the system performances and network burdens. Finally, to demonstrate the effectiveness of the aforementioned control scheme, a simulation example is presented to validate its effectiveness.","sentences":["This article focuses on the problem of adaptive tracking control for a specific type of nonlinear system that is subject to full-state constraints via a hybrid event-triggered control (HETC) strategy.","With the auxiliary system, we proposed a 'log' function to deal with the full-state constraint.","Additionally, a disturbance observer (DO) is constructed to handle the unmeasurable external disturbance.","Then, by employing radial basis function neural networks (RBFNNs) and a first-order differentiator, an opportune backstepping design procedure is given to avoid the problem of \"explosion of complexity\".","The HETC strategy, including the fixed and relative threshold, is presented to provide more flexibility in balancing the system performances and network burdens.","Finally, to demonstrate the effectiveness of the aforementioned control scheme, a simulation example is presented to validate its effectiveness."],"url":"http://arxiv.org/abs/2405.13564v1","category":"eess.SY"}
{"created":"2024-05-22 11:49:40","title":"Navigating User Experience of ChatGPT-based Conversational Recommender Systems: The Effects of Prompt Guidance and Recommendation Domain","abstract":"Conversational recommender systems (CRS) enable users to articulate their preferences and provide feedback through natural language. With the advent of large language models (LLMs), the potential to enhance user engagement with CRS and augment the recommendation process with LLM-generated content has received increasing attention. However, the efficacy of LLM-powered CRS is contingent upon the use of prompts, and the subjective perception of recommendation quality can differ across various recommendation domains. Therefore, we have developed a ChatGPT-based CRS to investigate the impact of these two factors, prompt guidance (PG) and recommendation domain (RD), on the overall user experience of the system. We conducted an online empirical study (N = 100) by employing a mixed-method approach that utilized a between-subjects design for the variable of PG (with vs. without) and a within-subjects design for RD (book recommendations vs. job recommendations). The findings reveal that PG can substantially enhance the system's explainability, adaptability, perceived ease of use, and transparency. Moreover, users are inclined to perceive a greater sense of novelty and demonstrate a higher propensity to engage with and try recommended items in the context of book recommendations as opposed to job recommendations. Furthermore, the influence of PG on certain user experience metrics and interactive behaviors appears to be modulated by the recommendation domain, as evidenced by the interaction effects between the two examined factors. This work contributes to the user-centered evaluation of ChatGPT-based CRS by investigating two prominent factors and offers practical design guidance.","sentences":["Conversational recommender systems (CRS) enable users to articulate their preferences and provide feedback through natural language.","With the advent of large language models (LLMs), the potential to enhance user engagement with CRS and augment the recommendation process with LLM-generated content has received increasing attention.","However, the efficacy of LLM-powered CRS is contingent upon the use of prompts, and the subjective perception of recommendation quality can differ across various recommendation domains.","Therefore, we have developed a ChatGPT-based CRS to investigate the impact of these two factors, prompt guidance (PG) and recommendation domain (RD), on the overall user experience of the system.","We conducted an online empirical study (N = 100) by employing a mixed-method approach that utilized a between-subjects design for the variable of PG (with vs. without) and a within-subjects design for RD (book recommendations vs. job recommendations).","The findings reveal that PG can substantially enhance the system's explainability, adaptability, perceived ease of use, and transparency.","Moreover, users are inclined to perceive a greater sense of novelty and demonstrate a higher propensity to engage with and try recommended items in the context of book recommendations as opposed to job recommendations.","Furthermore, the influence of PG on certain user experience metrics and interactive behaviors appears to be modulated by the recommendation domain, as evidenced by the interaction effects between the two examined factors.","This work contributes to the user-centered evaluation of ChatGPT-based CRS by investigating two prominent factors and offers practical design guidance."],"url":"http://arxiv.org/abs/2405.13560v1","category":"cs.HC"}
{"created":"2024-05-22 11:19:26","title":"Ultra-Fast Adaptive Track Detection Network","abstract":"Railway detection is critical for the automation of railway systems. Existing models often prioritize either speed or accuracy, but achieving both remains a challenge. To address the limitations of presetting anchor groups that struggle with varying track proportions from different camera angles, an ultra-fast adaptive track detection network is proposed in this paper. This network comprises a backbone network and two specialized branches (Horizontal Coordinate Locator and Perspective Identifier). The Perspective Identifier selects the suitable anchor group from preset anchor groups, thereby determining the row coordinates of the railway track. Subsequently, the Horizontal Coordinate Locator provides row classification results based on multiple preset anchor groups. Then, utilizing the results from the Perspective Identifier, it generates the column coordinates of the railway track. This network is evaluated on multiple datasets, with the lightweight version achieving an F1 score of 98.68% on the SRail dataset and a detection rate of up to 473 FPS. Compared to the SOTA, the proposed model is competitive in both speed and accuracy. The dataset and code are available at https://github.com/idnihai/UFATD","sentences":["Railway detection is critical for the automation of railway systems.","Existing models often prioritize either speed or accuracy, but achieving both remains a challenge.","To address the limitations of presetting anchor groups that struggle with varying track proportions from different camera angles, an ultra-fast adaptive track detection network is proposed in this paper.","This network comprises a backbone network and two specialized branches (Horizontal Coordinate Locator and Perspective Identifier).","The Perspective Identifier selects the suitable anchor group from preset anchor groups, thereby determining the row coordinates of the railway track.","Subsequently, the Horizontal Coordinate Locator provides row classification results based on multiple preset anchor groups.","Then, utilizing the results from the Perspective Identifier, it generates the column coordinates of the railway track.","This network is evaluated on multiple datasets, with the lightweight version achieving an F1 score of 98.68% on the SRail dataset and a detection rate of up to 473 FPS.","Compared to the SOTA, the proposed model is competitive in both speed and accuracy.","The dataset and code are available at https://github.com/idnihai/UFATD"],"url":"http://arxiv.org/abs/2405.13538v1","category":"cs.CV"}
{"created":"2024-05-22 10:26:44","title":"PerSense: Personalized Instance Segmentation in Dense Images","abstract":"Leveraging large-scale pre-training, vision foundational models showcase notable performance benefits. While recent years have witnessed significant advancements in segmentation algorithms, existing models still face challenges to automatically segment personalized instances in dense and crowded scenarios. The primary factor behind this limitation stems from bounding box-based detections, which are constrained by occlusions, background clutter, and object orientation, particularly when dealing with dense images. To this end, we propose PerSense, an end-to-end, training-free, and model-agnostic one-shot framework to address the personalized instance segmentation in dense images. Towards developing this framework, we make following core contributions. (a) We propose an Instance Detection Module (IDM) and leverage a Vision-Language Model, a grounding object detector, and a few-shot object counter (FSOC) to realize a new baseline. (b) To tackle false positives within candidate point prompts, we design Point Prompt Selection Module (PPSM). Both IDM and PPSM transform density maps from FSOC into personalized instance-level point prompts for segmentation and offer a seamless integration in our model-agnostic framework. (c) We introduce a feedback mechanism which enables PerSense to harness the full potential of FSOC by automating the exemplar selection process. (d) To promote algorithmic advances and effective tools for this relatively underexplored task, we introduce PerSense-D, a dataset exclusive to personalized instance segmentation in dense images. We validate the effectiveness of PerSense on the task of personalized instance segmentation in dense images on PerSense-D and comparison with SOTA. Additionally, our qualitative findings demonstrate the adaptability of our framework to images captured in-the-wild.","sentences":["Leveraging large-scale pre-training, vision foundational models showcase notable performance benefits.","While recent years have witnessed significant advancements in segmentation algorithms, existing models still face challenges to automatically segment personalized instances in dense and crowded scenarios.","The primary factor behind this limitation stems from bounding box-based detections, which are constrained by occlusions, background clutter, and object orientation, particularly when dealing with dense images.","To this end, we propose PerSense, an end-to-end, training-free, and model-agnostic one-shot framework to address the personalized instance segmentation in dense images.","Towards developing this framework, we make following core contributions.","(a) We propose an Instance Detection Module (IDM) and leverage a Vision-Language Model, a grounding object detector, and a few-shot object counter (FSOC) to realize a new baseline.","(b) To tackle false positives within candidate point prompts, we design Point Prompt Selection Module (PPSM).","Both IDM and PPSM transform density maps from FSOC into personalized instance-level point prompts for segmentation and offer a seamless integration in our model-agnostic framework.","(c) We introduce a feedback mechanism which enables PerSense to harness the full potential of FSOC by automating the exemplar selection process.","(d) To promote algorithmic advances and effective tools for this relatively underexplored task, we introduce PerSense-D, a dataset exclusive to personalized instance segmentation in dense images.","We validate the effectiveness of PerSense on the task of personalized instance segmentation in dense images on PerSense-D and comparison with SOTA.","Additionally, our qualitative findings demonstrate the adaptability of our framework to images captured in-the-wild."],"url":"http://arxiv.org/abs/2405.13518v1","category":"cs.CV"}
{"created":"2024-05-22 10:05:52","title":"Euclid. IV. The NISP Calibration Unit","abstract":"The near-infrared calibration unit (NI-CU) onboard Euclid's Near-Infrared Spectrometer and Photometer (NISP) is the first astronomical calibration lamp based on light-emitting diodes (LEDs) to be operated in space. Euclid is a mission in ESA's 'Cosmic Vision 2015-2025' framework, to explore the dark universe and provide a next-level characterisation of the nature of gravitation, dark matter, and dark energy. Calibrating photometric and spectrometric measurements of galaxies to better than 1.5% accuracy in a survey homogeneously mapping ~14000 deg^2 of extragalactic sky requires a very detailed characterisation of near-infrared (NIR) detector properties, as well their constant monitoring in flight. To cover two of the main contributions - relative pixel-to-pixel sensitivity and non-linearity characteristics - as well as support other calibration activities, NI-CU was designed to provide spatially approximately homogeneous (<12% variations) and temporally stable illumination (0.1%-0.2% over 1200s) over the NISP detector plane, with minimal power consumption and energy dissipation. NI-CU is covers the spectral range ~[900,1900] nm - at cryo-operating temperature - at 5 fixed independent wavelengths to capture wavelength-dependent behaviour of the detectors, with fluence over a dynamic range of >=100 from ~15 ph s^-1 pixel^-1 to >1500 ph s^-1 pixel^-1. For this functionality, NI-CU is based on LEDs. We describe the rationale behind the decision and design process, describe the challenges in sourcing the right LEDs, as well as the qualification process and lessons learned. We also provide a description of the completed NI-CU, its capabilities and performance as well as its limits. NI-CU has been integrated into NISP and the Euclid satellite, and since Euclid's launch in July 2023 has started supporting survey operations.","sentences":["The near-infrared calibration unit (NI-CU) onboard Euclid's Near-Infrared Spectrometer and Photometer (NISP) is the first astronomical calibration lamp based on light-emitting diodes (LEDs) to be operated in space.","Euclid is a mission in ESA's 'Cosmic Vision 2015-2025' framework, to explore the dark universe and provide a next-level characterisation of the nature of gravitation, dark matter, and dark energy.","Calibrating photometric and spectrometric measurements of galaxies to better than 1.5% accuracy in a survey homogeneously mapping ~14000 deg^2 of extragalactic sky requires a very detailed characterisation of near-infrared (NIR) detector properties, as well their constant monitoring in flight.","To cover two of the main contributions - relative pixel-to-pixel sensitivity and non-linearity characteristics - as well as support other calibration activities, NI-CU was designed to provide spatially approximately homogeneous (<12% variations) and temporally stable illumination (0.1%-0.2% over 1200s) over the NISP detector plane, with minimal power consumption and energy dissipation.","NI-CU is covers the spectral range ~[900,1900] nm - at cryo-operating temperature - at 5 fixed independent wavelengths to capture wavelength-dependent behaviour of the detectors, with fluence over a dynamic range of >=100 from ~15 ph","s^-1 pixel^-1 to >1500","ph","s^-1 pixel^-1.","For this functionality, NI-CU is based on LEDs.","We describe the rationale behind the decision and design process, describe the challenges in sourcing the right LEDs, as well as the qualification process and lessons learned.","We also provide a description of the completed NI-CU, its capabilities and performance as well as its limits.","NI-CU has been integrated into NISP and the Euclid satellite, and since Euclid's launch in July 2023 has started supporting survey operations."],"url":"http://arxiv.org/abs/2405.13494v1","category":"astro-ph.IM"}
{"created":"2024-05-22 09:56:05","title":"Generative AI: The power of the new education","abstract":"The effective integration of generative artificial intelligence in education is a fundamental aspect to prepare future generations. This study proposes an accelerated learning methodology in artificial intelligence, focused on its generative capacity, as a way to achieve this goal. It recognizes the challenge of getting teachers to engage with new technologies and adapt their methods in all subjects, not just those related to AI. This methodology not only promotes interest in science, technology, engineering and mathematics, but also facilitates student understanding of the ethical uses and risks associated with AI. Students' perceptions of generative AI are examined, addressing their emotions towards its evolution, evaluation of its ethical implications, and everyday use of AI tools. In addition, AI applications commonly used by students and their integration into other disciplines are investigated. The study aims to provide educators with a deeper understanding of students' perceptions of AI and its relevance in society and in their future career paths.","sentences":["The effective integration of generative artificial intelligence in education is a fundamental aspect to prepare future generations.","This study proposes an accelerated learning methodology in artificial intelligence, focused on its generative capacity, as a way to achieve this goal.","It recognizes the challenge of getting teachers to engage with new technologies and adapt their methods in all subjects, not just those related to AI.","This methodology not only promotes interest in science, technology, engineering and mathematics, but also facilitates student understanding of the ethical uses and risks associated with AI.","Students' perceptions of generative AI are examined, addressing their emotions towards its evolution, evaluation of its ethical implications, and everyday use of AI tools.","In addition, AI applications commonly used by students and their integration into other disciplines are investigated.","The study aims to provide educators with a deeper understanding of students' perceptions of AI and its relevance in society and in their future career paths."],"url":"http://arxiv.org/abs/2405.13487v1","category":"cs.CY"}
{"created":"2024-05-22 09:47:44","title":"What is a typical signalized intersection in a city? A pipeline for intersection data imputation from OpenStreetMap","abstract":"Signalized intersections, arguably the most complicated type of traffic scenario, are essential to urban mobility systems. With recent advancements in intelligent transportation technologies, signalized intersections have great prospects for making transportation greener, safer, and faster. Several studies have been conducted focusing on intersection-level control and optimization. However, arbitrarily structured signalized intersections that are often used do not represent the ground-truth distribution, and there is no standardized way that exists to extract information about real-world signalized intersections. As the largest open-source map in the world, OpenStreetMap (OSM) has been used by many transportation researchers for a variety of studies, including intersection-level research such as adaptive traffic signal control and eco-driving. However, the quality of OSM data has been a serious concern.   In this paper, we propose a pipeline for effectively extracting information about signalized intersections from OSM and constructing a comprehensive dataset. We thoroughly discuss challenges related to this task and we propose our solution for each challenge. We also use Salt Lake City as an example to demonstrate the performance of our methods. The pipeline has been published as an open-source Python library so everyone can freely download and use it to facilitate their research. Hopefully, this paper can serve as a starting point that inspires more efforts to build a standardized and systematic data pipeline for various types of transportation problems.","sentences":["Signalized intersections, arguably the most complicated type of traffic scenario, are essential to urban mobility systems.","With recent advancements in intelligent transportation technologies, signalized intersections have great prospects for making transportation greener, safer, and faster.","Several studies have been conducted focusing on intersection-level control and optimization.","However, arbitrarily structured signalized intersections that are often used do not represent the ground-truth distribution, and there is no standardized way that exists to extract information about real-world signalized intersections.","As the largest open-source map in the world, OpenStreetMap (OSM) has been used by many transportation researchers for a variety of studies, including intersection-level research such as adaptive traffic signal control and eco-driving.","However, the quality of OSM data has been a serious concern.   ","In this paper, we propose a pipeline for effectively extracting information about signalized intersections from OSM and constructing a comprehensive dataset.","We thoroughly discuss challenges related to this task and we propose our solution for each challenge.","We also use Salt Lake City as an example to demonstrate the performance of our methods.","The pipeline has been published as an open-source Python library so everyone can freely download and use it to facilitate their research.","Hopefully, this paper can serve as a starting point that inspires more efforts to build a standardized and systematic data pipeline for various types of transportation problems."],"url":"http://arxiv.org/abs/2405.13480v1","category":"physics.soc-ph"}
{"created":"2024-05-22 09:28:25","title":"On the geometry of singular EPW cubes","abstract":"EPW cubes form a locally complete family of smooth projective hyper-K\\\"ahler varieties of dimension 6, constructed by Iliev--Kapustka--Kapustka--Ranestad.\\ Their construction and behavior share a lot of similarities with the double EPW sextics constructed by O'Grady.\\ Adapting the methods of O'Grady, we construct a projective smooth small resolution of singular EPW cubes.","sentences":["EPW cubes form a locally complete family of smooth projective hyper-K\\\"ahler varieties of dimension 6, constructed by Iliev--Kapustka--Kapustka--Ranestad.\\","Their construction and behavior share a lot of similarities with the double EPW sextics constructed by O'Grady.\\","Adapting the methods of O'Grady, we construct a projective smooth small resolution of singular EPW cubes."],"url":"http://arxiv.org/abs/2405.13472v1","category":"math.AG"}
{"created":"2024-05-22 09:25:58","title":"Machine Learning for Exoplanet Detection in High-Contrast Spectroscopy: Revealing Exoplanets by Leveraging Hidden Molecular Signatures in Cross-Correlated Spectra with Convolutional Neural Networks","abstract":"The new generation of observatories and instruments (VLT/ERIS, JWST, ELT) motivate the development of robust methods to detect and characterise faint and close-in exoplanets. Molecular mapping and cross-correlation for spectroscopy use molecular templates to isolate a planet's spectrum from its host star. However, reliance on signal-to-noise ratio (S/N) metrics can lead to missed discoveries, due to strong assumptions of Gaussian independent and identically distributed noise. We introduce machine learning for cross-correlation spectroscopy (MLCCS); the method aims to leverage weak assumptions on exoplanet characterisation, such as the presence of specific molecules in atmospheres, to improve detection sensitivity for exoplanets. MLCCS methods, including a perceptron and unidimensional convolutional neural networks, operate in the cross-correlated spectral dimension, in which patterns from molecules can be identified. We test on mock datasets of synthetic planets inserted into real noise from SINFONI at K-band. The results from MLCCS show outstanding improvements. The outcome on a grid of faint synthetic gas giants shows that for a false discovery rate up to 5%, a perceptron can detect about 26 times the amount of planets compared to an S/N metric. This factor increases up to 77 times with convolutional neural networks, with a statistical sensitivity shift from 0.7% to 55.5%. In addition, MLCCS methods show a drastic improvement in detection confidence and conspicuity on imaging spectroscopy. Once trained, MLCCS methods offer sensitive and rapid detection of exoplanets and their molecular species in the spectral dimension. They handle systematic noise and challenging seeing conditions, can adapt to many spectroscopic instruments and modes, and are versatile regarding atmospheric characteristics, which can enable identification of various planets in archival and future data.","sentences":["The new generation of observatories and instruments (VLT/ERIS, JWST, ELT) motivate the development of robust methods to detect and characterise faint and close-in exoplanets.","Molecular mapping and cross-correlation for spectroscopy use molecular templates to isolate a planet's spectrum from its host star.","However, reliance on signal-to-noise ratio (S/N) metrics can lead to missed discoveries, due to strong assumptions of Gaussian independent and identically distributed noise.","We introduce machine learning for cross-correlation spectroscopy (MLCCS); the method aims to leverage weak assumptions on exoplanet characterisation, such as the presence of specific molecules in atmospheres, to improve detection sensitivity for exoplanets.","MLCCS methods, including a perceptron and unidimensional convolutional neural networks, operate in the cross-correlated spectral dimension, in which patterns from molecules can be identified.","We test on mock datasets of synthetic planets inserted into real noise from SINFONI at K-band.","The results from MLCCS show outstanding improvements.","The outcome on a grid of faint synthetic gas giants shows that for a false discovery rate up to 5%, a perceptron can detect about 26 times the amount of planets compared to an S/N metric.","This factor increases up to 77 times with convolutional neural networks, with a statistical sensitivity shift from 0.7% to 55.5%.","In addition, MLCCS methods show a drastic improvement in detection confidence and conspicuity on imaging spectroscopy.","Once trained, MLCCS methods offer sensitive and rapid detection of exoplanets and their molecular species in the spectral dimension.","They handle systematic noise and challenging seeing conditions, can adapt to many spectroscopic instruments and modes, and are versatile regarding atmospheric characteristics, which can enable identification of various planets in archival and future data."],"url":"http://arxiv.org/abs/2405.13469v1","category":"astro-ph.EP"}
{"created":"2024-05-22 09:19:25","title":"AdaFedFR: Federated Face Recognition with Adaptive Inter-Class Representation Learning","abstract":"With the growing attention on data privacy and communication security in face recognition applications, federated learning has been introduced to learn a face recognition model with decentralized datasets in a privacy-preserving manner. However, existing works still face challenges such as unsatisfying performance and additional communication costs, limiting their applicability in real-world scenarios. In this paper, we propose a simple yet effective federated face recognition framework called AdaFedFR, by devising an adaptive inter-class representation learning algorithm to enhance the generalization of the generic face model and the efficiency of federated training under strict privacy-preservation. In particular, our work delicately utilizes feature representations of public identities as learnable negative knowledge to optimize the local objective within the feature space, which further encourages the local model to learn powerful representations and optimize personalized models for clients. Experimental results demonstrate that our method outperforms previous approaches on several prevalent face recognition benchmarks within less than 3 communication rounds, which shows communication-friendly and great efficiency.","sentences":["With the growing attention on data privacy and communication security in face recognition applications, federated learning has been introduced to learn a face recognition model with decentralized datasets in a privacy-preserving manner.","However, existing works still face challenges such as unsatisfying performance and additional communication costs, limiting their applicability in real-world scenarios.","In this paper, we propose a simple yet effective federated face recognition framework called AdaFedFR, by devising an adaptive inter-class representation learning algorithm to enhance the generalization of the generic face model and the efficiency of federated training under strict privacy-preservation.","In particular, our work delicately utilizes feature representations of public identities as learnable negative knowledge to optimize the local objective within the feature space, which further encourages the local model to learn powerful representations and optimize personalized models for clients.","Experimental results demonstrate that our method outperforms previous approaches on several prevalent face recognition benchmarks within less than 3 communication rounds, which shows communication-friendly and great efficiency."],"url":"http://arxiv.org/abs/2405.13467v1","category":"cs.CV"}
{"created":"2024-05-22 09:01:56","title":"Adapting Multi-modal Large Language Model to Concept Drift in the Long-tailed Open World","abstract":"Real-world data often exhibit extreme imbalances and out-of-distribution (OOD) instances, which significantly biases the model training. While it has been extensively studied in vision and language domains separately, the impact of long-tailed open worlds on multi-modal large language models (MLLMs) has been largely overlooked. In this paper, we first demonstrate the susceptibility and vulnerability of vision-language models to significant biases caused by tail drift and out-of-distribution (OOD) drift during both the pre-training and fine-tuning stages. To eliminate the bias from different sources, we integrate the tailed drift adaptation and OOD drift detection into a unified framework by extending the concept drift theory to multi-modal. Specifically, a T-distribution-based drift adapter is proposed to effectively mitigate the bias induced by the long-tailed problem, which also facilitates the model in distinguishing OOD data through explicit distribution modelling. Extensive experiments show significant improvements in our model's ability to adapt to tailed drift and OOD drift. Moreover, it enhances the efficiency and accuracy of image-text alignment in vision language model pre-training, particularly in the long-tail open world scenario. Furthermore, we create a set of multi-modal datasets called OpenMMlo, specifically tailored for the long-tailed open world scenario, to validate our findings. To foster the development of the multi-modal community, we have made both OpenMMlo datasets and our code publicly available at: https://github.com/Anonymous0Knight/ConceptDriftMLLMs.","sentences":["Real-world data often exhibit extreme imbalances and out-of-distribution (OOD) instances, which significantly biases the model training.","While it has been extensively studied in vision and language domains separately, the impact of long-tailed open worlds on multi-modal large language models (MLLMs) has been largely overlooked.","In this paper, we first demonstrate the susceptibility and vulnerability of vision-language models to significant biases caused by tail drift and out-of-distribution (OOD) drift during both the pre-training and fine-tuning stages.","To eliminate the bias from different sources, we integrate the tailed drift adaptation and OOD drift detection into a unified framework by extending the concept drift theory to multi-modal.","Specifically, a T-distribution-based drift adapter is proposed to effectively mitigate the bias induced by the long-tailed problem, which also facilitates the model in distinguishing OOD data through explicit distribution modelling.","Extensive experiments show significant improvements in our model's ability to adapt to tailed drift and OOD drift.","Moreover, it enhances the efficiency and accuracy of image-text alignment in vision language model pre-training, particularly in the long-tail open world scenario.","Furthermore, we create a set of multi-modal datasets called OpenMMlo, specifically tailored for the long-tailed open world scenario, to validate our findings.","To foster the development of the multi-modal community, we have made both OpenMMlo datasets and our code publicly available at: https://github.com/Anonymous0Knight/ConceptDriftMLLMs."],"url":"http://arxiv.org/abs/2405.13459v1","category":"cs.CV"}
{"created":"2024-05-22 08:46:45","title":"A Huber Loss Minimization Approach to Mean Estimation under User-level Differential Privacy","abstract":"Privacy protection of users' entire contribution of samples is important in distributed systems. The most effective approach is the two-stage scheme, which finds a small interval first and then gets a refined estimate by clipping samples into the interval. However, the clipping operation induces bias, which is serious if the sample distribution is heavy-tailed. Besides, users with large local sample sizes can make the sensitivity much larger, thus the method is not suitable for imbalanced users. Motivated by these challenges, we propose a Huber loss minimization approach to mean estimation under user-level differential privacy. The connecting points of Huber loss can be adaptively adjusted to deal with imbalanced users. Moreover, it avoids the clipping operation, thus significantly reducing the bias compared with the two-stage approach. We provide a theoretical analysis of our approach, which gives the noise strength needed for privacy protection, as well as the bound of mean squared error. The result shows that the new method is much less sensitive to the imbalance of user-wise sample sizes and the tail of sample distributions. Finally, we perform numerical experiments to validate our theoretical analysis.","sentences":["Privacy protection of users' entire contribution of samples is important in distributed systems.","The most effective approach is the two-stage scheme, which finds a small interval first and then gets a refined estimate by clipping samples into the interval.","However, the clipping operation induces bias, which is serious if the sample distribution is heavy-tailed.","Besides, users with large local sample sizes can make the sensitivity much larger, thus the method is not suitable for imbalanced users.","Motivated by these challenges, we propose a Huber loss minimization approach to mean estimation under user-level differential privacy.","The connecting points of Huber loss can be adaptively adjusted to deal with imbalanced users.","Moreover, it avoids the clipping operation, thus significantly reducing the bias compared with the two-stage approach.","We provide a theoretical analysis of our approach, which gives the noise strength needed for privacy protection, as well as the bound of mean squared error.","The result shows that the new method is much less sensitive to the imbalance of user-wise sample sizes and the tail of sample distributions.","Finally, we perform numerical experiments to validate our theoretical analysis."],"url":"http://arxiv.org/abs/2405.13453v1","category":"cs.LG"}
{"created":"2024-05-22 08:15:50","title":"Adaptive Fuzzy C-Means with Graph Embedding","abstract":"Fuzzy clustering algorithms can be roughly categorized into two main groups: Fuzzy C-Means (FCM) based methods and mixture model based methods. However, for almost all existing FCM based methods, how to automatically selecting proper membership degree hyper-parameter values remains a challenging and unsolved problem. Mixture model based methods, while circumventing the difficulty of manually adjusting membership degree hyper-parameters inherent in FCM based methods, often have a preference for specific distributions, such as the Gaussian distribution. In this paper, we propose a novel FCM based clustering model that is capable of automatically learning an appropriate membership degree hyper-parameter value and handling data with non-Gaussian clusters. Moreover, by removing the graph embedding regularization, the proposed FCM model can degenerate into the simplified generalized Gaussian mixture model. Therefore, the proposed FCM model can be also seen as the generalized Gaussian mixture model with graph embedding. Extensive experiments are conducted on both synthetic and real-world datasets to demonstrate the effectiveness of the proposed model.","sentences":["Fuzzy clustering algorithms can be roughly categorized into two main groups: Fuzzy C-Means (FCM) based methods and mixture model based methods.","However, for almost all existing FCM based methods, how to automatically selecting proper membership degree hyper-parameter values remains a challenging and unsolved problem.","Mixture model based methods, while circumventing the difficulty of manually adjusting membership degree hyper-parameters inherent in FCM based methods, often have a preference for specific distributions, such as the Gaussian distribution.","In this paper, we propose a novel FCM based clustering model that is capable of automatically learning an appropriate membership degree hyper-parameter value and handling data with non-Gaussian clusters.","Moreover, by removing the graph embedding regularization, the proposed FCM model can degenerate into the simplified generalized Gaussian mixture model.","Therefore, the proposed FCM model can be also seen as the generalized Gaussian mixture model with graph embedding.","Extensive experiments are conducted on both synthetic and real-world datasets to demonstrate the effectiveness of the proposed model."],"url":"http://arxiv.org/abs/2405.13427v1","category":"cs.LG"}
{"created":"2024-05-22 07:33:24","title":"Dynamic Context Adaptation and Information Flow Control in Transformers: Introducing the Evaluator Adjuster Unit and Gated Residual Connections","abstract":"Transformers have revolutionized various domains of artificial intelligence due to their unique ability to model long-range dependencies in data. However, they lack in nuanced, context-dependent modulation of features and information flow. This paper introduces two significant enhancements to the transformer architecture - the Evaluator Adjuster Unit (EAU) and Gated Residual Connections (GRC) - designed to address these limitations. The EAU dynamically modulates attention outputs based on the relevance of the input context, allowing for more adaptive response patterns. Concurrently, the GRC modifies the transformer's residual connections through a gating mechanism that selectively controls the information flow, thereby enhancing the network's ability to focus on contextually important features. We evaluate the performance of these enhancements across several benchmarks in natural language processing. Our results demonstrate improved adaptability and efficiency, suggesting that these modifications could set new standards for designing flexible and context-aware transformer models.","sentences":["Transformers have revolutionized various domains of artificial intelligence due to their unique ability to model long-range dependencies in data.","However, they lack in nuanced, context-dependent modulation of features and information flow.","This paper introduces two significant enhancements to the transformer architecture - the Evaluator Adjuster Unit (EAU) and Gated Residual Connections (GRC) - designed to address these limitations.","The EAU dynamically modulates attention outputs based on the relevance of the input context, allowing for more adaptive response patterns.","Concurrently, the GRC modifies the transformer's residual connections through a gating mechanism that selectively controls the information flow, thereby enhancing the network's ability to focus on contextually important features.","We evaluate the performance of these enhancements across several benchmarks in natural language processing.","Our results demonstrate improved adaptability and efficiency, suggesting that these modifications could set new standards for designing flexible and context-aware transformer models."],"url":"http://arxiv.org/abs/2405.13407v1","category":"cs.LG"}
{"created":"2024-05-22 07:24:20","title":"Adaptive Wireless Image Semantic Transmission and Over-The-Air Testing","abstract":"Semantic communication has undergone considerable evolution due to the recent rapid development of artificial intelligence (AI), significantly enhancing both communication robustness and efficiency. Despite these advancements, most current semantic communication methods for image transmission pay little attention to the differing importance of objects and backgrounds in images. To address this issue, we propose a novel scheme named ASCViT-JSCC, which utilizes vision transformers (ViTs) integrated with an orthogonal frequency division multiplexing (OFDM) system. This scheme adaptively allocates bandwidth for objects and backgrounds in images according to the importance order of different parts determined by object detection of you only look once version 5 (YOLOv5) and feature points detection of scale invariant feature transform (SIFT). Furthermore, the proposed scheme adheres to digital modulation standards by incorporating quantization modules. We validate this approach through an over-the-air (OTA) testbed named intelligent communication prototype validation platform (ICP) based on a software-defined radio (SDR) and NVIDIA embedded kits. Our findings from both simulations and practical measurements show that ASCViT-JSCC significantly preserves objects in images and enhances reconstruction quality compared to existing methods.","sentences":["Semantic communication has undergone considerable evolution due to the recent rapid development of artificial intelligence (AI), significantly enhancing both communication robustness and efficiency.","Despite these advancements, most current semantic communication methods for image transmission pay little attention to the differing importance of objects and backgrounds in images.","To address this issue, we propose a novel scheme named ASCViT-JSCC, which utilizes vision transformers (ViTs) integrated with an orthogonal frequency division multiplexing (OFDM) system.","This scheme adaptively allocates bandwidth for objects and backgrounds in images according to the importance order of different parts determined by object detection of you only look once version 5 (YOLOv5) and feature points detection of scale invariant feature transform (SIFT).","Furthermore, the proposed scheme adheres to digital modulation standards by incorporating quantization modules.","We validate this approach through an over-the-air (OTA) testbed named intelligent communication prototype validation platform (ICP) based on a software-defined radio (SDR) and NVIDIA embedded kits.","Our findings from both simulations and practical measurements show that ASCViT-JSCC significantly preserves objects in images and enhances reconstruction quality compared to existing methods."],"url":"http://arxiv.org/abs/2405.13403v1","category":"eess.IV"}
{"created":"2024-05-22 07:02:35","title":"Convergence analysis of kernel learning FBSDE filter","abstract":"Kernel learning forward backward SDE filter is an iterative and adaptive meshfree approach to solve the nonlinear filtering problem. It builds from forward backward SDE for Fokker-Planker equation, which defines evolving density for the state variable, and employs KDE to approximate density. This algorithm has shown more superior performance than mainstream particle filter method, in both convergence speed and efficiency of solving high dimension problems.   However, this method has only been shown to converge empirically. In this paper, we present a rigorous analysis to demonstrate its local and global convergence, and provide theoretical support for its empirical results.","sentences":["Kernel learning forward backward SDE filter is an iterative and adaptive meshfree approach to solve the nonlinear filtering problem.","It builds from forward backward SDE for Fokker-Planker equation, which defines evolving density for the state variable, and employs KDE to approximate density.","This algorithm has shown more superior performance than mainstream particle filter method, in both convergence speed and efficiency of solving high dimension problems.   ","However, this method has only been shown to converge empirically.","In this paper, we present a rigorous analysis to demonstrate its local and global convergence, and provide theoretical support for its empirical results."],"url":"http://arxiv.org/abs/2405.13390v1","category":"cs.LG"}
{"created":"2024-05-22 06:38:51","title":"Elastic-gap free strain gradient crystal plasticity model that effectively account for plastic slip gradient and grain boundary dissipation","abstract":"This paper proposes an elastic-gap free strain gradient crystal plasticity model that addresses dissipation caused by plastic slip gradient and grain boundary (GB) Burger tensor. The model involves splitting plastic slip gradient and GB Burger tensor into energetic dissipative quantities. Unlike conventional models, the bulk and GB defect energy are considered to be a quadratic functional of the energetic portion of slip gradient and GB Burgers tensor. The higher-order stresses for each individual slip systems and GB stresses are derived from the defect energy, following a similar evolution as the Armstrong-Frederick type backstress model in classical plasticity. The evolution equations consist of a hardening and a relaxation term. The relaxation term brings the nonlinearity in hardening and causes an additional dissipation. The applicability of the proposed model is numerically established with the help of two-dimensional finite element implementation. Specifically, the bulk and GB relaxation coefficients are critically evaluated based on various circumstances, considering single crystal infinite shear layer, periodic bicrystal shearing, and bicrystal tension problem. In contrast to the Gurtin-type model, the proposed model smoothly captures the apparent strengthening at saturation without causing any abrupt stress jump under non-proportional loading conditions. Moreover, when subjected to cyclic loading, the stress-strain curve maintains its curvature during reverse loading. The numerical simulation reveals that the movement of geometrically necessary dislocation (GND) towards the GB is influenced by the bulk recovery coefficient, while the dissipation and amount of accumulation of GND near the GB are controlled by the GB recovery coefficient.","sentences":["This paper proposes an elastic-gap free strain gradient crystal plasticity model that addresses dissipation caused by plastic slip gradient and grain boundary (GB) Burger tensor.","The model involves splitting plastic slip gradient and GB Burger tensor into energetic dissipative quantities.","Unlike conventional models, the bulk and GB defect energy are considered to be a quadratic functional of the energetic portion of slip gradient and GB Burgers tensor.","The higher-order stresses for each individual slip systems and GB stresses are derived from the defect energy, following a similar evolution as the Armstrong-Frederick type backstress model in classical plasticity.","The evolution equations consist of a hardening and a relaxation term.","The relaxation term brings the nonlinearity in hardening and causes an additional dissipation.","The applicability of the proposed model is numerically established with the help of two-dimensional finite element implementation.","Specifically, the bulk and GB relaxation coefficients are critically evaluated based on various circumstances, considering single crystal infinite shear layer, periodic bicrystal shearing, and bicrystal tension problem.","In contrast to the Gurtin-type model, the proposed model smoothly captures the apparent strengthening at saturation without causing any abrupt stress jump under non-proportional loading conditions.","Moreover, when subjected to cyclic loading, the stress-strain curve maintains its curvature during reverse loading.","The numerical simulation reveals that the movement of geometrically necessary dislocation (GND) towards the GB is influenced by the bulk recovery coefficient, while the dissipation and amount of accumulation of GND near the GB are controlled by the GB recovery coefficient."],"url":"http://arxiv.org/abs/2405.13384v1","category":"cs.CE"}
{"created":"2024-05-22 06:33:48","title":"Gradient Projection For Parameter-Efficient Continual Learning","abstract":"Catastrophic forgetting poses the primary challenge in the continual learning. Nowadays, methods based on parameter-efficient tuning (PET) have demonstrated impressive performance in continual learning. However, these methods are still confronted with a common problem: fine-tuning on consecutive distinct tasks can disrupt the existing parameter distribution and lead to forgetting. Recent progress mainly focused in empirically designing efficient tuning engineering, lacking investigation of forgetting generation mechanism, anti-forgetting criteria and providing theoretical support. Additionally, the unresolved trade-off between learning new content and protecting old knowledge further complicates these challenges. The gradient projection methodology restricts gradient updates to the orthogonal direction of the old feature space, preventing distribution of the parameters from being damaged during updating and significantly suppressing forgetting. Developing on it, in this paper, we reformulate Adapter, LoRA, Prefix, and Prompt to continual learning setting from the perspective of gradient projection, and propose a unified framework called Parameter Efficient Gradient Projection (PEGP). Based on the hypothesis that old tasks should have the same results after model updated, we introduce orthogonal gradient projection into different PET paradigms and theoretically demonstrate that the orthogonal condition for the gradient can effectively resist forgetting in PET-based continual methods. Notably, PEGP is the first unified method to provide an anti-forgetting mechanism with mathematical demonstration for different tuning paradigms. We extensively evaluate our method with different backbones on diverse datasets, and experiments demonstrate its efficiency in reducing forgetting in various incremental settings.","sentences":["Catastrophic forgetting poses the primary challenge in the continual learning.","Nowadays, methods based on parameter-efficient tuning (PET) have demonstrated impressive performance in continual learning.","However, these methods are still confronted with a common problem: fine-tuning on consecutive distinct tasks can disrupt the existing parameter distribution and lead to forgetting.","Recent progress mainly focused in empirically designing efficient tuning engineering, lacking investigation of forgetting generation mechanism, anti-forgetting criteria and providing theoretical support.","Additionally, the unresolved trade-off between learning new content and protecting old knowledge further complicates these challenges.","The gradient projection methodology restricts gradient updates to the orthogonal direction of the old feature space, preventing distribution of the parameters from being damaged during updating and significantly suppressing forgetting.","Developing on it, in this paper, we reformulate Adapter, LoRA, Prefix, and Prompt to continual learning setting from the perspective of gradient projection, and propose a unified framework called Parameter Efficient Gradient Projection (PEGP).","Based on the hypothesis that old tasks should have the same results after model updated, we introduce orthogonal gradient projection into different PET paradigms and theoretically demonstrate that the orthogonal condition for the gradient can effectively resist forgetting in PET-based continual methods.","Notably, PEGP is the first unified method to provide an anti-forgetting mechanism with mathematical demonstration for different tuning paradigms.","We extensively evaluate our method with different backbones on diverse datasets, and experiments demonstrate its efficiency in reducing forgetting in various incremental settings."],"url":"http://arxiv.org/abs/2405.13383v1","category":"cs.LG"}
{"created":"2024-05-22 06:19:43","title":"FedCache 2.0: Exploiting the Potential of Distilled Data in Knowledge Cache-driven Federated Learning","abstract":"Federated Edge Learning (FEL) has emerged as a promising approach for enabling edge devices to collaboratively train machine learning models while preserving data privacy. Despite its advantages, practical FEL deployment faces significant challenges related to device constraints and device-server interactions, necessitating heterogeneous, user-adaptive model training with limited and uncertain communication. In this paper, we introduce FedCache 2.0, a novel personalized FEL architecture that simultaneously addresses these challenges. FedCache 2.0 incorporates the benefits of both dataset distillation and knowledge cache-driven federated learning by storing and organizing distilled data as knowledge in the server-side knowledge cache. Moreover, a device-centric cache sampling strategy is introduced to tailor transferred knowledge for individual devices within controlled communication bandwidth. Extensive experiments on five datasets covering image recognition, audio understanding, and mobile sensor data mining tasks demonstrate that (1) FedCache 2.0 significantly outperforms state-of-the-art methods regardless of model structures, data distributions, and modalities. (2) FedCache 2.0 can train splendid personalized on-device models with at least $\\times$28.6 improvement in communication efficiency.","sentences":["Federated Edge Learning (FEL) has emerged as a promising approach for enabling edge devices to collaboratively train machine learning models while preserving data privacy.","Despite its advantages, practical FEL deployment faces significant challenges related to device constraints and device-server interactions, necessitating heterogeneous, user-adaptive model training with limited and uncertain communication.","In this paper, we introduce FedCache 2.0, a novel personalized FEL architecture that simultaneously addresses these challenges.","FedCache 2.0 incorporates the benefits of both dataset distillation and knowledge cache-driven federated learning by storing and organizing distilled data as knowledge in the server-side knowledge cache.","Moreover, a device-centric cache sampling strategy is introduced to tailor transferred knowledge for individual devices within controlled communication bandwidth.","Extensive experiments on five datasets covering image recognition, audio understanding, and mobile sensor data mining tasks demonstrate that (1) FedCache 2.0 significantly outperforms state-of-the-art methods regardless of model structures, data distributions, and modalities.","(2) FedCache 2.0 can train splendid personalized on-device models with at least $\\times$28.6 improvement in communication efficiency."],"url":"http://arxiv.org/abs/2405.13378v1","category":"cs.LG"}
{"created":"2024-05-22 06:17:58","title":"Adaptive Data Analysis for Growing Data","abstract":"Reuse of data in adaptive workflows poses challenges regarding overfitting and the statistical validity of results. Previous work has demonstrated that interacting with data via differentially private algorithms can mitigate overfitting, achieving worst-case generalization guarantees with asymptotically optimal data requirements. However, such past work assumes data is static and cannot accommodate situations where data grows over time. In this paper we address this gap, presenting the first generalization bounds for adaptive analysis in the dynamic data setting. We allow the analyst to adaptively schedule their queries conditioned on the current size of the data, in addition to previous queries and responses. We also incorporate time-varying empirical accuracy bounds and mechanisms, allowing for tighter guarantees as data accumulates. In a batched query setting, the asymptotic data requirements of our bound grows with the square-root of the number of adaptive queries, matching prior works' improvement over data splitting for the static setting. We instantiate our bound for statistical queries with the clipped Gaussian mechanism, where it empirically outperforms baselines composed from static bounds.","sentences":["Reuse of data in adaptive workflows poses challenges regarding overfitting and the statistical validity of results.","Previous work has demonstrated that interacting with data via differentially private algorithms can mitigate overfitting, achieving worst-case generalization guarantees with asymptotically optimal data requirements.","However, such past work assumes data is static and cannot accommodate situations where data grows over time.","In this paper we address this gap, presenting the first generalization bounds for adaptive analysis in the dynamic data setting.","We allow the analyst to adaptively schedule their queries conditioned on the current size of the data, in addition to previous queries and responses.","We also incorporate time-varying empirical accuracy bounds and mechanisms, allowing for tighter guarantees as data accumulates.","In a batched query setting, the asymptotic data requirements of our bound grows with the square-root of the number of adaptive queries, matching prior works' improvement over data splitting for the static setting.","We instantiate our bound for statistical queries with the clipped Gaussian mechanism, where it empirically outperforms baselines composed from static bounds."],"url":"http://arxiv.org/abs/2405.13375v1","category":"cs.LG"}
{"created":"2024-05-22 06:15:50","title":"Ada-HGNN: Adaptive Sampling for Scalable Hypergraph Neural Networks","abstract":"Hypergraphs serve as an effective model for depicting complex connections in various real-world scenarios, from social to biological networks. The development of Hypergraph Neural Networks (HGNNs) has emerged as a valuable method to manage the intricate associations in data, though scalability is a notable challenge due to memory limitations. In this study, we introduce a new adaptive sampling strategy specifically designed for hypergraphs, which tackles their unique complexities in an efficient manner. We also present a Random Hyperedge Augmentation (RHA) technique and an additional Multilayer Perceptron (MLP) module to improve the robustness and generalization capabilities of our approach. Thorough experiments with real-world datasets have proven the effectiveness of our method, markedly reducing computational and memory demands while maintaining performance levels akin to conventional HGNNs and other baseline models. This research paves the way for improving both the scalability and efficacy of HGNNs in extensive applications. We will also make our codebase publicly accessible.","sentences":["Hypergraphs serve as an effective model for depicting complex connections in various real-world scenarios, from social to biological networks.","The development of Hypergraph Neural Networks (HGNNs) has emerged as a valuable method to manage the intricate associations in data, though scalability is a notable challenge due to memory limitations.","In this study, we introduce a new adaptive sampling strategy specifically designed for hypergraphs, which tackles their unique complexities in an efficient manner.","We also present a Random Hyperedge Augmentation (RHA) technique and an additional Multilayer Perceptron (MLP) module to improve the robustness and generalization capabilities of our approach.","Thorough experiments with real-world datasets have proven the effectiveness of our method, markedly reducing computational and memory demands while maintaining performance levels akin to conventional HGNNs and other baseline models.","This research paves the way for improving both the scalability and efficacy of HGNNs in extensive applications.","We will also make our codebase publicly accessible."],"url":"http://arxiv.org/abs/2405.13372v1","category":"cs.LG"}
{"created":"2024-05-22 05:48:25","title":"Clipped Uniform Quantizers for Communication-Efficient Federated Learning","abstract":"This paper introduces an approach to employ clipped uniform quantization in federated learning settings, aiming to enhance model efficiency by reducing communication overhead without compromising accuracy. By employing optimal clipping thresholds and adaptive quantization schemes, our method significantly curtails the bit requirements for model weight transmissions between clients and the server. We explore the implications of symmetric clipping and uniform quantization on model performance, highlighting the utility of stochastic quantization to mitigate quantization artifacts and improve model robustness. Through extensive simulations on the MNIST dataset, our results demonstrate that the proposed method achieves near full-precision performance while ensuring substantial communication savings. Specifically, our approach facilitates efficient weight averaging based on quantization errors, effectively balancing the trade-off between communication efficiency and model accuracy. The comparative analysis with conventional quantization methods further confirms the superiority of our technique.","sentences":["This paper introduces an approach to employ clipped uniform quantization in federated learning settings, aiming to enhance model efficiency by reducing communication overhead without compromising accuracy.","By employing optimal clipping thresholds and adaptive quantization schemes, our method significantly curtails the bit requirements for model weight transmissions between clients and the server.","We explore the implications of symmetric clipping and uniform quantization on model performance, highlighting the utility of stochastic quantization to mitigate quantization artifacts and improve model robustness.","Through extensive simulations on the MNIST dataset, our results demonstrate that the proposed method achieves near full-precision performance while ensuring substantial communication savings.","Specifically, our approach facilitates efficient weight averaging based on quantization errors, effectively balancing the trade-off between communication efficiency and model accuracy.","The comparative analysis with conventional quantization methods further confirms the superiority of our technique."],"url":"http://arxiv.org/abs/2405.13365v1","category":"cs.LG"}
{"created":"2024-05-22 05:32:11","title":"AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs","abstract":"The ever-growing computational complexity of Large Language Models (LLMs) necessitates efficient deployment strategies. The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy. This paper presents AdpQ, a novel zero-shot adaptive PTQ method for LLMs that achieves the state-of-the-art performance in low-precision quantization (e.g. 3-bit) without requiring any calibration data. Inspired by Adaptive LASSO regression model, our proposed approach tackles the challenge of outlier activations by separating salient weights using an adaptive soft-thresholding method. Guided by Adaptive LASSO, this method ensures that the quantized weights distribution closely follows the originally trained weights and eliminates the need for calibration data entirely, setting our method apart from popular approaches such as SpQR and AWQ. Furthermore, our method offers an additional benefit in terms of privacy preservation by eliminating any calibration or training data. We also delve deeper into the information-theoretic underpinnings of the proposed method. We demonstrate that it leverages the Adaptive LASSO to minimize the Kullback-Leibler divergence between the quantized weights and the originally trained weights. This minimization ensures the quantized model retains the Shannon information content of the original model to a great extent, guaranteeing efficient deployment without sacrificing accuracy or information. Our results achieve the same accuracy as the existing methods on various LLM benchmarks while the quantization time is reduced by at least 10x, solidifying our contribution to efficient and privacy-preserving LLM deployment.","sentences":["The ever-growing computational complexity of Large Language Models (LLMs) necessitates efficient deployment strategies.","The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy.","This paper presents AdpQ, a novel zero-shot adaptive PTQ method for LLMs that achieves the state-of-the-art performance in low-precision quantization (e.g. 3-bit) without requiring any calibration data.","Inspired by Adaptive LASSO regression model, our proposed approach tackles the challenge of outlier activations by separating salient weights using an adaptive soft-thresholding method.","Guided by Adaptive LASSO, this method ensures that the quantized weights distribution closely follows the originally trained weights and eliminates the need for calibration data entirely, setting our method apart from popular approaches such as SpQR and AWQ.","Furthermore, our method offers an additional benefit in terms of privacy preservation by eliminating any calibration or training data.","We also delve deeper into the information-theoretic underpinnings of the proposed method.","We demonstrate that it leverages the Adaptive LASSO to minimize the Kullback-Leibler divergence between the quantized weights and the originally trained weights.","This minimization ensures the quantized model retains the Shannon information content of the original model to a great extent, guaranteeing efficient deployment without sacrificing accuracy or information.","Our results achieve the same accuracy as the existing methods on various LLM benchmarks while the quantization time is reduced by at least 10x, solidifying our contribution to efficient and privacy-preserving LLM deployment."],"url":"http://arxiv.org/abs/2405.13358v1","category":"cs.CL"}
{"created":"2024-05-22 05:19:51","title":"Large Language Models (LLMs) Assisted Wireless Network Deployment in Urban Settings","abstract":"The advent of Large Language Models (LLMs) has revolutionized language understanding and human-like text generation, drawing interest from many other fields with this question in mind: What else are the LLMs capable of? Despite their widespread adoption, ongoing research continues to explore new ways to integrate LLMs into diverse systems.   This paper explores new techniques to harness the power of LLMs for 6G (6th Generation) wireless communication technologies, a domain where automation and intelligent systems are pivotal. The inherent adaptability of LLMs to domain-specific tasks positions them as prime candidates for enhancing wireless systems in the 6G landscape.   We introduce a novel Reinforcement Learning (RL) based framework that leverages LLMs for network deployment in wireless communications. Our approach involves training an RL agent, utilizing LLMs as its core, in an urban setting to maximize coverage. The agent's objective is to navigate the complexities of urban environments and identify the network parameters for optimal area coverage. Additionally, we integrate LLMs with Convolutional Neural Networks (CNNs) to capitalize on their strengths while mitigating their limitations. The Deep Deterministic Policy Gradient (DDPG) algorithm is employed for training purposes. The results suggest that LLM-assisted models can outperform CNN-based models in some cases while performing at least as well in others.","sentences":["The advent of Large Language Models (LLMs) has revolutionized language understanding and human-like text generation, drawing interest from many other fields with this question in mind: What else are the LLMs capable of?","Despite their widespread adoption, ongoing research continues to explore new ways to integrate LLMs into diverse systems.   ","This paper explores new techniques to harness the power of LLMs for 6G (6th Generation) wireless communication technologies, a domain where automation and intelligent systems are pivotal.","The inherent adaptability of LLMs to domain-specific tasks positions them as prime candidates for enhancing wireless systems in the 6G landscape.   ","We introduce a novel Reinforcement Learning (RL) based framework that leverages LLMs for network deployment in wireless communications.","Our approach involves training an RL agent, utilizing LLMs as its core, in an urban setting to maximize coverage.","The agent's objective is to navigate the complexities of urban environments and identify the network parameters for optimal area coverage.","Additionally, we integrate LLMs with Convolutional Neural Networks (CNNs) to capitalize on their strengths while mitigating their limitations.","The Deep Deterministic Policy Gradient (DDPG) algorithm is employed for training purposes.","The results suggest that LLM-assisted models can outperform CNN-based models in some cases while performing at least as well in others."],"url":"http://arxiv.org/abs/2405.13356v1","category":"cs.AI"}
{"created":"2024-05-22 05:14:52","title":"Adaptive Bayesian Multivariate Spline Knot Inference with Prior Specifications on Model Complexity","abstract":"In multivariate spline regression, the number and locations of knots influence the performance and interpretability significantly. However, due to non-differentiability and varying dimensions, there is no desirable frequentist method to make inference on knots. In this article, we propose a fully Bayesian approach for knot inference in multivariate spline regression. The existing Bayesian method often uses BIC to calculate the posterior, but BIC is too liberal and it will heavily overestimate the knot number when the candidate model space is large. We specify a new prior on the knot number to take into account the complexity of the model space and derive an analytic formula in the normal model. In the non-normal cases, we utilize the extended Bayesian information criterion to approximate the posterior density. The samples are simulated in the space with differing dimensions via reversible jump Markov chain Monte Carlo. We apply the proposed method in knot inference and manifold denoising. Experiments demonstrate the splendid capability of the algorithm, especially in function fitting with jumping discontinuity.","sentences":["In multivariate spline regression, the number and locations of knots influence the performance and interpretability significantly.","However, due to non-differentiability and varying dimensions, there is no desirable frequentist method to make inference on knots.","In this article, we propose a fully Bayesian approach for knot inference in multivariate spline regression.","The existing Bayesian method often uses BIC to calculate the posterior, but BIC is too liberal and it will heavily overestimate the knot number when the candidate model space is large.","We specify a new prior on the knot number to take into account the complexity of the model space and derive an analytic formula in the normal model.","In the non-normal cases, we utilize the extended Bayesian information criterion to approximate the posterior density.","The samples are simulated in the space with differing dimensions via reversible jump Markov chain Monte Carlo.","We apply the proposed method in knot inference and manifold denoising.","Experiments demonstrate the splendid capability of the algorithm, especially in function fitting with jumping discontinuity."],"url":"http://arxiv.org/abs/2405.13353v1","category":"stat.ME"}
{"created":"2024-05-22 03:56:55","title":"DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction Model with Slot Querying","abstract":"Recent advancements in event argument extraction (EAE) involve incorporating beneficial auxiliary information into models during training and inference, such as retrieved instances and event templates. Additionally, some studies introduce learnable prefix vectors to models. These methods face three challenges: (1) insufficient utilization of relevant event instances due to deficiencies in retrieval; (2) neglect of important information provided by relevant event templates; (3) the advantages of prefixes are constrained due to their inability to meet the specific informational needs of EAE. In this work, we propose DEGAP, which addresses the above challenges through two simple yet effective components: (1) dual prefixes, where the instance-oriented prefix and template-oriented prefix are trained to learn information from different event instances and templates, respectively, and then provide relevant information as cues to EAE model without retrieval; (2) event-guided adaptive gating mechanism, which guides the prefixes based on the target event to fully leverage their advantages. Extensive experiments demonstrate that our method achieves new state-of-the-art performance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further analysis verifies the importance of the proposed design and the effectiveness of the main components.","sentences":["Recent advancements in event argument extraction (EAE) involve incorporating beneficial auxiliary information into models during training and inference, such as retrieved instances and event templates.","Additionally, some studies introduce learnable prefix vectors to models.","These methods face three challenges: (1) insufficient utilization of relevant event instances due to deficiencies in retrieval; (2) neglect of important information provided by relevant event templates; (3) the advantages of prefixes are constrained due to their inability to meet the specific informational needs of EAE.","In this work, we propose DEGAP, which addresses the above challenges through two simple yet effective components: (1) dual prefixes, where the instance-oriented prefix and template-oriented prefix are trained to learn information from different event instances and templates, respectively, and then provide relevant information as cues to EAE model without retrieval; (2) event-guided adaptive gating mechanism, which guides the prefixes based on the target event to fully leverage their advantages.","Extensive experiments demonstrate that our method achieves new state-of-the-art performance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE).","Further analysis verifies the importance of the proposed design and the effectiveness of the main components."],"url":"http://arxiv.org/abs/2405.13325v1","category":"cs.CL"}
{"created":"2024-05-22 03:47:55","title":"Adversarial Training via Adaptive Knowledge Amalgamation of an Ensemble of Teachers","abstract":"Adversarial training (AT) is a popular method for training robust deep neural networks (DNNs) against adversarial attacks. Yet, AT suffers from two shortcomings: (i) the robustness of DNNs trained by AT is highly intertwined with the size of the DNNs, posing challenges in achieving robustness in smaller models; and (ii) the adversarial samples employed during the AT process exhibit poor generalization, leaving DNNs vulnerable to unforeseen attack types. To address these dual challenges, this paper introduces adversarial training via adaptive knowledge amalgamation of an ensemble of teachers (AT-AKA). In particular, we generate a diverse set of adversarial samples as the inputs to an ensemble of teachers; and then, we adaptively amalgamate the logtis of these teachers to train a generalized-robust student. Through comprehensive experiments, we illustrate the superior efficacy of AT-AKA over existing AT methods and adversarial robustness distillation techniques against cutting-edge attacks, including AutoAttack.","sentences":["Adversarial training (AT) is a popular method for training robust deep neural networks (DNNs) against adversarial attacks.","Yet, AT suffers from two shortcomings: (i) the robustness of DNNs trained by AT is highly intertwined with the size of the DNNs, posing challenges in achieving robustness in smaller models; and (ii) the adversarial samples employed during the AT process exhibit poor generalization, leaving DNNs vulnerable to unforeseen attack types.","To address these dual challenges, this paper introduces adversarial training via adaptive knowledge amalgamation of an ensemble of teachers (AT-AKA).","In particular, we generate a diverse set of adversarial samples as the inputs to an ensemble of teachers; and then, we adaptively amalgamate the logtis of these teachers to train a generalized-robust student.","Through comprehensive experiments, we illustrate the superior efficacy of AT-AKA over existing AT methods and adversarial robustness distillation techniques against cutting-edge attacks, including AutoAttack."],"url":"http://arxiv.org/abs/2405.13324v1","category":"cs.LG"}
{"created":"2024-05-22 02:44:46","title":"Accelerated Evaluation of Ollivier-Ricci Curvature Lower Bounds: Bridging Theory and Computation","abstract":"Curvature serves as a potent and descriptive invariant, with its efficacy validated both theoretically and practically within graph theory. We employ a definition of generalized Ricci curvature proposed by Ollivier, which Lin and Yau later adapted to graph theory, known as Ollivier-Ricci curvature (ORC). ORC measures curvature using the Wasserstein distance, thereby integrating geometric concepts with probability theory and optimal transport. Jost and Liu previously discussed the lower bound of ORC by showing the upper bound of the Wasserstein distance. We extend the applicability of these bounds to discrete spaces with metrics on integers, specifically hypergraphs. Compared to prior work on ORC in hypergraphs by Coupette, Dalleiger, and Rieck, which faced computational challenges, our method introduces a simplified approach with linear computational complexity, making it particularly suitable for analyzing large-scale networks. Through extensive simulations and application to synthetic and real-world datasets, we demonstrate the significant improvements our method offers in evaluating ORC.","sentences":["Curvature serves as a potent and descriptive invariant, with its efficacy validated both theoretically and practically within graph theory.","We employ a definition of generalized Ricci curvature proposed by Ollivier, which Lin and Yau later adapted to graph theory, known as Ollivier-Ricci curvature (ORC).","ORC measures curvature using the Wasserstein distance, thereby integrating geometric concepts with probability theory and optimal transport.","Jost and Liu previously discussed the lower bound of ORC by showing the upper bound of the Wasserstein distance.","We extend the applicability of these bounds to discrete spaces with metrics on integers, specifically hypergraphs.","Compared to prior work on ORC in hypergraphs by Coupette, Dalleiger, and Rieck, which faced computational challenges, our method introduces a simplified approach with linear computational complexity, making it particularly suitable for analyzing large-scale networks.","Through extensive simulations and application to synthetic and real-world datasets, we demonstrate the significant improvements our method offers in evaluating ORC."],"url":"http://arxiv.org/abs/2405.13302v1","category":"stat.ML"}
{"created":"2024-05-22 02:37:02","title":"FAITH: Frequency-domain Attention In Two Horizons for Time Series Forecasting","abstract":"Time Series Forecasting plays a crucial role in various fields such as industrial equipment maintenance, meteorology, energy consumption, traffic flow and financial investment. However, despite their considerable advantages over traditional statistical approaches, current deep learning-based predictive models often exhibit a significant deviation between their forecasting outcomes and the ground truth. This discrepancy is largely due to an insufficient emphasis on extracting the sequence's latent information, particularly its global information within the frequency domain and the relationship between different variables. To address this issue, we propose a novel model Frequency-domain Attention In Two Horizons, which decomposes time series into trend and seasonal components using a multi-scale sequence adaptive decomposition and fusion architecture, and processes them separately. FAITH utilizes Frequency Channel feature Extraction Module and Frequency Temporal feature Extraction Module to capture inter-channel relationships and temporal global information in the sequence, significantly improving its ability to handle long-term dependencies and complex patterns. Furthermore, FAITH achieves theoretically linear complexity by modifying the time-frequency domain transformation method, effectively reducing computational costs. Extensive experiments on 6 benchmarks for long-term forecasting and 3 benchmarks for short-term forecasting demonstrate that FAITH outperforms existing models in many fields, such as electricity, weather and traffic, proving its effectiveness and superiority both in long-term and short-term time series forecasting tasks. Our codes and data are available at https://github.com/LRQ577/FAITH.","sentences":["Time Series Forecasting plays a crucial role in various fields such as industrial equipment maintenance, meteorology, energy consumption, traffic flow and financial investment.","However, despite their considerable advantages over traditional statistical approaches, current deep learning-based predictive models often exhibit a significant deviation between their forecasting outcomes and the ground truth.","This discrepancy is largely due to an insufficient emphasis on extracting the sequence's latent information, particularly its global information within the frequency domain and the relationship between different variables.","To address this issue, we propose a novel model Frequency-domain Attention In Two Horizons, which decomposes time series into trend and seasonal components using a multi-scale sequence adaptive decomposition and fusion architecture, and processes them separately.","FAITH utilizes Frequency Channel feature Extraction Module and Frequency Temporal feature Extraction Module to capture inter-channel relationships and temporal global information in the sequence, significantly improving its ability to handle long-term dependencies and complex patterns.","Furthermore, FAITH achieves theoretically linear complexity by modifying the time-frequency domain transformation method, effectively reducing computational costs.","Extensive experiments on 6 benchmarks for long-term forecasting and 3 benchmarks for short-term forecasting demonstrate that FAITH outperforms existing models in many fields, such as electricity, weather and traffic, proving its effectiveness and superiority both in long-term and short-term time series forecasting tasks.","Our codes and data are available at https://github.com/LRQ577/FAITH."],"url":"http://arxiv.org/abs/2405.13300v1","category":"cs.LG"}
{"created":"2024-05-22 02:32:58","title":"An Efficient Approach for Solving Expensive Constrained Multiobjective Optimization Problems","abstract":"To solve real-world expensive constrained multi-objective optimization problems (ECMOPs), surrogate/approximation models are commonly incorporated in evolutionary algorithms to pre-select promising candidate solutions for evaluation. However, the performance of existing approaches are highly dependent on the relative position of unconstrained and constrained Pareto fronts (UPF and CPF, respectively). In addition, the uncertainty information of surrogate models is often ignored, which can misguide the search. To mitigate these key issues (among others), an efficient probabilistic selection based constrained multi-objective EA is proposed, referred to as PSCMOEA. It comprises novel elements such as (a) an adaptive search bound identification scheme based on the feasibility and convergence status of evaluated solutions (b) a probabilistic selection method backed by theoretical formulations of model mean and uncertainties to conduct search in the predicted space to identify promising solutions (c) an efficient single infill sampling approach to balance feasibility, convergence and diversity across different stages of the search and (d) an adaptive switch to unconstrained search based on certain search conditions. Numerical experiments are conducted on an extensive range of challenging constrained problems using low evaluation budgets to simulate ECMOPs. The performance of PSCMOEA is benchmarked against five competitive state-of-the-art algorithms, to demonstrate its competitive and consistent performance.","sentences":["To solve real-world expensive constrained multi-objective optimization problems (ECMOPs), surrogate/approximation models are commonly incorporated in evolutionary algorithms to pre-select promising candidate solutions for evaluation.","However, the performance of existing approaches are highly dependent on the relative position of unconstrained and constrained Pareto fronts (UPF and CPF, respectively).","In addition, the uncertainty information of surrogate models is often ignored, which can misguide the search.","To mitigate these key issues (among others), an efficient probabilistic selection based constrained multi-objective EA is proposed, referred to as PSCMOEA.","It comprises novel elements such as (a) an adaptive search bound identification scheme based on the feasibility and convergence status of evaluated solutions (b) a probabilistic selection method backed by theoretical formulations of model mean and uncertainties to conduct search in the predicted space to identify promising solutions (c) an efficient single infill sampling approach to balance feasibility, convergence and diversity across different stages of the search and (d) an adaptive switch to unconstrained search based on certain search conditions.","Numerical experiments are conducted on an extensive range of challenging constrained problems using low evaluation budgets to simulate ECMOPs.","The performance of PSCMOEA is benchmarked against five competitive state-of-the-art algorithms, to demonstrate its competitive and consistent performance."],"url":"http://arxiv.org/abs/2405.13298v1","category":"cs.NE"}
{"created":"2024-05-22 02:09:22","title":"Theoretical Analysis of Meta Reinforcement Learning: Generalization Bounds and Convergence Guarantees","abstract":"This research delves deeply into Meta Reinforcement Learning (Meta RL) through a exploration focusing on defining generalization limits and ensuring convergence. By employing a approach this article introduces an innovative theoretical framework to meticulously assess the effectiveness and performance of Meta RL algorithms. We present an explanation of generalization limits measuring how well these algorithms can adapt to learning tasks while maintaining consistent results. Our analysis delves into the factors that impact the adaptability of Meta RL revealing the relationship, between algorithm design and task complexity. Additionally we establish convergence assurances by proving conditions under which Meta RL strategies are guaranteed to converge towards solutions. We examine the convergence behaviors of Meta RL algorithms across scenarios providing a comprehensive understanding of the driving forces behind their long term performance. This exploration covers both convergence and real time efficiency offering a perspective, on the capabilities of these algorithms.","sentences":["This research delves deeply into Meta Reinforcement Learning (Meta RL) through a exploration focusing on defining generalization limits and ensuring convergence.","By employing a approach this article introduces an innovative theoretical framework to meticulously assess the effectiveness and performance of Meta RL algorithms.","We present an explanation of generalization limits measuring how well these algorithms can adapt to learning tasks while maintaining consistent results.","Our analysis delves into the factors that impact the adaptability of Meta RL revealing the relationship, between algorithm design and task complexity.","Additionally we establish convergence assurances by proving conditions under which Meta RL strategies are guaranteed to converge towards solutions.","We examine the convergence behaviors of Meta RL algorithms across scenarios providing a comprehensive understanding of the driving forces behind their long term performance.","This exploration covers both convergence and real time efficiency offering a perspective, on the capabilities of these algorithms."],"url":"http://arxiv.org/abs/2405.13290v1","category":"cs.LG"}
{"created":"2024-05-22 02:02:56","title":"Monge-Amp\u00e8re equation, hyperk\u00e4hler structure and adapted complex structure","abstract":"In the tangent bundle of $(M,g)$, it is well-known that the Monge-Amp\\`ere equation $(\\partial\\bar\\partial \\sqrt\\rho)^n=0$ has the asymptotic expansion $ \\rho(x+iy)=\\sum_{ij} g_{ij} (x) y_{i} y_{j} + O(y^4)$ near $M$. Those 4th order terms are made explicit in this article: $$\\rho(x+iy)=\\sum_{i}y_{i}^2-\\frac 13\\sum_{pqij} R_{i p j q}(0)x_p x_q y_{i}y_{j}+O(5).$$ At $M$, sectional curvatures of the K\\\"ahler metric $2i\\partial\\bar\\partial\\rho$ can be computed. This has enabled us to find a family of K\\\"ahler manifolds whose tangent bundles have admitted complete hyperk\\\"ahler structures whereas the adapted complex structure can only be partially defined on the tangent bundles.   In these cases, the study of the adapted complex structure is equivalent to the study of some gauge transformations on the baby Nahm's equation $\\dot T_1+[T_0,T_1]=0.$","sentences":["In the tangent bundle of $(M,g)$, it is well-known that the Monge-Amp\\`ere equation $(\\partial\\bar\\partial \\sqrt\\rho)^n=0$ has the asymptotic expansion $ \\rho(x+iy)=\\sum_{ij} g_{ij} (x) y_{i} y_{j} + O(y^4)$ near $M$. Those 4th order terms are made explicit in this article: $$\\rho(x+iy)=\\sum_{i}y_{i}^2-\\frac 13\\sum_{pqij} R_{i p j q}(0)x_p x_q","y_{i}y_{j}+O(5).$$","At $M$, sectional curvatures of the K\\\"ahler metric $2i\\partial\\bar\\partial\\rho$ can be computed.","This has enabled us to find a family of K\\\"ahler manifolds whose tangent bundles have admitted complete hyperk\\\"ahler structures whereas the adapted complex structure can only be partially defined on the tangent bundles.   ","In these cases, the study of the adapted complex structure is equivalent to the study of some gauge transformations on the baby Nahm's equation $\\dot T_1+[T_0,T_1]=0.$"],"url":"http://arxiv.org/abs/2405.13287v1","category":"math.DG"}
{"created":"2024-05-22 00:25:02","title":"Homodyne detection is optimal for quantum interferometry with path-entangled coherent states","abstract":"We present measurement schemes that do not rely on photon-number resolving detectors, but that are nevertheless optimal for estimating a differential phase shift in interferometry with either an entangled coherent state or a qubit-which-path state (where the path taken by a coherent-state wavepacket is entangled with the state of a qubit). The homodyning schemes analyzed here achieve optimality (saturate the quantum Cram\\'er-Rao bound) by maximizing the sensitivity of measurement outcomes to phase-dependent interference fringes in a reduced Wigner distribution. In the presence of photon loss, the schemes become suboptimal, but we find that their performance is independent of the phase to be measured. They can therefore be implemented without any prior information about the phase and without adapting the strategy during measurement, unlike strategies based on photon-number parity measurements or direct photon counting.","sentences":["We present measurement schemes that do not rely on photon-number resolving detectors, but that are nevertheless optimal for estimating a differential phase shift in interferometry with either an entangled coherent state or a qubit-which-path state (where the path taken by a coherent-state wavepacket is entangled with the state of a qubit).","The homodyning schemes analyzed here achieve optimality (saturate the quantum Cram\\'er-Rao bound) by maximizing the sensitivity of measurement outcomes to phase-dependent interference fringes in a reduced Wigner distribution.","In the presence of photon loss, the schemes become suboptimal, but we find that their performance is independent of the phase to be measured.","They can therefore be implemented without any prior information about the phase and without adapting the strategy during measurement, unlike strategies based on photon-number parity measurements or direct photon counting."],"url":"http://arxiv.org/abs/2405.13265v1","category":"quant-ph"}
{"created":"2024-05-21 22:44:08","title":"Granular temperature controls local rheology of vibrated granular flows","abstract":"We use numerical simulations to demonstrate a local rheology for sheared, vibrated granular flows. We consider a granular assembly that is subjected to simple shear and harmonic vibration at the boundary. This configuration allows us to isolate the effects of vibration, as parameterized by granular temperature. We find that friction is reduced due to local velocity fluctuations of grains. All data obey a local rheology that relates the material friction coefficient, the granular temperature, and the dimensionless shear rate. We also observe that reduction in material friction due to granular temperature is associated with reduction in fabric anisotropy. We demonstrate that the temperature can be modeled by a heat equation with dissipation with appropriate boundary conditions, which provides complete closure of the system and allows a fully local continuum description of sheared, vibrated granular flows. This success suggests local rheology based on temperature, as suggested previously, combined with the new, empirical heat diffusion equation may provide a general strategy for dense granular flows.","sentences":["We use numerical simulations to demonstrate a local rheology for sheared, vibrated granular flows.","We consider a granular assembly that is subjected to simple shear and harmonic vibration at the boundary.","This configuration allows us to isolate the effects of vibration, as parameterized by granular temperature.","We find that friction is reduced due to local velocity fluctuations of grains.","All data obey a local rheology that relates the material friction coefficient, the granular temperature, and the dimensionless shear rate.","We also observe that reduction in material friction due to granular temperature is associated with reduction in fabric anisotropy.","We demonstrate that the temperature can be modeled by a heat equation with dissipation with appropriate boundary conditions, which provides complete closure of the system and allows a fully local continuum description of sheared, vibrated granular flows.","This success suggests local rheology based on temperature, as suggested previously, combined with the new, empirical heat diffusion equation may provide a general strategy for dense granular flows."],"url":"http://arxiv.org/abs/2405.13236v1","category":"cond-mat.soft"}
{"created":"2024-05-21 20:08:52","title":"Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting","abstract":"In the domain of large language models (LLMs), arXiv:2305.16938 showed that few-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and Pattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize similarly on Out-Of-Domain (OOD) datasets, but vary in terms of task adaptation. However, they both pose challenges, especially in term of memory requirements. In this paper, we further try to push the understanding of different fine-tuning strategies for LLM and aim to bring a myriad of these on the same pedestal for an elaborate comparison with full-model fine-tuning on two diverse datasets. To that end, we conducted a series of experiments, beginning with state-of-the-art methods like vanilla fine-tuning and Pattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets, COLA and MNLI. We then investigate adaptive fine-tuning and the efficiency of LoRA adapters in a few-shot setting. Finally, we also compare an alternative approach that has gained recent popularity -- context distillation -- with the vanilla FT and PBFT with and without few-shot setup.   Our findings suggest that these alternative strategies that we explored can exhibit out-of-domain generalization comparable to that of vanilla FT and PBFT. PBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the need for effective prompts. Further, our adaptive-fine tuning and LoRA experiments perform comparable or slightly worse than the standard fine-tunings as anticipated, since standard fine-tunings involve tuning the entire model. Finally, our context distillation experiments out-perform the standard fine-tuning methods. These findings underscore that eventually the choice of an appropriate fine-tuning method depends on the available resources (memory, compute, data) and task adaptability.","sentences":["In the domain of large language models (LLMs), arXiv:2305.16938 showed that few-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and Pattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize similarly on Out-Of-Domain (OOD) datasets, but vary in terms of task adaptation.","However, they both pose challenges, especially in term of memory requirements.","In this paper, we further try to push the understanding of different fine-tuning strategies for LLM and aim to bring a myriad of these on the same pedestal for an elaborate comparison with full-model fine-tuning on two diverse datasets.","To that end, we conducted a series of experiments, beginning with state-of-the-art methods like vanilla fine-tuning and Pattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets, COLA and MNLI.","We then investigate adaptive fine-tuning and the efficiency of LoRA adapters in a few-shot setting.","Finally, we also compare an alternative approach that has gained recent popularity -- context distillation -- with the vanilla FT and PBFT with and without few-shot setup.   ","Our findings suggest that these alternative strategies that we explored can exhibit out-of-domain generalization comparable to that of vanilla FT and PBFT.","PBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the need for effective prompts.","Further, our adaptive-fine tuning and LoRA experiments perform comparable or slightly worse than the standard fine-tunings as anticipated, since standard fine-tunings involve tuning the entire model.","Finally, our context distillation experiments out-perform the standard fine-tuning methods.","These findings underscore that eventually the choice of an appropriate fine-tuning method depends on the available resources (memory, compute, data) and task adaptability."],"url":"http://arxiv.org/abs/2405.13181v1","category":"cs.CL"}
{"created":"2024-05-21 19:37:09","title":"Launching Your VR Neuroscience Laboratory","abstract":"The proliferation and refinement of affordable virtual reality (VR) technologies and wearable sensors have opened new frontiers in cognitive and behavioral neuroscience. This chapter offers a broad overview of VR for anyone interested in leveraging it as a research tool. In the first section, it examines the fundamental functionalities of VR and outlines important considerations that inform the development of immersive content that stimulates the senses. In the second section, the focus of the discussion shifts to the implementation of VR in the context of the neuroscience lab. Practical advice is offered on adapting commercial, off-theshelf devices to specific research purposes. Further, methods are explored for recording, synchronizing, and fusing heterogeneous forms of data obtained through the VR system or add-on sensors, as well as for labeling events and capturing game play.","sentences":["The proliferation and refinement of affordable virtual reality (VR) technologies and wearable sensors have opened new frontiers in cognitive and behavioral neuroscience.","This chapter offers a broad overview of VR for anyone interested in leveraging it as a research tool.","In the first section, it examines the fundamental functionalities of VR and outlines important considerations that inform the development of immersive content that stimulates the senses.","In the second section, the focus of the discussion shifts to the implementation of VR in the context of the neuroscience lab.","Practical advice is offered on adapting commercial, off-theshelf devices to specific research purposes.","Further, methods are explored for recording, synchronizing, and fusing heterogeneous forms of data obtained through the VR system or add-on sensors, as well as for labeling events and capturing game play."],"url":"http://arxiv.org/abs/2405.13171v1","category":"cs.HC"}
{"created":"2024-05-21 19:23:40","title":"FairLENS: Assessing Fairness in Law Enforcement Speech Recognition","abstract":"Automatic speech recognition (ASR) techniques have become powerful tools, enhancing efficiency in law enforcement scenarios. To ensure fairness for demographic groups in different acoustic environments, ASR engines must be tested across a variety of speakers in realistic settings. However, describing the fairness discrepancies between models with confidence remains a challenge. Meanwhile, most public ASR datasets are insufficient to perform a satisfying fairness evaluation. To address the limitations, we built FairLENS - a systematic fairness evaluation framework. We propose a novel and adaptable evaluation method to examine the fairness disparity between different models. We also collected a fairness evaluation dataset covering multiple scenarios and demographic dimensions. Leveraging this framework, we conducted fairness assessments on 1 open-source and 11 commercially available state-of-the-art ASR models. Our results reveal that certain models exhibit more biases than others, serving as a fairness guideline for users to make informed choices when selecting ASR models for a given real-world scenario. We further explored model biases towards specific demographic groups and observed that shifts in the acoustic domain can lead to the emergence of new biases.","sentences":["Automatic speech recognition (ASR) techniques have become powerful tools, enhancing efficiency in law enforcement scenarios.","To ensure fairness for demographic groups in different acoustic environments, ASR engines must be tested across a variety of speakers in realistic settings.","However, describing the fairness discrepancies between models with confidence remains a challenge.","Meanwhile, most public ASR datasets are insufficient to perform a satisfying fairness evaluation.","To address the limitations, we built FairLENS - a systematic fairness evaluation framework.","We propose a novel and adaptable evaluation method to examine the fairness disparity between different models.","We also collected a fairness evaluation dataset covering multiple scenarios and demographic dimensions.","Leveraging this framework, we conducted fairness assessments on 1 open-source and 11 commercially available state-of-the-art ASR models.","Our results reveal that certain models exhibit more biases than others, serving as a fairness guideline for users to make informed choices when selecting ASR models for a given real-world scenario.","We further explored model biases towards specific demographic groups and observed that shifts in the acoustic domain can lead to the emergence of new biases."],"url":"http://arxiv.org/abs/2405.13166v1","category":"eess.AS"}
{"created":"2024-05-21 19:21:51","title":"Adaptive coupling of 3D and 2D fluid flow models","abstract":"Similar to the notion of h-adaptivity, where the discretization resolution is adaptively changed, I propose the notion of model adaptivity, where the underlying model (the governing equations) is adaptively changed in space and time. Specifically, this work introduces a hybrid and adaptive coupling of a 3D bulk fluid flow model with a 2D thin film flow model. As a result, this work extends the applicability of existing thin film flow models to complex scenarios where, for example, bulk flow develops into thin films after striking a surface. At each location in space and time, the proposed framework automatically decides whether a 3D model or a 2D model must be applied. Using a meshless approach for both 3D and 2D models, at each particle, the decision to apply a 2D or 3D model is based on the user-prescribed resolution and a local principal component analysis. When a particle needs to be changed from a 3D model to 2D, or vice versa, the discretization is changed, and all relevant data mapping is done on-the-fly. Appropriate two-way coupling conditions and mass conservation considerations between the 3D and 2D models are also developed. Numerical results show that this model adaptive framework shows higher flexibility and compares well against finely resolved 3D simulations. In an actual application scenario, a 3 factor speed up is obtained, while maintaining the accuracy of the solution.","sentences":["Similar to the notion of h-adaptivity, where the discretization resolution is adaptively changed, I propose the notion of model adaptivity, where the underlying model (the governing equations) is adaptively changed in space and time.","Specifically, this work introduces a hybrid and adaptive coupling of a 3D bulk fluid flow model with a 2D thin film flow model.","As a result, this work extends the applicability of existing thin film flow models to complex scenarios where, for example, bulk flow develops into thin films after striking a surface.","At each location in space and time, the proposed framework automatically decides whether a 3D model or a 2D model must be applied.","Using a meshless approach for both 3D and 2D models, at each particle, the decision to apply a 2D or 3D model is based on the user-prescribed resolution and a local principal component analysis.","When a particle needs to be changed from a 3D model to 2D, or vice versa, the discretization is changed, and all relevant data mapping is done on-the-fly.","Appropriate two-way coupling conditions and mass conservation considerations between the 3D and 2D models are also developed.","Numerical results show that this model adaptive framework shows higher flexibility and compares well against finely resolved 3D simulations.","In an actual application scenario, a 3 factor speed up is obtained, while maintaining the accuracy of the solution."],"url":"http://arxiv.org/abs/2405.13165v1","category":"physics.flu-dyn"}
{"created":"2024-05-21 18:50:51","title":"ReALLM: A general framework for LLM compression and fine-tuning","abstract":"We introduce ReALLM, a novel approach for compression and memory-efficient adaptation of pre-trained language models that encompasses most of the post-training quantization and fine-tuning methods for a budget of <4 bits. Pre-trained matrices are decomposed into a high-precision low-rank component and a vector-quantized latent representation (using an autoencoder). During the fine-tuning step, only the low-rank components are updated. Our results show that pre-trained matrices exhibit different patterns. ReALLM adapts the shape of the encoder (small/large embedding, high/low bit VQ, etc.) to each matrix. ReALLM proposes to represent each matrix with a small embedding on $b$ bits and a neural decoder model $\\mathcal{D}_\\phi$ with its weights on $b_\\phi$ bits. The decompression of a matrix requires only one embedding and a single forward pass with the decoder. Our weight-only quantization algorithm yields the best results on language generation tasks (C4 and WikiText-2) for a budget of $3$ bits without any training. With a budget of $2$ bits, ReALLM achieves state-of-the art performance after fine-tuning on a small calibration dataset.","sentences":["We introduce ReALLM, a novel approach for compression and memory-efficient adaptation of pre-trained language models that encompasses most of the post-training quantization and fine-tuning methods for a budget of <4 bits.","Pre-trained matrices are decomposed into a high-precision low-rank component and a vector-quantized latent representation (using an autoencoder).","During the fine-tuning step, only the low-rank components are updated.","Our results show that pre-trained matrices exhibit different patterns.","ReALLM adapts the shape of the encoder (small/large embedding, high/low bit VQ, etc.) to each matrix.","ReALLM proposes to represent each matrix with a small embedding on $b$ bits and a neural decoder model $\\mathcal{D}_\\phi$ with its weights on $b_\\phi$ bits.","The decompression of a matrix requires only one embedding and a single forward pass with the decoder.","Our weight-only quantization algorithm yields the best results on language generation tasks (C4 and WikiText-2) for a budget of $3$ bits without any training.","With a budget of $2$ bits, ReALLM achieves state-of-the art performance after fine-tuning on a small calibration dataset."],"url":"http://arxiv.org/abs/2405.13155v1","category":"cs.LG"}
{"created":"2024-05-21 18:07:38","title":"Illustrating an Effective Workflow for Accelerated Materials Discovery","abstract":"Algorithmic materials discovery is a multi-disciplinary domain that integrates insights from specialists in alloy design, synthesis, characterization, experimental methodologies, computational modeling, and optimization. Central to this effort is a robust data management system paired with an interactive work platform. This platform should empower users to not only access others data but also integrate their analyses, paving the way for sophisticated data pipelines. To realize this vision, there is a need for an integrative collaboration platform, streamlined data sharing and analysis tools, and efficient communication channels. Such a collaborative mechanism should transcend geographical barriers, facilitating remote interaction and fostering a challenge-response dynamic. In this paper, we present our ongoing efforts in addressing the critical challenges related to an accelerated Materials Discovery Framework as a part of the High-Throughput Materials Discovery for Extreme Conditions Initiative. Our BIRDSHOT Center has successfully harnessed various tools and strategies, including the utilization of cloud-based storage, a standardized sample naming convention, a structured file system, the implementation of sample travelers, a robust sample tracking method, and the incorporation of knowledge graphs for efficient data management. Additionally, we present the development of a data collection platform, reinforcing seamless collaboration among our team members. In summary, this paper provides an illustration and insight into the various elements of an efficient and effective workflow within an accelerated materials discovery framework while highlighting the dynamic and adaptable nature of the data management tools and sharing platforms.","sentences":["Algorithmic materials discovery is a multi-disciplinary domain that integrates insights from specialists in alloy design, synthesis, characterization, experimental methodologies, computational modeling, and optimization.","Central to this effort is a robust data management system paired with an interactive work platform.","This platform should empower users to not only access others data but also integrate their analyses, paving the way for sophisticated data pipelines.","To realize this vision, there is a need for an integrative collaboration platform, streamlined data sharing and analysis tools, and efficient communication channels.","Such a collaborative mechanism should transcend geographical barriers, facilitating remote interaction and fostering a challenge-response dynamic.","In this paper, we present our ongoing efforts in addressing the critical challenges related to an accelerated Materials Discovery Framework as a part of the High-Throughput Materials Discovery for Extreme Conditions Initiative.","Our BIRDSHOT Center has successfully harnessed various tools and strategies, including the utilization of cloud-based storage, a standardized sample naming convention, a structured file system, the implementation of sample travelers, a robust sample tracking method, and the incorporation of knowledge graphs for efficient data management.","Additionally, we present the development of a data collection platform, reinforcing seamless collaboration among our team members.","In summary, this paper provides an illustration and insight into the various elements of an efficient and effective workflow within an accelerated materials discovery framework while highlighting the dynamic and adaptable nature of the data management tools and sharing platforms."],"url":"http://arxiv.org/abs/2405.13132v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-21 17:50:12","title":"Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control","abstract":"Current face reenactment and swapping methods mainly rely on GAN frameworks, but recent focus has shifted to pre-trained diffusion models for their superior generation capabilities. However, training these models is resource-intensive, and the results have not yet achieved satisfactory performance levels. To address this issue, we introduce Face-Adapter, an efficient and effective adapter designed for high-precision and high-fidelity face editing for pre-trained diffusion models. We observe that both face reenactment/swapping tasks essentially involve combinations of target structure, ID and attribute. We aim to sufficiently decouple the control of these factors to achieve both tasks in one model. Specifically, our method contains: 1) A Spatial Condition Generator that provides precise landmarks and background; 2) A Plug-and-play Identity Encoder that transfers face embeddings to the text space by a transformer decoder. 3) An Attribute Controller that integrates spatial conditions and detailed attributes. Face-Adapter achieves comparable or even superior performance in terms of motion control precision, ID retention capability, and generation quality compared to fully fine-tuned face reenactment/swapping models. Additionally, Face-Adapter seamlessly integrates with various StableDiffusion models.","sentences":["Current face reenactment and swapping methods mainly rely on GAN frameworks, but recent focus has shifted to pre-trained diffusion models for their superior generation capabilities.","However, training these models is resource-intensive, and the results have not yet achieved satisfactory performance levels.","To address this issue, we introduce Face-Adapter, an efficient and effective adapter designed for high-precision and high-fidelity face editing for pre-trained diffusion models.","We observe that both face reenactment/swapping tasks essentially involve combinations of target structure, ID and attribute.","We aim to sufficiently decouple the control of these factors to achieve both tasks in one model.","Specifically, our method contains: 1) A Spatial Condition Generator that provides precise landmarks and background; 2) A Plug-and-play Identity Encoder that transfers face embeddings to the text space by a transformer decoder.","3) An Attribute Controller that integrates spatial conditions and detailed attributes.","Face-Adapter achieves comparable or even superior performance in terms of motion control precision, ID retention capability, and generation quality compared to fully fine-tuned face reenactment/swapping models.","Additionally, Face-Adapter seamlessly integrates with various StableDiffusion models."],"url":"http://arxiv.org/abs/2405.12970v1","category":"cs.CV"}
{"created":"2024-05-21 17:32:04","title":"Soft Synergies: Model Order Reduction of Hybrid Soft-Rigid Robots via Optimal Strain Parameterization","abstract":"Soft robots offer remarkable adaptability and safety advantages over rigid robots, but modeling their complex, nonlinear dynamics remains challenging. Strain-based models have recently emerged as a promising candidate to describe such systems, however, they tend to be high-dimensional and time consuming. This paper presents a novel model order reduction approach for soft and hybrid robots by combining strain-based modeling with Proper Orthogonal Decomposition (POD). The method identifies optimal coupled strain basis functions -- or mechanical synergies -- from simulation data, enabling the description of soft robot configurations with a minimal number of generalized coordinates. The reduced order model (ROM) achieves substantial dimensionality reduction while preserving accuracy. Rigorous testing demonstrates the interpolation and extrapolation capabilities of the ROM for soft manipulators under static and dynamic conditions. The approach is further validated on a snake-like hyper-redundant rigid manipulator and a closed-chain system with soft and rigid components, illustrating its broad applicability. Finally, the approach is leveraged for shape estimation of a real six-actuator soft manipulator using only two position markers, showcasing its practical utility. This POD-based ROM offers significant computational speed-ups, paving the way for real-time simulation and control of complex soft and hybrid robots.","sentences":["Soft robots offer remarkable adaptability and safety advantages over rigid robots, but modeling their complex, nonlinear dynamics remains challenging.","Strain-based models have recently emerged as a promising candidate to describe such systems, however, they tend to be high-dimensional and time consuming.","This paper presents a novel model order reduction approach for soft and hybrid robots by combining strain-based modeling with Proper Orthogonal Decomposition (POD).","The method identifies optimal coupled strain basis functions -- or mechanical synergies -- from simulation data, enabling the description of soft robot configurations with a minimal number of generalized coordinates.","The reduced order model (ROM) achieves substantial dimensionality reduction while preserving accuracy.","Rigorous testing demonstrates the interpolation and extrapolation capabilities of the ROM for soft manipulators under static and dynamic conditions.","The approach is further validated on a snake-like hyper-redundant rigid manipulator and a closed-chain system with soft and rigid components, illustrating its broad applicability.","Finally, the approach is leveraged for shape estimation of a real six-actuator soft manipulator using only two position markers, showcasing its practical utility.","This POD-based ROM offers significant computational speed-ups, paving the way for real-time simulation and control of complex soft and hybrid robots."],"url":"http://arxiv.org/abs/2405.12959v1","category":"cs.RO"}
{"created":"2024-05-21 17:27:00","title":"Strategic Deployment of Honeypots in Blockchain-based IoT Systems","abstract":"This paper addresses the challenge of enhancing cybersecurity in Blockchain-based Internet of Things (BIoTs) systems, which are increasingly vulnerable to sophisticated cyberattacks. It introduces an AI-powered system model for the dynamic deployment of honeypots, utilizing an Intrusion Detection System (IDS) integrated with smart contract functionalities on IoT nodes. This model enables the transformation of regular nodes into decoys in response to suspicious activities, thereby strengthening the security of BIoT networks. The paper analyses strategic interactions between potential attackers and the AI-enhanced IDS through a game-theoretic model, specifically Bayesian games. The model focuses on understanding and predicting sophisticated attacks that may initially appear normal, emphasizing strategic decision-making, optimized honeypot deployment, and adaptive strategies in response to evolving attack patterns.","sentences":["This paper addresses the challenge of enhancing cybersecurity in Blockchain-based Internet of Things (BIoTs) systems, which are increasingly vulnerable to sophisticated cyberattacks.","It introduces an AI-powered system model for the dynamic deployment of honeypots, utilizing an Intrusion Detection System (IDS) integrated with smart contract functionalities on IoT nodes.","This model enables the transformation of regular nodes into decoys in response to suspicious activities, thereby strengthening the security of BIoT networks.","The paper analyses strategic interactions between potential attackers and the AI-enhanced IDS through a game-theoretic model, specifically Bayesian games.","The model focuses on understanding and predicting sophisticated attacks that may initially appear normal, emphasizing strategic decision-making, optimized honeypot deployment, and adaptive strategies in response to evolving attack patterns."],"url":"http://arxiv.org/abs/2405.12951v1","category":"cs.CR"}
{"created":"2024-05-21 17:21:28","title":"Adaptive Variant of Frank-Wolfe Method for Relative Smooth Convex Optimization Problems","abstract":"The paper introduces a new adaptive version of the Frank-Wolfe algorithm for relatively smooth convex functions. It is proposed to use the Bregman divergence other than half the square of the Euclidean norm in the formula for step-size. Algorithm convergence estimates for minimization problems of relatively smooth convex functions with the triangle scaling property are proved. Computational experiments are performed, and conditions are shown in which the obvious advantage of the proposed algorithm over its Euclidean norm analogue is shown. We also found examples of problems for which the proposed variation of the Frank-Wolfe method works better than known accelerated gradient-type methods for relatively smooth convex functions with the triangle scaling property.","sentences":["The paper introduces a new adaptive version of the Frank-Wolfe algorithm for relatively smooth convex functions.","It is proposed to use the Bregman divergence other than half the square of the Euclidean norm in the formula for step-size.","Algorithm convergence estimates for minimization problems of relatively smooth convex functions with the triangle scaling property are proved.","Computational experiments are performed, and conditions are shown in which the obvious advantage of the proposed algorithm over its Euclidean norm analogue is shown.","We also found examples of problems for which the proposed variation of the Frank-Wolfe method works better than known accelerated gradient-type methods for relatively smooth convex functions with the triangle scaling property."],"url":"http://arxiv.org/abs/2405.12948v1","category":"math.OC"}
{"created":"2024-05-21 17:17:17","title":"AMFD: Distillation via Adaptive Multimodal Fusion for Multispectral Pedestrian Detection","abstract":"Multispectral pedestrian detection has been shown to be effective in improving performance within complex illumination scenarios. However, prevalent double-stream networks in multispectral detection employ two separate feature extraction branches for multi-modal data, leading to nearly double the inference time compared to single-stream networks utilizing only one feature extraction branch. This increased inference time has hindered the widespread employment of multispectral pedestrian detection in embedded devices for autonomous systems. To address this limitation, various knowledge distillation methods have been proposed. However, traditional distillation methods focus only on the fusion features and ignore the large amount of information in the original multi-modal features, thereby restricting the student network's performance. To tackle the challenge, we introduce the Adaptive Modal Fusion Distillation (AMFD) framework, which can fully utilize the original modal features of the teacher network. Specifically, a Modal Extraction Alignment (MEA) module is utilized to derive learning weights for student networks, integrating focal and global attention mechanisms. This methodology enables the student network to acquire optimal fusion strategies independent from that of teacher network without necessitating an additional feature fusion module. Furthermore, we present the SMOD dataset, a well-aligned challenging multispectral dataset for detection. Extensive experiments on the challenging KAIST, LLVIP and SMOD datasets are conducted to validate the effectiveness of AMFD. The results demonstrate that our method outperforms existing state-of-the-art methods in both reducing log-average Miss Rate and improving mean Average Precision. The code is available at https://github.com/bigD233/AMFD.git.","sentences":["Multispectral pedestrian detection has been shown to be effective in improving performance within complex illumination scenarios.","However, prevalent double-stream networks in multispectral detection employ two separate feature extraction branches for multi-modal data, leading to nearly double the inference time compared to single-stream networks utilizing only one feature extraction branch.","This increased inference time has hindered the widespread employment of multispectral pedestrian detection in embedded devices for autonomous systems.","To address this limitation, various knowledge distillation methods have been proposed.","However, traditional distillation methods focus only on the fusion features and ignore the large amount of information in the original multi-modal features, thereby restricting the student network's performance.","To tackle the challenge, we introduce the Adaptive Modal Fusion Distillation (AMFD) framework, which can fully utilize the original modal features of the teacher network.","Specifically, a Modal Extraction Alignment (MEA) module is utilized to derive learning weights for student networks, integrating focal and global attention mechanisms.","This methodology enables the student network to acquire optimal fusion strategies independent from that of teacher network without necessitating an additional feature fusion module.","Furthermore, we present the SMOD dataset, a well-aligned challenging multispectral dataset for detection.","Extensive experiments on the challenging KAIST, LLVIP and SMOD datasets are conducted to validate the effectiveness of AMFD.","The results demonstrate that our method outperforms existing state-of-the-art methods in both reducing log-average Miss Rate and improving mean Average Precision.","The code is available at https://github.com/bigD233/AMFD.git."],"url":"http://arxiv.org/abs/2405.12944v1","category":"cs.CV"}
{"created":"2024-05-21 17:14:10","title":"Metacognitive particles, mental action and the sense of agency","abstract":"This paper articulates metacognition using the language of statistical physics and Bayesian mechanics. Metacognitive beliefs, defined as beliefs about beliefs, find a natural description within this formalism, which allows us to define the dynamics of 'metacognitive particles', i.e., systems possessing metacognitive beliefs. We further unpack this typology of metacognitive systems by distinguishing passive and active metacognitive particles, where active particles are endowed with the capacity for mental actions that update the parameters of other beliefs. We provide arguments for the necessity of this architecture in the emergence of a subjective sense of agency and the experience of being separate from the environment. The motivation is to pave the way towards a mathematical and physical understanding of cognition -- and higher forms thereof -- furthering the study and formalization of cognitive science in the language of mathematical physics.","sentences":["This paper articulates metacognition using the language of statistical physics and Bayesian mechanics.","Metacognitive beliefs, defined as beliefs about beliefs, find a natural description within this formalism, which allows us to define the dynamics of 'metacognitive particles', i.e., systems possessing metacognitive beliefs.","We further unpack this typology of metacognitive systems by distinguishing passive and active metacognitive particles, where active particles are endowed with the capacity for mental actions that update the parameters of other beliefs.","We provide arguments for the necessity of this architecture in the emergence of a subjective sense of agency and the experience of being separate from the environment.","The motivation is to pave the way towards a mathematical and physical understanding of cognition -- and higher forms thereof -- furthering the study and formalization of cognitive science in the language of mathematical physics."],"url":"http://arxiv.org/abs/2405.12941v1","category":"q-bio.NC"}
{"created":"2024-05-21 17:12:19","title":"Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models","abstract":"Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks. Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency. However, this approach fails in scenarios where the correct answers are in the minority. We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers. To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains. Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task. Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods. Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods.","sentences":["Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks.","Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency.","However, this approach fails in scenarios where the correct answers are in the minority.","We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers.","To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains.","Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task.","Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods.","Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods."],"url":"http://arxiv.org/abs/2405.12939v1","category":"cs.CL"}
{"created":"2024-05-21 17:06:06","title":"Circuit QED theory of direct and dual Shapiro steps with finite-size transmission line resonators","abstract":"We investigate the occurrence of direct and dual Shapiro steps for a Josephson junction coupled to a finite-size transmission line resonator. We treat both problems through a circuit QED approach with a large, but finite number of photon modes. For the dual case, we do not assume the (approximate) charge-phase duality, but include the full multi-band dynamics for the Josephson junction. Mean-field equations within such Hamiltonian approach reproduce the result obtained through a dissipative classical equation when the number of transmission line modes is large enough. To account for quantum and thermal fluctuations, we go beyond the mean-field treatment within a truncated Wigner approach. The fluctuations are shown to modify both the direct and the dual steps. We show how the dual steps are very sensitive to these fluctuations and identify the key physical parameters for the junction and the transmission line controlling their robustness, which is essential for applications to close the quantum metrological triangle.","sentences":["We investigate the occurrence of direct and dual Shapiro steps for a Josephson junction coupled to a finite-size transmission line resonator.","We treat both problems through a circuit QED approach with a large, but finite number of photon modes.","For the dual case, we do not assume the (approximate) charge-phase duality, but include the full multi-band dynamics for the Josephson junction.","Mean-field equations within such Hamiltonian approach reproduce the result obtained through a dissipative classical equation when the number of transmission line modes is large enough.","To account for quantum and thermal fluctuations, we go beyond the mean-field treatment within a truncated Wigner approach.","The fluctuations are shown to modify both the direct and the dual steps.","We show how the dual steps are very sensitive to these fluctuations and identify the key physical parameters for the junction and the transmission line controlling their robustness, which is essential for applications to close the quantum metrological triangle."],"url":"http://arxiv.org/abs/2405.12935v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-21 16:35:02","title":"An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation","abstract":"One critical prerequisite for faithful text-to-image generation is the accurate understanding of text inputs. Existing methods leverage the text encoder of the CLIP model to represent input prompts. However, the pre-trained CLIP model can merely encode English with a maximum token length of 77. Moreover, the model capacity of the text encoder from CLIP is relatively limited compared to Large Language Models (LLMs), which offer multilingual input, accommodate longer context, and achieve superior text representation. In this paper, we investigate LLMs as the text encoder to improve the language understanding in text-to-image generation. Unfortunately, training text-to-image generative model with LLMs from scratch demands significant computational resources and data. To this end, we introduce a three-stage training pipeline that effectively and efficiently integrates the existing text-to-image model with LLMs. Specifically, we propose a lightweight adapter that enables fast training of the text-to-image model using the textual representations from LLMs. Extensive experiments demonstrate that our model supports not only multilingual but also longer input context with superior image generation quality.","sentences":["One critical prerequisite for faithful text-to-image generation is the accurate understanding of text inputs.","Existing methods leverage the text encoder of the CLIP model to represent input prompts.","However, the pre-trained CLIP model can merely encode English with a maximum token length of 77.","Moreover, the model capacity of the text encoder from CLIP is relatively limited compared to Large Language Models (LLMs), which offer multilingual input, accommodate longer context, and achieve superior text representation.","In this paper, we investigate LLMs as the text encoder to improve the language understanding in text-to-image generation.","Unfortunately, training text-to-image generative model with LLMs from scratch demands significant computational resources and data.","To this end, we introduce a three-stage training pipeline that effectively and efficiently integrates the existing text-to-image model with LLMs.","Specifically, we propose a lightweight adapter that enables fast training of the text-to-image model using the textual representations from LLMs.","Extensive experiments demonstrate that our model supports not only multilingual but also longer input context with superior image generation quality."],"url":"http://arxiv.org/abs/2405.12914v1","category":"cs.CV"}
{"created":"2024-05-21 16:14:55","title":"Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents","abstract":"Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model's resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.","sentences":["Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies.","Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience.","In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO).","The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token.","We demonstrate that ADPO enhances the model's resilience against harmful conversations while minimizing performance degradation.","Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO.","To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data."],"url":"http://arxiv.org/abs/2405.12900v1","category":"cs.CL"}
{"created":"2024-05-21 16:04:32","title":"Decentralized Federated Learning Over Imperfect Communication Channels","abstract":"This paper analyzes the impact of imperfect communication channels on decentralized federated learning (D-FL) and subsequently determines the optimal number of local aggregations per training round, adapting to the network topology and imperfect channels. We start by deriving the bias of locally aggregated D-FL models under imperfect channels from the ideal global models requiring perfect channels and aggregations. The bias reveals that excessive local aggregations can accumulate communication errors and degrade convergence. Another important aspect is that we analyze a convergence upper bound of D-FL based on the bias. By minimizing the bound, the optimal number of local aggregations is identified to balance a trade-off with accumulation of communication errors in the absence of knowledge of the channels. With this knowledge, the impact of communication errors can be alleviated, allowing the convergence upper bound to decrease throughout aggregations. Experiments validate our convergence analysis and also identify the optimal number of local aggregations on two widely considered image classification tasks. It is seen that D-FL, with an optimal number of local aggregations, can outperform its potential alternatives by over 10% in training accuracy.","sentences":["This paper analyzes the impact of imperfect communication channels on decentralized federated learning (D-FL) and subsequently determines the optimal number of local aggregations per training round, adapting to the network topology and imperfect channels.","We start by deriving the bias of locally aggregated D-FL models under imperfect channels from the ideal global models requiring perfect channels and aggregations.","The bias reveals that excessive local aggregations can accumulate communication errors and degrade convergence.","Another important aspect is that we analyze a convergence upper bound of D-FL based on the bias.","By minimizing the bound, the optimal number of local aggregations is identified to balance a trade-off with accumulation of communication errors in the absence of knowledge of the channels.","With this knowledge, the impact of communication errors can be alleviated, allowing the convergence upper bound to decrease throughout aggregations.","Experiments validate our convergence analysis and also identify the optimal number of local aggregations on two widely considered image classification tasks.","It is seen that D-FL, with an optimal number of local aggregations, can outperform its potential alternatives by over 10% in training accuracy."],"url":"http://arxiv.org/abs/2405.12894v1","category":"cs.DC"}
{"created":"2024-05-21 16:04:25","title":"Better Simulations for Validating Causal Discovery with the DAG-Adaptation of the Onion Method","abstract":"The number of artificial intelligence algorithms for learning causal models from data is growing rapidly. Most ``causal discovery'' or ``causal structure learning'' algorithms are primarily validated through simulation studies. However, no widely accepted simulation standards exist and publications often report conflicting performance statistics -- even when only considering publications that simulate data from linear models. In response, several manuscripts have criticized a popular simulation design for validating algorithms in the linear case.   We propose a new simulation design for generating linear models for directed acyclic graphs (DAGs): the DAG-adaptation of the Onion (DaO) method. DaO simulations are fundamentally different from existing simulations because they prioritize the distribution of correlation matrices rather than the distribution of linear effects. Specifically, the DaO method uniformly samples the space of all correlation matrices consistent with (i.e. Markov to) a DAG. We also discuss how to sample DAGs and present methods for generating DAGs with scale-free in-degree or out-degree. We compare the DaO method against two alternative simulation designs and provide implementations of the DaO method in Python and R: https://github.com/bja43/DaO_simulation. We advocate for others to adopt DaO simulations as a fair universal benchmark.","sentences":["The number of artificial intelligence algorithms for learning causal models from data is growing rapidly.","Most ``causal discovery'' or ``causal structure learning'' algorithms are primarily validated through simulation studies.","However, no widely accepted simulation standards exist and publications often report conflicting performance statistics -- even when only considering publications that simulate data from linear models.","In response, several manuscripts have criticized a popular simulation design for validating algorithms in the linear case.   ","We propose a new simulation design for generating linear models for directed acyclic graphs (DAGs): the DAG-adaptation of the Onion (DaO) method.","DaO simulations are fundamentally different from existing simulations because they prioritize the distribution of correlation matrices rather than the distribution of linear effects.","Specifically, the DaO method uniformly samples the space of all correlation matrices consistent with (i.e. Markov to) a DAG.","We also discuss how to sample DAGs and present methods for generating DAGs with scale-free in-degree or out-degree.","We compare the DaO method against two alternative simulation designs and provide implementations of the DaO method in Python and R: https://github.com/bja43/DaO_simulation.","We advocate for others to adopt DaO simulations as a fair universal benchmark."],"url":"http://arxiv.org/abs/2405.13100v1","category":"stat.ME"}
{"created":"2024-05-21 15:46:13","title":"Approximating TSP Variants Using a Bridge Lemma","abstract":"We give improved approximations for two metric \\textsc{Traveling Salesman Problem} (TSP) variants. In \\textsc{Ordered TSP} (OTSP) we are given a linear ordering on a subset of nodes $o_1, \\ldots, o_k$. The TSP solution must have that $o_{i+1}$ is visited at some point after $o_i$ for each $1 \\leq i < k$. This is the special case of \\textsc{Precedence-Constrained TSP} ($PTSP$) in which the precedence constraints are given by a single chain on a subset of nodes. In \\textsc{$k$-Person TSP Path} (k-TSPP), we are given pairs of nodes $(s_1,t_1), \\ldots, (s_k,t_k)$. The goal is to find an $s_i$-$t_i$ path with minimum total cost such that every node is visited by at least one path.   We obtain a $3/2 + e^{-1} < 1.878$ approximation for OTSP, the first improvement over a trivial $\\alpha+1$ approximation where $\\alpha$ is the current best TSP approximation. We also obtain a $1 + 2 \\cdot e^{-1/2} < 2.214$ approximation for k-TSPP, the first improvement over a trivial $3$-approximation.   These algorithms both use an adaptation of the Bridge Lemma that was initially used to obtain improved \\textsc{Steiner Tree} approximations [Byrka et al., 2013]. Roughly speaking, our variant states that the cost of a cheapest forest rooted at a given set of terminal nodes will decrease by a substantial amount if we randomly sample a set of non-terminal nodes to also become terminals such provided each non-terminal has a constant probability of being sampled. We believe this view of the Bridge Lemma will find further use for improved vehicle routing approximations beyond this paper.","sentences":["We give improved approximations for two metric \\textsc{Traveling Salesman Problem} (TSP) variants.","In \\textsc{Ordered TSP} (OTSP) we are given a linear ordering on a subset of nodes $o_1, \\ldots, o_k$.","The TSP solution must have that $o_{i+1}$ is visited at some point after $o_i$ for each $1 \\leq","i < k$.","This is the special case of \\textsc{Precedence-Constrained TSP} ($PTSP$) in which the precedence constraints are given by a single chain on a subset of nodes.","In \\textsc{$k$-Person TSP Path} (k-TSPP), we are given pairs of nodes $(s_1,t_1), \\ldots, (s_k,t_k)$. The goal is to find an $s_i$-$t_i$ path with minimum total cost such that every node is visited by at least one path.   ","We obtain a $3/2 + e^{-1} < 1.878$ approximation for OTSP, the first improvement over a trivial $\\alpha+1$ approximation where $\\alpha$ is the current best TSP approximation.","We also obtain a $1 + 2 \\cdot e^{-1/2} < 2.214$ approximation for k-TSPP, the first improvement over a trivial $3$-approximation.   ","These algorithms both use an adaptation of the Bridge Lemma that was initially used to obtain improved \\textsc{Steiner","Tree} approximations [Byrka et al., 2013].","Roughly speaking, our variant states that the cost of a cheapest forest rooted at a given set of terminal nodes will decrease by a substantial amount if we randomly sample a set of non-terminal nodes to also become terminals such provided each non-terminal has a constant probability of being sampled.","We believe this view of the Bridge Lemma will find further use for improved vehicle routing approximations beyond this paper."],"url":"http://arxiv.org/abs/2405.12876v1","category":"cs.DS"}
{"created":"2024-05-21 15:43:25","title":"Out of equilibrium response and fluctuation-dissipation violations across scales in flocking systems","abstract":"Flocking systems are known to be strongly out of equilibrium. Energy input occurs at the individual level to ensure self-propulsion, and the individual motility in turn contributes to ordering, enhancing information propagation and strengthening collective motion. However, even beyond ordering, a crucial feature of natural aggregations is response. How, then, do off-equilibrium features affect the response of the system? In this work, we consider a minimal model of flocking and investigate response behavior under directional perturbations. We show that equilibrium dynamical fluctuation-dissipation relations between response and correlations are violated, both at the local and at the global level. The amount of violation peaks at the ordering transition, exactly as for the entropy production rate. Entropy is always produced locally and connected to the local fluctuation-dissipation violation via Harada-Sasa relationships. However, cooperative mechanisms close to the transition spread off-equilibrium effects to the whole system, producing an out of equilibrium response on the global scale. Our findings elucidate the role of activity and interactions in the cost repartition of collective behavior and explain what observed in experiments on natural living groups.","sentences":["Flocking systems are known to be strongly out of equilibrium.","Energy input occurs at the individual level to ensure self-propulsion, and the individual motility in turn contributes to ordering, enhancing information propagation and strengthening collective motion.","However, even beyond ordering, a crucial feature of natural aggregations is response.","How, then, do off-equilibrium features affect the response of the system?","In this work, we consider a minimal model of flocking and investigate response behavior under directional perturbations.","We show that equilibrium dynamical fluctuation-dissipation relations between response and correlations are violated, both at the local and at the global level.","The amount of violation peaks at the ordering transition, exactly as for the entropy production rate.","Entropy is always produced locally and connected to the local fluctuation-dissipation violation via Harada-Sasa relationships.","However, cooperative mechanisms close to the transition spread off-equilibrium effects to the whole system, producing an out of equilibrium response on the global scale.","Our findings elucidate the role of activity and interactions in the cost repartition of collective behavior and explain what observed in experiments on natural living groups."],"url":"http://arxiv.org/abs/2405.12874v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-21 15:11:35","title":"Inconsistency-Aware Cross-Attention for Audio-Visual Fusion in Dimensional Emotion Recognition","abstract":"Leveraging complementary relationships across modalities has recently drawn a lot of attention in multimodal emotion recognition. Most of the existing approaches explored cross-attention to capture the complementary relationships across the modalities. However, the modalities may also exhibit weak complementary relationships, which may deteriorate the cross-attended features, resulting in poor multimodal feature representations. To address this problem, we propose Inconsistency-Aware Cross-Attention (IACA), which can adaptively select the most relevant features on-the-fly based on the strong or weak complementary relationships across audio and visual modalities. Specifically, we design a two-stage gating mechanism that can adaptively select the appropriate relevant features to deal with weak complementary relationships. Extensive experiments are conducted on the challenging Aff-Wild2 dataset to show the robustness of the proposed model.","sentences":["Leveraging complementary relationships across modalities has recently drawn a lot of attention in multimodal emotion recognition.","Most of the existing approaches explored cross-attention to capture the complementary relationships across the modalities.","However, the modalities may also exhibit weak complementary relationships, which may deteriorate the cross-attended features, resulting in poor multimodal feature representations.","To address this problem, we propose Inconsistency-Aware Cross-Attention (IACA), which can adaptively select the most relevant features on-the-fly based on the strong or weak complementary relationships across audio and visual modalities.","Specifically, we design a two-stage gating mechanism that can adaptively select the appropriate relevant features to deal with weak complementary relationships.","Extensive experiments are conducted on the challenging Aff-Wild2 dataset to show the robustness of the proposed model."],"url":"http://arxiv.org/abs/2405.12853v1","category":"cs.CV"}
{"created":"2024-05-21 14:59:39","title":"Training and inference in the ReckON RSNN architecture implemented on a MPSoC","abstract":"With the rise of artificial intelligence, biological neuron models are being used to implement neural networks that can learn certain tasks after a training phase. One type of such networks are spiking neural networks (SNNs) that rely on a simplified model for biological neurons, the Integrate and Fire neuron. Several accelerators have emerged to implement SNNs with this kind of neuron. The ReckON system is one of these that allows both the training and execution of a recurrent SNN. The ReckON architecture, implemented on a custom ASIC, can be fully described using a hardware description language. In this work, we adapt the Verilog description to implement it on a Xilinx Multiprocessor System on Chip system (MPSoC). We present the circuits required for the efficient operation of the system, and a Python framework to use it on the Pynq ZU platform. We validate the architecture and implementation in two different scenarios, and show how the simulated accuracy is preserved with a peak performance of 3.8M events processed per second.","sentences":["With the rise of artificial intelligence, biological neuron models are being used to implement neural networks that can learn certain tasks after a training phase.","One type of such networks are spiking neural networks (SNNs) that rely on a simplified model for biological neurons, the Integrate and Fire neuron.","Several accelerators have emerged to implement SNNs with this kind of neuron.","The ReckON system is one of these that allows both the training and execution of a recurrent SNN.","The ReckON architecture, implemented on a custom ASIC, can be fully described using a hardware description language.","In this work, we adapt the Verilog description to implement it on a Xilinx Multiprocessor System on Chip system (MPSoC).","We present the circuits required for the efficient operation of the system, and a Python framework to use it on the Pynq ZU platform.","We validate the architecture and implementation in two different scenarios, and show how the simulated accuracy is preserved with a peak performance of 3.8M events processed per second."],"url":"http://arxiv.org/abs/2405.12849v1","category":"cs.AR"}
{"created":"2024-05-21 14:49:12","title":"SmartFlow: Robotic Process Automation using LLMs","abstract":"Robotic Process Automation (RPA) systems face challenges in handling complex processes and diverse screen layouts that require advanced human-like decision-making capabilities. These systems typically rely on pixel-level encoding through drag-and-drop or automation frameworks such as Selenium to create navigation workflows, rather than visual understanding of screen elements. In this context, we present SmartFlow, an AI-based RPA system that uses pre-trained large language models (LLMs) coupled with deep-learning based image understanding. Our system can adapt to new scenarios, including changes in the user interface and variations in input data, without the need for human intervention. SmartFlow uses computer vision and natural language processing to perceive visible elements on the graphical user interface (GUI) and convert them into a textual representation. This information is then utilized by LLMs to generate a sequence of actions that are executed by a scripting engine to complete an assigned task. To assess the effectiveness of SmartFlow, we have developed a dataset that includes a set of generic enterprise applications with diverse layouts, which we are releasing for research use. Our evaluations on this dataset demonstrate that SmartFlow exhibits robustness across different layouts and applications. SmartFlow can automate a wide range of business processes such as form filling, customer service, invoice processing, and back-office operations. SmartFlow can thus assist organizations in enhancing productivity by automating an even larger fraction of screen-based workflows. The demo-video and dataset are available at https://smartflow-4c5a0a.webflow.io/.","sentences":["Robotic Process Automation (RPA) systems face challenges in handling complex processes and diverse screen layouts that require advanced human-like decision-making capabilities.","These systems typically rely on pixel-level encoding through drag-and-drop or automation frameworks such as Selenium to create navigation workflows, rather than visual understanding of screen elements.","In this context, we present SmartFlow, an AI-based RPA system that uses pre-trained large language models (LLMs) coupled with deep-learning based image understanding.","Our system can adapt to new scenarios, including changes in the user interface and variations in input data, without the need for human intervention.","SmartFlow uses computer vision and natural language processing to perceive visible elements on the graphical user interface (GUI) and convert them into a textual representation.","This information is then utilized by LLMs to generate a sequence of actions that are executed by a scripting engine to complete an assigned task.","To assess the effectiveness of SmartFlow, we have developed a dataset that includes a set of generic enterprise applications with diverse layouts, which we are releasing for research use.","Our evaluations on this dataset demonstrate that SmartFlow exhibits robustness across different layouts and applications.","SmartFlow can automate a wide range of business processes such as form filling, customer service, invoice processing, and back-office operations.","SmartFlow can thus assist organizations in enhancing productivity by automating an even larger fraction of screen-based workflows.","The demo-video and dataset are available at https://smartflow-4c5a0a.webflow.io/."],"url":"http://arxiv.org/abs/2405.12842v1","category":"cs.RO"}
{"created":"2024-05-21 14:36:16","title":"Wav-KAN: Wavelet Kolmogorov-Arnold Networks","abstract":"In this paper , we introduce Wav-KAN, an innovative neural network architecture that leverages the Wavelet Kolmogorov-Arnold Networks (Wav-KAN) framework to enhance interpretability and performance. Traditional multilayer perceptrons (MLPs) and even recent advancements like Spl-KAN face challenges related to interpretability, training speed, robustness, computational efficiency, and performance. Wav-KAN addresses these limitations by incorporating wavelet functions into the Kolmogorov-Arnold network structure, enabling the network to capture both high-frequency and low-frequency components of the input data efficiently. Wavelet-based approximations employ orthogonal or semi-orthogonal basis and also maintains a balance between accurately representing the underlying data structure and avoiding overfitting to the noise. Analogous to how water conforms to the shape of its container, Wav-KAN adapts to the data structure, resulting in enhanced accuracy, faster training speeds, and increased robustness compared to Spl-KAN and MLPs. Our results highlight the potential of Wav-KAN as a powerful tool for developing interpretable and high-performance neural networks, with applications spanning various fields. This work sets the stage for further exploration and implementation of Wav-KAN in frameworks such as PyTorch, TensorFlow, and also it makes wavelet in KAN in wide-spread usage like nowadays activation functions like ReLU, sigmoid in universal approximation theory (UAT).","sentences":["In this paper , we introduce Wav-KAN, an innovative neural network architecture that leverages the Wavelet Kolmogorov-Arnold Networks (Wav-KAN) framework to enhance interpretability and performance.","Traditional multilayer perceptrons (MLPs) and even recent advancements like Spl-KAN face challenges related to interpretability, training speed, robustness, computational efficiency, and performance.","Wav-KAN addresses these limitations by incorporating wavelet functions into the Kolmogorov-Arnold network structure, enabling the network to capture both high-frequency and low-frequency components of the input data efficiently.","Wavelet-based approximations employ orthogonal or semi-orthogonal basis and also maintains a balance between accurately representing the underlying data structure and avoiding overfitting to the noise.","Analogous to how water conforms to the shape of its container, Wav-KAN adapts to the data structure, resulting in enhanced accuracy, faster training speeds, and increased robustness compared to Spl-KAN and MLPs.","Our results highlight the potential of Wav-KAN as a powerful tool for developing interpretable and high-performance neural networks, with applications spanning various fields.","This work sets the stage for further exploration and implementation of Wav-KAN in frameworks such as PyTorch, TensorFlow, and also it makes wavelet in KAN in wide-spread usage like nowadays activation functions like ReLU, sigmoid in universal approximation theory (UAT)."],"url":"http://arxiv.org/abs/2405.12832v1","category":"cs.LG"}
{"created":"2024-05-21 14:04:56","title":"Optimizing Coded-Apertures for Depth-Resolved Diffraction","abstract":"Coded apertures, traditionally employed in x-ray astronomy for imaging celestial objects, are now being adapted for micro-scale applications, particularly in studying microscopic specimens with synchrotron light diffraction. In this paper, we focus on micro-coded aperture imaging and its capacity to accomplish depth-resolved micro-diffraction analysis within crystalline specimens. We study aperture specifications and scanning parameters by assessing characteristics like size, thickness, and patterns. Numerical experiments assist in assessing their impact on reconstruction quality. Empirical data from a Laue diffraction microscope at a synchrotron undulator beamline supports our findings. Overall, our results offer key insights for optimizing aperture design in advancing micro-scale diffraction imaging at synchrotrons. This study contributes insights to this expanding field and suggests significant advancements, especially when coupled with the enhanced flux anticipated from the global upgrades of synchrotron sources.","sentences":["Coded apertures, traditionally employed in x-ray astronomy for imaging celestial objects, are now being adapted for micro-scale applications, particularly in studying microscopic specimens with synchrotron light diffraction.","In this paper, we focus on micro-coded aperture imaging and its capacity to accomplish depth-resolved micro-diffraction analysis within crystalline specimens.","We study aperture specifications and scanning parameters by assessing characteristics like size, thickness, and patterns.","Numerical experiments assist in assessing their impact on reconstruction quality.","Empirical data from a Laue diffraction microscope at a synchrotron undulator beamline supports our findings.","Overall, our results offer key insights for optimizing aperture design in advancing micro-scale diffraction imaging at synchrotrons.","This study contributes insights to this expanding field and suggests significant advancements, especially when coupled with the enhanced flux anticipated from the global upgrades of synchrotron sources."],"url":"http://arxiv.org/abs/2405.12813v1","category":"eess.SP"}
{"created":"2024-05-21 13:58:17","title":"FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information","abstract":"This paper establishes a mathematical foundation for the Adam optimizer, elucidating its connection to natural gradient descent through Riemannian and information geometry. We rigorously analyze the diagonal empirical Fisher information matrix (FIM) in Adam, clarifying all detailed approximations and advocating for the use of log probability functions as loss, which should be based on discrete distributions, due to the limitations of empirical FIM. Our analysis uncovers flaws in the original Adam algorithm, leading to proposed corrections such as enhanced momentum calculations, adjusted bias corrections, adaptive epsilon, and gradient clipping. We refine the weight decay term based on our theoretical framework. Our modified algorithm, Fisher Adam (FAdam), demonstrates superior performance across diverse domains including LLM, ASR, and VQ-VAE, achieving state-of-the-art results in ASR.","sentences":["This paper establishes a mathematical foundation for the Adam optimizer, elucidating its connection to natural gradient descent through Riemannian and information geometry.","We rigorously analyze the diagonal empirical Fisher information matrix (FIM) in Adam, clarifying all detailed approximations and advocating for the use of log probability functions as loss, which should be based on discrete distributions, due to the limitations of empirical FIM.","Our analysis uncovers flaws in the original Adam algorithm, leading to proposed corrections such as enhanced momentum calculations, adjusted bias corrections, adaptive epsilon, and gradient clipping.","We refine the weight decay term based on our theoretical framework.","Our modified algorithm, Fisher Adam (FAdam), demonstrates superior performance across diverse domains including LLM, ASR, and VQ-VAE, achieving state-of-the-art results in ASR."],"url":"http://arxiv.org/abs/2405.12807v2","category":"cs.LG"}
{"created":"2024-05-21 13:42:35","title":"Adaptive local boundary conditions to improve Deformable Image Registration","abstract":"Objective: In medical imaging, it is often crucial to accurately assess and correct movement during image-guided therapy. Deformable image registration (DIR) consists in estimating the required spatial transformation to align a moving image with a fixed one. However, it is acknowledged that, boundary conditions applied to the solution are critical in preventing mis-registration. Despite the extensive research on registration techniques, relatively few have addressed the issue of boundary conditions in the context of medical DIR. Our aim is a step towards customizing boundary conditions to suit the diverse registration tasks at hand.   Approach: We propose a generic, locally adaptive, Robin-type condition enabling to balance between Dirichlet and Neumann boundary conditions, depending on incoming/outgoing flow fields on the image boundaries. The proposed framework is entirely automatized through the determination of a reduced set of hyperparameters optimized via energy minimization.   Main results: The proposed approach was tested on a mono-modal CT thorax registration task and an abdominal CT to MRI registration task. For the first task, we observed a relative improvement in terms of target registration error of up to 12% (mean 4%), compared to homogeneous Dirichlet and homogeneous Neumann. For the second task, the automatic framework provides results closed to the best achievable.   Significance: This study underscores the importance of tailoring the registration problem at the image boundaries. In this research, we introduce a novel method to adapt the boundary conditions on a voxel-by-voxel basis, yielding optimized results in two distinct tasks: mono-modal CT thorax registration and abdominal CT to MRI registration. The proposed framework enables optimized boundary conditions in image registration without any a priori assumptions regarding the images or the motion.","sentences":["Objective: In medical imaging, it is often crucial to accurately assess and correct movement during image-guided therapy.","Deformable image registration (DIR) consists in estimating the required spatial transformation to align a moving image with a fixed one.","However, it is acknowledged that, boundary conditions applied to the solution are critical in preventing mis-registration.","Despite the extensive research on registration techniques, relatively few have addressed the issue of boundary conditions in the context of medical DIR.","Our aim is a step towards customizing boundary conditions to suit the diverse registration tasks at hand.   ","Approach:","We propose a generic, locally adaptive, Robin-type condition enabling to balance between Dirichlet and Neumann boundary conditions, depending on incoming/outgoing flow fields on the image boundaries.","The proposed framework is entirely automatized through the determination of a reduced set of hyperparameters optimized via energy minimization.   ","Main results: The proposed approach was tested on a mono-modal CT thorax registration task and an abdominal CT to MRI registration task.","For the first task, we observed a relative improvement in terms of target registration error of up to 12% (mean 4%), compared to homogeneous Dirichlet and homogeneous Neumann.","For the second task, the automatic framework provides results closed to the best achievable.   ","Significance: This study underscores the importance of tailoring the registration problem at the image boundaries.","In this research, we introduce a novel method to adapt the boundary conditions on a voxel-by-voxel basis, yielding optimized results in two distinct tasks: mono-modal CT thorax registration and abdominal CT to MRI registration.","The proposed framework enables optimized boundary conditions in image registration without any a priori assumptions regarding the images or the motion."],"url":"http://arxiv.org/abs/2405.12791v1","category":"cs.CV"}
{"created":"2024-05-21 13:35:46","title":"Enhanced Dissipation via the Malliavin Calculus","abstract":"In this work we investigate the phenomenon of enhanced dissipation using techniques from the Malliavin Calculus. In particular, we construct a concise, elementary argument, that allows us to recover the well-known enhanced dissipation timescale for shear flows, first obtained by Bedrossian and Coti Zelati in 2017, as well as the precise hypoelliptic regularisation in x.","sentences":["In this work we investigate the phenomenon of enhanced dissipation using techniques from the Malliavin Calculus.","In particular, we construct a concise, elementary argument, that allows us to recover the well-known enhanced dissipation timescale for shear flows, first obtained by Bedrossian and Coti Zelati in 2017, as well as the precise hypoelliptic regularisation in x."],"url":"http://arxiv.org/abs/2405.12787v1","category":"math.AP"}
{"created":"2024-05-21 13:28:32","title":"Self-Supervised Modality-Agnostic Pre-Training of Swin Transformers","abstract":"Unsupervised pre-training has emerged as a transformative paradigm, displaying remarkable advancements in various domains. However, the susceptibility to domain shift, where pre-training data distribution differs from fine-tuning, poses a significant obstacle. To address this, we augment the Swin Transformer to learn from different medical imaging modalities, enhancing downstream performance. Our model, dubbed SwinFUSE (Swin Multi-Modal Fusion for UnSupervised Enhancement), offers three key advantages: (i) it learns from both Computed Tomography (CT) and Magnetic Resonance Images (MRI) during pre-training, resulting in complementary feature representations; (ii) a domain-invariance module (DIM) that effectively highlights salient input regions, enhancing adaptability; (iii) exhibits remarkable generalizability, surpassing the confines of tasks it was initially pre-trained on. Our experiments on two publicly available 3D segmentation datasets show a modest 1-2% performance trade-off compared to single-modality models, yet significant out-performance of up to 27% on out-of-distribution modality. This substantial improvement underscores our proposed approach's practical relevance and real-world applicability. Code is available at: https://github.com/devalab/SwinFUSE","sentences":["Unsupervised pre-training has emerged as a transformative paradigm, displaying remarkable advancements in various domains.","However, the susceptibility to domain shift, where pre-training data distribution differs from fine-tuning, poses a significant obstacle.","To address this, we augment the Swin Transformer to learn from different medical imaging modalities, enhancing downstream performance.","Our model, dubbed SwinFUSE (Swin Multi-Modal Fusion for UnSupervised Enhancement), offers three key advantages: (i) it learns from both Computed Tomography (CT) and Magnetic Resonance Images (MRI) during pre-training, resulting in complementary feature representations; (ii) a domain-invariance module (DIM) that effectively highlights salient input regions, enhancing adaptability; (iii) exhibits remarkable generalizability, surpassing the confines of tasks it was initially pre-trained on.","Our experiments on two publicly available 3D segmentation datasets show a modest 1-2% performance trade-off compared to single-modality models, yet significant out-performance of up to 27% on out-of-distribution modality.","This substantial improvement underscores our proposed approach's practical relevance and real-world applicability.","Code is available at: https://github.com/devalab/SwinFUSE"],"url":"http://arxiv.org/abs/2405.12781v1","category":"cs.CV"}
{"created":"2024-05-21 13:02:27","title":"Generative AI and Large Language Models for Cyber Security: All Insights You Need","abstract":"This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.","sentences":["This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs).","We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection.","We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA.","Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions.","We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques.","Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses.","We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research.","In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG).","These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response.","Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats."],"url":"http://arxiv.org/abs/2405.12750v1","category":"cs.CR"}
{"created":"2024-05-21 12:57:10","title":"Graph neural networks informed locally by thermodynamics","abstract":"Thermodynamics-informed neural networks employ inductive biases for the enforcement of the first and second principles of thermodynamics. To construct these biases, a metriplectic evolution of the system is assumed. This provides excellent results, when compared to uninformed, black box networks. While the degree of accuracy can be increased in one or two orders of magnitude, in the case of graph networks, this requires assembling global Poisson and dissipation matrices, which breaks the local structure of such networks. In order to avoid this drawback, a local version of the metriplectic biases has been developed in this work, which avoids the aforementioned matrix assembly, thus preserving the node-by-node structure of the graph networks. We apply this framework for examples in the fields of solid and fluid mechanics. Our approach demonstrates significant computational efficiency and strong generalization capabilities, accurately making inferences on examples significantly different from those encountered during training.","sentences":["Thermodynamics-informed neural networks employ inductive biases for the enforcement of the first and second principles of thermodynamics.","To construct these biases, a metriplectic evolution of the system is assumed.","This provides excellent results, when compared to uninformed, black box networks.","While the degree of accuracy can be increased in one or two orders of magnitude, in the case of graph networks, this requires assembling global Poisson and dissipation matrices, which breaks the local structure of such networks.","In order to avoid this drawback, a local version of the metriplectic biases has been developed in this work, which avoids the aforementioned matrix assembly, thus preserving the node-by-node structure of the graph networks.","We apply this framework for examples in the fields of solid and fluid mechanics.","Our approach demonstrates significant computational efficiency and strong generalization capabilities, accurately making inferences on examples significantly different from those encountered during training."],"url":"http://arxiv.org/abs/2405.13093v1","category":"cs.LG"}
{"created":"2024-05-21 12:36:53","title":"Learning tensor trains from noisy functions with application to quantum simulation","abstract":"Tensor cross interpolation (TCI) is a powerful technique for learning a tensor train (TT) by adaptively sampling a target tensor based on an interpolation formula. However, when the tensor evaluations contain random noise, optimizing the TT is more advantageous than interpolating the noise. Here, we propose a new method that starts with an initial guess of TT and optimizes it using non-linear least-squares by fitting it to measured points obtained from TCI. We use quantics TCI (QTCI) in this method and demonstrate its effectiveness on sine and two-time correlation functions, with each evaluated with random noise. The resulting TT exhibits increased robustness against noise compared to the QTCI method. Furthermore, we employ this optimized TT of the correlation function in quantum simulation based on pseudo-imaginary-time evolution, resulting in ground-state energy with higher accuracy than the QTCI or Monte Carlo methods.","sentences":["Tensor cross interpolation (TCI) is a powerful technique for learning a tensor train (TT) by adaptively sampling a target tensor based on an interpolation formula.","However, when the tensor evaluations contain random noise, optimizing the TT is more advantageous than interpolating the noise.","Here, we propose a new method that starts with an initial guess of TT and optimizes it using non-linear least-squares by fitting it to measured points obtained from TCI.","We use quantics TCI (QTCI) in this method and demonstrate its effectiveness on sine and two-time correlation functions, with each evaluated with random noise.","The resulting TT exhibits increased robustness against noise compared to the QTCI method.","Furthermore, we employ this optimized TT of the correlation function in quantum simulation based on pseudo-imaginary-time evolution, resulting in ground-state energy with higher accuracy than the QTCI or Monte Carlo methods."],"url":"http://arxiv.org/abs/2405.12730v1","category":"quant-ph"}
{"created":"2024-05-21 12:20:19","title":"How to Train a Backdoor-Robust Model on a Poisoned Dataset without Auxiliary Data?","abstract":"Backdoor attacks have attracted wide attention from academia and industry due to their great security threat to deep neural networks (DNN). Most of the existing methods propose to conduct backdoor attacks by poisoning the training dataset with different strategies, so it's critical to identify the poisoned samples and then train a clean model on the unreliable dataset in the context of defending backdoor attacks. Although numerous backdoor countermeasure researches are proposed, their inherent weaknesses render them limited in practical scenarios, such as the requirement of enough clean samples, unstable defense performance under various attack conditions, poor defense performance against adaptive attacks, and so on.Therefore, in this paper, we are committed to overcome the above limitations and propose a more practical backdoor defense method. Concretely, we first explore the inherent relationship between the potential perturbations and the backdoor trigger, and the theoretical analysis and experimental results demonstrate that the poisoned samples perform more robustness to perturbation than the clean ones. Then, based on our key explorations, we introduce AdvrBD, an Adversarial perturbation-based and robust Backdoor Defense framework, which can effectively identify the poisoned samples and train a clean model on the poisoned dataset. Constructively, our AdvrBD eliminates the requirement for any clean samples or knowledge about the poisoned dataset (e.g., poisoning ratio), which significantly improves the practicality in real-world scenarios.","sentences":["Backdoor attacks have attracted wide attention from academia and industry due to their great security threat to deep neural networks (DNN).","Most of the existing methods propose to conduct backdoor attacks by poisoning the training dataset with different strategies, so it's critical to identify the poisoned samples and then train a clean model on the unreliable dataset in the context of defending backdoor attacks.","Although numerous backdoor countermeasure researches are proposed, their inherent weaknesses render them limited in practical scenarios, such as the requirement of enough clean samples, unstable defense performance under various attack conditions, poor defense performance against adaptive attacks, and so on.","Therefore, in this paper, we are committed to overcome the above limitations and propose a more practical backdoor defense method.","Concretely, we first explore the inherent relationship between the potential perturbations and the backdoor trigger, and the theoretical analysis and experimental results demonstrate that the poisoned samples perform more robustness to perturbation than the clean ones.","Then, based on our key explorations, we introduce AdvrBD, an Adversarial perturbation-based and robust Backdoor Defense framework, which can effectively identify the poisoned samples and train a clean model on the poisoned dataset.","Constructively, our AdvrBD eliminates the requirement for any clean samples or knowledge about the poisoned dataset (e.g., poisoning ratio), which significantly improves the practicality in real-world scenarios."],"url":"http://arxiv.org/abs/2405.12719v1","category":"cs.CR"}
{"created":"2024-05-21 12:16:20","title":"RecGPT: Generative Pre-training for Text-based Recommendation","abstract":"We present the first domain-adapted and fully-trained large language model, RecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for text-based recommendation. Experimental results on rating prediction and sequential recommendation tasks show that our model, RecGPT-7B-Instruct, outperforms previous strong baselines. We are releasing our RecGPT models as well as their pre-training and fine-tuning datasets to facilitate future research and downstream applications in text-based recommendation. Public \"huggingface\" links to our RecGPT models and datasets are available at: https://github.com/VinAIResearch/RecGPT","sentences":["We present the first domain-adapted and fully-trained large language model, RecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for text-based recommendation.","Experimental results on rating prediction and sequential recommendation tasks show that our model, RecGPT-7B-Instruct, outperforms previous strong baselines.","We are releasing our RecGPT models as well as their pre-training and fine-tuning datasets to facilitate future research and downstream applications in text-based recommendation.","Public \"huggingface\" links to our RecGPT models and datasets are available at: https://github.com/VinAIResearch/RecGPT"],"url":"http://arxiv.org/abs/2405.12715v1","category":"cs.IR"}
{"created":"2024-05-23 17:59:57","title":"NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections","abstract":"Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render highly specular objects, whose appearance varies quickly with changes in viewpoint. Recent works have improved NeRF's ability to render detailed specular appearance of distant environment illumination, but are unable to synthesize consistent reflections of closer content. Moreover, these techniques rely on large computationally-expensive neural networks to model outgoing radiance, which severely limits optimization and rendering speed. We address these issues with an approach based on ray tracing: instead of querying an expensive neural network for the outgoing view-dependent radiance at points along each camera ray, our model casts reflection rays from these points and traces them through the NeRF representation to render feature vectors which are decoded into color using a small inexpensive network. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing shiny objects, and that it is the only existing NeRF method that can synthesize photorealistic specular appearance and reflections in real-world scenes, while requiring comparable optimization time to current state-of-the-art view synthesis models.","sentences":["Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render highly specular objects, whose appearance varies quickly with changes in viewpoint.","Recent works have improved NeRF's ability to render detailed specular appearance of distant environment illumination, but are unable to synthesize consistent reflections of closer content.","Moreover, these techniques rely on large computationally-expensive neural networks to model outgoing radiance, which severely limits optimization and rendering speed.","We address these issues with an approach based on ray tracing: instead of querying an expensive neural network for the outgoing view-dependent radiance at points along each camera ray, our model casts reflection rays from these points and traces them through the NeRF representation to render feature vectors which are decoded into color using a small inexpensive network.","We demonstrate that our model outperforms prior methods for view synthesis of scenes containing shiny objects, and that it is the only existing NeRF method that can synthesize photorealistic specular appearance and reflections in real-world scenes, while requiring comparable optimization time to current state-of-the-art view synthesis models."],"url":"http://arxiv.org/abs/2405.14871v1","category":"cs.CV"}
{"created":"2024-05-23 17:56:34","title":"Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling","abstract":"Novel-view synthesis of specular objects like shiny metals or glossy paints remains a significant challenge. Not only the glossy appearance but also global illumination effects, including reflections of other objects in the environment, are critical components to faithfully reproduce a scene. In this paper, we present Neural Directional Encoding (NDE), a view-dependent appearance encoding of neural radiance fields (NeRF) for rendering specular objects. NDE transfers the concept of feature-grid-based spatial encoding to the angular domain, significantly improving the ability to model high-frequency angular signals. In contrast to previous methods that use encoding functions with only angular input, we additionally cone-trace spatial features to obtain a spatially varying directional encoding, which addresses the challenging interreflection effects. Extensive experiments on both synthetic and real datasets show that a NeRF model with NDE (1) outperforms the state of the art on view synthesis of specular objects, and (2) works with small networks to allow fast (real-time) inference. The project webpage and source code are available at: \\url{https://lwwu2.github.io/nde/}.","sentences":["Novel-view synthesis of specular objects like shiny metals or glossy paints remains a significant challenge.","Not only the glossy appearance but also global illumination effects, including reflections of other objects in the environment, are critical components to faithfully reproduce a scene.","In this paper, we present Neural Directional Encoding (NDE), a view-dependent appearance encoding of neural radiance fields (NeRF) for rendering specular objects.","NDE transfers the concept of feature-grid-based spatial encoding to the angular domain, significantly improving the ability to model high-frequency angular signals.","In contrast to previous methods that use encoding functions with only angular input, we additionally cone-trace spatial features to obtain a spatially varying directional encoding, which addresses the challenging interreflection effects.","Extensive experiments on both synthetic and real datasets show that a NeRF model with NDE (1) outperforms the state of the art on view synthesis of specular objects, and (2) works with small networks to allow fast (real-time) inference.","The project webpage and source code are available at: \\url{https://lwwu2.github.io/nde/}."],"url":"http://arxiv.org/abs/2405.14847v1","category":"cs.CV"}
{"created":"2024-05-23 17:56:07","title":"Li-Yau sub-gradient estimates and Perelman-type entropy formulas for the heat equation in quaternionic contact geometry","abstract":"We establish in the present paper two sub-gradient estimates for the quaternionic contact (qc) heat equation on a compact qc manifold of dimension $4n+3$, provided some positivity conditions are satisfied. These are qc versions of the prominent Li-Yau gradient estimate in Riemannian geometry. Another goal of this paper is to get two Perelman-type entropy formulas for the qc heat equation on a compact qc-Einstein manifold of dimension $4n+3$ with non-negative qc scalar curvature (e.g. compact $3$-Sasakian manifold), as well as an integral sub-gradient estimate for the positive solutions of the qc heat equation.","sentences":["We establish in the present paper two sub-gradient estimates for the quaternionic contact (qc) heat equation on a compact qc manifold of dimension $4n+3$, provided some positivity conditions are satisfied.","These are qc versions of the prominent Li-Yau gradient estimate in Riemannian geometry.","Another goal of this paper is to get two Perelman-type entropy formulas for the qc heat equation on a compact qc-Einstein manifold of dimension $4n+3$ with non-negative qc scalar curvature (e.g. compact $3$-Sasakian manifold), as well as an integral sub-gradient estimate for the positive solutions of the qc heat equation."],"url":"http://arxiv.org/abs/2405.14845v1","category":"math.DG"}
{"created":"2024-05-23 17:20:56","title":"Interacting phase diagram of twisted bilayer MoTe$_2$ in magnetic field","abstract":"We study electron-electron interaction induced states of twisted bilayer MoTe$_2$ in an out-of-plane magnetic field $B\\hat{\\bf z}$ near one hole per moir\\'e unit cell filling. The 3D phase diagram showing the evolution of competing phases with $B$, interaction strength and an out-of-plane electric field is presented at electron fillings that follow the Diophantine equation along Chern number $-\\text{sign}\\left(B\\right)$ line, that is pointing away from the charge neutral filling, where we find prominent Chern insulators consistent with the experiments. We also explain the experimental absence of prominent Chern insulators along the Chern number $+\\text{sign}\\left(B\\right)$ line.","sentences":["We study electron-electron interaction induced states of twisted bilayer MoTe$_2$ in an out-of-plane magnetic field $B\\hat{\\bf z}$ near one hole per moir\\'e unit cell filling.","The 3D phase diagram showing the evolution of competing phases with $B$, interaction strength and an out-of-plane electric field is presented at electron fillings that follow the Diophantine equation along Chern number $-\\text{sign}\\left(B\\right)$ line, that is pointing away from the charge neutral filling, where we find prominent Chern insulators consistent with the experiments.","We also explain the experimental absence of prominent Chern insulators along the Chern number $+\\text{sign}\\left(B\\right)$ line."],"url":"http://arxiv.org/abs/2405.14811v1","category":"cond-mat.str-el"}
{"created":"2024-05-23 17:06:50","title":"Koszul duality and the Poincar\u00e9-Birkhoff-Witt theorem","abstract":"Using a homotopy introduced by de Wilde and Lecomte and homological perturbation theory for $A_\\infty$-algebras, we give an explicit proof that the universal enveloping algebra $UL$ of a differential graded Lie algebra $L$ is Koszul, via an explicit contracting homotopy from the cobar construction $\\Omega CL$ of the Chevalley-Eilenberg chain coalgebra $CL$ of $L$ to $UL$.","sentences":["Using a homotopy introduced by de Wilde and Lecomte and homological perturbation theory for $A_\\infty$-algebras, we give an explicit proof that the universal enveloping algebra $UL$ of a differential graded Lie algebra $L$ is Koszul, via an explicit contracting homotopy from the cobar construction $\\Omega CL$ of the Chevalley-Eilenberg chain coalgebra $CL$ of $L$ to $UL$."],"url":"http://arxiv.org/abs/2405.14798v1","category":"math.KT"}
{"created":"2024-05-23 16:44:19","title":"Second order analysis for the optimal selection of time delays","abstract":"For a nonlinear ordinary differential equation with time delay, the differentiation of the solution with respect to the delay is investigated. Special emphasis is laid on the second-order derivative. The results are applied to an associated optimization problem for the time delay. A first- and second-order sensitivity analysis is performed including an adjoint calculus that avoids the second derivative of the state with respect to the delay.","sentences":["For a nonlinear ordinary differential equation with time delay, the differentiation of the solution with respect to the delay is investigated.","Special emphasis is laid on the second-order derivative.","The results are applied to an associated optimization problem for the time delay.","A first- and second-order sensitivity analysis is performed including an adjoint calculus that avoids the second derivative of the state with respect to the delay."],"url":"http://arxiv.org/abs/2405.14775v1","category":"math.OC"}
{"created":"2024-05-23 16:40:26","title":"Vortex-capturing multiscale spaces for the Ginzburg-Landau equation","abstract":"This paper considers minimizers of the Ginzburg-Landau energy functional in particular multiscale spaces which are based on finite elements. The spaces are constructed by localized orthogonal decomposition techniques and their usage for solving the Ginzburg-Landau equation was first suggested in [D\\\"orich, Henning, SINUM 2024]. In this work we further explore their approximation properties and give an analytical explanation for why vortex structures of energy minimizers can be captured more accurately in these spaces. We quantify the necessary mesh resolution in terms of the Ginzburg-Landau parameter $\\kappa$ and a stabilization parameter $\\beta \\ge 0$ that is used in the construction of the multiscale spaces. Furthermore, we analyze how $\\kappa$ affects the necessary locality of the multiscale basis functions and we prove that the choice $\\beta=0$ yields typically the highest accuracy. Our findings are supported by numerical experiments.","sentences":["This paper considers minimizers of the Ginzburg-Landau energy functional in particular multiscale spaces which are based on finite elements.","The spaces are constructed by localized orthogonal decomposition techniques and their usage for solving the Ginzburg-Landau equation was first suggested in [D\\\"orich, Henning, SINUM 2024].","In this work we further explore their approximation properties and give an analytical explanation for why vortex structures of energy minimizers can be captured more accurately in these spaces.","We quantify the necessary mesh resolution in terms of the Ginzburg-Landau parameter $\\kappa$ and a stabilization parameter $\\beta \\ge 0$ that is used in the construction of the multiscale spaces.","Furthermore, we analyze how $\\kappa$ affects the necessary locality of the multiscale basis functions and we prove that the choice $\\beta=0$ yields typically the highest accuracy.","Our findings are supported by numerical experiments."],"url":"http://arxiv.org/abs/2405.14772v1","category":"math.NA"}
{"created":"2024-05-23 15:49:42","title":"Convolutional Neural Network Model Observers Discount Signal-like Anatomical Structures During Search in Virtual Digital Breast Tomosynthesis Phantoms","abstract":"Model observers are computational tools to evaluate and optimize task-based medical image quality. Linear model observers, such as the Channelized Hotelling Observer (CHO), predict human accuracy in detection tasks with a few possible signal locations in clinical phantoms or real anatomic backgrounds. In recent years, Convolutional Neural Networks (CNNs) have been proposed as a new type of model observer. What is not well understood is what CNNs add over the more common linear model observer approaches. We compare the CHO and CNN detection accuracy to the radiologist's accuracy in searching for two types of signals (mass and microcalcification) embedded in 2D/3D breast tomosynthesis phantoms (DBT). We show that the CHO model's accuracy is comparable to the CNN's performance for a location-known-exactly detection task. However, for the search task with 2D/3D DBT phantoms, the CHO's detection accuracy was significantly lower than the CNN accuracy. A comparison to the radiologist's accuracy showed that the CNN but not the CHO could match or exceed the radiologist's accuracy in the 2D microcalcification and 3D mass search conditions. An analysis of the eye position showed that radiologists fixated more often and longer at the locations corresponding to CNN false positives. Most CHO false positives were the phantom's normal anatomy and were not fixated by radiologists. In conclusion, we show that CNNs can be used as an anthropomorphic model observer for the search task for which traditional linear model observers fail due to their inability to discount false positives arising from the anatomical backgrounds.","sentences":["Model observers are computational tools to evaluate and optimize task-based medical image quality.","Linear model observers, such as the Channelized Hotelling Observer (CHO), predict human accuracy in detection tasks with a few possible signal locations in clinical phantoms or real anatomic backgrounds.","In recent years, Convolutional Neural Networks (CNNs) have been proposed as a new type of model observer.","What is not well understood is what CNNs add over the more common linear model observer approaches.","We compare the CHO and CNN detection accuracy to the radiologist's accuracy in searching for two types of signals (mass and microcalcification) embedded in 2D/3D breast tomosynthesis phantoms (DBT).","We show that the CHO model's accuracy is comparable to the CNN's performance for a location-known-exactly detection task.","However, for the search task with 2D/3D DBT phantoms, the CHO's detection accuracy was significantly lower than the CNN accuracy.","A comparison to the radiologist's accuracy showed that the CNN but not the CHO could match or exceed the radiologist's accuracy in the 2D microcalcification and 3D mass search conditions.","An analysis of the eye position showed that radiologists fixated more often and longer at the locations corresponding to CNN false positives.","Most CHO false positives were the phantom's normal anatomy and were not fixated by radiologists.","In conclusion, we show that CNNs can be used as an anthropomorphic model observer for the search task for which traditional linear model observers fail due to their inability to discount false positives arising from the anatomical backgrounds."],"url":"http://arxiv.org/abs/2405.14720v1","category":"eess.IV"}
{"created":"2024-05-23 15:23:58","title":"Efficient Algorithms for the Sensitivities of the Pearson Correlation Coefficient and Its Statistical Significance to Online Data","abstract":"Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford's online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.","sentences":["Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates.","Leveraging identities from Welford's online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data.","Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms.","Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics.","Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development."],"url":"http://arxiv.org/abs/2405.14686v1","category":"stat.ME"}
{"created":"2024-05-23 15:11:53","title":"Stable minimal hypersurfaces in $\\mathbb R^6$","abstract":"Following the strategy developed by Chodosh, Li, Minter and Stryker, and using the volume estimate of Antonelli and Xu, we prove that, in $\\mathbb R^6$, a complete, two-sided, stable minimal hypersurfaces is flat.","sentences":["Following the strategy developed by Chodosh, Li, Minter and Stryker, and using the volume estimate of Antonelli and Xu, we prove that, in $\\mathbb R^6$, a complete, two-sided, stable minimal hypersurfaces is flat."],"url":"http://arxiv.org/abs/2405.14676v1","category":"math.DG"}
{"created":"2024-05-23 14:36:52","title":"Bounds for the smallest eigenvalue of the NTK for arbitrary spherical data of arbitrary dimension","abstract":"Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization. However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension $d_0$ scales at least logarithmically in the number of samples $n$. In this work we remove both of these requirements and instead provide bounds in terms of a measure of the collinearity of the data: notably these bounds hold with high probability even when $d_0$ is held constant versus $n$. We prove our results through a novel application of the hemisphere transform.","sentences":["Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization.","However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension $d_0$ scales at least logarithmically in the number of samples $n$. In this work we remove both of these requirements and instead provide bounds in terms of a measure of the collinearity of the data: notably these bounds hold with high probability even when $d_0$ is held constant versus $n$. We prove our results through a novel application of the hemisphere transform."],"url":"http://arxiv.org/abs/2405.14630v1","category":"stat.ML"}
{"created":"2024-05-23 14:25:55","title":"Last-iterate convergence of modified predictive method via high-resolution differential equation on bilinear game","abstract":"This paper discusses the convergence of the modified predictive method (MPM) proposed by Liang and stokes corresponding to high-resolution differential equations (HRDE) in bilinear games. First, we present the high-resolution differential equations (MPM-HRDE) corresponding to the MPM. Then, we discuss the uniqueness of the solution for MPM-HRDE in bilinear games. Finally, we provide the convergence results of MPM-HRDE in bilinear games. The results obtained in this paper address the gap in the existing literature and extend the conclusions of related works.","sentences":["This paper discusses the convergence of the modified predictive method (MPM) proposed by Liang and stokes corresponding to high-resolution differential equations (HRDE) in bilinear games.","First, we present the high-resolution differential equations (MPM-HRDE) corresponding to the MPM.","Then, we discuss the uniqueness of the solution for MPM-HRDE in bilinear games.","Finally, we provide the convergence results of MPM-HRDE in bilinear games.","The results obtained in this paper address the gap in the existing literature and extend the conclusions of related works."],"url":"http://arxiv.org/abs/2405.14613v1","category":"math.OC"}
{"created":"2024-05-23 13:40:28","title":"Beyond the Buzz: Strategic Paths for Enabling Useful NISQ Applications","abstract":"There is much debate on whether quantum computing on current NISQ devices, consisting of noisy hundred qubits and requiring a non-negligible usage of classical computing as part of the algorithms, has utility and will ever offer advantages for scientific and industrial applications with respect to traditional computing. In this position paper, we argue that while real-world NISQ quantum applications have yet to surpass their classical counterparts, strategic approaches can be used to facilitate advancements in both industrial and scientific applications. We have identified three key strategies to guide NISQ computing towards practical and useful implementations. Firstly, prioritizing the identification of a \"killer app\" is a key point. An application demonstrating the distinctive capabilities of NISQ devices can catalyze broader development. We suggest focusing on applications that are inherently quantum, e.g., pointing towards quantum chemistry and material science as promising domains. These fields hold the potential to exhibit benefits, setting benchmarks for other applications to follow. Secondly, integrating AI and deep-learning methods into NISQ computing is a promising approach. Examples such as quantum Physics-Informed Neural Networks and Differentiable Quantum Circuits (DQC) demonstrate the synergy between quantum computing and AI. Lastly, recognizing the interdisciplinary nature of NISQ computing, we advocate for a co-design approach. Achieving synergy between classical and quantum computing necessitates an effort in co-designing quantum applications, algorithms, and programming environments, and the integration of HPC with quantum hardware. The interoperability of these components is crucial for enabling the full potential of NISQ computing.","sentences":["There is much debate on whether quantum computing on current NISQ devices, consisting of noisy hundred qubits and requiring a non-negligible usage of classical computing as part of the algorithms, has utility and will ever offer advantages for scientific and industrial applications with respect to traditional computing.","In this position paper, we argue that while real-world NISQ quantum applications have yet to surpass their classical counterparts, strategic approaches can be used to facilitate advancements in both industrial and scientific applications.","We have identified three key strategies to guide NISQ computing towards practical and useful implementations.","Firstly, prioritizing the identification of a \"killer app\" is a key point.","An application demonstrating the distinctive capabilities of NISQ devices can catalyze broader development.","We suggest focusing on applications that are inherently quantum, e.g., pointing towards quantum chemistry and material science as promising domains.","These fields hold the potential to exhibit benefits, setting benchmarks for other applications to follow.","Secondly, integrating AI and deep-learning methods into NISQ computing is a promising approach.","Examples such as quantum Physics-Informed Neural Networks and Differentiable Quantum Circuits (DQC) demonstrate the synergy between quantum computing and AI.","Lastly, recognizing the interdisciplinary nature of NISQ computing, we advocate for a co-design approach.","Achieving synergy between classical and quantum computing necessitates an effort in co-designing quantum applications, algorithms, and programming environments, and the integration of HPC with quantum hardware.","The interoperability of these components is crucial for enabling the full potential of NISQ computing."],"url":"http://arxiv.org/abs/2405.14561v1","category":"quant-ph"}
{"created":"2024-05-23 13:22:36","title":"Measurement of the production cross section of prompt $\u039e^0_{\\rm c}$ baryons in p$-$Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}~=~5.02$ TeV","abstract":"The transverse momentum ($p_{\\rm T}$) differential production cross section of the promptly-produced charm-strange baryon $\\Xi_{\\rm c}^{0}$ (and its charge conjugate $\\overline{\\Xi_{\\rm c}^{0}}$) is measured at midrapidity via its hadronic decay into ${\\rm \\pi^{+}}\\Xi^{-}$ in p$-$Pb collisions at a centre-of-mass energy per nucleon$-$nucleon collision $\\sqrt{s_{\\mathrm{NN}}}~=~5.02$ TeV with the ALICE detector at the LHC. The $\\Xi_{\\rm c}^{0}$ nuclear modification factor ($R_{\\rm pPb}$), calculated from the cross sections in pp and p$-$Pb collisions, is presented and compared with the $R_{\\rm pPb}$ of $\\Lambda_{\\rm c}^{+}$ baryons. The ratios between the $p_{\\rm T}$-differential production cross section of $\\Xi_{\\rm c}^{0}$ baryons and those of $\\mathrm {D^0}$ mesons and $\\Lambda_{\\rm c}^{+}$ baryons are also reported and compared with results at forward and backward rapidity from the LHCb Collaboration. The measurements of the production cross section of prompt $\\Xi^0_{\\rm c}$ baryons are compared with a model based on perturbative QCD calculations of charm-quark production cross sections, which includes only cold nuclear matter effects in p$-$Pb collisions, and underestimates the measurement by a factor of about 50. This discrepancy is reduced when the data is compared with a model in which hadronisation is implemented via quark coalescence. The $p_{\\rm T}$-integrated cross section of prompt $\\Xi^0_{\\rm c}$-baryon production at midrapidity extrapolated down to $p_{\\rm T}$ = 0 is also reported. These measurements offer insights and constraints for theoretical calculations of the hadronisation process. Additionally, they provide inputs for the calculation of the charm production cross section in p$-$Pb collisions at midrapidity.","sentences":["The transverse momentum ($p_{\\rm T}$) differential production cross section of the promptly-produced charm-strange baryon $\\Xi_{\\rm c}^{0}$ (and its charge conjugate $\\overline{\\Xi_{\\rm c}^{0}}$) is measured at midrapidity via its hadronic decay into ${\\rm \\pi^{+}}\\Xi^{-}$ in p$-$Pb collisions at a centre-of-mass energy per nucleon$-$nucleon collision $\\sqrt{s_{\\mathrm{NN}}}~=~5.02$ TeV with the ALICE detector at the LHC.","The $\\Xi_{\\rm c}^{0}$ nuclear modification factor ($R_{\\rm pPb}$), calculated from the cross sections in pp and p$-$Pb collisions, is presented and compared with the $R_{\\rm pPb}$ of $\\Lambda_{\\rm c}^{+}$ baryons.","The ratios between the $p_{\\rm T}$-differential production cross section of $\\Xi_{\\rm c}^{0}$ baryons and those of $\\mathrm {D^0}$ mesons and $\\Lambda_{\\rm c}^{+}$ baryons are also reported and compared with results at forward and backward rapidity from the LHCb Collaboration.","The measurements of the production cross section of prompt $\\Xi^0_{\\rm c}$ baryons are compared with a model based on perturbative QCD calculations of charm-quark production cross sections, which includes only cold nuclear matter effects in p$-$Pb collisions, and underestimates the measurement by a factor of about 50.","This discrepancy is reduced when the data is compared with a model in which hadronisation is implemented via quark coalescence.","The $p_{\\rm T}$-integrated cross section of prompt $\\Xi^0_{\\rm c}$-baryon production at midrapidity extrapolated down to $p_{\\rm T}$","= 0 is also reported.","These measurements offer insights and constraints for theoretical calculations of the hadronisation process.","Additionally, they provide inputs for the calculation of the charm production cross section in p$-$Pb collisions at midrapidity."],"url":"http://arxiv.org/abs/2405.14538v1","category":"nucl-ex"}
{"created":"2024-05-23 12:47:20","title":"Closed-form estimators for an exponential family derived from likelihood equations","abstract":"In this paper, we derive closed-form estimators for the parameters of some probability distributions belonging to the exponential family. A bootstrap bias-reduced version of these proposed closed-form estimators are also derived. A Monte Carlo simulation is performed for the assessment of the estimators. The results are seen to be quite favorable to the proposed bootstrap bias-reduce estimators.","sentences":["In this paper, we derive closed-form estimators for the parameters of some probability distributions belonging to the exponential family.","A bootstrap bias-reduced version of these proposed closed-form estimators are also derived.","A Monte Carlo simulation is performed for the assessment of the estimators.","The results are seen to be quite favorable to the proposed bootstrap bias-reduce estimators."],"url":"http://arxiv.org/abs/2405.14509v1","category":"stat.ME"}
{"created":"2024-05-23 11:55:49","title":"Neural Collapse versus Low-rank Bias: Is Deep Neural Collapse Really Optimal?","abstract":"Deep neural networks (DNNs) exhibit a surprising structure in their final layer known as neural collapse (NC), and a growing body of works has currently investigated the propagation of neural collapse to earlier layers of DNNs -- a phenomenon called deep neural collapse (DNC). However, existing theoretical results are restricted to special cases: linear models, only two layers or binary classification. In contrast, we focus on non-linear models of arbitrary depth in multi-class classification and reveal a surprising qualitative shift. As soon as we go beyond two layers or two classes, DNC stops being optimal for the deep unconstrained features model (DUFM) -- the standard theoretical framework for the analysis of collapse. The main culprit is a low-rank bias of multi-layer regularization schemes: this bias leads to optimal solutions of even lower rank than the neural collapse. We support our theoretical findings with experiments on both DUFM and real data, which show the emergence of the low-rank structure in the solution found by gradient descent.","sentences":["Deep neural networks (DNNs) exhibit a surprising structure in their final layer known as neural collapse (NC), and a growing body of works has currently investigated the propagation of neural collapse to earlier layers of DNNs -- a phenomenon called deep neural collapse (DNC).","However, existing theoretical results are restricted to special cases: linear models, only two layers or binary classification.","In contrast, we focus on non-linear models of arbitrary depth in multi-class classification and reveal a surprising qualitative shift.","As soon as we go beyond two layers or two classes, DNC stops being optimal for the deep unconstrained features model (DUFM) -- the standard theoretical framework for the analysis of collapse.","The main culprit is a low-rank bias of multi-layer regularization schemes: this bias leads to optimal solutions of even lower rank than the neural collapse.","We support our theoretical findings with experiments on both DUFM and real data, which show the emergence of the low-rank structure in the solution found by gradient descent."],"url":"http://arxiv.org/abs/2405.14468v1","category":"cs.LG"}
{"created":"2024-05-23 11:22:32","title":"On Liouville's Theorem","abstract":"In this paper we explore Liouville's theorem on Riemannian cones as defined below. We also study the Strong Liouville Property, that is, the property of a cone having spaces of harmonic functions of a fixed polynomial growth of finite dimension.","sentences":["In this paper we explore Liouville's theorem on Riemannian cones as defined below.","We also study the Strong Liouville Property, that is, the property of a cone having spaces of harmonic functions of a fixed polynomial growth of finite dimension."],"url":"http://arxiv.org/abs/2405.14443v1","category":"math.AP"}
{"created":"2024-05-23 11:21:12","title":"Fully parallel implementation of digital memcomputing on FPGA","abstract":"We present a fully parallel digital memcomputing solver implemented on a field-programmable gate array (FPGA) board. For this purpose, we have designed an FPGA code that solves the ordinary differential equations associated with digital memcomputing in parallel. A feature of the code is the use of only integer-type variables and integer constants to enhance optimization. Consequently, each integration step in our solver is executed in 96~ns. This method was utilized for difficult instances of the Boolean satisfiability (SAT) problem close to a phase transition, involving up to about 150 variables. Our results demonstrate that the parallel implementation reduces the scaling exponent by about 1 compared to a sequential C++ code on a standard computer. Additionally, compared to C++ code, we observed a time-to-solution advantage of about three orders of magnitude. Given the limitations of FPGA resources, the current implementation of digital memcomputing will be especially useful for solving compact but challenging problems.","sentences":["We present a fully parallel digital memcomputing solver implemented on a field-programmable gate array (FPGA) board.","For this purpose, we have designed an FPGA code that solves the ordinary differential equations associated with digital memcomputing in parallel.","A feature of the code is the use of only integer-type variables and integer constants to enhance optimization.","Consequently, each integration step in our solver is executed in 96~ns.","This method was utilized for difficult instances of the Boolean satisfiability (SAT) problem close to a phase transition, involving up to about 150 variables.","Our results demonstrate that the parallel implementation reduces the scaling exponent by about 1 compared to a sequential C++ code on a standard computer.","Additionally, compared to C++ code, we observed a time-to-solution advantage of about three orders of magnitude.","Given the limitations of FPGA resources, the current implementation of digital memcomputing will be especially useful for solving compact but challenging problems."],"url":"http://arxiv.org/abs/2405.14442v1","category":"cs.ET"}
{"created":"2024-05-23 10:48:30","title":"When predict can also explain: few-shot prediction to select better neural latents","abstract":"Latent variable models serve as powerful tools to infer underlying dynamics from observed neural activity. However, due to the absence of ground truth data, prediction benchmarks are often employed as proxies. In this study, we reveal the limitations of the widely-used 'co-smoothing' prediction framework and propose an improved few-shot prediction approach that encourages more accurate latent dynamics. Utilizing a student-teacher setup with Hidden Markov Models, we demonstrate that the high co-smoothing model space can encompass models with arbitrary extraneous dynamics within their latent representations. To address this, we introduce a secondary metric -- a few-shot version of co-smoothing. This involves performing regression from the latent variables to held-out channels in the data using fewer trials. Our results indicate that among models with near-optimal co-smoothing, those with extraneous dynamics underperform in the few-shot co-smoothing compared to 'minimal' models devoid of such dynamics. We also provide analytical insights into the origin of this phenomenon. We further validate our findings on real neural data using two state-of-the-art methods: LFADS and STNDT. In the absence of ground truth, we suggest a proxy measure to quantify extraneous dynamics. By cross-decoding the latent variables of all model pairs with high co-smoothing, we identify models with minimal extraneous dynamics. We find a correlation between few-shot co-smoothing performance and this new measure. In summary, we present a novel prediction metric designed to yield latent variables that more accurately reflect the ground truth, offering a significant improvement for latent dynamics inference.","sentences":["Latent variable models serve as powerful tools to infer underlying dynamics from observed neural activity.","However, due to the absence of ground truth data, prediction benchmarks are often employed as proxies.","In this study, we reveal the limitations of the widely-used 'co-smoothing' prediction framework and propose an improved few-shot prediction approach that encourages more accurate latent dynamics.","Utilizing a student-teacher setup with Hidden Markov Models, we demonstrate that the high co-smoothing model space can encompass models with arbitrary extraneous dynamics within their latent representations.","To address this, we introduce a secondary metric -- a few-shot version of co-smoothing.","This involves performing regression from the latent variables to held-out channels in the data using fewer trials.","Our results indicate that among models with near-optimal co-smoothing, those with extraneous dynamics underperform in the few-shot co-smoothing compared to 'minimal' models devoid of such dynamics.","We also provide analytical insights into the origin of this phenomenon.","We further validate our findings on real neural data using two state-of-the-art methods: LFADS and STNDT.","In the absence of ground truth, we suggest a proxy measure to quantify extraneous dynamics.","By cross-decoding the latent variables of all model pairs with high co-smoothing, we identify models with minimal extraneous dynamics.","We find a correlation between few-shot co-smoothing performance and this new measure.","In summary, we present a novel prediction metric designed to yield latent variables that more accurately reflect the ground truth, offering a significant improvement for latent dynamics inference."],"url":"http://arxiv.org/abs/2405.14425v1","category":"cs.LG"}
{"created":"2024-05-23 10:20:59","title":"Roots and Logarithms of Multipliers","abstract":"By now it is a well-known fact that if $f$ is a multiplier for the Drury-Arveson space $H^2_n$, and if there is a $c>0$ such that $|f(z)|\\geq c$ for every $z\\in B$, then the reciprocal function 1/f is also a multiplier for $H^2_n$. We show that for such an $f$ and for every $t\\in \\mathbb{R}$, $f^t$ is also a multiplier for $H^2_n$. We do so by deriving a differentiation formula for $R^m(f^th)$.Moreover, by this formula the same result holds for spaces $H_{m,s}$ of the Besov-Dirichlet type. The same technique also gives us the result that for a non-vanishing multiplier $f$ of $H^2_n$, $log f$ is a multiplier of $H^2_n$ if and only if log $f$ is bounded on $B$.","sentences":["By now it is a well-known fact that if $f$ is a multiplier for the Drury-Arveson space $H^2_n$, and if there is a $c>0$ such that $|f(z)|\\geq c$ for every $z\\in B$, then the reciprocal function 1/f is also a multiplier for $H^2_n$. We show that for such an $f$ and for every $t\\in \\mathbb{R}$, $f^t$ is also a multiplier for $H^2_n$. We do so by deriving a differentiation formula for $R^m(f^th)$.Moreover, by this formula the same result holds for spaces $H_{m,s}$ of the Besov-Dirichlet type.","The same technique also gives us the result that for a non-vanishing multiplier $f$ of $H^2_n$, $log f$ is a multiplier of $H^2_n$ if and only if log $f$ is bounded on $B$."],"url":"http://arxiv.org/abs/2405.14401v1","category":"math.FA"}
{"created":"2024-05-23 10:19:35","title":"Verifying Global Two-Safety Properties in Neural Networks with Confidence","abstract":"We present the first automated verification technique for confidence-based 2-safety properties, such as global robustness and global fairness, in deep neural networks (DNNs). Our approach combines self-composition to leverage existing reachability analysis techniques and a novel abstraction of the softmax function, which is amenable to automated verification. We characterize and prove the soundness of our static analysis technique. Furthermore, we implement it on top of Marabou, a safety analysis tool for neural networks, conducting a performance evaluation on several publicly available benchmarks for DNN verification.","sentences":["We present the first automated verification technique for confidence-based 2-safety properties, such as global robustness and global fairness, in deep neural networks (DNNs).","Our approach combines self-composition to leverage existing reachability analysis techniques and a novel abstraction of the softmax function, which is amenable to automated verification.","We characterize and prove the soundness of our static analysis technique.","Furthermore, we implement it on top of Marabou, a safety analysis tool for neural networks, conducting a performance evaluation on several publicly available benchmarks for DNN verification."],"url":"http://arxiv.org/abs/2405.14400v1","category":"cs.LO"}
{"created":"2024-05-23 09:45:57","title":"RoPINN: Region Optimized Physics-Informed Neural Networks","abstract":"Physics-informed neural networks (PINNs) have been widely applied to solve partial differential equations (PDEs) by enforcing outputs and gradients of deep models to satisfy target equations. Due to the limitation of numerical computation, PINNs are conventionally optimized on finite selected points. However, since PDEs are usually defined on continuous domains, solely optimizing models on scattered points may be insufficient to obtain an accurate solution for the whole domain. To mitigate this inherent deficiency of the default scatter-point optimization, this paper proposes and theoretically studies a new training paradigm as region optimization. Concretely, we propose to extend the optimization process of PINNs from isolated points to their continuous neighborhood regions, which can theoretically decrease the generalization error, especially for hidden high-order constraints of PDEs. A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from this new paradigm, which is implemented by a straightforward but effective Monte Carlo sampling method. By calibrating the sampling process into trust regions, RoPINN finely balances sampling efficiency and generalization error. Experimentally, RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation.","sentences":["Physics-informed neural networks (PINNs) have been widely applied to solve partial differential equations (PDEs) by enforcing outputs and gradients of deep models to satisfy target equations.","Due to the limitation of numerical computation, PINNs are conventionally optimized on finite selected points.","However, since PDEs are usually defined on continuous domains, solely optimizing models on scattered points may be insufficient to obtain an accurate solution for the whole domain.","To mitigate this inherent deficiency of the default scatter-point optimization, this paper proposes and theoretically studies a new training paradigm as region optimization.","Concretely, we propose to extend the optimization process of PINNs from isolated points to their continuous neighborhood regions, which can theoretically decrease the generalization error, especially for hidden high-order constraints of PDEs.","A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from this new paradigm, which is implemented by a straightforward but effective Monte Carlo sampling method.","By calibrating the sampling process into trust regions, RoPINN finely balances sampling efficiency and generalization error.","Experimentally, RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation."],"url":"http://arxiv.org/abs/2405.14369v1","category":"cs.LG"}
{"created":"2024-05-23 09:39:12","title":"Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators","abstract":"Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible. However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy. Drawing inspiration from the central pattern generators (CPGs) in the human brain, which produce rhythmic patterned outputs without requiring rhythmic inputs, we propose a novel PE technique for SNNs, termed CPG-PE. We demonstrate that the commonly used sinusoidal PE is mathematically a specific solution to the membrane potential dynamics of a particular CPG. Moreover, extensive experiments across various domains, including time-series forecasting, natural language processing, and image classification, show that SNNs with CPG-PE outperform their conventional counterparts. Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain. This investigation may offer valuable insights into the fundamental principles of neural computation.","sentences":["Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible.","However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy.","Drawing inspiration from the central pattern generators (CPGs) in the human brain, which produce rhythmic patterned outputs without requiring rhythmic inputs, we propose a novel PE technique for SNNs, termed CPG-PE.","We demonstrate that the commonly used sinusoidal PE is mathematically a specific solution to the membrane potential dynamics of a particular CPG.","Moreover, extensive experiments across various domains, including time-series forecasting, natural language processing, and image classification, show that SNNs with CPG-PE outperform their conventional counterparts.","Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain.","This investigation may offer valuable insights into the fundamental principles of neural computation."],"url":"http://arxiv.org/abs/2405.14362v1","category":"cs.NE"}
{"created":"2024-05-23 09:20:56","title":"Klein-Gordon oscillators, RQM and quantum time","abstract":"We show that turning on the interaction of relativistic spinless particles with the vacuum of relativistic quantum mechanics (RQM) leads to the replacement of the Klein-Gordon equation with the Klein-Gordon oscillator equation. In this case, coordinate time becomes an operator and free relativistic particles go into a virtual state. We discuss geometry associated with classical and quantum Klein-Gordon oscillators, and its relation to the geometry underlying the description of free particles.","sentences":["We show that turning on the interaction of relativistic spinless particles with the vacuum of relativistic quantum mechanics (RQM) leads to the replacement of the Klein-Gordon equation with the Klein-Gordon oscillator equation.","In this case, coordinate time becomes an operator and free relativistic particles go into a virtual state.","We discuss geometry associated with classical and quantum Klein-Gordon oscillators, and its relation to the geometry underlying the description of free particles."],"url":"http://arxiv.org/abs/2405.14349v1","category":"hep-th"}
{"created":"2024-05-23 09:07:35","title":"I$^2$VC: A Unified Framework for Intra- \\& Inter-frame Video Compression","abstract":"Video compression aims to reconstruct seamless frames by encoding the motion and residual information from existing frames. Previous neural video compression methods necessitate distinct codecs for three types of frames (I-frame, P-frame and B-frame), which hinders a unified approach and generalization across different video contexts. Intra-codec techniques lack the advanced Motion Estimation and Motion Compensation (MEMC) found in inter-codec, leading to fragmented frameworks lacking uniformity. Our proposed \\textbf{Intra- \\& Inter-frame Video Compression (I$^2$VC)} framework employs a single spatio-temporal codec that guides feature compression rates according to content importance. This unified codec transforms the dependence across frames into a conditional coding scheme, thus integrating intra- and inter-frame compression into one cohesive strategy. Given the absence of explicit motion data, achieving competent inter-frame compression with only a conditional codec poses a challenge. To resolve this, our approach includes an implicit inter-frame alignment mechanism. With the pre-trained diffusion denoising process, the utilization of a diffusion-inverted reference feature rather than random noise supports the initial compression state. This process allows for selective denoising of motion-rich regions based on decoded features, facilitating accurate alignment without the need for MEMC. Our experimental findings, across various compression configurations (AI, LD and RA) and frame types, prove that I$^2$VC outperforms the state-of-the-art perceptual learned codecs. Impressively, it exhibits a 58.4\\% enhancement in perceptual reconstruction performance when benchmarked against the H.266/VVC standard (VTM). Official implementation can be found at \\href{https://github.com/GYukai/I2VC}{https://github.com/GYukai/I2VC}","sentences":["Video compression aims to reconstruct seamless frames by encoding the motion and residual information from existing frames.","Previous neural video compression methods necessitate distinct codecs for three types of frames (I-frame, P-frame and B-frame), which hinders a unified approach and generalization across different video contexts.","Intra-codec techniques lack the advanced Motion Estimation and Motion Compensation (MEMC) found in inter-codec, leading to fragmented frameworks lacking uniformity.","Our proposed \\textbf{Intra- \\& Inter-frame Video Compression (I$^2$VC)} framework employs a single spatio-temporal codec that guides feature compression rates according to content importance.","This unified codec transforms the dependence across frames into a conditional coding scheme, thus integrating intra- and inter-frame compression into one cohesive strategy.","Given the absence of explicit motion data, achieving competent inter-frame compression with only a conditional codec poses a challenge.","To resolve this, our approach includes an implicit inter-frame alignment mechanism.","With the pre-trained diffusion denoising process, the utilization of a diffusion-inverted reference feature rather than random noise supports the initial compression state.","This process allows for selective denoising of motion-rich regions based on decoded features, facilitating accurate alignment without the need for MEMC.","Our experimental findings, across various compression configurations (AI, LD and RA) and frame types, prove that I$^2$VC outperforms the state-of-the-art perceptual learned codecs.","Impressively, it exhibits a 58.4\\% enhancement in perceptual reconstruction performance when benchmarked against the H.266/VVC standard (VTM).","Official implementation can be found at \\href{https://github.com/GYukai/I2VC}{https://github.com/GYukai/I2VC}"],"url":"http://arxiv.org/abs/2405.14336v1","category":"eess.IV"}
{"created":"2024-05-23 08:53:45","title":"The Sun's Non-Potential Corona over Solar Cycle 24","abstract":"The global magnetic field in the solar corona is known to contain free magnetic energy and magnetic helicity above that of a current-free (potential) state. But the strength of this non-potentiality and its evolution over the solar cycle remain uncertain. Here we model the corona over Solar Cycle 24 using a simplified magneto-frictional model that retains the magnetohydrodynamic induction equation but assumes relaxation towards force-free equilibrium, driven by solar surface motions and flux emergence. The model is relatively conservative compared to some others in the literature, with free energy approximately 20-25% of the potential field energy. We find that unsigned helicity is about a factor 10 higher at Maximum than Minimum, while free magnetic energy shows an even greater increase. The cycle averages of these two quantities are linearly correlated, extending a result found previously for active regions. Also, we propose a practical measure of eruptivity for these simulations, and show that this increases concurrently with the sunspot number, in accordance with observed coronal mass ejection rates. Whilst shearing by surface motions generates 50% or more of the free energy and helicity in the corona, we show that active regions must emerge with their own internal helicity otherwise the eruptivity is substantially reduced and follows the wrong pattern over time.","sentences":["The global magnetic field in the solar corona is known to contain free magnetic energy and magnetic helicity above that of a current-free (potential) state.","But the strength of this non-potentiality and its evolution over the solar cycle remain uncertain.","Here we model the corona over Solar Cycle 24 using a simplified magneto-frictional model that retains the magnetohydrodynamic induction equation but assumes relaxation towards force-free equilibrium, driven by solar surface motions and flux emergence.","The model is relatively conservative compared to some others in the literature, with free energy approximately 20-25% of the potential field energy.","We find that unsigned helicity is about a factor 10 higher at Maximum than Minimum, while free magnetic energy shows an even greater increase.","The cycle averages of these two quantities are linearly correlated, extending a result found previously for active regions.","Also, we propose a practical measure of eruptivity for these simulations, and show that this increases concurrently with the sunspot number, in accordance with observed coronal mass ejection rates.","Whilst shearing by surface motions generates 50% or more of the free energy and helicity in the corona, we show that active regions must emerge with their own internal helicity otherwise the eruptivity is substantially reduced and follows the wrong pattern over time."],"url":"http://arxiv.org/abs/2405.14322v1","category":"astro-ph.SR"}
{"created":"2024-05-23 08:32:40","title":"Deep Learning Fusion For Effective Malware Detection: Leveraging Visual Features","abstract":"Malware has become a formidable threat as it has been growing exponentially in number and sophistication, thus, it is imperative to have a solution that is easy to implement, reliable, and effective. While recent research has introduced deep learning multi-feature fusion algorithms, they lack a proper explanation. In this work, we investigate the power of fusing Convolutional Neural Network models trained on different modalities of a malware executable. We are proposing a novel multimodal fusion algorithm, leveraging three different visual malware features: Grayscale Image, Entropy Graph, and SimHash Image, with which we conducted exhaustive experiments independently on each feature and combinations of all three of them using fusion operators such as average, maximum, add, and concatenate for effective malware detection and classification. The proposed strategy has a detection rate of 1.00 (on a scale of 0-1) in identifying malware in the given dataset. We explained its interpretability with visualization techniques such as t-SNE and Grad-CAM. Experimental results show the model works even for a highly imbalanced dataset. We also assessed the effectiveness of the proposed method on obfuscated malware and achieved state-of-the-art results. The proposed methodology is more reliable as our findings prove VGG16 model can detect and classify malware in a matter of seconds in real-time.","sentences":["Malware has become a formidable threat as it has been growing exponentially in number and sophistication, thus, it is imperative to have a solution that is easy to implement, reliable, and effective.","While recent research has introduced deep learning multi-feature fusion algorithms, they lack a proper explanation.","In this work, we investigate the power of fusing Convolutional Neural Network models trained on different modalities of a malware executable.","We are proposing a novel multimodal fusion algorithm, leveraging three different visual malware features:","Grayscale Image, Entropy Graph, and SimHash Image, with which we conducted exhaustive experiments independently on each feature and combinations of all three of them using fusion operators such as average, maximum, add, and concatenate for effective malware detection and classification.","The proposed strategy has a detection rate of 1.00 (on a scale of 0-1) in identifying malware in the given dataset.","We explained its interpretability with visualization techniques such as t-SNE and Grad-CAM.","Experimental results show the model works even for a highly imbalanced dataset.","We also assessed the effectiveness of the proposed method on obfuscated malware and achieved state-of-the-art results.","The proposed methodology is more reliable as our findings prove VGG16 model can detect and classify malware in a matter of seconds in real-time."],"url":"http://arxiv.org/abs/2405.14311v1","category":"cs.CR"}
{"created":"2024-05-23 08:22:00","title":"Graphcode: Learning from multiparameter persistent homology using graph neural networks","abstract":"We introduce graphcodes, a novel multi-scale summary of the topological properties of a dataset that is based on the well-established theory of persistent homology. Graphcodes handle datasets that are filtered along two real-valued scale parameters. Such multi-parameter topological summaries are usually based on complicated theoretical foundations and difficult to compute; in contrast, graphcodes yield an informative and interpretable summary and can be computed as efficient as one-parameter summaries. Moreover, a graphcode is simply an embedded graph and can therefore be readily integrated in machine learning pipelines using graph neural networks. We describe such a pipeline and demonstrate that graphcodes achieve better classification accuracy than state-of-the-art approaches on various datasets.","sentences":["We introduce graphcodes, a novel multi-scale summary of the topological properties of a dataset that is based on the well-established theory of persistent homology.","Graphcodes handle datasets that are filtered along two real-valued scale parameters.","Such multi-parameter topological summaries are usually based on complicated theoretical foundations and difficult to compute; in contrast, graphcodes yield an informative and interpretable summary and can be computed as efficient as one-parameter summaries.","Moreover, a graphcode is simply an embedded graph and can therefore be readily integrated in machine learning pipelines using graph neural networks.","We describe such a pipeline and demonstrate that graphcodes achieve better classification accuracy than state-of-the-art approaches on various datasets."],"url":"http://arxiv.org/abs/2405.14302v1","category":"math.AT"}
{"created":"2024-05-23 07:56:03","title":"Two-dimensional fluids via matrix hydrodynamics","abstract":"Two-dimensional (2-D) incompressible, inviscid fluids produce fascinating patterns of swirling motion. How and why the patterns emerge are long-standing questions, first addressed in the 19th century by Helmholtz, Kirchhoff, and Kelvin. Countless researchers have since contributed to innovative techniques and results, but the overarching problem of swirling 2-D motion and its long-time behavior remains largely open. Here we advocate an alternative view-point that sheds light on this problem via a link to isospectral matrix flows. The link is established through V.\\;Zeitlin's beautiful model for the numerical discretization of Euler's equations in 2-D. When considered on the sphere, Zeitlin's model enables a deep connection between 2-D hydrodynamics and unitary representation theory of Lie algebras as pursued in quantum theory. Consequently, it provides a dictionary that maps hydrodynamical concepts to matrix Lie theory, which in turn gives connections to matrix factorizations, random matrices, and integrability theory, for example. The transferal of outcomes, from finite-dimensional matrices to infinite-dimensional fluids, is then supported by hard-fought results in quantization theory -- the field which describes the limit between quantum and classical physics. We demonstrate how the dictionary is constructed and how it unveils techniques for 2-D hydrodynamics. We also give accompanying convergence results for Zeitlin's model on the sphere.","sentences":["Two-dimensional (2-D) incompressible, inviscid fluids produce fascinating patterns of swirling motion.","How and why the patterns emerge are long-standing questions, first addressed in the 19th century by Helmholtz, Kirchhoff, and Kelvin.","Countless researchers have since contributed to innovative techniques and results, but the overarching problem of swirling 2-D motion and its long-time behavior remains largely open.","Here we advocate an alternative view-point that sheds light on this problem via a link to isospectral matrix flows.","The link is established through V.\\;Zeitlin's beautiful model for the numerical discretization of Euler's equations in 2-D. When considered on the sphere, Zeitlin's model enables a deep connection between 2-D hydrodynamics and unitary representation theory of Lie algebras as pursued in quantum theory.","Consequently, it provides a dictionary that maps hydrodynamical concepts to matrix Lie theory, which in turn gives connections to matrix factorizations, random matrices, and integrability theory, for example.","The transferal of outcomes, from finite-dimensional matrices to infinite-dimensional fluids, is then supported by hard-fought results in quantization theory -- the field which describes the limit between quantum and classical physics.","We demonstrate how the dictionary is constructed and how it unveils techniques for 2-D hydrodynamics.","We also give accompanying convergence results for Zeitlin's model on the sphere."],"url":"http://arxiv.org/abs/2405.14282v1","category":"math.AP"}
{"created":"2024-05-23 07:54:57","title":"ASI++: Towards Distributionally Balanced End-to-End Generative Retrieval","abstract":"Generative retrieval, a promising new paradigm in information retrieval, employs a seq2seq model to encode document features into parameters and decode relevant document identifiers (IDs) based on search queries. Existing generative retrieval solutions typically rely on a preprocessing stage to pre-define document IDs, which can suffer from a semantic gap between these IDs and the retrieval task. However, end-to-end training for both ID assignments and retrieval tasks is challenging due to the long-tailed distribution characteristics of real-world data, resulting in inefficient and unbalanced ID space utilization. To address these issues, we propose ASI++, a novel fully end-to-end generative retrieval method that aims to simultaneously learn balanced ID assignments and improve retrieval performance. ASI++ builds on the fully end-to-end training framework of vanilla ASI and introduces several key innovations. First, a distributionally balanced criterion addresses the imbalance in ID assignments, promoting more efficient utilization of the ID space. Next, a representation bottleneck criterion enhances dense representations to alleviate bottlenecks in learning ID assignments. Finally, an information consistency criterion integrates these processes into a joint optimization framework grounded in information theory. We further explore various module structures for learning ID assignments, including neural quantization, differentiable product quantization, and residual quantization. Extensive experiments on both public and industrial datasets demonstrate the effectiveness of ASI++ in improving retrieval performance and achieving balanced ID assignments.","sentences":["Generative retrieval, a promising new paradigm in information retrieval, employs a seq2seq model to encode document features into parameters and decode relevant document identifiers (IDs) based on search queries.","Existing generative retrieval solutions typically rely on a preprocessing stage to pre-define document IDs, which can suffer from a semantic gap between these IDs and the retrieval task.","However, end-to-end training for both ID assignments and retrieval tasks is challenging due to the long-tailed distribution characteristics of real-world data, resulting in inefficient and unbalanced ID space utilization.","To address these issues, we propose ASI++, a novel fully end-to-end generative retrieval method that aims to simultaneously learn balanced ID assignments and improve retrieval performance.","ASI++ builds on the fully end-to-end training framework of vanilla ASI and introduces several key innovations.","First, a distributionally balanced criterion addresses the imbalance in ID assignments, promoting more efficient utilization of the ID space.","Next, a representation bottleneck criterion enhances dense representations to alleviate bottlenecks in learning ID assignments.","Finally, an information consistency criterion integrates these processes into a joint optimization framework grounded in information theory.","We further explore various module structures for learning ID assignments, including neural quantization, differentiable product quantization, and residual quantization.","Extensive experiments on both public and industrial datasets demonstrate the effectiveness of ASI++ in improving retrieval performance and achieving balanced ID assignments."],"url":"http://arxiv.org/abs/2405.14280v1","category":"cs.IR"}
{"created":"2024-05-23 07:38:40","title":"Target Field Design of Surface Permanent Magnets","abstract":"We present a target field approach to analytically design magnetic fields using permanent magnets. We assume that their magnetisation is bound to a two-dimensional surface and is composed of a complete basis of surface modes. By posing the Poisson's equation relating the magnetic scalar potential to the magnetisation using Green's functions, we derive simple integrals which determine the magnetic field generated by each mode. This approach is demonstrated by deriving the governing integrals for optimising axial magnetisation on cylindrical and circular-planar surfaces. We approximate the governing integrals numerically and implement them into a regularised least-squares optimisation routine to design permanent magnets that generate uniform axial and transverse target magnetic fields. The resulting uniform axial magnetic field profiles demonstrate more than a tenfold increase in uniformity across equivalent target regions compared to the field generated by an optimally separated axially magnetised pair of rings, as validated using finite element method simulations. We use a simple example to examine how two-dimensional surface magnetisation profiles can be emulated using thin three-dimensional volumes and determine how many discrete intervals are required to accurately approximate a continuously-varying surface pattern. Magnets designed using our approach may enable higher quality bias fields for electric machines, nuclear fusion, fundamental physics, magnetic trapping, and beyond.","sentences":["We present a target field approach to analytically design magnetic fields using permanent magnets.","We assume that their magnetisation is bound to a two-dimensional surface and is composed of a complete basis of surface modes.","By posing the Poisson's equation relating the magnetic scalar potential to the magnetisation using Green's functions, we derive simple integrals which determine the magnetic field generated by each mode.","This approach is demonstrated by deriving the governing integrals for optimising axial magnetisation on cylindrical and circular-planar surfaces.","We approximate the governing integrals numerically and implement them into a regularised least-squares optimisation routine to design permanent magnets that generate uniform axial and transverse target magnetic fields.","The resulting uniform axial magnetic field profiles demonstrate more than a tenfold increase in uniformity across equivalent target regions compared to the field generated by an optimally separated axially magnetised pair of rings, as validated using finite element method simulations.","We use a simple example to examine how two-dimensional surface magnetisation profiles can be emulated using thin three-dimensional volumes and determine how many discrete intervals are required to accurately approximate a continuously-varying surface pattern.","Magnets designed using our approach may enable higher quality bias fields for electric machines, nuclear fusion, fundamental physics, magnetic trapping, and beyond."],"url":"http://arxiv.org/abs/2405.14258v1","category":"physics.app-ph"}
{"created":"2024-05-23 07:31:20","title":"Higher-Rank Irreducible Cartesian Tensors for Equivariant Message Passing","abstract":"The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences. By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost. The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections. In particular, the field has notably advanced with the emergence of equivariant message-passing architectures. Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding. This work introduces higher-rank irreducible Cartesian tensors as an alternative to spherical tensors, addressing the above limitations. We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance of the resulting layers. Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical models.","sentences":["The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences.","By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost.","The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections.","In particular, the field has notably advanced with the emergence of equivariant message-passing architectures.","Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding.","This work introduces higher-rank irreducible Cartesian tensors as an alternative to spherical tensors, addressing the above limitations.","We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance of the resulting layers.","Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical models."],"url":"http://arxiv.org/abs/2405.14253v1","category":"cs.LG"}
{"created":"2024-05-23 07:28:56","title":"Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors","abstract":"Diffusion or score-based models recently showed high performance in image generation. They rely on a forward and a backward stochastic differential equations (SDE). The sampling of a data distribution is achieved by solving numerically the backward SDE or its associated flow ODE. Studying the convergence of these models necessitates to control four different types of error: the initialization error, the truncation error, the discretization and the score approximation. In this paper, we study theoretically the behavior of diffusion models and their numerical implementation when the data distribution is Gaussian. In this restricted framework where the score function is a linear operator, we can derive the analytical solutions of the forward and backward SDEs as well as the associated flow ODE. This provides exact expressions for various Wasserstein errors which enable us to compare the influence of each error type for any sampling scheme, thus allowing to monitor convergence directly in the data space instead of relying on Inception features. Our experiments show that the recommended numerical schemes from the diffusion models literature are also the best sampling schemes for Gaussian distributions.","sentences":["Diffusion or score-based models recently showed high performance in image generation.","They rely on a forward and a backward stochastic differential equations (SDE).","The sampling of a data distribution is achieved by solving numerically the backward SDE or its associated flow ODE.","Studying the convergence of these models necessitates to control four different types of error: the initialization error, the truncation error, the discretization and the score approximation.","In this paper, we study theoretically the behavior of diffusion models and their numerical implementation when the data distribution is Gaussian.","In this restricted framework where the score function is a linear operator, we can derive the analytical solutions of the forward and backward SDEs as well as the associated flow ODE.","This provides exact expressions for various Wasserstein errors which enable us to compare the influence of each error type for any sampling scheme, thus allowing to monitor convergence directly in the data space instead of relying on Inception features.","Our experiments show that the recommended numerical schemes from the diffusion models literature are also the best sampling schemes for Gaussian distributions."],"url":"http://arxiv.org/abs/2405.14250v1","category":"cs.LG"}
{"created":"2024-05-23 07:26:05","title":"New identities for the Laplace, Glasser, and Widder potential transforms and their applications","abstract":"In this paper, we begin by applying the Laplace transform to derive closed forms for several challenging integrals that seem nearly impossible to evaluate. By utilizing the solution to the Pythagorean equation $a^2 + b^2 = c^2$, these closed forms become even more intriguing. This method allows us to provide new integral representations for the error function. Following this, we use the Fourier transform to derive formulas for the Glasser and Widder potential transforms, leading to several new and interesting corollaries. As part of the applications, we demonstrate the use of one of these integral formulas to provide a new real analytic proof of Euler's reflection formula for the gamma function. Of particular interest is a generalized integral involving the Riemann zeta function, which we also present as an application.","sentences":["In this paper, we begin by applying the Laplace transform to derive closed forms for several challenging integrals that seem nearly impossible to evaluate.","By utilizing the solution to the Pythagorean equation $a^2 + b^2 = c^2$, these closed forms become even more intriguing.","This method allows us to provide new integral representations for the error function.","Following this, we use the Fourier transform to derive formulas for the Glasser and Widder potential transforms, leading to several new and interesting corollaries.","As part of the applications, we demonstrate the use of one of these integral formulas to provide a new real analytic proof of Euler's reflection formula for the gamma function.","Of particular interest is a generalized integral involving the Riemann zeta function, which we also present as an application."],"url":"http://arxiv.org/abs/2405.14248v1","category":"math.GM"}
{"created":"2024-05-23 07:25:51","title":"Text-Based Correlation Matrix in Multi-Asset Allocation","abstract":"The purpose of this study is to estimate the correlation structure between multiple assets using financial text analysis. In recent years, as the background of elevating inflation in the global economy and monetary policy tightening by central banks, the correlation structure between assets, especially interest rate sensitivity and inflation sensitivity, has changed dramatically, increasing the impact on the performance of investors' portfolios. Therefore, the importance of estimating a robust correlation structure in portfolio management has increased. On the other hand, the correlation coefficient using only the historical price data observed in the financial market is accompanied by a certain degree of time lag, and also has the aspect that prediction errors can occur due to the nonstationarity of financial time series data, and that the interpretability from the viewpoint of fundamentals is a little poor when a phase change occurs. In this study, we performed natural language processing on news text and central bank text to verify the prediction accuracy of future correlation coefficient changes. As a result, it was suggested that this method is useful in comparison with the prediction from ordinary time series data.","sentences":["The purpose of this study is to estimate the correlation structure between multiple assets using financial text analysis.","In recent years, as the background of elevating inflation in the global economy and monetary policy tightening by central banks, the correlation structure between assets, especially interest rate sensitivity and inflation sensitivity, has changed dramatically, increasing the impact on the performance of investors' portfolios.","Therefore, the importance of estimating a robust correlation structure in portfolio management has increased.","On the other hand, the correlation coefficient using only the historical price data observed in the financial market is accompanied by a certain degree of time lag, and also has the aspect that prediction errors can occur due to the nonstationarity of financial time series data, and that the interpretability from the viewpoint of fundamentals is a little poor when a phase change occurs.","In this study, we performed natural language processing on news text and central bank text to verify the prediction accuracy of future correlation coefficient changes.","As a result, it was suggested that this method is useful in comparison with the prediction from ordinary time series data."],"url":"http://arxiv.org/abs/2405.14247v1","category":"cs.CL"}
{"created":"2024-05-23 07:24:50","title":"Simple Hamiltonian dynamics is a powerful quantum processing resource","abstract":"A quadrillion dimensional Hilbert space hosted by a quantum processor with over 50 physical qubits has been expected to be powerful enough to perform computational tasks ranging from simulations of many-body physics to complex financial modeling. Despite few examples and demonstrations, it is still not clear how we can utilize such a large Hilbert space as a computational resource; in particular, how a simple and small quantum system could solve non-trivial computational tasks. In this paper, we show a simple Ising model capable of performing such non-trivial computational tasks in a quantum neural network model. An Ising spin chain as small as ten qubits can solve a practical image classification task with high accuracy. To evaluate the mechanism of its computation, we examine how the symmetries of the Hamiltonian would affect its computational power. We show how the interplay between complexity and integrability/symmetries of the quantum system dictates the performance as quantum neural network.","sentences":["A quadrillion dimensional Hilbert space hosted by a quantum processor with over 50 physical qubits has been expected to be powerful enough to perform computational tasks ranging from simulations of many-body physics to complex financial modeling.","Despite few examples and demonstrations, it is still not clear how we can utilize such a large Hilbert space as a computational resource; in particular, how a simple and small quantum system could solve non-trivial computational tasks.","In this paper, we show a simple Ising model capable of performing such non-trivial computational tasks in a quantum neural network model.","An Ising spin chain as small as ten qubits can solve a practical image classification task with high accuracy.","To evaluate the mechanism of its computation, we examine how the symmetries of the Hamiltonian would affect its computational power.","We show how the interplay between complexity and integrability/symmetries of the quantum system dictates the performance as quantum neural network."],"url":"http://arxiv.org/abs/2405.14245v1","category":"quant-ph"}
{"created":"2024-05-23 06:22:12","title":"Emergence of spatial patterns and synchronization in superconducting time crystals","abstract":"We identify a time crystal phase characterized by a frequency half of the driving frequency in disordered superconductors by employing the time dependent Bogoliubov-de Gennes formalism at zero temperature with a periodically driven coupling constant. After a period of exponential increase of spatial inhomogeneities and exponential suppression of the order parameter amplitude, the time crystal develops islands of different sizes. Each of these islands is a time crystal with the same frequency albeit with a phase shift $\\pi$ with respect to the homogeneous time crystal. After its emergence, the island gradually becomes smaller, though the phase shift persists, until it is abruptly synchronized at a time that it depends on its initial size. We find a critical disorder strength, still deep in the metallic phase, at which the time crystal phase terminates. For even stronger disorder, the order parameter oscillates with the driving frequency in regions where localization effects are not important.","sentences":["We identify a time crystal phase characterized by a frequency half of the driving frequency in disordered superconductors by employing the time dependent Bogoliubov-de Gennes formalism at zero temperature with a periodically driven coupling constant.","After a period of exponential increase of spatial inhomogeneities and exponential suppression of the order parameter amplitude, the time crystal develops islands of different sizes.","Each of these islands is a time crystal with the same frequency albeit with a phase shift $\\pi$ with respect to the homogeneous time crystal.","After its emergence, the island gradually becomes smaller, though the phase shift persists, until it is abruptly synchronized at a time that it depends on its initial size.","We find a critical disorder strength, still deep in the metallic phase, at which the time crystal phase terminates.","For even stronger disorder, the order parameter oscillates with the driving frequency in regions where localization effects are not important."],"url":"http://arxiv.org/abs/2405.14216v1","category":"cond-mat.supr-con"}
{"created":"2024-05-23 06:21:16","title":"Exact finite-size scaling spectra in the quenched and annealed Sherrington-Kirkpatrick spin glass","abstract":"Fine resolution of the discrete eigenvalues at the spectral edge of an $N\\times N$ random matrix is required in many applications. Starting from a finite-size scaling ansatz for the Stieltjes transform of the maximum likelihood spectrum, we show that the scaling function satisfies a first-order ODE of the Riccati type. Further transformation yields a linear second-order ODE for the characteristic function, whose nodes yield the exact locations of leading eigenvalues. With this new technique, we examine in detail the spectral crossover of the annealed Sherrington-Kirkpatrick (SK) spin glass model where a gap develops below a certain critical temperature. Our analysis yields analytic predictions for the finite-size scaling of the spin condensation phenomenon in both quenched and annealed SK models, which are then compared and validated in Monte Carlo simulations of the annealed model. More generally, rescaling of the spectral axis, adjusted to the distance of neighboring eigenvalues, offers a powerful strategy to deal with singularities that arise in the infinite size limit.","sentences":["Fine resolution of the discrete eigenvalues at the spectral edge of an $N\\times N$ random matrix is required in many applications.","Starting from a finite-size scaling ansatz for the Stieltjes transform of the maximum likelihood spectrum, we show that the scaling function satisfies a first-order ODE of the Riccati type.","Further transformation yields a linear second-order ODE for the characteristic function, whose nodes yield the exact locations of leading eigenvalues.","With this new technique, we examine in detail the spectral crossover of the annealed Sherrington-Kirkpatrick (SK) spin glass model where a gap develops below a certain critical temperature.","Our analysis yields analytic predictions for the finite-size scaling of the spin condensation phenomenon in both quenched and annealed SK models, which are then compared and validated in Monte Carlo simulations of the annealed model.","More generally, rescaling of the spectral axis, adjusted to the distance of neighboring eigenvalues, offers a powerful strategy to deal with singularities that arise in the infinite size limit."],"url":"http://arxiv.org/abs/2405.14215v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-23 06:14:35","title":"Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data","abstract":"As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning scenarios, directly using external LLMs on private data is strictly prohibited by stringent data security and privacy regulations. For local clients, the utilization of LLMs to improve the domain-specific small language models (SLMs), characterized by limited computational resources and domain-specific data, has attracted considerable research attention. By observing that LLMs can empower domain-specific SLMs, existing methods predominantly concentrate on leveraging the public data or LLMs to generate more data to transfer knowledge from LLMs to SLMs. However, due to the discrepancies between LLMs' generated data and clients' domain-specific data, these methods cannot yield substantial improvements in the domain-specific tasks. In this paper, we introduce a Federated Domain-specific Knowledge Transfer (FDKT) framework, which enables domain-specific knowledge transfer from LLMs to SLMs while preserving clients' data privacy. The core insight is to leverage LLMs to augment data based on domain-specific few-shot demonstrations, which are synthesized from private domain data using differential privacy. Such synthetic samples share similar data distribution with clients' private data and allow the server LLM to generate particular knowledge to improve clients' SLMs. The extensive experimental results demonstrate that the proposed FDKT framework consistently and greatly improves SLMs' task performance by around 5\\% with a privacy budget of less than 10, compared to local training on private data.","sentences":["As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications.","When it comes to sensitive domains, as commonly described in federated learning scenarios, directly using external LLMs on private data is strictly prohibited by stringent data security and privacy regulations.","For local clients, the utilization of LLMs to improve the domain-specific small language models (SLMs), characterized by limited computational resources and domain-specific data, has attracted considerable research attention.","By observing that LLMs can empower domain-specific SLMs, existing methods predominantly concentrate on leveraging the public data or LLMs to generate more data to transfer knowledge from LLMs to SLMs.","However, due to the discrepancies between LLMs' generated data and clients' domain-specific data, these methods cannot yield substantial improvements in the domain-specific tasks.","In this paper, we introduce a Federated Domain-specific Knowledge Transfer (FDKT) framework, which enables domain-specific knowledge transfer from LLMs to SLMs while preserving clients' data privacy.","The core insight is to leverage LLMs to augment data based on domain-specific few-shot demonstrations, which are synthesized from private domain data using differential privacy.","Such synthetic samples share similar data distribution with clients' private data and allow the server LLM to generate particular knowledge to improve clients' SLMs.","The extensive experimental results demonstrate that the proposed FDKT framework consistently and greatly improves SLMs' task performance by around 5\\% with a privacy budget of less than 10, compared to local training on private data."],"url":"http://arxiv.org/abs/2405.14212v1","category":"cs.CR"}
{"created":"2024-05-23 05:39:14","title":"Differential conductance and noise spectrum in a quantum dot coupled to a superconducting nanowire","abstract":"We investigate the quantum transport through a quantum dot coupled with a superconducting (SC) nanowire. The distinguishing characteristics between the Majorana bound states (MBSs) and Andreev bound states (ABSs) hosted in SC wire are demonstrated in the differential conductance and finite-frequency current noise spectrum. The former shows the well-known zero-bias peak (ZBP), while the latter displays the degenerate Rabi dip (DRD) for MBSs with topological quality factor q = 1. However, for ABSs with q < 1, the ZBP and DRD split. We confirm that the ABSs with high quality factors greater than critical values can mimic the signatures of topological MBSs. Especially, the corresponding critical quality factors for the occurrence of ZBP and DRD are explicitly obtained and their underlying mechanisms are addressed.","sentences":["We investigate the quantum transport through a quantum dot coupled with a superconducting (SC) nanowire.","The distinguishing characteristics between the Majorana bound states (MBSs) and Andreev bound states (ABSs) hosted in SC wire are demonstrated in the differential conductance and finite-frequency current noise spectrum.","The former shows the well-known zero-bias peak (ZBP), while the latter displays the degenerate Rabi dip (DRD) for MBSs with topological quality factor q = 1.","However, for ABSs with q < 1, the ZBP and DRD split.","We confirm that the ABSs with high quality factors greater than critical values can mimic the signatures of topological MBSs.","Especially, the corresponding critical quality factors for the occurrence of ZBP and DRD are explicitly obtained and their underlying mechanisms are addressed."],"url":"http://arxiv.org/abs/2405.14193v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 05:33:16","title":"Strangelets at finite temperature","abstract":"We study the properties of strangelets at finite temperature $T$, employing an equivparticle model that incorporates both linear confinement and leading-order perturbative interactions with density-dependent quark masses. The shell effects are analyzed by solving the Dirac equations for quarks within the mean-field approximation. As temperature increases, these effects weaken due to the occupation probability of single-particle levels being governed by the Fermi-Dirac statistics, a phenomenon known as shell dampening. Surprisingly, the surface tension, derived from a liquid-drop formula, does not decrease with temperature but instead rises until it peaks at $T \\approx 20-40$ MeV. At this temperature, shell corrections become negligible, and the formula provides a reasonable approximation for the free energy per baryon of strangelets. However, the curvature term decreases with $T$ despite the presence of shell effects. The neutron and proton emission rates are determined microscopically by the external nucleon gas densities that are in equilibrium with strangelets. These emission rate generally increases with $T$ for stable strangelets, but decrease for those that are unstable to nucleon emission at $T$ = 0. The other properties of $\\beta$-stable strangelets obtained with various parameter sets are presented as well. The results indicated in this work are useful for understanding the products of binary compact star mergers and heavy-ion collisions.","sentences":["We study the properties of strangelets at finite temperature $T$, employing an equivparticle model that incorporates both linear confinement and leading-order perturbative interactions with density-dependent quark masses.","The shell effects are analyzed by solving the Dirac equations for quarks within the mean-field approximation.","As temperature increases, these effects weaken due to the occupation probability of single-particle levels being governed by the Fermi-Dirac statistics, a phenomenon known as shell dampening.","Surprisingly, the surface tension, derived from a liquid-drop formula, does not decrease with temperature but instead rises until it peaks at $T \\approx 20-40$ MeV. At this temperature, shell corrections become negligible, and the formula provides a reasonable approximation for the free energy per baryon of strangelets.","However, the curvature term decreases with $T$ despite the presence of shell effects.","The neutron and proton emission rates are determined microscopically by the external nucleon gas densities that are in equilibrium with strangelets.","These emission rate generally increases with $T$ for stable strangelets, but decrease for those that are unstable to nucleon emission at $T$ = 0.","The other properties of $\\beta$-stable strangelets obtained with various parameter sets are presented as well.","The results indicated in this work are useful for understanding the products of binary compact star mergers and heavy-ion collisions."],"url":"http://arxiv.org/abs/2405.14190v1","category":"hep-ph"}
{"created":"2024-05-23 05:15:06","title":"Odd-frequency pairing of Bogoliubov quasiparticles in superconductor junction","abstract":"We study a superconductor Josephson junction with a Bogoliubov Fermi surface, employing McMillan's Green's function technique. The low-energy degrees of freedom are described by spinless fermions (bogolons), where the characteristic feature appears as an odd-frequency pair potential. The differential equation of the Green's function is reduced to the eigenvalue problem of the non-Hermitian effective Hamiltonian. The physical quantities such as the density of states and pair amplitude are then extracted from the obtained Green's function. We find that the zero energy local density of states at the interface decreases as the relative phase of the Josephson junction increases. This decrease is accompanied by the generation of an even-frequency pair amplitude near the interface. We also clarify that the $\\pi$-junction-like current phase relation is realized in terms of bogolons. In contrast to conventional $s$-wave superconductor junctions, where even-frequency pairs dominate in the bulk and odd-frequency pairs are generated near the interface, our findings illuminate the distinct behaviors of junctions with Bogoliubov Fermi surfaces. We further explore spatial dependences of these physical quantities systematically using quasiclassical Green's functions.","sentences":["We study a superconductor Josephson junction with a Bogoliubov Fermi surface, employing McMillan's Green's function technique.","The low-energy degrees of freedom are described by spinless fermions (bogolons), where the characteristic feature appears as an odd-frequency pair potential.","The differential equation of the Green's function is reduced to the eigenvalue problem of the non-Hermitian effective Hamiltonian.","The physical quantities such as the density of states and pair amplitude are then extracted from the obtained Green's function.","We find that the zero energy local density of states at the interface decreases as the relative phase of the Josephson junction increases.","This decrease is accompanied by the generation of an even-frequency pair amplitude near the interface.","We also clarify that the $\\pi$-junction-like current phase relation is realized in terms of bogolons.","In contrast to conventional $s$-wave superconductor junctions, where even-frequency pairs dominate in the bulk and odd-frequency pairs are generated near the interface, our findings illuminate the distinct behaviors of junctions with Bogoliubov Fermi surfaces.","We further explore spatial dependences of these physical quantities systematically using quasiclassical Green's functions."],"url":"http://arxiv.org/abs/2405.14181v1","category":"cond-mat.supr-con"}
{"created":"2024-05-23 04:30:56","title":"Relaxing the sharp density stratification and columnar motion assumptions in layered shallow water systems","abstract":"We rigorously justify the bilayer shallow-water system as an approximation to the hydrostatic Euler equations in situations where the flow is density-stratified with close-to-piecewise constant density profiles, and close-to-columnar velocity profiles. Our theory accommodates with continuous stratification, so that admissible deviations from bilayer profiles are not pointwise small. This leads us to define refined approximate solutions that are able to describe at first order the flow in the pycnocline. Because the hydrostatic Euler equations are not known to enjoy suitable stability estimates, we rely on thickness-diffusivity contributions proposed by Gent and McWilliams. Our strategy also applies to one-layer and multilayer frameworks.","sentences":["We rigorously justify the bilayer shallow-water system as an approximation to the hydrostatic Euler equations in situations where the flow is density-stratified with close-to-piecewise constant density profiles, and close-to-columnar velocity profiles.","Our theory accommodates with continuous stratification, so that admissible deviations from bilayer profiles are not pointwise small.","This leads us to define refined approximate solutions that are able to describe at first order the flow in the pycnocline.","Because the hydrostatic Euler equations are not known to enjoy suitable stability estimates, we rely on thickness-diffusivity contributions proposed by Gent and McWilliams.","Our strategy also applies to one-layer and multilayer frameworks."],"url":"http://arxiv.org/abs/2405.14164v1","category":"math.AP"}
{"created":"2024-05-23 03:57:09","title":"Mutations of ordinary torsion theories and generalized torsion theories connected by a Serre subcategory","abstract":"Understanding how torsion theories are described and constructed is crucial to the study of torsion theory. Mutations of torsion theories have been studied as a method of constructing another torsion theory from a given one. We have already obtained how to mutate ordinary torsion theories into generalized torsion theories associated with a Serre subcategory. The paper investigates when the generalized torsion theories give ordinary torsion theories and when ordinary torsion theories provide each other.","sentences":["Understanding how torsion theories are described and constructed is crucial to the study of torsion theory.","Mutations of torsion theories have been studied as a method of constructing another torsion theory from a given one.","We have already obtained how to mutate ordinary torsion theories into generalized torsion theories associated with a Serre subcategory.","The paper investigates when the generalized torsion theories give ordinary torsion theories and when ordinary torsion theories provide each other."],"url":"http://arxiv.org/abs/2405.14152v1","category":"math.AC"}
{"created":"2024-05-23 03:28:52","title":"Contribute to balance, wire in accordance: Emergence of backpropagation from a simple, bio-plausible neuroplasticity rule","abstract":"Backpropagation (BP) has been pivotal in advancing machine learning and remains essential in computational applications and comparative studies of biological and artificial neural networks. Despite its widespread use, the implementation of BP in the brain remains elusive, and its biological plausibility is often questioned due to inherent issues such as the need for symmetry of weights between forward and backward connections, and the requirement of distinct forward and backward phases of computation. Here, we introduce a novel neuroplasticity rule that offers a potential mechanism for implementing BP in the brain. Similar in general form to the classical Hebbian rule, this rule is based on the core principles of maintaining the balance of excitatory and inhibitory inputs as well as on retrograde signaling, and operates over three progressively slower timescales: neural firing, retrograde signaling, and neural plasticity. We hypothesize that each neuron possesses an internal state, termed credit, in addition to its firing rate. After achieving equilibrium in firing rates, neurons receive credits based on their contribution to the E-I balance of postsynaptic neurons through retrograde signaling. As the network's credit distribution stabilizes, connections from those presynaptic neurons are strengthened that significantly contribute to the balance of postsynaptic neurons. We demonstrate mathematically that our learning rule precisely replicates BP in layered neural networks without any approximations. Simulations on artificial neural networks reveal that this rule induces varying community structures in networks, depending on the learning rate. This simple theoretical framework presents a biologically plausible implementation of BP, with testable assumptions and predictions that may be evaluated through biological experiments.","sentences":["Backpropagation (BP) has been pivotal in advancing machine learning and remains essential in computational applications and comparative studies of biological and artificial neural networks.","Despite its widespread use, the implementation of BP in the brain remains elusive, and its biological plausibility is often questioned due to inherent issues such as the need for symmetry of weights between forward and backward connections, and the requirement of distinct forward and backward phases of computation.","Here, we introduce a novel neuroplasticity rule that offers a potential mechanism for implementing BP in the brain.","Similar in general form to the classical Hebbian rule, this rule is based on the core principles of maintaining the balance of excitatory and inhibitory inputs as well as on retrograde signaling, and operates over three progressively slower timescales: neural firing, retrograde signaling, and neural plasticity.","We hypothesize that each neuron possesses an internal state, termed credit, in addition to its firing rate.","After achieving equilibrium in firing rates, neurons receive credits based on their contribution to the E-I balance of postsynaptic neurons through retrograde signaling.","As the network's credit distribution stabilizes, connections from those presynaptic neurons are strengthened that significantly contribute to the balance of postsynaptic neurons.","We demonstrate mathematically that our learning rule precisely replicates BP in layered neural networks without any approximations.","Simulations on artificial neural networks reveal that this rule induces varying community structures in networks, depending on the learning rate.","This simple theoretical framework presents a biologically plausible implementation of BP, with testable assumptions and predictions that may be evaluated through biological experiments."],"url":"http://arxiv.org/abs/2405.14139v1","category":"q-bio.NC"}
{"created":"2024-05-23 03:19:23","title":"Efficient Multitask Dense Predictor via Binarization","abstract":"Multi-task learning for dense prediction has emerged as a pivotal area in computer vision, enabling simultaneous processing of diverse yet interrelated pixel-wise prediction tasks. However, the substantial computational demands of state-of-the-art (SoTA) models often limit their widespread deployment. This paper addresses this challenge by introducing network binarization to compress resource-intensive multi-task dense predictors. Specifically, our goal is to significantly accelerate multi-task dense prediction models via Binary Neural Networks (BNNs) while maintaining and even improving model performance at the same time. To reach this goal, we propose a Binary Multi-task Dense Predictor, Bi-MTDP, and several variants of Bi-MTDP, in which a multi-task dense predictor is constructed via specified binarized modules. Our systematical analysis of this predictor reveals that performance drop from binarization is primarily caused by severe information degradation. To address this issue, we introduce a deep information bottleneck layer that enforces representations for downstream tasks satisfying Gaussian distribution in forward propagation. Moreover, we introduce a knowledge distillation mechanism to correct the direction of information flow in backward propagation. Intriguingly, one variant of Bi-MTDP outperforms full-precision (FP) multi-task dense prediction SoTAs, ARTC (CNN-based) and InvPT (ViT-Based). This result indicates that Bi-MTDP is not merely a naive trade-off between performance and efficiency, but is rather a benefit of the redundant information flow thanks to the multi-task architecture. Code is available at https://github.com/42Shawn/BiMTDP.","sentences":["Multi-task learning for dense prediction has emerged as a pivotal area in computer vision, enabling simultaneous processing of diverse yet interrelated pixel-wise prediction tasks.","However, the substantial computational demands of state-of-the-art (SoTA) models often limit their widespread deployment.","This paper addresses this challenge by introducing network binarization to compress resource-intensive multi-task dense predictors.","Specifically, our goal is to significantly accelerate multi-task dense prediction models via Binary Neural Networks (BNNs) while maintaining and even improving model performance at the same time.","To reach this goal, we propose a Binary Multi-task Dense Predictor, Bi-MTDP, and several variants of Bi-MTDP, in which a multi-task dense predictor is constructed via specified binarized modules.","Our systematical analysis of this predictor reveals that performance drop from binarization is primarily caused by severe information degradation.","To address this issue, we introduce a deep information bottleneck layer that enforces representations for downstream tasks satisfying Gaussian distribution in forward propagation.","Moreover, we introduce a knowledge distillation mechanism to correct the direction of information flow in backward propagation.","Intriguingly, one variant of Bi-MTDP outperforms full-precision (FP) multi-task dense prediction SoTAs, ARTC (CNN-based) and InvPT (ViT-Based).","This result indicates that Bi-MTDP is not merely a naive trade-off between performance and efficiency, but is rather a benefit of the redundant information flow thanks to the multi-task architecture.","Code is available at https://github.com/42Shawn/BiMTDP."],"url":"http://arxiv.org/abs/2405.14136v1","category":"cs.CV"}
{"created":"2024-05-23 03:01:22","title":"Non-unique solutions for electron MHD","abstract":"We consider the electron magnetohydrodynamics (MHD) equation on the 3D torus $\\mathbb T^3$. For a given smooth vector field $H$ with zero mean and zero divergence, we can construct a weak solution $B$ to the electron MHD in the space $L^\\gamma_tW^{1,p}_x$ for appropriate $(\\gamma, p)$ such that $B$ is arbitrarily close to $H$ in this space. The parameters $\\gamma$ and $p$ depend on the resistivity. As a consequence, non-uniqueness of weak solutions is obtained for the electron MHD with hyper-resistivity. In particular, non-Leray-Hopf solutions can be constructed. As a byproduct, we also show the existence of weak solutions to the electron MHD without resistivity.","sentences":["We consider the electron magnetohydrodynamics (MHD) equation on the 3D torus $\\mathbb T^3$. For a given smooth vector field $H$ with zero mean and zero divergence, we can construct a weak solution $B$ to the electron MHD in the space $L^\\gamma_tW^{1,p}_x$ for appropriate $(\\gamma, p)$ such that $B$ is arbitrarily close to $H$ in this space.","The parameters $\\gamma$ and $p$ depend on the resistivity.","As a consequence, non-uniqueness of weak solutions is obtained for the electron MHD with hyper-resistivity.","In particular, non-Leray-Hopf solutions can be constructed.","As a byproduct, we also show the existence of weak solutions to the electron MHD without resistivity."],"url":"http://arxiv.org/abs/2405.14127v1","category":"math.AP"}
{"created":"2024-05-23 02:49:57","title":"Equations for the overlaps of a SIC","abstract":"We give a holomorphic quartic polynomial in the overlap variables whose zeros on the torus are precisely the Weyl-Heisenberg SICs (symmetric informationally complete positive operator valued measures). By way of comparison, all the other known systems of equations that determine a Weyl-Heisenberg SIC involve variables and their complex conjugates. We also give a related interesting result about the powers of the projective Fourier transform of the group G = Z d x Z d .","sentences":["We give a holomorphic quartic polynomial in the overlap variables whose zeros on the torus are precisely the Weyl-Heisenberg SICs (symmetric informationally complete positive operator valued measures).","By way of comparison, all the other known systems of equations that determine a Weyl-Heisenberg SIC involve variables and their complex conjugates.","We also give a related interesting result about the powers of the projective Fourier transform of the group G = Z d","x Z d ."],"url":"http://arxiv.org/abs/2405.14123v1","category":"cs.IT"}
{"created":"2024-05-23 02:36:26","title":"The stochastic Landau--Lifshitz--Baryakhtar equation: Global solution and invariant measure","abstract":"The Landau--Lifshitz--Baryakhtar (LLBar) equation perturbed by a space-dependent noise is a system of fourth order stochastic PDEs which models the evolution of magnetic spin fields in ferromagnetic materials at elevated temperatures, taking into account longitudinal damping, long-range interactions, and noise-induced phenomena at high temperatures. In this paper, we show the existence of a martingale solution (which is analytically strong) to the stochastic LLBar equation posed in a bounded domain $\\mathscr{D}\\subset \\mathbb{R}^d$, where $d=1,2,3$. We also prove pathwise uniqueness of the solution, which implies the existence of a unique probabilistically strong solution. Finally, we show the Feller property of the Markov semigroup associated with the strong solution, which implies the existence of invariant measures.","sentences":["The Landau--Lifshitz--Baryakhtar (LLBar) equation perturbed by a space-dependent noise is a system of fourth order stochastic PDEs which models the evolution of magnetic spin fields in ferromagnetic materials at elevated temperatures, taking into account longitudinal damping, long-range interactions, and noise-induced phenomena at high temperatures.","In this paper, we show the existence of a martingale solution (which is analytically strong) to the stochastic LLBar equation posed in a bounded domain $\\mathscr{D}\\subset \\mathbb{R}^d$, where $d=1,2,3$. We also prove pathwise uniqueness of the solution, which implies the existence of a unique probabilistically strong solution.","Finally, we show the Feller property of the Markov semigroup associated with the strong solution, which implies the existence of invariant measures."],"url":"http://arxiv.org/abs/2405.14112v1","category":"math.PR"}
{"created":"2024-05-23 02:31:53","title":"Regularity-Conforming Neural Networks (ReCoNNs) for solving Partial Differential Equations","abstract":"Whilst the Universal Approximation Theorem guarantees the existence of approximations to Sobolev functions -- the natural function spaces for PDEs -- by Neural Networks (NNs) of sufficient size, low-regularity solutions may lead to poor approximations in practice. For example, classical fully-connected feed-forward NNs fail to approximate continuous functions whose gradient is discontinuous when employing strong formulations like in Physics Informed Neural Networks (PINNs). In this article, we propose the use of regularity-conforming neural networks, where a priori information on the regularity of solutions to PDEs can be employed to construct proper architectures. We illustrate the potential of such architectures via a two-dimensional (2D) transmission problem, where the solution may admit discontinuities in the gradient across interfaces, as well as power-like singularities at certain points. In particular, we formulate the weak transmission problem in a PINNs-like strong formulation with interface and continuity conditions. Such architectures are partially explainable; discontinuities are explicitly described, allowing the introduction of novel terms into the loss function. We demonstrate via several model problems in one and two dimensions the advantages of using regularity-conforming architectures in contrast to classical architectures. The ideas presented in this article easily extend to problems in higher dimensions.","sentences":["Whilst the Universal Approximation Theorem guarantees the existence of approximations to Sobolev functions -- the natural function spaces for PDEs -- by Neural Networks (NNs) of sufficient size, low-regularity solutions may lead to poor approximations in practice.","For example, classical fully-connected feed-forward NNs fail to approximate continuous functions whose gradient is discontinuous when employing strong formulations like in Physics Informed Neural Networks (PINNs).","In this article, we propose the use of regularity-conforming neural networks, where a priori information on the regularity of solutions to PDEs can be employed to construct proper architectures.","We illustrate the potential of such architectures via a two-dimensional (2D) transmission problem, where the solution may admit discontinuities in the gradient across interfaces, as well as power-like singularities at certain points.","In particular, we formulate the weak transmission problem in a PINNs-like strong formulation with interface and continuity conditions.","Such architectures are partially explainable; discontinuities are explicitly described, allowing the introduction of novel terms into the loss function.","We demonstrate via several model problems in one and two dimensions the advantages of using regularity-conforming architectures in contrast to classical architectures.","The ideas presented in this article easily extend to problems in higher dimensions."],"url":"http://arxiv.org/abs/2405.14110v1","category":"math.NA"}
{"created":"2024-05-23 02:24:52","title":"Nearly Tight Black-Box Auditing of Differentially Private Machine Learning","abstract":"This paper presents a nearly tight audit of the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm in the black-box model. Our auditing procedure empirically estimates the privacy leakage from DP-SGD using membership inference attacks; unlike prior work, the estimates are appreciably close to the theoretical DP bounds. The main intuition is to craft worst-case initial model parameters, as DP-SGD's privacy analysis is agnostic to the choice of the initial model parameters. For models trained with theoretical $\\varepsilon=10.0$ on MNIST and CIFAR-10, our auditing procedure yields empirical estimates of $7.21$ and $6.95$, respectively, on 1,000-record samples and $6.48$ and $4.96$ on the full datasets. By contrast, previous work achieved tight audits only in stronger (i.e., less realistic) white-box models that allow the adversary to access the model's inner parameters and insert arbitrary gradients. Our auditing procedure can be used to detect bugs and DP violations more easily and offers valuable insight into how the privacy analysis of DP-SGD can be further improved.","sentences":["This paper presents a nearly tight audit of the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm in the black-box model.","Our auditing procedure empirically estimates the privacy leakage from DP-SGD using membership inference attacks; unlike prior work, the estimates are appreciably close to the theoretical DP bounds.","The main intuition is to craft worst-case initial model parameters, as DP-SGD's privacy analysis is agnostic to the choice of the initial model parameters.","For models trained with theoretical $\\varepsilon=10.0$ on MNIST and CIFAR-10, our auditing procedure yields empirical estimates of $7.21$ and $6.95$, respectively, on 1,000-record samples and $6.48$ and $4.96$ on the full datasets.","By contrast, previous work achieved tight audits only in stronger (i.e., less realistic) white-box models that allow the adversary to access the model's inner parameters and insert arbitrary gradients.","Our auditing procedure can be used to detect bugs and DP violations more easily and offers valuable insight into how the privacy analysis of DP-SGD can be further improved."],"url":"http://arxiv.org/abs/2405.14106v1","category":"cs.CR"}
{"created":"2024-05-23 02:01:05","title":"Automatic Differentiation is Essential in Training Neural Networks for Solving Differential Equations","abstract":"Neural network-based approaches have recently shown significant promise in solving partial differential equations (PDEs) in science and engineering, especially in scenarios featuring complex domains or the incorporation of empirical data. One advantage of the neural network method for PDEs lies in its automatic differentiation (AD), which necessitates only the sample points themselves, unlike traditional finite difference (FD) approximations that require nearby local points to compute derivatives. In this paper, we quantitatively demonstrate the advantage of AD in training neural networks. The concept of truncated entropy is introduced to characterize the training property. Specifically, through comprehensive experimental and theoretical analyses conducted on random feature models and two-layer neural networks, we discover that the defined truncated entropy serves as a reliable metric for quantifying the residual loss of random feature models and the training speed of neural networks for both AD and FD methods. Our experimental and theoretical analyses demonstrate that, from a training perspective, AD outperforms FD in solving partial differential equations.","sentences":["Neural network-based approaches have recently shown significant promise in solving partial differential equations (PDEs) in science and engineering, especially in scenarios featuring complex domains or the incorporation of empirical data.","One advantage of the neural network method for PDEs lies in its automatic differentiation (AD), which necessitates only the sample points themselves, unlike traditional finite difference (FD) approximations that require nearby local points to compute derivatives.","In this paper, we quantitatively demonstrate the advantage of AD in training neural networks.","The concept of truncated entropy is introduced to characterize the training property.","Specifically, through comprehensive experimental and theoretical analyses conducted on random feature models and two-layer neural networks, we discover that the defined truncated entropy serves as a reliable metric for quantifying the residual loss of random feature models and the training speed of neural networks for both AD and FD methods.","Our experimental and theoretical analyses demonstrate that, from a training perspective, AD outperforms FD in solving partial differential equations."],"url":"http://arxiv.org/abs/2405.14099v1","category":"cs.LG"}
{"created":"2024-05-23 01:52:54","title":"Newton Informed Neural Operator for Computing Multiple Solutions of Nonlinear Partials Differential Equations","abstract":"Solving nonlinear partial differential equations (PDEs) with multiple solutions using neural networks has found widespread applications in various fields such as physics, biology, and engineering. However, classical neural network methods for solving nonlinear PDEs, such as Physics-Informed Neural Networks (PINN), Deep Ritz methods, and DeepONet, often encounter challenges when confronted with the presence of multiple solutions inherent in the nonlinear problem. These methods may encounter ill-posedness issues. In this paper, we propose a novel approach called the Newton Informed Neural Operator, which builds upon existing neural network techniques to tackle nonlinearities. Our method combines classical Newton methods, addressing well-posed problems, and efficiently learns multiple solutions in a single learning process while requiring fewer supervised data points compared to existing neural network methods.","sentences":["Solving nonlinear partial differential equations (PDEs) with multiple solutions using neural networks has found widespread applications in various fields such as physics, biology, and engineering.","However, classical neural network methods for solving nonlinear PDEs, such as Physics-Informed Neural Networks (PINN), Deep Ritz methods, and DeepONet, often encounter challenges when confronted with the presence of multiple solutions inherent in the nonlinear problem.","These methods may encounter ill-posedness issues.","In this paper, we propose a novel approach called the Newton Informed Neural Operator, which builds upon existing neural network techniques to tackle nonlinearities.","Our method combines classical Newton methods, addressing well-posed problems, and efficiently learns multiple solutions in a single learning process while requiring fewer supervised data points compared to existing neural network methods."],"url":"http://arxiv.org/abs/2405.14096v1","category":"cs.LG"}
{"created":"2024-05-23 01:43:54","title":"A Survey on Vision-Language-Action Models for Embodied AI","abstract":"Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multi-modality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components through pretraining. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.","sentences":["Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning.","Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks.","Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition.","The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs).","Their multi-modality capability has become a foundational element in robot learning.","Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability.","Some models focus on refining specific components through pretraining.","Others aim to develop control policies adept at predicting low-level actions.","Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks.","Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI.","Therefore, it is imperative to capture the evolving landscape through a comprehensive survey."],"url":"http://arxiv.org/abs/2405.14093v1","category":"cs.RO"}
{"created":"2024-05-23 01:22:24","title":"Classification of Lagrangian translators and Lagrangian self-expanders in $\\mathbb{C}^{2}$","abstract":"In this paper, we obtain several classification results of $2$-dimensional complete Lagrangian translators and lagrangian self-expanders with constant squared norm $|\\vec{H}|^{2}$ of the mean curvature vector in $\\mathbb{C}^{2}$ by using a new Omori-Yau type maximum principle which was proved by Chen and Qiu \\cite{CQ}. The same idea is also used to give a similar result of Lagrangian $\\xi$-translators in $\\mathbb{C}^{2}$.","sentences":["In this paper, we obtain several classification results of $2$-dimensional complete Lagrangian translators and lagrangian self-expanders with constant squared norm $|\\vec{H}|^{2}$ of the mean curvature vector in $\\mathbb{C}^{2}$ by using a new Omori-Yau type maximum principle which was proved by Chen and Qiu \\cite{CQ}.","The same idea is also used to give a similar result of Lagrangian $\\xi$-translators in $\\mathbb{C}^{2}$."],"url":"http://arxiv.org/abs/2405.14086v1","category":"math.DG"}
{"created":"2024-05-23 00:36:45","title":"Enhancing Critical Infrastructure Cybersecurity: Collaborative DNN Synthesis in the Cloud Continuum","abstract":"Researchers are exploring the integration of IoT and the cloud continuum, together with AI to enhance the cost-effectiveness and efficiency of critical infrastructure (CI) systems. This integration, however, increases susceptibility of CI systems to cyberattacks, potentially leading to disruptions like power outages, oil spills, or even a nuclear mishap. CI systems are inherently complex and generate vast amounts of heterogeneous and high-dimensional data, which crosses many trust boundaries in their journey across the IoT, edge, and cloud domains over the communication network interconnecting them. As a result, they face expanded attack surfaces. To ensure the security of these dataflows, researchers have used deep neural network models with encouraging results. Nevertheless, two important challenges that remain are tackling the computational complexity of these models to reduce convergence times and preserving the accuracy of detection of integrity-violating intrusions. In this paper, we propose an innovative approach that utilizes trained edge cloud models to synthesize central cloud models, effectively overcoming these challenges. We empirically validate the effectiveness of the proposed method by comparing it with traditional centralized and distributed techniques, including a contemporary collaborative technique.","sentences":["Researchers are exploring the integration of IoT and the cloud continuum, together with AI to enhance the cost-effectiveness and efficiency of critical infrastructure (CI) systems.","This integration, however, increases susceptibility of CI systems to cyberattacks, potentially leading to disruptions like power outages, oil spills, or even a nuclear mishap.","CI systems are inherently complex and generate vast amounts of heterogeneous and high-dimensional data, which crosses many trust boundaries in their journey across the IoT, edge, and cloud domains over the communication network interconnecting them.","As a result, they face expanded attack surfaces.","To ensure the security of these dataflows, researchers have used deep neural network models with encouraging results.","Nevertheless, two important challenges that remain are tackling the computational complexity of these models to reduce convergence times and preserving the accuracy of detection of integrity-violating intrusions.","In this paper, we propose an innovative approach that utilizes trained edge cloud models to synthesize central cloud models, effectively overcoming these challenges.","We empirically validate the effectiveness of the proposed method by comparing it with traditional centralized and distributed techniques, including a contemporary collaborative technique."],"url":"http://arxiv.org/abs/2405.14074v1","category":"cs.CR"}
{"created":"2024-05-22 22:50:08","title":"Topological Spherical T-duality -- Dimension change from higher degree $H$-flux","abstract":"Topological Spherical T-duality was introduced by Bouwknegt, Evslin and Mathai in [BEM15] as an extension of topological T-duality from $S^1$ bundles to $\\mathrm{SU}(2)$-bundles endowed with closed 7-forms. This notion was further extended to sphere bundles by Lind, Sati and Westerland [LSW16] as a duality between $S^{2n-1}$-bundles endowed with closed $(4n-1)$-forms. We generalise this relation one step further and define T-duality for $S^{2n-1}$-bundles endowed with closed odd forms of arbitrary degree. The degree of the form determines the dimension of the fibers of the dual spaces. We show that $T$-duals exist and, as in the previous cases, $T$-dual spaces have isomorphic twisted cohomology.","sentences":["Topological Spherical T-duality was introduced by Bouwknegt, Evslin and Mathai in [BEM15] as an extension of topological T-duality from $S^1$ bundles to $\\mathrm{SU}(2)$-bundles endowed with closed 7-forms.","This notion was further extended to sphere bundles by Lind, Sati and Westerland [LSW16] as a duality between $S^{2n-1}$-bundles endowed with closed $(4n-1)$-forms.","We generalise this relation one step further and define T-duality for $S^{2n-1}$-bundles endowed with closed odd forms of arbitrary degree.","The degree of the form determines the dimension of the fibers of the dual spaces.","We show that $T$-duals exist and, as in the previous cases, $T$-dual spaces have isomorphic twisted cohomology."],"url":"http://arxiv.org/abs/2405.14054v1","category":"math.DG"}
{"created":"2024-05-22 22:19:12","title":"FLIPHAT: Joint Differential Privacy for High Dimensional Sparse Linear Bandits","abstract":"High dimensional sparse linear bandits serve as an efficient model for sequential decision-making problems (e.g. personalized medicine), where high dimensional features (e.g. genomic data) on the users are available, but only a small subset of them are relevant. Motivated by data privacy concerns in these applications, we study the joint differentially private high dimensional sparse linear bandits, where both rewards and contexts are considered as private data. First, to quantify the cost of privacy, we derive a lower bound on the regret achievable in this setting. To further address the problem, we design a computationally efficient bandit algorithm, \\textbf{F}orgetfu\\textbf{L} \\textbf{I}terative \\textbf{P}rivate \\textbf{HA}rd \\textbf{T}hresholding (FLIPHAT). Along with doubling of episodes and episodic forgetting, FLIPHAT deploys a variant of Noisy Iterative Hard Thresholding (N-IHT) algorithm as a sparse linear regression oracle to ensure both privacy and regret-optimality. We show that FLIPHAT achieves optimal regret up to logarithmic factors. We analyze the regret by providing a novel refined analysis of the estimation error of N-IHT, which is of parallel interest.","sentences":["High dimensional sparse linear bandits serve as an efficient model for sequential decision-making problems (e.g. personalized medicine), where high dimensional features (e.g. genomic data) on the users are available, but only a small subset of them are relevant.","Motivated by data privacy concerns in these applications, we study the joint differentially private high dimensional sparse linear bandits, where both rewards and contexts are considered as private data.","First, to quantify the cost of privacy, we derive a lower bound on the regret achievable in this setting.","To further address the problem, we design a computationally efficient bandit algorithm, \\textbf{F}orgetfu\\textbf{L} \\textbf{I}terative \\textbf{P}rivate \\textbf{HA}rd \\textbf{T}hresholding (FLIPHAT).","Along with doubling of episodes and episodic forgetting, FLIPHAT deploys a variant of Noisy Iterative Hard Thresholding (N-IHT) algorithm as a sparse linear regression oracle to ensure both privacy and regret-optimality.","We show that FLIPHAT achieves optimal regret up to logarithmic factors.","We analyze the regret by providing a novel refined analysis of the estimation error of N-IHT, which is of parallel interest."],"url":"http://arxiv.org/abs/2405.14038v1","category":"stat.ML"}
{"created":"2024-05-22 22:08:13","title":"Adversarial Training of Two-Layer Polynomial and ReLU Activation Networks via Convex Optimization","abstract":"Training neural networks which are robust to adversarial attacks remains an important problem in deep learning, especially as heavily overparameterized models are adopted in safety-critical settings. Drawing from recent work which reformulates the training problems for two-layer ReLU and polynomial activation networks as convex programs, we devise a convex semidefinite program (SDP) for adversarial training of polynomial activation networks via the S-procedure. We also derive a convex SDP to compute the minimum distance from a correctly classified example to the decision boundary of a polynomial activation network. Adversarial training for two-layer ReLU activation networks has been explored in the literature, but, in contrast to prior work, we present a scalable approach which is compatible with standard machine libraries and GPU acceleration. The adversarial training SDP for polynomial activation networks leads to large increases in robust test accuracy against $\\ell^\\infty$ attacks on the Breast Cancer Wisconsin dataset from the UCI Machine Learning Repository. For two-layer ReLU networks, we leverage our scalable implementation to retrain the final two fully connected layers of a Pre-Activation ResNet-18 model on the CIFAR-10 dataset. Our 'robustified' model achieves higher clean and robust test accuracies than the same architecture trained with sharpness-aware minimization.","sentences":["Training neural networks which are robust to adversarial attacks remains an important problem in deep learning, especially as heavily overparameterized models are adopted in safety-critical settings.","Drawing from recent work which reformulates the training problems for two-layer ReLU and polynomial activation networks as convex programs, we devise a convex semidefinite program (SDP) for adversarial training of polynomial activation networks via the S-procedure.","We also derive a convex SDP to compute the minimum distance from a correctly classified example to the decision boundary of a polynomial activation network.","Adversarial training for two-layer ReLU activation networks has been explored in the literature, but, in contrast to prior work, we present a scalable approach which is compatible with standard machine libraries and GPU acceleration.","The adversarial training SDP for polynomial activation networks leads to large increases in robust test accuracy against $\\ell^\\infty$ attacks on the Breast Cancer Wisconsin dataset from the UCI Machine Learning Repository.","For two-layer ReLU networks, we leverage our scalable implementation to retrain the final two fully connected layers of a Pre-Activation ResNet-18 model on the CIFAR-10 dataset.","Our 'robustified' model achieves higher clean and robust test accuracies than the same architecture trained with sharpness-aware minimization."],"url":"http://arxiv.org/abs/2405.14033v1","category":"cs.LG"}
{"created":"2024-05-22 22:00:20","title":"Pion gravitational form factors in the QCD instanton vacuum I","abstract":"The pion form factors of the QCD energy-momentum tensor (EMT) are studied in the instanton liquid model (ILM) of the QCD vacuum. In this approach the breaking of conformal symmetry is encoded in the form of stronger-than-Poisson fluctuations in the number of instantons. For the trace of the EMT, it is shown that the gluonic trace anomaly term contributes half the pion mass, with the other half coming from the quark-mass-dependent sigma term. The $Q^2$ dependence of the form factors is governed by glueball and scalar meson exchanges. For the traceless EMT, the spin-0 and 2 form factors are computed at next-to-leading order in the instanton density using effective quark operators. Relations between the gluon and quark contributions to the EMT form factors are derived. The form factors are also expressed in terms of the pion light-front wave functions in the ILM. The results at the low resolution scale of the inverse instanton size are evolved to higher scales using the renormalization group equation. The ILM results compare well with those of recent lattice QCD calculations.","sentences":["The pion form factors of the QCD energy-momentum tensor (EMT) are studied in the instanton liquid model (ILM) of the QCD vacuum.","In this approach the breaking of conformal symmetry is encoded in the form of stronger-than-Poisson fluctuations in the number of instantons.","For the trace of the EMT, it is shown that the gluonic trace anomaly term contributes half the pion mass, with the other half coming from the quark-mass-dependent sigma term.","The $Q^2$ dependence of the form factors is governed by glueball and scalar meson exchanges.","For the traceless EMT, the spin-0 and 2 form factors are computed at next-to-leading order in the instanton density using effective quark operators.","Relations between the gluon and quark contributions to the EMT form factors are derived.","The form factors are also expressed in terms of the pion light-front wave functions in the ILM.","The results at the low resolution scale of the inverse instanton size are evolved to higher scales using the renormalization group equation.","The ILM results compare well with those of recent lattice QCD calculations."],"url":"http://arxiv.org/abs/2405.14026v1","category":"hep-ph"}
{"created":"2024-05-22 21:59:58","title":"A Dynamic By-example BTF Synthesis Scheme","abstract":"Measured Bidirectional Texture Function (BTF) can faithfully reproduce a realistic appearance but is costly to acquire and store due to its 6D nature (2D spatial and 4D angular). Therefore, it is practical and necessary for rendering to synthesize BTFs from a small example patch. While previous methods managed to produce plausible results, we find that they seldomly take into consideration the property of being dynamic, so a BTF must be synthesized before the rendering process, resulting in limited size, costly pre-generation and storage issues. In this paper, we propose a dynamic BTF synthesis scheme, where a BTF at any position only needs to be synthesized when being queried. Our insight is that, with the recent advances in neural dimension reduction methods, a BTF can be decomposed into disjoint low-dimensional components. We can perform dynamic synthesis only on the positional dimensions, and during rendering, recover the BTF by querying and combining these low-dimensional functions with the help of a lightweight Multilayer Perceptron (MLP). Consequently, we obtain a fully dynamic 6D BTF synthesis scheme that does not require any pre-generation, which enables efficient rendering of our infinitely large and non-repetitive BTFs on the fly. We demonstrate the effectiveness of our method through various types of BTFs taken from UBO2014.","sentences":["Measured Bidirectional Texture Function (BTF) can faithfully reproduce a realistic appearance but is costly to acquire and store due to its 6D nature (2D spatial and 4D angular).","Therefore, it is practical and necessary for rendering to synthesize BTFs from a small example patch.","While previous methods managed to produce plausible results, we find that they seldomly take into consideration the property of being dynamic, so a BTF must be synthesized before the rendering process, resulting in limited size, costly pre-generation and storage issues.","In this paper, we propose a dynamic BTF synthesis scheme, where a BTF at any position only needs to be synthesized when being queried.","Our insight is that, with the recent advances in neural dimension reduction methods, a BTF can be decomposed into disjoint low-dimensional components.","We can perform dynamic synthesis only on the positional dimensions, and during rendering, recover the BTF by querying and combining these low-dimensional functions with the help of a lightweight Multilayer Perceptron (MLP).","Consequently, we obtain a fully dynamic 6D BTF synthesis scheme that does not require any pre-generation, which enables efficient rendering of our infinitely large and non-repetitive BTFs on the fly.","We demonstrate the effectiveness of our method through various types of BTFs taken from UBO2014."],"url":"http://arxiv.org/abs/2405.14025v1","category":"cs.GR"}
{"created":"2024-05-22 21:22:44","title":"Neural Scaling Laws for Embodied AI","abstract":"Scaling laws have driven remarkable progress across machine learning domains like language modeling and computer vision. However, the exploration of scaling laws in embodied AI and robotics has been limited, despite the rapidly increasing usage of machine learning in this field. This paper presents the first study to quantify scaling laws for Robot Foundation Models (RFMs) and the use of LLMs in robotics tasks. Through a meta-analysis spanning 198 research papers, we analyze how key factors like compute, model size, and training data quantity impact model performance across various robotic tasks. Our findings confirm that scaling laws apply to both RFMs and LLMs in robotics, with performance consistently improving as resources increase. The power law coefficients for RFMs closely match those of LLMs in robotics, resembling those found in computer vision and outperforming those for LLMs in the language domain. We also note that these coefficients vary with task complexity, with familiar tasks scaling more efficiently than unfamiliar ones, emphasizing the need for large and diverse datasets. Furthermore, we highlight the absence of standardized benchmarks in embodied AI. Most studies indicate diminishing returns, suggesting that significant resources are necessary to achieve high performance, posing challenges due to data and computational limitations. Finally, as models scale, we observe the emergence of new capabilities, particularly related to data and model size.","sentences":["Scaling laws have driven remarkable progress across machine learning domains like language modeling and computer vision.","However, the exploration of scaling laws in embodied AI and robotics has been limited, despite the rapidly increasing usage of machine learning in this field.","This paper presents the first study to quantify scaling laws for Robot Foundation Models (RFMs) and the use of LLMs in robotics tasks.","Through a meta-analysis spanning 198 research papers, we analyze how key factors like compute, model size, and training data quantity impact model performance across various robotic tasks.","Our findings confirm that scaling laws apply to both RFMs and LLMs in robotics, with performance consistently improving as resources increase.","The power law coefficients for RFMs closely match those of LLMs in robotics, resembling those found in computer vision and outperforming those for LLMs in the language domain.","We also note that these coefficients vary with task complexity, with familiar tasks scaling more efficiently than unfamiliar ones, emphasizing the need for large and diverse datasets.","Furthermore, we highlight the absence of standardized benchmarks in embodied AI.","Most studies indicate diminishing returns, suggesting that significant resources are necessary to achieve high performance, posing challenges due to data and computational limitations.","Finally, as models scale, we observe the emergence of new capabilities, particularly related to data and model size."],"url":"http://arxiv.org/abs/2405.14005v1","category":"cs.RO"}
{"created":"2024-05-22 21:17:52","title":"Nondeterministic Causal Models","abstract":"I generalize acyclic deterministic structural equation models to the nondeterministic case and argue that it offers an improved semantics for counterfactuals. The standard, deterministic, semantics developed by Halpern (and based on the initial proposal of Galles & Pearl) assumes that for each assignment of values to parent variables there is a unique assignment to their child variable, and it assumes that the actual world (an assignment of values to all variables of a model) specifies a unique counterfactual world for each intervention. Both assumptions are unrealistic, and therefore I drop both of them in my proposal. I do so by allowing multi-valued functions in the structural equations. In addition, I adjust the semantics so that the solutions to the equations that obtained in the actual world are preserved in any counterfactual world. I motivate the resulting logic by comparing it to the standard one by Halpern and to more recent proposals that are closer to mine. Finally, I extend these models to the probabilistic case and show that they open up the way to identifying counterfactuals even in Causal Bayesian Networks.","sentences":["I generalize acyclic deterministic structural equation models to the nondeterministic case and argue that it offers an improved semantics for counterfactuals.","The standard, deterministic, semantics developed by Halpern (and based on the initial proposal of Galles & Pearl) assumes that for each assignment of values to parent variables there is a unique assignment to their child variable, and it assumes that the actual world (an assignment of values to all variables of a model) specifies a unique counterfactual world for each intervention.","Both assumptions are unrealistic, and therefore I drop both of them in my proposal.","I do so by allowing multi-valued functions in the structural equations.","In addition, I adjust the semantics so that the solutions to the equations that obtained in the actual world are preserved in any counterfactual world.","I motivate the resulting logic by comparing it to the standard one by Halpern and to more recent proposals that are closer to mine.","Finally, I extend these models to the probabilistic case and show that they open up the way to identifying counterfactuals even in Causal Bayesian Networks."],"url":"http://arxiv.org/abs/2405.14001v1","category":"cs.AI"}
{"created":"2024-05-22 20:56:34","title":"Learning Cut Generating Functions for Integer Programming","abstract":"The branch-and-cut algorithm is the method of choice to solve large scale integer programming problems in practice. A key ingredient of branch-and-cut is the use of cutting planes which are derived constraints that reduce the search space for an optimal solution. Selecting effective cutting planes to produce small branch-and-cut trees is a critical challenge in the branch-and-cut algorithm. Recent advances have employed a data-driven approach to select optimal cutting planes from a parameterized family, aimed at reducing the branch-and-bound tree size (in expectation) for a given distribution of integer programming instances. We extend this idea to the selection of the best cut generating function (CGF), which is a tool in the integer programming literature for generating a wide variety of cutting planes that generalize the well-known Gomory Mixed-Integer (GMI) cutting planes. We provide rigorous sample complexity bounds for the selection of an effective CGF from certain parameterized families that provably performs well for any specified distribution on the problem instances. Our empirical results show that the selected CGF can outperform the GMI cuts for certain distributions. Additionally, we explore the sample complexity of using neural networks for instance-dependent CGF selection.","sentences":["The branch-and-cut algorithm is the method of choice to solve large scale integer programming problems in practice.","A key ingredient of branch-and-cut is the use of cutting planes which are derived constraints that reduce the search space for an optimal solution.","Selecting effective cutting planes to produce small branch-and-cut trees is a critical challenge in the branch-and-cut algorithm.","Recent advances have employed a data-driven approach to select optimal cutting planes from a parameterized family, aimed at reducing the branch-and-bound tree size (in expectation) for a given distribution of integer programming instances.","We extend this idea to the selection of the best cut generating function (CGF), which is a tool in the integer programming literature for generating a wide variety of cutting planes that generalize the well-known Gomory Mixed-Integer (GMI) cutting planes.","We provide rigorous sample complexity bounds for the selection of an effective CGF from certain parameterized families that provably performs well for any specified distribution on the problem instances.","Our empirical results show that the selected CGF can outperform the GMI cuts for certain distributions.","Additionally, we explore the sample complexity of using neural networks for instance-dependent CGF selection."],"url":"http://arxiv.org/abs/2405.13992v1","category":"math.OC"}
{"created":"2024-05-22 20:48:00","title":"High order finite-difference ghost-point methods for elliptic problems in domains with curved boundaries","abstract":"In this paper a fourth order finite difference ghost point method for the Poisson equation on regular Cartesian mesh is presented. The method can be considered the high order extension of the second ghost method introduced earlier by the authors. Three different discretizations are considered, which differ in the stencil that discretizes the Laplacian and the source term. It is shown that only two of them provide a stable method. The accuracy of such stable methods are numerically verified on several test problems.","sentences":["In this paper a fourth order finite difference ghost point method for the Poisson equation on regular Cartesian mesh is presented.","The method can be considered the high order extension of the second ghost method introduced earlier by the authors.","Three different discretizations are considered, which differ in the stencil that discretizes the Laplacian and the source term.","It is shown that only two of them provide a stable method.","The accuracy of such stable methods are numerically verified on several test problems."],"url":"http://arxiv.org/abs/2405.13986v1","category":"math.NA"}
{"created":"2024-05-22 20:29:15","title":"Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning","abstract":"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task attention module toward task distribution. Through extensive empirical evaluation, we show that AGILE significantly improves generalization performance by mitigating task interference and outperforming rehearsal-based approaches in several CL scenarios. Furthermore, AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","sentences":["Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge.","Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem.","Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation.","Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks.","To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task attention to effectively reduce interference between tasks.","AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task attention module toward task distribution.","Through extensive empirical evaluation, we show that AGILE significantly improves generalization performance by mitigating task interference and outperforming rehearsal-based approaches in several CL scenarios.","Furthermore, AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias."],"url":"http://arxiv.org/abs/2405.13978v1","category":"cs.LG"}
{"created":"2024-05-22 20:17:28","title":"Numerical Simulations of 3D Ion Crystal Dynamics in a Penning Trap using the Fast Multipole Method","abstract":"We simulate the dynamics, including laser cooling, of 3D ion crystals confined in a Penning trap using a newly developed molecular dynamics-like code. The numerical integration of the ions' equations of motion is accelerated using the fast multipole method to calculate the Coulomb interaction between ions, which allows us to efficiently study large ion crystals with thousands of ions. In particular, we show that the simulation time scales linearly with ion number, rather than with the square of the ion number. By treating the ions' absorption of photons as a Poisson process, we simulate individual photon scattering events to study laser cooling of 3D ellipsoidal ion crystals. Initial simulations suggest that these crystals can be efficiently cooled to ultracold temperatures, aided by the mixing of the easily cooled axial motional modes with the low frequency planar modes. In our simulations of a spherical crystal of 1,000 ions, the planar kinetic energy is cooled to several millikelvin in a few milliseconds while the axial kinetic energy and total potential energy are cooled even further. This suggests that 3D ion crystals could be well-suited as platforms for future quantum science experiments.","sentences":["We simulate the dynamics, including laser cooling, of 3D ion crystals confined in a Penning trap using a newly developed molecular dynamics-like code.","The numerical integration of the ions' equations of motion is accelerated using the fast multipole method to calculate the Coulomb interaction between ions, which allows us to efficiently study large ion crystals with thousands of ions.","In particular, we show that the simulation time scales linearly with ion number, rather than with the square of the ion number.","By treating the ions' absorption of photons as a Poisson process, we simulate individual photon scattering events to study laser cooling of 3D ellipsoidal ion crystals.","Initial simulations suggest that these crystals can be efficiently cooled to ultracold temperatures, aided by the mixing of the easily cooled axial motional modes with the low frequency planar modes.","In our simulations of a spherical crystal of 1,000 ions, the planar kinetic energy is cooled to several millikelvin in a few milliseconds while the axial kinetic energy and total potential energy are cooled even further.","This suggests that 3D ion crystals could be well-suited as platforms for future quantum science experiments."],"url":"http://arxiv.org/abs/2405.13973v1","category":"quant-ph"}
{"created":"2024-05-22 20:17:22","title":"Infinite-Dimensional Feature Interaction","abstract":"The past neural network design has largely focused on feature representation space dimension and its capacity scaling (e.g., width, depth), but overlooked the feature interaction space scaling.   Recent advancements have shown shifted focus towards element-wise multiplication to facilitate higher-dimensional feature interaction space for better information transformation. Despite this progress, multiplications predominantly capture low-order interactions, thus remaining confined to a finite-dimensional interaction space. To transcend this limitation, classic kernel methods emerge as a promising solution to engage features in an infinite-dimensional space. We introduce InfiNet, a model architecture that enables feature interaction within an infinite-dimensional space created by RBF kernel. Our experiments reveal that InfiNet achieves new state-of-the-art, owing to its capability to leverage infinite-dimensional interactions, significantly enhancing model performance.","sentences":["The past neural network design has largely focused on feature representation space dimension and its capacity scaling (e.g., width, depth), but overlooked the feature interaction space scaling.   ","Recent advancements have shown shifted focus towards element-wise multiplication to facilitate higher-dimensional feature interaction space for better information transformation.","Despite this progress, multiplications predominantly capture low-order interactions, thus remaining confined to a finite-dimensional interaction space.","To transcend this limitation, classic kernel methods emerge as a promising solution to engage features in an infinite-dimensional space.","We introduce InfiNet, a model architecture that enables feature interaction within an infinite-dimensional space created by RBF kernel.","Our experiments reveal that InfiNet achieves new state-of-the-art, owing to its capability to leverage infinite-dimensional interactions, significantly enhancing model performance."],"url":"http://arxiv.org/abs/2405.13972v1","category":"cs.LG"}
{"created":"2024-05-22 20:00:03","title":"Active dynamics of charged macromolecules","abstract":"We study the role of active coupling on the transport properties of homogeneously charged macromolecules in an infinitely dilute solution. An enzyme becomes actively bound to a segment of the macromolecule, exerting an electrostatic force on it. Eventually, thermal fluctuations cause it to become unbound, introducing active coupling into the system. We study the mean-squared displacement (MSD) and find a new scaling regime compared to the thermal counterpart in the presence of hydrodynamic and segment-segment electrostatic interactions. Furthermore, the study of segment-segment equal-time correlation reveals the swelling of the macromolecule. Further, we derive the concentration equation of the macromolecule with active binding and study how the cooperative diffusivity of the macromolecules get modified by its environment, including the macromolecules itself. It turns out that these active fluctuations enhance the effective diffusivity of the macromolecules. The derived closed-form expression for diffusion constant is pertinent to the accurate interpretation of light scattering data in multi-component systems with binding-unbinding equilibria.","sentences":["We study the role of active coupling on the transport properties of homogeneously charged macromolecules in an infinitely dilute solution.","An enzyme becomes actively bound to a segment of the macromolecule, exerting an electrostatic force on it.","Eventually, thermal fluctuations cause it to become unbound, introducing active coupling into the system.","We study the mean-squared displacement (MSD) and find a new scaling regime compared to the thermal counterpart in the presence of hydrodynamic and segment-segment electrostatic interactions.","Furthermore, the study of segment-segment equal-time correlation reveals the swelling of the macromolecule.","Further, we derive the concentration equation of the macromolecule with active binding and study how the cooperative diffusivity of the macromolecules get modified by its environment, including the macromolecules itself.","It turns out that these active fluctuations enhance the effective diffusivity of the macromolecules.","The derived closed-form expression for diffusion constant is pertinent to the accurate interpretation of light scattering data in multi-component systems with binding-unbinding equilibria."],"url":"http://arxiv.org/abs/2405.13963v1","category":"cond-mat.soft"}
{"created":"2024-05-22 19:47:34","title":"Construction of a Geometric Basis of K\u00e4hler differentials for plane branches","abstract":"We show that there is a basis of the set of K\\\"{a}hler differentials of an irreducible germ of holomorphic plane curve whose non-trivial elements correspond to dicritical foliations. Indeed, we discuss several concepts of generation for the semimodule of values of K\\\"{a}hler differentials of the curve and provide basis of K\\\"{a}hler differentials, for every of these concepts, whose geometric properties are described. Moreover, we give an algorithmic construction of the bases.","sentences":["We show that there is a basis of the set of K\\\"{a}hler differentials of an irreducible germ of holomorphic plane curve whose non-trivial elements correspond to dicritical foliations.","Indeed, we discuss several concepts of generation for the semimodule of values of K\\\"{a}hler differentials of the curve and provide basis of K\\\"{a}hler differentials, for every of these concepts, whose geometric properties are described.","Moreover, we give an algorithmic construction of the bases."],"url":"http://arxiv.org/abs/2405.13958v1","category":"math.AG"}
{"created":"2024-05-22 19:45:01","title":"Attention as an RNN","abstract":"The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \\textit{many-to-one} RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's \\textit{many-to-many} RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce \\textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.","sentences":["The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism.","However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices).","Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \\textit{many-to-one} RNN output efficiently.","We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants.","However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling.","Tackling this, we (3) introduce a new efficient method of computing attention's \\textit{many-to-many} RNN output based on the parallel prefix scan algorithm.","Building on the new attention formulation, we (4) introduce \\textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs).","Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient."],"url":"http://arxiv.org/abs/2405.13956v1","category":"cs.LG"}
{"created":"2024-05-23 17:59:04","title":"Not All Language Model Features Are Linear","abstract":"Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts (\"features\") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Finally, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we find further circular representations by breaking down the hidden states for these tasks into interpretable components.","sentences":["Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts (\"features\") in activation space.","In contrast, we explore whether some language model representations may be inherently multi-dimensional.","We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features.","Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B.","These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year.","We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year.","Finally, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we find further circular representations by breaking down the hidden states for these tasks into interpretable components."],"url":"http://arxiv.org/abs/2405.14860v1","category":"cs.LG"}
{"created":"2024-05-23 17:55:51","title":"Dual-comb correlation spectroscopy of thermal light","abstract":"The detection of light of thermal origin is the principal means by which humanity has learned about our world and the cosmos. In optical astronomy, in particular, direct detection of thermal photons and the resolution of their spectra have enabled discoveries of the broadest scope and impact. Such measurements, however, do not capture the phase of the thermal fields--a parameter that has proven crucial to transformative techniques in radio astronomy such as synthetic aperture imaging. Over the last 25 years, tremendous progress has occurred in laser science, notably in the phase-sensitive, broad bandwidth, high resolution, and traceable spectroscopy enabled by the optical frequency comb. In this work, we directly connect the fields of frequency comb laser spectroscopy and passive optical sensing as applied to astronomy, remote sensing, and atmospheric science. We provide fundamental sensitivity analysis of dual-comb correlation spectroscopy (DCCS), whereby broadband thermal light is measured via interferometry with two optical frequency combs. We define and experimentally verify the sensitivity scaling of DCCS at black body temperatures relevant for astrophysical observations. Moreover, we provide comparison with direct detection techniques and more conventional laser heterodyne radiometry. Our work provides the foundation for future exploration of comb-based broadband synthetic aperture hyperspectral imaging across the infrared and optical spectrum.","sentences":["The detection of light of thermal origin is the principal means by which humanity has learned about our world and the cosmos.","In optical astronomy, in particular, direct detection of thermal photons and the resolution of their spectra have enabled discoveries of the broadest scope and impact.","Such measurements, however, do not capture the phase of the thermal fields--a parameter that has proven crucial to transformative techniques in radio astronomy such as synthetic aperture imaging.","Over the last 25 years, tremendous progress has occurred in laser science, notably in the phase-sensitive, broad bandwidth, high resolution, and traceable spectroscopy enabled by the optical frequency comb.","In this work, we directly connect the fields of frequency comb laser spectroscopy and passive optical sensing as applied to astronomy, remote sensing, and atmospheric science.","We provide fundamental sensitivity analysis of dual-comb correlation spectroscopy (DCCS), whereby broadband thermal light is measured via interferometry with two optical frequency combs.","We define and experimentally verify the sensitivity scaling of DCCS at black body temperatures relevant for astrophysical observations.","Moreover, we provide comparison with direct detection techniques and more conventional laser heterodyne radiometry.","Our work provides the foundation for future exploration of comb-based broadband synthetic aperture hyperspectral imaging across the infrared and optical spectrum."],"url":"http://arxiv.org/abs/2405.14842v1","category":"physics.optics"}
{"created":"2024-05-23 17:01:53","title":"Recurrent Early Exits for Federated Learning with Heterogeneous Clients","abstract":"Federated learning (FL) has enabled distributed learning of a model across multiple clients in a privacy-preserving manner. One of the main challenges of FL is to accommodate clients with varying hardware capacities; clients have differing compute and memory requirements. To tackle this challenge, recent state-of-the-art approaches leverage the use of early exits. Nonetheless, these approaches fall short of mitigating the challenges of joint learning multiple exit classifiers, often relying on hand-picked heuristic solutions for knowledge distillation among classifiers and/or utilizing additional layers for weaker classifiers. In this work, instead of utilizing multiple classifiers, we propose a recurrent early exit approach named ReeFL that fuses features from different sub-models into a single shared classifier. Specifically, we use a transformer-based early-exit module shared among sub-models to i) better exploit multi-layer feature representations for task-specific prediction and ii) modulate the feature representation of the backbone model for subsequent predictions. We additionally present a per-client self-distillation approach where the best sub-model is automatically selected as the teacher of the other sub-models at each client. Our experiments on standard image and speech classification benchmarks across various emerging federated fine-tuning baselines demonstrate ReeFL's effectiveness over previous works.","sentences":["Federated learning (FL) has enabled distributed learning of a model across multiple clients in a privacy-preserving manner.","One of the main challenges of FL is to accommodate clients with varying hardware capacities; clients have differing compute and memory requirements.","To tackle this challenge, recent state-of-the-art approaches leverage the use of early exits.","Nonetheless, these approaches fall short of mitigating the challenges of joint learning multiple exit classifiers, often relying on hand-picked heuristic solutions for knowledge distillation among classifiers and/or utilizing additional layers for weaker classifiers.","In this work, instead of utilizing multiple classifiers, we propose a recurrent early exit approach named ReeFL that fuses features from different sub-models into a single shared classifier.","Specifically, we use a transformer-based early-exit module shared among sub-models to i) better exploit multi-layer feature representations for task-specific prediction and ii) modulate the feature representation of the backbone model for subsequent predictions.","We additionally present a per-client self-distillation approach where the best sub-model is automatically selected as the teacher of the other sub-models at each client.","Our experiments on standard image and speech classification benchmarks across various emerging federated fine-tuning baselines demonstrate ReeFL's effectiveness over previous works."],"url":"http://arxiv.org/abs/2405.14791v1","category":"cs.LG"}
{"created":"2024-05-23 16:45:59","title":"Smart Bilingual Focused Crawling of Parallel Documents","abstract":"Crawling parallel texts $\\unicode{x2014}$texts that are mutual translations$\\unicode{x2014}$ from the Internet is usually done following a brute-force approach: documents are massively downloaded in an unguided process, and only a fraction of them end up leading to actual parallel content. In this work we propose a smart crawling method that guides the crawl towards finding parallel content more rapidly. Our approach builds on two different models: one that infers the language of a document from its URL, and another that infers whether a pair of URLs link to parallel documents. We evaluate both models in isolation and their integration into a crawling tool. The results demonstrate the individual effectiveness of both models and highlight that their combination enables the early discovery of parallel content during crawling, leading to a reduction in the amount of downloaded documents deemed useless, and yielding a greater quantity of parallel documents compared to conventional crawling approaches.","sentences":["Crawling parallel texts $\\unicode{x2014}$texts that are mutual translations$\\unicode{x2014}$ from the Internet is usually done following a brute-force approach: documents are massively downloaded in an unguided process, and only a fraction of them end up leading to actual parallel content.","In this work we propose a smart crawling method that guides the crawl towards finding parallel content more rapidly.","Our approach builds on two different models: one that infers the language of a document from its URL, and another that infers whether a pair of URLs link to parallel documents.","We evaluate both models in isolation and their integration into a crawling tool.","The results demonstrate the individual effectiveness of both models and highlight that their combination enables the early discovery of parallel content during crawling, leading to a reduction in the amount of downloaded documents deemed useless, and yielding a greater quantity of parallel documents compared to conventional crawling approaches."],"url":"http://arxiv.org/abs/2405.14779v1","category":"cs.CL"}
{"created":"2024-05-23 16:36:16","title":"Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from Human Input","abstract":"Humans use social context to specify preferences over behaviors, i.e. their reward functions. Yet, algorithms for inferring reward models from preference data do not take this social learning view into account. Inspired by pragmatic human communication, we study how to extract fine-grained data regarding why an example is preferred that is useful for learning more accurate reward models. We propose to enrich binary preference queries to ask both (1) which features of a given example are preferable in addition to (2) comparisons between examples themselves. We derive an approach for learning from these feature-level preferences, both for cases where users specify which features are reward-relevant, and when users do not. We evaluate our approach on linear bandit settings in both vision- and language-based domains. Results support the efficiency of our approach in quickly converging to accurate rewards with fewer comparisons vs. example-only labels. Finally, we validate the real-world applicability with a behavioral experiment on a mushroom foraging task. Our findings suggest that incorporating pragmatic feature preferences is a promising approach for more efficient user-aligned reward learning.","sentences":["Humans use social context to specify preferences over behaviors, i.e. their reward functions.","Yet, algorithms for inferring reward models from preference data do not take this social learning view into account.","Inspired by pragmatic human communication, we study how to extract fine-grained data regarding why an example is preferred that is useful for learning more accurate reward models.","We propose to enrich binary preference queries to ask both (1) which features of a given example are preferable in addition to (2) comparisons between examples themselves.","We derive an approach for learning from these feature-level preferences, both for cases where users specify which features are reward-relevant, and when users do not.","We evaluate our approach on linear bandit settings in both vision- and language-based domains.","Results support the efficiency of our approach in quickly converging to accurate rewards with fewer comparisons vs. example-only labels.","Finally, we validate the real-world applicability with a behavioral experiment on a mushroom foraging task.","Our findings suggest that incorporating pragmatic feature preferences is a promising approach for more efficient user-aligned reward learning."],"url":"http://arxiv.org/abs/2405.14769v1","category":"cs.LG"}
{"created":"2024-05-23 16:33:18","title":"Evaluating Large Language Models for Public Health Classification and Extraction Tasks","abstract":"Advances in Large Language Models (LLMs) have led to significant interest in their potential to support human experts across a range of domains, including public health. In this work we present automated evaluations of LLMs for public health tasks involving the classification and extraction of free text. We combine six externally annotated datasets with seven new internally annotated datasets to evaluate LLMs for processing text related to: health burden, epidemiological risk factors, and public health interventions. We initially evaluate five open-weight LLMs (7-70 billion parameters) across all tasks using zero-shot in-context learning. We find that Llama-3-70B-Instruct is the highest performing model, achieving the best results on 15/17 tasks (using micro-F1 scores). We see significant variation across tasks with all open-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as Contact Classification, while all LLMs achieve greater than 80% micro-F1 on others, such as GI Illness Classification. For a subset of 12 tasks, we also evaluate GPT-4 and find comparable results to Llama-3-70B-Instruct, which scores equally or outperforms GPT-4 on 6 of the 12 tasks. Overall, based on these initial results we find promising signs that LLMs may be useful tools for public health experts to extract information from a wide variety of free text sources, and support public health surveillance, research, and interventions.","sentences":["Advances in Large Language Models (LLMs) have led to significant interest in their potential to support human experts across a range of domains, including public health.","In this work we present automated evaluations of LLMs for public health tasks involving the classification and extraction of free text.","We combine six externally annotated datasets with seven new internally annotated datasets to evaluate LLMs for processing text related to: health burden, epidemiological risk factors, and public health interventions.","We initially evaluate five open-weight LLMs (7-70 billion parameters) across all tasks using zero-shot in-context learning.","We find that Llama-3-70B-Instruct is the highest performing model, achieving the best results on 15/17 tasks (using micro-F1 scores).","We see significant variation across tasks with all open-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as Contact Classification, while all LLMs achieve greater than 80% micro-F1 on others, such as GI Illness Classification.","For a subset of 12 tasks, we also evaluate GPT-4 and find comparable results to Llama-3-70B-Instruct, which scores equally or outperforms GPT-4 on 6 of the 12 tasks.","Overall, based on these initial results we find promising signs that LLMs may be useful tools for public health experts to extract information from a wide variety of free text sources, and support public health surveillance, research, and interventions."],"url":"http://arxiv.org/abs/2405.14766v1","category":"cs.CL"}
{"created":"2024-05-23 16:21:57","title":"Large language models can be zero-shot anomaly detectors for time series?","abstract":"Recent studies have shown the ability of large language models to perform a variety of tasks, including time series forecasting. The flexible nature of these models allows them to be used for many applications. In this paper, we present a novel study of large language models used for the challenging task of time series anomaly detection. This problem entails two aspects novel for LLMs: the need for the model to identify part of the input sequence (or multiple parts) as anomalous; and the need for it to work with time series data rather than the traditional text input. We introduce sigllm, a framework for time series anomaly detection using large language models. Our framework includes a time-series-to-text conversion module, as well as end-to-end pipelines that prompt language models to perform time series anomaly detection. We investigate two paradigms for testing the abilities of large language models to perform the detection task. First, we present a prompt-based detection method that directly asks a language model to indicate which elements of the input are anomalies. Second, we leverage the forecasting capability of a large language model to guide the anomaly detection process. We evaluated our framework on 11 datasets spanning various sources and 10 pipelines. We show that the forecasting method significantly outperformed the prompting method in all 11 datasets with respect to the F1 score. Moreover, while large language models are capable of finding anomalies, state-of-the-art deep learning models are still superior in performance, achieving results 30% better than large language models.","sentences":["Recent studies have shown the ability of large language models to perform a variety of tasks, including time series forecasting.","The flexible nature of these models allows them to be used for many applications.","In this paper, we present a novel study of large language models used for the challenging task of time series anomaly detection.","This problem entails two aspects novel for LLMs: the need for the model to identify part of the input sequence (or multiple parts) as anomalous; and the need for it to work with time series data rather than the traditional text input.","We introduce sigllm, a framework for time series anomaly detection using large language models.","Our framework includes a time-series-to-text conversion module, as well as end-to-end pipelines that prompt language models to perform time series anomaly detection.","We investigate two paradigms for testing the abilities of large language models to perform the detection task.","First, we present a prompt-based detection method that directly asks a language model to indicate which elements of the input are anomalies.","Second, we leverage the forecasting capability of a large language model to guide the anomaly detection process.","We evaluated our framework on 11 datasets spanning various sources and 10 pipelines.","We show that the forecasting method significantly outperformed the prompting method in all 11 datasets with respect to the F1 score.","Moreover, while large language models are capable of finding anomalies, state-of-the-art deep learning models are still superior in performance, achieving results 30% better than large language models."],"url":"http://arxiv.org/abs/2405.14755v1","category":"cs.LG"}
{"created":"2024-05-23 16:21:51","title":"Applied Machine Learning to Anomaly Detection in Enterprise Purchase Processes","abstract":"In a context of a continuous digitalisation of processes, organisations must deal with the challenge of detecting anomalies that can reveal suspicious activities upon an increasing volume of data. To pursue this goal, audit engagements are carried out regularly, and internal auditors and purchase specialists are constantly looking for new methods to automate these processes. This work proposes a methodology to prioritise the investigation of the cases detected in two large purchase datasets from real data. The goal is to contribute to the effectiveness of the companies' control efforts and to increase the performance of carrying out such tasks. A comprehensive Exploratory Data Analysis is carried out before using unsupervised Machine Learning techniques addressed to detect anomalies. A univariate approach has been applied through the z-Score index and the DBSCAN algorithm, while a multivariate analysis is implemented with the k-Means and Isolation Forest algorithms, and the Silhouette index, resulting in each method having a transaction candidates' proposal to be reviewed. An ensemble prioritisation of the candidates is provided jointly with a proposal of explicability methods (LIME, Shapley, SHAP) to help the company specialists in their understanding.","sentences":["In a context of a continuous digitalisation of processes, organisations must deal with the challenge of detecting anomalies that can reveal suspicious activities upon an increasing volume of data.","To pursue this goal, audit engagements are carried out regularly, and internal auditors and purchase specialists are constantly looking for new methods to automate these processes.","This work proposes a methodology to prioritise the investigation of the cases detected in two large purchase datasets from real data.","The goal is to contribute to the effectiveness of the companies' control efforts and to increase the performance of carrying out such tasks.","A comprehensive Exploratory Data Analysis is carried out before using unsupervised Machine Learning techniques addressed to detect anomalies.","A univariate approach has been applied through the z-Score index and the DBSCAN algorithm, while a multivariate analysis is implemented with the k-Means and Isolation Forest algorithms, and the Silhouette index, resulting in each method having a transaction candidates' proposal to be reviewed.","An ensemble prioritisation of the candidates is provided jointly with a proposal of explicability methods (LIME, Shapley, SHAP) to help the company specialists in their understanding."],"url":"http://arxiv.org/abs/2405.14754v1","category":"cs.CE"}
{"created":"2024-05-23 16:03:55","title":"CLIPScope: Enhancing Zero-Shot OOD Detection with Bayesian Scoring","abstract":"Detection of out-of-distribution (OOD) samples is crucial for safe real-world deployment of machine learning models. Recent advances in vision language foundation models have made them capable of detecting OOD samples without requiring in-distribution (ID) images. However, these zero-shot methods often underperform as they do not adequately consider ID class likelihoods in their detection confidence scoring. Hence, we introduce CLIPScope, a zero-shot OOD detection approach that normalizes the confidence score of a sample by class likelihoods, akin to a Bayesian posterior update. Furthermore, CLIPScope incorporates a novel strategy to mine OOD classes from a large lexical database. It selects class labels that are farthest and nearest to ID classes in terms of CLIP embedding distance to maximize coverage of OOD samples. We conduct extensive ablation studies and empirical evaluations, demonstrating state of the art performance of CLIPScope across various OOD detection benchmarks.","sentences":["Detection of out-of-distribution (OOD) samples is crucial for safe real-world deployment of machine learning models.","Recent advances in vision language foundation models have made them capable of detecting OOD samples without requiring in-distribution (ID) images.","However, these zero-shot methods often underperform as they do not adequately consider ID class likelihoods in their detection confidence scoring.","Hence, we introduce CLIPScope, a zero-shot OOD detection approach that normalizes the confidence score of a sample by class likelihoods, akin to a Bayesian posterior update.","Furthermore, CLIPScope incorporates a novel strategy to mine OOD classes from a large lexical database.","It selects class labels that are farthest and nearest to ID classes in terms of CLIP embedding distance to maximize coverage of OOD samples.","We conduct extensive ablation studies and empirical evaluations, demonstrating state of the art performance of CLIPScope across various OOD detection benchmarks."],"url":"http://arxiv.org/abs/2405.14737v1","category":"cs.CV"}
{"created":"2024-05-23 15:54:59","title":"Distilling Vision-Language Pretraining for Efficient Cross-Modal Retrieval","abstract":"``Learning to hash'' is a practical solution for efficient retrieval, offering fast search speed and low storage cost. It is widely applied in various applications, such as image-text cross-modal search. In this paper, we explore the potential of enhancing the performance of learning to hash with the proliferation of powerful large pre-trained models, such as Vision-Language Pre-training (VLP) models. We introduce a novel method named Distillation for Cross-Modal Quantization (DCMQ), which leverages the rich semantic knowledge of VLP models to improve hash representation learning. Specifically, we use the VLP as a `teacher' to distill knowledge into a `student' hashing model equipped with codebooks. This process involves the replacement of supervised labels, which are composed of multi-hot vectors and lack semantics, with the rich semantics of VLP. In the end, we apply a transformation termed Normalization with Paired Consistency (NPC) to achieve a discriminative target for distillation. Further, we introduce a new quantization method, Product Quantization with Gumbel (PQG) that promotes balanced codebook learning, thereby improving the retrieval performance. Extensive benchmark testing demonstrates that DCMQ consistently outperforms existing supervised cross-modal hashing approaches, showcasing its significant potential.","sentences":["``Learning to hash'' is a practical solution for efficient retrieval, offering fast search speed and low storage cost.","It is widely applied in various applications, such as image-text cross-modal search.","In this paper, we explore the potential of enhancing the performance of learning to hash with the proliferation of powerful large pre-trained models, such as Vision-Language Pre-training (VLP) models.","We introduce a novel method named Distillation for Cross-Modal Quantization (DCMQ), which leverages the rich semantic knowledge of VLP models to improve hash representation learning.","Specifically, we use the VLP as a `teacher' to distill knowledge into a `student' hashing model equipped with codebooks.","This process involves the replacement of supervised labels, which are composed of multi-hot vectors and lack semantics, with the rich semantics of VLP.","In the end, we apply a transformation termed Normalization with Paired Consistency (NPC) to achieve a discriminative target for distillation.","Further, we introduce a new quantization method, Product Quantization with Gumbel (PQG) that promotes balanced codebook learning, thereby improving the retrieval performance.","Extensive benchmark testing demonstrates that DCMQ consistently outperforms existing supervised cross-modal hashing approaches, showcasing its significant potential."],"url":"http://arxiv.org/abs/2405.14726v1","category":"cs.CV"}
{"created":"2024-05-23 15:45:21","title":"Zero-inflation in the Multivariate Poisson Lognormal Family","abstract":"Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn't account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to $90\\%$ of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing $90.6\\%$ of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.","sentences":["Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability.","The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies.","However PLN doesn't account for zero-inflation, a feature frequently observed in real-world datasets.","Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable.","The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates.","We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one.","The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to $90\\%$ of the observed counts.","We then apply both ZIPLN and PLN to a cow microbiome dataset, containing $90.6\\%$ of zeroes.","Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination."],"url":"http://arxiv.org/abs/2405.14711v1","category":"stat.ME"}
{"created":"2024-05-23 15:08:31","title":"Towards Imperceptible Backdoor Attack in Self-supervised Learning","abstract":"Self-supervised learning models are vulnerable to backdoor attacks. Existing backdoor attacks that are effective in self-supervised learning often involve noticeable triggers, like colored patches, which are vulnerable to human inspection. In this paper, we propose an imperceptible and effective backdoor attack against self-supervised models. We first find that existing imperceptible triggers designed for supervised learning are not as effective in compromising self-supervised models. We then identify this ineffectiveness is attributed to the overlap in distributions between the backdoor and augmented samples used in self-supervised learning. Building on this insight, we design an attack using optimized triggers that are disentangled to the augmented transformation in the self-supervised learning, while also remaining imperceptible to human vision. Experiments on five datasets and seven SSL algorithms demonstrate our attack is highly effective and stealthy. It also has strong resistance to existing backdoor defenses. Our code can be found at https://github.com/Zhang-Henry/IMPERATIVE.","sentences":["Self-supervised learning models are vulnerable to backdoor attacks.","Existing backdoor attacks that are effective in self-supervised learning often involve noticeable triggers, like colored patches, which are vulnerable to human inspection.","In this paper, we propose an imperceptible and effective backdoor attack against self-supervised models.","We first find that existing imperceptible triggers designed for supervised learning are not as effective in compromising self-supervised models.","We then identify this ineffectiveness is attributed to the overlap in distributions between the backdoor and augmented samples used in self-supervised learning.","Building on this insight, we design an attack using optimized triggers that are disentangled to the augmented transformation in the self-supervised learning, while also remaining imperceptible to human vision.","Experiments on five datasets and seven SSL algorithms demonstrate our attack is highly effective and stealthy.","It also has strong resistance to existing backdoor defenses.","Our code can be found at https://github.com/Zhang-Henry/IMPERATIVE."],"url":"http://arxiv.org/abs/2405.14672v1","category":"cs.CV"}
{"created":"2024-05-23 14:50:52","title":"Leveraging Machine Learning for Advanced Nanoscale X-ray Analysis: Unmixing Multicomponent Signals and Enhancing Chemical Quantification","abstract":"Energy dispersive X-ray (EDX) spectroscopy in the transmission electron microscope is a key tool for nanomaterials analysis, providing a direct link between spatial and chemical information. However, using it for precisely determining chemical compositions presents challenges of noisy data from low X-ray yields and mixed signals from phases that overlap along the electron beam trajectory. Here, we introduce a novel method, non-negative matrix factorisation based pan-sharpening (PSNMF), to address these limitations. Leveraging the Poisson nature of EDX spectral noise and binning operations, PSNMF retrieves high quality phase spectral and spatial signatures via consecutive factorisations. After validating PSNMF with synthetic datasets of different noise levels, we illustrate its effectiveness on two distinct experimental cases: a nano-mineralogical lamella, and supported catalytic nanoparticles. Not only does PSNMF obtain accurate phase signatures, datasets reconstructed from the outputs have demonstrably lower noise and better fidelity than from the benchmark denoising method of principle component analysis.","sentences":["Energy dispersive X-ray (EDX) spectroscopy in the transmission electron microscope is a key tool for nanomaterials analysis, providing a direct link between spatial and chemical information.","However, using it for precisely determining chemical compositions presents challenges of noisy data from low X-ray yields and mixed signals from phases that overlap along the electron beam trajectory.","Here, we introduce a novel method, non-negative matrix factorisation based pan-sharpening (PSNMF), to address these limitations.","Leveraging the Poisson nature of EDX spectral noise and binning operations, PSNMF retrieves high quality phase spectral and spatial signatures via consecutive factorisations.","After validating PSNMF with synthetic datasets of different noise levels, we illustrate its effectiveness on two distinct experimental cases: a nano-mineralogical lamella, and supported catalytic nanoparticles.","Not only does PSNMF obtain accurate phase signatures, datasets reconstructed from the outputs have demonstrably lower noise and better fidelity than from the benchmark denoising method of principle component analysis."],"url":"http://arxiv.org/abs/2405.14649v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 13:52:36","title":"Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling","abstract":"In current deep learning tasks, Adam style optimizers such as Adam, Adagrad, RMSProp, Adafactor, and Lion have been widely used as alternatives to SGD style optimizers. These optimizers typically update model parameters using the sign of gradients, resulting in more stable convergence curves. The learning rate and the batch size are the most critical hyperparameters for optimizers, which require careful tuning to enable effective convergence. Previous research has shown that the optimal learning rate increases linearly or follows similar rules with batch size for SGD style optimizers. However, this conclusion is not applicable to Adam style optimizers. In this paper, we elucidate the connection between optimal learning rates and batch sizes for Adam style optimizers through both theoretical analysis and extensive experiments. First, we raise the scaling law between batch sizes and optimal learning rates in the sign of gradient case, in which we prove that the optimal learning rate first rises and then falls as the batch size increases. Moreover, the peak value of the surge will gradually move toward the larger batch size as training progresses. Second, we conducted experiments on various CV and NLP tasks and verified the correctness of the scaling law.","sentences":["In current deep learning tasks, Adam style optimizers such as Adam, Adagrad, RMSProp, Adafactor, and Lion have been widely used as alternatives to SGD style optimizers.","These optimizers typically update model parameters using the sign of gradients, resulting in more stable convergence curves.","The learning rate and the batch size are the most critical hyperparameters for optimizers, which require careful tuning to enable effective convergence.","Previous research has shown that the optimal learning rate increases linearly or follows similar rules with batch size for SGD style optimizers.","However, this conclusion is not applicable to Adam style optimizers.","In this paper, we elucidate the connection between optimal learning rates and batch sizes for Adam style optimizers through both theoretical analysis and extensive experiments.","First, we raise the scaling law between batch sizes and optimal learning rates in the sign of gradient case, in which we prove that the optimal learning rate first rises and then falls as the batch size increases.","Moreover, the peak value of the surge will gradually move toward the larger batch size as training progresses.","Second, we conducted experiments on various CV and NLP tasks and verified the correctness of the scaling law."],"url":"http://arxiv.org/abs/2405.14578v1","category":"cs.LG"}
{"created":"2024-05-23 13:38:34","title":"HemSeg-200: A Voxel-Annotated Dataset for Intracerebral Hemorrhages Segmentation in Brain CT Scans","abstract":"Acute intracerebral hemorrhage is a life-threatening condition that demands immediate medical intervention. Intraparenchymal hemorrhage (IPH) and intraventricular hemorrhage (IVH) are critical subtypes of this condition. Clinically, when such hemorrhages are suspected, immediate CT scanning is essential to assess the extent of the bleeding and to facilitate the formulation of a targeted treatment plan. While current research in deep learning has largely focused on qualitative analyses, such as identifying subtypes of cerebral hemorrhages, there remains a significant gap in quantitative analysis crucial for enhancing clinical treatments. Addressing this gap, our paper introduces a dataset comprising 222 CT annotations, sourced from the RSNA 2019 Brain CT Hemorrhage Challenge and meticulously annotated at the voxel level for precise IPH and IVH segmentation. This dataset was utilized to train and evaluate seven advanced medical image segmentation algorithms, with the goal of refining the accuracy of segmentation for these hemorrhages. Our findings demonstrate that this dataset not only furthers the development of sophisticated segmentation algorithms but also substantially aids scientific research and clinical practice by improving the diagnosis and management of these severe hemorrhages. Our dataset and codes are available at \\url{https://github.com/songchangwei/3DCT-SD-IVH-ICH}.","sentences":["Acute intracerebral hemorrhage is a life-threatening condition that demands immediate medical intervention.","Intraparenchymal hemorrhage (IPH) and intraventricular hemorrhage (IVH) are critical subtypes of this condition.","Clinically, when such hemorrhages are suspected, immediate CT scanning is essential to assess the extent of the bleeding and to facilitate the formulation of a targeted treatment plan.","While current research in deep learning has largely focused on qualitative analyses, such as identifying subtypes of cerebral hemorrhages, there remains a significant gap in quantitative analysis crucial for enhancing clinical treatments.","Addressing this gap, our paper introduces a dataset comprising 222 CT annotations, sourced from the RSNA 2019 Brain CT Hemorrhage Challenge and meticulously annotated at the voxel level for precise IPH and IVH segmentation.","This dataset was utilized to train and evaluate seven advanced medical image segmentation algorithms, with the goal of refining the accuracy of segmentation for these hemorrhages.","Our findings demonstrate that this dataset not only furthers the development of sophisticated segmentation algorithms but also substantially aids scientific research and clinical practice by improving the diagnosis and management of these severe hemorrhages.","Our dataset and codes are available at \\url{https://github.com/songchangwei/3DCT-SD-IVH-ICH}."],"url":"http://arxiv.org/abs/2405.14559v1","category":"eess.IV"}
{"created":"2024-05-23 13:25:20","title":"A Cross-Field Fusion Strategy for Drug-Target Interaction Prediction","abstract":"Drug-target interaction (DTI) prediction is a critical component of the drug discovery process. In the drug development engineering field, predicting novel drug-target interactions is extremely crucial.However, although existing methods have achieved high accuracy levels in predicting known drugs and drug targets, they fail to utilize global protein information during DTI prediction. This leads to an inability to effectively predict interaction the interactions between novel drugs and their targets. As a result, the cross-field information fusion strategy is employed to acquire local and global protein information. Thus, we propose the siamese drug-target interaction SiamDTI prediction method, which utilizes a double channel network structure for cross-field supervised learning.Experimental results on three benchmark datasets demonstrate that SiamDTI achieves higher accuracy levels than other state-of-the-art (SOTA) methods on novel drugs and targets.Additionally, SiamDTI's performance with known drugs and targets is comparable to that of SOTA approachs. The code is available at https://anonymous.4open.science/r/DDDTI-434D.","sentences":["Drug-target interaction (DTI) prediction is a critical component of the drug discovery process.","In the drug development engineering field, predicting novel drug-target interactions is extremely crucial.","However, although existing methods have achieved high accuracy levels in predicting known drugs and drug targets, they fail to utilize global protein information during DTI prediction.","This leads to an inability to effectively predict interaction the interactions between novel drugs and their targets.","As a result, the cross-field information fusion strategy is employed to acquire local and global protein information.","Thus, we propose the siamese drug-target interaction SiamDTI prediction method, which utilizes a double channel network structure for cross-field supervised learning.","Experimental results on three benchmark datasets demonstrate that SiamDTI achieves higher accuracy levels than other state-of-the-art (SOTA) methods on novel drugs and targets.","Additionally, SiamDTI's performance with known drugs and targets is comparable to that of SOTA approachs.","The code is available at https://anonymous.4open.science/r/DDDTI-434D."],"url":"http://arxiv.org/abs/2405.14545v1","category":"q-bio.BM"}
{"created":"2024-05-23 13:18:51","title":"Aligning Embeddings and Geometric Random Graphs: Informational Results and Computational Approaches for the Procrustes-Wasserstein Problem","abstract":"The Procrustes-Wasserstein problem consists in matching two high-dimensional point clouds in an unsupervised setting, and has many applications in natural language processing and computer vision. We consider a planted model with two datasets $X,Y$ that consist of $n$ datapoints in $\\mathbb{R}^d$, where $Y$ is a noisy version of $X$, up to an orthogonal transformation and a relabeling of the data points. This setting is related to the graph alignment problem in geometric models. In this work, we focus on the euclidean transport cost between the point clouds as a measure of performance for the alignment. We first establish information-theoretic results, in the high ($d \\gg \\log n$) and low ($d \\ll \\log n$) dimensional regimes. We then study computational aspects and propose the Ping-Pong algorithm, alternatively estimating the orthogonal transformation and the relabeling, initialized via a Franke-Wolfe convex relaxation. We give sufficient conditions for the method to retrieve the planted signal after one single step. We provide experimental results to compare the proposed approach with the state-of-the-art method of Grave et al. (2019).","sentences":["The Procrustes-Wasserstein problem consists in matching two high-dimensional point clouds in an unsupervised setting, and has many applications in natural language processing and computer vision.","We consider a planted model with two datasets $X,Y$ that consist of $n$ datapoints in $\\mathbb{R}^d$, where $Y$ is a noisy version of $X$, up to an orthogonal transformation and a relabeling of the data points.","This setting is related to the graph alignment problem in geometric models.","In this work, we focus on the euclidean transport cost between the point clouds as a measure of performance for the alignment.","We first establish information-theoretic results, in the high ($d \\gg \\log n$) and low ($d \\ll \\log n$) dimensional regimes.","We then study computational aspects and propose the Ping-Pong algorithm, alternatively estimating the orthogonal transformation and the relabeling, initialized via a Franke-Wolfe convex relaxation.","We give sufficient conditions for the method to retrieve the planted signal after one single step.","We provide experimental results to compare the proposed approach with the state-of-the-art method of Grave et al. (2019)."],"url":"http://arxiv.org/abs/2405.14532v1","category":"stat.ML"}
{"created":"2024-05-23 13:01:36","title":"A New Formulation for Zeroth-Order Optimization of Adversarial EXEmples in Malware Detection","abstract":"Machine learning malware detectors are vulnerable to adversarial EXEmples, i.e. carefully-crafted Windows programs tailored to evade detection. Unlike other adversarial problems, attacks in this context must be functionality-preserving, a constraint which is challenging to address. As a consequence heuristic algorithms are typically used, that inject new content, either randomly-picked or harvested from legitimate programs. In this paper, we show how learning malware detectors can be cast within a zeroth-order optimization framework which allows to incorporate functionality-preserving manipulations. This permits the deployment of sound and efficient gradient-free optimization algorithms, which come with theoretical guarantees and allow for minimal hyper-parameters tuning. As a by-product, we propose and study ZEXE, a novel zero-order attack against Windows malware detection. Compared to state-of-the-art techniques, ZEXE provides drastic improvement in the evasion rate, while reducing to less than one third the size of the injected content.","sentences":["Machine learning malware detectors are vulnerable to adversarial EXEmples, i.e. carefully-crafted Windows programs tailored to evade detection.","Unlike other adversarial problems, attacks in this context must be functionality-preserving, a constraint which is challenging to address.","As a consequence heuristic algorithms are typically used, that inject new content, either randomly-picked or harvested from legitimate programs.","In this paper, we show how learning malware detectors can be cast within a zeroth-order optimization framework which allows to incorporate functionality-preserving manipulations.","This permits the deployment of sound and efficient gradient-free optimization algorithms, which come with theoretical guarantees and allow for minimal hyper-parameters tuning.","As a by-product, we propose and study ZEXE, a novel zero-order attack against Windows malware detection.","Compared to state-of-the-art techniques, ZEXE provides drastic improvement in the evasion rate, while reducing to less than one third the size of the injected content."],"url":"http://arxiv.org/abs/2405.14519v1","category":"cs.LG"}
{"created":"2024-05-23 12:45:29","title":"Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast","abstract":"Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling model size while maintaining computational efficiency. In MoE, each token in the input sequence activates a different subset of experts determined by a routing mechanism. However, the unchosen experts in MoE models do not contribute to the output, potentially leading to underutilization of the model's capacity. In this work, we first conduct exploratory studies to demonstrate that increasing the number of activated experts does not necessarily improve and can even degrade the output quality. Then, we show that output distributions from an MoE model using different routing strategies substantially differ, indicating that different experts do not always act synergistically. Motivated by these findings, we propose Self-Contrast Mixture-of-Experts (SCMoE), a training-free strategy that utilizes unchosen experts in a self-contrast manner during inference. In SCMoE, the next-token probabilities are determined by contrasting the outputs from strong and weak activation using the same MoE model. Our method is conceptually simple and computationally lightweight, as it incurs minimal latency compared to greedy decoding. Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and HumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B's reasoning capability across various domains. For example, it improves the accuracy on GSM8K from 61.79 to 66.94. Moreover, combining SCMoE with self-consistency yields additional gains, increasing major@20 accuracy from 75.59 to 78.31.","sentences":["Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling model size while maintaining computational efficiency.","In MoE, each token in the input sequence activates a different subset of experts determined by a routing mechanism.","However, the unchosen experts in MoE models do not contribute to the output, potentially leading to underutilization of the model's capacity.","In this work, we first conduct exploratory studies to demonstrate that increasing the number of activated experts does not necessarily improve and can even degrade the output quality.","Then, we show that output distributions from an MoE model using different routing strategies substantially differ, indicating that different experts do not always act synergistically.","Motivated by these findings, we propose Self-Contrast Mixture-of-Experts (SCMoE), a training-free strategy that utilizes unchosen experts in a self-contrast manner during inference.","In SCMoE, the next-token probabilities are determined by contrasting the outputs from strong and weak activation using the same MoE model.","Our method is conceptually simple and computationally lightweight, as it incurs minimal latency compared to greedy decoding.","Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and HumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B's reasoning capability across various domains.","For example, it improves the accuracy on GSM8K from 61.79 to 66.94.","Moreover, combining SCMoE with self-consistency yields additional gains, increasing major@20 accuracy from 75.59 to 78.31."],"url":"http://arxiv.org/abs/2405.14507v1","category":"cs.CL"}
{"created":"2024-05-23 12:25:22","title":"Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data","abstract":"Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, we consider full-scale approximations (FSAs) that combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce the computational costs for calculating likelihoods, gradients, and predictive distributions with FSAs. We introduce a novel preconditioner and show that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Further, we present a novel, accurate, and fast way to calculate predictive variances relying on stochastic estimations and iterative methods. In both simulated and real-world data experiments, we find that our proposed methodology achieves the same accuracy as Cholesky-based computations with a substantial reduction in computational time. Finally, we also compare different approaches for determining inducing points in predictive process and FSA models. All methods are implemented in a free C++ software library with high-level Python and R packages.","sentences":["Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning.","However, a drawback is their limited scalability to large data sets.","To alleviate this, we consider full-scale approximations (FSAs) that combine predictive process methods and covariance tapering, thus approximating both global and local structures.","We show how iterative methods can be used to reduce the computational costs for calculating likelihoods, gradients, and predictive distributions with FSAs.","We introduce a novel preconditioner and show that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner.","Further, we present a novel, accurate, and fast way to calculate predictive variances relying on stochastic estimations and iterative methods.","In both simulated and real-world data experiments, we find that our proposed methodology achieves the same accuracy as Cholesky-based computations with a substantial reduction in computational time.","Finally, we also compare different approaches for determining inducing points in predictive process and FSA models.","All methods are implemented in a free C++ software library with high-level Python and R packages."],"url":"http://arxiv.org/abs/2405.14492v1","category":"stat.ME"}
{"created":"2024-05-23 10:54:14","title":"Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs","abstract":"Modern large language models (LLMs) have established state-of-the-art performance through architectural improvements, but still require significant computational cost for inference. In an effort to reduce the inference cost, post-training quantization (PTQ) has become a popular approach, quantizing weights and activations to lower precision, such as INT8. In this paper, we reveal the challenges of activation quantization in GLU variants, which are widely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family. The problem is that severe local quantization errors, caused by excessive magnitudes of activation in GLU variants, significantly degrade the performance of the quantized LLM. We denote these activations as activation spikes. Our further observations provide a systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of specific layers, particularly in the early and late layers, 2) The activation spikes are dedicated to a couple of tokens, rather than being shared across a sequence. Based on our observations, we propose two empirical methods, Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during quantization. Our extensive experiments validate the effectiveness of the proposed methods for the activation quantization, especially with coarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR, and Gemma. In particular, our methods enhance the current alleviation techniques (e.g., SmoothQuant) that fail to control the activation spikes. Code is available at https://github.com/onnoo/activation-spikes.","sentences":["Modern large language models (LLMs) have established state-of-the-art performance through architectural improvements, but still require significant computational cost for inference.","In an effort to reduce the inference cost, post-training quantization (PTQ) has become a popular approach, quantizing weights and activations to lower precision, such as INT8.","In this paper, we reveal the challenges of activation quantization in GLU variants, which are widely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.","The problem is that severe local quantization errors, caused by excessive magnitudes of activation in GLU variants, significantly degrade the performance of the quantized LLM.","We denote these activations as activation spikes.","Our further observations provide a systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of specific layers, particularly in the early and late layers, 2) The activation spikes are dedicated to a couple of tokens, rather than being shared across a sequence.","Based on our observations, we propose two empirical methods, Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during quantization.","Our extensive experiments validate the effectiveness of the proposed methods for the activation quantization, especially with coarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR, and Gemma.","In particular, our methods enhance the current alleviation techniques (e.g., SmoothQuant) that fail to control the activation spikes.","Code is available at https://github.com/onnoo/activation-spikes."],"url":"http://arxiv.org/abs/2405.14428v1","category":"cs.CL"}
{"created":"2024-05-23 10:11:14","title":"Qualifying and Quantifying the Benefits of Mindfulness Practices for IT Workers","abstract":"The well-being and productivity of IT workers are crucial for both individual success and the overall prosperity of the organisations they serve. This study proposes mindfulness to alleviate stress and improve mental well-being for IT workers. During an 8-week program, IT workers learn about mindfulness, coupled with breathing practices. This study investigates the potential effects of these practices by analysing participants' reflections through thematic analysis and daily well-being ratings. The analysis showcased an increase in mental well-being and perceived productivity. It also indicated a change in the participants' perception, which showed increased self-awareness. The study recommends continuing the program in the industry to see its impact on work outputs.","sentences":["The well-being and productivity of IT workers are crucial for both individual success and the overall prosperity of the organisations they serve.","This study proposes mindfulness to alleviate stress and improve mental well-being for IT workers.","During an 8-week program, IT workers learn about mindfulness, coupled with breathing practices.","This study investigates the potential effects of these practices by analysing participants' reflections through thematic analysis and daily well-being ratings.","The analysis showcased an increase in mental well-being and perceived productivity.","It also indicated a change in the participants' perception, which showed increased self-awareness.","The study recommends continuing the program in the industry to see its impact on work outputs."],"url":"http://arxiv.org/abs/2405.14393v1","category":"cs.SE"}
{"created":"2024-05-23 09:34:28","title":"Look into the Future: Deep Contextualized Sequential Recommendation","abstract":"Sequential recommendation focuses on mining useful patterns from the user behavior history to better estimate his preference on the candidate items. Previous solutions adopt recurrent networks or retrieval methods to obtain the user's profile representation so as to perform the preference estimation. In this paper, we propose a novel framework of sequential recommendation called Look into the Future (LIFT), which builds and leverages the contexts of sequential recommendation. The context in LIFT refers to a user's current profile that can be represented based on both past and future behaviors. As such, the learned context will be more effective in predicting the user's behaviors in sequential recommendation. Apparently, it is impossible to use real future information to predict the current behavior, we thus propose a novel retrieval-based framework to use the most similar interaction's future information as the future context of the target interaction without data leakage. Furthermore, in order to exploit the intrinsic information embedded within the context itself, we introduce an innovative pretraining methodology incorporating behavior masking. This approach is designed to facilitate the efficient acquisition of context representations. We demonstrate that finding relevant contexts from the global user pool via retrieval methods will greatly improve preference estimation performance. In our extensive experiments over real-world datasets, LIFT demonstrates significant performance improvement on click-through rate prediction tasks in sequential recommendation over strong baselines.","sentences":["Sequential recommendation focuses on mining useful patterns from the user behavior history to better estimate his preference on the candidate items.","Previous solutions adopt recurrent networks or retrieval methods to obtain the user's profile representation so as to perform the preference estimation.","In this paper, we propose a novel framework of sequential recommendation called Look into the Future (LIFT), which builds and leverages the contexts of sequential recommendation.","The context in LIFT refers to a user's current profile that can be represented based on both past and future behaviors.","As such, the learned context will be more effective in predicting the user's behaviors in sequential recommendation.","Apparently, it is impossible to use real future information to predict the current behavior, we thus propose a novel retrieval-based framework to use the most similar interaction's future information as the future context of the target interaction without data leakage.","Furthermore, in order to exploit the intrinsic information embedded within the context itself, we introduce an innovative pretraining methodology incorporating behavior masking.","This approach is designed to facilitate the efficient acquisition of context representations.","We demonstrate that finding relevant contexts from the global user pool via retrieval methods will greatly improve preference estimation performance.","In our extensive experiments over real-world datasets, LIFT demonstrates significant performance improvement on click-through rate prediction tasks in sequential recommendation over strong baselines."],"url":"http://arxiv.org/abs/2405.14359v1","category":"cs.IR"}
{"created":"2024-05-23 08:33:07","title":"Smooth Pseudo-Labeling","abstract":"Semi-Supervised Learning (SSL) seeks to leverage large amounts of non-annotated data along with the smallest amount possible of annotated data in order to achieve the same level of performance as if all data were annotated. A fruitful method in SSL is Pseudo-Labeling (PL), which, however, suffers from the important drawback that the associated loss function has discontinuities in its derivatives, which cause instabilities in performance when labels are very scarce. In the present work, we address this drawback with the introduction of a Smooth Pseudo-Labeling (SP L) loss function. It consists in adding a multiplicative factor in the loss function that smooths out the discontinuities in the derivative due to thresholding. In our experiments, we test our improvements on FixMatch and show that it significantly improves the performance in the regime of scarce labels, without addition of any modules, hyperparameters, or computational overhead. In the more stable regime of abundant labels, performance remains at the same level. Robustness with respect to variation of hyperparameters and training parameters is also significantly improved. Moreover, we introduce a new benchmark, where labeled images are selected randomly from the whole dataset, without imposing representation of each class proportional to its frequency in the dataset. We see that the smooth version of FixMatch does appear to perform better than the original, non-smooth implementation. However, more importantly, we notice that both implementations do not necessarily see their performance improve when labeled images are added, an important issue in the design of SSL algorithms that should be addressed so that Active Learning algorithms become more reliable and explainable.","sentences":["Semi-Supervised Learning (SSL) seeks to leverage large amounts of non-annotated data along with the smallest amount possible of annotated data in order to achieve the same level of performance as if all data were annotated.","A fruitful method in SSL is Pseudo-Labeling (PL), which, however, suffers from the important drawback that the associated loss function has discontinuities in its derivatives, which cause instabilities in performance when labels are very scarce.","In the present work, we address this drawback with the introduction of a Smooth Pseudo-Labeling (SP L) loss function.","It consists in adding a multiplicative factor in the loss function that smooths out the discontinuities in the derivative due to thresholding.","In our experiments, we test our improvements on FixMatch and show that it significantly improves the performance in the regime of scarce labels, without addition of any modules, hyperparameters, or computational overhead.","In the more stable regime of abundant labels, performance remains at the same level.","Robustness with respect to variation of hyperparameters and training parameters is also significantly improved.","Moreover, we introduce a new benchmark, where labeled images are selected randomly from the whole dataset, without imposing representation of each class proportional to its frequency in the dataset.","We see that the smooth version of FixMatch does appear to perform better than the original, non-smooth implementation.","However, more importantly, we notice that both implementations do not necessarily see their performance improve when labeled images are added, an important issue in the design of SSL algorithms that should be addressed so that Active Learning algorithms become more reliable and explainable."],"url":"http://arxiv.org/abs/2405.14313v1","category":"cs.LG"}
{"created":"2024-05-23 17:46:40","title":"Analog Counterdiabatic Quantum Computing","abstract":"We propose analog counterdiabatic quantum computing (ACQC) to tackle combinatorial optimization problems on neutral-atom quantum processors. While these devices allow for the use of hundreds of qubits, adiabatic quantum computing struggles with non-adiabatic errors, which are inevitable due to the hardware's restricted coherence time. We design counterdiabatic protocols to circumvent those limitations via ACQC on analog quantum devices with ground-Rydberg qubits. To demonstrate the effectiveness of our paradigm, we experimentally apply it to the maximum independent set (MIS) problem with up to 100 qubits and show an enhancement in the approximation ratio with a short evolution time. We believe ACQC establishes a path toward quantum advantage for a variety of industry use cases.","sentences":["We propose analog counterdiabatic quantum computing (ACQC) to tackle combinatorial optimization problems on neutral-atom quantum processors.","While these devices allow for the use of hundreds of qubits, adiabatic quantum computing struggles with non-adiabatic errors, which are inevitable due to the hardware's restricted coherence time.","We design counterdiabatic protocols to circumvent those limitations via ACQC on analog quantum devices with ground-Rydberg qubits.","To demonstrate the effectiveness of our paradigm, we experimentally apply it to the maximum independent set (MIS) problem with up to 100 qubits and show an enhancement in the approximation ratio with a short evolution time.","We believe ACQC establishes a path toward quantum advantage for a variety of industry use cases."],"url":"http://arxiv.org/abs/2405.14829v1","category":"quant-ph"}
{"created":"2024-05-23 13:28:52","title":"Efficient recursive encoders for quantum Reed-Muller codes towards Fault tolerance","abstract":"Transversal gates are logical gate operations on encoded quantum information that are efficient in gate count and depth, and are designed to minimize error propagation. Efficient encoding circuits for quantum codes that admit transversal gates are thus crucial to reduce noise and realize useful quantum computers. The class of punctured Quantum Reed-Muller codes admit transversal gates. We construct resource efficient recursive encoders for the class of quantum codes constructed from Reed-Muller and punctured Reed-Muller codes. These encoders on $n$ qubits have circuit depth of $O(\\log n)$ and lower gate counts compared to previous works. The number of CNOT gates in the encoder across bi-partitions of the qubits is found to be equal to the entanglement entropy across these partitions, demonstrating that the encoder is optimal in terms of CNOT gates across these partitions. Finally, connecting these ideas, we explicitly show that entanglement can be extracted from QRM codewords.","sentences":["Transversal gates are logical gate operations on encoded quantum information that are efficient in gate count and depth, and are designed to minimize error propagation.","Efficient encoding circuits for quantum codes that admit transversal gates are thus crucial to reduce noise and realize useful quantum computers.","The class of punctured Quantum Reed-Muller codes admit transversal gates.","We construct resource efficient recursive encoders for the class of quantum codes constructed from Reed-Muller and punctured Reed-Muller codes.","These encoders on $n$ qubits have circuit depth of $O(\\log n)$ and lower gate counts compared to previous works.","The number of CNOT gates in the encoder across bi-partitions of the qubits is found to be equal to the entanglement entropy across these partitions, demonstrating that the encoder is optimal in terms of CNOT gates across these partitions.","Finally, connecting these ideas, we explicitly show that entanglement can be extracted from QRM codewords."],"url":"http://arxiv.org/abs/2405.14549v1","category":"quant-ph"}
{"created":"2024-05-23 11:31:01","title":"Large-Scale Epitaxial Integration of Single-Crystalline BiSb Topological Insulator on GaAs (111)A","abstract":"Topological insulators (TI) are promising materials for future spintronics applications and their epitaxial integration would allow the realization of new hybrid interfaces. As the first materials studied, Bismuth Antimony alloys (Bi1-xSbx) show great potential due to their tuneable electronic band structure and efficient charge-to-spin conversion. Here, we report the growth of Bi1-xSbx thin films on GaAs (111)A substrates following two different protocols. For the conventional epitaxy process, the grown films show excellent crystallinity and twin domains corresponding to an in-plane 180{\\textdegree} rotation of the crystalline structure. Domain walls are found to be composition-dependent and have a lower density for Antimony-rich films. For the optimized process, depositing an Antimony bilayer prior to BiSb growth allows achieving single crystallinity of the TI films. The topologically protected surface states are evidenced by ex-situ ARPES measurements for domains-free and conventional films. To the best of our knowledge, this work presents the first large-scale epitaxial integration of single crystalline Bi1-xSbx thin films on industrial substrates.","sentences":["Topological insulators (TI) are promising materials for future spintronics applications and their epitaxial integration would allow the realization of new hybrid interfaces.","As the first materials studied, Bismuth Antimony alloys (Bi1-xSbx) show great potential due to their tuneable electronic band structure and efficient charge-to-spin conversion.","Here, we report the growth of Bi1-xSbx thin films on GaAs (111)A substrates following two different protocols.","For the conventional epitaxy process, the grown films show excellent crystallinity and twin domains corresponding to an in-plane 180{\\textdegree} rotation of the crystalline structure.","Domain walls are found to be composition-dependent and have a lower density for Antimony-rich films.","For the optimized process, depositing an Antimony bilayer prior to BiSb growth allows achieving single crystallinity of the TI films.","The topologically protected surface states are evidenced by ex-situ ARPES measurements for domains-free and conventional films.","To the best of our knowledge, this work presents the first large-scale epitaxial integration of single crystalline Bi1-xSbx thin films on industrial substrates."],"url":"http://arxiv.org/abs/2405.14450v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 10:40:36","title":"Charged Particles Orbiting a Weakly Magnetized Black Hole Immersed in Quintessential Matter","abstract":"A new exact magnetized solution describing a Kiselev black hole immersed in a magnetic field is used for studying the dynamics of charged particles. For a weak magnetic induction, we employ a first-order perturbative approach to analyze the perturbed circular orbits near the minimum of the effective potential. We obtain an approximate solution for the bounded equatorial trajectory subjected to small radial and latitudinal oscillations. The shape of the trajectory localized near the stable circular orbit depends on the relation between the attractive gravitational force and the repulsive quintessence contribution.","sentences":["A new exact magnetized solution describing a Kiselev black hole immersed in a magnetic field is used for studying the dynamics of charged particles.","For a weak magnetic induction, we employ a first-order perturbative approach to analyze the perturbed circular orbits near the minimum of the effective potential.","We obtain an approximate solution for the bounded equatorial trajectory subjected to small radial and latitudinal oscillations.","The shape of the trajectory localized near the stable circular orbit depends on the relation between the attractive gravitational force and the repulsive quintessence contribution."],"url":"http://arxiv.org/abs/2405.14420v1","category":"gr-qc"}
{"created":"2024-05-23 10:21:18","title":"Representative electricity price profiles for European day-ahead and intraday spot markets","abstract":"We propose a method to construct representative price profiles of the day-ahead (DA) and the intraday (ID) electricity spot markets and use this method to provide examples of ready-to-use price data sets. In contrast to common scenario generation approaches, the method is deterministic and relies on a small number of degrees of freedom, with the aim to be well defined and easy to use. We thereby target an enhanced comparability of future research studies on demand-side management and energy cost optimization. We construct the price profiles based on historical time series from the spot markets of interest, e.g., European Power Exchange (EPEX) spot. To this end, we extract key price components from the data while also accounting for known dominant mechanisms in the price variation. Further, the method is able to preserve key statistical features of the historical data (e.g., mean and standard deviation) when constructing the benchmark profile. Finally, our approach ensures comparability of ID and DA price profiles by design, as their cumulative (integral) price can be made identical if needed.","sentences":["We propose a method to construct representative price profiles of the day-ahead (DA) and the intraday (ID) electricity spot markets and use this method to provide examples of ready-to-use price data sets.","In contrast to common scenario generation approaches, the method is deterministic and relies on a small number of degrees of freedom, with the aim to be well defined and easy to use.","We thereby target an enhanced comparability of future research studies on demand-side management and energy cost optimization.","We construct the price profiles based on historical time series from the spot markets of interest, e.g., European Power Exchange (EPEX) spot.","To this end, we extract key price components from the data while also accounting for known dominant mechanisms in the price variation.","Further, the method is able to preserve key statistical features of the historical data (e.g., mean and standard deviation) when constructing the benchmark profile.","Finally, our approach ensures comparability of ID and DA price profiles by design, as their cumulative (integral) price can be made identical if needed."],"url":"http://arxiv.org/abs/2405.14403v1","category":"stat.AP"}
{"created":"2024-05-23 09:59:00","title":"A high-level comparison of state-of-the-art quantum algorithms for breaking asymmetric cryptography","abstract":"We provide a high-level cost comparison between Regev's quantum algorithm with Eker{\\aa}-G\\\"artner's extensions on the one hand, and existing state-of-the-art quantum algorithms for factoring and computing discrete logarithms on the other. This when targeting cryptographically relevant problem instances, and when accounting for the space-saving optimizations of Ragavan and Vaikuntanathan that apply to Regev's algorithm, and optimizations such as windowing that apply to the existing algorithms.   Our conclusion is that Regev's algorithm without the space-saving optimizations may achieve a per-run advantage, but not an overall advantage, if non-computational quantum memory is cheap. Regev's algorithm with the space-saving optimizations does not achieve an advantage, since it uses more computational memory, whilst also performing more work, per run and overall, compared to the existing state-of-the-art algorithms. As such, further optimizations are required for it to achieve an advantage for cryptographically relevant problem instances.","sentences":["We provide a high-level cost comparison between Regev's quantum algorithm with Eker{\\aa}-G\\\"artner's extensions on the one hand, and existing state-of-the-art quantum algorithms for factoring and computing discrete logarithms on the other.","This when targeting cryptographically relevant problem instances, and when accounting for the space-saving optimizations of Ragavan and Vaikuntanathan that apply to Regev's algorithm, and optimizations such as windowing that apply to the existing algorithms.   ","Our conclusion is that Regev's algorithm without the space-saving optimizations may achieve a per-run advantage, but not an overall advantage, if non-computational quantum memory is cheap.","Regev's algorithm with the space-saving optimizations does not achieve an advantage, since it uses more computational memory, whilst also performing more work, per run and overall, compared to the existing state-of-the-art algorithms.","As such, further optimizations are required for it to achieve an advantage for cryptographically relevant problem instances."],"url":"http://arxiv.org/abs/2405.14381v1","category":"cs.CR"}
{"created":"2024-05-23 09:11:47","title":"RoGS: Large Scale Road Surface Reconstruction based on 2D Gaussian Splatting","abstract":"Road surface reconstruction plays a crucial role in autonomous driving, which can be used for road lane perception and autolabeling tasks. Recently, mesh-based road surface reconstruction algorithms show promising reconstruction results. However, these mesh-based methods suffer from slow speed and poor rendering quality. In contrast, the 3D Gaussian Splatting (3DGS) shows superior rendering speed and quality. Although 3DGS employs explicit Gaussian spheres to represent the scene, it lacks the ability to directly represent the geometric information of the scene. To address this limitation, we propose a novel large-scale road surface reconstruction approach based on 2D Gaussian Splatting (2DGS), named RoGS. The geometric shape of the road is explicitly represented using 2D Gaussian surfels, where each surfel stores color, semantics, and geometric information. Compared to Gaussian spheres, the Gaussian surfels aligns more closely with the physical reality of the road. Distinct from previous initialization methods that rely on point clouds for Gaussian spheres, we introduce a trajectory-based initialization for Gaussian surfels. Thanks to the explicit representation of the Gaussian surfels and a good initialization, our method achieves a significant acceleration while improving reconstruction quality. We achieve excellent results in reconstruction of roads surfaces in a variety of challenging real-world scenes.","sentences":["Road surface reconstruction plays a crucial role in autonomous driving, which can be used for road lane perception and autolabeling tasks.","Recently, mesh-based road surface reconstruction algorithms show promising reconstruction results.","However, these mesh-based methods suffer from slow speed and poor rendering quality.","In contrast, the 3D Gaussian Splatting (3DGS) shows superior rendering speed and quality.","Although 3DGS employs explicit Gaussian spheres to represent the scene, it lacks the ability to directly represent the geometric information of the scene.","To address this limitation, we propose a novel large-scale road surface reconstruction approach based on 2D Gaussian Splatting (2DGS), named RoGS.","The geometric shape of the road is explicitly represented using 2D Gaussian surfels, where each surfel stores color, semantics, and geometric information.","Compared to Gaussian spheres, the Gaussian surfels aligns more closely with the physical reality of the road.","Distinct from previous initialization methods that rely on point clouds for Gaussian spheres, we introduce a trajectory-based initialization for Gaussian surfels.","Thanks to the explicit representation of the Gaussian surfels and a good initialization, our method achieves a significant acceleration while improving reconstruction quality.","We achieve excellent results in reconstruction of roads surfaces in a variety of challenging real-world scenes."],"url":"http://arxiv.org/abs/2405.14342v1","category":"cs.CV"}
{"created":"2024-05-23 09:00:15","title":"A confined random walk locally looks like tilted random interlacements","abstract":"In this paper we consider the simple random walk on $\\mathbb{Z}^d$, $d \\geq 3$, conditioned to stay in a large domain $D_N$ of typical diameter $N$. Considering the range up to time $t_N \\geq N^{2+\\delta}$ for some $\\delta > 0$, we establish a coupling with what Teixeira (2009) and Li & Sznitman (2014) defined as \"tilted random interlacements\". This tilted interlacement can be described as random interlacements but with trajectories given by random walks on conductances $c_N(x,y) = \\phi_N(x) \\phi_N(y)$, where $\\phi_N$ is the first eigenvector of the discrete Laplace-Beltrami operator on $D_N$. The coupling follows the methodology of the soft local times, introduced by Popov & Teixeira (2015) and used by \\v{C}ern\\'y & Teixeira (2016) to prove the well-known coupling between the simple random walk on the torus and the random interlacements.","sentences":["In this paper we consider the simple random walk on $\\mathbb{Z}^d$, $d \\geq 3$, conditioned to stay in a large domain $D_N$ of typical diameter $N$. Considering the range up to time $t_N \\geq N^{2+\\delta}$ for some $\\delta > 0$, we establish a coupling with what Teixeira (2009) and Li & Sznitman (2014) defined as \"tilted random interlacements\".","This tilted interlacement can be described as random interlacements but with trajectories given by random walks on conductances $c_N(x,y) = \\phi_N(x) \\phi_N(y)$, where $\\phi_N$ is the first eigenvector of the discrete Laplace-Beltrami operator on $D_N$. The coupling follows the methodology of the soft local times, introduced by Popov & Teixeira (2015) and used by \\v{C}ern\\'y & Teixeira (2016) to prove the well-known coupling between the simple random walk on the torus and the random interlacements."],"url":"http://arxiv.org/abs/2405.14329v1","category":"math.PR"}
{"created":"2024-05-23 08:53:24","title":"An 808 Line Phasor-Based Ddehomogenisation Matlab Code For Multi-Scale Topology Optimisation","abstract":"This work presents an 808-line Matlab educational code for combined multi-scale topology optimisation and phasor-based dehomogenisation titled deHomTop808. The multi-scale formulation utilises homogenisation of optimal microstructures to facilitate efficient coarse-scale optimisation. Dehomogenisation allows for a high-resolution single-scale reconstruction of the optimised multi-scale structure, achieving minor losses in structural performance, at a fraction of the computational cost, compared to its large-scale topology optimisation counterpart. The presented code utilises stiffness optimal Rank-2 microstructures to minimise the compliance of a single-load case problem, subject to a volume fraction constraint. By exploiting the inherent efficiency benefits of the phasor-based dehomogenisation procedure, on-the-fly dehomogenisation to a single-scale structure is obtained. The presented code includes procedures for structural verification of the final dehomogenised structure by comparison to the multi-scale solution. The code is introduced in terms of the underlying theory and its major components, including examples and potential extensions, and can be downloaded from https://github.com/peterdorffler/deHomTop808.git.","sentences":["This work presents an 808-line Matlab educational code for combined multi-scale topology optimisation and phasor-based dehomogenisation titled deHomTop808.","The multi-scale formulation utilises homogenisation of optimal microstructures to facilitate efficient coarse-scale optimisation.","Dehomogenisation allows for a high-resolution single-scale reconstruction of the optimised multi-scale structure, achieving minor losses in structural performance, at a fraction of the computational cost, compared to its large-scale topology optimisation counterpart.","The presented code utilises stiffness optimal Rank-2 microstructures to minimise the compliance of a single-load case problem, subject to a volume fraction constraint.","By exploiting the inherent efficiency benefits of the phasor-based dehomogenisation procedure, on-the-fly dehomogenisation to a single-scale structure is obtained.","The presented code includes procedures for structural verification of the final dehomogenised structure by comparison to the multi-scale solution.","The code is introduced in terms of the underlying theory and its major components, including examples and potential extensions, and can be downloaded from https://github.com/peterdorffler/deHomTop808.git."],"url":"http://arxiv.org/abs/2405.14321v1","category":"cs.MS"}
{"created":"2024-05-23 08:30:34","title":"Employing weak-field homodyne detection for quantum communications","abstract":"We investigate the role of weak-field homodyne (WF) measurement for quantum communications over a lossy bosonic channel with coherent state encoding. This kind of receiver employs photon-number resolving (PNR) detectors with finite resolution and low-intensity local oscillator. As a figure of merit, we consider the mutual information for a Gaussian input modulation. We prove an enhancement over Shannon capacity in the photon starved regime, obtained by exploiting information on the mean signal energy to suitably optimize the local oscillator intensity. Thereafter, we investigate the performance of non-Gaussian modulation, by considering a Gamma distribution of the energy of the encoded pulses, and achieve an increase in the information rate with respect to the Gaussian modulation case in the intermediate energy regime, being more accentuated for low values of the PNR resolution.","sentences":["We investigate the role of weak-field homodyne (WF) measurement for quantum communications over a lossy bosonic channel with coherent state encoding.","This kind of receiver employs photon-number resolving (PNR) detectors with finite resolution and low-intensity local oscillator.","As a figure of merit, we consider the mutual information for a Gaussian input modulation.","We prove an enhancement over Shannon capacity in the photon starved regime, obtained by exploiting information on the mean signal energy to suitably optimize the local oscillator intensity.","Thereafter, we investigate the performance of non-Gaussian modulation, by considering a Gamma distribution of the energy of the encoded pulses, and achieve an increase in the information rate with respect to the Gaussian modulation case in the intermediate energy regime, being more accentuated for low values of the PNR resolution."],"url":"http://arxiv.org/abs/2405.14310v1","category":"quant-ph"}
{"created":"2024-05-23 08:00:07","title":"Transient Nonlinear Electrothermal Adjoint Sensitivity Analysis for HVDC Cable Joints","abstract":"Efficient computation of sensitivities is a promising approach for efficiently of designing and optimizing high voltage direct current cable joints. This paper presents the adjoint variable method for coupled nonlinear transient electrothermal problems as an efficient approach to compute sensitivities with respect to a large number of design parameters. The method is used to compute material sensitivities of a 320kV high voltage direct current cable joint specimen. The results are validated against sensitivities obtained via the direct sensitivity method.","sentences":["Efficient computation of sensitivities is a promising approach for efficiently of designing and optimizing high voltage direct current cable joints.","This paper presents the adjoint variable method for coupled nonlinear transient electrothermal problems as an efficient approach to compute sensitivities with respect to a large number of design parameters.","The method is used to compute material sensitivities of a 320kV high voltage direct current cable joint specimen.","The results are validated against sensitivities obtained via the direct sensitivity method."],"url":"http://arxiv.org/abs/2405.14284v1","category":"cs.CE"}
{"created":"2024-05-23 07:43:14","title":"Optimizing Four-Wave Mixing in Rydberg Atoms for Microwave-Optical Conversion","abstract":"We perform a numerical and analytical investigation of microwave-to-optical conversion based on four-wave mixing in Rydberg atoms. Our work demonstrates that both all-resonant and off-resonant frequency-mixing configurations achieve near-unit photon conversion efficiencies. We review the conditions that can lead to the presence of two possible dark states. We find that for both configurations, one of the dark states can be detrimental at high microwave powers, and show that an additional limitation to all-resonant frequency mixing is microwave-induced fluorescence. Finally, we confirm that the off-resonant configuration is more appropriate as it allows for efficient photon conversion on a wider range of input microwave intensities with reduced total power of the auxiliary fields.","sentences":["We perform a numerical and analytical investigation of microwave-to-optical conversion based on four-wave mixing in Rydberg atoms.","Our work demonstrates that both all-resonant and off-resonant frequency-mixing configurations achieve near-unit photon conversion efficiencies.","We review the conditions that can lead to the presence of two possible dark states.","We find that for both configurations, one of the dark states can be detrimental at high microwave powers, and show that an additional limitation to all-resonant frequency mixing is microwave-induced fluorescence.","Finally, we confirm that the off-resonant configuration is more appropriate as it allows for efficient photon conversion on a wider range of input microwave intensities with reduced total power of the auxiliary fields."],"url":"http://arxiv.org/abs/2405.14263v1","category":"physics.atom-ph"}
{"created":"2024-05-23 07:41:40","title":"Unlocking Profit Potential: Maximizing Returns with Bayesian Optimization of Supertrend Indicator Parameters","abstract":"This paper investigates the potential of Bayesian optimization (BO) to optimize the atr multiplier and atr period -the parameters of the Supertrend indicator for maximizing trading profits across diverse stock datasets. By employing BO, the thesis aims to automate the identification of optimal parameter settings, leading to a more data-driven and potentially more profitable trading strategy compared to relying on manually chosen parameters. The effectiveness of the BO-optimized Supertrend strategy will be evaluated through backtesting on a variety of stock datasets.","sentences":["This paper investigates the potential of Bayesian optimization (BO) to optimize the atr multiplier and atr period -the parameters of the Supertrend indicator for maximizing trading profits across diverse stock datasets.","By employing BO, the thesis aims to automate the identification of optimal parameter settings, leading to a more data-driven and potentially more profitable trading strategy compared to relying on manually chosen parameters.","The effectiveness of the BO-optimized Supertrend strategy will be evaluated through backtesting on a variety of stock datasets."],"url":"http://arxiv.org/abs/2405.14262v1","category":"q-fin.TR"}
{"created":"2024-05-23 06:32:27","title":"Survey on Visual Signal Coding and Processing with Generative Models: Technologies, Standards and Optimization","abstract":"This paper provides a survey of the latest developments in visual signal coding and processing with generative models. Specifically, our focus is on presenting the advancement of generative models and their influence on research in the domain of visual signal coding and processing. This survey study begins with a brief introduction of well-established generative models, including the Variational Autoencoder (VAE) models, Generative Adversarial Network (GAN) models, Autoregressive (AR) models, Normalizing Flows and Diffusion models. The subsequent section of the paper explores the advancements in visual signal coding based on generative models, as well as the ongoing international standardization activities. In the realm of visual signal processing, our focus lies on the application and development of various generative models in the research of visual signal restoration. We also present the latest developments in generative visual signal synthesis and editing, along with visual signal quality assessment using generative models and quality assessment for generative models. The practical implementation of these studies is closely linked to the investigation of fast optimization. This paper additionally presents the latest advancements in fast optimization on visual signal coding and processing with generative models. We hope to advance this field by providing researchers and practitioners a comprehensive literature review on the topic of visual signal coding and processing with generative models.","sentences":["This paper provides a survey of the latest developments in visual signal coding and processing with generative models.","Specifically, our focus is on presenting the advancement of generative models and their influence on research in the domain of visual signal coding and processing.","This survey study begins with a brief introduction of well-established generative models, including the Variational Autoencoder (VAE) models, Generative Adversarial Network (GAN) models, Autoregressive (AR) models, Normalizing Flows and Diffusion models.","The subsequent section of the paper explores the advancements in visual signal coding based on generative models, as well as the ongoing international standardization activities.","In the realm of visual signal processing, our focus lies on the application and development of various generative models in the research of visual signal restoration.","We also present the latest developments in generative visual signal synthesis and editing, along with visual signal quality assessment using generative models and quality assessment for generative models.","The practical implementation of these studies is closely linked to the investigation of fast optimization.","This paper additionally presents the latest advancements in fast optimization on visual signal coding and processing with generative models.","We hope to advance this field by providing researchers and practitioners a comprehensive literature review on the topic of visual signal coding and processing with generative models."],"url":"http://arxiv.org/abs/2405.14221v1","category":"eess.IV"}
{"created":"2024-05-23 06:17:23","title":"From Text to Pixel: Advancing Long-Context Understanding in MLLMs","abstract":"The rapid progress in Multimodal Large Language Models (MLLMs) has significantly advanced their ability to process and understand complex visual and textual information. However, the integration of multiple images and extensive textual contexts remains a challenge due to the inherent limitation of the models' capacity to handle long input sequences efficiently. In this paper, we introduce SEEKER, a multimodal large language model designed to tackle this issue. SEEKER aims to optimize the compact encoding of long text by compressing the text sequence into the visual pixel space via images, enabling the model to handle long text within a fixed token-length budget efficiently. Our empirical experiments on six long-context multimodal tasks demonstrate that SEEKER can leverage fewer image tokens to convey the same amount of textual information compared with the OCR-based approach, and is more efficient in understanding long-form multimodal input and generating long-form textual output, outperforming all existing proprietary and open-source MLLMs by large margins.","sentences":["The rapid progress in Multimodal Large Language Models (MLLMs) has significantly advanced their ability to process and understand complex visual and textual information.","However, the integration of multiple images and extensive textual contexts remains a challenge due to the inherent limitation of the models' capacity to handle long input sequences efficiently.","In this paper, we introduce SEEKER, a multimodal large language model designed to tackle this issue.","SEEKER aims to optimize the compact encoding of long text by compressing the text sequence into the visual pixel space via images, enabling the model to handle long text within a fixed token-length budget efficiently.","Our empirical experiments on six long-context multimodal tasks demonstrate that SEEKER can leverage fewer image tokens to convey the same amount of textual information compared with the OCR-based approach, and is more efficient in understanding long-form multimodal input and generating long-form textual output, outperforming all existing proprietary and open-source MLLMs by large margins."],"url":"http://arxiv.org/abs/2405.14213v1","category":"cs.CV"}
{"created":"2024-05-23 06:09:08","title":"Eidos: Efficient, Imperceptible Adversarial 3D Point Clouds","abstract":"Classification of 3D point clouds is a challenging machine learning (ML) task with important real-world applications in a spectrum from autonomous driving and robot-assisted surgery to earth observation from low orbit. As with other ML tasks, classification models are notoriously brittle in the presence of adversarial attacks. These are rooted in imperceptible changes to inputs with the effect that a seemingly well-trained model ends up misclassifying the input. This paper adds to the understanding of adversarial attacks by presenting Eidos, a framework providing Efficient Imperceptible aDversarial attacks on 3D pOint cloudS. Eidos supports a diverse set of imperceptibility metrics. It employs an iterative, two-step procedure to identify optimal adversarial examples, thereby enabling a runtime-imperceptibility trade-off. We provide empirical evidence relative to several popular 3D point cloud classification models and several established 3D attack methods, showing Eidos' superiority with respect to efficiency as well as imperceptibility.","sentences":["Classification of 3D point clouds is a challenging machine learning (ML) task with important real-world applications in a spectrum from autonomous driving and robot-assisted surgery to earth observation from low orbit.","As with other ML tasks, classification models are notoriously brittle in the presence of adversarial attacks.","These are rooted in imperceptible changes to inputs with the effect that a seemingly well-trained model ends up misclassifying the input.","This paper adds to the understanding of adversarial attacks by presenting Eidos, a framework providing Efficient Imperceptible aDversarial attacks on 3D pOint cloudS. Eidos supports a diverse set of imperceptibility metrics.","It employs an iterative, two-step procedure to identify optimal adversarial examples, thereby enabling a runtime-imperceptibility trade-off.","We provide empirical evidence relative to several popular 3D point cloud classification models and several established 3D attack methods, showing Eidos' superiority with respect to efficiency as well as imperceptibility."],"url":"http://arxiv.org/abs/2405.14210v1","category":"cs.CV"}
{"created":"2024-05-23 06:06:13","title":"The Boolean polynomial polytope with multiple choice constraints","abstract":"We propose a polytope to study multiple choice polynomial programming (MCPP). It is the convex hull of $0$-$1$ vectors satisfying multiple choice constraints and production constraints both of which are associated with a hypergraph. With the help of the decomposability property, we obtain an explicit half-space representation of the MCPP polytope when the underlying hypergraph is $\\alpha$-acyclic by induction on the number of hyperedges. We also present a necessary and sufficient condition for the inequalities lifted from the facet-inducing ones for the multilinear polytope to be still facet-inducing for the MCPP polytope.","sentences":["We propose a polytope to study multiple choice polynomial programming (MCPP).","It is the convex hull of $0$-$1$ vectors satisfying multiple choice constraints and production constraints both of which are associated with a hypergraph.","With the help of the decomposability property, we obtain an explicit half-space representation of the MCPP polytope when the underlying hypergraph is $\\alpha$-acyclic by induction on the number of hyperedges.","We also present a necessary and sufficient condition for the inequalities lifted from the facet-inducing ones for the multilinear polytope to be still facet-inducing for the MCPP polytope."],"url":"http://arxiv.org/abs/2405.14207v1","category":"math.OC"}
{"created":"2024-05-23 05:31:41","title":"Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs","abstract":"With the rising popularity of Large Language Models (LLMs), assessing their trustworthiness through security tasks has gained critical importance. Regarding the new task of universal goal hijacking, previous efforts have concentrated solely on optimization algorithms, overlooking the crucial role of the prompt. To fill this gap, we propose a universal goal hijacking method called POUGH that incorporates semantic-guided prompt processing strategies. Specifically, the method starts with a sampling strategy to select representative prompts from a candidate pool, followed by a ranking strategy that prioritizes the prompts. Once the prompts are organized sequentially, the method employs an iterative optimization algorithm to generate the universal fixed suffix for the prompts. Experiments conducted on four popular LLMs and ten types of target responses verified the effectiveness of our method.","sentences":["With the rising popularity of Large Language Models (LLMs), assessing their trustworthiness through security tasks has gained critical importance.","Regarding the new task of universal goal hijacking, previous efforts have concentrated solely on optimization algorithms, overlooking the crucial role of the prompt.","To fill this gap, we propose a universal goal hijacking method called POUGH that incorporates semantic-guided prompt processing strategies.","Specifically, the method starts with a sampling strategy to select representative prompts from a candidate pool, followed by a ranking strategy that prioritizes the prompts.","Once the prompts are organized sequentially, the method employs an iterative optimization algorithm to generate the universal fixed suffix for the prompts.","Experiments conducted on four popular LLMs and ten types of target responses verified the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.14189v1","category":"cs.CL"}
{"created":"2024-05-23 05:29:29","title":"A structure-aware framework for learning device placements on computation graphs","abstract":"Existing approaches for device placement ignore the topological features of computation graphs and rely mostly on heuristic methods for graph partitioning. At the same time, they either follow a grouper-placer or an encoder-placer architecture, which requires understanding the interaction structure between code operations. To bridge the gap between encoder-placer and grouper-placer techniques, we propose a novel framework for the task of device placement, relying on smaller computation graphs extracted from the OpenVINO toolkit using reinforcement learning. The framework consists of five steps, including graph coarsening, node representation learning and policy optimization. It facilitates end-to-end training and takes into consideration the directed and acyclic nature of the computation graphs. We also propose a model variant, inspired by graph parsing networks and complex network analysis, enabling graph representation learning and personalized graph partitioning jointly, using an unspecified number of groups. To train the entire framework, we utilize reinforcement learning techniques by employing the execution time of the suggested device placements to formulate the reward. We demonstrate the flexibility and effectiveness of our approach through multiple experiments with three benchmark models, namely Inception-V3, ResNet, and BERT. The robustness of the proposed framework is also highlighted through an ablation study. The suggested placements improve the inference speed for the benchmark models by up to $58.2\\%$ over CPU execution and by up to $60.24\\%$ compared to other commonly used baselines.","sentences":["Existing approaches for device placement ignore the topological features of computation graphs and rely mostly on heuristic methods for graph partitioning.","At the same time, they either follow a grouper-placer or an encoder-placer architecture, which requires understanding the interaction structure between code operations.","To bridge the gap between encoder-placer and grouper-placer techniques, we propose a novel framework for the task of device placement, relying on smaller computation graphs extracted from the OpenVINO toolkit using reinforcement learning.","The framework consists of five steps, including graph coarsening, node representation learning and policy optimization.","It facilitates end-to-end training and takes into consideration the directed and acyclic nature of the computation graphs.","We also propose a model variant, inspired by graph parsing networks and complex network analysis, enabling graph representation learning and personalized graph partitioning jointly, using an unspecified number of groups.","To train the entire framework, we utilize reinforcement learning techniques by employing the execution time of the suggested device placements to formulate the reward.","We demonstrate the flexibility and effectiveness of our approach through multiple experiments with three benchmark models, namely Inception-V3, ResNet, and BERT.","The robustness of the proposed framework is also highlighted through an ablation study.","The suggested placements improve the inference speed for the benchmark models by up to $58.2\\%$ over CPU execution and by up to $60.24\\%$ compared to other commonly used baselines."],"url":"http://arxiv.org/abs/2405.14185v1","category":"cs.LG"}
{"created":"2024-05-23 05:03:25","title":"Long Time Behavior of Optimal Liquidation Problems","abstract":"In this paper, we study the long time behavior of an optimal liquidation problem with semimartingale strategies and external flows. To investigate the limit rigorously, we study the convergence of three BSDEs characterizing the value function and the optimal strategy, from finite horizon to infinite horizon. We find that in the long time limit the player may not necessarily liquidate her assets at all due to the existence of external flows, even if in any given finite time horizon, the player is forced to liquidate all assets. Moreover, when the intensity of the external flow is damped, the player will liquidate her assets in the long run.","sentences":["In this paper, we study the long time behavior of an optimal liquidation problem with semimartingale strategies and external flows.","To investigate the limit rigorously, we study the convergence of three BSDEs characterizing the value function and the optimal strategy, from finite horizon to infinite horizon.","We find that in the long time limit the player may not necessarily liquidate her assets at all due to the existence of external flows, even if in any given finite time horizon, the player is forced to liquidate all assets.","Moreover, when the intensity of the external flow is damped, the player will liquidate her assets in the long run."],"url":"http://arxiv.org/abs/2405.14177v1","category":"q-fin.MF"}
{"created":"2024-05-23 04:12:16","title":"Computation-efficient Virtual Sensing Approach with Multichannel Adjoint Least Mean Square Algorithm","abstract":"Multichannel active noise control (ANC) systems are designed to create a large zone of quietness (ZoQ) around the error microphones, however, the placement of these microphones often presents challenges due to physical limitations. Virtual sensing technique that effectively suppresses the noise far from the physical error microphones is one of the most promising solutions. Nevertheless, the conventional multichannel virtual sensing ANC (MVANC) system based on the multichannel filtered reference least mean square (MCFxLMS) algorithm often suffers from high computational complexity. This paper proposes a feedforward MVANC system that incorporates the multichannel adjoint least mean square (MCALMS) algorithm to overcome these limitations effectively. Computational analysis demonstrates the improvement of computational efficiency and numerical simulations exhibit comparable noise reduction performance at virtual locations compared to the conventional MCFxLMS algorithm. Additionally, the effects of varied tuning noises on system performance are also investigated, providing insightful findings on optimizing MVANC systems.","sentences":["Multichannel active noise control (ANC) systems are designed to create a large zone of quietness (ZoQ) around the error microphones, however, the placement of these microphones often presents challenges due to physical limitations.","Virtual sensing technique that effectively suppresses the noise far from the physical error microphones is one of the most promising solutions.","Nevertheless, the conventional multichannel virtual sensing ANC (MVANC) system based on the multichannel filtered reference least mean square (MCFxLMS) algorithm often suffers from high computational complexity.","This paper proposes a feedforward MVANC system that incorporates the multichannel adjoint least mean square (MCALMS) algorithm to overcome these limitations effectively.","Computational analysis demonstrates the improvement of computational efficiency and numerical simulations exhibit comparable noise reduction performance at virtual locations compared to the conventional MCFxLMS algorithm.","Additionally, the effects of varied tuning noises on system performance are also investigated, providing insightful findings on optimizing MVANC systems."],"url":"http://arxiv.org/abs/2405.14158v1","category":"eess.SP"}
{"created":"2024-05-23 03:20:51","title":"RET-CLIP: A Retinal Image Foundation Model Pre-trained with Clinical Diagnostic Reports","abstract":"The Vision-Language Foundation model is increasingly investigated in the fields of computer vision and natural language processing, yet its exploration in ophthalmology and broader medical applications remains limited. The challenge is the lack of labeled data for the training of foundation model. To handle this issue, a CLIP-style retinal image foundation model is developed in this paper. Our foundation model, RET-CLIP, is specifically trained on a dataset of 193,865 patients to extract general features of color fundus photographs (CFPs), employing a tripartite optimization strategy to focus on left eye, right eye, and patient level to reflect real-world clinical scenarios. Extensive experiments demonstrate that RET-CLIP outperforms existing benchmarks across eight diverse datasets spanning four critical diagnostic categories: diabetic retinopathy, glaucoma, multiple disease diagnosis, and multi-label classification of multiple diseases, which demonstrate the performance and generality of our foundation model. The sourse code and pre-trained model are available at https://github.com/sStonemason/RET-CLIP.","sentences":["The Vision-Language Foundation model is increasingly investigated in the fields of computer vision and natural language processing, yet its exploration in ophthalmology and broader medical applications remains limited.","The challenge is the lack of labeled data for the training of foundation model.","To handle this issue, a CLIP-style retinal image foundation model is developed in this paper.","Our foundation model, RET-CLIP, is specifically trained on a dataset of 193,865 patients to extract general features of color fundus photographs (CFPs), employing a tripartite optimization strategy to focus on left eye, right eye, and patient level to reflect real-world clinical scenarios.","Extensive experiments demonstrate that RET-CLIP outperforms existing benchmarks across eight diverse datasets spanning four critical diagnostic categories: diabetic retinopathy, glaucoma, multiple disease diagnosis, and multi-label classification of multiple diseases, which demonstrate the performance and generality of our foundation model.","The sourse code and pre-trained model are available at https://github.com/sStonemason/RET-CLIP."],"url":"http://arxiv.org/abs/2405.14137v1","category":"cs.CV"}
{"created":"2024-05-23 03:15:20","title":"Sharp convergence rate on Schr\u00f6dinger type operators","abstract":"For Schr\\\"{o}dinger type operators in one dimension, we consider the relationship between the convergence rate and the regularity for initial data. By establishing the associated frequency-localized maximal estimates, we prove sharp results up to the endpoints. The optimal range for the wave operator in all dimensions is also obtained.","sentences":["For Schr\\\"{o}dinger type operators in one dimension, we consider the relationship between the convergence rate and the regularity for initial data.","By establishing the associated frequency-localized maximal estimates, we prove sharp results up to the endpoints.","The optimal range for the wave operator in all dimensions is also obtained."],"url":"http://arxiv.org/abs/2405.14134v1","category":"math.AP"}
{"created":"2024-05-23 02:47:39","title":"DendroPy 5: a mature Python library for phylogenetic computing","abstract":"Contemporary bioinformatics has seen in profound new visibility into the composition, structure, and history of the natural world around us. Arguably, the central pillar of bioinformatics is phylogenetics -- the study of hereditary relatedness among organisms. Insight from phylogenetic analysis has touched nearly every corner of biology. Examples range across natural history, population genetics and phylogeography, conservation biology, public health, medicine, in vivo and in silico experimental evolution, application-oriented evolutionary algorithms, and beyond.   High-throughput genetic and phenotypic data has realized groundbreaking results, in large part, through conjunction with open-source software used to process and analyze it. Indeed, the preceding decades have ushered in a flourishing ecosystem of bioinformatics software applications and libraries. Over the course of its nearly fifteen-year history, the DendroPy library for phylogenetic computation in Python has established a generalist niche in serving the bioinformatics community. Here, we report on the recent major release of the library, DendroPy version 5. The software release represents a major milestone in transitioning the library to a sustainable long-term development and maintenance trajectory. As such, this work positions DendroPy to continue fulfilling a key supporting role in phyloinformatics infrastructure.","sentences":["Contemporary bioinformatics has seen in profound new visibility into the composition, structure, and history of the natural world around us.","Arguably, the central pillar of bioinformatics is phylogenetics -- the study of hereditary relatedness among organisms.","Insight from phylogenetic analysis has touched nearly every corner of biology.","Examples range across natural history, population genetics and phylogeography, conservation biology, public health, medicine, in vivo and in silico experimental evolution, application-oriented evolutionary algorithms, and beyond.   High-throughput genetic and phenotypic data has realized groundbreaking results, in large part, through conjunction with open-source software used to process and analyze it.","Indeed, the preceding decades have ushered in a flourishing ecosystem of bioinformatics software applications and libraries.","Over the course of its nearly fifteen-year history, the DendroPy library for phylogenetic computation in Python has established a generalist niche in serving the bioinformatics community.","Here, we report on the recent major release of the library, DendroPy version 5.","The software release represents a major milestone in transitioning the library to a sustainable long-term development and maintenance trajectory.","As such, this work positions DendroPy to continue fulfilling a key supporting role in phyloinformatics infrastructure."],"url":"http://arxiv.org/abs/2405.14120v1","category":"q-bio.PE"}
{"created":"2024-05-23 02:44:46","title":"PuTR: A Pure Transformer for Decoupled and Online Multi-Object Tracking","abstract":"Recent advances in Multi-Object Tracking (MOT) have achieved remarkable success in short-term association within the decoupled tracking-by-detection online paradigm. However, long-term tracking still remains a challenging task. Although graph-based approaches can address this issue by modeling trajectories as a graph in the decoupled manner, their non-online nature poses obstacles for real-time applications. In this paper, we demonstrate that the trajectory graph is a directed acyclic graph, which can be represented by an object sequence arranged by frame and a binary adjacency matrix. It is a coincidence that the binary matrix matches the attention mask in the Transformer, and the object sequence serves exactly as a natural input sequence. Intuitively, we propose that a pure Transformer can naturally unify short- and long-term associations in a decoupled and online manner. Our experiments show that a classic Transformer architecture naturally suits the association problem and achieves a strong baseline compared to existing foundational methods across four datasets: DanceTrack, SportsMOT, MOT17, and MOT20, as well as superior generalizability in domain shift. Moreover, the decoupled property also enables efficient training and inference. This work pioneers a promising Transformer-based approach for the MOT task, and provides code to facilitate further research. https://github.com/chongweiliu/PuTR","sentences":["Recent advances in Multi-Object Tracking (MOT) have achieved remarkable success in short-term association within the decoupled tracking-by-detection online paradigm.","However, long-term tracking still remains a challenging task.","Although graph-based approaches can address this issue by modeling trajectories as a graph in the decoupled manner, their non-online nature poses obstacles for real-time applications.","In this paper, we demonstrate that the trajectory graph is a directed acyclic graph, which can be represented by an object sequence arranged by frame and a binary adjacency matrix.","It is a coincidence that the binary matrix matches the attention mask in the Transformer, and the object sequence serves exactly as a natural input sequence.","Intuitively, we propose that a pure Transformer can naturally unify short- and long-term associations in a decoupled and online manner.","Our experiments show that a classic Transformer architecture naturally suits the association problem and achieves a strong baseline compared to existing foundational methods across four datasets: DanceTrack, SportsMOT, MOT17, and MOT20, as well as superior generalizability in domain shift.","Moreover, the decoupled property also enables efficient training and inference.","This work pioneers a promising Transformer-based approach for the MOT task, and provides code to facilitate further research.","https://github.com/chongweiliu/PuTR"],"url":"http://arxiv.org/abs/2405.14119v1","category":"cs.CV"}
{"created":"2024-05-23 00:04:19","title":"Control landscapes for high-fidelity generation of C-NOT and C-PHASE gates with coherent and environmental driving","abstract":"High fidelity generation of two-qubit gates is important for quantum computation, since such gates are components of popular universal sets of gates. Here we consider the problem of high fidelity generation of two-qubit C-NOT and C-PHASE (with a detailed study of C-Z) gates in presence of the environment. We consider the general situation when qubits are manipulated by coherent and incoherent controls; the latter is used to induce generally time-dependent decoherence rates. For estimating efficiency of optimization methods for high fidelity generation of these gates, we study quantum control landscapes which describe the behaviour of the fidelity as a function of the controls. For this, we generate and analyze the statistical distributions of best objective values obtained by incoherent GRadient Ascent Pulse Engineering (inGRAPE) approach. We also apply inGRAPE and stochastic zero-order method to numerically estimate minimal infidelity values. The results are different from the case of single-qubit gates and indicate a smooth trap-free behaviour of the fidelity.","sentences":["High fidelity generation of two-qubit gates is important for quantum computation, since such gates are components of popular universal sets of gates.","Here we consider the problem of high fidelity generation of two-qubit C-NOT and C-PHASE (with a detailed study of C-Z) gates in presence of the environment.","We consider the general situation when qubits are manipulated by coherent and incoherent controls; the latter is used to induce generally time-dependent decoherence rates.","For estimating efficiency of optimization methods for high fidelity generation of these gates, we study quantum control landscapes which describe the behaviour of the fidelity as a function of the controls.","For this, we generate and analyze the statistical distributions of best objective values obtained by incoherent GRadient Ascent Pulse Engineering (inGRAPE) approach.","We also apply inGRAPE and stochastic zero-order method to numerically estimate minimal infidelity values.","The results are different from the case of single-qubit gates and indicate a smooth trap-free behaviour of the fidelity."],"url":"http://arxiv.org/abs/2405.14069v1","category":"quant-ph"}
{"created":"2024-05-22 22:32:12","title":"Deep Reinforcement Learning Based Resource Allocation for MIMO Bistatic Backscatter Networks","abstract":"Bistatic backscatter communication promises ubiquitous, massive connectivity by utilizing passive tags to connect with a reader by reflecting carrier emitter (CE) signals for future Internet-of-Things (IoT) networks. This study focuses on the joint design of the transmit/received beamformers at the CE/reader and the reflection coefficient of the tag. A throughput maximization problem is thus formulated, subject to satisfying the tag requirements. We develop a joint design through a series of trial-and-error interactions within the environment, driven by a predefined reward system in a continuous state and action context. We propose two deep reinforcement learning (DRL) algorithms to address the underlying optimization problem, namely deep deterministic policy gradient (DDPG) and soft actor-critic (SAC). Simulation results indicate that the proposed algorithm can learn from the environment and incrementally enhance its behavior, achieving performance that is on par with two leading benchmarks. Further, we also compared the performance of the proposed method with deep Q-network (DQN), double deep Q-network (DDQN), and dueling DQN (DuelDQN). For a system with twelve antennas, SAC leads with a 26.76% gain over DQN, followed by alternative optimization (AO) and DDPG at 23.02% and 19.16%. DDQN and DuelDQN show smaller improvements of 10.40% and 14.36%, respectively, against DQN.","sentences":["Bistatic backscatter communication promises ubiquitous, massive connectivity by utilizing passive tags to connect with a reader by reflecting carrier emitter (CE) signals for future Internet-of-Things (IoT) networks.","This study focuses on the joint design of the transmit/received beamformers at the CE/reader and the reflection coefficient of the tag.","A throughput maximization problem is thus formulated, subject to satisfying the tag requirements.","We develop a joint design through a series of trial-and-error interactions within the environment, driven by a predefined reward system in a continuous state and action context.","We propose two deep reinforcement learning (DRL) algorithms to address the underlying optimization problem, namely deep deterministic policy gradient (DDPG) and soft actor-critic (SAC).","Simulation results indicate that the proposed algorithm can learn from the environment and incrementally enhance its behavior, achieving performance that is on par with two leading benchmarks.","Further, we also compared the performance of the proposed method with deep Q-network (DQN), double deep Q-network (DDQN), and dueling DQN (DuelDQN).","For a system with twelve antennas, SAC leads with a 26.76% gain over DQN, followed by alternative optimization (AO) and DDPG at 23.02% and 19.16%.","DDQN and DuelDQN show smaller improvements of 10.40% and 14.36%, respectively, against DQN."],"url":"http://arxiv.org/abs/2405.14046v1","category":"cs.IT"}
{"created":"2024-05-22 22:07:42","title":"Scalable Multi-Period AC Optimal Power Flow Utilizing GPUs with High Memory Capacities","abstract":"This paper demonstrates the scalability of open-source GPU-accelerated nonlinear programming (NLP) frameworks -- ExaModels.jl and MadNLP.jl -- for solving multi-period alternating current (AC) optimal power flow (OPF) problems on GPUs with high memory capacities (e.g., NVIDIA GH200 with 480 GB of unified memory). There has been a growing interest in solving multi-period AC OPF problems, as the increasingly fluctuating electricity market requires operation planning over multiple periods. These problems, formerly deemed intractable, are now becoming technologically feasible to solve thanks to the advent of high-memory GPU hardware and accelerated NLP tools. This study evaluates the capability of these tools to tackle previously unsolvable multi-period AC OPF instances. Our numerical experiments, run on an NVIDIA GH200, demonstrate that we can solve a multi-period OPF instance with more than 10 million variables up to $10^{-4}$ precision in less than 10 minutes. These results demonstrate the efficacy of the GPU-accelerated NLP frameworks for the solution of extreme-scale multi-period OPF. We provide ExaModelsPower.jl, an open-source modeling tool for multi-period AC OPF models for GPUs.","sentences":["This paper demonstrates the scalability of open-source GPU-accelerated nonlinear programming (NLP) frameworks -- ExaModels.jl and MadNLP.jl -- for solving multi-period alternating current (AC) optimal power flow (OPF) problems on GPUs with high memory capacities (e.g., NVIDIA GH200 with 480 GB of unified memory).","There has been a growing interest in solving multi-period AC OPF problems, as the increasingly fluctuating electricity market requires operation planning over multiple periods.","These problems, formerly deemed intractable, are now becoming technologically feasible to solve thanks to the advent of high-memory GPU hardware and accelerated NLP tools.","This study evaluates the capability of these tools to tackle previously unsolvable multi-period AC OPF instances.","Our numerical experiments, run on an NVIDIA GH200, demonstrate that we can solve a multi-period OPF instance with more than 10 million variables up to $10^{-4}$ precision in less than 10 minutes.","These results demonstrate the efficacy of the GPU-accelerated NLP frameworks for the solution of extreme-scale multi-period OPF.","We provide ExaModelsPower.jl, an open-source modeling tool for multi-period AC OPF models for GPUs."],"url":"http://arxiv.org/abs/2405.14032v1","category":"math.OC"}
{"created":"2024-05-22 22:01:36","title":"HoverFast: an accurate, high-throughput, clinically deployable nuclear segmentation tool for brightfield digital pathology images","abstract":"In computational digital pathology, accurate nuclear segmentation of Hematoxylin and Eosin (H&E) stained whole slide images (WSIs) is a critical step for many analyses and tissue characterizations. One popular deep learning-based nuclear segmentation approach, HoverNet, offers remarkably accurate results but lacks the high-throughput performance needed for clinical deployment in resource-constrained settings. Our approach, HoverFast, aims to provide fast and accurate nuclear segmentation in H&E images using knowledge distillation from HoverNet. By redesigning the tool with software engineering best practices, HoverFast introduces advanced parallel processing capabilities, efficient data handling, and optimized postprocessing. These improvements facilitate scalable high-throughput performance, making HoverFast more suitable for real-time analysis and application in resource-limited environments. Using a consumer grade Nvidia A5000 GPU, HoverFast showed a 21x speed improvement as compared to HoverNet; reducing mean analysis time for 40x WSIs from ~2 hours to 6 minutes while retaining a concordant mean Dice score of 0.91 against the original HoverNet output. Peak memory usage was also reduced 71% from 44.4GB, to 12.8GB, without requiring SSD-based caching. To ease adoption in research and clinical contexts, HoverFast aligns with best-practices in terms of (a) installation, and (b) containerization, while (c) providing outputs compatible with existing popular open-source image viewing tools such as QuPath. HoverFast has been made open-source and is available at andrewjanowczyk.com/open-source-tools/hoverfast.","sentences":["In computational digital pathology, accurate nuclear segmentation of Hematoxylin and Eosin (H&E) stained whole slide images (WSIs) is a critical step for many analyses and tissue characterizations.","One popular deep learning-based nuclear segmentation approach, HoverNet, offers remarkably accurate results but lacks the high-throughput performance needed for clinical deployment in resource-constrained settings.","Our approach, HoverFast, aims to provide fast and accurate nuclear segmentation in H&E images using knowledge distillation from HoverNet.","By redesigning the tool with software engineering best practices, HoverFast introduces advanced parallel processing capabilities, efficient data handling, and optimized postprocessing.","These improvements facilitate scalable high-throughput performance, making HoverFast more suitable for real-time analysis and application in resource-limited environments.","Using a consumer grade Nvidia A5000 GPU, HoverFast showed a 21x speed improvement as compared to HoverNet; reducing mean analysis time for 40x WSIs from ~2 hours to 6 minutes while retaining a concordant mean Dice score of 0.91 against the original HoverNet output.","Peak memory usage was also reduced 71% from 44.4GB, to 12.8GB, without requiring SSD-based caching.","To ease adoption in research and clinical contexts, HoverFast aligns with best-practices in terms of (a) installation, and (b) containerization, while (c) providing outputs compatible with existing popular open-source image viewing tools such as QuPath.","HoverFast has been made open-source and is available at andrewjanowczyk.com/open-source-tools/hoverfast."],"url":"http://arxiv.org/abs/2405.14028v1","category":"q-bio.QM"}
{"created":"2024-05-22 21:43:15","title":"Lineshape Optimization in Inhomogeneous $\u039b$-type Quantum Memory","abstract":"Photonic quantum memory is a crucial elementary operation in photonic quantum information processing. While many physically distinct memory protocols and hardware implementations have been applied to this task, the development of a quantum memory performant in all relevant metrics simultaneously (e.g., efficiency, bandwidth, lifetime, etc.) is still an open challenge. In this work, we focus on inhomogeneously broadened ensembles of $\\Lambda$-type quantum emitters, which have long coherence lifetimes and broad bandwidth compatibility, but tend to exhibit low efficiency, in part due to technical constraints on medium growth and preparation, and in part due to inefficient use of a key resource in these systems: the inhomogeneously broadened excited state lineshape. We investigate the properties of electromagnetically induced transparency (EIT) for a survey of inhomogeneous lineshapes that are straightforward to realize experimentally, and optimize the memory efficiency for each lineshape over a large range of experimental parameters. We compare the optimal EIT efficiency to the well-known atomic frequency comb (AFC) protocol, which also relies on spectral shaping of the inhomogeneous broadening, and observe that with sufficient control field power the optimized lineshapes allow more efficient storage. Finally, we optimize over the inhomogeneous lineshape in a protocol agnostic fashion by numerically constructing the linear integral kernel describing the memory interaction and using a singular value decomposition and interpolation procedure to ensure optimality of the resulting lineshape.","sentences":["Photonic quantum memory is a crucial elementary operation in photonic quantum information processing.","While many physically distinct memory protocols and hardware implementations have been applied to this task, the development of a quantum memory performant in all relevant metrics simultaneously (e.g., efficiency, bandwidth, lifetime, etc.) is still an open challenge.","In this work, we focus on inhomogeneously broadened ensembles of $\\Lambda$-type quantum emitters, which have long coherence lifetimes and broad bandwidth compatibility, but tend to exhibit low efficiency, in part due to technical constraints on medium growth and preparation, and in part due to inefficient use of a key resource in these systems: the inhomogeneously broadened excited state lineshape.","We investigate the properties of electromagnetically induced transparency (EIT) for a survey of inhomogeneous lineshapes that are straightforward to realize experimentally, and optimize the memory efficiency for each lineshape over a large range of experimental parameters.","We compare the optimal EIT efficiency to the well-known atomic frequency comb (AFC) protocol, which also relies on spectral shaping of the inhomogeneous broadening, and observe that with sufficient control field power the optimized lineshapes allow more efficient storage.","Finally, we optimize over the inhomogeneous lineshape in a protocol agnostic fashion by numerically constructing the linear integral kernel describing the memory interaction and using a singular value decomposition and interpolation procedure to ensure optimality of the resulting lineshape."],"url":"http://arxiv.org/abs/2405.14013v1","category":"quant-ph"}
{"created":"2024-05-22 21:05:35","title":"Leveraging World Events to Predict E-Commerce Consumer Demand under Anomaly","abstract":"Consumer demand forecasting is of high importance for many e-commerce applications, including supply chain optimization, advertisement placement, and delivery speed optimization. However, reliable time series sales forecasting for e-commerce is difficult, especially during periods with many anomalies, as can often happen during pandemics, abnormal weather, or sports events. Although many time series algorithms have been applied to the task, prediction during anomalies still remains a challenge. In this work, we hypothesize that leveraging external knowledge found in world events can help overcome the challenge of prediction under anomalies. We mine a large repository of 40 years of world events and their textual representations. Further, we present a novel methodology based on transformers to construct an embedding of a day based on the relations of the day's events. Those embeddings are then used to forecast future consumer behavior. We empirically evaluate the methods over a large e-commerce products sales dataset, extracted from eBay, one of the world's largest online marketplaces. We show over numerous categories that our method outperforms state-of-the-art baselines during anomalies.","sentences":["Consumer demand forecasting is of high importance for many e-commerce applications, including supply chain optimization, advertisement placement, and delivery speed optimization.","However, reliable time series sales forecasting for e-commerce is difficult, especially during periods with many anomalies, as can often happen during pandemics, abnormal weather, or sports events.","Although many time series algorithms have been applied to the task, prediction during anomalies still remains a challenge.","In this work, we hypothesize that leveraging external knowledge found in world events can help overcome the challenge of prediction under anomalies.","We mine a large repository of 40 years of world events and their textual representations.","Further, we present a novel methodology based on transformers to construct an embedding of a day based on the relations of the day's events.","Those embeddings are then used to forecast future consumer behavior.","We empirically evaluate the methods over a large e-commerce products sales dataset, extracted from eBay, one of the world's largest online marketplaces.","We show over numerous categories that our method outperforms state-of-the-art baselines during anomalies."],"url":"http://arxiv.org/abs/2405.13995v1","category":"cs.LG"}
{"created":"2024-05-22 20:30:14","title":"Optimizing Curvature Learning for Robust Hyperbolic Deep Learning in Computer Vision","abstract":"Hyperbolic deep learning has become a growing research direction in computer vision for the unique properties afforded by the alternate embedding space. The negative curvature and exponentially growing distance metric provide a natural framework for capturing hierarchical relationships between datapoints and allowing for finer separability between their embeddings. However, these methods are still computationally expensive and prone to instability, especially when attempting to learn the negative curvature that best suits the task and the data. Current Riemannian optimizers do not account for changes in the manifold which greatly harms performance and forces lower learning rates to minimize projection errors. Our paper focuses on curvature learning by introducing an improved schema for popular learning algorithms and providing a novel normalization approach to constrain embeddings within the variable representative radius of the manifold. Additionally, we introduce a novel formulation for Riemannian AdamW, and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations, greatly reducing the computational penalty of the hyperbolic embedding space. Our approach demonstrates consistent performance improvements across both direct classification and hierarchical metric learning tasks while allowing for larger hyperbolic models.","sentences":["Hyperbolic deep learning has become a growing research direction in computer vision for the unique properties afforded by the alternate embedding space.","The negative curvature and exponentially growing distance metric provide a natural framework for capturing hierarchical relationships between datapoints and allowing for finer separability between their embeddings.","However, these methods are still computationally expensive and prone to instability, especially when attempting to learn the negative curvature that best suits the task and the data.","Current Riemannian optimizers do not account for changes in the manifold which greatly harms performance and forces lower learning rates to minimize projection errors.","Our paper focuses on curvature learning by introducing an improved schema for popular learning algorithms and providing a novel normalization approach to constrain embeddings within the variable representative radius of the manifold.","Additionally, we introduce a novel formulation for Riemannian AdamW, and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations, greatly reducing the computational penalty of the hyperbolic embedding space.","Our approach demonstrates consistent performance improvements across both direct classification and hierarchical metric learning tasks while allowing for larger hyperbolic models."],"url":"http://arxiv.org/abs/2405.13979v1","category":"cs.CV"}
{"created":"2024-05-22 20:09:21","title":"Uncertainty-Aware DRL for Autonomous Vehicle Crowd Navigation in Shared Space","abstract":"Safe, socially compliant, and efficient navigation of low-speed autonomous vehicles (AVs) in pedestrian-rich environments necessitates considering pedestrians' future positions and interactions with the vehicle and others. Despite the inevitable uncertainties associated with pedestrians' predicted trajectories due to their unobserved states (e.g., intent), existing deep reinforcement learning (DRL) algorithms for crowd navigation often neglect these uncertainties when using predicted trajectories to guide policy learning. This omission limits the usability of predictions when diverging from ground truth. This work introduces an integrated prediction and planning approach that incorporates the uncertainties of predicted pedestrian states in the training of a model-free DRL algorithm. A novel reward function encourages the AV to respect pedestrians' personal space, decrease speed during close approaches, and minimize the collision probability with their predicted paths. Unlike previous DRL methods, our model, designed for AV operation in crowded spaces, is trained in a novel simulation environment that reflects realistic pedestrian behaviour in a shared space with vehicles. Results show a 40% decrease in collision rate and a 15% increase in minimum distance to pedestrians compared to the state of the art model that does not account for prediction uncertainty. Additionally, the approach outperforms model predictive control methods that incorporate the same prediction uncertainties in terms of both performance and computational time, while producing trajectories closer to human drivers in similar scenarios.","sentences":["Safe, socially compliant, and efficient navigation of low-speed autonomous vehicles (AVs) in pedestrian-rich environments necessitates considering pedestrians' future positions and interactions with the vehicle and others.","Despite the inevitable uncertainties associated with pedestrians' predicted trajectories due to their unobserved states (e.g., intent), existing deep reinforcement learning (DRL) algorithms for crowd navigation often neglect these uncertainties when using predicted trajectories to guide policy learning.","This omission limits the usability of predictions when diverging from ground truth.","This work introduces an integrated prediction and planning approach that incorporates the uncertainties of predicted pedestrian states in the training of a model-free DRL algorithm.","A novel reward function encourages the AV to respect pedestrians' personal space, decrease speed during close approaches, and minimize the collision probability with their predicted paths.","Unlike previous DRL methods, our model, designed for AV operation in crowded spaces, is trained in a novel simulation environment that reflects realistic pedestrian behaviour in a shared space with vehicles.","Results show a 40% decrease in collision rate and a 15% increase in minimum distance to pedestrians compared to the state of the art model that does not account for prediction uncertainty.","Additionally, the approach outperforms model predictive control methods that incorporate the same prediction uncertainties in terms of both performance and computational time, while producing trajectories closer to human drivers in similar scenarios."],"url":"http://arxiv.org/abs/2405.13969v1","category":"cs.RO"}
{"created":"2024-05-22 20:08:48","title":"DeTox: Toxic Subspace Projection for Model Editing","abstract":"Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking in controllability and transparency, making them prone to jailbreaking and inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative (DeTox) and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, DeTox is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic sub-space is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that DeTox is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we establish both theoretical and empirical connections between DeTox and DPO, showing that DeTox can be interpreted as a denoised version of a single DPO step.","sentences":["Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data.","However, these methods are both computationally intensive and lacking in controllability and transparency, making them prone to jailbreaking and inhibiting their widespread use.","Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data.","In this paper, we introduce a tuning-free alignment alternative (DeTox) and demonstrate its effectiveness under the use case of toxicity reduction.","Grounded on theory from factor analysis, DeTox is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace.","The toxic sub-space is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings.","We show that DeTox is more sample-efficient than DPO, further showcasing greater robustness to noisy data.","Finally, we establish both theoretical and empirical connections between DeTox and DPO, showing that DeTox can be interpreted as a denoised version of a single DPO step."],"url":"http://arxiv.org/abs/2405.13967v1","category":"cs.CL"}
{"created":"2024-05-22 20:00:19","title":"Design Editing for Offline Model-based Optimization","abstract":"Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. A prevalent approach involves training a conditional generative model on existing designs and their associated scores, followed by the generation of new designs conditioned on higher target scores. However, these newly generated designs often underperform due to the lack of high-scoring training data. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which consists of two phases. In the first phase, termed pseudo-target distribution generation, we apply gradient ascent on the offline dataset using a trained surrogate model, producing a synthetic dataset where the predicted scores serve as new labels. A conditional diffusion model is subsequently trained on this synthetic dataset to capture a pseudo-target distribution, which enhances the accuracy of the conditional diffusion model in generating higher-scoring designs. Nevertheless, the pseudo-target distribution is susceptible to noise stemming from inaccuracies in the surrogate model, consequently predisposing the conditional diffusion model to generate suboptimal designs. We hence propose the second phase, existing design editing, to directly incorporate the high-scoring features from the offline dataset into design generation. In this phase, top designs from the offline dataset are edited by introducing noise, which are subsequently refined using the conditional diffusion model to produce high-scoring designs. Overall, high-scoring designs begin with inheriting high-scoring features from the second phase and are further refined with a more accurate conditional diffusion model in the first phase. Empirical evaluations on 7 offline MBO tasks show that DEMO outperforms various baseline methods.","sentences":["Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores.","A prevalent approach involves training a conditional generative model on existing designs and their associated scores, followed by the generation of new designs conditioned on higher target scores.","However, these newly generated designs often underperform due to the lack of high-scoring training data.","To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which consists of two phases.","In the first phase, termed pseudo-target distribution generation, we apply gradient ascent on the offline dataset using a trained surrogate model, producing a synthetic dataset where the predicted scores serve as new labels.","A conditional diffusion model is subsequently trained on this synthetic dataset to capture a pseudo-target distribution, which enhances the accuracy of the conditional diffusion model in generating higher-scoring designs.","Nevertheless, the pseudo-target distribution is susceptible to noise stemming from inaccuracies in the surrogate model, consequently predisposing the conditional diffusion model to generate suboptimal designs.","We hence propose the second phase, existing design editing, to directly incorporate the high-scoring features from the offline dataset into design generation.","In this phase, top designs from the offline dataset are edited by introducing noise, which are subsequently refined using the conditional diffusion model to produce high-scoring designs.","Overall, high-scoring designs begin with inheriting high-scoring features from the second phase and are further refined with a more accurate conditional diffusion model in the first phase.","Empirical evaluations on 7 offline MBO tasks show that DEMO outperforms various baseline methods."],"url":"http://arxiv.org/abs/2405.13964v1","category":"cs.LG"}
{"created":"2024-05-22 19:40:37","title":"Cognitive Internet of Vulnerable Road Users in Traffic: Predictive Neural Modulations of Road Crossing Intention","abstract":"Vulnerable Road Users (VRUs) present a significant challenge for road safety due to the frequent unpredictability of their behaviors. In typical Intelligent Transportation Systems, vision-based approaches supported by networked cameras are often used to anticipate VRUs motion intentions and trajectories. However, several limitations posed by occlusions and distractions set a boundary for the efficacy of such methods. To address these challenges, this study introduces a framework that leverages data collected using wearable neurophysiological sensors on VRUs to integrate them seamlessly into the Vehicle-to-Everything communication framework. This integration empowers VRUs to autonomously broadcast their intended movements to other road agents, especially autonomous vehicles, thereby bridging a critical gap in current vehicular communication systems. To validate this concept, we conducted an experiment involving 12 participants, from whom EEG signals were collected as they engaged in road-crossing decisions within simulated environments. Employing Hidden Markov Models, we identified four cognitive stages intrinsic to a pedestrian's decision-making process. Our statistical analysis further revealed significant variations in EEG activities across these stages, shedding light on the neural correlates and cognitive dynamics underpinning pedestrian road-crossing behavior. We then developed a predictive cognitive model using dynamic time warping and K-nearest neighbors algorithms, optimized through a data-driven sliding window approach. This model demonstrated high predictive accuracy, evidenced by an Area Under the Curve of 0.91, indicating its capability to anticipate pedestrian road-crossing actions approximately 1 second in advance of any pedestrian movement. This research paves the way for a novel VRU-Vehicle interaction paradigm and signifies a shift towards a forward-thinking ecosystem.","sentences":["Vulnerable Road Users (VRUs) present a significant challenge for road safety due to the frequent unpredictability of their behaviors.","In typical Intelligent Transportation Systems, vision-based approaches supported by networked cameras are often used to anticipate VRUs motion intentions and trajectories.","However, several limitations posed by occlusions and distractions set a boundary for the efficacy of such methods.","To address these challenges, this study introduces a framework that leverages data collected using wearable neurophysiological sensors on VRUs to integrate them seamlessly into the Vehicle-to-Everything communication framework.","This integration empowers VRUs to autonomously broadcast their intended movements to other road agents, especially autonomous vehicles, thereby bridging a critical gap in current vehicular communication systems.","To validate this concept, we conducted an experiment involving 12 participants, from whom EEG signals were collected as they engaged in road-crossing decisions within simulated environments.","Employing Hidden Markov Models, we identified four cognitive stages intrinsic to a pedestrian's decision-making process.","Our statistical analysis further revealed significant variations in EEG activities across these stages, shedding light on the neural correlates and cognitive dynamics underpinning pedestrian road-crossing behavior.","We then developed a predictive cognitive model using dynamic time warping and K-nearest neighbors algorithms, optimized through a data-driven sliding window approach.","This model demonstrated high predictive accuracy, evidenced by an Area Under the Curve of 0.91, indicating its capability to anticipate pedestrian road-crossing actions approximately 1 second in advance of any pedestrian movement.","This research paves the way for a novel VRU-Vehicle interaction paradigm and signifies a shift towards a forward-thinking ecosystem."],"url":"http://arxiv.org/abs/2405.13955v1","category":"cs.HC"}
{"created":"2024-05-22 19:33:58","title":"Actor-critic algorithms for fiber sampling problems","abstract":"We propose an actor-critic algorithm for a family of complex problems arising in algebraic statistics and discrete optimization. The core task is to produce a sample from a finite subset of the non-negative integer lattice defined by a high-dimensional polytope. We translate the problem into a Markov decision process and devise an actor-critic reinforcement learning (RL) algorithm to learn a set of good moves that can be used for sampling. We prove that the actor-critic algorithm converges to an approximately optimal sampling policy.   To tackle complexity issues that typically arise in these sampling problems, and to allow the RL to function at scale, our solution strategy takes three steps: decomposing the starting point of the sample, using RL on each induced subproblem, and reconstructing to obtain a sample in the original polytope. In this setup, the proof of convergence applies to each subproblem in the decomposition.   We test the method in two regimes. In statistical applications, a high-dimensional polytope arises as the support set for the reference distribution in a model/data fit test for a broad family of statistical models for categorical data. We demonstrate how RL can be used for model fit testing problems for data sets for which traditional MCMC samplers converge too slowly due to problem size and sparsity structure. To test the robustness of the algorithm and explore its generalization properties, we apply it to synthetically generated data of various sizes and sparsity levels.","sentences":["We propose an actor-critic algorithm for a family of complex problems arising in algebraic statistics and discrete optimization.","The core task is to produce a sample from a finite subset of the non-negative integer lattice defined by a high-dimensional polytope.","We translate the problem into a Markov decision process and devise an actor-critic reinforcement learning (RL) algorithm to learn a set of good moves that can be used for sampling.","We prove that the actor-critic algorithm converges to an approximately optimal sampling policy.   ","To tackle complexity issues that typically arise in these sampling problems, and to allow the RL to function at scale, our solution strategy takes three steps: decomposing the starting point of the sample, using RL on each induced subproblem, and reconstructing to obtain a sample in the original polytope.","In this setup, the proof of convergence applies to each subproblem in the decomposition.   ","We test the method in two regimes.","In statistical applications, a high-dimensional polytope arises as the support set for the reference distribution in a model/data fit test for a broad family of statistical models for categorical data.","We demonstrate how RL can be used for model fit testing problems for data sets for which traditional MCMC samplers converge too slowly due to problem size and sparsity structure.","To test the robustness of the algorithm and explore its generalization properties, we apply it to synthetically generated data of various sizes and sparsity levels."],"url":"http://arxiv.org/abs/2405.13950v1","category":"stat.ML"}
{"created":"2024-05-22 19:27:03","title":"Leader Reward for POMO-Based Neural Combinatorial Optimization","abstract":"Deep neural networks based on reinforcement learning (RL) for solving combinatorial optimization (CO) problems are developing rapidly and have shown a tendency to approach or even outperform traditional solvers. However, existing methods overlook an important distinction: CO problems differ from other traditional problems in that they focus solely on the optimal solution provided by the model within a specific length of time, rather than considering the overall quality of all solutions generated by the model. In this paper, we propose Leader Reward and apply it during two different training phases of the Policy Optimization with Multiple Optima (POMO) model to enhance the model's ability to generate optimal solutions. This approach is applicable to a variety of CO problems, such as the Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem (CVRP), and the Flexible Flow Shop Problem (FFSP), but also works well with other POMO-based models or inference phase's strategies. We demonstrate that Leader Reward greatly improves the quality of the optimal solutions generated by the model. Specifically, we reduce the POMO's gap to the optimum by more than 100 times on TSP100 with almost no additional computational overhead.","sentences":["Deep neural networks based on reinforcement learning (RL) for solving combinatorial optimization (CO) problems are developing rapidly and have shown a tendency to approach or even outperform traditional solvers.","However, existing methods overlook an important distinction: CO problems differ from other traditional problems in that they focus solely on the optimal solution provided by the model within a specific length of time, rather than considering the overall quality of all solutions generated by the model.","In this paper, we propose Leader Reward and apply it during two different training phases of the Policy Optimization with Multiple Optima (POMO) model to enhance the model's ability to generate optimal solutions.","This approach is applicable to a variety of CO problems, such as the Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem (CVRP), and the Flexible Flow Shop Problem (FFSP), but also works well with other POMO-based models or inference phase's strategies.","We demonstrate that Leader Reward greatly improves the quality of the optimal solutions generated by the model.","Specifically, we reduce the POMO's gap to the optimum by more than 100 times on TSP100 with almost no additional computational overhead."],"url":"http://arxiv.org/abs/2405.13947v1","category":"cs.LG"}
{"created":"2024-05-22 19:19:09","title":"A Survey on Design-space Dimensionality Reduction Methods for Shape Optimization","abstract":"The rapidly evolving field of engineering design of functional surfaces necessitates sophisticated tools to manage the inherent complexity of high-dimensional design spaces. This review delves into the field of design-space dimensionality reduction techniques tailored for shape optimization, bridging traditional methods and cutting-edge technologies. Dissecting the spectrum of these techniques, from classical linear approaches like principal component analysis to more nuanced nonlinear methods such as autoencoders, the discussion extends to innovative physics-informed methods that integrate physical data into the dimensionality reduction process, enhancing the predictive accuracy and relevance of reduced models. By integrating these methods into optimization frameworks, it is shown how they significantly mitigate the curse of dimensionality, streamline computational processes, and refine the exploration and optimization of complex functional surfaces. The survey provides a classification of method and highlights the transformative impact of these techniques in simplifying design challenges, thereby fostering more efficient and effective engineering solutions.","sentences":["The rapidly evolving field of engineering design of functional surfaces necessitates sophisticated tools to manage the inherent complexity of high-dimensional design spaces.","This review delves into the field of design-space dimensionality reduction techniques tailored for shape optimization, bridging traditional methods and cutting-edge technologies.","Dissecting the spectrum of these techniques, from classical linear approaches like principal component analysis to more nuanced nonlinear methods such as autoencoders, the discussion extends to innovative physics-informed methods that integrate physical data into the dimensionality reduction process, enhancing the predictive accuracy and relevance of reduced models.","By integrating these methods into optimization frameworks, it is shown how they significantly mitigate the curse of dimensionality, streamline computational processes, and refine the exploration and optimization of complex functional surfaces.","The survey provides a classification of method and highlights the transformative impact of these techniques in simplifying design challenges, thereby fostering more efficient and effective engineering solutions."],"url":"http://arxiv.org/abs/2405.13944v1","category":"math.OC"}
{"created":"2024-05-22 19:13:05","title":"Principal eigenstate classical shadows","abstract":"Given many copies of an unknown quantum state $\\rho$, we consider the task of learning a classical description of its principal eigenstate. Namely, assuming that $\\rho$ has an eigenstate $|\\phi\\rangle$ with (unknown) eigenvalue $\\lambda > 1/2$, the goal is to learn a (classical shadows style) classical description of $|\\phi\\rangle$ which can later be used to estimate expectation values $\\langle \\phi |O| \\phi \\rangle$ for any $O$ in some class of observables. We consider the sample-complexity setting in which generating a copy of $\\rho$ is expensive, but joint measurements on many copies of the state are possible. We present a protocol for this task scaling with the principal eigenvalue $\\lambda$ and show that it is optimal within a space of natural approaches, e.g., applying quantum state purification followed by a single-copy classical shadows scheme. Furthermore, when $\\lambda$ is sufficiently close to $1$, the performance of our algorithm is optimal--matching the sample complexity for pure state classical shadows.","sentences":["Given many copies of an unknown quantum state $\\rho$, we consider the task of learning a classical description of its principal eigenstate.","Namely, assuming that $\\rho$ has an eigenstate $|\\phi\\rangle$ with (unknown) eigenvalue $\\lambda > 1/2$, the goal is to learn a (classical shadows style) classical description of $|\\phi\\rangle$ which can later be used to estimate expectation values $\\langle \\phi |O| \\phi \\rangle$ for any $O$ in some class of observables.","We consider the sample-complexity setting in which generating a copy of $\\rho$ is expensive, but joint measurements on many copies of the state are possible.","We present a protocol for this task scaling with the principal eigenvalue $\\lambda$ and show that it is optimal within a space of natural approaches, e.g., applying quantum state purification followed by a single-copy classical shadows scheme.","Furthermore, when $\\lambda$ is sufficiently close to $1$, the performance of our algorithm is optimal--matching the sample complexity for pure state classical shadows."],"url":"http://arxiv.org/abs/2405.13939v1","category":"quant-ph"}
{"created":"2024-05-22 19:11:28","title":"eXmY: A Data Type and Technique for Arbitrary Bit Precision Quantization","abstract":"eXmY is a novel data type for quantization of ML models. It supports both arbitrary bit widths and arbitrary integer and floating point formats. For example, it seamlessly supports 3, 5, 6, 7, 9 bit formats. For a specific bit width, say 7, it defines all possible formats e.g. e0m6, e1m5, e2m4, e3m3, e4m2, e5m1 and e6m0. For non-power of two bit widths e.g. 5, 6, 7, we created a novel encoding and decoding scheme which achieves perfect compression, byte addressability and is amenable to sharding and vector processing. We implemented libraries for emulation, encoding and decoding tensors and checkpoints in C++, TensorFlow, JAX and PAX. For optimal performance, the codecs use SIMD instructions on CPUs and vector instructions on TPUs and GPUs. eXmY is also a technique and exploits the statistical distribution of exponents in tensors. It can be used to quantize weights, static and dynamic activations, gradients, master weights and optimizer state. It can reduce memory (CPU DRAM and accelerator HBM), network and disk storage and transfers. It can increase multi tenancy and accelerate compute. eXmY has been deployed in production for almost 2 years.","sentences":["eXmY is a novel data type for quantization of ML models.","It supports both arbitrary bit widths and arbitrary integer and floating point formats.","For example, it seamlessly supports 3, 5, 6, 7, 9 bit formats.","For a specific bit width, say 7, it defines all possible formats e.g. e0m6, e1m5, e2m4, e3m3, e4m2, e5m1 and e6m0.","For non-power of two bit widths e.g. 5, 6, 7, we created a novel encoding and decoding scheme which achieves perfect compression, byte addressability and is amenable to sharding and vector processing.","We implemented libraries for emulation, encoding and decoding tensors and checkpoints in C++, TensorFlow, JAX and PAX.","For optimal performance, the codecs use SIMD instructions on CPUs and vector instructions on TPUs and GPUs.","eXmY is also a technique and exploits the statistical distribution of exponents in tensors.","It can be used to quantize weights, static and dynamic activations, gradients, master weights and optimizer state.","It can reduce memory (CPU DRAM and accelerator HBM), network and disk storage and transfers.","It can increase multi tenancy and accelerate compute.","eXmY has been deployed in production for almost 2 years."],"url":"http://arxiv.org/abs/2405.13938v1","category":"cs.LG"}
{"created":"2024-05-22 18:57:38","title":"Some models are useful, but for how long?: A decision theoretic approach to choosing when to refit large-scale prediction models","abstract":"Large-scale prediction models (typically using tools from artificial intelligence, AI, or machine learning, ML) are increasingly ubiquitous across a variety of industries and scientific domains. Such methods are often paired with detailed data from sources such as electronic health records, wearable sensors, and omics data (high-throughput technology used to understand biology). Despite their utility, implementing AI and ML tools at the scale necessary to work with this data introduces two major challenges. First, it can cost tens of thousands of dollars to train a modern AI/ML model at scale. Second, once the model is trained, its predictions may become less relevant as patient and provider behavior change, and predictions made for one geographical area may be less accurate for another. These two challenges raise a fundamental question: how often should you refit the AI/ML model to optimally trade-off between cost and relevance? Our work provides a framework for making decisions about when to {\\it refit} AI/ML models when the goal is to maintain valid statistical inference (e.g. estimating a treatment effect in a clinical trial). Drawing on portfolio optimization theory, we treat the decision of {\\it recalibrating} versus {\\it refitting} the model as a choice between ''investing'' in one of two ''assets.'' One asset, recalibrating the model based on another model, is quick and relatively inexpensive but bears uncertainty from sampling and the possibility that the other model is not relevant to current circumstances. The other asset, {\\it refitting} the model, is costly but removes the irrelevance concern (though not the risk of sampling error). We explore the balancing act between these two potential investments in this paper.","sentences":["Large-scale prediction models (typically using tools from artificial intelligence, AI, or machine learning, ML) are increasingly ubiquitous across a variety of industries and scientific domains.","Such methods are often paired with detailed data from sources such as electronic health records, wearable sensors, and omics data (high-throughput technology used to understand biology).","Despite their utility, implementing AI and ML tools at the scale necessary to work with this data introduces two major challenges.","First, it can cost tens of thousands of dollars to train a modern AI/ML model at scale.","Second, once the model is trained, its predictions may become less relevant as patient and provider behavior change, and predictions made for one geographical area may be less accurate for another.","These two challenges raise a fundamental question: how often should you refit the AI/ML model to optimally trade-off between cost and relevance?","Our work provides a framework for making decisions about when to {\\it refit} AI/ML models when the goal is to maintain valid statistical inference (e.g. estimating a treatment effect in a clinical trial).","Drawing on portfolio optimization theory, we treat the decision of {\\it recalibrating} versus {\\it refitting} the model as a choice between ''investing'' in one of two ''assets.''","One asset, recalibrating the model based on another model, is quick and relatively inexpensive but bears uncertainty from sampling and the possibility that the other model is not relevant to current circumstances.","The other asset, {\\it refitting} the model, is costly but removes the irrelevance concern (though not the risk of sampling error).","We explore the balancing act between these two potential investments in this paper."],"url":"http://arxiv.org/abs/2405.13926v1","category":"stat.ME"}
{"created":"2024-05-22 18:50:49","title":"Algebraic Conditions for Stability in Runge-Kutta Methods and Their Certification via Semidefinite Programming","abstract":"In this work, we present approaches to rigorously certify $A$- and $A(\\alpha)$-stability in Runge-Kutta methods through the solution of convex feasibility problems defined by linear matrix inequalities. We adopt two approaches. The first is based on sum-of-squares programming applied to the Runge-Kutta $E$-polynomial and is applicable to both $A$- and $A(\\alpha)$-stability. In the second, we sharpen the algebraic conditions for $A$-stability of Cooper, Scherer, T{\\\"u}rke, and Wendler to incorporate the Runge-Kutta order conditions. We demonstrate how the theoretical improvement enables the practical use of these conditions for certification of $A$-stability within a computational framework. We then use both approaches to obtain rigorous certificates of stability for several diagonally implicit schemes devised in the literature.","sentences":["In this work, we present approaches to rigorously certify $A$- and $A(\\alpha)$-stability in Runge-Kutta methods through the solution of convex feasibility problems defined by linear matrix inequalities.","We adopt two approaches.","The first is based on sum-of-squares programming applied to the Runge-Kutta $E$-polynomial and is applicable to both $A$- and $A(\\alpha)$-stability.","In the second, we sharpen the algebraic conditions for $A$-stability of Cooper, Scherer, T{\\\"u}rke, and Wendler to incorporate the Runge-Kutta order conditions.","We demonstrate how the theoretical improvement enables the practical use of these conditions for certification of $A$-stability within a computational framework.","We then use both approaches to obtain rigorous certificates of stability for several diagonally implicit schemes devised in the literature."],"url":"http://arxiv.org/abs/2405.13921v1","category":"math.NA"}
{"created":"2024-05-22 18:38:10","title":"Matrix Denoising with Doubly Heteroscedastic Noise: Fundamental Limits and Optimal Spectral Methods","abstract":"We study the matrix denoising problem of estimating the singular vectors of a rank-$1$ signal corrupted by noise with both column and row correlations. Existing works are either unable to pinpoint the exact asymptotic estimation error or, when they do so, the resulting approaches (e.g., based on whitening or singular value shrinkage) remain vastly suboptimal. On top of this, most of the literature has focused on the special case of estimating the left singular vector of the signal when the noise only possesses row correlation (one-sided heteroscedasticity). In contrast, our work establishes the information-theoretic and algorithmic limits of matrix denoising with doubly heteroscedastic noise. We characterize the exact asymptotic minimum mean square error, and design a novel spectral estimator with rigorous optimality guarantees: under a technical condition, it attains positive correlation with the signals whenever information-theoretically possible and, for one-sided heteroscedasticity, it also achieves the Bayes-optimal error. Numerical experiments demonstrate the significant advantage of our theoretically principled method with the state of the art. The proofs draw connections with statistical physics and approximate message passing, departing drastically from standard random matrix theory techniques.","sentences":["We study the matrix denoising problem of estimating the singular vectors of a rank-$1$ signal corrupted by noise with both column and row correlations.","Existing works are either unable to pinpoint the exact asymptotic estimation error or, when they do so, the resulting approaches (e.g., based on whitening or singular value shrinkage) remain vastly suboptimal.","On top of this, most of the literature has focused on the special case of estimating the left singular vector of the signal when the noise only possesses row correlation (one-sided heteroscedasticity).","In contrast, our work establishes the information-theoretic and algorithmic limits of matrix denoising with doubly heteroscedastic noise.","We characterize the exact asymptotic minimum mean square error, and design a novel spectral estimator with rigorous optimality guarantees: under a technical condition, it attains positive correlation with the signals whenever information-theoretically possible and, for one-sided heteroscedasticity, it also achieves the Bayes-optimal error.","Numerical experiments demonstrate the significant advantage of our theoretically principled method with the state of the art.","The proofs draw connections with statistical physics and approximate message passing, departing drastically from standard random matrix theory techniques."],"url":"http://arxiv.org/abs/2405.13912v1","category":"math.ST"}
{"created":"2024-05-22 18:28:26","title":"Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries","abstract":"State-of-the-art large language models are sometimes distributed as open-source software but are also increasingly provided as a closed-source service. These closed-source large-language models typically see the widest usage by the public, however, they often do not provide an estimate of their uncertainty when responding to queries. As even the best models are prone to ``hallucinating\" false information with high confidence, a lack of a reliable estimate of uncertainty limits the applicability of these models in critical settings. We explore estimating the uncertainty of closed-source LLMs via multiple rephrasings of an original base query. Specifically, we ask the model, multiple rephrased questions, and use the similarity of the answers as an estimate of uncertainty. We diverge from previous work in i) providing rules for rephrasing that are simple to memorize and use in practice ii) proposing a theoretical framework for why multiple rephrased queries obtain calibrated uncertainty estimates. Our method demonstrates significant improvements in the calibration of uncertainty estimates compared to the baseline and provides intuition as to how query strategies should be designed for optimal test calibration.","sentences":["State-of-the-art large language models are sometimes distributed as open-source software but are also increasingly provided as a closed-source service.","These closed-source large-language models typically see the widest usage by the public, however, they often do not provide an estimate of their uncertainty when responding to queries.","As even the best models are prone to ``hallucinating\" false information with high confidence, a lack of a reliable estimate of uncertainty limits the applicability of these models in critical settings.","We explore estimating the uncertainty of closed-source LLMs via multiple rephrasings of an original base query.","Specifically, we ask the model, multiple rephrased questions, and use the similarity of the answers as an estimate of uncertainty.","We diverge from previous work in i) providing rules for rephrasing that are simple to memorize and use in practice ii) proposing a theoretical framework for why multiple rephrased queries obtain calibrated uncertainty estimates.","Our method demonstrates significant improvements in the calibration of uncertainty estimates compared to the baseline and provides intuition as to how query strategies should be designed for optimal test calibration."],"url":"http://arxiv.org/abs/2405.13907v1","category":"cs.CL"}
{"created":"2024-05-22 18:11:42","title":"Bias-field digitized counterdiabatic quantum optimization","abstract":"We introduce a method for solving combinatorial optimization problems on digital quantum computers, where we incorporate auxiliary counterdiabatic (CD) terms into the adiabatic Hamiltonian, while integrating bias terms derived from an iterative digitized counterdiabatic quantum algorithm. We call this protocol bias-field digitized counterdiabatic quantum optimization (BF-DCQO). Designed to effectively tackle large-scale combinatorial optimization problems, BF-DCQO demonstrates resilience against the limitations posed by the restricted coherence times of current quantum processors and shows clear enhancement even in the presence of noise. Additionally, our purely quantum approach eliminates the dependency on classical optimization required in hybrid classical-quantum schemes, thereby circumventing the trainability issues often associated with variational quantum algorithms. Through the analysis of an all-to-all connected general Ising spin-glass problem, we exhibit a polynomial scaling enhancement in ground state success probability compared to traditional DCQO and finite-time adiabatic quantum optimization methods. Furthermore, it achieves scaling improvements in ground state success probabilities, increasing by up to two orders of magnitude, and offers an average 1.3x better approximation ratio than the quantum approximate optimization algorithm for the problem sizes studied. We validate these findings through experimental implementations on both trapped-ion quantum computers and superconducting processors, tackling a maximum weighted independent set problem with 36 qubits and a spin-glass on a heavy-hex lattice with 100 qubits, respectively. These results mark a significant advancement in gate-based quantum computing, employing a fully quantum algorithmic approach.","sentences":["We introduce a method for solving combinatorial optimization problems on digital quantum computers, where we incorporate auxiliary counterdiabatic (CD) terms into the adiabatic Hamiltonian, while integrating bias terms derived from an iterative digitized counterdiabatic quantum algorithm.","We call this protocol bias-field digitized counterdiabatic quantum optimization (BF-DCQO).","Designed to effectively tackle large-scale combinatorial optimization problems, BF-DCQO demonstrates resilience against the limitations posed by the restricted coherence times of current quantum processors and shows clear enhancement even in the presence of noise.","Additionally, our purely quantum approach eliminates the dependency on classical optimization required in hybrid classical-quantum schemes, thereby circumventing the trainability issues often associated with variational quantum algorithms.","Through the analysis of an all-to-all connected general Ising spin-glass problem, we exhibit a polynomial scaling enhancement in ground state success probability compared to traditional DCQO and finite-time adiabatic quantum optimization methods.","Furthermore, it achieves scaling improvements in ground state success probabilities, increasing by up to two orders of magnitude, and offers an average 1.3x better approximation ratio than the quantum approximate optimization algorithm for the problem sizes studied.","We validate these findings through experimental implementations on both trapped-ion quantum computers and superconducting processors, tackling a maximum weighted independent set problem with 36 qubits and a spin-glass on a heavy-hex lattice with 100 qubits, respectively.","These results mark a significant advancement in gate-based quantum computing, employing a fully quantum algorithmic approach."],"url":"http://arxiv.org/abs/2405.13898v1","category":"quant-ph"}
{"created":"2024-05-22 18:00:41","title":"Marrying Causal Representation Learning with Dynamical Systems for Science","abstract":"Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements. However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application. At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification. In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems. At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical. Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation. We experiment with a wind simulator with partially known factors of variation. We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change.","sentences":["Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements.","However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application.","At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification.","In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems.","At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical.","Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation.","We experiment with a wind simulator with partially known factors of variation.","We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change."],"url":"http://arxiv.org/abs/2405.13888v1","category":"cs.LG"}
{"created":"2024-05-23 17:58:48","title":"Quadrupolar resonance spectroscopy of individual nuclei using a room-temperature quantum sensor","abstract":"Nuclear quadrupolar resonance (NQR) spectroscopy reveals chemical bonding patterns in materials and molecules through the unique coupling between nuclear spins and local fields. However, traditional NQR techniques require macroscopic ensembles of nuclei to yield a detectable signal, which precludes the study of individual molecules and obscures molecule-to-molecule variations due to local perturbations or deformations. Optically active electronic spin qubits, such as the nitrogen-vacancy (NV) center in diamond, facilitate the detection and control of individual nuclei through their local magnetic couplings. Here, we use NV centers to perform NQR spectroscopy on their associated nitrogen-14 ($^{14}$N) nuclei at room temperature. In mapping the nuclear quadrupolar Hamiltonian, we resolve minute variations between individual nuclei. The measurements further reveal correlations between the parameters in the NV center's electronic spin Hamiltonian and the $^{14}$N quadropolar Hamiltonian, as well as a previously unreported Hamiltonian term that results from symmetry breaking. We further design pulse sequences to initialize, readout, and control the quantum evolution of the $^{14}$N nuclear state using the nuclear quadrupolar Hamiltonian.","sentences":["Nuclear quadrupolar resonance (NQR) spectroscopy reveals chemical bonding patterns in materials and molecules through the unique coupling between nuclear spins and local fields.","However, traditional NQR techniques require macroscopic ensembles of nuclei to yield a detectable signal, which precludes the study of individual molecules and obscures molecule-to-molecule variations due to local perturbations or deformations.","Optically active electronic spin qubits, such as the nitrogen-vacancy (NV) center in diamond, facilitate the detection and control of individual nuclei through their local magnetic couplings.","Here, we use NV centers to perform NQR spectroscopy on their associated nitrogen-14 ($^{14}$N) nuclei at room temperature.","In mapping the nuclear quadrupolar Hamiltonian, we resolve minute variations between individual nuclei.","The measurements further reveal correlations between the parameters in the NV center's electronic spin Hamiltonian and the $^{14}$N quadropolar Hamiltonian, as well as a previously unreported Hamiltonian term that results from symmetry breaking.","We further design pulse sequences to initialize, readout, and control the quantum evolution of the $^{14}$N nuclear state using the nuclear quadrupolar Hamiltonian."],"url":"http://arxiv.org/abs/2405.14859v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-23 17:40:02","title":"Efficient first-principles approach to Gibbs free energy with thermal expansion","abstract":"We propose a method to evaluate the Gibbs free energy from constant-volume first-principles calculations. The volume integral of the pressure is performed by determining the volume and the bulk modulus in equilibrium at finite temperatures, where the pressure and its volume derivative are evaluated utilizing first-principles calculations of the Gr\\\"uneisen parameter without varying the volume. As an example, the validity of our method is demonstrated for fcc-Al by comparing with the conventional quasiharmonic approximation that is much more computationally-demanding.","sentences":["We propose a method to evaluate the Gibbs free energy from constant-volume first-principles calculations.","The volume integral of the pressure is performed by determining the volume and the bulk modulus in equilibrium at finite temperatures, where the pressure and its volume derivative are evaluated utilizing first-principles calculations of the Gr\\\"uneisen parameter without varying the volume.","As an example, the validity of our method is demonstrated for fcc-Al by comparing with the conventional quasiharmonic approximation that is much more computationally-demanding."],"url":"http://arxiv.org/abs/2405.14823v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 17:19:18","title":"Increasing AGN sample completeness using long-term near-infrared variability","abstract":"In this work, we use 8 years of deep near-infrared imaging to select and study a new set of 601 active galaxies identified through long-term near-infrared (NIR) variability in the UKIDSS Ultra Deep Survey (UDS). These objects are compared to 710 X-ray bright AGN detected by the Chandra X-ray observatory. We show that infrared variability and X-ray emission select distinct sets of active galaxies, finding only a 37 per cent overlap of galaxies detected by both techniques and confirming NIR-variable AGN to be typically X-ray quiet. Examining the mass functions of the active galaxies shows that NIR variability detects AGN activity in galaxies over a significantly wider range of host stellar mass compared to X-ray detection. For example, at z $\\sim$ 1, variable AGN are identified among approximately 1 per cent of galaxies in a roughly flat distribution above the stellar mass completeness limit (> 10$^{9}$M$_{\\odot}$), while X-ray detection primarily identifies AGN in galaxies of higher mass (> 10$^{10}$M$_{\\odot}$). We conclude that long-term near-infrared variability provides an important new tool for obtaining more complete samples of AGN in deep survey fields.","sentences":["In this work, we use 8 years of deep near-infrared imaging to select and study a new set of 601 active galaxies identified through long-term near-infrared (NIR) variability in the UKIDSS Ultra Deep Survey (UDS).","These objects are compared to 710 X-ray bright AGN detected by the Chandra X-ray observatory.","We show that infrared variability and X-ray emission select distinct sets of active galaxies, finding only a 37 per cent overlap of galaxies detected by both techniques and confirming NIR-variable AGN to be typically X-ray quiet.","Examining the mass functions of the active galaxies shows that NIR variability detects AGN activity in galaxies over a significantly wider range of host stellar mass compared to X-ray detection.","For example, at z $\\sim$ 1, variable AGN are identified among approximately 1 per cent of galaxies in a roughly flat distribution above the stellar mass completeness limit (> 10$^{9}$M$_{\\odot}$), while X-ray detection primarily identifies AGN in galaxies of higher mass (> 10$^{10}$M$_{\\odot}$).","We conclude that long-term near-infrared variability provides an important new tool for obtaining more complete samples of AGN in deep survey fields."],"url":"http://arxiv.org/abs/2405.14809v1","category":"astro-ph.GA"}
{"created":"2024-05-23 16:52:14","title":"Low-Energy Line Codes for On-Chip Networks","abstract":"Energy is a primary constraint in processor design, and much of that energy is consumed in on-chip communication. Communication can be intra-core (e.g., from a register file to an ALU) or inter-core (e.g., over the on-chip network). In this paper, we use the on-chip network (OCN) as a case study for saving on-chip communication energy. We have identified a new way to reduce the OCN's link energy consumption by using line coding, a longstanding technique in information theory. Our line codes, called Low-Energy Line Codes (LELCs), reduce energy by reducing the frequency of voltage transitions of the links, and they achieve a range of energy/performance trade-offs.","sentences":["Energy is a primary constraint in processor design, and much of that energy is consumed in on-chip communication.","Communication can be intra-core (e.g., from a register file to an ALU) or inter-core (e.g., over the on-chip network).","In this paper, we use the on-chip network (OCN) as a case study for saving on-chip communication energy.","We have identified a new way to reduce the OCN's link energy consumption by using line coding, a longstanding technique in information theory.","Our line codes, called Low-Energy Line Codes (LELCs), reduce energy by reducing the frequency of voltage transitions of the links, and they achieve a range of energy/performance trade-offs."],"url":"http://arxiv.org/abs/2405.14783v1","category":"cs.HC"}
{"created":"2024-05-23 16:45:07","title":"Topological Weyl Altermagnetism in CrSb","abstract":"Altermagnets constitute a novel, third fundamental class of collinear magnetic ordered materials, alongside with ferro- and antiferromagnets. They share with conventional antiferromagnets the feature of a vanishing net magnetization. At the same time they show a spin-splitting of electronic bands, just as in ferromagnets, caused by the atomic exchange interaction. Interestingly, the altermagnetic spin-splitting may in principle allow for topological electronic structures far beyond collinear antiferromagnets with spin-degenerate bands. Here, using high-resolution and spin angle-resolved photoemission spectroscopy, we observe that the momentum-dependent spin-splitting in altermagnetic CrSb reaches up to 1 eV, while vanishing at high symmetry lines in the Brillouin zone. We find it causes spin-carrying altermagnetic Weyl points, and observe their protected Fermi-arc surface states. This establishes that in altermagnets the large energy scale intrinsic to the spin-splitting - orders of magnitude larger that the relativistic spin-orbit coupling - creates its own realm of robust electronic topology, with spin-polarized topological surface states that are altermagnetic.","sentences":["Altermagnets constitute a novel, third fundamental class of collinear magnetic ordered materials, alongside with ferro- and antiferromagnets.","They share with conventional antiferromagnets the feature of a vanishing net magnetization.","At the same time they show a spin-splitting of electronic bands, just as in ferromagnets, caused by the atomic exchange interaction.","Interestingly, the altermagnetic spin-splitting may in principle allow for topological electronic structures far beyond collinear antiferromagnets with spin-degenerate bands.","Here, using high-resolution and spin angle-resolved photoemission spectroscopy, we observe that the momentum-dependent spin-splitting in altermagnetic CrSb reaches up to 1 eV, while vanishing at high symmetry lines in the Brillouin zone.","We find it causes spin-carrying altermagnetic Weyl points, and observe their protected Fermi-arc surface states.","This establishes that in altermagnets the large energy scale intrinsic to the spin-splitting - orders of magnitude larger that the relativistic spin-orbit coupling - creates its own realm of robust electronic topology, with spin-polarized topological surface states that are altermagnetic."],"url":"http://arxiv.org/abs/2405.14777v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-23 16:27:33","title":"Perazzo $n$-folds and the weak Lefschetz property","abstract":"In this paper, we determine the maximum $h_{max}$ and the minimum $h_{min}$ of the Hilbert vectors of Perazzo algebras $A_F$, where $F$ is a Perazzo polynomial of degree $d$ in $n+m+1$ variables. These algebras always fail the Strong Lefschetz Property. We determine the integers $n,m,d$ such that $h_{max}$ (resp. $h_{min}$) is unimodal, and we prove that $A_F$ always fails the Weak Lefschetz Property if its Hilbert vector is maximum, while it satisfies the Weak Lefschetz Property if it is minimum, unimodal, and satisfies an additional mild condition. We determine the minimal free resolution of Perazzo algebras associated to Perazzo threefolds in $\\mathbb P^4$ with minimum Hilbert vectors. Finally we pose some open problems in this context.   Dedicated to Enrique Arrondo on the occasion of his $60^{th}$ birthday.","sentences":["In this paper, we determine the maximum $h_{max}$ and the minimum $h_{min}$ of the Hilbert vectors of Perazzo algebras $A_F$, where $F$ is a Perazzo polynomial of degree $d$ in $n+m+1$ variables.","These algebras always fail the Strong Lefschetz Property.","We determine the integers $n,m,d$ such that $h_{max}$ (resp.","$h_{min}$) is unimodal, and we prove that $A_F$ always fails the Weak Lefschetz Property if its Hilbert vector is maximum, while it satisfies the Weak Lefschetz Property if it is minimum, unimodal, and satisfies an additional mild condition.","We determine the minimal free resolution of Perazzo algebras associated to Perazzo threefolds in $\\mathbb P^4$ with minimum Hilbert vectors.","Finally we pose some open problems in this context.   ","Dedicated to Enrique Arrondo on the occasion of his $60^{th}$ birthday."],"url":"http://arxiv.org/abs/2405.14756v1","category":"math.AC"}
{"created":"2024-05-23 15:32:53","title":"$B$ meson mixing at NNLO: technical aspects","abstract":"We provide details to several technical aspects which are important for the calculation of next-to-next-to-leading order corrections to the mixing of neutral $B$ mesons. This includes the computation of the master integrals for finite charm and bottom quark masses, traces over products of up to 22 $\\gamma$ matrices and tensor integrals with up to rank 11.","sentences":["We provide details to several technical aspects which are important for the calculation of next-to-next-to-leading order corrections to the mixing of neutral $B$ mesons.","This includes the computation of the master integrals for finite charm and bottom quark masses, traces over products of up to 22 $\\gamma$ matrices and tensor integrals with up to rank 11."],"url":"http://arxiv.org/abs/2405.14698v1","category":"hep-ph"}
{"created":"2024-05-23 15:24:51","title":"Quantum thermodynamic derivation of the energy resolution limit in magnetometry","abstract":"It was recently demonstrated that a large number of magnetic sensing technologies satisfy the energy resolution limit, which connects a quantity composed by the variance of the magnetic field estimate, the sensor volume and the measurement time, and having units of action, with $\\hbar$. A first-principles derivation of the energy resolution limit is still elusive. We here present such a derivation based on quantum thermodynamic arguments. We show that the energy resolution limit is a result of quantum thermodynamic work associated with quantum measurement and Landauer erasure, the work being exchanged with the magnetic field. We apply these considerations to atomic magnetometers and SQUIDS. Regarding the former, we unravel a new spin correlation effect relevant to the magnetic noise produced by atomic vapors.","sentences":["It was recently demonstrated that a large number of magnetic sensing technologies satisfy the energy resolution limit, which connects a quantity composed by the variance of the magnetic field estimate, the sensor volume and the measurement time, and having units of action, with $\\hbar$. A first-principles derivation of the energy resolution limit is still elusive.","We here present such a derivation based on quantum thermodynamic arguments.","We show that the energy resolution limit is a result of quantum thermodynamic work associated with quantum measurement and Landauer erasure, the work being exchanged with the magnetic field.","We apply these considerations to atomic magnetometers and SQUIDS.","Regarding the former, we unravel a new spin correlation effect relevant to the magnetic noise produced by atomic vapors."],"url":"http://arxiv.org/abs/2405.14687v1","category":"quant-ph"}
{"created":"2024-05-23 15:00:48","title":"WTP19aalnxx: Discovery of a bright mid-infrared transient in the emerging class of low luminosity supernovae revealed by delayed circumstellar interaction","abstract":"While core-collapse supernovae (SNe) often show early and consistent signs of circumstellar (CSM) interaction, some exhibit delayed signatures due to interaction with distant material around the progenitor star. Here we present the discovery in NEOWISE data of WTP19aalnxx, a luminous mid-infrared (IR) transient in the outskirts of the galaxy KUG 0022-007 at $\\approx 190$ Mpc. First detected in 2018, WTP19aalnxx reaches a peak absolute (Vega) magnitude of $\\approx-22$ at $4.6 \\, \\mu$m in $\\approx3$ yr, comparable to the most luminous interacting SNe. Archival data reveal a $\\gtrsim 5\\times$ fainter optical counterpart detected since 2015, while follow-up near-IR observations in 2022 reveal an extremely red ($Ks-W2 \\approx 3.7$ mag) active transient. Deep optical spectroscopy confirm strong CSM interaction signatures via intermediate-width Balmer emission lines and coronal metal lines. Modeling the broadband spectral energy distribution, we estimate the presence of $\\gtrsim 10^{-2}$ M$_\\odot$ of warm dust, likely formed in the shock interaction region. Together with the lack of nebular Fe emission, we suggest that WTP19aalnxx is a missed, low (optical) luminosity SN in an emerging family of core-collapse SNe distinguished by their CSM-interaction-powered mid-IR emission that outshines the optical bands. Investigating the Zwicky Transient Facility sample of SNe in NEOWISE data, we find $17$ core-collapse SNe ($\\gtrsim 3$% in a volume-limited sample) without early signs of CSM interaction that exhibit delayed IR brightening, suggestive of dense CSM shells at $\\lesssim 10^{17}$cm. We suggest that synoptic IR surveys offer a new route to revealing late-time CSM interaction and the prevalence of intense terminal mass loss in massive stars.","sentences":["While core-collapse supernovae (SNe) often show early and consistent signs of circumstellar (CSM) interaction, some exhibit delayed signatures due to interaction with distant material around the progenitor star.","Here we present the discovery in NEOWISE data of WTP19aalnxx, a luminous mid-infrared (IR) transient in the outskirts of the galaxy KUG 0022-007 at $\\approx 190$ Mpc.","First detected in 2018, WTP19aalnxx reaches a peak absolute (Vega) magnitude of $\\approx-22$ at $4.6 \\, \\mu$m in $\\approx3$ yr, comparable to the most luminous interacting SNe.","Archival data reveal a $\\gtrsim 5\\times$ fainter optical counterpart detected since 2015, while follow-up near-IR observations in 2022 reveal an extremely red ($Ks-W2 \\approx 3.7$ mag) active transient.","Deep optical spectroscopy confirm strong CSM interaction signatures via intermediate-width Balmer emission lines and coronal metal lines.","Modeling the broadband spectral energy distribution, we estimate the presence of $\\gtrsim 10^{-2}$ M$_\\odot$ of warm dust, likely formed in the shock interaction region.","Together with the lack of nebular Fe emission, we suggest that WTP19aalnxx is a missed, low (optical) luminosity SN in an emerging family of core-collapse SNe distinguished by their CSM-interaction-powered mid-IR emission that outshines the optical bands.","Investigating the Zwicky Transient Facility sample of SNe in NEOWISE data, we find $17$ core-collapse SNe ($\\gtrsim 3$% in a volume-limited sample) without early signs of CSM interaction that exhibit delayed IR brightening, suggestive of dense CSM shells at $\\lesssim 10^{17}$cm.","We suggest that synoptic IR surveys offer a new route to revealing late-time CSM interaction and the prevalence of intense terminal mass loss in massive stars."],"url":"http://arxiv.org/abs/2405.14663v1","category":"astro-ph.HE"}
{"created":"2024-05-23 14:42:37","title":"Repurposing of the Run 2 CMS High Level Trigger Infrastructure as a Cloud Resource for Offline Computing","abstract":"The former CMS Run 2 High Level Trigger (HLT) farm is one of the largest contributors to CMS compute resources, providing about 25k job slots for offline computing. This CPU farm was initially employed as an opportunistic resource, exploited during inter-fill periods, in the LHC Run 2. Since then, it has become a nearly transparent extension of the CMS capacity at CERN, being located on-site at the LHC interaction point 5 (P5), where the CMS detector is installed. This resource has been configured to support the execution of critical CMS tasks, such as prompt detector data reconstruction. It can therefore be used in combination with the dedicated Tier 0 capacity at CERN, in order to process and absorb peaks in the stream of data coming from the CMS detector. The initial configuration for this resource, based on statically configured VMs, provided the required level of functionality. However, regular operations of this cluster revealed certain limitations compared to the resource provisioning and use model employed in the case of WLCG sites. A new configuration, based on a vacuum-like model, has been implemented for this resource in order to solve the detected shortcomings. This paper reports about this redeployment work on the permanent cloud for an enhanced support to CMS offline computing, comparing the former and new models' respective functionalities, along with the commissioning effort for the new setup.","sentences":["The former CMS Run 2 High Level Trigger (HLT) farm is one of the largest contributors to CMS compute resources, providing about 25k job slots for offline computing.","This CPU farm was initially employed as an opportunistic resource, exploited during inter-fill periods, in the LHC Run 2.","Since then, it has become a nearly transparent extension of the CMS capacity at CERN, being located on-site at the LHC interaction point 5 (P5), where the CMS detector is installed.","This resource has been configured to support the execution of critical CMS tasks, such as prompt detector data reconstruction.","It can therefore be used in combination with the dedicated Tier 0 capacity at CERN, in order to process and absorb peaks in the stream of data coming from the CMS detector.","The initial configuration for this resource, based on statically configured VMs, provided the required level of functionality.","However, regular operations of this cluster revealed certain limitations compared to the resource provisioning and use model employed in the case of WLCG sites.","A new configuration, based on a vacuum-like model, has been implemented for this resource in order to solve the detected shortcomings.","This paper reports about this redeployment work on the permanent cloud for an enhanced support to CMS offline computing, comparing the former and new models' respective functionalities, along with the commissioning effort for the new setup."],"url":"http://arxiv.org/abs/2405.14639v1","category":"cs.DC"}
{"created":"2024-05-23 14:40:12","title":"Critical Short-Time Behavior of Majority-Vote Model on Scale-Free Networks","abstract":"We discuss the short-time behavior of the majority vote dynamics on scale-free networks at the critical threshold. We introduce a heterogeneous mean-field theory on the critical short-time behavior of the majority-vote model on scale-free networks. In addition, we also compare the heterogeneous mean-field predictions with extensive Monte Carlo simulations of the short-time dependencies of the order parameter and the susceptibility. We obtained a closed expression for the dynamical exponent $z$ and the time correlation exponent $\\nu_\\parallel$. Short-time scaling is compatible with a non-universal critical behavior for $5/2 < \\gamma < 7/2$, and for $\\gamma \\geq 7/2$, we have the mean-field Ising criticality with additional logarithmic corrections for $\\gamma=7/2$, in the same way as the stationary scaling.","sentences":["We discuss the short-time behavior of the majority vote dynamics on scale-free networks at the critical threshold.","We introduce a heterogeneous mean-field theory on the critical short-time behavior of the majority-vote model on scale-free networks.","In addition, we also compare the heterogeneous mean-field predictions with extensive Monte Carlo simulations of the short-time dependencies of the order parameter and the susceptibility.","We obtained a closed expression for the dynamical exponent $z$ and the time correlation exponent $\\nu_\\parallel$. Short-time scaling is compatible with a non-universal critical behavior for $5/2 < \\gamma < 7/2$, and for $\\gamma \\geq 7/2$, we have the mean-field Ising criticality with additional logarithmic corrections for $\\gamma=7/2$, in the same way as the stationary scaling."],"url":"http://arxiv.org/abs/2405.14634v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-23 14:34:19","title":"Functional Renormalization Group Analysis of $O(3)$ Nonlinear Sigma Model and Non-Abelian Bosonization Duality","abstract":"It is known that the $SU(2)$ Wess-Zumino-Witten model is dual to the free fermion theory in two dimensions via non-Abelian bosonization. Additionally, the $SU(2)$ Wess-Zumino-Witten model is believed to be equivalent to the $O(3)$ nonlinear sigma model with the theta term. In this work, we reexamine this duality through the lens of renormalization group (RG) flow. We analyze the RG flow structure of the $O(3)$ nonlinear sigma model with the theta term in two dimensions using the functional renormalization group. Our results reveal a nontrivial fixed point with a nonzero value of the topological coupling. The scaling dimensions (critical exponents) at this fixed point suggest the realization of duality between the $O(3)$ nonlinear sigma model with the theta term and the free fermion theory, indicating that these models belong to the same universality class.","sentences":["It is known that the $SU(2)$ Wess-Zumino-Witten model is dual to the free fermion theory in two dimensions via non-Abelian bosonization.","Additionally, the $SU(2)$ Wess-Zumino-Witten model is believed to be equivalent to the $O(3)$ nonlinear sigma model with the theta term.","In this work, we reexamine this duality through the lens of renormalization group (RG) flow.","We analyze the RG flow structure of the $O(3)$ nonlinear sigma model with the theta term in two dimensions using the functional renormalization group.","Our results reveal a nontrivial fixed point with a nonzero value of the topological coupling.","The scaling dimensions (critical exponents) at this fixed point suggest the realization of duality between the $O(3)$ nonlinear sigma model with the theta term and the free fermion theory, indicating that these models belong to the same universality class."],"url":"http://arxiv.org/abs/2405.14627v1","category":"hep-th"}
{"created":"2024-05-23 14:22:58","title":"Flavored Dark Matter: Freeze-out Scenarios and LHC Signatures","abstract":"We present the phenomenology of a simplified dark matter model within the dark minimal flavor violation framework, extending the standard model with a Majorana fermion flavor triplet and colored scalar mediator. We explore the allowed parameter space considering constraints from flavor observables, direct and indirect dark matter detection, and the relic density measurement. Our analysis reveals previously unexplored scenarios such as conversion-driven freeze-out within the model. We study limits from the LHC and discuss new signatures of the model, highlighting its richer phenomenology compared to single flavor dark matter.","sentences":["We present the phenomenology of a simplified dark matter model within the dark minimal flavor violation framework, extending the standard model with a Majorana fermion flavor triplet and colored scalar mediator.","We explore the allowed parameter space considering constraints from flavor observables, direct and indirect dark matter detection, and the relic density measurement.","Our analysis reveals previously unexplored scenarios such as conversion-driven freeze-out within the model.","We study limits from the LHC and discuss new signatures of the model, highlighting its richer phenomenology compared to single flavor dark matter."],"url":"http://arxiv.org/abs/2405.14610v1","category":"hep-ph"}
{"created":"2024-05-23 14:00:23","title":"A Study of the Spectral properties of Gamma-Ray Bursts with the Precursors and Main bursts","abstract":"There is no consensus yet on whether the precursor and the main burst of gamma-ray bursts (GRBs) have the same origin, and their jet composition is still unclear. In order to further investigate this issue, we systematically search 21 Fermi GRBs with both precursor and main burst for spectral analysis. We first perform Bayesian time-resolved spectral analysis and find that almost all the precursors and the main bursts (94.4$\\%$) exhibit thermal components, and the vast majority of them have low-energy spectral index ($\\alpha$) (72.2$\\%$) that exceed the limit of synchrotron radiation. We then analyse the evolution and correlation of the spectral parameters and find that approximately half of the $\\alpha$ (50$\\%$) of the precursors and the main bursts evolve in a similar pattern, while peak energy ($E_{p}$) (55.6$\\%$) behave similarly, and their evolution is mainly characterized by flux tracking; for the $\\alpha-F$ (the flux) relation, more than half of the precursors and the main bursts (61.1$\\%$) exhibit roughly similar patterns; the $E_{p}-F$ relation in both the precursor and main burst (100$\\%$) exhibits a positive correlation of at least moderate strength. Next, we constrain the outflow properties of the precursors and the main bursts and find that most of them exhibit typical properties of photosphere radiation. Finally, we compare the time-integrated spectra of the precursors and the main bursts and find that nearly all of them are located in similar regions of the Amati relation and follow the Yonetoku relation. Therefore, we conclude that main bursts are continuations of precursors and they may share a common physical origin.","sentences":["There is no consensus yet on whether the precursor and the main burst of gamma-ray bursts (GRBs) have the same origin, and their jet composition is still unclear.","In order to further investigate this issue, we systematically search 21 Fermi GRBs with both precursor and main burst for spectral analysis.","We first perform Bayesian time-resolved spectral analysis and find that almost all the precursors and the main bursts (94.4$\\%$) exhibit thermal components, and the vast majority of them have low-energy spectral index ($\\alpha$) (72.2$\\%$) that exceed the limit of synchrotron radiation.","We then analyse the evolution and correlation of the spectral parameters and find that approximately half of the $\\alpha$ (50$\\%$) of the precursors and the main bursts evolve in a similar pattern, while peak energy ($E_{p}$) (55.6$\\%$) behave similarly, and their evolution is mainly characterized by flux tracking; for the $\\alpha-F$ (the flux) relation, more than half of the precursors and the main bursts (61.1$\\%$) exhibit roughly similar patterns; the $E_{p}-F$ relation in both the precursor and main burst (100$\\%$) exhibits a positive correlation of at least moderate strength.","Next, we constrain the outflow properties of the precursors and the main bursts and find that most of them exhibit typical properties of photosphere radiation.","Finally, we compare the time-integrated spectra of the precursors and the main bursts and find that nearly all of them are located in similar regions of the Amati relation and follow the Yonetoku relation.","Therefore, we conclude that main bursts are continuations of precursors and they may share a common physical origin."],"url":"http://arxiv.org/abs/2405.14588v1","category":"astro-ph.HE"}
{"created":"2024-05-23 13:53:27","title":"Free p-algebras revisited: an algebraic investigation of implication-free intuitionism","abstract":"We give a new construction of free distributive p-algebras. Our construction relies on a detailed description of completely meet-irreducible congruences, so it is purely universal algebraic. It yields a normal form theorem for p-algebra terms, simpler proofs of several existing results, as well as a complete characterisation of structurally complete varieties of p-algebras.","sentences":["We give a new construction of free distributive p-algebras.","Our construction relies on a detailed description of completely meet-irreducible congruences, so it is purely universal algebraic.","It yields a normal form theorem for p-algebra terms, simpler proofs of several existing results, as well as a complete characterisation of structurally complete varieties of p-algebras."],"url":"http://arxiv.org/abs/2405.14581v1","category":"math.LO"}
{"created":"2024-05-23 13:42:13","title":"Giant graviton expansions and ETW brane","abstract":"We study the giant gravitons in the $AdS_4$ bagpipe geometries involving end-of-the-world (ETW) brane constructed by a single $5$-brane and either two stacks or one stack of D3-branes in Type IIB string theory. From the exact formulae and giant graviton expansions of the half-indices for the half-BPS boundary conditions and interfaces in $\\mathcal{N}=4$ super Yang-Mills theory, we obtain the BPS spectra of the fluctuation modes of the $AdS_4$ bagpipe geometries including the ETW brane region.","sentences":["We study the giant gravitons in the $AdS_4$ bagpipe geometries involving end-of-the-world (ETW) brane constructed by a single $5$-brane and either two stacks or one stack of D3-branes in Type IIB string theory.","From the exact formulae and giant graviton expansions of the half-indices for the half-BPS boundary conditions and interfaces in $\\mathcal{N}=4$ super Yang-Mills theory, we obtain the BPS spectra of the fluctuation modes of the $AdS_4$ bagpipe geometries including the ETW brane region."],"url":"http://arxiv.org/abs/2405.14564v1","category":"hep-th"}
{"created":"2024-05-23 13:22:36","title":"Tau and low multiplicity decays at Belle and Belle II","abstract":"We present recent measurements of $\\tau$ physics at the Belle II experiment at SuperKEKB. Measurements include a test of $e-\\mu$ lepton flavor universality in $\\tau$ decays, a search for the lepton flavor violating $\\tau \\rightarrow \\mu \\mu \\mu$ decay and a precision measurement of the $\\tau$ lepton invariant mass. In addition, we present a measurement of the low multiplicity decay cross section $\\sigma(e^+ e^- \\rightarrow \\pi^+ \\pi^- \\pi^0)$, a key input in the theoretical prediction of the anomalous magnetic moment of the muon.","sentences":["We present recent measurements of $\\tau$ physics at the Belle II experiment at SuperKEKB.","Measurements include a test of $e-\\mu$ lepton flavor universality in $\\tau$ decays, a search for the lepton flavor violating $\\tau \\rightarrow \\mu \\mu \\mu$ decay and a precision measurement of the $\\tau$ lepton invariant mass.","In addition, we present a measurement of the low multiplicity decay cross section $\\sigma(e^+ e^- \\rightarrow \\pi^+ \\pi^- \\pi^0)$, a key input in the theoretical prediction of the anomalous magnetic moment of the muon."],"url":"http://arxiv.org/abs/2405.14537v1","category":"hep-ex"}
{"created":"2024-05-23 13:07:57","title":"Measurement of the impact-parameter dependent azimuthal anisotropy in coherent $\u03c1^0$ photoproduction in Pb$-$Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV","abstract":"The first measurement of the impact-parameter dependent angular anisotropy in the decay of coherently photoproduced $\\rho^0$ mesons is presented. The $\\rho^0$ mesons are reconstructed through their decay into a pion pair. The measured anisotropy corresponds to the amplitude of the $\\cos(2\\phi)$ modulation, where $\\phi$ is the angle between the two vectors formed by the sum and the difference of the transverse momenta of the pions, respectively. The measurement was performed by the ALICE Collaboration at the LHC using data from ultraperipheral Pb$-$Pb collisions at a center-of-mass energy of $\\sqrt{s_{\\mathrm{NN}}}~=~5.02$ TeV per nucleon pair. Different impact-parameter regions are selected by classifying the events in nuclear-breakup classes. The amplitude of the $\\cos(2\\phi)$ modulation is found to increase by about one order of magnitude from large to small impact parameters. Theoretical calculations, which describe the measurement, explain the $\\cos(2\\phi)$ anisotropy as the result of a quantum interference effect at the femtometer scale that arises from the ambiguity as to which of the nuclei is the source of the photon in the interaction.","sentences":["The first measurement of the impact-parameter dependent angular anisotropy in the decay of coherently photoproduced $\\rho^0$ mesons is presented.","The $\\rho^0$ mesons are reconstructed through their decay into a pion pair.","The measured anisotropy corresponds to the amplitude of the $\\cos(2\\phi)$ modulation, where $\\phi$ is the angle between the two vectors formed by the sum and the difference of the transverse momenta of the pions, respectively.","The measurement was performed by the ALICE Collaboration at the LHC using data from ultraperipheral Pb$-$Pb collisions at a center-of-mass energy of $\\sqrt{s_{\\mathrm{NN}}}~=~5.02$ TeV per nucleon pair.","Different impact-parameter regions are selected by classifying the events in nuclear-breakup classes.","The amplitude of the $\\cos(2\\phi)$ modulation is found to increase by about one order of magnitude from large to small impact parameters.","Theoretical calculations, which describe the measurement, explain the $\\cos(2\\phi)$ anisotropy as the result of a quantum interference effect at the femtometer scale that arises from the ambiguity as to which of the nuclei is the source of the photon in the interaction."],"url":"http://arxiv.org/abs/2405.14525v1","category":"nucl-ex"}
{"created":"2024-05-23 12:48:30","title":"Quasi-isotropic UV Emission in the ULX NGC~1313~X--1","abstract":"A major prediction of most super-Eddington accretion theories is the presence of anisotropic emission from supercritical disks, but the degree of anisotropy and its dependency with energy remain poorly constrained observationally. A key breakthrough allowing to test such predictions was the discovery of high-excitation photoionized nebulae around Ultraluminous X-ray sources (ULXs). We present efforts to tackle the degree of anisotropy of the UV/EUV emission in super-Eddington accretion flows by studying the emission-line nebula around the archetypical ULX NGC~1313~X--1. We first take advantage of the extensive wealth of optical/near-UV and X-ray data from \\textit{Hubble Space Telescope}, \\textit{XMM-Newton}, \\textit{Swift}-XRT and \\textit{NuSTAR} observatories to perform multi-band, state-resolved spectroscopy of the source to constrain the spectral energy distribution (SED) along the line of sight. We then compare spatially-resolved \\texttt{Cloudy} predictions using the observed line-of-sight SED with the nebular line ratios to assess whether the nebula `sees' the same SED as observed along the line of sight. We show that to reproduce the line ratios in the surrounding nebula, the photo-ionizing SED must be a factor $\\approx 4$ dimmer in ultraviolet emission than along the line-of-sight. Such nearly-iosotropic UV emission may be attributed to the quasi-spherical emission from the wind photosphere. We also discuss the apparent dichotomy in the observational properties of emission-line nebulae around soft and hard ULXs, and suggest only differences in mass-transfer rates can account for the EUV/X-ray spectral differences, as opposed to inclination effects. Finally, our multi-band spectroscopy suggest the optical/near-UV emission is not dominated by the companion star.","sentences":["A major prediction of most super-Eddington accretion theories is the presence of anisotropic emission from supercritical disks, but the degree of anisotropy and its dependency with energy remain poorly constrained observationally.","A key breakthrough allowing to test such predictions was the discovery of high-excitation photoionized nebulae around Ultraluminous X-ray sources (ULXs).","We present efforts to tackle the degree of anisotropy of the UV/EUV emission in super-Eddington accretion flows by studying the emission-line nebula around the archetypical ULX NGC~1313~X--1.","We first take advantage of the extensive wealth of optical/near-UV and X-ray data from \\textit{Hubble Space Telescope}, \\textit{XMM-Newton}, \\textit{Swift}-XRT and \\textit{NuSTAR} observatories to perform multi-band, state-resolved spectroscopy of the source to constrain the spectral energy distribution (SED) along the line of sight.","We then compare spatially-resolved \\texttt{Cloudy} predictions using the observed line-of-sight SED with the nebular line ratios to assess whether the nebula `sees' the same SED as observed along the line of sight.","We show that to reproduce the line ratios in the surrounding nebula, the photo-ionizing SED must be a factor $\\approx 4$ dimmer in ultraviolet emission than along the line-of-sight.","Such nearly-iosotropic UV emission may be attributed to the quasi-spherical emission from the wind photosphere.","We also discuss the apparent dichotomy in the observational properties of emission-line nebulae around soft and hard ULXs, and suggest only differences in mass-transfer rates can account for the EUV/X-ray spectral differences, as opposed to inclination effects.","Finally, our multi-band spectroscopy suggest the optical/near-UV emission is not dominated by the companion star."],"url":"http://arxiv.org/abs/2405.14512v1","category":"astro-ph.HE"}
{"created":"2024-05-23 12:16:37","title":"Experimental observations of bifurcated power decay lengths in the near Scrape-Off Layer of tokamak plasmas","abstract":"The scrape-off layer parallel heat flux decay lengths measured at ST40, a high field, low aspect ratio spherical tokamak, have been observed to bifurcate into two groups. The wide group matches closely with the scale of ion poloidal Larmour radius and follows existing H-mode scalings, while the narrow group falls up to 10 times below scalings, on the scale of ion total Larmour radius. The onset of the narrow scrape-off layer width is observed to be associated with suppressed magnetic fluctuations, suggesting reduced electromagnetic turbulence levels in the SOL.","sentences":["The scrape-off layer parallel heat flux decay lengths measured at ST40, a high field, low aspect ratio spherical tokamak, have been observed to bifurcate into two groups.","The wide group matches closely with the scale of ion poloidal Larmour radius and follows existing H-mode scalings, while the narrow group falls up to 10 times below scalings, on the scale of ion total Larmour radius.","The onset of the narrow scrape-off layer width is observed to be associated with suppressed magnetic fluctuations, suggesting reduced electromagnetic turbulence levels in the SOL."],"url":"http://arxiv.org/abs/2405.14485v1","category":"physics.plasm-ph"}
{"created":"2024-05-23 10:41:35","title":"Schwarzschild metric from Scattering Amplitudes to all orders in $G_N$","abstract":"We apply a formulation of Einstein's general relativity with only cubic interactions for deriving the metric of a Schwarzschild black hole to all orders in perturbation theory. This cubic interactions formulation coupled to effective worldline action of a massive point particle allows to derive a recursion relation for the form factors of the off-shell graviton emission current. The unique solution to the recursion relation leads to the Schwarzschild black-hole solution in four dimensions. This provides the first derivation of the black hole metric from a matter source to all orders in perturbation theory from an amplitude approach.","sentences":["We apply a formulation of Einstein's general relativity with only cubic interactions for deriving the metric of a Schwarzschild black hole to all orders in perturbation theory.","This cubic interactions formulation coupled to effective worldline action of a massive point particle allows to derive a recursion relation for the form factors of the off-shell graviton emission current.","The unique solution to the recursion relation leads to the Schwarzschild black-hole solution in four dimensions.","This provides the first derivation of the black hole metric from a matter source to all orders in perturbation theory from an amplitude approach."],"url":"http://arxiv.org/abs/2405.14421v1","category":"hep-th"}
{"created":"2024-05-23 10:33:01","title":"Experimental investigation of an electronegative cylindrical capacitively coupled geometrically asymmetric plasma discharge with an axisymmetric magnetic field","abstract":"In this study, we have investigated the production of negative ions by mixing electronegative oxygen gas with electropositive argon gas in a geometrically asymmetric cylindrical capacitively coupled radio frequency plasma discharge. The plasma parameters such as density (electron, positive and negative ion), negative ion fraction, and electron temperature are investigated for fixed gas pressure and increasing axial magnetic field strength. The axisymmetric magnetic field creates an ExB drift in the azimuthal direction, leading to the confinement of high-energy electrons at the radial edge of the chamber, resulting in decreased species density and negative ion fraction in the plasma bulk. However, the electron temperature increases with the magnetic field. It is concluded that low magnetic fields are better suited for negative ion production in such devices. Furthermore, in addition to the percentage ratio of the two gases, the applied axial magnetic field also plays a vital role in controlling negative ion fraction.","sentences":["In this study, we have investigated the production of negative ions by mixing electronegative oxygen gas with electropositive argon gas in a geometrically asymmetric cylindrical capacitively coupled radio frequency plasma discharge.","The plasma parameters such as density (electron, positive and negative ion), negative ion fraction, and electron temperature are investigated for fixed gas pressure and increasing axial magnetic field strength.","The axisymmetric magnetic field creates an ExB drift in the azimuthal direction, leading to the confinement of high-energy electrons at the radial edge of the chamber, resulting in decreased species density and negative ion fraction in the plasma bulk.","However, the electron temperature increases with the magnetic field.","It is concluded that low magnetic fields are better suited for negative ion production in such devices.","Furthermore, in addition to the percentage ratio of the two gases, the applied axial magnetic field also plays a vital role in controlling negative ion fraction."],"url":"http://arxiv.org/abs/2405.14412v1","category":"physics.plasm-ph"}
{"created":"2024-05-23 09:32:25","title":"Cosmological Constraints on Long Gamma-Ray Bursts from Fermi observations","abstract":"In this paper, we compile a \\emph{Fermi} sample of the \\emph{long} GRBs from \\emph{Fermi} observations with 15 years of the Fermi-GBM catalogue with identified redshift, in which the GOLD sample contains 123 long GRBs at $z\\le5.6$ and the FULL sample contains 151 long GRBs with redshifts at $z\\le8.2$. The Amati relation (the $E_{\\rm p,i}$-$E_{\\rm iso}$ correlation) are calibrated at $z<1.4$ by a Gaussian Process from the latest observational Hubble data (OHD) with the cosmic chronometers method to obtain GRBs at high-redshift $z\\ge1.4$ which can be used to constrain cosmological models via the Markov chain Monte Carlo (MCMC) method. With the cosmology-independent GRBs with the GOLD sample at $z\\ge1.4$ and the Pantheon sample of type Ia supernovae (SNe Ia) at $0.01<z\\leq2.3$, we obtain $\\Omega_{\\rm m} = 0.354\\pm0.018, H_0 = 73.05\\pm0.002$ for the flat $\\Lambda$CDM model; $w_0 = -1.22^{+0.18}_{-0.15}$ for the flat $w$CDM model; and $w_{a} = -1.12^{+0.45}_{-0.83}$ for the flat Chevallier-Polarski-Linder model at the 1$\\sigma$ confidence level. Our results with the GOLD and FULL sample are almost identical, which are more stringent than the previous results with GRBs.","sentences":["In this paper, we compile a \\emph{Fermi} sample of the \\emph{long} GRBs from \\emph{Fermi} observations with 15 years of the Fermi-GBM catalogue with identified redshift, in which the GOLD sample contains 123 long GRBs at $z\\le5.6$ and the FULL sample contains 151 long GRBs with redshifts at $z\\le8.2$. The Amati relation (the $E_{\\rm p,i}$-$E_{\\rm iso}$ correlation) are calibrated at $z<1.4$ by a Gaussian Process from the latest observational Hubble data (OHD) with the cosmic chronometers method to obtain GRBs at high-redshift $z\\ge1.4$ which can be used to constrain cosmological models via the Markov chain Monte Carlo (MCMC) method.","With the cosmology-independent GRBs with the GOLD sample at $z\\ge1.4$ and the Pantheon sample of type Ia supernovae (SNe Ia) at $0.01<z\\leq2.3$, we obtain $\\Omega_{\\rm m} = 0.354\\pm0.018, H_0 = 73.05\\pm0.002$ for the flat $\\Lambda$CDM model; $w_0 = -1.22^{+0.18}_{-0.15}$ for the flat $w$CDM model; and $w_{a} = -1.12^{+0.45}_{-0.83}$ for the flat Chevallier-Polarski-Linder model at the 1$\\sigma$ confidence level.","Our results with the GOLD and FULL sample are almost identical, which are more stringent than the previous results with GRBs."],"url":"http://arxiv.org/abs/2405.14357v1","category":"astro-ph.CO"}
{"created":"2024-05-23 09:32:23","title":"Enhanced area law in the Widom-Sobolev formula for the free Dirac operator in arbitrary dimension","abstract":"We prove a logarithmically enhanced area law for all R\\'enyi entanglement entropies of the ground state of a free gas of relativistic Dirac fermions. Such asymptotics occur in any dimension if the modulus of the Fermi energy is larger than the mass of the particles and in the massless case at Fermi energy zero in one space dimension. In all other cases of mass, Fermi energy and dimension, the entanglement entropy grows no faster than the area of the involved spatial region. The result is established for a general class of test functions which includes the ones corresponding to R\\'enyi entropies and relies on a recently proved extension of the Widom-Sobolev formula to matrix-valued symbols by the authors.","sentences":["We prove a logarithmically enhanced area law for all R\\'enyi entanglement entropies of the ground state of a free gas of relativistic Dirac fermions.","Such asymptotics occur in any dimension if the modulus of the Fermi energy is larger than the mass of the particles and in the massless case at Fermi energy zero in one space dimension.","In all other cases of mass, Fermi energy and dimension, the entanglement entropy grows no faster than the area of the involved spatial region.","The result is established for a general class of test functions which includes the ones corresponding to R\\'enyi entropies and relies on a recently proved extension of the Widom-Sobolev formula to matrix-valued symbols by the authors."],"url":"http://arxiv.org/abs/2405.14356v1","category":"math-ph"}
{"created":"2024-05-23 09:22:17","title":"The ORT and the uGMRT Pulsar Monitoring Program : Pulsar Timing Irregularities & the Gaussian Process Realization","abstract":"The spin-down law of pulsars is generally perturbed by two types of timing irregularities: glitches and timing noise. Glitches are sudden changes in the rotational frequency of pulsars, while timing noise is a discernible stochastic wandering in the phase, period, or spin-down rate of a pulsar. We present the timing results of a sample of glitching pulsars observed using the Ooty Radio Telescope (ORT) and the upgraded Giant Metrewave Radio Telescope (uGMRT). Our findings include timing noise analysis for 17 pulsars, with seven being reported for the first time. We detected five glitches in four pulsars and a glitch-like event in J1825-0935. The frequency evolution of glitch in pulsars, J0742-2822 and J1740-3015, is presented for the first time. Additionally, we report timing noise results for three glitching pulsars. The timing noise was analyzed separately in the pre-glitch region and post-glitch regions. We observed an increase in the red noise parameters in the post-glitch regions, where exponential recovery was considered in the noise analysis. Timing noise can introduce ambiguities in the correct evaluation of glitch observations. Hence, it is important to consider timing noise in glitch analysis. We propose an innovative glitch verification approach designed to discern between a glitch and strong timing noise. The novel glitch analysis technique is also demonstrated using the observed data.","sentences":["The spin-down law of pulsars is generally perturbed by two types of timing irregularities: glitches and timing noise.","Glitches are sudden changes in the rotational frequency of pulsars, while timing noise is a discernible stochastic wandering in the phase, period, or spin-down rate of a pulsar.","We present the timing results of a sample of glitching pulsars observed using the Ooty Radio Telescope (ORT) and the upgraded Giant Metrewave Radio Telescope (uGMRT).","Our findings include timing noise analysis for 17 pulsars, with seven being reported for the first time.","We detected five glitches in four pulsars and a glitch-like event in J1825-0935.","The frequency evolution of glitch in pulsars, J0742-2822 and J1740-3015, is presented for the first time.","Additionally, we report timing noise results for three glitching pulsars.","The timing noise was analyzed separately in the pre-glitch region and post-glitch regions.","We observed an increase in the red noise parameters in the post-glitch regions, where exponential recovery was considered in the noise analysis.","Timing noise can introduce ambiguities in the correct evaluation of glitch observations.","Hence, it is important to consider timing noise in glitch analysis.","We propose an innovative glitch verification approach designed to discern between a glitch and strong timing noise.","The novel glitch analysis technique is also demonstrated using the observed data."],"url":"http://arxiv.org/abs/2405.14351v1","category":"astro-ph.HE"}
{"created":"2024-05-23 09:02:20","title":"Thermal Wash-in Leptogenesis via Heavy Higgs Decay","abstract":"We present a conceptually simple model to generate asymmetries that are not directly related to baryon nor lepton charges. The model employs a three-Higgs doublet framework, wherein the other two Higgs fields are significantly heavier than the Standard Model (SM) Higgs field. The decay of these heavier Higgs fields generates asymmetry for approximately conserved charges in the Standard Model at a high temperature. These asymmetries will be converted into baryon/lepton asymmetry through $B-L$ violating interactions associated with right-handed neutrinos via the wash-in mechanism.","sentences":["We present a conceptually simple model to generate asymmetries that are not directly related to baryon nor lepton charges.","The model employs a three-Higgs doublet framework, wherein the other two Higgs fields are significantly heavier than the Standard Model (SM) Higgs field.","The decay of these heavier Higgs fields generates asymmetry for approximately conserved charges in the Standard Model at a high temperature.","These asymmetries will be converted into baryon/lepton asymmetry through $B-L$ violating interactions associated with right-handed neutrinos via the wash-in mechanism."],"url":"http://arxiv.org/abs/2405.14332v1","category":"hep-ph"}
{"created":"2024-05-23 08:29:52","title":"Gamma-ray Signal from $Z_{N\\geq 3}$ Dark Matter-Companion Models","abstract":"In Ref.~\\cite{Guo:2021rre}, we proposed to replace the final dark matter (DM) particle in the semi-annihilation mode $\\rm DM+DM\\to antiDM+Higgs~boson$ with its $Z_{N\\geq 3}$ companion, thus reducing DM number density without DM-nucleon scattering. In this work, we study the indirect detection signals from DM annihilation, the Higgs boson pair with one of them from the companion decay being on- or off- shell, depending on the DM-companion mass splitting. We generate the photon spectrum by using PYTHIA8 and study the properties of the spectrum, to find that the hard part of the spectrum in our model is mainly shaped by the direct Higgs boson and thus does not differ much from that of the conventional semi-annihilation mode. Using the Fermi-LAT data of white dwarfs, we derive the current limit of the DM annihilation cross section for ${\\rm DM+DM\\to companion^*+Higgs~ boson}$, and for the relatively light DM, it reaches the typical thermal cross section. However, for the TeV scale DM, we have to rely on the Cherenkov Telescope Array, which is able to rule out the whole parameter space except for the coannihilation region.","sentences":["In Ref.~\\cite{Guo:2021rre}, we proposed to replace the final dark matter (DM) particle in the semi-annihilation mode $\\rm DM+DM\\to antiDM+Higgs~boson$ with its $Z_{N\\geq 3}$ companion, thus reducing DM number density without DM-nucleon scattering.","In this work, we study the indirect detection signals from DM annihilation, the Higgs boson pair with one of them from the companion decay being on- or off- shell, depending on the DM-companion mass splitting.","We generate the photon spectrum by using PYTHIA8 and study the properties of the spectrum, to find that the hard part of the spectrum in our model is mainly shaped by the direct Higgs boson and thus does not differ much from that of the conventional semi-annihilation mode.","Using the Fermi-LAT data of white dwarfs, we derive the current limit of the DM annihilation cross section for ${\\rm DM+DM\\to companion^*+Higgs~ boson}$, and for the relatively light DM, it reaches the typical thermal cross section.","However, for the TeV scale DM, we have to rely on the Cherenkov Telescope Array, which is able to rule out the whole parameter space except for the coannihilation region."],"url":"http://arxiv.org/abs/2405.14309v1","category":"hep-ph"}
{"created":"2024-05-23 08:19:32","title":"Dominating $K_t$-Models","abstract":"A $\\textit{dominating $K_t$-model}$ in a graph $G$ is a sequence $(T_1,\\dots,T_t)$ of pairwise disjoint non-empty connected subgraphs of $G$, such that for $1 \\leqslant i<j \\leqslant t$ every vertex in $T_j$ has a neighbour in $T_i$. Replacing \"every vertex in $T_j$\" by \"some vertex in $T_j$\" retrieves the standard definition of $K_t$-model, which is equivalent to $K_t$ being a minor of $G$. We explore in what sense dominating $K_t$-models behave like (non-dominating) $K_t$-models. The two notions are equivalent for $t \\leqslant 3$, but are already very different for $t = 4$, since the 1-subdivision of any graph has no dominating $K_4$-model. Nevertheless, we show that every graph with no dominating $K_4$-model is 2-degenerate and 3-colourable. More generally, we prove that every graph with no dominating $K_t$-model is $2^{t-2}$-colourable.   Motivated by the connection to chromatic number, we study the maximum average degree of graphs with no dominating $K_t$-model. We give an upper bound of $2^{t-2}$, and show that random graphs provide a lower bound of $(1-o(1))t\\log t$, which we conjecture is asymptotically tight. This result is in contrast to the $K_t$-minor-free setting, where the maximum average degree is $\\Theta(t\\sqrt{\\log t})$. The natural strengthening of Hadwiger's Conjecture arises: is every graph with no dominating $K_t$-model $(t-1)$-colourable? We provide two pieces of evidence for this: (1) It is true for almost every graph, (2) Every graph $G$ with no dominating $K_t$-model has a $(t-1)$-colourable induced subgraph on at least half the vertices, which implies there is an independent set of size at least $\\frac{\\lvert V(G) \\rvert}{2t-2}$.","sentences":["A $\\textit{dominating $K_t$-model}$ in a graph $G$ is a sequence $(T_1,\\dots,T_t)$ of pairwise disjoint non-empty connected subgraphs of $G$, such that for $1 \\leqslant i<j \\leqslant t$ every vertex in $","T_j$ has a neighbour in $T_i$. Replacing \"every vertex in $T_j$\" by \"some vertex in $T_j$\" retrieves the standard definition of $K_t$-model, which is equivalent to $K_t$ being a minor of $G$. We explore in what sense dominating $K_t$-models behave like (non-dominating) $K_t$-models.","The two notions are equivalent for $t \\leqslant 3$, but are already very different for $t = 4$, since the 1-subdivision of any graph has no dominating $K_4$-model.","Nevertheless, we show that every graph with no dominating $K_4$-model is 2-degenerate and 3-colourable.","More generally, we prove that every graph with no dominating $K_t$-model is $2^{t-2}$-colourable.   ","Motivated by the connection to chromatic number, we study the maximum average degree of graphs with no dominating $K_t$-model.","We give an upper bound of $2^{t-2}$, and show that random graphs provide a lower bound of $(1-o(1))t\\log t$, which we conjecture is asymptotically tight.","This result is in contrast to the $K_t$-minor-free setting, where the maximum average degree is $\\Theta(t\\sqrt{\\log t})$.","The natural strengthening of Hadwiger's Conjecture arises: is every graph with no dominating $K_t$-model $(t-1)$-colourable?","We provide two pieces of evidence for this: (1) It is true for almost every graph, (2) Every graph $G$ with no dominating $K_t$-model has a $(t-1)$-colourable induced subgraph on at least half the vertices, which implies there is an independent set of size at least $\\frac{\\lvert V(G)","\\rvert}{2t-2}$."],"url":"http://arxiv.org/abs/2405.14299v1","category":"math.CO"}
